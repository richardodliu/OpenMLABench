{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear-regression solver.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target y and the remaining\n        columns are the features X.\n    alpha : float, default 0.0\n        Regularisation strength (\u03b1 \u2265 0).\n        \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias/intercept as the first element)\n        rounded to 4 decimal places.  \n        If (X\u1d40X + \u03b1I) is numerically singular the function returns -1.\n    \"\"\"\n    # Split features and target\n    X, y = data[:, :-1], data[:, -1]\n    n_samples = X.shape[0]\n\n    # Add bias (intercept) column of 1s\n    X_bias = np.hstack((np.ones((n_samples, 1)), X))\n\n    # Form normal equation components\n    A = X_bias.T @ X_bias + alpha * np.eye(X_bias.shape[1])\n    \n    # Check invertibility via determinant\n    try:\n        det = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det, 0.0, atol=1e-12):\n        return -1\n    \n    # Closed-form solution\n    w = np.linalg.inv(A) @ X_bias.T @ y\n    \n    # Round to 4 decimals and return as Python list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Labels (+1 or -1), length n_samples.\n    eta : float, optional\n        Learning rate, default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        Weight vector (as a list) and bias, both rounded to 4 decimals.\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float).ravel()\n    if not set(np.unique(y)).issubset({-1.0, 1.0}):\n        raise ValueError(\"y_train must contain only +1 and -1.\")\n\n    n_samples, n_features = X.shape\n\n    # --- initialise parameters ----------------------------------------------\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    updates = 0          # how many updates have been performed\n\n    # --- pre-compute Gram matrix K(i,j) = x_i \u00b7 x_j --------------------------\n    gram = X @ X.T       # shape (n_samples, n_samples)\n\n    # --- training loop -------------------------------------------------------\n    while updates < n_iter:\n        no_mistake_in_pass = True\n\n        for i in range(n_samples):\n            # activation = \u03a3_j \u03b1_j y_j K(x_j , x_i)\n            activation = np.dot(alpha * y, gram[:, i])\n            margin = y[i] * (activation + b)\n\n            # mis-classified?\n            if margin <= 0.0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updates += 1\n                no_mistake_in_pass = False\n                break                    # restart scan from the first sample\n\n        if no_mistake_in_pass:\n            # completed a full pass with no update \u2192 converged\n            break\n\n    # --- compute primal weight vector ---------------------------------------\n    w = np.dot(alpha * y, X)            # shape (n_features,)\n\n    # --- rounding and return -------------------------------------------------\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A dictionary with three entries  (\"identity\", \"log\", \"logit\").\n        Each entry itself is a dictionary with three callables\n\n            \u2022 \"link\"      :  g(\u03bc)\n            \u2022 \"inv_link\"  :  g\u207b\u00b9(\u03b7)\n            \u2022 \"derivative\":  g\u2032(\u03bc)\n\n        All functions work element\u2013wise on numpy scalars, 1-D and 2-D\n        numpy arrays.\n    \"\"\"\n\n    # ------------------ identity -------------------------------------------\n    def _id_link(mu):\n        return mu\n\n    def _id_inv_link(eta):\n        return eta\n\n    def _id_derivative(mu):\n        # g'(\u03bc) = 1                      (shape must follow \u03bc)\n        return np.ones_like(mu, dtype=float)\n\n    # ------------------ log -------------------------------------------------\n    def _log_link(mu):\n        #  g(\u03bc) = log(\u03bc)      (\u03bc > 0  is expected)\n        return np.log(mu)\n\n    def _log_inv_link(eta):\n        #  g\u207b\u00b9(\u03b7) = exp(\u03b7)\n        return np.exp(eta)\n\n    def _log_derivative(mu):\n        # g'(\u03bc) = 1/\u03bc\n        return 1.0 / mu\n\n    # ------------------ logit ----------------------------------------------\n    def _logit_link(mu):\n        # g(\u03bc) = log( \u03bc / (1-\u03bc) )\n        return np.log(mu / (1.0 - mu))\n\n    def _logit_inv_link(eta):\n        # numerically stable logistic / sigmoid\n        eta = np.asarray(eta)\n        return np.where(\n            eta >= 0,\n            1.0 / (1.0 + np.exp(-eta)),\n            np.exp(eta) / (1.0 + np.exp(eta)),\n        )\n\n    def _logit_derivative(mu):\n        # g'(\u03bc) = 1 / ( \u03bc (1-\u03bc) )\n        return 1.0 / (mu * (1.0 - mu))\n\n    # ------------------ assemble & return ----------------------------------\n    return {\n        \"identity\": {\n            \"link\": _id_link,\n            \"inv_link\": _id_inv_link,\n            \"derivative\": _id_derivative,\n        },\n        \"log\": {\n            \"link\": _log_link,\n            \"inv_link\": _log_inv_link,\n            \"derivative\": _log_derivative,\n        },\n        \"logit\": {\n            \"link\": _logit_link,\n            \"inv_link\": _logit_inv_link,\n            \"derivative\": _logit_derivative,\n        },\n    }\n```\n\nExample (rounded to 4 dp as requested):\n\n```python\nlinks = glm_links()\n\nmu  = np.array([0.2, 0.8])\neta = links[\"logit\"][\"link\"](mu)          # [-1.38629436, 1.38629436]\nprint(np.round(eta, 4))                   # [-1.3863  1.3863]\n\nmu2 = links[\"logit\"][\"inv_link\"](eta)     # [0.2, 0.8]\nprint(np.round(mu2, 4))                   # [0.2 0.8]\n\ndmu = links[\"logit\"][\"derivative\"](mu)    # [6.25, 6.25]\nprint(np.round(dmu, 4))                   # [6.25 6.25]\n```\n\nThe helper now provides fully-vectorised, numerically stable implementations of the three most common GLM link functions together with their inverses and derivatives."}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter      # kept because it was in the template\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample, the depth of the leaf it lands in inside a\n    single random isolation tree grown as described in the task statement.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Reproducibility\n    # ------------------------------------------------------------------ #\n    np.random.seed(0)\n\n    # ------------------------------------------------------------------ #\n    # Trivial / corner-case handling\n    # ------------------------------------------------------------------ #\n    if data.size == 0:                        # no samples at all\n        return []\n\n    n_samples, n_features = data.shape\n    depths = [-1] * n_samples                # will be filled in exactly once\n\n    # ------------------------------------------------------------------ #\n    # Recursive construction and bookkeeping\n    # ------------------------------------------------------------------ #\n    def grow(indices: list[int], depth: int) -> None:\n        \"\"\"Recursively grow the isolation tree while filling `depths`.\"\"\"\n        # 1. Stopping condition \u2013 fewer than three samples \u21d2 leaf\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        # 2. Pick a random feature\n        f = np.random.randint(0, n_features)\n\n        # 3. Determine split bounds and draw the split value\n        vals  = data[indices, f]\n        down, up = vals.min(), vals.max()\n\n        # If there is no variability on this feature we cannot split \u2013 make leaf\n        if down == up:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        v = np.random.uniform(down, up)\n\n        # 4. Partition the samples\n        left  = [idx for idx in indices if data[idx, f] <= v]\n        right = [idx for idx in indices if data[idx, f] >  v]\n\n        # If the random split failed to separate anything, make this a leaf\n        if len(left) == 0 or len(right) == 0:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        # 5. Recurse on both children\n        grow(left,  depth + 1)\n        grow(right, depth + 1)\n\n    # Kick off the recursion with the root node\n    grow(list(range(n_samples)), depth=0)\n\n    return depths\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A nested Python list where element [i][j] is the squared Euclidean\n        distance between samples i and j, rounded to 4 decimal places.\n    \"\"\"\n    # Handle the empty input case\n    if X.shape[0] == 0:          # no samples\n        return []\n\n    # Squared \u21132 norm of each sample:  ||x||\u00b2\n    sq_norms = np.sum(X ** 2, axis=1, keepdims=True)          # (n_samples, 1)\n\n    # Pair-wise squared distances using \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2\u00b7x\u1d40y\n    D = sq_norms + sq_norms.T - 2 * X @ X.T                   # (n_samples, n_samples)\n\n    # Numerical accuracy: any tiny negative values -> 0\n    D = np.maximum(D, 0.0)\n\n    # Round to 4 decimal places and convert to plain Python lists\n    return np.round(D, 4).tolist()\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Each element is a 2-D array-like object of shape (n_samples, n_classes)\n        containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning-rate (eta) used by the booster.\n\n    Returns\n    -------\n    list[int]\n        The predicted class label for every sample.\n    \"\"\"\n    # ---- 1. Accumulate tree outputs ----------------------------------------\n    # Convert first element to an array so we know the final shape\n    total = np.asarray(predictions[0], dtype=float)\n\n    for tree_out in predictions[1:]:\n        total += np.asarray(tree_out, dtype=float)\n\n    raw_scores = -learning_rate * total        # score = 0 - lr * \u03a3(tree_pred)\n\n    # ---- 2. Row-wise soft-max ----------------------------------------------\n    # Numerical-stable implementation: subtract row-wise maximum first\n    row_max = np.max(raw_scores, axis=1, keepdims=True)\n    exp_scores = np.exp(raw_scores - row_max)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n    # ---- 3. Arg-max per sample ---------------------------------------------\n    return np.argmax(probs, axis=1).tolist()\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Iterable, FrozenSet\n\n\ndef _sorted_tuple(it: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the items of *it* as an ascending tuple.\n\n    When item types are not mutually comparable we fall back to comparing their\n    textual representation, which is at least deterministic.\n    \"\"\"\n    try:                              # works for homogeneous, comparable types\n        return tuple(sorted(it))\n    except TypeError:                 # e.g. mix of str / int  -> not comparable\n        return tuple(sorted(it, key=repr))\n\n\ndef find_frequent_itemsets(\n    transactions: List[List[Hashable]], minsup: float\n) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Args\n    ----\n    transactions : list[list[hashable]]\n        Transactional data base \u2013 duplicates inside one transaction are ignored.\n    minsup       : float\n        Minimum support given as a fraction in the (0, 1] interval.\n\n    Returns\n    -------\n    dict[tuple, int]\n        Maps every frequent item-set (tuple with ascending items) to its support.\n        The empty dict is returned when no item-set fulfils the threshold.\n    \"\"\"\n    if not (0 < minsup <= 1):\n        raise ValueError(\"minsup must be in the interval (0, 1].\")\n\n    # --- basic preprocessing -------------------------------------------------\n    transactions = [set(t) for t in transactions]        # drop duplicates\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return {}\n\n    min_count = max(1, math.ceil(minsup * n_transactions))\n\n    # -------------------------------------------------------------------------\n    # Apriori algorithm\n    # -------------------------------------------------------------------------\n    result: Dict[Tuple[Hashable, ...], int] = {}\n\n    # 1) frequent 1-itemsets ---------------------------------------------------\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n\n    # L_k keeps the frequent k-itemsets *and* their support for the current k\n    L_k: Dict[FrozenSet[Hashable], int] = {\n        frozenset([item]): supp\n        for item, supp in item_counter.items()\n        if supp >= min_count\n    }\n\n    for fs, supp in L_k.items():                       # store L1 in the result\n        result[_sorted_tuple(fs)] = supp\n\n    k = 2\n    while L_k:\n        # 2) candidate generation (join + prune) ------------------------------\n        L_prev = list(L_k.keys())\n        candidates: set[FrozenSet[Hashable]] = set()\n\n        for i in range(len(L_prev)):\n            for j in range(i + 1, len(L_prev)):\n                union = L_prev[i] | L_prev[j]\n                if len(union) != k:\n                    continue\n                # Apriori-prune: all (k-1) subsets must themselves be frequent\n                if all((union - frozenset([item])) in L_k for item in union):\n                    candidates.add(union)\n\n        if not candidates:                             # nothing more to count\n            break\n\n        # 3) support counting -------------------------------------------------\n        cand_counter = Counter()\n        for t in transactions:\n            for cand in candidates:\n                if cand.issubset(t):\n                    cand_counter[cand] += 1\n\n        # 4) keep only the frequent ones --------------------------------------\n        L_k = {\n            cand: supp for cand, supp in cand_counter.items() if supp >= min_count\n        }\n\n        # 5) add to global result ---------------------------------------------\n        for fs, supp in L_k.items():\n            result[_sorted_tuple(fs)] = supp\n\n        k += 1\n\n    return result\n```\n\nExplanation of the main steps\n1. Transactions are converted to `set`s so every item appears at most once per basket (duplicates do not increase support).\n2. The minimum absolute support is `ceil(minsup \u00b7 N)`.\n3. Apriori is used:\n   \u2022 Count single items \u2192 keep only those reaching the threshold.  \n   \u2022 Repeatedly  \n     \u2013 join frequent (k\u22121)-itemsets to build length-k candidates,  \n     \u2013 prune candidates whose (k\u22121) subsets are not all frequent,  \n     \u2013 count their support with a single scan over the data,  \n     \u2013 retain the frequent ones and store them.  \n   The loop stops once no new frequent itemset is found.\n4. Keys are returned as tuples whose items are stored in ascending order. When direct comparison of mixed types would raise a `TypeError`, the `repr` of the items is used as a tiebreaker, guaranteeing a deterministic order.\n"}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        Python list with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Helper: numerically stable sigmoid\n    # ------------------------------------------------------------------\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        # Clip z to keep exp within float64 range\n        z_clipped = np.clip(z, -709, 709)   # np.exp(709) ~= 8.2e307 < 1.8e308\n        return 1.0 / (1.0 + np.exp(-z_clipped))\n\n    # ------------------------------------------------------------------\n    # Initialisation\n    # ------------------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=np.float64)\n    y_train = np.asarray(y_train, dtype=np.float64).flatten()   # ensure (n_samples,)\n    n_samples, n_features = X_train.shape\n\n    # Parameters (weights and bias) start at zero\n    w = np.zeros(n_features, dtype=np.float64)\n    b = 0.0\n\n    # ------------------------------------------------------------------\n    # Batch Gradient Descent\n    # ------------------------------------------------------------------\n    for _ in range(n_iterations):\n        # Forward pass\n        z = X_train @ w + b           # shape: (n_samples,)\n        p = _sigmoid(z)               # predicted probabilities\n\n        # Errors and gradients\n        error = p - y_train           # shape: (n_samples,)\n        grad_w = (X_train.T @ error) / n_samples     # shape: (n_features,)\n        grad_b = error.mean()                         \n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # ------------------------------------------------------------------\n    # Prediction on X_test\n    # ------------------------------------------------------------------\n    z_test = X_test @ w + b\n    probs  = _sigmoid(z_test)\n    preds  = np.round(probs).astype(int)     # 0/1 by standard rounding\n\n    return preds.tolist()\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (X\u0303_{t-1}).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (X_t).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \u03b5, 0 \u2264 weight \u2264 1.  Scalars or one\u2013dimensional\n        vectors are supported and broadcast against each other.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n    # Helper to decide whether an object is a scalar\n    def _is_scalar(x):\n        return np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0)\n\n    # Convert everything to numpy arrays (scalar \u2192 0-d array)\n    prev_arr   = np.asarray(prev,   dtype=float)\n    cur_arr    = np.asarray(cur,    dtype=float)\n    weight_arr = np.asarray(weight, dtype=float)\n\n    # Validate the weight range\n    if np.any(weight_arr < 0) or np.any(weight_arr > 1):\n        raise ValueError(\"weight must lie in the interval [0, 1].\")\n\n    # Exponential smoothing with implicit NumPy broadcasting\n    result = weight_arr * prev_arr + (1.0 - weight_arr) * cur_arr\n    result = np.round(result, 4)           # round to four decimals\n\n    # Decide what to return\n    if _is_scalar(prev) and _is_scalar(cur) and _is_scalar(weight):\n        # All inputs were scalar \u2192 return a single float\n        return float(result)\n    else:\n        # At least one input was vector-like \u2192 return a Python list\n        return result.tolist()\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray,\n           k: int,\n           epsilon: float = 1e-3,\n           max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, default 1e-3\n        Stop if the largest centre movement is smaller than *epsilon*.\n    max_iter : int, default 2000\n        Hard iteration cap.\n\n    Returns\n    -------\n    (centres, labels)\n        centres : list[list[float]]\n            k cluster centres rounded to 4 decimals.\n        labels : list[int]\n            Cluster index (0 \u2026 k-1) assigned to every sample.\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError('data must be a 2-D array')\n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_samples):\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 number of samples')\n\n    # Work with float copy to avoid surprising integer division\n    data = data.astype(float, copy=False)\n\n    # --- initialisation ------------------------------------------------------\n    centres = data[:k].copy()            # first k samples  (deterministic)\n\n    for _ in range(max_iter):\n        # ----- 1. assignment step ------------------------------------------\n        # squared Euclidean distances:  (n_samples \u00d7 k)\n        dists = np.sum((data[:, None, :] - centres[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)     # (n_samples,)\n\n        # ----- 2. update step ----------------------------------------------\n        new_centres = centres.copy()\n        for j in range(k):\n            mask = labels == j\n            if mask.any():                       # normal: use mean\n                new_centres[j] = data[mask].mean(axis=0)\n            else:                                # empty cluster -> random sample\n                rnd_idx = np.random.randint(0, n_samples)\n                new_centres[j] = data[rnd_idx]\n\n        # ----- 3. convergence test -----------------------------------------\n        shifts = np.linalg.norm(new_centres - centres, axis=1)  # movement per centre\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n        centres = new_centres\n\n    # --- rounding & output ---------------------------------------------------\n    centres_rounded = np.round(centres, 4).tolist()\n    return centres_rounded, labels.tolist()\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef power_spectrum(frames: np.ndarray, scale: bool = False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True, each spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        Power spectrum (positive frequencies only) for every frame,\n        rounded to four decimal places and converted to a regular Python list.\n    \"\"\"\n    # Number of samples per frame\n    N = frames.shape[1]\n\n    # Real-valued FFT along the frame axis (axis=1)\n    fft_vals = np.fft.rfft(frames, axis=1)\n\n    # Squared magnitude -> power spectrum\n    power = np.abs(fft_vals) ** 2\n\n    # Optional scaling by the number of returned bins\n    if scale:\n        power /= (N // 2 + 1)\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(power, 4).tolist()\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict          # (import kept \u2013 not strictly needed)\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item\u2013based k-NN collaborative\u2013filtering recommender.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Rating matrix of shape (n_user, n_item); 0 = *not rated*.\n    user_ind : int\n        Index of the active user.\n    k : int\n        Maximal number of items to recommend.\n    criterion : {'cosine', 'pearson'}, default 'cosine'\n        Similarity measure to use.\n\n    Returns\n    -------\n    list[int]\n        Indices of at most *k* unseen items ordered by decreasing\n        predicted attractiveness.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 0. Some basic shapes / checks\n    # ------------------------------------------------------------------\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueError(\"criterion must be either 'cosine' or 'pearson'\")\n\n    n_user, n_item = data.shape\n    if not (0 <= user_ind < n_user):\n        raise IndexError('user_ind out of bounds')\n\n    # ------------------------------------------------------------------\n    # 1. Build the item\u2013item similarity matrix S\n    # ------------------------------------------------------------------\n    S = np.zeros((n_item, n_item), dtype=np.float64)          # S[i, i] stays 0/unused\n\n    for i in range(n_item - 1):\n        col_i = data[:, i]\n        r_i_mask = col_i > 0                                   # users that rated i\n\n        for j in range(i + 1, n_item):\n            col_j = data[:, j]\n            common = r_i_mask & (col_j > 0)                    # users that rated *both*\n\n            if not np.any(common):                             # empty intersection\n                continue                                       # similarity already 0\n\n            v1 = col_i[common].astype(np.float64)\n            v2 = col_j[common].astype(np.float64)\n\n            # ---- cosine ------------------------------------------------\n            if criterion == 'cosine':\n                # optional mean\u2013centering (only if variance large enough)\n                if np.std(v1, ddof=1) > 1e-3:\n                    v1c = v1 - v1.mean()\n                else:\n                    v1c = v1\n\n                if np.std(v2, ddof=1) > 1e-3:\n                    v2c = v2 - v2.mean()\n                else:\n                    v2c = v2\n\n                n1, n2 = np.linalg.norm(v1c), np.linalg.norm(v2c)\n                sim = 0.0 if n1 == 0.0 or n2 == 0.0 else float(v1c @ v2c / (n1 * n2))\n\n            # ---- Pearson -----------------------------------------------\n            else:                                               # criterion == 'pearson'\n                # correlation needs at least 2 points with variance\n                if len(v1) < 2 or np.std(v1, ddof=1) < 1e-3 or np.std(v2, ddof=1) < 1e-3:\n                    sim = 0.0\n                else:\n                    sim = float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n                    if np.isnan(sim):\n                        sim = 0.0\n\n            S[i, j] = S[j, i] = sim\n\n    # ------------------------------------------------------------------\n    # 2. Items already rated by the active user\n    # ------------------------------------------------------------------\n    user_row = data[user_ind]\n    rated_mask = user_row > 0\n    rated_idx = np.where(rated_mask)[0]\n\n    # If everything is already rated \u2192 nothing to recommend\n    if len(rated_idx) == n_item:\n        return []\n\n    # ------------------------------------------------------------------\n    # 3. Predict scores for every *un*-rated item\n    # ------------------------------------------------------------------\n    scores = {}                                                  # item_index -> score\n\n    for t in range(n_item):\n        if rated_mask[t]:                                        # already seen\n            continue\n\n        sims_to_rated = S[t, rated_idx]                          # similarities S[t, i]\n        ratings = user_row[rated_idx]                            # the r_i\n\n        denom = np.sum(np.abs(sims_to_rated))\n        if denom == 0.0:\n            score = 0.0\n        else:\n            score = float(ratings @ sims_to_rated / denom)       # weighted average\n\n        scores[t] = score\n\n    # ------------------------------------------------------------------\n    # 4. Sort and pick the best *k*\n    # ------------------------------------------------------------------\n    sorted_items = sorted(scores.items(),\n                          key=lambda x: (-x[1], x[0]))           # desc score, asc index\n\n    return [idx for idx, _ in sorted_items[:k]]\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss, its gradient and Hessian diagonal.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) with shape (n,).\n        y_pred (np.ndarray): Logits (pre-sigmoid scores) with shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian_diagonal) with every value\n               rounded to 4 decimal places.\n    \"\"\"\n    # Make sure the inputs are 1-D numpy arrays\n    y      = np.asarray(y).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n\n    # Sigmoid probabilities\n    p = 1.0 / (1.0 + np.exp(-y_pred))\n\n    # Clip for numerical stability when taking the log\n    eps = 1e-15\n    p_clipped = np.clip(p, eps, 1.0 - eps)\n\n    # Individual sample losses\n    losses = -(y * np.log(p_clipped) + (1.0 - y) * np.log(1.0 - p_clipped))\n\n    # Mean loss\n    mean_loss = round(float(np.mean(losses)), 4)\n\n    # Gradient (first derivative wrt logits)\n    grad = p - y\n    grad_rounded = [round(g, 4) for g in grad]\n\n    # Hessian diagonal (second derivative wrt logits)\n    hess = p * (1.0 - p)\n    hess_rounded = [round(h, 4) for h in hess]\n\n    return mean_loss, grad_rounded, hess_rounded\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    if N < 1:\n        raise ValueError(\"N must be an integer \u2265 1\")\n\n    seq_len = len(sequence)\n    if N > seq_len:\n        return []\n\n    # Build the list of N-grams using slicing.\n    # sequence[i : i + N] is a window of length N starting at position i.\n    return [tuple(sequence[i : i + N]) for i in range(seq_len - N + 1)]\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    # Handle the degenerate case of no nodes\n    n = len(items)\n    if n == 0:\n        return {}\n\n    # Pre-create a dictionary for every node\n    nodes = [\n        {'item': items[i], 'count': counts[i], 'children': []}\n        for i in range(n)\n    ]\n\n    # Identify the root and attach children to their parents\n    root_idx = None\n    for i, parent_idx in enumerate(parents):\n        if parent_idx == -1:\n            root_idx = i\n        else:\n            # Append the current node to its parent's children list.\n            # Iterating in input order preserves the required order of children.\n            nodes[parent_idx]['children'].append(nodes[i])\n\n    # root_idx is guaranteed to be set because exactly one parent is -1\n    return nodes[root_idx]\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs\n        Supported keyword argument\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is Python float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    # Extract supported keyword(s)\n    dtype = kwargs.pop('dtype', float)\n\n    # Guard against unsupported keywords\n    if kwargs:\n        unexpected = ', '.join(kwargs.keys())\n        raise TypeError(f\"zero() got unexpected keyword argument(s): {unexpected}\")\n\n    # Delegate the heavy lifting to NumPy\n    return np.zeros(shape, dtype=dtype)\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split maximises the reduction in the regularised loss used by\n    gradient-boosting decision trees.  If no legal split gives a positive\n    gain the function returns ``None``.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # A legal split must leave at least two samples on each side\n    if n_samples < 4:\n        return None\n\n    # Helper: regularised loss of a leaf that contains gradients G and H\n    def leaf_loss(G: float, H: float) -> float:\n        return -0.5 * (G * G) / (H + lam) + gamma\n\n    # Statistics of the current (unsplit) node\n    G_total = g.sum()\n    H_total = h.sum()\n    parent_loss = leaf_loss(G_total, H_total)\n\n    best_gain = 0.0          # strictly positive gain is required\n    best_feature = None\n    best_threshold = None\n\n    # Evaluate every feature separately\n    for feat_idx in range(n_features):\n        # Sort the samples by the current feature so we can sweep once\n        order = np.argsort(X[:, feat_idx], kind='mergesort')\n        x_sorted = X[order, feat_idx]\n        g_sorted = g[order]\n        h_sorted = h[order]\n\n        # Cumulative sums give statistics for the *left* side after each sample\n        g_cumsum = np.cumsum(g_sorted)\n        h_cumsum = np.cumsum(h_sorted)\n\n        # Iterate over potential split positions.\n        # A split is *after* position i (i.e. left uses 0..i, right i+1..end)\n        # We need at least two samples per side \u21d2 i \u2208 [1, n-3]\n        for i in range(1, n_samples - 2):\n            # Skip positions where the feature value doesn\u2019t change \u2013 no real split\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n\n            G_left  = g_cumsum[i]\n            H_left  = h_cumsum[i]\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n\n            # Ensure two samples on the right as well (implicit for left)\n            # (Because duplicates were skipped, right size \u2265 n-i-1)\n            if n_samples - i - 1 < 2:\n                break  # further positions will only shrink the right side\n\n            loss_left  = leaf_loss(G_left,  H_left)\n            loss_right = leaf_loss(G_right, H_right)\n\n            gain = parent_loss - loss_left - loss_right\n\n            if gain > best_gain:           # strictly larger (positive as well)\n                best_gain = gain\n                best_feature = feat_idx\n                best_threshold = x_sorted[i]   # threshold is the current value\n\n    if best_feature is None:     # no positive gain found\n        return None\n    return best_feature, float(best_threshold)\n```\n\nExplanation of the core steps\n1. The total gradient (`G_total`) and Hessian (`H_total`) of the current node are\n   computed once, giving the node\u2019s present loss (`parent_loss`).\n\n2. For every feature  \n   \u2022 Instances are sorted by that feature, giving a single left-to-right sweep  \n   \u2022 Cumulative sums of gradients/Hessians give statistics of the *left* child\n     for any potential split position  \n   \u2022 Only positions where the feature value changes and both sides retain at\n     least two samples are considered  \n   \u2022 For each legal split the gain is `parent_loss \u2212 loss_left \u2212 loss_right`\n\n3. The split with the largest strictly positive gain is returned; if no such\n   split exists the function yields `None`.\n"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    #-------------------------------------------------\n    # 1. Detect whether the input is a scalar\n    #-------------------------------------------------\n    is_scalar = np.isscalar(x)\n\n    #-------------------------------------------------\n    # 2. Convert the input to a NumPy array (for uniform math operations)\n    #-------------------------------------------------\n    x_arr = np.asarray(x, dtype=float)     # ensures float precision\n\n    #-------------------------------------------------\n    # 3. Compute sigmoid and its gradient\n    #-------------------------------------------------\n    sigmoid_arr  = 1.0 / (1.0 + np.exp(-x_arr))\n    gradient_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n\n    #-------------------------------------------------\n    # 4. Round results to 4 decimal places\n    #-------------------------------------------------\n    sigmoid_arr  = np.round(sigmoid_arr, 4)\n    gradient_arr = np.round(gradient_arr, 4)\n\n    #-------------------------------------------------\n    # 5. Return results in the required format\n    #-------------------------------------------------\n    if is_scalar:                                # scalar \u2192 float\n        return float(sigmoid_arr), float(gradient_arr)\n    else:                                        # array-like \u2192 Python lists\n        return sigmoid_arr.tolist(), gradient_arr.tolist()\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom collections import defaultdict          # (needed by the template \u2013 not used)\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test:  list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble of decision stumps and returns test predictions.\"\"\"\n    \n    # --- helper that applies one stump ----------------------------------------\n    def stump_predict(x: np.ndarray, d: int, theta: float) -> np.ndarray:\n        \"\"\"Return \u00b11 predictions of the stump (d , \u03b8) on vector x.\"\"\"\n        if d == 0:                 #  +1  for  x \u2264 \u03b8\n            return np.where(x <= theta,  1, -1)\n        else:                      #  +1  for  x > \u03b8\n            return np.where(x >  theta,  1, -1)\n    # --------------------------------------------------------------------------\n\n    # ---- prepare data --------------------------------------------------------\n    x_train = np.asarray(x_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    x_test  = np.asarray(x_test , dtype=float)\n\n    n = len(x_train)\n    if n == 0:                            # trivial corner-case\n        return [1] * len(x_test)\n\n    # ---- enumerate all candidate thresholds ---------------------------------\n    order      = np.argsort(x_train)\n    x_sorted   = x_train[order]\n\n    thresholds = [x_sorted[0] - 1.0]      # a value below the smallest sample\n    for i in range(n - 1):                # mid-points between successive, *distinct* values\n        if x_sorted[i] != x_sorted[i + 1]:\n            thresholds.append(0.5 * (x_sorted[i] + x_sorted[i + 1]))\n    thresholds.append(x_sorted[-1] + 1.0) # a value above the largest sample\n\n    stumps = [(d, th) for th in thresholds for d in (0, 1)]\n    # -------------------------------------------------------------------------\n\n    # ---- AdaBoost ------------------------------------------------------------\n    w = np.full(n, 1.0 / n)               # uniform initial weights\n    alphas, chosen_stumps = [], []\n\n    while True:\n        # pick best stump wrt current weights\n        best_err   = float('inf')\n        best_pred  = None\n        best_stump = None\n\n        for d, th in stumps:\n            pred = stump_predict(x_train, d, th)\n            err  = w[pred != y_train].sum()           # weighted error\n            if err < best_err - 1e-12:                # strict improvement\n                best_err, best_stump, best_pred = err, (d, th), pred\n\n        # safety \u2013 if no weak learner beats chance we stop\n        if best_err >= 0.5 - 1e-12:\n            break\n\n        # stump weight\n        err = max(best_err, 1e-12)                    # avoid log(0)\n        alpha = 0.5 * math.log((1.0 - err) / err)\n\n        chosen_stumps.append(best_stump)\n        alphas.append(alpha)\n\n        # update sample weights\n        w *= np.exp(-alpha * y_train * best_pred)\n        w  = w / w.sum()\n\n        # check current training error of the ensemble\n        F = np.zeros(n)\n        for a, (d, th) in zip(alphas, chosen_stumps):\n            F += a * stump_predict(x_train, d, th)\n        train_pred   = np.where(F >= 0.0, 1, -1)\n        train_error  = (train_pred != y_train).mean()\n\n        if train_error <= epsilon:\n            break\n    # -------------------------------------------------------------------------\n\n    # ---- predict on test set -------------------------------------------------\n    if not alphas:                                # no stump selected\n        return [1] * len(x_test)\n\n    F_test = np.zeros(len(x_test))\n    for a, (d, th) in zip(alphas, chosen_stumps):\n        F_test += a * stump_predict(x_test, d, th)\n\n    y_pred = np.where(F_test >= 0.0, 1, -1)\n    return y_pred.tolist()\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as\n        offset + u\n    where\n        offset ~ DiscreteUniform{-300, \u2026, 299}\n        u      ~ ContinuousUniform[0, 1)\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, every column (axis-0 slice) is shifted to mean 0\n            and scaled to unit variance.  A tiny constant \u03b5 is added to the\n            denominator to avoid division by zero.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    rng = np.random.default_rng()\n\n    # Integer offsets in [\u2212300, 299]  (high is exclusive in np.random.integers)\n    offset = rng.integers(low=-300, high=300, size=shape)\n\n    # Uniform noise in [0, 1)\n    u = rng.random(size=shape)\n\n    # The raw tensor\n    data = offset + u          # dtype becomes float automatically\n\n    if not standardize:\n        return data\n\n    # ------------------------------------------------------------------\n    # Standardisation (column-wise, i.e. along axis 0)\n    # ------------------------------------------------------------------\n    eps = np.finfo(float).eps\n\n    # Scalars / 1-D vectors are handled separately for clarity\n    if data.ndim == 0:                       # scalar\n        return np.array(0.0, dtype=float)\n\n    if data.ndim == 1:                       # vector \u2192 single \"column\"\n        mean = data.mean()\n        std = data.std()\n        return (data - mean) / (std + eps)\n\n    # 2-D or higher: treat axis 0 as the \u201csample\u201d axis and\n    # standardise each column independently.\n    mean = data.mean(axis=0, keepdims=True)\n    std = data.std(axis=0, keepdims=True)\n    return (data - mean) / (std + eps)\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\"\"\"\n    \n    # --- Convert inputs to NumPy arrays ------------------------------------\n    w = np.asarray(w, dtype=float)\n    \n    # If `velocity` is empty, initialise it to zeros with the same shape as `w`\n    if len(velocity) == 0:\n        v = np.zeros_like(w, dtype=float)\n    else:\n        v = np.asarray(velocity, dtype=float)\n\n    # --- Step 1: look-ahead parameters -------------------------------------\n    w_tilde = w - momentum * v\n\n    # --- Step 2: gradient (with clipping to [-1, 1]) -----------------------\n    g = np.asarray(grad_func(w_tilde), dtype=float)\n    g = np.clip(g, -1.0, 1.0)\n\n    # --- Step 3: velocity update -------------------------------------------\n    v_new = momentum * v + learning_rate * g\n\n    # --- Step 4: parameter update (gradient-descent direction) -------------\n    w_new = w - v_new\n\n    # --- Round to 4 decimals and convert to plain Python lists -------------\n    w_new = np.round(w_new, 4).tolist()\n    v_new = np.round(v_new, 4).tolist()\n\n    return w_new, v_new\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math   # kept because the header requires it (not actually needed below)\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    \n    # Ensure 1-D views of vectors (no copy unless necessary)\n    y_train = y_train.ravel()\n    alpha   = alpha.ravel()\n    \n    # Pre-compute the joint coefficient \u03b1_i y_i\n    coef = alpha * y_train          # shape (n,)\n    \n    # Squared \u21132 norms of train and test points\n    train_sq = np.sum(X_train**2, axis=1)          # (n,)\n    test_sq  = np.sum(X_test**2,  axis=1)          # (m,)\n    \n    # Pairwise squared Euclidean distances using the identity\n    # ||x \u2212 z||\u00b2 = ||x||\u00b2 + ||z||\u00b2 \u2212 2 x\u00b7z\n    # produces an (m \u00d7 n) distance matrix without explicit Python loops\n    dists_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T\n    \n    # Gaussian (RBF) kernel matrix\n    K = np.exp(-gamma * dists_sq)                  # (m, n)\n    \n    # Decision values for all test samples\n    g = K @ coef + b                               # (m,)\n    \n    # Predicted labels: use sign; map 0 \u2192 1 (arbitrary, but required \u00b11)\n    pred = np.sign(g)\n    pred[pred == 0] = 1\n    \n    return pred.astype(int).tolist()\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(x: list | int | float) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient mask.\n\n    Parameters\n    ----------\n    x : list or nested list (or scalar)\n        Input data of arbitrary (rectangular) shape.\n\n    Returns\n    -------\n    tuple[list, list]\n        `activation, gradient`, both as (nested) Python lists that keep the\n        exact same shape as the input.\n    \"\"\"\n    # Convert the input to a NumPy array for vectorised operations\n    arr = np.asarray(x, dtype=float)\n\n    # ReLU activation: max(0, x)\n    activation = np.maximum(0, arr)\n\n    # Gradient mask: 1 where x >= 0, else 0\n    gradient = np.where(arr >= 0, 1, 0)\n\n    # Convert results back to (nested) Python lists\n    return activation.tolist(), gradient.tolist()\n```\n\nExample usage\n```python\na, g = relu([[-2, 0, 3.5],\n             [7, -1, 0]])\n\nprint(a)  # [[0.0, 0.0, 3.5], [7.0, 0.0, 0.0]]\nprint(g)  # [[0,   1,   1],   [1,   0,   1]]\n```\n\nThe function works for any dimensionality provided the input can be represented as a regular NumPy array; the shapes of the returned activation and gradient lists exactly mirror that of the original input."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        (X_hat, mse) where\n            \u2022 X_hat \u2013 rank-k reconstruction of X (each entry rounded to 4 decimals)\n            \u2022 mse    \u2013 mean-squared reconstruction error (rounded to 4 decimals)\n        If k is not in [1, min(m, n)] the function returns -1.\n    \"\"\"\n    # ------------ basic validation ------------\n    if not isinstance(k, int):\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n\n    m, n = X_arr.shape\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # ------------ truncated SVD ---------------\n    # X = U \u03a3 V^T  (full_matrices=False \u21d2 U: m\u00d7r, \u03a3: r, V^T: r\u00d7n, r=min(m,n))\n    U, S, VT = np.linalg.svd(X_arr, full_matrices=False)\n\n    # keep the first k singular values/vectors\n    U_k  = U[:, :k]              # m \u00d7 k\n    S_k  = S[:k]                 # k\n    VT_k = VT[:k, :]             # k \u00d7 n\n\n    # optimal rank-k reconstruction:  U_k \u03a3_k V_k^T\n    X_hat = (U_k * S_k) @ VT_k   # broadcasting: multiply cols of U_k by S_k\n\n    # ------------ error & rounding ------------\n    mse = float(np.round(np.mean((X_arr - X_hat) ** 2), 4))\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n\n    return X_hat_rounded, mse\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np            # <- kept deliberately: required by the exercise\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"1-D gradient boosting with decision stumps and prediction for *x_query*.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Convert to numpy arrays and guard against trivial situations\n    # ------------------------------------------------------------------ #\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n\n    if x.size == 0:                         # no data -> return NaN\n        return float('nan')\n    \n    # If all x are identical (or only one sample), the best we can do is the mean\n    if np.all(x == x[0]) or x.size == 1:\n        return round(float(y.mean()), 4)\n    \n    # ------------------------------------------------------------------ #\n    # 1. Sort the data once (makes the rest easier)\n    # ------------------------------------------------------------------ #\n    order          = np.argsort(x)\n    x_sorted       = x[order]\n    y_sorted       = y[order]\n\n    # ------------------------------------------------------------------ #\n    # 2. Candidate split points = mid points between different neighbours\n    # ------------------------------------------------------------------ #\n    distinct_mask  = x_sorted[1:] != x_sorted[:-1]\n    mids           = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n    candidate_s    = mids[distinct_mask]                 # shape (K,)\n\n    if candidate_s.size == 0:                # All identical x\u2019s\n        return round(float(y.mean()), 4)\n\n    # ------------------------------------------------------------------ #\n    # 3. Gradient-boosting loop\n    # ------------------------------------------------------------------ #\n    residuals      = y_sorted.copy()\n    stumps         = []                      # list of (split, c_left, c_right)\n\n    rss = np.sum(residuals ** 2)\n\n    max_iter = 1000                          # simple safeguard\n    iteration = 0\n    while rss > epsilon and iteration < max_iter:\n        best_rss      = np.inf\n        best_params   = None                 # (split, c1, c2, mask_left)\n\n        for s in candidate_s:\n            mask_left     = x_sorted <= s\n            mask_right    = ~mask_left\n\n            # Both sides non-empty by construction, but check anyway\n            if not mask_left.any() or not mask_right.any():\n                continue\n\n            c1 = residuals[mask_left].mean()\n            c2 = residuals[mask_right].mean()\n\n            # residuals after adding the stump\n            new_residuals = residuals - np.where(mask_left, c1, c2)\n            rss_candidate = np.sum(new_residuals ** 2)\n\n            if rss_candidate < best_rss:\n                best_rss    = rss_candidate\n                best_params = (s, c1, c2, mask_left)\n\n        # If for some strange reason we cannot improve, stop\n        if best_params is None or best_rss >= rss - 1e-12:\n            break\n\n        # Commit the best stump found\n        s_best, c1_best, c2_best, mask_left_best = best_params\n        stumps.append((s_best, c1_best, c2_best))\n\n        residuals = residuals - np.where(mask_left_best, c1_best, c2_best)\n        rss       = best_rss\n        iteration += 1\n\n    # ------------------------------------------------------------------ #\n    # 4. Prediction for the query point\n    # ------------------------------------------------------------------ #\n    prediction = 0.0\n    for s, c1, c2 in stumps:\n        prediction += c1 if x_query <= s else c2\n\n    return round(float(prediction), 4)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\ndef check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    Accepts ints, floats, or numeric strings.  Any string is converted\n    with `float()`; already-numeric values are left unchanged.\n    Returns a tuple (a, b) in the original order.\n    \"\"\"\n    def _convert(x):\n        # If it's a string, convert with float(); otherwise, if it's already\n        # an int or float leave it as is.  For every other type, defer to\n        # float() \u2013 this will raise the expected errors for bad inputs.\n        if isinstance(x, str):\n            return float(x)\n        if isinstance(x, (int, float)):\n            return x\n        # Let float() raise ValueError / TypeError as appropriate.\n        return float(x)\n\n    return _convert(a), _convert(b)\n\n\n# Do NOT change this decorator \u2013 it is required by the grader.\ndef validate_input(function):\n    def wrapper(a, b):\n        a, b = check_data(a, b)\n        return function(a, b)\n\n    return wrapper\n\n\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\n\n    If the result is mathematically an integer, return it as an `int`;\n    otherwise return a `float`.\n    \"\"\"\n    result = a + b\n    # Convert purely integral floating results to int.\n    if isinstance(result, float) and result.is_integer():\n        return int(result)\n    return result\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Create a tensor whose entries are 0.0 or 1.0.\n    \n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the returned NumPy array.\n    sparsity : float, default=0.5\n        Probability that any individual entry is 1.0.  Must satisfy\n        0.0 \u2264 sparsity \u2264 1.0.  Values outside this range cause the\n        function to return -1.\n    seed : int | None, default=None\n        If provided, ``np.random.seed(seed)`` is called first so the\n        output is reproducible.\n    \n    Returns\n    -------\n    numpy.ndarray | int\n        \u2022 A floating-dtype NumPy array containing only 0.0 and 1.0, if\n          `sparsity` is legal.  \n        \u2022 The integer -1 when `sparsity` is outside the closed interval\n          [0, 1].\n    \"\"\"\n    \n    # 1. Validate sparsity ----------------------------------------------------\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n    \n    # 2. Handle optional seeding ---------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # 3. Draw uniform random numbers and threshold ---------------------------\n    #    `True` where random value < sparsity  ->  1.0\n    #    `False` otherwise                     ->  0.0\n    random_values = np.random.rand(*shape)\n    binary_tensor = (random_values < sparsity).astype(float)\n    \n    return binary_tensor\n```\n\nUsage example\n--------------\n```python\n>>> arr = random_binary_tensor((2, 3), sparsity=0.3, seed=42)\n>>> arr\narray([[0., 1., 1.],\n       [0., 0., 0.]])\n>>> arr.dtype\ndtype('float64')\n```\n\nThe function obeys every requirement:  \n\u2022 Validates sparsity, returning -1 when invalid.  \n\u2022 Seeds NumPy\u2019s RNG when requested.  \n\u2022 Generates each entry as 1.0 with probability equal to `sparsity`.  \n\u2022 Ensures only floating 0.0/1.0 values in the returned `numpy.ndarray`."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nfrom typing import Any\nfrom typing import Any\n\n\nclass node:  # noqa: N801 \u2013 keep the original name used by the problem description\n    \"\"\"A minimal tree node for decision-tree-like structures.\"\"\"\n\n    def __init__(\n        self, fea: int = -1, res: Any | None = None, child: dict | None = None\n    ) -> None:\n        self.fea = fea          # index of the splitting feature (-1 \u279c leaf)\n        self.res = res          # label stored in a leaf\n        self.child = child or {}  # maps feature value \u279c sub-tree\n\n\ndef classify(root: node, sample: list[Any]) -> Any:  # noqa: D401\n    \"\"\"Classify *sample* by walking the decision tree rooted at *root*.\n\n    If at any point the traversal cannot continue (missing feature or\n    missing branch), the function returns -1.\n    \"\"\"\n    # An empty tree cannot classify anything\n    if root is None:\n        return -1\n\n    current = root\n    while current.fea != -1:          # while not a leaf\n        idx = current.fea\n\n        # 1. Feature index must exist in the sample\n        if idx < 0 or idx >= len(sample):\n            return -1\n\n        value = sample[idx]\n\n        # 2. Required branch must exist\n        if value not in current.child:\n            return -1\n\n        # 3. Follow the branch\n        current = current.child[value]\n\n    # Leaf reached \u2013 return its stored label\n    return current.res\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nfrom typing import List, Optional\n\n\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node: rule  = (feature_index, threshold)\n    Leaf node:     rule  = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional[\"Node\"], right: Optional[\"Node\"], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\n\n\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"\n    Run a binary decision tree on a batch of samples.\n\n    Parameters\n    ----------\n    root : Optional[Node]\n        Root node of the decision tree (may be None for an empty tree).\n    X : List[List[float]]\n        Batch of input samples; one sample = list/sequence of feature values.\n\n    Returns\n    -------\n    List[float]\n        Predictions for each sample, in the same order as `X`.\n    \"\"\"\n    # Empty tree \u21d2 no predictions\n    if root is None:\n        return []\n\n    preds: List[float] = []\n\n    for sample in X:\n        node = root\n        # Traverse until a leaf is reached\n        while True:\n            is_leaf = node.left is None and node.right is None\n            if is_leaf:\n                # At a leaf, the \"threshold\" field stores the prediction\n                preds.append(node.threshold)\n                break\n\n            # Internal node: decide which child to visit next\n            feat_idx = node.feature\n            if sample[feat_idx] <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n\n    return preds\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import cycle\n\ndef build_kd_tree(data: np.ndarray) -> dict | None:\n    \"\"\"Build a kd-tree from *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features+1).  The first\n        n_features columns contain the point coordinates, the last\n        column an arbitrary label/value.\n\n    Returns\n    -------\n    dict | None\n        Nested dictionary that represents the kd-tree or *None* when\n        *data* is empty.\n    \"\"\"\n    # Nothing to build --------------------------------------------------------\n    if data.size == 0:\n        return None\n\n    m: int = data.shape[1] - 1              # number of feature columns\n    dim_generator = cycle(range(m))         # global cyclic dimension supplier\n\n    # -------------------------------------------------------------------------\n    def _build(subset: np.ndarray) -> dict | None:\n        \"\"\"Recursive helper that uses the *global* ``dim_generator``.\"\"\"\n        if subset.size == 0:                # Empty subset -> terminate branch\n            return None\n\n        d = next(dim_generator)             # next splitting dimension\n\n        # Sort by column *d* and pick the median ------------------------------\n        order = subset[:, d].argsort(kind='mergesort')  # stable sort\n        sorted_subset = subset[order]\n\n        n = sorted_subset.shape[0]\n        median_idx = n // 2\n        median_row = sorted_subset[median_idx]\n\n        # Split into left / right subsets -------------------------------------\n        left_subset  = sorted_subset[:median_idx]\n        right_subset = sorted_subset[median_idx + 1 :]\n\n        # Construct current node ----------------------------------------------\n        node = {\n            \"point\": median_row[:m].tolist(),          # coordinates\n            \"label\": median_row[m],                   # associated value/label\n            \"left\":  _build(left_subset),             # recurse\n            \"right\": _build(right_subset)\n        }\n        return node\n\n    # -------------------------------------------------------------------------\n    return _build(data)\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    scores = np.empty(n_arms, dtype=float)\n\n    for a in range(n_arms):\n        A_a = np.asarray(A[a], dtype=float)\n        b_a = np.asarray(b[a], dtype=float)\n        c_a = context[:, a]\n\n        # \u03b8\u0302_a = A_a^{-1} b_a  via solve (more stable than explicit inverse)\n        theta_hat = np.linalg.solve(A_a, b_a)\n\n        # A_a^{-1} c_a  (reuse solve to avoid explicit inverse)\n        A_inv_c = np.linalg.solve(A_a, c_a)\n\n        # p_a = \u03b8\u0302_a\u1d40 c_a + \u03b1 sqrt(c_a\u1d40 A_a^{-1} c_a)\n        exploit = theta_hat @ c_a\n        explore = alpha * np.sqrt(c_a @ A_inv_c)\n        scores[a] = exploit + explore\n\n    return int(np.argmax(scores))\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median\n          (upper median for even *n*);\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger  values.\n        The two lists are sorted increasingly.\n    \"\"\"\n    # column to be examined\n    col = data[:, d]\n    n   = col.shape[0]\n\n    # position of the median (upper median when n is even)\n    k = n // 2\n\n    # argpartition gives the indices that would place the k-th element\n    # in its sorted position without fully sorting the array (O(n))\n    part_idx    = np.argpartition(col, k)\n    pivot       = int(part_idx[k])        # row index of the median element\n    pivot_value = col[pivot]\n\n    # Build the lists of indices in one linear scan (keeps O(n) overall)\n    left  = []\n    right = []\n    for idx in range(n):          # idx already increases, so the result is sorted\n        if idx == pivot:\n            continue              # skip the pivot itself\n        v = col[idx]\n        if v < pivot_value:\n            left.append(idx)\n        elif v > pivot_value:\n            right.append(idx)\n        # elements equal to the median (if any) are ignored\n\n    return pivot, left, right\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into equally sized, possibly overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length N.\n    frame_width : int\n        Number of samples per frame.\n    stride : int\n        Hop size (number of samples between the starts of successive frames).\n    writeable : bool, default=False\n        If False the returned view is marked read-only.\n        If True the view is writeable only when the source array is writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View on *x* with shape (n_frames, frame_width), where\n            n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    # ---------- pre-condition checks ----------\n    assert x.ndim == 1, \"x must be one-dimensional\"\n    assert frame_width > 0, \"frame_width must be a positive integer\"\n    assert stride > 0, \"stride must be a positive integer\"\n    assert len(x) >= frame_width, \"frame_width larger than signal length\"\n\n    # ---------- compute framing parameters ----------\n    n_frames = (len(x) - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    item_stride = x.strides[0]                   # bytes between two consecutive samples\n    strides = (item_stride * stride, item_stride)\n\n    # ---------- build the view ----------\n    # Make it read-only unless the caller explicitly requested writeability\n    # and the original array is itself writeable.\n    view_writeable = x.flags.writeable if writeable else False\n    frames = as_strided(x, shape=shape, strides=strides, writeable=view_writeable)\n\n    return frames\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(\n    X_data: list[list[float]], \n    y_data: list, \n    X: list[list[float]], \n    k: int = 5\n) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    if not X_data:\n        raise ValueError(\"Training set is empty.\")\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # Convert once to NumPy arrays for efficient vectorised distance computations\n    X_train = np.asarray(X_data, dtype=float)\n    X_query = np.asarray(X,      dtype=float)\n\n    n_train = X_train.shape[0]\n    k_eff = min(k, n_train)          # use all samples if k > n_train\n\n    predictions: list = []\n\n    for x in X_query:\n        # Squared Euclidean distances (sqrt not needed for ranking)\n        diff   = X_train - x\n        dists2 = np.einsum('ij,ij->i', diff, diff)   # faster than (diff**2).sum(axis=1)\n\n        # Indices of the k smallest distances\n        knn_idx = np.argpartition(dists2, k_eff-1)[:k_eff]\n\n        # Majority vote\n        neighbour_labels = [y_data[i] for i in knn_idx]\n        counts = Counter(neighbour_labels)\n        max_count = max(counts.values())\n\n        # Resolve ties by lexicographical order\n        best_labels = [lbl for lbl, cnt in counts.items() if cnt == max_count]\n        predictions.append(min(best_labels))\n\n    return predictions\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    # -------- Validation --------\n    # 1) Both inputs must be list-like and share the same, non-zero length\n    if not isinstance(actual, (list, tuple, np.ndarray)) \\\n       or not isinstance(predicted, (list, tuple, np.ndarray)):\n        return -1\n\n    if len(actual) == 0 or len(actual) != len(predicted):\n        return -1\n\n    # 2) All values must be >= 0\n    #    Using numpy for fast vectorised check & later computation\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n\n    if np.any(a < 0) or np.any(p < 0):\n        return -1\n\n    # -------- RMSLE computation --------\n    log_diff_sq = (np.log1p(p) - np.log1p(a)) ** 2\n    rmsle_val = np.sqrt(np.mean(log_diff_sq))\n\n    # -------- Return rounded result --------\n    return round(float(rmsle_val), 4)\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_activation(x, derivative: bool = False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh; otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    # Convert input to 1-D NumPy array of floats\n    z = np.asarray(x, dtype=float).ravel()\n\n    # Compute tanh using the prescribed formula (no np.tanh!)\n    tanh_z = 2.0 / (1.0 + np.exp(-2.0 * z)) - 1.0\n\n    # Select output: tanh itself or its derivative\n    out = 1.0 - tanh_z ** 2 if derivative else tanh_z\n\n    # Round to 4 decimals and convert back to Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence (Python list or NumPy array) of length N.\n\n    Returns:\n        A Python list [a0, a1, \u2026, a_{N-1}] where\n            a_k = \u03a3_{n=0}^{N-k-1} x_{n+k} * x_n\n        If N == 0 the empty list is returned.\n    \"\"\"\n    # Accept both lists and NumPy arrays without altering the original object\n    N = len(x)\n    if N == 0:\n        return []\n\n    # For fast random access turn a list into a lightweight read-only view.\n    # (Avoids copying when x is already an ndarray.)\n    x_arr = np.asarray(x)\n\n    result: list = []\n    for k in range(N):                 # lag\n        s = 0\n        # sum_{n=0}^{N-k-1} x[n+k] * x[n]\n        for n in range(N - k):\n            s += x_arr[n + k] * x_arr[n]\n        result.append(s)\n\n    return result\n```\n\nExplanation of the implementation\n\u2022 Length `N` is taken once; if `N == 0` we immediately return `[]`.  \n\u2022 Converting to `np.asarray` guarantees constant-time indexed access for both lists and arrays without affecting complexity (still O(N\u00b2)), and avoids copying when `x` is already a NumPy array.  \n\u2022 Two nested loops implement the direct definition of autocorrelation:  \n  outer loop over lags `k = 0 \u2026 N-1`,  \n  inner loop over valid indices `n = 0 \u2026 N-k-1`.  \n\u2022 Each computed sum `s` is appended to `result`, which is ultimately returned as a standard Python list as required."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\"\"\"\n    try:\n        # Convert to NumPy array of floats\n        A = np.asarray(X, dtype=float)\n    except Exception:\n        return False                      # non-numeric or unconvertible input\n\n    # Must be a 2-D matrix with at least one element\n    if A.ndim != 2 or A.size == 0:\n        return False\n\n    # All entries must be finite and lie within [0, 1]\n    if ( ~np.isfinite(A) ).any():        # catches NaN/Inf\n        return False\n    if ((A < 0) | (A > 1)).any():\n        return False\n\n    # Each row must sum to 1 (within numerical tolerance)\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-8):\n        return False\n\n    return True\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    Scan *nums* once while maintaining a max-heap (size \u2264 k) implemented\n    with a plain Python list.  At the end, the heap contains exactly the\n    k smallest elements (in no particular order) \u2013 we then sort and\n    return it.\n\n    If *k* is not strictly between 0 and len(nums) (inclusive), return -1.\n\n    The standard ``heapq`` module is **not** used: the helpers\n    ``_move_up`` and ``_move_down`` restore the heap property after\n    insertions and replacements.\n    \"\"\"\n    # Guard against invalid k\n    n = len(nums)\n    if k <= 0 or k > n:\n        return -1\n\n    # -------------  Heap helpers (max-heap) ------------- #\n    def _move_up(heap: List[Number], idx: int) -> None:\n        \"\"\"Bubble the element at *idx* up until the max-heap property holds.\"\"\"\n        while idx > 0:\n            parent = (idx - 1) // 2\n            if heap[idx] > heap[parent]:          # bigger than parent \u2192 swap\n                heap[idx], heap[parent] = heap[parent], heap[idx]\n                idx = parent\n            else:\n                break\n\n    def _move_down(heap: List[Number], idx: int) -> None:\n        \"\"\"Push the element at *idx* down until the max-heap property holds.\"\"\"\n        size = len(heap)\n        while True:\n            left = 2 * idx + 1\n            right = left + 1\n            largest = idx\n\n            if left < size and heap[left] > heap[largest]:\n                largest = left\n            if right < size and heap[right] > heap[largest]:\n                largest = right\n\n            if largest != idx:                    # child is larger \u2192 swap\n                heap[idx], heap[largest] = heap[largest], heap[idx]\n                idx = largest\n            else:\n                break\n    # ---------------------------------------------------- #\n\n    heap: List[Number] = []\n\n    for x in nums:\n        if len(heap) < k:                 # heap not full \u2192 simple insert\n            heap.append(x)\n            _move_up(heap, len(heap) - 1)\n        elif x < heap[0]:                 # new value smaller than current max\n            heap[0] = x                   # put it at root\n            _move_down(heap, 0)           # restore heap\n\n    # Heap now contains the k smallest elements in arbitrary order\n    heap.sort()\n    return heap\n```"}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n    \"\"\"Newton\u2019s method for 2\u2013D linear least\u2013squares.\n\n    Parameters\n    ----------\n    X : (n, 2) array_like\n        Design matrix.\n    y : (n,) or (n, 1) array_like\n        Target vector.\n    epsilon : float, optional\n        Stopping threshold for \u2016\u2207\u2016\u2082.\n    max_iter : int, optional\n        Maximum number of Newton steps.\n\n    Returns\n    -------\n    list[list[float]]\n        2\u00d71 list with each coefficient rounded to 4 decimals.\n    \"\"\"\n    # Ensure proper shapes & dtypes\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).reshape(-1)          # flatten to 1-D\n    n = X.shape[0]\n\n    # Initial weights\n    w = np.ones(2, dtype=float)\n\n    # Hessian is constant for quadratic loss \u2014 compute once\n    H = (2.0 / n) * X.T @ X\n    H_inv = np.linalg.pinv(H)      # works for singular or full-rank\n\n    for _ in range(max_iter):\n        # Gradient\n        grad = (2.0 / n) * X.T @ (X @ w - y)\n\n        # Stopping criterion\n        if np.linalg.norm(grad) < epsilon:\n            break\n\n        # Newton update\n        w -= H_inv @ grad\n\n    # Round to 4 decimals and return as 2\u00d71 Python list\n    return [[round(float(w[0]), 4)],\n            [round(float(w[1]), 4)]]\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport string\n\n# A small but reasonably comprehensive list of the most common English stop-words\n_STOP_WORDS: set[str] = {\n    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\",\n    \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\",\n    \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\",\n    \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\",\n    \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\",\n    \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\",\n    \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\",\n    \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\",\n    \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\",\n    \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n    \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\",\n    \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\",\n    \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\n    \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\",\n    \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\",\n    \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\",\n    \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\",\n    \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\",\n    \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\",\n    \"yourselves\",\n}\n\n\ndef tokenize_whitespace(\n    line: str,\n    lowercase: bool = True,\n    filter_stopwords: bool = True,\n    filter_punctuation: bool = True,\n    **kwargs,\n) -> list[str]:\n    \"\"\"Tokenize *line* by whitespace with optional lower-casing, punctuation\n    stripping and stop-word removal.\n\n    Args:\n        line: Input text string.\n        lowercase: If True, convert text to lower-case before tokenisation.\n        filter_stopwords: If True, remove common English stop-words.\n        filter_punctuation: If True, strip ASCII punctuation from each token.\n        **kwargs: Placeholder for future options.\n\n    Returns:\n        List of processed tokens in their original order.\n    \"\"\"\n\n    # 1. Optional case normalisation\n    if lowercase:\n        line = line.lower()\n\n    # 2. Initial whitespace tokenisation\n    raw_tokens = line.split()\n\n    processed: list[str] = []\n    punct_table = string.punctuation  # local alias for speed\n\n    for token in raw_tokens:\n        # 3. Optional leading/trailing punctuation stripping\n        if filter_punctuation:\n            token = token.strip(punct_table)\n\n        # Skip empty results (may become empty after stripping punctuation)\n        if not token:\n            continue\n\n        # 4. Optional stop-word filtering (case-insensitive)\n        if filter_stopwords and token.lower() in _STOP_WORDS:\n            continue\n\n        processed.append(token)\n\n    return processed\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    n, d = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)           # running average of squared gradients\n\n    # use full-batch if the data set is smaller than the desired batch size\n    batch_size = n if n < batch_size else batch_size\n\n    for it in range(max_iter):\n        # ---- mini-batch selection (cyclic, wraps around) --------------------\n        start = (it * batch_size) % n\n        idx = np.arange(start, start + batch_size) % n\n        X_b = X[idx]\n        y_b = y[idx]\n\n        # ---- gradient on the current mini-batch ----------------------------\n        err = X_b @ w - y_b\n        g = (X_b.T @ err) / len(y_b)          # shape: (d,)\n\n        # ---- RMSprop parameter update --------------------------------------\n        s = rho * s + (1.0 - rho) * (g ** 2)\n        w -= eta * g / (np.sqrt(s) + eps_station)\n\n        # ---- early stopping (check gradient on the whole data set) ---------\n        full_grad = (X.T @ (X @ w - y)) / n\n        if np.linalg.norm(full_grad) < epsilon:\n            break\n\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Numerically\u2013stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x     : scalar, list or np.ndarray\n        Input value(s).\n    deriv : bool, default False\n        If True return the gradient (sigmoid), otherwise SoftPlus.\n\n    Returns\n    -------\n    float or list\n        Rounded result(s) \u2013 float when the input was a scalar,\n        otherwise a Python list.\n    \"\"\"\n    # Remember if the caller passed a plain scalar\n    is_scalar = np.isscalar(x)\n\n    # Convert to a NumPy array for vectorised math\n    x = np.asarray(x, dtype=np.float64)\n\n    if deriv:                       # ----- gradient: sigmoid(x)\n        # stable sigmoid:\n        #  \u03c3(x) = 1 / (1 + e^-x)           for x >= 0\n        #  \u03c3(x) = e^x / (1 + e^x)          for x < 0\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n\n        out = np.empty_like(x)\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    else:                           # ----- SoftPlus(x)\n        # stable softplus:\n        #  softplus(x) = log1p(exp(-|x|)) + max(x, 0)\n        out = np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n\n    # Round to 4 decimals\n    out = np.round(out, 4)\n\n    # Return in the requested format\n    if is_scalar:\n        return float(out)           # plain Python float\n    else:\n        return out.tolist()\n```\n\nThe implementation  \n\n\u2022 accepts scalars, lists or NumPy arrays,  \n\u2022 uses numerically-stable piece-wise formulas for both SoftPlus and the sigmoid,  \n\u2022 rounds every value to four decimal places,  \n\u2022 and returns a float for scalar input or a Python list otherwise."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef _count_ngrams(tokens: list[str], n: int) -> Counter:\n    \"\"\"\n    Helper: return a Counter of all length-n n-grams found in *tokens*.\n    Unigrams are counted as the string itself, higher orders as tuples.\n    \"\"\"\n    if n == 1:\n        return Counter(tokens)\n    counts = Counter()\n    for i in range(len(tokens) - n + 1):\n        counts[tuple(tokens[i:i + n])] += 1\n    return counts\n\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Unsmooth\u00aded MLE N-gram log-probability of *sequence* given *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be >= 1')\n\n    # Special cases: empty sequence or shorter than the requested n-gram\n    if len(sequence) < N:\n        return 0.0\n\n    # Build the required count tables\n    if N == 1:\n        unigram_counts = _count_ngrams(corpus, 1)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts = _count_ngrams(corpus, N)\n        prefix_counts = _count_ngrams(corpus, N - 1)\n\n    total_log_prob = 0.0\n\n    # Iterate over every length-N window in the sequence\n    for i in range(len(sequence) - N + 1):\n        if N == 1:\n            word = sequence[i]\n            numerator = unigram_counts.get(word, 0)\n            denominator = corpus_len\n        else:\n            ngram = tuple(sequence[i:i + N])\n            prefix = ngram[:-1]\n            numerator = ngram_counts.get(ngram, 0)\n            denominator = prefix_counts.get(prefix, 0)\n\n        # Unseen n-gram or prefix \u21d2 probability 0 \u21d2 log-probability \u2013inf\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n\n        prob = numerator / denominator\n        total_log_prob += float(np.log(prob))\n\n    # Round to 4 decimal places as required\n    return round(total_log_prob, 4)\n```\n\nThe function:\n\n1. Counts all n-grams (and prefixes) in the training corpus.  \n2. Slides a window of length `N` over the target sequence.  \n3. For each window, computes the MLE probability  \n\u2003 `count(ngram) / count(prefix)` (unigram case: `count(word)/|corpus|`).  \n4. As soon as a required count is zero it returns `float('-inf')`.  \n5. Otherwise it sums log-probabilities and returns the total rounded to 4 decimals."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    # Safety checks -----------------------------------------------------------\n    if y.size == 0:                 # empty node\n        return 0.0\n\n    criterion = criterion.lower()\n    \n    # Classification criteria -------------------------------------------------\n    if criterion in (\"entropy\", \"gini\"):\n        # Class probabilities p_k\n        _, counts = np.unique(y, return_counts=True)\n        p = counts / counts.sum()            # proportions\n        \n        if criterion == \"entropy\":\n            impurity = -np.sum(p * np.log2(p))\n        else:                                # gini\n            impurity = 1.0 - np.sum(p ** 2)\n    \n    # Regression criterion ----------------------------------------------------\n    elif criterion == \"mse\":\n        mean_y = y.mean()\n        impurity = np.mean((y - mean_y) ** 2)\n    \n    # Unsupported criterion ---------------------------------------------------\n    else:\n        raise ValueError(f\"Unsupported criterion: {criterion!r}\")\n\n    return round(float(impurity), 4)\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    # Convert all inputs to numpy arrays of type float\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    W_arr = np.asarray(W, dtype=float)\n\n    # X must be 2-D\n    if X_arr.ndim != 2:\n        return -1\n\n    n_samples, n_features = X_arr.shape\n\n    # Reshape y to (n_samples, 1) if it is one-dimensional\n    if y_arr.ndim == 1:\n        y_arr = y_arr.reshape(-1, 1)\n    # Check y dimensions\n    if y_arr.ndim != 2 or y_arr.shape[1] != 1 or y_arr.shape[0] != n_samples:\n        return -1\n\n    # Reshape W to (n_features, 1) if it is one-dimensional\n    if W_arr.ndim == 1:\n        W_arr = W_arr.reshape(-1, 1)\n    # Check W dimensions\n    if W_arr.ndim != 2 or W_arr.shape[1] != 1 or W_arr.shape[0] != n_features:\n        return -1\n\n    # Compute the gradient: (X\u1d40 (XW \u2013 y)) / N\n    residual = X_arr @ W_arr - y_arr               # shape: (n_samples, 1)\n    gradient = (X_arr.T @ residual) / n_samples    # shape: (n_features, 1)\n\n    # Round to 4 decimal places and convert to nested list\n    gradient_rounded = np.round(gradient, 4).tolist()\n\n    return gradient_rounded\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef lasso_regression(\n    X: list[float] | \"np.ndarray\",\n    y: list[float] | \"np.ndarray\",\n    degree: int,\n    reg_factor: float,\n    n_iterations: int = 1000,\n    tol: float = 1e-6,\n) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Convert input to 1-D float arrays\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = X.size\n\n    if m == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    if y.size != m:\n        raise ValueError(\"X and y must have the same length.\")\n    if degree < 0:\n        raise ValueError(\"degree must be non-negative.\")\n    if reg_factor < 0:\n        raise ValueError(\"reg_factor must be non-negative.\")\n\n    # ------------------------------------------------------------------ #\n    # 2. Build the design matrix [1, x, x\u00b2, \u2026 , x^degree]\n    # ------------------------------------------------------------------ #\n    X_poly = np.column_stack([np.ones_like(X)] + [X ** k for k in range(1, degree + 1)])\n\n    # ------------------------------------------------------------------ #\n    # 3. Coordinate-descent initialisation\n    # ------------------------------------------------------------------ #\n    d_plus_1 = degree + 1                      # number of coefficients (incl. bias)\n    w = np.zeros(d_plus_1)                    # weights initialised to 0\n\n    # helper: soft-thresholding operator\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n\n    # ------------------------------------------------------------------ #\n    # 4. Optimisation loop\n    # ------------------------------------------------------------------ #\n    for _ in range(n_iterations):\n        w_old = w.copy()\n\n        # ---- update the bias exactly (NOT regularised) ----------------\n        # w0 \u2190 mean( y \u2212 X_{\u00ac0}\u00b7w_{\u00ac0} )\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n\n        # ---- update the remaining coordinates -------------------------\n        # iterate over j = 1 \u2026 degree\n        y_minus_pred = None   # will lazily compute A @ w once per pass\n        for j in range(1, d_plus_1):\n            x_j = X_poly[:, j]\n\n            if y_minus_pred is None:                 # compute only once per pass\n                y_minus_pred = y - X_poly @ w\n\n            # \u03c1 = x\u2c7c\u1d40 (y \u2212 (X\u00b7w) + w\u2c7c x\u2c7c)\n            rho = x_j @ (y_minus_pred + w[j] * x_j)\n\n            # w\u2c7c \u2190 soft_threshold(\u03c1, \u03bb) / (x\u2c7c\u1d40x\u2c7c)\n            if reg_factor == 0.0:\n                w[j] = rho / (x_j @ x_j)\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / (x_j @ x_j)\n\n            # predictions changed -> refresh residual for later coordinates\n            y_minus_pred = y_minus_pred - (w[j] - w_old[j]) * x_j\n\n        # ---- convergence check ----------------------------------------\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n\n    # ------------------------------------------------------------------ #\n    # 5. Return rounded coefficients\n    # ------------------------------------------------------------------ #\n    return [round(float(coef), 4) for coef in w]\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]],\n                  B: list[list[float]],\n                  obs: list[int],\n                  t: int) -> list[float]:\n    \"\"\"\n    Compute the backward-probability vector \u03b2_t for a discrete Hidden Markov\n    Model.\n\n    Parameters\n    ----------\n    A   : transition-probability matrix, shape (N, N)\n    B   : emission-probability matrix, shape (N, M)\n    obs : list with the indices of the observed symbols; length = T\n    t   : time index for which \u03b2_t is required  (0 \u2264 t < T)\n\n    Returns\n    -------\n    list[float]\n        The N backward probabilities \u03b2_t(i) (i = 0 \u2026 N-1), each rounded\n        to 4 decimal places.\n    \"\"\"\n    A = np.asarray(A, dtype=float)          # (N, N)\n    B = np.asarray(B, dtype=float)          # (N, M)\n    T  = len(obs)\n    N  = A.shape[0]\n\n    if not (0 <= t < T):\n        raise ValueError(\"t must satisfy 0 \u2264 t < len(obs)\")\n\n    # \u03b2_{T-1}(i) = 1  for all i\n    beta_next = np.ones(N)\n\n    # Work backwards: k = T-2, \u2026, t\n    for k in range(T - 2, t - 1, -1):\n        # element-wise product b_j(o_{k+1}) * \u03b2_{k+1}(j)\n        emis_beta = B[:, obs[k + 1]] * beta_next           # shape (N,)\n        # \u03b2_k(i) = \u03a3_j a_ij * b_j(o_{k+1}) * \u03b2_{k+1}(j)\n        beta_curr = A.dot(emis_beta)                       # shape (N,)\n        beta_next = beta_curr                              # becomes \u03b2_{k} for next iteration\n\n    # beta_next now holds \u03b2_t\n    return np.round(beta_next, 4).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft(frame: np.ndarray, positive_only: bool = True) -> list:\n    \"\"\"Compute the (naive) Discrete Fourier Transform of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array (real or complex) containing the samples.\n        positive_only: If True, return only the non-negative-frequency\n            terms (first N//2+1 coefficients).\n\n    Returns\n    -------\n        list[complex]: DFT spectrum rounded to 4 decimals (real and imag\n        parts separately).  Length is N//2+1 when `positive_only` is True,\n        otherwise N.\n    \"\"\"\n    # Ensure input is a 1-D complex ndarray\n    x = np.asarray(frame, dtype=complex).ravel()\n    N = x.size\n    if N == 0:\n        return []\n\n    # Allocate output array\n    coeffs = np.empty(N, dtype=complex)\n\n    # Na\u00efve O(N\u00b2) DFT\n    n = np.arange(N)\n    for k in range(N):\n        # e^{-j 2\u03c0kn/N} for all n\n        coeffs[k] = np.sum(x * np.exp(-2j * np.pi * k * n / N))\n\n    # Keep only non-negative frequencies if requested\n    if positive_only:\n        coeffs = coeffs[: N // 2 + 1]\n\n    # Round to 4 decimals (real and imag separately) and convert to list\n    rounded = [complex(round(c.real, 4), round(c.imag, 4)) for c in coeffs]\n    return rounded\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Args:\n        A: transition matrix (NxN)\n        B: emission   matrix (NxM)\n        pi: initial state distribution (N)\n        obs: observation index sequence (length T)\n\n    Returns:\n        Probability P(O | \u03bb) rounded to 6 decimals.\n    \"\"\"\n    # If any list is empty -> nothing to compute\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    # convert to numpy arrays for fast vectorised computation\n    A  = np.asarray(A,  dtype=float)   # (N,N)\n    B  = np.asarray(B,  dtype=float)   # (N,M)\n    pi = np.asarray(pi, dtype=float)   # (N,)\n    obs = np.asarray(obs, dtype=int)   # (T,)\n\n    N = len(pi)\n    T = len(obs)\n\n    # \u03b2_{T-1}(i) = 1   for every state i\n    beta = np.ones(N, dtype=float)\n\n    # Iterate backwards: t = T-2 \u2026 0\n    for t in range(T - 2, -1, -1):\n        # factor_j = B[j, o_{t+1}] * \u03b2_{t+1}(j)\n        factor = B[:, obs[t + 1]] * beta         # shape (N,)\n        # \u03b2_t(i) = \u03a3_j  A[i,j] * factor_j\n        beta = A.dot(factor)                     # new \u03b2 becomes \u03b2_t\n\n    # Finally P(O|\u03bb) = \u03a3_i \u03c0_i B[i, o_0] \u03b2_0(i)\n    prob = float(np.dot(pi * B[:, obs[0]], beta))\n\n    return round(prob, 6)\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import accumulate   # (import kept \u2011 it is not used but harmless)\n\nTOL = 1e-8          # tolerance for the \u201csums to one\u201d check\n\n\ndef _row_valid(row: list[float]) -> bool:\n    \"\"\"\n    True  -> row is a valid probability vector\n    False -> row is invalid\n    \"\"\"\n    if any(p < 0 for p in row):          # negative probabilities never allowed\n        return False\n    return abs(sum(row) - 1.0) <= TOL\n\n\ndef gen_hmm_data(\n    S: list[float],\n    A: list[list[float]],\n    B: list[list[float]],\n    n_sample: int,\n    seed: int | None = None,\n) -> tuple[list[int], list[int]] | int:\n    \"\"\"\n    Generate a sequence of hidden states and observations from a Hidden Markov\n    Model defined by (S, A, B).\n\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Returns (states, observations) or -1 when *any* probability distribution\n    is invalid (i.e. a row does not sum to 1 within 1 \u00d7 10\u207b\u2078 or contains a\n    negative entry).\n    \"\"\"\n    # ------------------------ basic sanity checks ------------------------\n    if n_sample < 1:\n        raise ValueError(\"n_sample must be \u2265 1\")\n\n    N = len(S)                       # number of hidden states\n    if len(A) != N or len(B) != N:\n        return -1                    # shape mismatch is considered \u201cinvalid\u201d\n\n    # -------------------- validate all probability rows ------------------\n    if not _row_valid(S):\n        return -1\n\n    for row in A:\n        if len(row) != N or not _row_valid(row):\n            return -1\n\n    # emission matrix: every row length must be identical (same M)\n    M = len(B[0])\n    for row in B:\n        if len(row) != M or not _row_valid(row):\n            return -1\n\n    # --------------------- RNG seeding (if requested) --------------------\n    if seed is not None:\n        np.random.seed(seed)\n\n    # ------------------------------ sampling -----------------------------\n    states: list[int] = []\n    observations: list[int] = []\n\n    # 1) sample the initial hidden state\n    curr_state = int(np.random.choice(N, p=S))\n    states.append(curr_state)\n\n    # 2) sample its observation\n    obs = int(np.random.choice(M, p=B[curr_state]))\n    observations.append(obs)\n\n    # 3) produce the remaining n_sample-1 pairs\n    for _ in range(1, n_sample):\n        curr_state = int(np.random.choice(N, p=A[curr_state]))\n        states.append(curr_state)\n\n        obs = int(np.random.choice(M, p=B[curr_state]))\n        observations.append(obs)\n\n    return states, observations\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float = 0.01, momentum: float = 0.0, prev_update: np.ndarray | None = None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n    \"\"\"\n    # Ensure the previous update (velocity) exists\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    \n    # Compute the new momentum update (velocity)\n    new_update = momentum * prev_update + (1.0 - momentum) * grad\n    \n    # Update the parameters moving *against* the gradient\n    updated_w = w - learning_rate * new_update\n    \n    # Round to 4 decimal places and convert to Python lists\n    updated_w_list = np.round(updated_w, 4).tolist()\n    new_update_list = np.round(new_update, 4).tolist()\n    \n    return updated_w_list, new_update_list\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # -----------------------------------------------------------\n    # 1. Basic size checks\n    # -----------------------------------------------------------\n    n = len(S)                                  # #states\n    if n == 0 or not observations:              # empty S or observation seq\n        return -1\n\n    # A must be n \u00d7 n\n    if len(A) != n or any(len(row) != n for row in A):\n        return -1\n\n    # B must be n \u00d7 m   (all rows same length)\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any(len(row) != m for row in B):\n        return -1\n\n    # -----------------------------------------------------------\n    # 2. Observation indices in range\n    # -----------------------------------------------------------\n    for o in observations:\n        if o < 0 or o >= m:\n            return -1\n\n    # -----------------------------------------------------------\n    # 3. Convert to numpy for vectorised computation\n    # -----------------------------------------------------------\n    S = np.asarray(S, dtype=float)          # (n,)\n    A = np.asarray(A, dtype=float)          # (n, n)\n    B = np.asarray(B, dtype=float)          # (n, m)\n\n    # -----------------------------------------------------------\n    # 4. Forward algorithm\n    # -----------------------------------------------------------\n    # \u03b1_0(i) = S(i) * B(i, o_0)\n    alpha = S * B[:, observations[0]]       # (n,)\n\n    # \u03b1_t(j) = B(j, o_t) * \u03a3_i \u03b1_{t-1}(i) A(i, j)\n    for ot in observations[1:]:\n        alpha = B[:, ot] * (alpha @ A)      # (n,)  \u2190  (n,) @ (n,n) gives (n,)\n\n    likelihood = float(alpha.sum())\n\n    # -----------------------------------------------------------\n    # 5. Return rounded result\n    # -----------------------------------------------------------\n    return round(likelihood, 4)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Optional, Dict, List\n\n\nclass _FPNode:\n    \"\"\"A node in an FP-tree.\"\"\"\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item: Optional[str], parent: Optional['._FPNode']):\n        self.item: Optional[str] = item          # item name (None for root)\n        self.count: int = 0                      # support counter\n        self.parent: Optional['_FPNode'] = parent\n        self.children: Dict[str, _FPNode] = {}   # item -> child node\n        self.link: Optional['_FPNode'] = None    # next node with the same item\n\n\n# ----------------------------------------------------------------------\n# FP-tree construction\n# ----------------------------------------------------------------------\ndef _build_fp_tree(transactions: List[List[str]],\n                   min_support: int) -> tuple[Optional[_FPNode], Optional[dict]]:\n    \"\"\"\n    Build an FP-tree and the accompanying header table from *transactions*.\n    Returns (root, header_table) or (None, None) if the tree would be empty.\n    The header table maps an item -> [support, head-of-node-link].\n    \"\"\"\n    # 1. global item frequencies\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {item: c for item, c in freq.items() if c >= min_support}\n    if not freq:                       # no frequent single items\n        return None, None\n\n    # 2. prepare header table  item -> [support, first_node]\n    header: dict[str, list] = {i: [c, None] for i, c in freq.items()}\n\n    # 3. create the root of the FP-tree\n    root = _FPNode(None, None)\n\n    # 4. insert every transaction\n    for transaction in transactions:\n        # keep only frequent items and sort by (\u2012support, item)  (Han et al.)\n        filtered = [i for i in transaction if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n\n        # walk / extend the tree\n        current = root\n        for item in ordered:\n            if item in current.children:\n                child = current.children[item]\n                child.count += 1\n            else:\n                child = _FPNode(item, current)\n                child.count = 1\n                current.children[item] = child\n\n                # attach to header table\u2019s node-link structure\n                head = header[item][1]\n                if head is None:\n                    header[item][1] = child\n                else:\n                    while head.link is not None:\n                        head = head.link\n                    head.link = child\n            current = child\n\n    return root, header\n\n\n# ----------------------------------------------------------------------\n# Mining\n# ----------------------------------------------------------------------\ndef _mine_tree(header: dict, prefix: List[str], freq_itemsets: List[List[str]],\n               min_support: int) -> None:\n    \"\"\"\n    Recursively mine an FP-tree that is represented by *header*.\n    All frequent item-sets found are appended (in lexicographic order)\n    to *freq_itemsets*.  *prefix* is the current prefix (already sorted).\n    \"\"\"\n    # items in ascending support, then lexicographic \u2192 deterministic order\n    items = sorted(header.items(), key=lambda x: (x[1][0], x[0]))\n\n    for item, (support, node) in items:\n        new_prefix = sorted(prefix + [item])         # alphabetically sorted\n        freq_itemsets.append(new_prefix)             # every prefix is frequent\n\n        # ---------------- conditional pattern base -----------------\n        cond_transactions: List[List[str]] = []\n        link_node = node\n        while link_node is not None:\n            path: List[str] = []\n            parent = link_node.parent\n            while parent and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                path.reverse()                       # root \u2192 leaf order\n                # replicate *path* link_node.count times\n                cond_transactions.extend([path] * link_node.count)\n            link_node = link_node.link\n\n        # --------------- build / mine conditional tree --------------\n        cond_root, cond_header = _build_fp_tree(cond_transactions, min_support)\n        if cond_header:\n            _mine_tree(cond_header, new_prefix, freq_itemsets, min_support)\n\n\n# ----------------------------------------------------------------------\n# User facing function\n# ----------------------------------------------------------------------\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings). `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 inside each item-set items are sorted alphabetically;\n      \u2022 the outer list is sorted by `(len(itemset), itemset)`.\n    \"\"\"\n    if not transactions or min_support < 1:\n        return []\n\n    # 1. build the global FP-tree\n    root, header = _build_fp_tree(transactions, min_support)\n    if header is None:                 # no frequent items at all\n        return []\n\n    # 2. mine the tree\n    frequent_itemsets: list[list[str]] = []\n    _mine_tree(header, [], frequent_itemsets, min_support)\n\n    # 3. final deterministic ordering\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function returns a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    counts: dict = {}\n    for sample in data:               # one pass through the data\n        if not sample:\n            raise ValueError(\n                \"Each sample must contain at least one element (the label).\"\n            )\n        label = sample[-1]            # last element is the label\n        counts[label] = counts.get(label, 0) + 1\n    return counts\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"\n    Compute the variance reduction obtained by splitting the parent\u2010node targets ``y`` into\n    the two child nodes ``y_left`` and ``y_right`` in a regression tree.\n\n    Variance reduction (VR) is defined as\n        VR = Var(y) - (n_left / n) * Var(y_left) - (n_right / n) * Var(y_right)\n\n    where Var(\u00b7) is the (population) variance computed feature-wise and then summed over\n    all features to yield a single scalar. ``n``, ``n_left`` and ``n_right`` denote the\n    number of samples in the parent and the two children, respectively.\n\n    The function returns the VR rounded to 4 decimal places.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Target values in the parent node, shape (n_samples, n_targets) or (n_samples,).\n    y_left : np.ndarray\n        Target values that would go to the left child, non-empty.\n    y_right : np.ndarray\n        Target values that would go to the right child, non-empty.\n\n    Returns\n    -------\n    float\n        The variance reduction produced by the split, rounded to 4 decimals.\n    \"\"\"\n\n    def total_variance(arr: np.ndarray) -> float:\n        \"\"\"\n        Population variance summed over all target dimensions.\n        \"\"\"\n        # Per-feature population variance (ddof=0), then sum.\n        return np.var(arr, axis=0).sum()\n\n    # Number of samples\n    n = y.shape[0]\n    n_left = y_left.shape[0]\n    n_right = y_right.shape[0]\n\n    # Total variances\n    var_parent = total_variance(y)\n    var_left = total_variance(y_left)\n    var_right = total_variance(y_right)\n\n    # Variance reduction formula\n    vr = var_parent - (n_left / n) * var_left - (n_right / n) * var_right\n\n    # Return rounded to 4 decimals\n    return round(float(vr), 4)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef magnitude_spectrum(frames: \"np.ndarray\") -> \"list[list[float]]\":\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array that is treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The positive-frequency magnitude spectrum of every frame\n        (length N//2+1), rounded to four decimals.\n    \"\"\"\n    # Convert input to an ndarray and normalize the shape to (M, N)\n    frames = np.asarray(frames)\n    if frames.ndim == 1:\n        frames = frames[np.newaxis, :]\n    elif frames.ndim != 2:\n        raise ValueError(\"Input must be a 1-D or 2-D NumPy array.\")\n\n    # Real FFT along the last axis -> shape (M, N//2+1)\n    spectrum = np.abs(np.fft.rfft(frames, axis=-1))\n\n    # Round to four decimal places\n    spectrum = np.round(spectrum, 4)\n\n    # Return as nested Python list\n    return spectrum.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list[int],\n                X_test:  list[list[float]],\n                k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbour classifier (Euclidean metric).\n\n    Returns a list with the predicted class for every row in `X_test`.\n    If *k* is not a positive integer or *k* > n_train the function\n    returns the scalar -1 (as required by the specification).\n    \"\"\"\n    # 1) --- sanity-check k ----------------------------------------------------\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n\n    # 2) --- convert input lists to numpy arrays ------------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test,  dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n\n    # 3) --- prediction --------------------------------------------------------\n    preds: list[int] = []\n\n    for x in X_test:\n        # Squared Euclidean distances (square-root not necessary)\n        diff   = X_train - x\n        dists  = np.einsum('ij,ij->i', diff, diff)   # fast row-wise dot\n\n        # indices of the k smallest distances\n        if k == 1:\n            nn_idx = [int(np.argmin(dists))]\n        else:\n            nn_idx = np.argpartition(dists, k-1)[:k]\n\n        # majority vote among the neighbours\n        votes: dict[int, int] = {}\n        for lbl in y_train[nn_idx]:\n            votes[int(lbl)] = votes.get(int(lbl), 0) + 1\n\n        max_vote = max(votes.values())\n        # tie-break by picking the smallest class label\n        winner   = min(lbl for lbl, cnt in votes.items() if cnt == max_vote)\n\n        preds.append(winner)\n\n    return preds\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities (same shape).\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert to NumPy arrays for fast vectorised operations\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Machine epsilon for numerical stability (avoids log(0))\n    eps = np.finfo(float).eps\n\n    # Cross-entropy computation (unnormalised, i.e., summed over the batch)\n    loss = -np.sum(y * np.log(y_pred + eps))\n\n    # Round to 4 decimal places and return as a Python float\n    return float(np.round(loss, 4))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n    \"\"\"Forward propagation for an L-layer fully-connected neural network\n    that uses ReLU in all hidden layers and a sigmoid in the output layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing weights W1 \u2026 WL and biases b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        \u2022 AL_list  \u2013 the final activation rounded to 4 decimals\n                     and converted to a regular Python list.\n        \u2022 caches   \u2013 list with one cache per layer (kept for a backward pass).\n    \"\"\"\n    # ---------- helper functions ----------\n    def linear_forward(A_prev, W, b):\n        Z = W @ A_prev + b\n        return Z, (A_prev, W, b)\n\n    def relu(Z):\n        A = np.maximum(0, Z)\n        return A, Z  # activation-cache is simply Z\n\n    def sigmoid(Z):\n        A = 1.0 / (1.0 + np.exp(-Z))\n        return A, Z\n\n    # ---------- main forward loop ----------\n    caches = []\n    A = X\n    L = len(parameters) // 2  # number of layers\n\n    # layers 1 \u2026 L-1  (ReLU)\n    for l in range(1, L):\n        W = parameters[f\"W{l}\"]\n        b = parameters[f\"b{l}\"]\n        Z, lin_cache = linear_forward(A, W, b)\n        A, act_cache = relu(Z)\n        caches.append((lin_cache, act_cache))\n\n    # layer L (sigmoid)\n    WL = parameters[f\"W{L}\"]\n    bL = parameters[f\"b{L}\"]\n    ZL, lin_cache = linear_forward(A, WL, bL)\n    AL, act_cache = sigmoid(ZL)\n    caches.append((lin_cache, act_cache))\n\n    # ---------- formatting the required output ----------\n    AL_list = np.round(AL, 4).tolist()  # (1, m) \u2192 list\n\n    return AL_list, caches\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step and return the updated state.\n\n    Args:\n        params:  Current parameter vector.\n        grads:   Current gradient vector (same length as `params`).\n        m:       First-moment estimates.\n        u:       Exponentially weighted infinity-norms.\n        t:       Current time-step counter (starts at 1).\n        learning_rate: Learning rate \u03b7 (default 0.002).\n        beta1:   Exponential decay rate for the first moment \u03b2\u2081.\n        beta2:   Exponential decay rate for the infinity-norm \u03b2\u2082.\n        epsilon: Small constant to avoid division by zero.\n\n    Returns:\n        (new_params, new_m, new_u, new_t) with every float rounded to 6 decimals.\n    \"\"\"\n    # Convert to NumPy arrays for element-wise vectorised operations\n    params_arr = np.asarray(params, dtype=np.float64)\n    grads_arr  = np.asarray(grads,  dtype=np.float64)\n    m_arr      = np.asarray(m,      dtype=np.float64)\n    u_arr      = np.asarray(u,      dtype=np.float64)\n\n    # First-moment update\n    m_t = beta1 * m_arr + (1.0 - beta1) * grads_arr\n\n    # Infinity-norm update (component-wise maximum)\n    u_t = np.maximum(beta2 * u_arr, np.abs(grads_arr))\n\n    # Bias-corrected step size uses the CURRENT time-step `t`\n    step_size = learning_rate / (1.0 - beta1 ** t)\n\n    # Parameter update\n    step      = step_size * (m_t / (u_t + epsilon))\n    new_params = params_arr - step\n\n    # Increment time-step counter\n    new_t = t + 1\n\n    # Round everything to 6 decimal places and convert back to Python lists\n    round_params = np.round(new_params, 6).tolist()\n    round_m      = np.round(m_t,       6).tolist()\n    round_u      = np.round(u_t,       6).tolist()\n\n    return (round_params, round_m, round_u, new_t)\n```"}
{"task_id": 80, "completion_id": 0, "completion": "Here is a compact, fully-vectorised implementation that follows the Adadelta equations for the very first optimisation step (both running averages start at 0):\n\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform ONE Adadelta update assuming both running averages are zero.\"\"\"\n    # Convert to NumPy arrays for element-wise maths\n    w      = np.asarray(w,       dtype=np.float64)\n    grad_w = np.asarray(grad_w,  dtype=np.float64)\n\n    one_minus_rho = 1.0 - rho\n\n    # First running average of squared gradients (E_grad)\n    E_grad = one_minus_rho * grad_w**2                   # \u03c1*0 + (1-\u03c1)*g\u00b2\n    RMS_grad = np.sqrt(E_grad + eps)\n\n    # Second running average of squared updates (E_delta) \u2013 still zero\n    RMS_delta = np.sqrt(eps)                             # \u221a(0 + \u03b5)\n\n    # Adaptive learning rate and actual parameter update\n    adaptive_lr = RMS_delta / RMS_grad                   # element-wise\n    delta_w     = adaptive_lr * grad_w                   # \u0394w\n\n    # Update second running average (not used further in this FIRST step)\n    E_delta = one_minus_rho * delta_w**2                 # \u03c1*0 + (1-\u03c1)*\u0394w\u00b2\n\n    # New weights\n    w_new = w - delta_w\n\n    # Return a plain Python list rounded to 4 decimals\n    return np.round(w_new, 4).tolist()\n```\n\nThe update keeps any weight whose gradient is zero unchanged because  \n\u0394w = adaptive_lr \u00d7 g = 0 in that case."}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n    \n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n    \n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    eps = 1e-15                       # small constant to avoid log(0)\n    A2 = np.clip(A2, eps, 1.0 - eps)  # clip predictions\n    m = Y.size                        # number of samples\n\n    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n    return float(np.round(cost, 6))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Q-values for every available action in the current state.\n    epsilon : float\n        Exploration parameter 0 \u2264 \u03b5 \u2264 1.\n\n    Returns\n    -------\n    list[float]\n        A list with the probability of choosing every action under the \u03b5-soft\n        policy.  All probabilities are rounded to 4 decimal places and sum to\n        1 (within 1\u00d710\u207b\u00b9\u00b2).\n    \"\"\"\n    # ---- basic checks -------------------------------------------------------\n    q = np.asarray(Q, dtype=float).ravel()          # ensure 1-D float array\n    if q.ndim != 1:\n        raise ValueError(\"Q must be a one-dimensional sequence of numbers.\")\n    if not (0.0 <= epsilon <= 1.0):\n        raise ValueError(\"epsilon must satisfy 0 \u2264 \u03b5 \u2264 1.\")\n\n    n = q.size\n    if n == 0:\n        raise ValueError(\"Q must contain at least one action.\")\n\n    # ---- \u03b5-soft probabilities (full precision) -----------------------------\n    greedy_idx = int(np.argmax(q))                  # first arg-max\n    base_prob = epsilon / n\n    probs = np.full(n, base_prob, dtype=float)\n    probs[greedy_idx] += (1.0 - epsilon)            # favour greedy action\n\n    # ---- convert to 4-decimal representation that still sums to one --------\n    #\n    # Work with integer \u201cticks\u201d of 0.0001 = 1/10000 to avoid rounding drift.\n    #\n    ticks = probs * 10000.0\n    int_part = np.floor(ticks).astype(int)          # 4-decimal truncation\n    remainder = ticks - int_part                    # fractional part in ticks\n    deficit = 10000 - int_part.sum()                # ticks left to distribute\n\n    if deficit > 0:                                # need to add \u2018deficit\u2019 ticks\n        # give extra ticks to the largest remainders\n        order = np.argsort(-remainder)              # descending remainders\n        int_part[order[:deficit]] += 1\n    elif deficit < 0:                              # (very rare) too many ticks\n        order = np.argsort(remainder)               # smallest remainders first\n        int_part[order[: -deficit]] -= 1\n\n    final_probs = int_part / 10000.0                # back to floats (4 dp)\n\n    # ---- final sanity check -------------------------------------------------\n    if abs(final_probs.sum() - 1.0) > 1e-12:\n        raise RuntimeError(\"Probabilities do not sum to one within tolerance.\")\n\n    return final_probs.round(4).tolist()\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean-squared-error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True target values.\n    y_pred : np.ndarray\n        Current model predictions.\n    h_pred : np.ndarray\n        Predictions of the new base learner.\n\n    Returns\n    -------\n    float\n        Optimal step size \u03b3 rounded to four decimal places.  If all h_pred are\n        zero (perfect residual fit) the function returns 1.0.\n    \"\"\"\n    # Compute numerator and denominator of the closed-form solution\n    residual = y - y_pred\n    numerator = np.dot(residual, h_pred)          # \u03a3 (y_i \u2212 \u0177_i) h_i\n    denominator = np.dot(h_pred, h_pred)          # \u03a3 h_i\u00b2\n\n    # If the learner contributes nothing (\u2016h\u2016\u00b2 == 0), take full step of 1\n    if np.isclose(denominator, 0.0):\n        gamma = 1.0\n    else:\n        gamma = numerator / denominator\n\n    # Round to 4 decimal places and return as Python float\n    return float(round(gamma, 4))\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax_regression_step(X: np.ndarray, \n                            Y: np.ndarray, \n                            W: np.ndarray, \n                            lr: float) -> list[list[float]]:\n    \"\"\"Perform one gradient\u2013descent update step for multi-class Softmax regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Input samples of shape (N, D).\n    Y : np.ndarray\n        One-hot encoded labels of shape (N, C).\n    W : np.ndarray\n        Current weight matrix of shape (D, C).\n    lr : float\n        Learning-rate (if 0 \u2192 no update).\n    \n    Returns\n    -------\n    list[list[float]]\n        Updated weight matrix rounded to 4 decimals.\n    \"\"\"\n    # Early-exit conditions \u2014 nothing to do\n    if lr == 0 or np.all(X == 0):\n        return np.round(W, 4).tolist()\n    \n    N = X.shape[0]                      # number of samples\n    \n    # 1. Logits\n    Z = X @ W                           # shape (N, C)\n    \n    # 2. Soft-max (numerically stable)\n    Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n    exp_Z   = np.exp(Z_shift)\n    Y_hat   = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n    \n    # 3. Gradient of average cross-entropy loss\n    grad = (X.T @ (Y_hat - Y)) / N      # shape (D, C)\n    \n    # 4. Gradient-descent update\n    W_new = W - lr * grad\n    \n    # 5. Return as list of lists, rounded to 4 decimals\n    return np.round(W_new, 4).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Union\n\nLabel = Union[int, float, str]\n\ndef aggregate_random_forest_votes(predictions: List[List[Label]]) -> List[Label]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Handle edge-cases: empty forest or empty sample set\n    if not predictions or not predictions[0]:\n        return []\n\n    n_samples = len(predictions[0])                # number of samples\n    majority_votes: List[Label] = []\n\n    for idx in range(n_samples):\n        # Collect all tree votes for the current sample\n        votes = [tree_preds[idx] for tree_preds in predictions]\n\n        # Count occurrences of each label\n        vote_counts = Counter(votes)\n        max_count = max(vote_counts.values())\n\n        # All labels that reached the highest vote count\n        tied_labels = [label for label, cnt in vote_counts.items()\n                       if cnt == max_count]\n\n        # Tie-break by picking the smallest label (numeric or lexicographic)\n        majority_votes.append(min(tied_labels))\n\n    return majority_votes\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) that contains the input data,\n           stored column-wise (each column is one example).\n        Y: 2-D NumPy array of shape (n_y, m) that contains the labels,\n           stored column-wise.\n\n    Returns:\n        A tuple (n_x, n_h, n_y) where\n            n_x \u2013 number of input features  (rows of X),\n            n_h \u2013 size of the hidden layer (fixed to 10),\n            n_y \u2013 number of output units    (rows of Y).\n    \"\"\"\n    n_x = X.shape[0]        # number of input features\n    n_h = 10                # fixed hidden layer size\n    n_y = Y.shape[0]        # number of output units\n\n    return n_x, n_h, n_y\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Determine if the incoming object should be treated as a scalar\n    is_scalar = np.isscalar(z) or (isinstance(z, np.ndarray) and z.shape == ())\n\n    # Convert to NumPy array for vectorized computation\n    z_arr = np.asarray(z, dtype=float)\n\n    # Numerically stable softplus: ln(1 + e**z) == logaddexp(0, z)\n    soft = np.logaddexp(0.0, z_arr)\n\n    # Round to 4 decimal places\n    soft = np.round(soft, 4)\n\n    # Return according to the input type requirement\n    if is_scalar:\n        return float(soft)\n    else:\n        return soft.tolist()\n```\n"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        Activation to use: \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        (A_as_list, (linear_cache, activation_cache))\n        where A_as_list is rounded to 4 decimals and converted to a list.\n    \"\"\"\n    # ----- Linear step -------------------------------------------------------\n    Z = W @ A_prev + b          # broadcasting takes care of the bias term\n    linear_cache = (A_prev, W, b)\n\n    # ----- Non-linear activation --------------------------------------------\n    if activation.lower() == \"relu\":\n        A = np.maximum(0, Z)\n    elif activation.lower() == \"sigmoid\":\n        A = 1.0 / (1.0 + np.exp(-Z))\n    else:\n        raise ValueError('activation must be either \"relu\" or \"sigmoid\"')\n\n    # ----- Shape check -------------------------------------------------------\n    assert A.shape == (W.shape[0], A_prev.shape[1]), (\n        f\"Output shape {A.shape} does not match \"\n        f\"expected {(W.shape[0], A_prev.shape[1])}\"\n    )\n\n    # ----- Prepare outputs ---------------------------------------------------\n    activation_cache = Z\n    A_out = A.round(4).tolist()          # required precision & format\n    cache = (linear_cache, activation_cache)\n\n    return A_out, cache\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\n\ndef _get_field(obj: Any, name: str):\n    \"\"\"Return attribute or (key) item `name` from `obj`, else raise KeyError.\"\"\"\n    if hasattr(obj, name):\n        return getattr(obj, name)\n    try:\n        return obj[name]\n    except Exception as exc:          # noqa: BLE001, E722\n        raise KeyError(f\"{name!r} not found in supplied object.\") from exc\n\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Mean-squared error between a policy\u2019s value estimates and the truth.\"\"\"\n    # Extract data ------------------------------------------------------------\n    try:\n        arm_evs = _get_field(bandit, \"arm_evs\")\n    except KeyError:\n        raise ValueError(\"`bandit` must supply an `arm_evs` field.\")\n\n    try:\n        ev_estimates = _get_field(policy, \"ev_estimates\")\n    except KeyError:\n        return np.nan  # no estimates supplied\n\n    # No estimates or empty dict \u2192 nan\n    if not ev_estimates:\n        return np.nan\n\n    # Compute squared errors --------------------------------------------------\n    errors = []\n    for arm_idx in sorted(ev_estimates):\n        # Skip indices that have no true EV (out-of-range); could also raise.\n        if 0 <= arm_idx < len(arm_evs):\n            diff = ev_estimates[arm_idx] - arm_evs[arm_idx]\n            errors.append(diff * diff)\n\n    # If nothing could be compared \u2192 nan\n    if not errors:\n        return np.nan\n\n    mse_val = float(np.mean(errors))\n    return round(mse_val, 4)\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_backward(\n    dA: list[list[int | float]],\n    activation_cache: list[list[int | float]]\n) -> list[list[int | float]] | int:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z (same shape as dA).\n        Returns -1 if dA and activation_cache do not share the same shape.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorised operations\n    dA_arr = np.asarray(dA)\n    Z_arr = np.asarray(activation_cache)\n\n    # Shape check\n    if dA_arr.shape != Z_arr.shape:\n        return -1\n\n    # Element-wise gradient: keep upstream gradient where Z > 0, else 0\n    dZ_arr = dA_arr * (Z_arr > 0)\n\n    # Convert back to (nested) Python lists\n    return dZ_arr.tolist()\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    # `bool` is a subclass of `int`, so we must explicitly rule it out.\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU).\n\n    Args\n    ----\n    Z : array-like\n        A NumPy array, Python scalar or (nested) list of numbers representing\n        the pre-activation values.\n\n    Returns\n    -------\n    tuple[list, list]\n        A 2-tuple ``(A, cache)`` where\n            \u2022 A     \u2013 ReLU(Z)  (all negatives set to 0),\n            \u2022 cache \u2013 the original input Z,\n        both converted to plain Python (sub)lists via ``tolist()`` so the\n        result is JSON-serialisable.\n    \"\"\"\n    # Convert the input to a NumPy array (this is a *view*, not a copy).\n    Z_arr = np.asarray(Z)\n\n    # Vectorised ReLU: keep positives / zeros, clip negatives to 0.\n    A_arr = np.maximum(0, Z_arr)\n\n    # Convert back to native Python lists (or scalars) for serialisation.\n    return A_arr.tolist(), Z_arr.tolist()\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Hashable, Sequence, TypeVar\n\nLabel = TypeVar(\"Label\", bound=Hashable)\n\n\ndef knn_majority_vote(neighbors_targets: Sequence[Label]) -> Label:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A non-empty sequence containing the class labels of\n            the k nearest neighbours.\n\n    Returns:\n        The label that appears most frequently.  In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n\n    Raises:\n        ValueError: If *neighbors_targets* is empty.\n    \"\"\"\n    if not neighbors_targets:  # empty input guard\n        raise ValueError(\"neighbors_targets must contain at least one label.\")\n\n    counts = Counter(neighbors_targets)\n\n    # highest vote count\n    top_freq = max(counts.values())\n\n    # all labels that reach the highest vote count\n    top_labels = (label for label, freq in counts.items() if freq == top_freq)\n\n    # default ordering chooses the smallest label in case of tie\n    return min(top_labels)\n```\n\nExplanation of the key steps\n1. Count occurrences with `Counter`.\n2. Find the maximum frequency (`top_freq`).\n3. Collect every label that reaches `top_freq`.\n4. Return the smallest of these labels with Python\u2019s built-in `min`, which\n   obeys the default ordering (numeric for numbers, lexicographic for strings,\n   etc.)."}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    # --------------------------- guard clauses ----------------------------\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:                 # \u201cno N-grams exist\u201d\n        return 0.0\n\n    # ----------------------------- training -------------------------------\n    # Build vocabulary (words seen in the training corpus)\n    vocab = set()\n    # n-gram counters: index 1..N  (Counter of tuples)\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n\n        # populate n-gram counters\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i : i + n])\n                ngram_counts[n][ngram] += 1\n\n    # add the explicit <UNK> token to the vocabulary\n    vocab.add(\"<UNK>\")\n    V = len(vocab)                          # vocabulary size |V|\n\n    # total token count (needed for unigram denominator)\n    total_tokens = sum(ngram_counts[1].values())\n\n    # ----------------------- prepare target sentence ----------------------\n    # replace unseen words by <UNK>\n    seq_tokens = [tok if tok in vocab else \"<UNK>\" for tok in seq_tokens]\n\n    # --------------------- compute sentence log-prob ----------------------\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i : i + N])\n        context = ngram[:-1]                # empty tuple for unigrams\n        target  = ngram[-1]\n\n        # counts from training corpus\n        count_ngram = ngram_counts[N].get(ngram, 0)\n\n        if N == 1:                          # unigram model: context is \u201c\u201d\n            context_count = total_tokens\n        else:\n            context_count = ngram_counts[N - 1].get(context, 0)\n\n        # Additive (Lidstone/Laplace) smoothing\n        prob = (count_ngram + K) / (context_count + K * V)\n        log_prob += math.log(prob)\n\n    return round(log_prob, 4)\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        0.0\u2006/\u20061.0 with the same layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    # -------- scalar branch --------------------------------------------------\n    if np.isscalar(Z) or (isinstance(Z, np.ndarray) and Z.ndim == 0):\n        return 1.0 if float(Z) > 0.0 else 0.0\n\n    # -------- array / list branch -------------------------------------------\n    arr = np.asarray(Z)\n    derivative = (arr > 0).astype(float)      # 1.0 where z>0 else 0.0\n    return derivative.tolist()\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The returned object keeps the *same container type* as the input:\n    scalar \u2192 scalar, list \u2192 list / list-of-lists, NumPy array \u2192 NumPy array.\n\n    Args\n    ----\n    Z : int | float | list | numpy.ndarray\n        Scalar, 1-D or 2-D data on which ReLU is to be applied.\n\n    Returns\n    -------\n    Same type as *Z* with all negative entries replaced by 0.\n    \"\"\"\n    # ---------- 1. NumPy array \u2013 easiest: just use np.maximum -------------\n    if isinstance(Z, np.ndarray):\n        return np.maximum(Z, 0)             # vectorised, keeps np.ndarray\n\n    # ---------- 2. Pure Python (or NumPy) scalar --------------------------\n    # np.number covers NumPy scalar types such as np.float32, np.int64 \u2026\n    if isinstance(Z, (int, float, np.number)):\n        # Built-in max keeps the incoming scalar type (int/float)\n        return max(Z, 0)\n\n    # ---------- 3. Python list (1-D or 2-D) -------------------------------\n    # Convert once to a NumPy array, apply the vectorised op, convert back.\n    if isinstance(Z, list):\n        return np.maximum(np.asarray(Z), 0).tolist()\n\n    # ---------- 4. Anything else is unsupported --------------------------\n    raise TypeError(\n        \"relu_ only accepts int, float, list, or numpy.ndarray as input.\"\n    )\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Labels predicted by a classifier.\n\n    Returns\n    -------\n    float | int\n        Classification error rate rounded to four decimals,\n        or \u22121 when the input lengths are not identical.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays\n    actual_arr    = np.asarray(actual).ravel()\n    predicted_arr = np.asarray(predicted).ravel()\n\n    # Check that both sequences have the same length\n    if actual_arr.size != predicted_arr.size:\n        return -1\n\n    total = actual_arr.size\n    if total == 0:                     # empty input: define error as 0\n        return 0.0\n\n    # Count mismatches\n    mismatches = np.count_nonzero(actual_arr != predicted_arr)\n\n    # Compute error rate and round to 4 decimals\n    error_rate = round(mismatches / total, 4)\n    return error_rate\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Axis along which to apply the softmax (negative values allowed).\n\n    Returns:\n        A (nested) Python list containing the soft-maxed probabilities,\n        each rounded to four decimal places.\n\n    Raises:\n        ValueError: If ``x`` is 1-D.\n    \"\"\"\n    # -------- input validation ------------------------------------------------\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n\n    # NumPy already handles negative axes, so no extra work is needed.\n    # -------- numerically-stable softmax --------------------------------------\n    # 1. Subtract the maximum for numerical stability\n    max_along_axis = np.max(x, axis=axis, keepdims=True)\n    shifted        = x - max_along_axis\n\n    # 2. Exponentiate\n    exp_shifted = np.exp(shifted)\n\n    # 3. Normalise by the sum of exponentials\n    sum_exp = np.sum(exp_shifted, axis=axis, keepdims=True)\n    softmax_vals = exp_shifted / sum_exp\n\n    # 4. Round to 4 decimal places and convert to pure Python lists\n    softmax_vals = np.round(softmax_vals, 4)\n\n    return softmax_vals.tolist()\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef polynomial_regression_predict(x: list[float], y: list[float], degree: int,\n                                  x_pred: list[float]) -> list[float]:\n    \"\"\"Fit a polynomial regression model and return predictions.\n\n    Parameters\n    ----------\n    x : list[float]\n        Training input values.\n    y : list[float]\n        Training target values.\n    degree : int\n        Degree of the polynomial to be fitted (must be >= 0).\n    x_pred : list[float]\n        Values at which the fitted model should be evaluated.\n\n    Returns\n    -------\n    list[float] | int\n        Predicted values for *x_pred* rounded to 4 decimal places, or -1 if\n        the model cannot be fitted (e.g. wrong input, not enough data).\n    \"\"\"\n    # Basic sanity checks ----------------------------------------------------\n    # degree\n    if not isinstance(degree, int) or degree < 0:\n        return -1\n\n    # Consistency of training data\n    if len(x) != len(y) or len(x) == 0:\n        return -1\n\n    # Need at least degree + 1 distinct samples to fit the polynomial\n    if len(x) < degree + 1:\n        return -1\n\n    # -----------------------------------------------------------------------\n    # Construct the design matrix for training data: [1, x, x\u00b2, \u2026, x^degree]\n    X = np.vander(np.asarray(x, dtype=float), N=degree + 1, increasing=True)\n\n    # Targets\n    y_vec = np.asarray(y, dtype=float)\n\n    # Solve for weights using least squares\n    # (works even if X is not full rank thanks to numpy's regularisation)\n    try:\n        w, *_ = np.linalg.lstsq(X, y_vec, rcond=None)\n    except Exception:\n        # Any numerical failure \u2192 cannot fit\n        return -1\n\n    # -----------------------------------------------------------------------\n    # Prediction phase\n    if len(x_pred) == 0:          # no prediction points \u2013 return empty list\n        return []\n\n    X_pred = np.vander(np.asarray(x_pred, dtype=float), N=degree + 1,\n                       increasing=True)\n    y_pred = X_pred @ w\n\n    # Round to 4 decimal places and return as plain Python list\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"Base-2 entropy of a 1-D label array.\"\"\"\n    n = labels.size\n    if n == 0:                       # empty node \u21d2 zero entropy\n        return 0.0\n    _, counts = np.unique(labels, return_counts=True)\n    probs = counts / n\n    return -np.sum(probs * np.log2(probs))\n\n\ndef information_gain(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Computes the information gain of a proposed split.\n\n    Args:\n        y: 1-D NumPy array containing the class labels of the parent node.\n        splits: List of 1-D NumPy arrays. Each array contains the labels that\n                fall into one child node produced by the split.\n\n    Returns:\n        Information gain rounded to 4 decimal places.\n    \"\"\"\n    parent_entropy = _entropy(y)\n    n_parent = y.size\n\n    weighted_child_entropy = 0.0\n    for child in splits:\n        weight = child.size / n_parent if n_parent else 0.0\n        weighted_child_entropy += weight * _entropy(child)\n\n    ig = parent_entropy - weighted_child_entropy\n\n    # Numerical safety: IG cannot be negative in theory\n    ig = max(ig, 0.0)\n\n    return round(float(ig), 4)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    # 1. Validate sigma\n    if sigma <= 0:\n        return -1\n\n    # 2. Convert *x* to a NumPy array for unified processing\n    arr = np.asarray(x, dtype=float)\n\n    # 3. Handle empty input\n    if arr.size == 0:\n        return []\n\n    # 4. Gaussian PDF computation\n    coeff = 1.0 / (np.sqrt(2.0 * np.pi) * sigma)\n    exponent = -((arr - mean) ** 2) / (2.0 * sigma ** 2)\n    pdf_vals = coeff * np.exp(exponent)\n\n    # 5. Round to 5 decimal places\n    pdf_vals = np.round(pdf_vals, 5)\n\n    # 6. Ensure the return type is a flat Python list\n    if pdf_vals.ndim == 0:                      # scalar input\n        return [float(pdf_vals)]\n    return pdf_vals.ravel().tolist()\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (left here to match the supplied header)\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\n\n    Returns\n    -------\n    tuple[list, list, list, int]\n        (updated parameters, new first-moment, new second-moment, next time-step).\n        All numerical arrays are rounded to 6 decimal places and converted to\n        native Python lists.\n    \"\"\"\n    # ---- 1. Update biased first & second moment estimates -----------------\n    m_t = beta1 * m_prev + (1.0 - beta1) * grad\n    v_t = beta2 * v_prev + (1.0 - beta2) * (grad ** 2)\n\n    # ---- 2. Bias-correct the moments --------------------------------------\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n\n    # ---- 3. Parameter update ----------------------------------------------\n    theta_new = theta - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # ---- 4. Prepare output: round & convert to Python lists ---------------\n    theta_out = np.round(theta_new, 6).tolist()\n    m_out     = np.round(m_t,        6).tolist()\n    v_out     = np.round(v_t,        6).tolist()\n\n    return theta_out, m_out, v_out, t + 1\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    # Ensure X is float64 for numerical stability\n    X = np.asarray(X, dtype=np.float64)\n    N, M = X.shape\n\n    # -------- initialisation -------------------------------------------------\n    rng = np.random.RandomState(0)          # deterministic seed\n    W = rng.randn(N, K)                     # (N, K)\n    H = rng.randn(K, M)                     # (K, M)\n\n    # Identity matrix used for the ridge terms\n    I_K = np.eye(K, dtype=np.float64)\n\n    def _solve(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n        \"\"\"Solve AX = B for X with fallback to pseudo-inverse if singular.\"\"\"\n        try:\n            return np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            return np.linalg.pinv(A) @ B\n\n    # --------------- main ALS loop ------------------------------------------\n    best_W, best_H = W.copy(), H.copy()\n    best_loss = np.inf\n\n    for _ in range(max_iter):\n        # ---- update W (fix H) ---------------------------------------------\n        HHT = H @ H.T + alpha * I_K            # (K, K)\n        # Solve (H H\u1d40 + \u03b1I) W\u1d40 = H X\u1d40   \u21d2   W = ( (H H\u1d40 + \u03b1I)\u207b\u00b9 H X\u1d40 )\u1d40\n        W = _solve(HHT, H @ X.T).T            # (N, K)\n\n        # ---- update H (fix W) ---------------------------------------------\n        WTW = W.T @ W + alpha * I_K           # (K, K)\n        # Solve (W\u1d40 W + \u03b1I) H = W\u1d40 X\n        H = _solve(WTW, W.T @ X)              # (K, M)\n\n        # ---- compute regularised loss -------------------------------------\n        diff = X - W @ H\n        loss = np.sum(diff ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n\n        # Keep the best factors seen so far (helps in non-converging cases)\n        if loss < best_loss:\n            best_loss = loss\n            best_W, best_H = W.copy(), H.copy()\n\n        # Termination criterion\n        if loss <= tol:\n            break\n\n    # ------------------- reconstruction -------------------------------------\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef _euclidean_squared(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Squared Euclidean distance between every row of `a` and `b` (broadcast).\"\"\"\n    # a : (m, n) , b : (k, n)\n    # returns (m, k)\n    return ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=2)\n\n\ndef _kmeans_pp_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Return K initial centroids picked with the K-Means++ heuristic.\"\"\"\n    m = X.shape[0]\n    # Choose the first centroid completely at random from the points.\n    centroids = [X[np.random.randint(m)]]\n\n    for _ in range(1, K):\n        # Compute squared distance to the nearest existing centroid\n        d2 = _euclidean_squared(X, np.array(centroids)).min(axis=1)\n        probs = d2 / d2.sum()\n\n        # Pick a new centroid with probability \u221d distance\u00b2\n        r = random.random()\n        cumulative = np.cumsum(probs)\n        idx = np.searchsorted(cumulative, r)\n        centroids.append(X[idx])\n\n    return np.array(centroids)\n\n\ndef kmeans(\n    X: np.ndarray,\n    K: int,\n    max_iter: int = 100,\n    random_state: int | None = None,\n) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    # ------------------------------------------------------------------ seeds\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    X = np.asarray(X, dtype=float)\n    m, n = X.shape\n\n    if K <= 0 or K > m:\n        raise ValueError(\"K must be a positive integer \u2264 number of samples.\")\n\n    # ---------------------------------------------------- K-Means++ seeding\n    centroids = _kmeans_pp_init(X, K)\n\n    # ------------------------------------------------------- Lloyd\u2019s loop\n    labels = np.full(m, -1, dtype=int)  # previous assignment\n    for _ in range(max_iter):\n        # ASSIGN step\n        distances = _euclidean_squared(X, centroids)  # shape (m, K)\n        new_labels = distances.argmin(axis=1)\n\n        # Convergence check\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # UPDATE step\n        for k in range(K):\n            mask = labels == k\n            if mask.any():  # non-empty cluster\n                centroids[k] = X[mask].mean(axis=0)\n            # else: keep centroid unchanged (specification)\n\n    # ----------------------------------------------------- Post-processing\n    centroids_rounded = [\n        [round(float(v), 4) for v in centroid] for centroid in centroids\n    ]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: (c[0], c))\n\n    return centroids_sorted\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20               # fixed length expected by the model\nPAD_TOKEN  = 0                # we reserve the id \u201c0\u201d for padding / \u201cunknown\u201d\n\n\ndef string_to_int(text: str,\n                  time_steps: int,\n                  vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode a raw string into a fixed-length list of integer ids.\n\n    \u2022 Each character is looked up in `vocabulary`.\n    \u2022 Unknown characters map to PAD_TOKEN (0).\n    \u2022 The returned list is exactly `time_steps` long:\n        \u2013 shorter input  \u2192 pad with 0 on the right\n        \u2013 longer  input  \u2192 truncate on the right\n    \"\"\"\n    # Encode (truncate if necessary)\n    encoded = [vocabulary.get(ch, PAD_TOKEN) for ch in text[:time_steps]]\n\n    # Pad up to the required length\n    if len(encoded) < time_steps:\n        encoded.extend([PAD_TOKEN] * (time_steps - len(encoded)))\n\n    return encoded\n\n\ndef int_to_string(indices,\n                  inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer ids back into a string.\n\n    \u2022 Works with Python lists, tuples or NumPy arrays.\n    \u2022 Padding / unknown tokens (id 0) are ignored.\n    \"\"\"\n    # Flatten in case a batched array comes in\n    indices = np.ravel(indices)\n\n    chars = []\n    for idx in indices:\n        if idx == PAD_TOKEN:          # skip padding\n            continue\n        char = inverse_vocab.get(idx)\n        if char is not None:          # skip truly unknown ids\n            chars.append(char)\n\n    return \"\".join(chars)\n\n\ndef run_example(model,\n                input_vocabulary: dict[str, int],\n                inv_output_vocabulary: dict[int, str],\n                text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n    1. encode text\n    2. feed the single example to `model.predict`\n    3. take arg-max over the last axis\n    4. decode the predicted ids back to text\n    \"\"\"\n    # 1. Encode ------------------------------------------------------------\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch   = np.array(encoded, dtype=np.int64)[np.newaxis, :]   # shape (1, TIME_STEPS)\n\n    # 2. Model prediction --------------------------------------------------\n    logits = model.predict(batch)        # expected shape (1, TIME_STEPS, vocab_size)\n\n    # 3. Convert logits to ids --------------------------------------------\n    pred_ids = np.argmax(logits, axis=-1)   # shape (1, TIME_STEPS)\n\n    # 4. Decode ------------------------------------------------------------\n    decoded = int_to_string(pred_ids[0], inv_output_vocabulary)\n\n    return decoded\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    \n    # 1. Extract contiguous alphabetic words, ignoring everything else.\n    #    re.IGNORECASE is unnecessary because we lowercase afterwards.\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:                         # 5. Return empty list when no words.\n        return []\n\n    # 2. Case-insensitive comparison \u2192 lowercase all words.\n    words = [w.lower() for w in words]\n\n    # 3. Count frequencies.\n    freq = Counter(words)\n\n    # 4. Sort by: descending frequency, then lexicographically.\n    sorted_words = sorted(freq.items(), key=lambda item: (-item[1], item[0]))\n\n    # Build Token objects in that order.\n    tokens: list[Token] = []\n    for word, count in sorted_words:\n        tok = Token(word)\n        tok.count = count\n        tokens.append(tok)\n\n    return tokens\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str] or None\n        A collection of input strings.  If *None*, the function uses the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Fall back to a global constant if `examples` was not supplied\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError(\n                \"No `examples` provided and the global constant `EXAMPLES` \"\n                \"is not defined.\"\n            )\n\n    predictions = []\n\n    # Iterate over each example, run the model, and collect / print the output\n    for example in examples:\n        # run_example is assumed to exist in the runtime\n        output_chars = run_example(\n            model,\n            input_vocabulary,\n            inv_output_vocabulary,\n            example,\n        )\n\n        # `run_example` is specified to return a list of characters;\n        # join them into a single string.\n        predicted_str = \"\".join(output_chars)\n\n        # Display the input / output pair\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n\n        predictions.append(predicted_str)\n\n    return predictions\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray (or array-like) containing real values.\n        derivative: If False (default) returns SELU(x);\n                    if True  returns d(SELU)/dx.\n\n    Returns:\n        A (nested) Python list with the same shape as *x*, with every value\n        rounded to six decimal places.\n    \"\"\"\n    # SELU constants from the original paper\n    alpha = 1.6732632423543772848170429916717\n    lam   = 1.0507009873554804934193349852946\n    \n    x = np.asarray(x, dtype=float)          # Ensure NumPy array\n    \n    if derivative:                          # Gradient\n        out = np.where(x >= 0,\n                       lam,                 # \u03bb for x \u2265 0\n                       lam * alpha * np.exp(x))   # \u03bb \u03b1 e^x for x < 0\n    else:                                   # Activation\n        out = np.where(x >= 0,\n                       lam * x,             # \u03bb x for x \u2265 0\n                       lam * alpha * (np.exp(x) - 1))  # \u03bb \u03b1 (e^x \u2212 1) for x < 0\n    \n    return np.round(out, 6).tolist()\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(\n        X: np.ndarray,\n        y: np.ndarray,\n        w: np.ndarray\n    ) -> tuple[float, list[list[float]]]:\n    \"\"\"Binary cross-entropy loss and gradient for logistic regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n)\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1)\n    w : np.ndarray\n        Weight vector of shape (n,) or (n, 1)\n    \n    Returns\n    -------\n    tuple\n        (loss, gradient) where\n          \u2022 loss is a float rounded to 4 decimals\n          \u2022 gradient is a nested list (shape (n, 1)) whose\n            elements are rounded to 4 decimals\n    \"\"\"\n    # Ensure correct shapes\n    y = y.reshape(-1)                  # (m,)\n    w = w.reshape(-1)                  # (n,)\n    m = X.shape[0]\n\n    # Forward pass \u2013 sigmoid predictions\n    z = X @ w                          # (m,)\n    p = 1 / (1 + np.exp(-z))           # (m,)\n    \n    # Numerical stability\n    eps = 1e-20\n    p = np.clip(p, eps, 1 - eps)\n\n    # Binary cross-entropy loss\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    loss = float(np.round(loss, 4))\n\n    # Gradient\n    grad = (X.T @ (p - y)) / m         # (n,)\n    grad = grad.reshape(-1, 1)         # (n, 1) column-vector\n    grad = np.round(grad, 4).tolist()  # nested Python list\n\n    return loss, grad\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Reduction in mean\u2013squared error obtained by a split.\n\n    Args:\n        y: 1-D array of original target values.\n        splits: List with one 1-D array per child node after the split.\n\n    Returns:\n        Parent MSE minus weighted child MSE, rounded to 4 decimals.\n    \"\"\"\n    # Guard against degenerate input\n    if y.size == 0 or not splits:\n        return 0.0\n\n    # --- Parent MSE ----------------------------------------------------------\n    y_mean = y.mean()\n    mse_parent = np.mean((y - y_mean) ** 2)\n\n    # --- Children MSE --------------------------------------------------------\n    N = y.size\n    mse_children = 0.0\n    for child in splits:\n        n_child = child.size\n        if n_child == 0:          # ignore empty child nodes\n            continue\n        child_mean = child.mean()\n        child_mse = np.mean((child - child_mean) ** 2)\n        mse_children += (n_child / N) * child_mse\n\n    # --- Criterion value -----------------------------------------------------\n    delta = mse_parent - mse_children\n    return round(float(delta), 4)\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    n_clf: int = 5\n) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array (m, n) with train features.\n        y_train: 1-D NumPy array (m,) with labels **-1** or **1**.\n        X_test : 2-D NumPy array (k, n) with test features.\n        n_clf  : Number of weak classifiers (\u22651).\n\n    Returns\n        Python list (length k) with predicted labels (-1 or 1).\n    \"\"\"\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _best_stump(X, y, w):\n        \"\"\"Find the decision stump that minimises weighted error.\"\"\"\n        m, n_features = X.shape\n        best_err = np.inf\n        stump = {}          # feature, thresh, polarity\n        best_pred = None\n\n        for j in range(n_features):\n            Xj = X[:, j]\n            uniq = np.unique(Xj)\n\n            # candidate thresholds: mid-points + two extremes\n            if uniq.size == 1:\n                thresholds = [uniq[0] - 1e-10, uniq[0] + 1e-10]\n            else:\n                thresholds = list((uniq[:-1] + uniq[1:]) / 2.0)\n                thresholds.append(uniq[0] - 1e-10)\n                thresholds.append(uniq[-1] + 1e-10)\n\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    # prediction rule\n                    preds = np.ones(m, dtype=int)\n                    if polarity == 1:\n                        preds[Xj < thresh] = -1\n                    else:                       # polarity == -1\n                        preds[Xj < thresh] = 1\n\n                    err = np.sum(w[preds != y])\n\n                    if err < best_err:\n                        best_err = err\n                        stump = {'feature': j,\n                                 'threshold': thresh,\n                                 'polarity': polarity}\n                        best_pred = preds\n\n        return stump, best_err, best_pred\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  parameter checks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if n_clf < 1:\n        n_clf = 1\n    y_train = y_train.astype(int)\n    m = X_train.shape[0]\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  AdaBoost training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    weights = np.full(m, 1 / m, dtype=float)\n    stumps = []\n\n    eps = 1e-10  # to avoid division by zero / log(0)\n\n    for _ in range(n_clf):\n        stump, err, preds = _best_stump(X_train, y_train, weights)\n\n        # safeguard for perfect or very bad stumps\n        err = np.clip(err, eps, 1 - eps)\n        alpha = 0.5 * np.log((1 - err) / err)\n\n        # update sample weights\n        weights *= np.exp(-alpha * y_train * preds)\n        weights /= weights.sum()\n\n        stump['alpha'] = alpha\n        stumps.append(stump)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  prediction on X_test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n\n    for stump in stumps:\n        j = stump['feature']\n        thresh = stump['threshold']\n        pol = stump['polarity']\n        pred = np.ones(k, dtype=int)\n        if pol == 1:\n            pred[X_test[:, j] < thresh] = -1\n        else:\n            pred[X_test[:, j] < thresh] = 1\n        agg += stump['alpha'] * pred\n\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1  # tie-break\n\n    return final_pred.astype(int).tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Corresponding gradients.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        A list of updated parameters converted to native Python lists, with all\n        values rounded to four decimal places.  If the inputs are invalid\n        (empty or of different lengths) an empty list is returned.\n    \"\"\"\n    \n    # Basic validation\n    if not params or not grads or len(params) != len(grads):\n        return []\n    \n    updated_params = []\n    \n    for p, g in zip(params, grads):\n        # Ensure NumPy array conversion (without touching the originals)\n        p_arr = np.asarray(p, dtype=float)\n        g_arr = np.asarray(g, dtype=float)\n        \n        # SGD update rule\n        new_p = p_arr - alpha * g_arr\n        \n        # Round to 4 decimals and convert to Python list\n        updated_params.append(np.round(new_p, 4).tolist())\n    \n    return updated_params\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef linear_regression(X: list[list[int | float]],\n                      y: list[int | float],\n                      n_iterations: int,\n                      learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\n\n    Args:\n        X: Training samples \u2013 list of lists with numerical feature values.\n        y: Target values \u2013 list of numbers, same length as X.\n        n_iterations: Number of gradient-descent steps to perform.\n        learning_rate: Positive learning rate controlling the step size.\n\n    Returns:\n        The learned weight vector as a list (bias first) rounded to 4 decimals.\n        If the input dimensions are incompatible the function returns -1.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1)  Basic input checks\n    # ------------------------------------------------------------------ #\n    if len(X) == 0 or len(X) != len(y):\n        return -1                          # incompatible dimensions\n\n    # ------------------------------------------------------------------ #\n    # 2)  Convert to NumPy arrays and add bias column\n    # ------------------------------------------------------------------ #\n    X_np = np.asarray(X, dtype=float)\n    if X_np.ndim == 1:                     # handle 1-D (single feature) input\n        X_np = X_np.reshape(-1, 1)\n\n    m = X_np.shape[0]                      # number of training samples\n    ones = np.ones((m, 1))\n    X_aug = np.hstack((ones, X_np))        # insert bias column as first column\n\n    y_np = np.asarray(y, dtype=float).reshape(-1, 1)\n\n    # ------------------------------------------------------------------ #\n    # 3)  Weight initialisation  w \u2208 [\u22121/\u221aN , 1/\u221aN]\n    # ------------------------------------------------------------------ #\n    N = X_aug.shape[1]                     # augmented feature count\n    limit = 1 / math.sqrt(N)\n    w = np.random.uniform(-limit, limit, size=(N, 1))\n\n    # ------------------------------------------------------------------ #\n    # 4)  Batch gradient-descent iterations\n    # ------------------------------------------------------------------ #\n    lr = float(learning_rate)\n    for _ in range(max(0, int(n_iterations))):\n        preds = X_aug @ w                  # m \u00d7 1\n        errors = preds - y_np              # m \u00d7 1\n        gradient = (X_aug.T @ errors) / m  # N \u00d7 1\n        w -= lr * gradient                 # weight update\n\n    # ------------------------------------------------------------------ #\n    # 5)  Return rounded weights (bias first) as Python list\n    # ------------------------------------------------------------------ #\n    w_rounded = np.round(w.flatten(), 4)\n    return w_rounded.tolist()\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_weights(\n    X: list[list[int | float]],\n    y: list[int | float]\n) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, \u2026, wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    # 1. Convert to NumPy arrays\n    X_np = np.asarray(X, dtype=float)\n    y_np = np.asarray(y, dtype=float).ravel()   # ensure shape (n_samples,)\n\n    if X_np.shape[0] != y_np.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n\n    # 2. Augment X with a column of ones for the intercept\n    ones = np.ones((X_np.shape[0], 1), dtype=float)\n    X_aug = np.hstack([ones, X_np])            # shape: (n_samples, d+1)\n\n    # 3. Compute optimal weights using Moore-Penrose pseudo-inverse\n    w = np.linalg.pinv(X_aug) @ y_np           # shape: (d+1,)\n\n    # 4. Round to 4 decimal places and return as a list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\n\n    The distinct categories are detected automatically, sorted, and each\n    element of *y* is converted to a binary vector that has length equal to\n    the number of unique categories.\n\n    Args:\n        y: A one-dimensional NumPy array (or array-like) of integers or\n           strings representing categorical data.\n\n    Returns:\n        A list of lists containing 0/1 integers \u2013 the one-hot encoded matrix.\n    \"\"\"\n    # Convert input to a NumPy array and ensure it is one-dimensional\n    arr = np.asarray(y)\n    if arr.ndim != 1:\n        raise ValueError(\"Input must be a one-dimensional array or list.\")\n\n    # Detect and sort the unique categories\n    categories = sorted(set(arr.tolist()))\n    n_classes = len(categories)\n\n    # Map each category to its column index\n    cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n\n    # Build the one-hot matrix\n    one_hot = [[0] * n_classes for _ in range(len(arr))]\n    for row_idx, value in enumerate(arr):\n        col_idx = cat2idx[value]\n        one_hot[row_idx][col_idx] = 1\n\n    return one_hot\n```\n\nExample\n-------\n```python\n>>> y = np.array(['dog', 'cat', 'dog', 'bird'])\n>>> one_hot_encoding(y)\n[[0, 0, 1],   # 'bird', 'cat', 'dog'  (alphabetical order)\n [0, 1, 0],\n [0, 0, 1],\n [1, 0, 0]]\n```\n\nThe function never relies on external ML libraries, returns plain Python lists, and ensures the columns follow the sorted order of the unique categories."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n    \n    Args\n    ----\n    X : list of list of numbers\n        First data matrix with shape (N, C).\n    Y : list of list of numbers or None, optional\n        Second data matrix with shape (M, C).  If *None* defaults to *X*.\n    d : int, default=3\n        Degree of the polynomial.\n    gamma : float or None, default=None\n        Scale factor.  Uses 1/C when *None*.\n    c0 : float, default=1\n        Bias (independent) term.\n    \n    Returns\n    -------\n    list of list of floats\n        The (N\u00d7M) Gram matrix rounded to 4 decimals.\n    \"\"\"\n    \n    # Convert inputs to NumPy arrays (float64 for numerical safety)\n    X_arr = np.asarray(X, dtype=np.float64)\n    Y_arr = X_arr if Y is None else np.asarray(Y, dtype=np.float64)\n    \n    # Infer feature dimension, validate compatibility\n    if X_arr.ndim != 2 or Y_arr.ndim != 2:\n        raise ValueError(\"X and Y must be 2-D matrices.\")\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        raise ValueError(\"X and Y must have the same number of columns/features.\")\n    \n    _, C = X_arr.shape\n    \n    # Default gamma = 1 / C\n    if gamma is None:\n        gamma = 1.0 / C\n    \n    # Core computation\n    #   K = (gamma * X Y^T + c0) ** d\n    dot_products = X_arr @ Y_arr.T                         # (N, M)\n    base = gamma * dot_products + c0\n    K = np.power(base, d)\n    \n    # Round to 4 decimals and return as nested Python list\n    K_rounded = np.round(K, 4)\n    return K_rounded.tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances whose true class equals the\n        *i-th* sorted unique label and whose predicted class equals the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1) Sanity-check the input lengths\n    # ------------------------------------------------------------------\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # If both sequences are empty, return an empty matrix\n    if len(y_true) == 0:\n        return []\n\n    # ------------------------------------------------------------------\n    # 2) Determine the set of all classes and keep them sorted\n    # ------------------------------------------------------------------\n    classes = sorted(set(y_true).union(y_pred))\n    c2idx = {cls: idx for idx, cls in enumerate(classes)}  # class \u2192 row/col\n\n    C = len(classes)\n\n    # ------------------------------------------------------------------\n    # 3) Allocate an empty C\u00d7C matrix filled with zeros\n    # ------------------------------------------------------------------\n    M = [[0 for _ in range(C)] for _ in range(C)]\n\n    # ------------------------------------------------------------------\n    # 4) Populate the matrix\n    # ------------------------------------------------------------------\n    for t, p in zip(y_true, y_pred):\n        i = c2idx[t]         # row index  (true label)\n        j = c2idx[p]         # column index (predicted label)\n        M[i][j] += 1\n\n    return M\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher LDA and project the data.\"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Quick exit\n    # ------------------------------------------------------------------ #\n    if n_components == 0:\n        return []\n\n    n_samples, n_features = X.shape\n    classes            = np.unique(y)\n    n_classes          = classes.size\n\n    # ------------------------------------------------------------------ #\n    # 1. Class means, global mean, scatters\n    # ------------------------------------------------------------------ #\n    global_mean = X.mean(axis=0)\n\n    # initialise scatter matrices\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros_like(S_W)\n\n    for cls in classes:\n        X_c      = X[y == cls]\n        n_c      = X_c.shape[0]\n        mean_c   = X_c.mean(axis=0)\n\n        # within\u2013class scatter\n        diff     = X_c - mean_c\n        S_W     += diff.T @ diff\n\n        # between\u2013class scatter\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B      += n_c * (mean_diff @ mean_diff.T)\n\n    # ------------------------------------------------------------------ #\n    # 2. Solve  pinv(S_W) S_B  eigen-problem\n    # ------------------------------------------------------------------ #\n    A = np.linalg.pinv(S_W) @ S_B\n\n    # A is (theoretically) symmetric \u2013 use eigh\n    eig_vals, eig_vecs = np.linalg.eigh(A)\n\n    # ------------------------------------------------------------------ #\n    # 3. Sort eigen-pairs (descending eigen-values)\n    # ------------------------------------------------------------------ #\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vecs   = eig_vecs[:, sorted_idx]          # columns are eigenvectors\n\n    # keep only the requested components\n    eig_vecs = eig_vecs[:, :n_components]\n\n    # ------------------------------------------------------------------ #\n    # 4. Deterministic sign flipping\n    # ------------------------------------------------------------------ #\n    for i in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, i]\n        # first non-zero element decides the sign\n        for v in vec:\n            if np.abs(v) > 1e-12:      # treat as non-zero\n                if v < 0:\n                    eig_vecs[:, i] = -vec\n                break\n\n    # ------------------------------------------------------------------ #\n    # 5. Project data and round\n    # ------------------------------------------------------------------ #\n    projected = X @ eig_vecs                # shape (n_samples, n_components)\n    projected = np.round(projected, 4)\n\n    return projected.tolist()\n```"}
{"task_id": 129, "completion_id": 0, "completion": "```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list | tuple, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    # Guard against empty sequences or non-positive sample requests\n    if not sequence or samples <= 0:\n        return []\n\n    # Take the first `samples` elements from an infinite cycle over `sequence`\n    return list(islice(cycle(sequence), samples))\n```"}
{"task_id": 130, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities are rounded to 4 decimal places.  If the\n    pair never occurred, an empty list is returned.\n    \"\"\"\n    # Keep only the transitions that match the queried (state, action)\n    relevant_next_states = [\n        next_state for s, a, _r, next_state in transitions\n        if s == state and a == action\n    ]\n\n    # If the pair never occurred, return empty list\n    if not relevant_next_states:\n        return []\n\n    # Count occurrences of every next_state\n    counts = Counter(relevant_next_states)\n    total = len(relevant_next_states)\n\n    # Build the probability list, rounded to 4 decimals, sorted by next_state\n    result = [\n        [ns, round(cnt / total, 4)]\n        for ns, cnt in sorted(counts.items(), key=lambda item: item[0])\n    ]\n\n    return result\n```"}
{"task_id": 131, "completion_id": 0, "completion": "```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # 1. Check that `uid` is a valid index for the shared list\n    try:\n        seq = _SHARED_SEQUENCES[uid]\n    except (IndexError, TypeError):\n        # `uid` is out of bounds or not an integer-like value\n        return None\n\n    # 2. Resolve the element index `i` within the selected sequence\n    seq_len = len(seq)\n\n    # Translate negative indices the same way Python's list does\n    if i < 0:\n        i += seq_len\n\n    # 3. Verify that the (possibly adjusted) index is within the sequence bounds\n    if i < 0 or i >= seq_len:\n        return None\n\n    # 4. Return the element without modifying the original sequence\n    return seq[i]\n```"}
{"task_id": 132, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    # 1. If z is a scalar (Python or NumPy), return it untouched.\n    if np.isscalar(z):\n        return z\n\n    # 2. If z is already a NumPy ndarray, return the *same* object (no copy).\n    if isinstance(z, np.ndarray):\n        return z\n\n    # 3. If z is a list/tuple (any nesting depth), convert it to an ndarray.\n    if isinstance(z, (list, tuple)):\n        return np.asarray(z)\n\n    # 4. Fallback: try to convert any remaining iterable/numeric object\n    #    into an ndarray; otherwise, let NumPy raise an informative error.\n    return np.asarray(z)\n```"}
{"task_id": 133, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    # Basic sanity check for the vocabulary size\n    if not isinstance(vocab_length, int) or vocab_length <= 0:\n        return -1\n\n    # Fast one-hot lookup table: identity matrix of size vocab_length\n    eye = np.eye(vocab_length, dtype=int)\n\n    Y_train: list[list[list[int]]] = []\n\n    for sequence in X_train:\n        y_seq: list[list[int]] = []\n        for token in sequence:\n            # Validate the token\n            if not isinstance(token, (int, np.integer)) or token < 0 or token >= vocab_length:\n                return -1\n            # Append the one-hot vector (convert NumPy row to plain list)\n            y_seq.append(eye[token].tolist())\n        Y_train.append(y_seq)\n\n    return Y_train\n```"}
{"task_id": 134, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        Success probabilities for each arm.  Each probability must satisfy\n        0.0 \u2264 p \u2264 1.0.\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid entry, returns (-1.0, -1).\n    \"\"\"\n    # -------------------- 1.  Basic input validation -------------------- #\n    if not payoff_probs:                       # Empty list\n        return (-1.0, -1)\n\n    # -------------------- 2.  Scan and keep best arm -------------------- #\n    best_prob  = -1.0          # Highest probability found so far\n    best_index = -1            # Corresponding arm index\n\n    for idx, p in enumerate(payoff_probs):\n        # Type / range check\n        try:\n            prob = float(p)\n        except (TypeError, ValueError):\n            return (-1.0, -1)\n\n        # Reject NaN or values outside [0, 1]\n        if not (0.0 <= prob <= 1.0):\n            return (-1.0, -1)\n\n        # Update best arm (strict > keeps the first \u2012 i.e., smallest index \u2012\n        # when there is a tie)\n        if prob > best_prob:\n            best_prob, best_index = prob, idx\n\n    return (best_prob, best_index)\n```"}
{"task_id": 135, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_predict(\n        X_train: list[list[int | float]],\n        y_train: list[int | float],\n        X_test: list[list[int | float]]) -> list[float] | int:\n    \"\"\"\n    Fits an Ordinary Least Squares (OLS) linear-regression model via the normal\n    equation and returns predictions for X_test rounded to 4 decimal places.\n\n    If (X\u1d40X) is singular (non-invertible) the function returns -1.\n    \"\"\"\n    # -------------------- 1. Convert inputs to NumPy -------------------------\n    X_tr = np.asarray(X_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=float).reshape(-1, 1)   # column vector\n    X_te = np.asarray(X_test,  dtype=float)\n\n    # -------------------- 2. Add intercept column (bias) ---------------------\n    ones_tr = np.ones((X_tr.shape[0], 1))\n    ones_te = np.ones((X_te.shape[0], 1))\n\n    X_tr_aug = np.hstack((ones_tr, X_tr))   # shape: (m, n+1)\n    X_te_aug = np.hstack((ones_te, X_te))   # shape: (k, n+1)\n\n    # -------------------- 3. Normal-equation components ----------------------\n    XtX = X_tr_aug.T @ X_tr_aug             # (n+1, n+1)\n    Xty = X_tr_aug.T @ y_tr                 # (n+1, 1)\n\n    # -------------------- 4. Check singularity of XtX ------------------------\n    if np.linalg.matrix_rank(XtX) < XtX.shape[0]:\n        return -1       # Singular; cannot invert\n\n    # -------------------- 5. Solve for coefficients --------------------------\n    beta = np.linalg.inv(XtX) @ Xty         # (n+1, 1)\n\n    # -------------------- 6. Predict on test set -----------------------------\n    preds = X_te_aug @ beta                 # (k, 1)\n    preds = np.round(preds.flatten(), 4)    # 1-D array, 4 decimals\n\n    # -------------------- 7. Return Python list ------------------------------\n    return preds.tolist()\n```"}
{"task_id": 136, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    Steps\n    -----\n    1. Shift by the maximum value along *axis* to prevent overflow.\n    2. Exponentiate the shifted values.\n    3. Normalize by the sum of exponentials along the same axis.\n    4. Round to four decimal places.\n    5. Return as a regular Python list.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array of raw scores.\n    axis : int, optional\n        Axis along which softmax is computed (default: -1).\n\n    Returns\n    -------\n    list\n        Softmax probabilities (same shape as *z*), rounded to four decimals.\n    \"\"\"\n    # Ensure input is a NumPy array\n    z = np.asarray(z)\n\n    # 1. Shift by max for numerical stability\n    z_max = np.max(z, axis=axis, keepdims=True)\n    z_shifted = z - z_max\n\n    # 2. Exponentiate\n    exp_z = np.exp(z_shifted)\n\n    # 3. Normalize\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    softmax_vals = exp_z / sum_exp\n\n    # 4. Round to four decimal places\n    softmax_vals = np.round(softmax_vals, 4)\n\n    # 5. Convert to Python list and return\n    return softmax_vals.tolist()\n```"}
{"task_id": 138, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    # Validate lengths\n    if len(y_true) == 0 or len(y_true) != len(y_pred):\n        return -1\n\n    # Count matching positions\n    matches = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n\n    # Compute and round accuracy\n    accuracy = matches / len(y_true)\n    return round(accuracy, 4)\n```"}
{"task_id": 139, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_activation(z):\n    \"\"\"Compute the hyperbolic tangent (tanh) for every element of the input.\n\n    Args:\n        z (list[int | float] | np.ndarray): A one-dimensional iterable of\n            numeric values.\n\n    Returns:\n        list[float]: Tanh applied element-wise, rounded to four decimals.\n    \"\"\"\n    # Convert the input to a 1-D NumPy array of floats\n    arr = np.asarray(list(z), dtype=float)\n\n    # Handle empty input early\n    if arr.size == 0:\n        return []\n\n    # Numerically stable tanh:   tanh(z) = 2 / (1 + exp(-2z)) - 1\n    tanh_vals = 2.0 / (1.0 + np.exp(-2.0 * arr)) - 1.0\n\n    # Round to four decimal places and return as a Python list\n    return np.round(tanh_vals, 4).tolist()\n```"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\n\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes required for the agent to reach\n    the lower-right corner of a square maze that contains spreading fire.\n\n    The fire spreads first every minute, then the agent moves.  The agent may\n    enter a cell only if that cell is not burning at the moment he arrives.\n\n    Parameters\n    ----------\n    grid : list[str]\n        An n\u00d7n list of strings consisting only of '.', '#', 'F'.\n\n    Returns\n    -------\n    int\n        The minimum time to reach the goal, or \u20111 if it is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n\n    # ------------------------------------------------------------------\n    # 1.  Multi\u2013source BFS \u2013 when does every cell catch fire?\n    # ------------------------------------------------------------------\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q_fire = deque()\n\n    for r in range(n):\n        for c, ch in enumerate(grid[r]):\n            if ch == 'F':\n                fire_time[r][c] = 0\n                q_fire.append((r, c))\n\n    dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))\n\n    while q_fire:\n        r, c = q_fire.popleft()\n        for dr, dc in dirs:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < n and 0 <= nc < n and grid[nr][nc] != '#':\n                if fire_time[nr][nc] == INF:\n                    fire_time[nr][nc] = fire_time[r][c] + 1\n                    q_fire.append((nr, nc))\n\n    # ------------------------------------------------------------------\n    # 2.  Basic impossibility checks\n    # ------------------------------------------------------------------\n    def burning_at_start(cell_r, cell_c) -> bool:\n        \"\"\"True if that cell is already burning at t = 0.\"\"\"\n        return fire_time[cell_r][cell_c] == 0\n\n    # walls, or burning instantly\n    if (\n        grid[0][0] == '#' or grid[n - 1][n - 1] == '#' or\n        burning_at_start(0, 0) or burning_at_start(n - 1, n - 1)\n    ):\n        return -1\n\n    # 1\u00d71 maze that is free and not burning\n    if n == 1:\n        return 0\n\n    # ------------------------------------------------------------------\n    # 3.  BFS for the agent, respecting fire-arrival times\n    # ------------------------------------------------------------------\n    q = deque([(0, 0, 0)])               # (row, col, time-of-arrival)\n    best_arrival = [[INF] * n for _ in range(n)]\n    best_arrival[0][0] = 0\n\n    while q:\n        r, c, t = q.popleft()\n\n        for dr, dc in dirs:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < n and 0 <= nc < n and grid[nr][nc] != '#':\n                next_t = t + 1\n\n                # Fire spreads FIRST, then we move \u2192 cell must start burning\n                # strictly AFTER the agent arrives.\n                if next_t >= fire_time[nr][nc]:\n                    continue\n\n                if next_t < best_arrival[nr][nc]:\n                    best_arrival[nr][nc] = next_t\n                    if nr == n - 1 and nc == n - 1:       # reached goal\n                        return next_t\n                    q.append((nr, nc, next_t))\n\n    return -1\n```\n\nExplanation of the algorithm\n----------------------------\n1. Fire time table  \n   \u2022 Treat every \u2018F\u2019 cell as a source at minute 0.  \n   \u2022 Multi-source BFS (four directions, ignoring walls `#`) gives `fire_time[r][c]` = the earliest minute that cell starts burning (`\u221e` if it never burns).\n\n2. Early rejection  \n   \u2022 If start or goal is a wall or already burning at \\(t=0\\) \u2192 return -1.  \n   \u2022 For a \\(1\\times1\\) clear, safe cell the answer is 0.\n\n3. Agent BFS  \n   \u2022 Queue stores `(row, col, minute-of-arrival)`.  \n   \u2022 From a position reached at time `t` the agent can enter a neighbour at `t+1`.  \n   \u2022 Because the fire spreads first each minute, the neighbour must satisfy  \n     `t + 1  <  fire_time[neighbour]`.  \n   \u2022 Standard BFS (shortest paths in an un-weighted graph) with this extra\n     constraint yields the minimum travel time.  \n   \u2022 If the goal is dequeued, that time is the answer; if the queue exhausts,\n     the goal is unreachable \u2192 return -1.\n\nComplexities  \n\u2022 Fire BFS: \\(O(n^2)\\).  \n\u2022 Agent BFS: \\(O(n^2)\\).  \n\u2022 Memory: \\(O(n^2)\\)."}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    k\u2013Nearest Neighbours classifier.\n    \n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training samples.\n    y_train : (n_train,) ndarray\n        Labels of the training samples.\n    X_test  : (n_test,  n_features) ndarray\n        Samples to classify.\n    k       : int\n        Number of neighbours to use (if k > n_train, k == n_train).\n    metric  : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Anything else falls back to 'euclidean'.\n    \n    Returns\n    -------\n    preds : (n_test,) ndarray\n        Predicted labels for X_test.\n    \"\"\"\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # ------------------------------------------------------------------\n    # Distance matrix D  shape: (n_test, n_train)\n    # ------------------------------------------------------------------\n    metric = metric.lower()\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n\n    if metric == 'manhattan':\n        # \u21131 distance\n        D = np.abs(X_test[:, None, :] - X_train[None, :, :]).sum(axis=2)\n\n    elif metric == 'cosine':\n        # cosine distance = 1 - cosine similarity\n        eps = 1e-10                                  # avoid division by zero\n        t_norm = np.linalg.norm(X_test,  axis=1, keepdims=True) + eps\n        r_norm = np.linalg.norm(X_train, axis=1)     + eps          # (n_train,)\n        sim = (X_test @ X_train.T) / (t_norm * r_norm)              # broadcasting\n        D = 1.0 - sim\n\n    else:  # default / 'euclidean'\n        diff = X_test[:, None, :] - X_train[None, :, :]\n        D = np.sum(diff ** 2, axis=2)                # squared \u21132 is enough for ranking\n\n    # ------------------------------------------------------------------\n    # k nearest neighbours' indices for every test point\n    # ------------------------------------------------------------------\n    nn_idx = np.argpartition(D, kth=k-1, axis=1)[:, :k]  # (n_test, k)\n\n    # ------------------------------------------------------------------\n    # Majority vote\n    # ------------------------------------------------------------------\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n\n    for i, neighbours in enumerate(nn_idx):\n        labels = y_train[neighbours]\n        uniq, counts = np.unique(labels, return_counts=True)\n        majority = uniq[counts == counts.max()]      # handle ties\n        preds[i] = majority.min()                    # smallest label wins in a tie\n\n    return preds\n```"}
{"task_id": 143, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    # Convert input to a NumPy array (no copy if already an ndarray)\n    x = np.asarray(x)\n\n    if derivative:\n        # f'(x) is 1 for x >= 0, else alpha\n        out = np.where(x >= 0, 1.0, alpha)\n    else:\n        # f(x) is x for x >= 0, else alpha * x\n        out = np.where(x >= 0, x, alpha * x)\n\n    return out\n```"}
{"task_id": 144, "completion_id": 0, "completion": "Here is a reference implementation that follows all the requirements:\n\n```python\nimport numpy as np\n\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise \u20111.\n    \"\"\"\n    # Convert inputs to NumPy arrays (will raise if non-numeric)\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1  # Non-numeric data or other conversion failure\n\n    # Check shape/length equality\n    if a.shape != p.shape or a.size == 0:\n        return -1\n\n    # Compute MAE with NumPy\n    mae = np.mean(np.abs(a - p))\n\n    # Round to 4 decimal places and return as float\n    return float(np.round(mae, 4))\n```\n\nUsage example:\n```python\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_absolute_error(y_true, y_pred))  # 0.5\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    \n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if len(X) != len(y):\n        raise ValueError('X and y must contain the same number of samples')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    \n    # ------------------------------------------------------------------ #\n    # 1. distance matrix: shape = (n_test, n_train)                       #\n    # ------------------------------------------------------------------ #\n    metric = metric.lower()\n    n_train = X.shape[0]\n    n_test  = X_test.shape[0]\n    k = min(k, n_train)                       # use all samples if k > n_train\n    \n    if metric == 'euclidean':\n        # ||x - x\u0302||\u2082\n        diff   = X_test[:, None, :] - X[None, :, :]      # (n_test, n_train, n_feat)\n        dists  = np.linalg.norm(diff, axis=2)            # (n_test, n_train)\n        \n    elif metric == 'manhattan':\n        # \u2211 |x - x\u0302|\n        diff   = X_test[:, None, :] - X[None, :, :]\n        dists  = np.abs(diff).sum(axis=2)\n        \n    elif metric == 'cosine':\n        # 1 - (x\u00b7x\u0302)/(\u2016x\u2016\u2016x\u0302\u2016)\n        eps      = 1e-12\n        x_norm   = np.linalg.norm(X,      axis=1) + eps  # (n_train,)\n        xt_norm  = np.linalg.norm(X_test, axis=1) + eps  # (n_test,)\n        dot_prod = X_test @ X.T                          # (n_test, n_train)\n        denom    = np.outer(xt_norm, x_norm)             # (n_test, n_train)\n        dists    = 1.0 - dot_prod / denom\n    else:\n        raise ValueError(\"metric must be 'euclidean', 'manhattan', or 'cosine'\")\n    \n    # ------------------------------------------------------------------ #\n    # 2. choose k nearest, 3. majority vote with smallest-label tie break #\n    # ------------------------------------------------------------------ #\n    predictions = []\n    for i in range(n_test):\n        # indices of k smallest distances\n        idx     = np.argpartition(dists[i], k-1)[:k]     # O(n)\n        neigh_y = y[idx]\n        \n        # count occurrences\n        unique, counts = np.unique(neigh_y, return_counts=True)\n        max_count      = counts.max()\n        \n        # labels with the highest vote\n        winners = unique[counts == max_count]\n        pred    = winners.min()          # smallest label in case of tie\n        predictions.append(pred)\n    \n    # ------------------------------------------------------------------ #\n    return predictions\n```\n\nThe function\n\n1. Computes all pairwise distances between test and training samples using the chosen metric (Euclidean, Manhattan or Cosine; the latter is stabilised with \u03b5 = 1 \u00d7 10\u207b\u00b9\u00b2).\n2. For every test point selects the `k` nearest neighbours (or all training samples if `k` is larger).\n3. Performs a majority vote on neighbour labels; if several labels share the highest count, the smallest label is chosen.\n4. Returns the predictions as a plain Python list."}
{"task_id": 147, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically\u2013stable sigmoid.\"\"\"\n    #  Too-large negative values overflow in exp(\u00b7); clipping is enough here.\n    z = np.clip(z, -500, 500)\n    return 1.0 / (1.0 + np.exp(-z))\n\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10_000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for X_test.\n    See task description for full documentation.\n    \"\"\"\n    # \u2500\u2500 Safety checks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        raise ValueError(\"X_train and X_test must be 2-D arrays.\")\n    if X_train.shape[1] != X_test.shape[1]:\n        raise ValueError(\"Train and test must have the same number of features.\")\n    if set(np.unique(y_train)) - {0, 1}:\n        raise ValueError(\"y_train must contain only 0 and 1.\")\n\n    # \u2500\u2500 Special case: only one class present \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * len(X_test)\n\n    n_samples, n_features = X_train.shape\n\n    # \u2500\u2500 Parameter initialisation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n\n    # \u2500\u2500 Batch gradient descent \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    for _ in range(epochs):\n        # Forward pass\n        z = X_train @ w + b            # shape: (n_samples,)\n        p = _sigmoid(z)                # predicted probabilities\n\n        # Gradients (averaged over the batch)\n        error = p - y_train            # shape: (n_samples,)\n        grad_w = (X_train.T @ error) / n_samples  # shape: (n_features,)\n        grad_b = error.mean()                           # scalar\n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # \u2500\u2500 Prediction on the test set \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    probs_test = _sigmoid(X_test @ w + b)\n    preds = (probs_test >= 0.5).astype(int)  # threshold at 0.5\n\n    return preds.tolist()\n```"}
{"task_id": 148, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average (EMA) of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    # -------- validation --------\n    if not data or not (0.0 <= alpha <= 1.0):\n        return -1\n\n    # -------- special cases --------\n    if alpha == 0:\n        # All values collapse to the first observation\n        return [round(data[0], 4)] * len(data)\n\n    if alpha == 1:\n        # EMA equals the original series\n        return [round(x, 4) for x in data]\n\n    # -------- general case 0 < alpha < 1 --------\n    smoothed: List[float] = [data[0]]  # s0 = x0\n    for k in range(1, len(data)):\n        s_k = (1 - alpha) * smoothed[-1] + alpha * data[k]\n        smoothed.append(s_k)\n\n    # Round each value to four decimal places\n    return [round(v, 4) for v in smoothed]\n```"}
{"task_id": 150, "completion_id": 0, "completion": "```python\nfrom heapq import heappush, heappop\nfrom typing import List, Tuple, Union\n\n\ndef find_shortest_path(grid: List[List[int]]) -> Union[int, List[Tuple[int, int]]]:\n    \"\"\"Return any shortest path from (0,0) to (m-1,n-1) in a 0/1 grid using A*.\n    If such a path does not exist, return -1.\n    \"\"\"\n    if not grid:\n        return -1\n\n    m, n = len(grid), len(grid[0])\n\n    # Reject immediately if start or goal is blocked.\n    if grid[0][0] == 1 or grid[m - 1][n - 1] == 1:\n        return -1\n\n    # Heuristic: Manhattan distance to the goal.\n    def h(r: int, c: int) -> int:\n        return abs(r - (m - 1)) + abs(c - (n - 1))\n\n    # Open list: priority queue ordered by f = g + h\n    open_heap: List[Tuple[int, int, int, int]] = []\n    heappush(open_heap, (h(0, 0), 0, 0, 0))  # (f, g, row, col)\n\n    # g-costs dictionary keeps the smallest known cost to each cell\n    g_cost = {(0, 0): 0}\n\n    # Parent pointers for path reconstruction\n    parent: dict[Tuple[int, int], Tuple[int, int]] = {(0, 0): None}\n\n    # Directions: up, down, left, right\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    while open_heap:\n        f, g, r, c = heappop(open_heap)\n\n        # Goal reached\n        if (r, c) == (m - 1, n - 1):\n            path: List[Tuple[int, int]] = []\n            cur: Tuple[int, int] | None = (r, c)\n            while cur:\n                path.append(cur)\n                cur = parent[cur]\n            return path[::-1]  # reverse to get start -> goal\n\n        # Expand neighbours\n        for dr, dc in dirs:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < m and 0 <= nc < n and grid[nr][nc] == 0:\n                new_g = g + 1\n                if new_g < g_cost.get((nr, nc), float('inf')):\n                    g_cost[(nr, nc)] = new_g\n                    parent[(nr, nc)] = (r, c)\n                    heappush(open_heap, (new_g + h(nr, nc), new_g, nr, nc))\n\n    # Exhausted the search space: no path found\n    return -1\n```"}
{"task_id": 151, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# ---------- individual activations -------------------------------------------------\ndef _sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef _sigmoid_grad(y: np.ndarray) -> np.ndarray:\n    # y is already \u03c3(x)\n    return y * (1.0 - y)\n\n\ndef _tanh(x: np.ndarray) -> np.ndarray:\n    return np.tanh(x)\n\n\ndef _tanh_grad(y: np.ndarray) -> np.ndarray:\n    # y is already tanh(x)\n    return 1.0 - y ** 2\n\n\ndef _relu(x: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, x)\n\n\ndef _relu_grad(y: np.ndarray) -> np.ndarray:\n    # y is already relu(x) \u21d2 derivative is 1 where y>0 else 0\n    return (y > 0).astype(y.dtype)\n\n\ndef _softmax(x: np.ndarray) -> np.ndarray:\n    # numerically-stable softmax along the last axis\n    z = x - np.max(x, axis=-1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n\n\ndef _softmax_grad(_: np.ndarray) -> np.ndarray:\n    # per instructions return ones; true Jacobian applied with loss\n    return 1.0\n\n\n# mapping --------------------------------------------------------------------------\n_FORWARD = {\n    \"sigmoid\": _sigmoid,\n    \"tanh\":    _tanh,\n    \"relu\":    _relu,\n    \"softmax\": _softmax,\n}\n\n_BACKWARD = {\n    \"sigmoid\": _sigmoid_grad,\n    \"tanh\":    _tanh_grad,\n    \"relu\":    _relu_grad,\n    \"softmax\": _softmax_grad,\n}\n\n\n# ---------- public helper ----------------------------------------------------------\ndef activation_forward_backward(\n    X: np.ndarray, activation: str, upstream_grad: np.ndarray\n) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply an activation function and its element-wise derivative in one pass.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Raw pre-activation values.\n    activation : str\n        One of: \"sigmoid\", \"tanh\", \"relu\", \"softmax\".\n    upstream_grad : np.ndarray\n        Gradient flowing from the layer above (same shape as X).\n\n    Returns\n    -------\n    (A, grad) : tuple[np.ndarray, np.ndarray]\n        A   \u2013 activation(X)  (rounded to 4 decimals)\n        grad \u2013 upstream_grad * local_derivative  (rounded to 4 decimals)\n    \"\"\"\n    activation = activation.lower()\n    if activation not in _FORWARD:\n        raise ValueError(\n            f\"Unsupported activation '{activation}'. \"\n            f\"Choose from {list(_FORWARD.keys())}.\"\n        )\n\n    # forward pass ---------------------------------------------------------------\n    A = _FORWARD[activation](X)\n\n    # backward (local gradient) ---------------------------------------------------\n    local_grad = _BACKWARD[activation](A)  # element-wise derivative\n\n    # multiply with upstream gradient; for softmax local_grad is scalar 1 -------\n    grad = upstream_grad * local_grad\n\n    # rounding as required --------------------------------------------------------\n    A = np.round(A, 4)\n    grad = np.round(grad, 4)\n\n    return A, grad\n```\n\nExplanation of key points\n\u2022 Each activation has its own forward routine and a derivative routine that **receives the already-computed activation output** (rule 3).  \n\u2022 Softmax is implemented in a numerically stable way (subtracting the row-wise/last-axis maximum) and its \u201cderivative\u201d routine returns `1` as required.  \n\u2022 Both the activated output and the final gradient are rounded to 4 decimal places (`np.round`).  \n\u2022 All functions operate element-wise on arbitrary-shaped `np.ndarray`s; for softmax the computation is carried out along the last axis."}
{"task_id": 152, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n\n    # Basic validation\n    if y_true.size == 0 or y_pred.size == 0 or y_true.size != y_pred.size:\n        return -1\n\n    # Count correct predictions\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n\n    # Round to 4 decimal places\n    return round(float(accuracy), 4)\n```\n\nExample usage\n```python\ny_true = [1, 0, 2, 2, 1]\ny_pred = [1, 0, 1, 2, 0]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.6\n```"}
{"task_id": 154, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF (Gaussian) kernel matrix between two sets of vectors.\n\n    Args:\n        X: Array of shape (n_samples, n_features) or a 1-D array treated as one sample.\n        Y: Array of shape (m_samples, n_features) or a 1-D array treated as one sample.\n        gamma: Positive scalar controlling the kernel width.\n\n    Returns:\n        Nested Python list with the RBF kernel matrix,\n        every entry rounded to six decimal places.\n    \"\"\"\n    # Convert inputs to float NumPy arrays and ensure 2-D shape\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n\n    if X.ndim == 1:\n        X = X[np.newaxis, :]          # shape -> (1, d)\n    if Y.ndim == 1:\n        Y = Y[np.newaxis, :]          # shape -> (1, d)\n\n    # Basic sanity checks\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\"X and Y must have the same number of features.\")\n    if gamma <= 0:\n        raise ValueError(\"gamma must be a positive value.\")\n\n    # Squared \u21132 norms of each row\n    X_norm_sq = np.sum(X ** 2, axis=1)[:, np.newaxis]   # shape (n, 1)\n    Y_norm_sq = np.sum(Y ** 2, axis=1)[np.newaxis, :]   # shape (1, m)\n\n    # Pair-wise squared Euclidean distances:\n    # ||x_i \u2212 y_j||\u00b2 = ||x_i||\u00b2 + ||y_j||\u00b2 \u2212 2 x_i\u00b7y_j\n    dist_sq = X_norm_sq + Y_norm_sq - 2 * X @ Y.T\n    dist_sq = np.maximum(dist_sq, 0.0)                  # numerical safety\n\n    # Apply the RBF kernel\n    K = np.exp(-gamma * dist_sq)\n\n    # Round to six decimals and convert to nested lists\n    return np.round(K, 6).tolist()\n```"}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # keeps the two-import header exactly as requested\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Forward pass of a two-hidden-layer actor network (Linear \u2192 ReLU \u2192\n    Linear \u2192 ReLU \u2192 Linear \u2192 tanh).\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the NumPy arrays\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each element rounded to four decimals.\n    \"\"\"\n    # Ensure state is a 1-D NumPy array of floats\n    x = np.asarray(state, dtype=float)\n\n    # Unpack weights and biases\n    W1, b1 = weights[\"W1\"], weights[\"b1\"]\n    W2, b2 = weights[\"W2\"], weights[\"b2\"]\n    W3, b3 = weights[\"W3\"], weights[\"b3\"]\n\n    # Layer 1: Linear \u2192 ReLU\n    z1 = x @ W1 + b1\n    a1 = np.maximum(0, z1)\n\n    # Layer 2: Linear \u2192 ReLU\n    z2 = a1 @ W2 + b2\n    a2 = np.maximum(0, z2)\n\n    # Output layer: Linear \u2192 tanh\n    z3 = a2 @ W3 + b3\n    actions = np.tanh(z3)\n\n    # Round to four decimals and convert to Python list\n    actions_rounded = np.round(actions, 4)\n    if actions_rounded.shape == ():                       # scalar action\n        return [float(actions_rounded)]\n    return actions_rounded.tolist()\n```"}
{"task_id": 157, "completion_id": 0, "completion": "```python\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    # 1. Length mismatch \u21d2 invalid input\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # 2. Both containers empty \u21d2 accuracy is defined as 0\n    if len(y_true) == 0:        # (since lengths are equal, y_pred is also empty)\n        return 0\n\n    # 3. Compute proportion of matches\n    correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n    accuracy = correct / len(y_true)\n\n    # 4. Round to 4 decimal places and return\n    return round(accuracy, 4)\n```"}
{"task_id": 158, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients (closed-form).\n\n    Args:\n        X: Feature matrix as nested list with shape (N, M).\n        y: Target vector of length N.\n        alpha: Non-negative regularisation strength.\n        fit_intercept: If True, adds an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.\n        When fit_intercept is True the first element is the intercept.\n    \"\"\"\n    # Convert inputs to NumPy arrays of dtype float\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float)\n\n    # Augment X with a column of ones for the intercept, if requested\n    if fit_intercept:\n        ones = np.ones((X_mat.shape[0], 1), dtype=float)\n        X_mat = np.hstack((ones, X_mat))\n\n    n_features = X_mat.shape[1]\n\n    # Build regularisation matrix A = alpha * I,\n    # but make sure the intercept (if any) is NOT regularised\n    A = np.eye(n_features) * alpha\n    if fit_intercept:\n        A[0, 0] = 0.0\n\n    # Closed-form solution: \u03b2\u0302 = (X\u1d40X + A)^{-1} X\u1d40 y\n    XtX_plus_A = X_mat.T @ X_mat + A\n    XTy = X_mat.T @ y_vec\n    beta = np.linalg.solve(XtX_plus_A, XTy)     # more stable than explicit inverse\n\n    # Round to four decimals and return as plain Python list\n    return [round(float(b), 4) for b in beta]\n```"}
{"task_id": 159, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute log p(x_i | \u03bc, \u03a3) for a multivariate normal distribution.\n\n    Parameters\n    ----------\n    x_i : np.ndarray\n        Sample vector of shape (d,).\n    mu : np.ndarray\n        Mean vector of shape (d,).\n    sigma : np.ndarray\n        Positive\u2013definite covariance matrix of shape (d, d).\n\n    Returns\n    -------\n    float\n        Log probability density rounded to four decimal places.\n    \"\"\"\n    # Make sure we are working with 1-D copies (avoids unwanted broadcasting)\n    x_i = np.asarray(x_i).ravel()\n    mu  = np.asarray(mu).ravel()\n    sigma = np.asarray(sigma)\n\n    d = x_i.size                       # dimensionality\n\n    # Difference vector (x - \u03bc)\n    diff = x_i - mu\n\n    # ln |\u03a3| via slogdet  (more stable than log(det(\u03a3)))\n    sign, logdet = np.linalg.slogdet(sigma)\n    if sign <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite.\")\n\n    # Solve \u03a3\u00b7\u03b1 = (x - \u03bc)  =>  \u03b1 = \u03a3\u207b\u00b9 (x - \u03bc)\n    alpha = np.linalg.solve(sigma, diff)\n\n    # Mahalanobis term (x-\u03bc)\u1d40 \u03a3\u207b\u00b9 (x-\u03bc)\n    quad_form = diff @ alpha\n\n    # The log density\n    log_prob = -0.5 * (d * np.log(2 * np.pi) + logdet + quad_form)\n\n    # Return rounded to four decimal places\n    return round(float(log_prob), 4)\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a >  tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\n\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate descent.\"\"\"\n    # ---- basic shapes -------------------------------------------------------\n    X  = np.asarray(X, dtype=float)\n    y  = np.asarray(y, dtype=float).ravel()\n    m, n = X.shape                           # m samples, n original features\n\n    # ---- add intercept column (never penalised) ----------------------------\n    intercept_col = np.ones((m, 1)) if fit_intercept else np.zeros((m, 1))\n    XA = np.hstack((intercept_col, X))       # (m , n + 1)\n    p  = n + 1                               # total number of coordinates\n\n    # ---- parameter initialisation ------------------------------------------\n    w = np.zeros(p)                          # first entry is the intercept\n    col_sq_norms = (XA ** 2).sum(axis=0)     # \u2016x_j\u2016\u00b2, used in closed form\n\n    # ---- outer optimisation loop -------------------------------------------\n    for _ in range(max_iters):\n\n        # (re-)estimate intercept as the mean residual\n        if fit_intercept:\n            y_hat_wo_b = XA[:, 1:].dot(w[1:])\n            w[0] = (y - y_hat_wo_b).mean()\n\n        # ---- coordinate updates (excluding intercept) ----------------------\n        for j in range(1, p):\n            x_j = XA[:, j]\n\n            # partial residual with w_j temporarily set to 0\n            r_j = y - XA.dot(w) + w[j] * x_j\n\n            rho = x_j.dot(r_j)               # \u27e8x_j , r_j\u27e9\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_sq_norms[j]\n\n    # ---- prepare rounded output --------------------------------------------\n    weights = np.round(w[1:], 4).tolist()    # exclude intercept\n    bias    = round(float(w[0]), 4)\n\n    return weights, bias\n```"}
{"task_id": 161, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"\n    Predict binary labels (+1 / -1) with a second-order Factorization Machine.\n\n    Parameters\n    ----------\n    X  : array_like, shape = (m, n)\n         Design matrix containing m samples with n features each.\n    w0 : float\n         Global bias term.\n    w  : array_like, shape = (n,)\n         Linear weights.\n    V  : array_like, shape = (n, k)\n         Latent factor matrix (k is the embedding dimension).\n\n    Returns\n    -------\n    list\n        Predicted labels (+1 or -1) for the m samples.\n    \"\"\"\n    # Cast inputs to NumPy arrays (no\u2010copy if already arrays)\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float)\n    V = np.asarray(V, dtype=float)\n\n    # ---------- 1. Linear part ----------\n    linear_term = X @ w                       # shape (m,)\n\n    # ---------- 2. Interaction part ----------\n    #   using \u00bd \u03a3_f [ (X V)_f\u00b2  \u2212 (X\u00b2 V\u00b2)_f ]\n    XV = X @ V                                # shape (m, k)\n    X2V2 = (X ** 2) @ (V ** 2)                # shape (m, k)\n    interaction_term = 0.5 * (XV ** 2 - X2V2).sum(axis=1)  # shape (m,)\n\n    # ---------- 3. Total score ----------\n    scores = w0 + linear_term + interaction_term           # shape (m,)\n\n    # ---------- 4. Convert to binary labels ----------\n    labels = np.where(scores >= 0, 1, -1)\n\n    return labels.tolist()\n```"}
{"task_id": 162, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray,\n                  epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as Y_hat.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        (loss, gradient) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    # shape check\n    assert Y_hat.shape == Y.shape, \"Y_hat and Y must have the same shape\"\n\n    m = Y_hat.shape[0]                         # batch size\n\n    # ----- loss -----\n    # add epsilon for numerical stability\n    log_probs = np.log(Y_hat + epsilon)\n    loss = -np.sum(Y * log_probs) / m\n\n    # ----- gradient -----\n    grad = (Y_hat - Y) / m                    # \u2207_{Y_hat}\n\n    # ----- rounding -----\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4).tolist()\n\n    return loss_rounded, grad_rounded\n```"}
{"task_id": 163, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, \n                            reg_factor: float, l1_ratio: float,\n                            learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Elastic-Net / Ridge (\u03b1=0) for one feature and arbitrary polynomial degree.\"\"\"\n    \n    # 1) design matrix ---------------------------------------------------------\n    X = np.asarray(X, dtype=float).reshape(-1)\n    y = np.asarray(y, dtype=float).reshape(-1)\n    n_samples = X.size\n    \n    # powers of x : [x^0, x^1, \u2026 , x^degree]\n    X_poly = np.vstack([X**p for p in range(degree + 1)]).T      # shape (n, degree+1)\n    \n    # ------------------------------------------------------------------------- \n    # 2) fit coefficients \u03b2\n    # ------------------------------------------------------------------------- \n    d_plus1 = degree + 1                # number of coefficients (incl. bias)\n    \n    if l1_ratio == 0.0:                 # ---- pure Ridge -> closed form -------\n        # matrix that *does not* penalise the bias term\n        R = np.eye(d_plus1)\n        R[0, 0] = 0                     # no penalty for \u03b20\n        \n        # normal equations with Tikhonov regularisation\n        lam = (n_samples * reg_factor) / 2.0     # comes from derivative (see analysis)\n        A = X_poly.T @ X_poly + lam * R\n        b = X_poly.T @ y\n        beta = np.linalg.pinv(A) @ b             # pinv is numerically safer\n    else:                               # ---- Elastic Net -> gradient descent -\n        beta = np.zeros(d_plus1)        # initialise\n        \n        for _ in range(n_iterations):\n            y_pred = X_poly @ beta\n            err    = y_pred - y\n            \n            # gradient of the data-fit term (MSE)\n            grad = (2.0 / n_samples) * (X_poly.T @ err)\n            \n            # L2 part (not on bias)\n            grad_l2 = reg_factor * (1 - l1_ratio) * beta\n            grad_l2[0] = 0.0\n            \n            # L1 part (sub-gradient, not on bias)\n            grad_l1 = reg_factor * l1_ratio * np.sign(beta)\n            grad_l1[0] = 0.0\n            \n            grad += grad_l1 + grad_l2\n            beta -= learning_rate * grad\n    \n    # ------------------------------------------------------------------------- \n    # 3) predictions -----------------------------------------------------------\n    preds = X_poly @ beta\n    return [round(float(v), 4) for v in preds]\n```"}
{"task_id": 164, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_log_loss(actual, predicted):\n    \"\"\"\n    Compute the binary classification log-loss (cross-entropy loss).\n\n    Parameters\n    ----------\n    actual : sequence of int (0 or 1)\n        Ground-truth binary labels.\n    predicted : sequence of float (in [0, 1])\n        Predicted probabilities for the positive class.\n\n    Returns\n    -------\n    float\n        The average log-loss rounded to six decimal places.\n\n    Raises\n    ------\n    ValueError\n        If *actual* and *predicted* have different lengths\n        or if *actual* contains values other than 0 or 1.\n    \"\"\"\n    EPS = 1e-15  # numerical stability constant\n\n    # Convert to NumPy arrays\n    y_true = np.asarray(actual, dtype=np.float64)\n    y_pred = np.asarray(predicted, dtype=np.float64)\n\n    # Basic validations\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(\"`actual` and `predicted` must be of the same length.\")\n    if not np.isin(y_true, [0, 1]).all():\n        raise ValueError(\"`actual` must contain only 0s and 1s.\")\n\n    # Clip predicted probabilities to avoid log(0) and log(1)\n    y_pred_clipped = np.clip(y_pred, EPS, 1.0 - EPS)\n\n    # Compute log-loss\n    loss = -np.mean(\n        y_true * np.log(y_pred_clipped) +\n        (1 - y_true) * np.log(1 - y_pred_clipped)\n    )\n\n    return round(float(loss), 6)\n```"}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\",\n                     y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length vectors.\n\n    Args:\n        x: 1-D list or NumPy array of integers.\n        y: 1-D list or NumPy array of integers.\n\n    Returns:\n        Float rounded to 4 decimal places representing the distance, or \u20111\n        if the vectors have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized comparison\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n\n    # Ensure vectors are 1-D\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        raise ValueError(\"Input vectors must be 1-D.\")\n\n    # Length check\n    if x_arr.size != y_arr.size:\n        return -1\n\n    N = x_arr.size\n    if N == 0:        # Both vectors are empty \u2192 distance conventionally 0\n        return 0.0\n\n    # Count differing positions and normalize\n    diff = np.count_nonzero(x_arr != y_arr)\n    distance = diff / N\n\n    return round(float(distance), 4)\n```"}
{"task_id": 166, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_iter: int = 1000\n) -> list[int]:\n    \"\"\"Classic binary Perceptron (learning-rate = 1).\"\"\"\n    # -------- initialisation -------------------------------------------------\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features, dtype=float)   # weight vector\n    b = 0.0                                 # bias / offset\n    \n    # -------- training -------------------------------------------------------\n    for _ in range(max_iter):\n        errors = 0\n        for x_i, y_i in zip(X_train, y_train):\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:                 # misclassified or on the margin\n                w += y_i * x_i              # weight update (\u03b7 = 1)\n                b += y_i                    # bias   update\n                errors += 1\n        if errors == 0:                     # early stopping: converged\n            break\n    \n    # -------- prediction -----------------------------------------------------\n    scores = X_test @ w + b                 # raw scores\n    preds = np.sign(scores)                 # map to {-1, 0, +1}\n    preds[preds == 0] = 1                   # tie-break: treat 0 as +1\n    \n    return preds.astype(int).tolist()\n```"}
{"task_id": 167, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_cross_entropy(y_hat, y, epsilon: float = 1e-10):\n    \"\"\"Compute the Binary Cross-Entropy (BCE) loss and its gradient.\n\n    Args:\n        y_hat (np.ndarray): Predicted probabilities, each in [0, 1].\n        y (np.ndarray): Ground-truth binary labels (0 or 1).\n        epsilon (float, optional): Small constant to avoid log(0). Defaults to 1e-10.\n\n    Returns:\n        tuple[float, list[float]]: (loss, gradient) both rounded to 4 decimals.\n    \"\"\"\n    # Ensure NumPy arrays\n    y_hat = np.asarray(y_hat, dtype=float)\n    y     = np.asarray(y,     dtype=float)\n\n    # 1. Clip predictions to avoid log(0)\n    y_hat_clipped = np.clip(y_hat, epsilon, 1.0 - epsilon)\n\n    # Number of samples\n    m = y_hat_clipped.size\n\n    # 2. Mean BCE loss\n    loss = -np.mean(y * np.log(y_hat_clipped) +\n                    (1.0 - y) * np.log(1.0 - y_hat_clipped))\n\n    # 3. Gradient w.r.t. y_hat (using clipped predictions)\n    grad = (y_hat_clipped - y) / m\n\n    # 4. Round to 4 decimal places and return\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4).tolist()\n\n    return loss_rounded, grad_rounded\n```"}
{"task_id": 168, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test:  list[list[float]]\n) -> list[int]:\n    \"\"\"\n    Train a Gaussian-Naive-Bayes classifier on (X_train, y_train) and predict the\n    labels of X_test.\n\n    Notes\n    -----\n    \u2022 Feature independence and a Gaussian distribution per feature/class are\n      assumed.\n    \u2022 A small \u03b5 is added to every variance to avoid division-by-zero.\n    \u2022 Only NumPy is used.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Convert Python lists to NumPy arrays\n    # ------------------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=float)          # (n_samples, n_features)\n    y_train = np.asarray(y_train, dtype=int)            # (n_samples,)\n    X_test  = np.asarray(X_test,  dtype=float)          # (m_samples, n_features)\n\n    n_samples, n_features = X_train.shape\n    classes = np.unique(y_train)                        # sorted unique class labels\n    n_classes = classes.size\n\n    # ------------------------------------------------------------------\n    # 2. Estimate class-conditional means and variances\n    # ------------------------------------------------------------------\n    eps = 1e-6                                           # numerical stability\n    means  = np.zeros((n_classes, n_features))\n    var    = np.zeros_like(means)\n    priors = np.zeros(n_classes)\n\n    for idx, c in enumerate(classes):\n        X_c = X_train[y_train == c]                     # samples of class c\n        priors[idx] = X_c.shape[0] / n_samples\n        means[idx]  = X_c.mean(axis=0)\n        var[idx]    = X_c.var(axis=0) + eps             # add \u03b5 to every variance\n\n    log_priors = np.log(priors)                         # (n_classes,)\n\n    # ------------------------------------------------------------------\n    # 3. Compute log-posterior for every test sample and class\n    # ------------------------------------------------------------------\n    # Pre-compute constant part of Gaussian log-pdf:\n    #  log N(x; \u03bc, \u03c3\u00b2) = -0.5 * [ log(2\u03c0\u03c3\u00b2) + (x-\u03bc)\u00b2/\u03c3\u00b2 ]\n    log_coef = -0.5 * np.log(2.0 * np.pi * var)         # (n_classes, n_features)\n\n    predictions = []\n    for x in X_test:                                    # iterate over test samples\n        # Broadcast x (shape (n_features,)) against means/var of shape\n        # (n_classes, n_features)\n        log_likelihood = log_coef - 0.5 * ((x - means) ** 2) / var\n        log_posterior  = log_priors + np.sum(log_likelihood, axis=1)\n        predicted_class = classes[np.argmax(log_posterior)]\n        predictions.append(int(predicted_class))\n\n    return predictions\n```\n\nExplanation of the main steps\n1. Transform the input lists into NumPy arrays for efficient vectorised operations.\n2. For each class:\n   \u2022 Compute the prior probability  \n   \u2022 Compute the per-feature mean and variance, adding a tiny \u03b5 to every variance\n3. For every test vector:\n   \u2022 Evaluate the log of the Gaussian density for every feature/class  \n   \u2022 Sum those logs and add the class log-prior to obtain the log-posterior  \n   \u2022 Pick the class with the highest log-posterior.\n4. Return the predicted labels as a regular Python list of integers."}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef generate_window(\n    window: str,\n    N: int,\n    coefficients: list[float] | None = None\n) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Window name: \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate (must be > 0).\n    coefficients : list[float] | None, optional\n        Cosine-series coefficients a\u2080\u2026a_M, required only when\n        window == \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        List with N floats rounded to 4 decimal places.\n    \"\"\"\n    # ------------------------------------------------------------------ checks\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    if N == 1:                      # conventional definition\n        return [1.0]\n\n    name = window.lower()\n    if name not in {\n        \"hamming\",\n        \"hann\",\n        \"blackman_harris\",\n        \"generalized_cosine\",\n    }:\n        raise ValueError(f\"Unknown window type: {window!r}\")\n\n    # ----------------------------------------------------------------- helpers\n    n = np.arange(N)\n    denom = N - 1\n    two_pi_term = 2.0 * np.pi * n / denom   # 2\u03c0n/(N\u22121)\n\n    # ------------------------------------------------------------- window core\n    if name == \"hamming\":\n        w = 0.54 - 0.46 * np.cos(two_pi_term)\n\n    elif name == \"hann\":\n        w = 0.5 - 0.5 * np.cos(two_pi_term)\n\n    elif name == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (\n            a0\n            - a1 * np.cos(two_pi_term)\n            + a2 * np.cos(2 * two_pi_term)   # 4\u03c0n/(N\u22121)\n            - a3 * np.cos(3 * two_pi_term)   # 6\u03c0n/(N\u22121)\n        )\n\n    elif name == \"generalized_cosine\":\n        if not coefficients:\n            raise ValueError(\n                \"For 'generalized_cosine' you must supply a non-empty \"\n                \"'coefficients' list.\"\n            )\n        w = np.zeros(N, dtype=float)\n        for k, ak in enumerate(coefficients):\n            w += ak * np.cos(2.0 * np.pi * k * n / denom)\n\n    # --------------------------------------------------------------- rounding\n    return [round(float(v), 4) for v in w]\n```"}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Binary Logistic-Regression trained with mini-batch Gradient Descent.\"\"\"\n    # --------------- preparation & basic checks ----------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n\n    uniq = np.unique(y_train)\n    if len(uniq) == 1:  # degenerate case \u2013 only one class present\n        return [int(uniq[0])] * len(X_test)\n\n    if len(uniq) != 2:\n        raise ValueError(\"Exactly two distinct labels are required for training.\")\n\n    # map original labels -> {0,1}\n    #   uniq[0] ==> 0 ,  uniq[1] ==> 1\n    pos_label = uniq[1]\n    y_bin = (y_train == pos_label).astype(float)\n\n    # add intercept term\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train = np.hstack([ones_train, X_train])\n    X_test_aug = np.hstack([ones_test, X_test])\n\n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)  # parameter vector\n\n    def sigmoid(z):\n        z = np.clip(z, -500, 500)  # numerical stability\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # -------------------- mini-batch gradient descent ----------------------\n    for _ in range(epochs):\n        permutation = np.random.permutation(n_samples)\n        for start in range(0, n_samples, batch_size):\n            idx = permutation[start : start + batch_size]\n            X_b = X_train[idx]\n            y_b = y_bin[idx]\n\n            preds = sigmoid(X_b @ w)\n            # gradient of NLL wrt w\n            grad = X_b.T @ (preds - y_b) / len(idx)\n            w -= learning_rate * grad\n\n    # ------------------------------ predict --------------------------------\n    probs = sigmoid(X_test_aug @ w)\n    preds_bin = (probs >= 0.5).astype(int)\n\n    # map back to original labels\n    label_map = {0: int(uniq[0]), 1: int(uniq[1])}\n    preds_original = [label_map[int(p)] for p in preds_bin]\n\n    return preds_original\n```"}
{"task_id": 172, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 \u2212 tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    # Detect whether the input is a (numpy) scalar or an array-like structure.\n    if np.isscalar(z):\n        # Scalar path\n        grad = 1.0 - np.tanh(z) ** 2\n        return float(np.round(grad, 4))\n    else:\n        # Vector / matrix path\n        arr = np.asarray(z, dtype=float)\n        grad = 1.0 - np.tanh(arr) ** 2          # Element-wise derivative\n        grad = np.round(grad, 4)                # Round to 4 decimals\n        return grad.tolist()\n```"}
{"task_id": 173, "completion_id": 0, "completion": "```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute Unicode-code-point (ASCII) differences between\n        corresponding characters.  If the strings differ in length, \u20131 is\n        returned.\n    \"\"\"\n    # Rule 1: strings must be the same length\n    if len(candidate) != len(target):\n        return -1\n\n    # Rule 2 (implicitly handled): empty strings yield a distance of 0\n    return sum(abs(ord(c) - ord(t)) for c, t in zip(candidate, target))\n```\n\nExamples\n---------\n```python\n>>> alphabetical_distance(\"HELLO\", \"WORLD\")\n52\n>>> alphabetical_distance(\"\", \"\")\n0\n>>> alphabetical_distance(\"GENETIC\", \"ALGO\")\n-1\n```"}
{"task_id": 174, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # duplicated on purpose \u2013 kept as in the given stub\n\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\"\"\"\n    \n    # Ensure input arrays are float64 for numerical stability\n    preds = predictions.astype(np.float64)\n    lbls  = labels.astype(np.float64)\n\n    # ----- Categorical cross-entropy loss -----\n    # Add a small \u03b5 to avoid log(0)\n    eps   = 1e-12\n    preds = np.clip(preds, eps, 1. - eps)\n    logp  = np.log(preds)\n    # Cross-entropy per sample: -\u03a3 y_i * log(p_i)\n    ce_per_sample = -np.sum(lbls * logp, axis=1)\n    loss = ce_per_sample.mean()\n\n    # ----- Accuracy -----\n    pred_classes = np.argmax(preds, 1)\n    true_classes = np.argmax(lbls, 1)\n    accuracy = np.mean(pred_classes == true_classes)\n\n    # Round to 4 decimal places as required\n    return round(loss, 4), round(accuracy, 4)\n```"}
{"task_id": 175, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    # Validate coefficient\n    if C < 0:\n        raise ValueError(\"Regularization strength C must be non-negative.\")\n    \n    # Convert to a 1-D NumPy array (handles list or ndarray input)\n    w = np.asarray(weights, dtype=float).ravel()   # ravel() flattens; works for empty\n    \n    # Compute squared-L2 norm and scale by C\n    penalty = float(C) * float(np.dot(w, w))       # dot(w, w) == sum(w_i ** 2)\n    \n    # Round to 4 decimal places\n    return round(penalty, 4)\n```"}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"AdaBoost (SAMME-R) with decision stumps, implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : list of list of float\n        Training features.\n    y_train : list of int (0/1)\n        Training labels, 0 = negative class, 1 = positive class.\n    X_test  : list of list of float\n        Samples whose labels must be predicted.\n    n_estimators : int, default=10\n        Maximum number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (0/1) for every row of *X_test*.\n    \"\"\"\n\n    # ------------------------------------------------------------------ helpers\n    def train_best_stump(X: np.ndarray,\n                         y: np.ndarray,\n                         w: np.ndarray):\n        \"\"\"\n        Exhaustively search the decision-stump with the smallest\n        *weighted* classification error.  Tie-breaking is deterministic:\n            1) smallest error\n            2) smallest feature index\n            3) smallest threshold\n            4) polarity  +1 before \u20131\n        \"\"\"\n        n_samples, n_features = X.shape\n        best = {\n            'feature': None,\n            'threshold': None,\n            'polarity': None,\n            'pred': None,\n            'error': np.inf\n        }\n\n        for j in range(n_features):                                #  tie 1\n            column = X[:, j]\n            thresholds = np.unique(column)                         #  tie 2\n            for thr in thresholds:                                 #  tie 3\n                for pol in (1, -1):                                #  tie 4\n                    if pol == 1:\n                        h = np.where(column < thr, 1, -1)\n                    else:  # pol == -1\n                        h = np.where(column >= thr, 1, -1)\n\n                    err = np.sum(w * (h != y))\n                    if err < best['error'] - 1e-12:                #  strict improvement\n                        best.update(feature=j,\n                                    threshold=float(thr),\n                                    polarity=pol,\n                                    pred=h,\n                                    error=err)\n        return best\n\n    def stump_predict(stump, X):\n        \"\"\"Vectorised prediction of a single stump on data matrix X.\"\"\"\n        col = X[:, stump['feature']]\n        if stump['polarity'] == 1:\n            return np.where(col < stump['threshold'], 1, -1)\n        else:\n            return np.where(col >= stump['threshold'], 1, -1)\n\n    # ----------------------------------------------------------- preparation\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=int)\n    y = 2 * y - 1        # {0,1}  ->  {-1, +1}\n\n    n_samples = X.shape[0]\n    # Initial uniform weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    stumps: list[dict] = []\n    alphas: list[float] = []\n\n    # -------------------------------------------------------------- boosting\n    for _ in range(n_estimators):\n        stump = train_best_stump(X, y, w)\n        error = stump['error']\n\n        # If perfect stump found, keep it and stop early\n        if error <= 1e-12:\n            alpha = 0.5 * np.log((1.0 - 1e-12) / (1e-12))\n            stumps.append(stump)\n            alphas.append(alpha)\n            break\n\n        # Compute the learner\u2019s vote weight\n        alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))\n\n        # Update sample weights\n        w *= np.exp(-alpha * y * stump['pred'])\n        w /= np.sum(w)          # normalise\n\n        stumps.append(stump)\n        alphas.append(alpha)\n\n    # ------------------------------------------------------------- inference\n    X_te = np.asarray(X_test, dtype=float)\n    agg = np.zeros(X_te.shape[0], dtype=float)\n\n    for stump, alpha in zip(stumps, alphas):\n        agg += alpha * stump_predict(stump, X_te)\n\n    # sign(agg)  ->  {-1,1} ; map back to {0,1}\n    preds = (agg >= 0).astype(int).tolist()\n    return preds\n```"}
{"task_id": 177, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Union\n\ndef tanh(z: Union[int, float, np.ndarray, list]) -> Union[float, list]:\n    \"\"\"Compute tanh without using numpy.tanh (4-decimal rounding).\"\"\"\n    \n    # ---------- helper that works element-wise on numpy arrays ----------\n    def _tanh_array(arr: np.ndarray) -> np.ndarray:\n        # Numerically stable formulation:\n        # tanh(z) = sign(z) * (1 - e^{-2|z|}) / (1 + e^{-2|z|})\n        exp_neg = np.exp(-2 * np.abs(arr))\n        return np.sign(arr) * (1 - exp_neg) / (1 + exp_neg)\n\n    # -------------------- scalar input --------------------\n    if np.isscalar(z):                          # int, float or numpy scalar\n        z = float(z)                            # guarantee Python float\n        result = _tanh_array(np.array(z))       # reuse the vectorised helper\n        return round(float(result), 4)\n\n    # ----------------- list / ndarray input -----------------\n    arr = np.asarray(z, dtype=float)            # convert once\n    result = _tanh_array(arr)\n    return np.round(result, 4).tolist()\n```\n\nExplanation of the key points\n\u2022 A numerically stable identity `tanh(z) = sign(z) * (1 - e^{-2|z|}) / (1 + e^{-2|z|})` is used, so no overflow occurs even for large |z|.  \n\u2022 Scalars are detected with `np.isscalar`; they are returned as a single `float` rounded to 4 decimals.  \n\u2022 Non-scalar inputs (lists, nested lists, NumPy arrays) are converted to an array, processed in a vectorised way, rounded, and finally converted back to a regular Python list via `.tolist()`."}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function with Particle Swarm Optimisation.\n\n    Args:\n        n_dims:            number of decision variables  (>0)\n        num_particles:     swarm size                   (>0)\n        num_iterations:    optimisation iterations      (>0)\n        seed:              RNG seed (default 1)\n\n    Returns\n        Best objective value found, rounded to 4 decimals, or \u20131 on bad input.\n    \"\"\"\n    # ----------------- sanity checks -----------------\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    rng = np.random.default_rng(seed)\n\n    # PSO hyper-parameters\n    w, c1, c2 = 0.5, 1.5, 1.5           # inertia, cognitive & social weights\n    lo, hi     = -1.0, 1.0              # search\u2010space bounds\n\n    # ----------------- initial swarm -----------------\n    positions   = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    velocities  = rng.uniform(lo, hi, size=(num_particles, n_dims))\n\n    pbest_pos   = positions.copy()                      # personal best positions\n    pbest_val   = np.sum(pbest_pos ** 2, axis=1)        # personal best values\n\n    gbest_idx   = np.argmin(pbest_val)                  # global best index\n    gbest_pos   = pbest_pos[gbest_idx].copy()\n    gbest_val   = pbest_val[gbest_idx]\n\n    # ----------------- optimisation loop -------------\n    for _ in range(num_iterations):\n        # random coefficients for every particle & dimension\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n\n        # velocity & position updates\n        velocities = (w * velocities\n                      + c1 * r1 * (pbest_pos - positions)\n                      + c2 * r2 * (gbest_pos - positions))\n        positions  = np.clip(positions + velocities, lo, hi)\n\n        # evaluate objective\n        curr_val = np.sum(positions ** 2, axis=1)\n\n        # update personal bests\n        improved         = curr_val < pbest_val\n        pbest_pos[improved] = positions[improved]\n        pbest_val[improved] = curr_val[improved]\n\n        # update global best\n        gbest_idx = np.argmin(pbest_val)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_val = pbest_val[gbest_idx]\n\n    # ----------------- result ------------------------\n    return round(float(gbest_val), 4)\n```"}
{"task_id": 179, "completion_id": 0, "completion": "```python\nimport re\nimport string\n\n\ndef tokenize_chars(line: str,\n                   lowercase: bool = True,\n                   filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case beforehand.\n        filter_punctuation: When *True*, remove every character contained in\n            ``string.punctuation`` (the 32 standard ASCII punctuation marks).\n\n    Returns\n    -------\n    list[str]\n        A list of single-character strings after the selected preprocessing\n        steps have been applied.  Runs of \u22652 spaces are collapsed into a single\n        space, and leading/trailing spaces are stripped.  If the final string\n        is empty, an empty list is returned.\n    \"\"\"\n    # 1) Handle `None`\n    if line is None:\n        return []\n\n    # 2) Optional lower-casing\n    if lowercase:\n        line = line.lower()\n\n    # 3) Optional punctuation filtering\n    if filter_punctuation:\n        # Build a translation table that deletes all punctuation marks\n        line = line.translate(str.maketrans('', '', string.punctuation))\n\n    # 4) Collapse runs of \u22652 spaces and strip leading/trailing spaces\n    #    (only the ASCII space \" \" is targeted, not other whitespace)\n    line = re.sub(r' {2,}', ' ', line).strip()\n\n    # 5) Return the character list (or [] if nothing is left)\n    return list(line) if line else []\n```\n\nExample usage\n-------------\n```python\n>>> tokenize_chars(\"Hello,  World!!\")\n['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n>>> tokenize_chars(\"  This\\tis a test...  \", lowercase=False, filter_punctuation=True)\n['T', 'h', 'i', 's', '\\t', 'i', 's', ' ', 'a', ' ', 't', 'e', 's', 't']\n>>> tokenize_chars(None)\n[]\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    # Convert to numpy arrays for convenience\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test,  dtype=float)\n    y_tr = np.asarray(y_train, dtype=object)   # keep objects (numbers or strings)\n\n    n_train = X_tr.shape[0]\n\n    # Guard-clauses / normalise k\n    if k <= 0 or k > n_train:\n        k = n_train\n\n    predictions: list = []\n\n    for x in X_te:\n        # Euclidean distances to every training sample\n        dists = np.linalg.norm(X_tr - x, axis=1)\n\n        # Indices of the k closest points\n        if k == n_train:\n            nn_idx = np.arange(n_train)\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n\n        neighbours = y_tr[nn_idx]\n\n        if task == \"regression\":\n            # Numeric mean rounded to 4 decimals\n            mean_val = float(np.mean(neighbours.astype(float)))\n            predictions.append(round(mean_val, 4))\n        else:  # classification (default)\n            counts = Counter(neighbours)\n            max_cnt = max(counts.values())\n            # smallest label among the ties (Python\u2019s normal ordering)\n            best_label = min(lbl for lbl, cnt in counts.items() if cnt == max_cnt)\n            predictions.append(best_label)\n\n    return predictions\n```"}
{"task_id": 181, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef knn_predict(\n    X_train: list[list[float]],\n    y_train: list,\n    X_test: list[list[float]],\n    k: int = 5,\n    problem: str = \"classify\",\n) -> list:\n    \"\"\"Predict targets of *X_test* using k-Nearest Neighbours.\"\"\"\n    # --- basic checks -------------------------------------------------------\n    if problem not in {\"classify\", \"regress\"}:\n        raise ValueError(\"`problem` must be 'classify' or 'regress'.\")\n    if k < 1:\n        raise ValueError(\"`k` must be a positive integer.\")\n    if len(X_train) == 0:\n        raise ValueError(\"`X_train` must contain at least one sample.\")\n\n    # --- convert to NumPy for fast distance computation --------------------\n    X_train_np = np.asarray(X_train, dtype=float)\n    X_test_np = np.asarray(X_test, dtype=float)\n\n    n_train = X_train_np.shape[0]\n    k = min(k, n_train)  # use as many neighbours as available\n\n    preds = []\n\n    # --- iterate over every query sample ------------------------------------\n    for x in X_test_np:\n        # Euclidean distances to all training points\n        diff = X_train_np - x\n        # squared distance is enough for ordering, avoids the costly sqrt\n        dists = np.einsum(\"ij,ij->i\", diff, diff)\n\n        # indices of k nearest neighbours (unsorted order is fine)\n        idx = np.argpartition(dists, k - 1)[:k]\n        neigh_targets = [y_train[i] for i in idx]\n\n        # ----- classification ------------------------------------------------\n        if problem == \"classify\":\n            cnt = Counter(neigh_targets)\n            max_votes = max(cnt.values())\n            winners = [label for label, v in cnt.items() if v == max_votes]\n            preds.append(sorted(winners)[0])          # deterministic tie-break\n\n        # ----- regression ----------------------------------------------------\n        else:  # problem == \"regress\"\n            mean_val = float(np.mean(neigh_targets))\n            preds.append(round(mean_val, 4))          # 4-decimal rounding\n\n    return preds\n```"}
{"task_id": 182, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"\n    Compute the polynomial-kernel (Gram) matrix K where\n    \n        K[i][j] = ( <X_i , Y_j> ) ** degree,\n    \n    <\u00b7 , \u00b7> being the ordinary dot product.  \n    The result is rounded to 4 decimal places and returned as a\n    plain nested Python list.\n    \n    Parameters\n    ----------\n    X : 2-D list or numpy.ndarray\n        Data matrix of shape (n_samples_X, n_features).\n    Y : 2-D list or numpy.ndarray\n        Data matrix of shape (n_samples_Y, n_features).\n    degree : int, optional (default=2)\n        Positive integer \u2265 1 \u2013 the polynomial degree.\n    \n    Returns\n    -------\n    list\n        Kernel matrix as a nested Python list, rounded to 4 decimals.\n        If any validation check fails the function returns -1.\n    \"\"\"\n    # ------------------------ Validation ------------------------ #\n    # Degree must be a positive *int* (bool is subclass of int -> exclude)\n    if not isinstance(degree, int) or isinstance(degree, bool) or degree < 1:\n        return -1\n    \n    # Convert inputs to float arrays (raises if conversion impossible)\n    try:\n        X_arr = np.asarray(X, dtype=float)\n        Y_arr = np.asarray(Y, dtype=float)\n    except Exception:          # non-numeric data or irregular lists\n        return -1\n    \n    # Must both be 2-D\n    if X_arr.ndim != 2 or Y_arr.ndim != 2:\n        return -1\n    \n    # Same number of features (columns)\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n    \n    # -------------------- Kernel computation -------------------- #\n    try:\n        kernel = (X_arr @ Y_arr.T) ** degree\n    except Exception:          # shape mismatch or other numerical issues\n        return -1\n    \n    # ------------------------- Rounding ------------------------- #\n    kernel = np.round(kernel, 4)\n    \n    # Return plain Python list\n    return kernel.tolist()\n```"}
{"task_id": 183, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    # Convert to tuple to allow both list and tuple inputs and make it immutable\n    weight_shape = tuple(weight_shape)\n\n    ndim = len(weight_shape)\n    if ndim == 2:                           # Dense / fully-connected layer\n        fan_in = weight_shape[0]\n    elif ndim == 4:                         # 2-D convolutional kernel\n        f_w, f_h, in_ch, _ = weight_shape   # unpack for clarity\n        fan_in = f_w * f_h * in_ch\n    else:\n        raise ValueError(\n            \"he_uniform only supports shapes of length 2 (dense) or 4 (conv); \"\n            f\"got shape with {ndim} dimensions.\"\n        )\n\n    if fan_in <= 0:\n        raise ValueError(\"fan_in must be positive, got {}\".format(fan_in))\n\n    limit = np.sqrt(6.0 / fan_in)\n\n    # Sample from the uniform distribution \ud835\udcb0[\u2212limit, limit]\n    return np.random.uniform(low=-limit, high=limit, size=weight_shape)\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"CART-style binary decision tree (Gini) completely from scratch.\"\"\"\n    # ------------------------------------------------------------------ #\n    # helpers\n    # ------------------------------------------------------------------ #\n    class Node:  # simple container for a tree node\n        __slots__ = (\n            \"is_leaf\",\n            \"prediction\",\n            \"feature\",\n            \"threshold\",\n            \"left\",\n            \"right\",\n        )\n\n        def __init__(\n            self,\n            *,\n            is_leaf: bool,\n            prediction: int | None = None,\n            feature: int | None = None,\n            threshold: float | None = None,\n            left: \"Node | None\" = None,\n            right: \"Node | None\" = None,\n        ):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n    y_train = y_train.astype(int, copy=False)\n    n_classes = int(y_train.max()) + 1  # labels are non-negative ints\n\n    def gini(counts: np.ndarray) -> float:\n        \"\"\"Gini impurity from class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts / total\n        return 1.0 - np.sum(probs**2)\n\n    def majority_class(counts: np.ndarray) -> int:\n        \"\"\"Smallest label in case of ties keeps result deterministic.\"\"\"\n        return int(np.argmax(counts))\n\n    # ------------------------------------------------------------------ #\n    # best split search for a node\n    # ------------------------------------------------------------------ #\n    def find_best_split(indices: np.ndarray) -> tuple[int, float, float, np.ndarray, np.ndarray] | None:\n        \"\"\"\n        Returns (best_feature, best_threshold, best_gain, left_idx, right_idx)\n        or None if no split brings positive gain.\n        \"\"\"\n        X_node = X_train[indices]\n        y_node = y_train[indices]\n        parent_counts = np.bincount(y_node, minlength=n_classes)\n        parent_gini = gini(parent_counts)\n\n        best_gain = 0.0\n        best_feature = -1\n        best_threshold = 0.0\n        best_left_idx = best_right_idx = None\n\n        n_samples, n_features = X_node.shape\n\n        # iterate through every feature\n        for feat in range(n_features):\n            values = X_node[:, feat]\n            order = np.argsort(values)\n            sorted_y = y_node[order]\n            sorted_vals = values[order]\n\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = parent_counts.copy()\n\n            # iterate over all possible split positions\n            for i in range(1, n_samples):\n                cls = sorted_y[i - 1]\n                left_counts[cls] += 1\n                right_counts[cls] -= 1\n\n                # skip if feature value does not change \u2013 cannot split here\n                if sorted_vals[i] == sorted_vals[i - 1]:\n                    continue\n\n                left_n = i\n                right_n = n_samples - i\n                g_left = gini(left_counts)\n                g_right = gini(right_counts)\n                weighted = (left_n * g_left + right_n * g_right) / n_samples\n                gain = parent_gini - weighted\n\n                if gain > best_gain + 1e-12:  # numeric tolerance\n                    best_gain = gain\n                    best_feature = feat\n                    best_threshold = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n                    # store actual indices of the split for later recursion\n                    mask = X_train[indices, feat] <= best_threshold\n                    best_left_idx = indices[mask]\n                    best_right_idx = indices[~mask]\n\n        if best_gain <= 0.0 or best_left_idx.size == 0 or best_right_idx.size == 0:\n            return None  # no useful split\n        return best_feature, best_threshold, best_gain, best_left_idx, best_right_idx\n\n    # ------------------------------------------------------------------ #\n    # recursive tree grow\n    # ------------------------------------------------------------------ #\n    def build(indices: np.ndarray, depth: int) -> Node:\n        y_subset = y_train[indices]\n        counts = np.bincount(y_subset, minlength=n_classes)\n\n        # stop-conditions ------------------------------------------------\n        if np.unique(y_subset).size == 1:  # pure node\n            return Node(is_leaf=True, prediction=int(y_subset[0]))\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n\n        # search for the best split -------------------------------------\n        split = find_best_split(indices)\n        if split is None:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n\n        feat, thr, _, left_idx, right_idx = split\n        left_child = build(left_idx, depth + 1)\n        right_child = build(right_idx, depth + 1)\n        return Node(\n            is_leaf=False,\n            feature=feat,\n            threshold=thr,\n            left=left_child,\n            right=right_child,\n        )\n\n    # ------------------------------------------------------------------ #\n    # build the tree\n    # ------------------------------------------------------------------ #\n    root = build(np.arange(X_train.shape[0]), depth=0)\n\n    # ------------------------------------------------------------------ #\n    # prediction traversal\n    # ------------------------------------------------------------------ #\n    def predict_single(x: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.prediction\n\n    return [predict_single(row) for row in X_test]\n```"}
{"task_id": 185, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    # Convert to ndarray \u2013 this never modifies the original object in-place\n    X_arr = np.asarray(X)\n\n    # 0. Empty input check\n    if X_arr.size == 0:\n        raise ValueError(\"Got an empty matrix.\")\n\n    # 1. One-dimensional = single sample\n    if X_arr.ndim == 1:\n        n_samples = 1\n        n_features = int(X_arr.shape[0])\n        return n_samples, n_features\n\n    # 2. Two (or more) dimensions \u2013 first axis stores the samples\n    if X_arr.ndim >= 2:\n        n_samples = int(X_arr.shape[0])\n        n_features = int(np.prod(X_arr.shape[1:], dtype=int))\n        return n_samples, n_features\n\n    # 3. Scalars (0-D arrays) fall through to here: not a valid input\n    raise ValueError(\"Input must be at least 1-dimensional (got a scalar).\")\n```"}
{"task_id": 186, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (ignored).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places. If *X_test* is empty an\n        empty list is returned.\n    \"\"\"\n    # Convert to numpy arrays for safe shape handling\n    X_test_arr = np.asarray(X_test)\n    y_train_arr = np.asarray(y_train)\n\n    # Number of samples to predict\n    m_samples = 0 if X_test_arr.ndim == 0 else X_test_arr.shape[0]\n    if m_samples == 0:\n        return []\n\n    # Compute the mean of y_train and round to 4 decimals\n    mean_val = round(float(np.mean(y_train_arr)), 4)\n\n    # Return the mean repeated for every test sample\n    return [mean_val] * m_samples\n```"}
{"task_id": 188, "completion_id": 0, "completion": "```python\nimport math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n\n    Returns the value rounded to 4 decimal places, or \u22121 if any input is invalid.\n    \"\"\"\n    # 1. -------- Input validation --------\n    # (a) Types and basic sanity\n    if not isinstance(step, int):\n        return -1\n    # (b) Numeric constraints\n    valid = (\n        0.0 <= min_epsilon < max_epsilon and   # 0 \u2264 \u03b5_min < \u03b5_max\n        decay_rate > 0.0 and                  # \u03bb > 0\n        step >= 0                             # t \u2265 0\n    )\n    if not valid:\n        return -1\n\n    # 2. -------- Compute \u03b5(t) ------------\n    decayed = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-decay_rate * step)\n\n    # 3. -------- Numerical safety clip ---\n    # Prevent any underflow that might push it below \u03b5_min\n    decayed = max(decayed, min_epsilon)\n\n    # 4. -------- Round to 4 decimals -----\n    return round(decayed, 4)\n```\n"}
{"task_id": 189, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _get_indices_weights(in_size, out_size):\n    \"\"\"\n    Compute the source indices and interpolation weights needed to resize one\n    dimension from ``in_size`` to ``out_size`` using the classic\n    (i + 0.5) * scale - 0.5  mapping (i \u2011 centre format).\n\n    Returns\n    -------\n    idx0 : ndarray (out_size,)   \u2013  left / top  indices  (\u230acoord\u230b)\n    idx1 : ndarray (out_size,)   \u2013  right / bottom indices (idx0+1, clipped)\n    w    : ndarray (out_size,)   \u2013  weight for idx1   (w = coord \u2212 idx0)\n                                     so that value = (1-w)*v[idx0] + w*v[idx1]\n    \"\"\"\n    scale            = in_size / out_size\n    coord            = (np.arange(out_size) + 0.5) * scale - 0.5\n    idx0             = np.floor(coord).astype(np.int64)\n    idx1             = idx0 + 1\n\n    # clip to valid range\n    idx0             = np.clip(idx0, 0, in_size - 1)\n    idx1             = np.clip(idx1, 0, in_size - 1)\n    w                = coord - idx0          # fractional part\n    w                = np.clip(w, 0.0, 1.0)  # numerical safety\n    return idx0, idx1, w\n\n\ndef batch_resample(X, new_dim, mode=\"bilinear\"):\n    \"\"\"Resample a batch of images to a new spatial resolution.\n\n    Parameters\n    ----------\n    X : numpy.ndarray of shape (n_ex, in_rows, in_cols, in_channels)\n        Input batch of images.\n    new_dim : tuple[int, int]\n        Target dimension ``(out_rows, out_cols)``.\n    mode : {\"bilinear\", \"neighbor\"}, default=\"bilinear\"\n        Interpolation method.\n\n    Returns\n    -------\n    numpy.ndarray\n        Resampled batch with shape (n_ex, out_rows, out_cols, in_channels),\n        rounded to 4 decimal places.\n    \"\"\"\n    if mode not in (\"bilinear\", \"neighbor\"):\n        raise NotImplementedError(f\"Unrecognized resampling mode: {mode}\")\n\n    if not (isinstance(new_dim, (tuple, list)) and len(new_dim) == 2):\n        raise ValueError(\"`new_dim` must be a tuple/list like (rows, cols)\")\n\n    n_ex, in_rows, in_cols, n_ch = X.shape\n    out_rows, out_cols = map(int, new_dim)\n\n    # ---------- Nearest\u2013neighbour -------------------------------------------------\n    if mode == \"neighbor\":\n        # mapping input centre \u2194 output centre\n        row_scale = in_rows / out_rows\n        col_scale = in_cols / out_cols\n\n        row_coords = (np.arange(out_rows) + 0.5) * row_scale - 0.5\n        col_coords = (np.arange(out_cols) + 0.5) * col_scale - 0.5\n\n        row_idx = np.clip(np.round(row_coords).astype(np.int64), 0, in_rows - 1)\n        col_idx = np.clip(np.round(col_coords).astype(np.int64), 0, in_cols - 1)\n\n        # first pick rows, then cols\n        tmp     = np.take(X, row_idx, axis=1)                 # shape (n_ex, out_rows, in_cols, n_ch)\n        out     = np.take(tmp, col_idx, axis=2)               # shape (n_ex, out_rows, out_cols, n_ch)\n\n    # ---------- Bilinear ----------------------------------------------------------\n    else:  # mode == \"bilinear\"\n        r0, r1, wy = _get_indices_weights(in_rows, out_rows)  # each (out_rows,)\n        c0, c1, wx = _get_indices_weights(in_cols, out_cols)  # each (out_cols,)\n\n        # Row interpolation -------------------------------------------------------\n        top    = np.take(X, r0, axis=1)          # (n_ex, out_rows, in_cols, n_ch)\n        bottom = np.take(X, r1, axis=1)\n\n        wy     = wy.reshape(1, -1, 1, 1)         # broadcast helpers\n        row_interp = (1.0 - wy) * top + wy * bottom   # (n_ex, out_rows, in_cols, n_ch)\n\n        # Column interpolation ----------------------------------------------------\n        left   = np.take(row_interp, c0, axis=2)  # (n_ex, out_rows, out_cols, n_ch)\n        right  = np.take(row_interp, c1, axis=2)\n\n        wx     = wx.reshape(1, 1, -1, 1)\n        out    = (1.0 - wx) * left + wx * right   # (n_ex, out_rows, out_cols, n_ch)\n\n    return np.round(out, 4)\n```"}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted\n    Gini impurity for a single binary split.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray  (n_samples \u00d7 n_features)\n    y : list[int] | np.ndarray          (n_samples,)\n\n    Returns\n    -------\n    (int, float | None, float)\n        (best_feature_index, best_threshold_value, best_gini_rounded)\n    \"\"\"\n    # ---------- Normalise the inputs ----------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-dimensional\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"y must be 1-D and have the same length as X\")\n    n_samples, n_features = X.shape\n    if n_samples == 0 or n_features == 0:          # empty data set\n        return (-1, None, 0.0)\n\n    # ---------- Encode class labels as 0 \u2026 C-1 ----------\n    classes, y_enc = np.unique(y, return_inverse=True)\n    n_classes = classes.size\n\n    # ---------- Helper : Gini of a node with counts  ----------\n    def gini(counts, n):\n        if n == 0:\n            return 0.0\n        p = counts / n\n        return 1.0 - np.sum(p * p)\n\n    # ---------- Gini of the whole data set ----------\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n    g_whole = gini(total_counts, n_samples)\n\n    # ---------- Initialise \u201cbest\u201d record ----------\n    best_gini     = np.inf\n    best_feat     = -1\n    best_th       = None\n    tol           = 1e-12      # for impurity equality test\n\n    # ---------- Examine every feature ----------\n    for j in range(n_features):\n        col = X[:, j]\n        if np.all(col == col[0]):        # constant \u21d2 no valid split\n            continue\n\n        # sort by the feature\n        order = np.argsort(col, kind='mergesort')   # stable for tie\u2010breaks\n        col_sorted = col[order]\n        y_sorted   = y_enc[order]\n\n        # cumulative class counts for the \u201cleft\u201d partition\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # iterate through the sorted samples\n        for i in range(n_samples):\n            cls = y_sorted[i]\n            left_counts[cls] += 1\n\n            # Only evaluate a split at the *last* occurrence of a value.\n            is_last_occurrence = (i == n_samples - 1) or (col_sorted[i] != col_sorted[i + 1])\n            if not is_last_occurrence:\n                continue\n\n            n_left  = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:        # should never happen here\n                continue\n\n            right_counts = total_counts - left_counts\n\n            g_left  = gini(left_counts,  n_left)\n            g_right = gini(right_counts, n_right)\n\n            g_split = (n_left  / n_samples) * g_left \\\n                    + (n_right / n_samples) * g_right\n\n            # ---------- Update the best split if necessary ----------\n            if g_split + tol < best_gini:\n                best_gini, best_feat, best_th = g_split, j, col_sorted[i]\n            elif abs(g_split - best_gini) < tol:\n                if j < best_feat:\n                    best_feat, best_th = j, col_sorted[i]\n                elif j == best_feat and col_sorted[i] < best_th:\n                    best_th = col_sorted[i]\n\n    # ---------- No valid split? ----------\n    if best_feat == -1:\n        return (-1, None, round(g_whole, 4))\n\n    return (best_feat, best_th, round(best_gini, 4))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np          # (kept twice as in the template)\n\ndef metric(name: str):\n    \"\"\"Return a distance function with integrated validation.\"\"\"\n    \n    supported = ('euclidean', 'manhattan', 'chebyshev', 'cosine')\n\n    def d(v1, v2):\n        # ------------------------------------------------------------------ #\n        # 1.  Metric name must be supported\n        # ------------------------------------------------------------------ #\n        if name not in supported:\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 2.  Type check \u2013 only list, tuple, np.ndarray are allowed\n        # ------------------------------------------------------------------ #\n        if not isinstance(v1, (list, tuple, np.ndarray)):\n            return -1\n        if not isinstance(v2, (list, tuple, np.ndarray)):\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 3.  Convert to numpy 1-D float arrays; catch non-numerical failures\n        # ------------------------------------------------------------------ #\n        try:\n            a = np.asarray(v1, dtype=float)\n            b = np.asarray(v2, dtype=float)\n        except Exception:          # non-convertible element encountered \u2026\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 4.  Shape, length, emptiness checks\n        # ------------------------------------------------------------------ #\n        if a.ndim != 1 or b.ndim != 1:          # one-dimensional only\n            return -1\n        if a.size == 0 or b.size == 0:          # non-empty\n            return -1\n        if a.size != b.size:                    # equal length\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 5.  Metric computations\n        # ------------------------------------------------------------------ #\n        if name == 'euclidean':\n            value = np.linalg.norm(a - b)\n\n        elif name == 'manhattan':\n            value = np.sum(np.abs(a - b))\n\n        elif name == 'chebyshev':\n            value = np.max(np.abs(a - b))\n\n        else:                                   # cosine distance\n            na, nb = np.linalg.norm(a), np.linalg.norm(b)\n            if na == 0 or nb == 0:              # zero-norm vectors forbidden\n                return -1\n            value = 1 - (a @ b) / (na * nb)\n\n        # ------------------------------------------------------------------ #\n        # 6.  Round & return\n        # ------------------------------------------------------------------ #\n        return round(float(value), 4)\n\n    return d\n```"}
{"task_id": 193, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    Parameters\n    ----------\n    z : int, float, list or np.ndarray\n        \u2022 Scalar (int/float/NumPy scalar) \u2013 returns a single float  \n        \u2022 1-D list / NumPy array        \u2013 returns a list of floats\n\n    Returns\n    -------\n    float or list\n        Softsign-transformed value(s), rounded to 4 decimal places.\n    \"\"\"\n    # ---------- scalar branch -------------------------------------------------\n    # np.isscalar covers Python scalars and NumPy 0-D scalars\n    if np.isscalar(z):\n        return round(z / (1 + abs(z)), 4)\n\n    # ---------- array-like branch ---------------------------------------------\n    # Convert list/array to a NumPy array (1-D expected)\n    arr = np.asarray(z, dtype=np.float64)\n\n    # Empty input \u2192 empty list\n    if arr.size == 0:\n        return []\n\n    # Element-wise softsign\n    result = arr / (1 + np.abs(arr))\n\n    # Return as a Python list with per-element rounding\n    return [round(float(x), 4) for x in result.tolist()]\n```"}
{"task_id": 194, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adagrad_update(weights: list[float],\n                   gradients: list[float],\n                   G: list[float] | None = None,\n                   learning_rate: float = 0.01,\n                   eps: float = 1e-8) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss with respect to *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple (new_weights, new_G) where\n            new_weights \u2013 list of updated parameters rounded to 6 decimals,\n            new_G       \u2013 list of updated accumulated squared gradients\n                           rounded to 6 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorised operations\n    w = np.asarray(weights, dtype=float)\n    grad = np.asarray(gradients, dtype=float)\n\n    # Sanity-check on matching dimensions\n    if w.shape != grad.shape:\n        raise ValueError(\"`weights` and `gradients` must have the same length.\")\n\n    # Initialise (or convert) the running squared\u2010gradient accumulator\n    if G is None:\n        G_prev = np.zeros_like(w)\n    else:\n        G_prev = np.asarray(G, dtype=float)\n        if G_prev.shape != w.shape:\n            raise ValueError(\"`G` must be the same length as `weights`.\")\n\n    # 1) Accumulate squared gradients\n    G_new = G_prev + grad ** 2\n\n    # 2) Adagrad parameter update\n    w_new = w - learning_rate * grad / np.sqrt(G_new + eps)\n\n    # Round results to 6 decimal places and return as Python lists\n    w_new = np.round(w_new, 6).tolist()\n    G_new = np.round(G_new, 6).tolist()\n\n    return w_new, G_new\n```"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport math\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\n\n\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a discrete set?\"\"\"\n    return hasattr(space, \"n\")\n\n\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a continuous set?\"\"\"\n    return hasattr(space, \"shape\")\n\n\ndef action_stats(\n    env: Any,\n    md_action: bool,\n    cont_action: bool,\n) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"\n    Summarise an RL environment's action space.\n\n    Parameters\n    ----------\n    env : Any\n        Environment exposing an `action_space` attribute \u00e0-la OpenAI-Gym.\n    md_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n    cont_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n\n    Returns\n    -------\n    Tuple containing\n        n_actions_per_dim : list[int | float]\n            Number of distinct actions for every dimension\n            (`np.inf` for continuous ones).\n        action_ids : list[tuple[int, ...]] | None\n            All possible discrete actions as tuples; `None` if at least one\n            dimension is continuous.\n        action_dim : int\n            Number of action dimensions.\n    \"\"\"\n    space = env.action_space\n\n    # --------------------------------------------------------------------- #\n    # 1. Collect per-dimension information\n    # --------------------------------------------------------------------- #\n    n_actions_per_dim: List[Union[int, float]] = []\n\n    # a) Composed spaces \u2013 e.g. gym.spaces.Tuple, gym.spaces.Dict -----------\n    if hasattr(space, \"spaces\"):\n        subspaces = space.spaces\n        for sub in subspaces:\n            if _is_discrete(sub):\n                n_actions_per_dim.append(int(sub.n))\n            elif _is_continuous(sub):\n                n_actions_per_dim.append(np.inf)\n            else:           # Fallback: treat as continuous\n                n_actions_per_dim.append(np.inf)\n\n        action_dim = len(subspaces)\n\n    # b) Single-container spaces ------------------------------------------\n    else:\n        if _is_discrete(space):\n            # Purely discrete, single logical dimension\n            n_actions_per_dim.append(int(space.n))\n            action_dim = 1\n\n        elif _is_continuous(space):\n            # A continuous \"vector\": every component is continuous\n            dim = int(space.shape[0]) if len(space.shape) > 0 else 1\n            n_actions_per_dim.extend([np.inf] * dim)\n            action_dim = dim\n\n        else:   # Unknown \u2013 assume one continuous dimension\n            n_actions_per_dim.append(np.inf)\n            action_dim = 1\n\n    # --------------------------------------------------------------------- #\n    # 2. Build the list of discrete actions (cartesian product)\n    # --------------------------------------------------------------------- #\n    contains_continuous = any(np.isinf(x) for x in n_actions_per_dim)\n\n    if contains_continuous:\n        action_ids = None\n    else:\n        ranges = [range(int(n)) for n in n_actions_per_dim]  # type: ignore[arg-type]\n        # Always return tuples \u2013 even for 1-D spaces (tuple length 1)\n        action_ids = [tuple(p) for p in product(*ranges)]\n\n    # --------------------------------------------------------------------- #\n    # 3. Return immutable summary\n    # --------------------------------------------------------------------- #\n    return n_actions_per_dim, action_ids, action_dim\n```"}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        One array per document. Array of document *d* has shape (N_d, T)\n        and contains the current variational parameter \u03d5 of this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the index of the *n*-th token\n        in document *d*.\n    V : int\n        Size of the vocabulary.\n\n    Returns\n    -------\n    list[list[float]]\n        (V \u00d7 T) matrix with the updated word\u2013topic probabilities.\n        Each column sums to one (\u2264 1e-4 rounding error) and every\n        number is rounded to four decimals.\n    \"\"\"\n    if not phi:                       # empty corpus \u21d2 return zeros\n        return [[0.0] * 0 for _ in range(V)]\n\n    T = phi[0].shape[1]               # number of topics\n    counts = np.zeros((V, T), dtype=np.float64)\n\n    # ---------- accumulate the expected counts ------------------------------\n    for doc_phi, doc_words in zip(phi, corpus):\n        # doc_words must be a NumPy array for np.add.at\n        words = np.asarray(doc_words, dtype=np.intp)\n        # add the whole matrix of the document at once\n        np.add.at(counts, words, doc_phi)\n\n    # which words have been observed at least once?\n    seen_words_mask = counts.sum(axis=1) > 0\n\n    # ---------- column normalisation ----------------------------------------\n    col_sums = counts.sum(axis=0)                     # shape (T,)\n    beta = np.zeros_like(counts)\n\n    non_zero_cols = col_sums > 0\n    beta[:, non_zero_cols] = counts[:, non_zero_cols] / col_sums[non_zero_cols]\n\n    # if a complete column is still zero (should be very rare) we distribute\n    # the mass uniformly over the *observed* words so that unseen rows keep 0.\n    zero_cols = ~non_zero_cols\n    if zero_cols.any():\n        observed_idx = np.where(seen_words_mask)[0]\n        if observed_idx.size == 0:            # no word occurs in the corpus\n            beta[:, zero_cols] = 1.0 / V      # fall back to a uniform column\n        else:\n            beta[observed_idx[:, None], zero_cols] = 1.0 / observed_idx.size\n\n    # ---------- round to four decimals & re-adjust tiny drift ----------------\n    beta = np.round(beta, 4)\n\n    # small numerical drift because of rounding \u2013 repair if necessary\n    for t in range(T):\n        diff = 1.0 - beta[:, t].sum()\n        if abs(diff) > 1e-4:\n            # add/subtract the missing mass to the largest (non-zero) entry\n            col = beta[:, t]\n            # pick an index that may actually carry probability mass\n            idx = int(np.argmax(col))\n            adj = np.round(diff, 4)            # keep four-decimal precision\n            new_val = np.round(col[idx] + adj, 4)\n            # guard against negative values due to rare corner cases\n            beta[idx, t] = max(new_val, 0.0)\n\n    return beta.tolist()\n```"}
{"task_id": 199, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    # Basic sanity-check (optional, but helps catch silent bugs)\n    if X.ndim != 1 or y.ndim != 1:\n        raise ValueError(\"X and y must be one-dimensional arrays.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same length.\")\n    \n    # Boolean masks for the two partitions\n    left_mask  = X < value\n    right_mask = ~left_mask          # equivalent to (X >= value)\n    \n    # Index y with the masks and convert to Python lists\n    y_left  = y[left_mask].tolist()\n    y_right = y[right_mask].tolist()\n    \n    return y_left, y_right\n```"}
{"task_id": 200, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Number\nfrom collections.abc import Iterable\n\ndef _abs_sum(x):\n    \"\"\"Recursively compute \u03a3|x\u1d62| for any nested structure.\"\"\"\n    if isinstance(x, Number):                 # plain number\n        return abs(float(x))\n    if isinstance(x, np.ndarray):             # NumPy array \u2192 fast path\n        return float(np.abs(x).sum())\n    if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n        total = 0.0\n        for item in x:\n            total += _abs_sum(item)\n        return total\n    raise TypeError(f\"Unsupported element type: {type(x)}\")\n\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights.\n        C: Non-negative regularization strength (float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    # Empty weights \u2192 0\n    if weights is None:\n        return 0.0\n\n    # Try the fast NumPy route first; fall back to recursion if it fails\n    try:\n        arr = np.asarray(weights, dtype=float)\n        if arr.size == 0:\n            return 0.0\n        base_sum = float(np.abs(arr).sum())\n    except (ValueError, TypeError):\n        base_sum = _abs_sum(weights)\n\n    penalty = C * base_sum\n    return round(penalty, 4)\n```"}
{"task_id": 201, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef max_norm(p, m=2.0, axis=0):\n    \"\"\"Clip the L2-norm of slices of *p* along *axis* so that they do not exceed *m*.\"\"\"\n    p = np.asarray(p, dtype=float)               # make sure we have a NumPy array\n    eps = 1e-12                                  # numerical-stability constant\n\n    if axis is None:                             # treat the whole tensor as one vector\n        norm = np.linalg.norm(p)\n        scale = min(1.0, m / (norm + eps))\n        p_hat = p * scale\n    else:                                        # clip independently along the chosen axis\n        # ensure negative axes work just like NumPy\u2019s default behaviour\n        axis = axis if axis >= 0 else p.ndim + axis\n\n        # compute L2 norms with the axis kept so broadcasting works\n        norms = np.linalg.norm(p, axis=axis, keepdims=True)\n        scales = np.minimum(1.0, m / (norms + eps))\n        p_hat = p * scales                       # broadcasted element-wise scaling\n\n    # round to 4 decimals and turn into nested Python lists\n    return np.round(p_hat, 4).tolist()\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _total_cost(dist_matrix: np.ndarray, medoid_idx: list[int]) -> float:\n    \"\"\"\n    Return the sum of distances of every sample to its nearest medoid.\n    \"\"\"\n    # distances to the current medoids -> shape (n_samples, k)\n    d_to_medoids = dist_matrix[:, medoid_idx]\n    # nearest distance for every sample\n    return np.min(d_to_medoids, axis=1).sum()\n\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (0 \u2026 k-1) for every sample.\n    \"\"\"\n    # ----------------------------- Checks ---------------------------------- #\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D NumPy array.\")\n    n_samples = X.shape[0]\n    if not (1 <= k <= n_samples):\n        raise ValueError(\"k must satisfy 1 \u2264 k \u2264 n_samples\")\n\n    # ---------------------- Pre-compute distances -------------------------- #\n    # Pair-wise Euclidean distance matrix (symmetric, dtype=float64)\n    # D[i, j] = ||X[i] \u2013 X[j]||\n    diff = X[:, None, :] - X[None, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=-1)\n\n    # ------------------------ Initial medoids ----------------------------- #\n    medoid_idx: list[int] = list(range(k))          # indices of current medoids\n    current_cost = _total_cost(dist_matrix, medoid_idx)\n\n    # ----------------------- Main optimisation loop ----------------------- #\n    improved = True\n    while improved:\n        best_swap = None\n        best_reduction = 0.0\n\n        non_medoid_idx = [i for i in range(n_samples) if i not in medoid_idx]\n\n        # Try every single (medoid \u2194 non-medoid) swap\n        for p_pos, p in enumerate(medoid_idx):          # position & index of medoid to replace\n            for h in non_medoid_idx:                    # candidate that may become a medoid\n                # Build the candidate medoid list: keep order, just replace p by h\n                candidate_medoid_idx = medoid_idx.copy()\n                candidate_medoid_idx[p_pos] = h\n\n                # Total cost with the candidate swap\n                cand_cost = _total_cost(dist_matrix, candidate_medoid_idx)\n                reduction = current_cost - cand_cost\n\n                # Keep the swap with the largest strictly positive reduction.\n                if reduction > best_reduction:           # \u201c>\u201d  \u2192 strictly lower cost\n                    best_reduction = reduction\n                    best_swap = (p_pos, h)               # remember position & new index\n\n        # Apply the best swap found in this iteration (if any)\n        if best_swap is not None:                        # some improvement exists\n            p_pos, h = best_swap\n            medoid_idx[p_pos] = h\n            current_cost -= best_reduction              # new lower cost\n        else:\n            improved = False                            # no further improvement possible\n\n    # ---------------------------- Labelling -------------------------------- #\n    # (n_samples, k) distances from every sample to every final medoid\n    final_dists = dist_matrix[:, medoid_idx]\n    # index (0..k-1) of the closest medoid for every sample;\n    # NumPy\u2019s argmin returns the first occurrence when ties happen \u2192 required rule.\n    labels = final_dists.argmin(axis=1).tolist()\n\n    return labels\n```"}
{"task_id": 203, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef build_alias_table(probs: list[float] | np.ndarray) -> tuple[list[float], list[int]] | int:\n    \"\"\"Build probability and alias tables for Vose\u2019s alias sampling method.\n\n    Parameters\n    ----------\n    probs : list[float] | np.ndarray\n        1-D array\u2010like of positive numbers that must sum to 1.\n\n    Returns\n    -------\n    (prob_table, alias_table) : tuple[list[float], list[int]]\n        Both lists have length N.  Values in `prob_table` are rounded\n        to four decimals.\n        If the input is invalid the function returns -1.\n    \"\"\"\n    # Convert to NumPy array of floats\n    probs = np.asarray(probs, dtype=float)\n\n    # Basic validation --------------------------------------------------------\n    if probs.ndim != 1 or probs.size == 0:        # must be 1-D and non-empty\n        return -1\n    if np.any(probs < 0):                         # no negative probabilities\n        return -1\n    if abs(probs.sum() - 1.0) > 1e-12:            # must sum to 1 within tol\n        return -1\n\n    N = probs.size\n    scaled_probs = probs * N                      # Step 1\n\n    # Step 2: split indices into small / large lists (ascending order)\n    small  = [i for i, p in enumerate(scaled_probs) if p < 1.0]\n    large  = [i for i, p in enumerate(scaled_probs) if p >= 1.0]\n\n    prob_table  = np.zeros(N, dtype=float)\n    alias_table = np.zeros(N, dtype=int)\n\n    # Step 3: main loop\n    while small and large:\n        l = small.pop()      # last element from small\n        g = large.pop()      # last element from large\n\n        prob_table[l]  = scaled_probs[l]\n        alias_table[l] = g\n\n        # Update the probability of the large index\n        scaled_probs[g] = scaled_probs[g] + scaled_probs[l] - 1.0\n\n        # Put g into the correct bucket again\n        if scaled_probs[g] < 1.0:\n            small.append(g)\n        else:\n            large.append(g)\n\n    # Step 4: whatever remains gets probability 1\n    for idx in large:\n        prob_table[idx] = 1.0\n    for idx in small:\n        prob_table[idx] = 1.0\n\n    # Round probabilities to 4 decimals and convert to Python lists\n    prob_list  = [round(float(x), 4) for x in prob_table]\n    alias_list = alias_table.tolist()\n\n    return prob_list, alias_list\n```\n\nExample\n-------\n```python\n>>> build_alias_table([0.1, 0.2, 0.7])\n([0.3, 0.6, 1.0], [2, 2, 0])\n```\n"}
{"task_id": 204, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or the special floating-point\n    value ``NaN`` (Not-a-Number).\n\n    Parameters\n    ----------\n    data : iterable\n        Any one-dimensional iterable (list, tuple, NumPy array, \u2026).\n\n    Returns\n    -------\n    int\n        Number of missing elements.  Returns 0 for an empty input.\n    \"\"\"\n    # Ensure the argument is iterable\n    try:\n        iterator = iter(data)\n    except TypeError:\n        raise TypeError(\"`data` must be an iterable\")\n\n    missing = 0\n\n    for item in iterator:\n        # 1. Explicit None\n        if item is None:\n            missing += 1\n            continue\n\n        # 2. Numeric NaN\n        #    First try math.isnan (fast for regular floats), then fall\n        #    back to numpy.isnan for NumPy scalar types.\n        try:\n            if math.isnan(item):\n                missing += 1\n                continue\n        except TypeError:\n            pass  # item isn't a real number accepted by math.isnan\n\n        try:\n            if np.isnan(item):\n                missing += 1\n        except (TypeError, ValueError):\n            # np.isnan fails on non-numeric types (e.g., strings)\n            pass\n\n    return int(missing)\n```"}
{"task_id": 205, "completion_id": 0, "completion": "```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []\n    seen = set()\n\n    for item in registry:\n        if isinstance(item, dict) and \"id\" in item:\n            env_id = item[\"id\"]\n            if env_id not in seen:\n                unique_ids.append(env_id)\n                seen.add(env_id)\n\n    return unique_ids\n```"}
{"task_id": 206, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    # 1. Convert inputs to NumPy arrays (fails fast on non-numeric data)\n    try:\n        actual_arr    = np.asarray(actual, dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (TypeError, ValueError):\n        return -1  # Non-numerical data encountered\n\n    # 2. Validate dimensionality and length\n    if (\n        actual_arr.ndim != 1 or\n        predicted_arr.ndim != 1 or\n        actual_arr.size != predicted_arr.size or\n        actual_arr.size == 0          # optional: disallow empty sequences\n    ):\n        return -1\n\n    # 3. Compute MAE\n    mae = np.mean(np.abs(actual_arr - predicted_arr))\n\n    # 4. Round to 4 decimal places and return\n    return float(np.round(mae, 4))\n```"}
{"task_id": 207, "completion_id": 0, "completion": "```python\ndef convolution_shape(img_height: int,\n                      img_width: int,\n                      filter_shape: tuple[int, int],\n                      stride: tuple[int, int],\n                      padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height  (H) of the input image.\n        img_width  (int): Width   (W) of the input image.\n        filter_shape (tuple[int, int]): (kernel_height KH, kernel_width KW).\n        stride       (tuple[int, int]): (stride_height SH, stride_width  SW).\n        padding      (tuple[int, int]): (pad_height    PH, pad_width     PW).\n\n    Returns\n    -------\n        (OH, OW): tuple[int, int]   if the configuration is valid.\n        -1:                        otherwise.\n    \"\"\"\n\n    # ------- helper predicates ---------------------------------------------\n    is_pos_int     = lambda x: isinstance(x, int) and x > 0\n    is_nonneg_int  = lambda x: isinstance(x, int) and x >= 0\n\n    # ------- unpack & basic validation --------------------------------------\n    if not (is_pos_int(img_height) and is_pos_int(img_width)):\n        return -1\n\n    if (not isinstance(filter_shape, tuple)) or len(filter_shape) != 2:\n        return -1\n    KH, KW = filter_shape\n    if not (is_pos_int(KH) and is_pos_int(KW)):\n        return -1\n\n    if (not isinstance(stride, tuple)) or len(stride) != 2:\n        return -1\n    SH, SW = stride\n    if not (is_pos_int(SH) and is_pos_int(SW)):\n        return -1\n\n    if (not isinstance(padding, tuple)) or len(padding) != 2:\n        return -1\n    PH, PW = padding\n    if not (is_nonneg_int(PH) and is_nonneg_int(PW)):\n        return -1\n\n    # ------- compute numerators --------------------------------------------\n    num_H = img_height + 2 * PH - KH\n    num_W = img_width  + 2 * PW - KW\n\n    # must be non-negative, divisible by stride and yield positive output ----\n    if num_H < 0 or num_W < 0:\n        return -1\n    if (num_H % SH) != 0 or (num_W % SW) != 0:\n        return -1\n\n    OH = num_H // SH + 1\n    OW = num_W // SW + 1\n\n    if OH <= 0 or OW <= 0:\n        return -1\n\n    return (OH, OW)\n```"}
{"task_id": 208, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one_hot_encode(labels: list[str], categories: list[str] | None = None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: A list (length N) with the categorical labels to be encoded.\n        categories: Optional list (length C) that contains the complete set\n            of allowed categories and fixes their order.  If *None*, the\n            categories are inferred from *labels* preserving the order of\n            their first appearance.\n\n    Returns\n    -------\n    list[list[int]]\n        An (N \u00d7 C) one-hot encoded matrix represented as a list of lists.\n        If *categories* is given and an unknown label is encountered, the\n        function returns **-1**.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Determine the list of categories\n    # ------------------------------------------------------------------\n    if categories is None:                           # infer from data\n        categories = []\n        seen = set()\n        for lbl in labels:                           # keep first appearance order\n            if lbl not in seen:\n                seen.add(lbl)\n                categories.append(lbl)\n    else:                                            # user-supplied list\n        categories = list(categories)                # make a (shallow) copy\n\n    # ------------------------------------------------------------------\n    # 2. Build a mapping category \u279c column index\n    # ------------------------------------------------------------------\n    cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n    C = len(categories)\n    N = len(labels)\n\n    # ------------------------------------------------------------------\n    # 3. Allocate the output matrix\n    # ------------------------------------------------------------------\n    one_hot = np.zeros((N, C), dtype=int)\n\n    # ------------------------------------------------------------------\n    # 4. Fill the matrix, checking for unknown labels\n    # ------------------------------------------------------------------\n    for row, lbl in enumerate(labels):\n        idx = cat2idx.get(lbl)\n        if idx is None:          # unknown label w.r.t. provided categories\n            return -1\n        one_hot[row, idx] = 1\n\n    # ------------------------------------------------------------------\n    # 5. Return the required Python list of lists\n    # ------------------------------------------------------------------\n    return one_hot.tolist()\n```"}
{"task_id": 209, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef analyze_bandit(payoffs: list[list[int | float]],\n                   payoff_probs: list[list[float]]\n                   ) -> tuple[list[float], int, float] | int:\n    \"\"\"Validate a multi-armed bandit description and compute the arms\u2019 expected pay-offs.\n\n    The function returns\n        (expected_values, best_arm, best_ev)\n\n    or the integer \u20131 if the input is invalid (see the doc-string in the task\n    description for the exact requirements).\n    \"\"\"\n    tol = 1e-8                       # tolerance for probability-sum check\n\n    # -------- basic structural checks ---------------------------------------\n    if not isinstance(payoffs, list) or not isinstance(payoff_probs, list):\n        return -1\n    if len(payoffs) != len(payoff_probs):\n        return -1                    # different number of arms\n\n    expected_values = []\n\n    # -------- per-arm validation & expectation computation -------------------\n    for arm_idx, (vals, probs) in enumerate(zip(payoffs, payoff_probs)):\n        # 1) length match\n        if len(vals) != len(probs) or len(vals) == 0:\n            return -1\n\n        # 2) all probabilities are finite numbers in [0, 1]\n        if not all(isinstance(p, (int, float)) and 0.0 <= p <= 1.0\n                   for p in probs):\n            return -1\n\n        # 3) probability sum\n        if abs(sum(probs) - 1.0) > tol:\n            return -1\n\n        # expected value for this arm\n        ev = float(np.dot(vals, probs))\n        expected_values.append(round(ev, 4))\n\n    # -------- find the best arm ---------------------------------------------\n    max_ev = max(expected_values)\n    best_arm = next(i for i, ev in enumerate(expected_values) if ev == max_ev)\n\n    return expected_values, best_arm, max_ev\n```\n\nExplanation of the main steps  \n1. Validation  \n   \u2022 Number of payoff/probability lists must match.  \n   \u2022 For every arm the two inner lists must have equal length and contain at least one element.  \n   \u2022 All probabilities must lie in `[0, 1]` and the sum of each arm\u2019s probabilities must be 1 within `1 e-8`.  \n   Any violation returns `-1`.\n\n2. Expectation computation  \n   The expected value of an arm is the dot product of its payoffs and their probabilities.  \n   Each expected value is rounded to four decimals immediately.\n\n3. Best arm  \n   The maximum of the rounded expected values is taken; in case of ties the smallest index is selected by taking the first occurrence.\n\n4. Return  \n   `(expected_values, best_arm, best_ev)` with all floats rounded to four decimals."}
{"task_id": 210, "completion_id": 0, "completion": "```python\nimport functools\nfrom typing import Callable, Any\n\n\ndef doublewrap(decorator_func: Callable[..., Callable[..., Any]]) -> Callable[..., Any]:\n    \"\"\"Make *decorator_func* usable with or without its own parentheses.\n\n    The returned object can therefore be written either as\n\n        @wrapped_deco\n        def func(...): ...\n\n    or\n\n        @wrapped_deco(arg, kw=val)\n        def func(...): ...\n\n    while in both cases *decorator_func* is finally invoked as\n\n        decorator_func(func, *arg, **kw)\n    \"\"\"\n\n    @functools.wraps(decorator_func)\n    def decorator_or_factory(*args, **kwargs):\n        # ------------------------------------------------------------------\n        # CASE 1 \u2013 used without parentheses: @wrapped_deco\n        # Python passes the function to decorate as *the first positional*\n        # argument and nothing else.\n        # ------------------------------------------------------------------\n        if args and callable(args[0]) and len(args) == 1 and not kwargs:\n            target_func = args[0]\n            return decorator_func(target_func)\n\n        # ------------------------------------------------------------------\n        # CASE 2 \u2013 used with its own arguments: @wrapped_deco(..., ...)\n        # Nothing (yet) is the target function.  We therefore return the\n        # *real* decorator that will be called later with that function.\n        # ------------------------------------------------------------------\n        @functools.wraps(decorator_func)\n        def real_decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n            return decorator_func(func, *args, **kwargs)\n\n        return real_decorator\n\n    return decorator_or_factory\n```"}
{"task_id": 211, "completion_id": 0, "completion": "```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    # Materialise the sequences so we can safely compute len() and iterate twice\n    actual_list     = list(actual)\n    predicted_list  = list(predicted)\n\n    # Rule-1: different lengths or empty \u2192 invalid\n    if not actual_list or len(actual_list) != len(predicted_list):\n        return -1\n\n    # Count mismatches\n    mismatches = sum(1 for a, p in zip(actual_list, predicted_list) if a != p)\n\n    # Classification error and accuracy\n    classification_error = mismatches / len(actual_list)\n    acc = 1.0 - classification_error\n\n    # Round to 4 decimals as required\n    return round(acc, 4)\n```"}
{"task_id": 212, "completion_id": 0, "completion": "```python\nfrom collections import deque\n\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Return one shortest \u2013 and lexicographically smallest \u2013 path from the\n    layer called \"input\" to the layer called \"output_realtime\".\n\n    Parameters\n    ----------\n    graph : dict[str, list[str]]\n        Directed graph given as adjacency list (layer -> list[successors]).\n\n    Returns\n    -------\n    list[str]\n        Names of the layers on the desired path or an empty list if no\n        such path exists.\n    \"\"\"\n    start = \"input\"\n    goal  = \"output_realtime\"\n\n    # Trivial corner-case ----------------------------------------------------\n    if start == goal:\n        return [start]\n\n    # Breadth-first search \u2013 keeps track of the parent of every discovered\n    # node so that the path can be reconstructed once the goal is reached.\n    queue   = deque([start])\n    visited = {start}\n    parent: dict[str, str] = {}          # child -> parent\n\n    while queue:\n        current = queue.popleft()\n\n        # Explore all outgoing neighbours in *lexicographic* order so that, for\n        # nodes on the same BFS level, the lexicographically smaller paths are\n        # visited first.  Sorting once per node is inexpensive compared to the\n        # whole search.\n        for neighbour in sorted(graph.get(current, [])):\n            if neighbour in visited:\n                continue\n\n            visited.add(neighbour)\n            parent[neighbour] = current\n\n            # Goal reached \u2013 reconstruct and return the path.\n            if neighbour == goal:\n                path = [goal]\n                while path[-1] != start:\n                    path.append(parent[path[-1]])\n                return list(reversed(path))\n\n            queue.append(neighbour)\n\n    # No path exists ---------------------------------------------------------\n    return []\n```"}
{"task_id": 213, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with the He (Kaiming) Normal strategy.\n    \n    Args:\n        shape: Desired tensor shape.\n        \n    Returns:\n        Nested Python list of weights drawn from \ud835\udca9(0, \u221a(2 / fan_in)).\n    \"\"\"\n    # ----- 1. compute fan_in (and fan_out) -----------------------------------\n    dim = len(shape)\n\n    if dim == 0:                              # scalar weight (rare)\n        fan_in = fan_out = 1\n    elif dim == 1:                            # vector\n        fan_in = fan_out = shape[0]\n    elif dim == 2:                            # linear layer (in, out)\n        fan_in, fan_out = shape[0], shape[1]\n    else:                                     # convolutional kernel\n        out_channels, in_channels = shape[0], shape[1]\n        receptive_field = np.prod(shape[2:])  # product of spatial dims\n        fan_in  = in_channels  * receptive_field\n        fan_out = out_channels * receptive_field\n\n    # Guard against degenerate case\n    if fan_in == 0:\n        return []\n\n    # ----- 2. draw samples ----------------------------------------------------\n    rng  = np.random.default_rng(seed=42)     # deterministic\n    std  = np.sqrt(2.0 / fan_in)\n    data = rng.normal(loc=0.0, scale=std, size=shape)\n\n    # ----- 3. return as nested Python list -----------------------------------\n    return data.tolist()\n```"}
{"task_id": 214, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function returns -1 if the two input sequences do not share the same\n    shape/length. Otherwise, it returns the MSE rounded to 4 decimal places.\n    Both Python lists and NumPy arrays are supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    # Convert inputs to NumPy arrays for unified processing\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n\n    # Check shape/length compatibility\n    if a.shape != p.shape:\n        return -1\n\n    # Compute MSE\n    mse = np.mean((a - p) ** 2)\n\n    # Round to four decimal places and return as a Python float\n    return float(np.round(mse, 4))\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, \n                            n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic-Net linear regression trained with batch gradient descent.\"\"\"\n    \n    # --------------- basic validation & conversion ----------------\n    try:\n        X_tr = np.asarray(x_train, dtype=float)\n        X_te = np.asarray(x_test,  dtype=float)\n        y_tr = np.asarray(y_train, dtype=float).ravel()\n    except Exception:          # non-numeric data etc.\n        return -1\n    \n    if X_tr.ndim == 1:   # allow 1-D feature vectors\n        X_tr = X_tr.reshape(-1, 1)\n    if X_te.ndim == 1:\n        X_te = X_te.reshape(-1, 1)\n        \n    if X_tr.shape[1] != X_te.shape[1] or n_iterations < 1:\n        return -1\n    \n    # --------------- polynomial expansion -------------------------\n    def poly_expansion(matrix, deg):\n        if deg <= 1:\n            return matrix.copy()\n        powers = [matrix ** p for p in range(1, deg + 1)]\n        return np.concatenate(powers, axis=1)\n    \n    X_tr_poly = poly_expansion(X_tr, degree)\n    X_te_poly = poly_expansion(X_te, degree)\n    \n    # --------------- standardisation (zero mean / unit var) -------\n    mu = X_tr_poly.mean(axis=0)\n    sigma = X_tr_poly.std(axis=0)\n    sigma[sigma == 0] = 1.0          # avoid division by zero\n    \n    X_tr_std = (X_tr_poly - mu) / sigma\n    X_te_std = (X_te_poly - mu) / sigma     # use train statistics!\n    \n    # --------------- add bias column ------------------------------\n    X_tr_std = np.hstack((np.ones((X_tr_std.shape[0], 1)), X_tr_std))\n    X_te_std = np.hstack((np.ones((X_te_std.shape[0], 1)), X_te_std))\n    \n    # --------------- gradient-descent training --------------------\n    n_samples, n_features = X_tr_std.shape\n    w = np.zeros(n_features)                 # weight vector (bias is w[0])\n    \n    l2_factor = reg_factor * (1.0 - l1_ratio)\n    l1_factor = reg_factor * l1_ratio\n    \n    for _ in range(int(n_iterations)):\n        y_hat   = X_tr_std @ w\n        error   = y_hat - y_tr\n        \n        # Gradient of mean-squared error part (1/n \u03a3 (y\u0302\u2212y)x )\n        grad = (X_tr_std.T @ error) / n_samples\n        \n        # ---- Elastic-Net penalties (do NOT regularise bias) -------\n        # L2 part\n        grad[1:] += l2_factor * w[1:]\n        # L1 part  (sub-gradient sign(w), sign(0)=0)\n        grad[1:] += l1_factor * np.sign(w[1:])\n        \n        # Parameter update\n        w -= learning_rate * grad\n    \n    # --------------- prediction on unseen data --------------------\n    y_pred = X_te_std @ w\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual    : 1-D container (list/tuple/np.ndarray) with elements +1 or \u22121.\n        predicted : 1-D container (list/tuple/np.ndarray) with raw model scores.\n\n    Returns:\n        (grad_list, hess_list, prob_list) \u2013 three ordinary Python lists,\n        each value rounded to exactly 6 decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    y = np.asarray(actual,    dtype=float)\n    z = np.asarray(predicted, dtype=float)\n\n    if y.shape != z.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same length.\")\n\n    # ------------------------------------------------------------------ #\n    # Helper: numerically stable sigmoid\n    # ------------------------------------------------------------------ #\n    def _sigmoid(x):\n        out = np.empty_like(x, dtype=float)\n\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n\n        # For non-negative x: 1 / (1 + exp(\u2212x))\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n\n        # For negative x use: exp(x) / (1 + exp(x)) to avoid overflow\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n\n        return out\n\n    # ------------------------------------------------------------------ #\n    # Core computations\n    # ------------------------------------------------------------------ #\n    prob  = _sigmoid(z)               # sigmoid(predicted)\n    grad  = y * _sigmoid(-y * z)      # actual * sigmoid(\u2212actual * predicted)\n    hess  = prob * (1.0 - prob)       # sigmoid * (1 \u2212 sigmoid)\n\n    # ------------------------------------------------------------------ #\n    # Rounding & conversion back to plain Python lists\n    # ------------------------------------------------------------------ #\n    prob  = np.round(prob,  6).tolist()\n    grad  = np.round(grad,  6).tolist()\n    hess  = np.round(hess,  6).tolist()\n\n    return grad, hess, prob\n```"}
{"task_id": 218, "completion_id": 0, "completion": "```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    # ---------- helper functions ---------- #\n    def hand_score(hand: list[int]) -> int:\n        \"\"\"Return the best score (\u226421 if possible) for this hand.\"\"\"\n        s = sum(hand)\n        # If there is at least one Ace and we can add 10 without busting,\n        # one Ace counts as 11 instead of 1 (usable ace rule).\n        if 1 in hand and s + 10 <= 21:\n            return s + 10\n        return s\n\n    def is_bust(hand: list[int]) -> bool:\n        \"\"\"True if the hand is bust (>21).\"\"\"\n        return hand_score(hand) > 21\n\n    def is_natural(hand: list[int]) -> bool:\n        \"\"\"True if the hand is a natural blackjack (exactly two cards: Ace + 10).\"\"\"\n        return len(hand) == 2 and 1 in hand and 10 in hand\n\n    # ---------- evaluate hands ---------- #\n    player_score = hand_score(player)\n    dealer_score = hand_score(dealer)\n\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n\n    # ---------- apply reward rules ---------- #\n    # 1. Player busts\n    if player_bust:\n        return -1.0\n\n    # 2. Dealer busts\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n\n    # 3. Neither busts \u2013 compare scores\n    if player_score > dealer_score:           # Player wins\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:           # Dealer wins\n        return -1.0\n\n    # 4. Draw\n    return 0.0\n```"}
{"task_id": 220, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef estimate_lda_params(C_wt, C_dt, alpha, beta):\n    \"\"\"Estimate the word-topic (phi) and document-topic (theta) \n    distributions for an LDA model with symmetric Dirichlet smoothing.\n\n    Parameters\n    ----------\n    C_wt : numpy.ndarray\n        Word\u2013topic count matrix  (V \u00d7 T).\n    C_dt : numpy.ndarray\n        Document\u2013topic count matrix (D \u00d7 T).\n    alpha : float\n        Symmetric Dirichlet prior for theta.\n    beta  : float\n        Symmetric Dirichlet prior for phi.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        (phi, theta) \u2013 both rounded to 4 decimals and given \n        as ordinary Python nested lists.  If a count matrix is\n        empty, the corresponding output is an empty list.\n    \"\"\"\n    \n    # --- make sure we have numpy arrays ------------------------------------\n    C_wt = np.asarray(C_wt)\n    C_dt = np.asarray(C_dt)\n\n    # ---------- \u03d5 : word\u2013topic ---------------------------------------------\n    if C_wt.size == 0:                # empty matrix \u2192 empty output\n        phi = []\n    else:\n        V, T = C_wt.shape\n        topic_totals = C_wt.sum(axis=0) + V * beta          # length T\n        phi_mat = (C_wt + beta) / topic_totals              # broadcasting\n        phi = np.round(phi_mat, 4).tolist()\n\n    # ---------- \u03b8 : document\u2013topic -----------------------------------------\n    if C_dt.size == 0:\n        theta = []\n    else:\n        D, T_ = C_dt.shape          # T_ should equal T, but we don\u2019t assume\n        doc_totals = C_dt.sum(axis=1, keepdims=True) + T_ * alpha\n        theta_mat = (C_dt + alpha) / doc_totals\n        theta = np.round(theta_mat, 4).tolist()\n\n    return phi, theta\n```"}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : np.ndarray\n        Arrays of the same shape containing non-negative numbers.\n\n    Returns\n    -------\n    np.ndarray\n        (log1p(actual) - log1p(predicted))**2 computed element-wise.\n    \"\"\"\n    return np.square(np.log1p(actual) - np.log1p(predicted))\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        True target values (must be non-negative).\n    predicted : list | tuple | np.ndarray\n        Predicted values (must be non-negative and same shape as `actual`).\n\n    Returns\n    -------\n    float\n        MSLE rounded to 4 decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    # --- Convert to NumPy arrays ------------------------------------------------\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr   = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1  # conversion failed (e.g., non-numeric input)\n\n    # --- Validation -------------------------------------------------------------\n    if actual_arr.shape != pred_arr.shape:          # shape / length check\n        return -1\n    if actual_arr.size == 0:                        # empty input\n        return -1\n    if np.isnan(actual_arr).any() or np.isnan(pred_arr).any():\n        return -1                                   # NaNs are invalid\n    if np.any(actual_arr < 0) or np.any(pred_arr < 0):\n        return -1                                   # negatives not allowed\n\n    # --- MSLE computation -------------------------------------------------------\n    sq_log_err = squared_log_error(actual_arr, pred_arr)\n    msle = float(np.mean(sq_log_err))\n\n    return round(msle, 4)\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    remains numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: If `z` is a scalar, the sigmoid value rounded to 4\n        decimals (float).  Otherwise, a Python list with the same nested\n        structure where every element is rounded to 4 decimals.\n    \"\"\"\n    # ----- scalar case ------------------------------------------------------\n    if np.isscalar(z):\n        z = float(z)\n        if z >= 0:\n            val = 1.0 / (1.0 + np.exp(-z))\n        else:                       # rewrite to avoid overflow when z << 0\n            exp_z = np.exp(z)\n            val  = exp_z / (1.0 + exp_z)\n        return round(val, 4)\n\n    # ----- array-like case ---------------------------------------------------\n    arr = np.asarray(z, dtype=float)\n\n    # allocate output array\n    out = np.empty_like(arr)\n\n    # boolean masks for positive / negative elements\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    # stable computation for z >= 0\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n\n    # stable computation for z < 0 using the alternative form\n    exp_vals = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_vals / (1.0 + exp_vals)\n\n    # round and convert back to a (nested) Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky-ReLU activation to every element in *z*.\n\n    Args:\n        z:  A scalar, (nested) list or ``numpy.ndarray`` of numbers.\n        a:  Slope used for the negative part, a float in the half-open\n            interval [0, 1).  Defaults to 0.01.\n\n    Returns\n    -------\n    Same \u201cshape\u201d as *z* but converted to plain Python objects\n    (i.e. lists / scalars).  For array or list inputs the function\n    returns a (possibly nested) list; for a scalar input it returns a\n    scalar.\n    \"\"\"\n    # Basic validation of `a`\n    if not (0.0 <= a < 1.0):\n        raise ValueError(\"Parameter 'a' must lie in the interval [0, 1).\")\n\n    # Convert the input to a NumPy array (no data copying if `z` is\n    # already an ndarray)\n    arr = np.asarray(z)\n\n    # Vectorised Leaky ReLU: x if x >= 0 else a*x\n    activated = np.where(arr >= 0, arr, a * arr)\n\n    # Convert the NumPy result back to pure Python objects.\n    # `.tolist()` keeps the original nesting and for a 0-d array\n    # returns a plain Python scalar, which satisfies the \u201csame shape\u201d\n    # requirement for scalar inputs.\n    return activated.tolist()\n```"}
{"task_id": 225, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str = \"htk\") -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use:\n        \u2022 \"htk\"    \u2013 2595 * log10(1 + hz / 700)\n        \u2022 \"slaney\" \u2013 1127 * ln   (1 + hz / 700)\n        Default is \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n\n    Raises\n    ------\n    AssertionError\n        If *formula* is not \"htk\" or \"slaney\".\n    \"\"\"\n    # Ensure the requested formula is supported\n    assert formula in {\"htk\", \"slaney\"}, \\\n        \"formula must be either 'htk' or 'slaney'\"\n    \n    # Convert input to a NumPy array without altering its shape\n    hz_arr = np.asarray(hz, dtype=float)\n    \n    if formula == \"htk\":\n        mel = 2595.0 * np.log10(1.0 + hz_arr / 700.0)\n    else:  # \"slaney\"\n        mel = 1127.0 * np.log(1.0 + hz_arr / 700.0)\n    \n    # Round to four decimal places and convert back to Python list\n    return np.round(mel, 4).tolist()\n```\n\nExample\n--------\n```python\nimport numpy as np\nprint(hz2mel(np.array([0, 6300]), formula=\"htk\"))\n# Output: [0.0, 2595.0]\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _stump_predict(column: np.ndarray, threshold: float, polarity: int) -> np.ndarray:\n    \"\"\"\n    Helper \u2013 returns the predictions of one decision stump for a single feature column.\n    \"\"\"\n    if polarity == 1:          # values < threshold  -> -1\n        return np.where(column < threshold, -1, 1)\n    else:                      # polarity == -1: values < threshold ->  1\n        return np.where(column < threshold, 1, -1)\n\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test:  np.ndarray,\n    n_clf:   int = 5\n) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    \n    n_samples, n_features = X_train.shape\n\n    # ------------------------------------------------------------------\n    # Initial (uniform) sample weights\n    # ------------------------------------------------------------------\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # Each element of `ensemble` is (feature_index, threshold, polarity, alpha)\n    ensemble: list[tuple[int, float, int, float]] = []\n\n    # ------------------------------------------------------------------\n    # Boosting rounds\n    # ------------------------------------------------------------------\n    for _ in range(n_clf):\n        best_feat       = 0\n        best_thresh     = 0.0\n        best_polarity   = 1\n        best_error      = float(\"inf\")\n        best_pred_train = None\n\n        # --------------------------------------------------------------\n        # Search best stump w.r.t. weighted error\n        # --------------------------------------------------------------\n        for feat_idx in range(n_features):\n            col = X_train[:, feat_idx]\n            thresholds = np.unique(col)              # candidate splits\n\n            for thr in thresholds:\n                # polarity  1  :  x < thr -> -1\n                pred = _stump_predict(col, thr, 1)\n                err  = np.sum(w[pred != y_train])\n                if err < best_error:\n                    best_error, best_feat, best_thresh, best_polarity = err, feat_idx, thr, 1\n                    best_pred_train = pred\n\n                # polarity -1 :  x < thr ->  1\n                pred = _stump_predict(col, thr, -1)\n                err  = np.sum(w[pred != y_train])\n                if err < best_error:\n                    best_error, best_feat, best_thresh, best_polarity = err, feat_idx, thr, -1\n                    best_pred_train = pred\n\n        # --------------------------------------------------------------\n        # Compute learner weight (alpha) and update sample weights\n        # --------------------------------------------------------------\n        eps   = 1e-10                          # avoids div-by-zero\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + eps))\n\n        # w_i <- w_i * exp(-alpha y_i h_i(x_i))\n        w *= np.exp(-alpha * y_train * best_pred_train)\n        w /= np.sum(w)                         # normalise\n\n        ensemble.append((best_feat, best_thresh, best_polarity, alpha))\n\n    # ------------------------------------------------------------------\n    # Predict unseen samples\n    # ------------------------------------------------------------------\n    agg = np.zeros(X_test.shape[0])\n\n    for feat_idx, thr, pol, alpha in ensemble:\n        preds = _stump_predict(X_test[:, feat_idx], thr, pol)\n        agg  += alpha * preds\n\n    final_preds = np.sign(agg)\n    final_preds[final_preds == 0] = 1          # replace 0 with positive class\n\n    return final_preds.astype(int).tolist()\n```"}
{"task_id": 227, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef value_network_forward(state: list[float],\n                          W1: list[list[float]],\n                          W2: list[list[float]],\n                          W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\"\"\"\n    try:\n        # Convert inputs to numpy arrays\n        s  = np.asarray(state, dtype=float)\n        W1 = np.asarray(W1 , dtype=float)\n        W2 = np.asarray(W2 , dtype=float)\n        W3 = np.asarray(W3 , dtype=float)\n    except Exception:       # conversion failed (e.g., ragged lists)\n        return -1\n\n    # ---------- Dimension checks ----------\n    if s.ndim != 1:                     # state must be 1-D\n        return -1\n    N = s.shape[0]\n\n    if W1.ndim != 2 or W1.shape[0] != N:\n        return -1\n    H1 = W1.shape[1]\n\n    if W2.ndim != 2 or W2.shape[0] != H1:\n        return -1\n    H2 = W2.shape[1]\n\n    if   W3.ndim == 2 and W3.shape != (H2, 1):\n        return -1\n    elif W3.ndim == 1 and W3.shape[0] != H2:\n        return -1\n    elif W3.ndim not in (1, 2):\n        return -1\n\n    # ---------- Forward pass ----------\n    h1 = np.tanh(s   @ W1)       # shape: (H1,)\n    h2 = np.tanh(h1  @ W2)       # shape: (H2,)\n    v  = float(h2 @ W3.squeeze())  # scalar\n\n    # ---------- Output ----------\n    return round(v, 4)\n```"}
{"task_id": 228, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the pair-wise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If ``Y`` is None the distances inside X itself are computed instead.\n    The distances are rounded to four decimal places and returned as a\n    pure Python nested list.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array of shape (N, C).\n    Y : np.ndarray | None, optional\n        Array of shape (M, C).  If None, Y is taken to be X.\n\n    Returns\n    -------\n    list[list[float]]\n        The (N \u00d7 M) matrix of pair-wise Euclidean distances.\n    \"\"\"\n    # If Y is not provided, compare X with itself\n    if Y is None:\n        Y = X\n\n    # Basic shape checking\n    if X.ndim != 2 or Y.ndim != 2:\n        raise ValueError(\"X and Y must be 2-D arrays.\")\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\"X and Y must have the same number of columns (features).\")\n\n    # Squared norms of each row (column vectors for broadcasting)\n    x2 = np.sum(X ** 2, axis=1, keepdims=True)        # (N, 1)\n    y2 = np.sum(Y ** 2, axis=1, keepdims=True).T      # (1, M)\n\n    # \u2016x \u2212 y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y\n    d2 = x2 + y2 - 2.0 * X @ Y.T\n\n    # Numerical safety: any negative tiny values -> 0\n    d2 = np.maximum(d2, 0.0)\n\n    # Euclidean distances\n    D = np.sqrt(d2)\n\n    # Round to 4 decimals and convert to Python lists\n    return np.round(D, 4).tolist()\n```"}
{"task_id": 230, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train, y_train, X_test):\n    \"\"\"\n    Gaussian Naive Bayes classifier (binary) implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : ndarray, shape (n_samples, n_features)\n    y_train : ndarray, shape (n_samples,)       \u2013 labels 0 / 1\n    X_test  : ndarray, shape (m_samples, n_features)\n\n    Returns\n    -------\n    list[int]  \u2013 predicted labels for X_test\n    \"\"\"\n    eps = 1e-9                            # for numerical stability\n    classes = np.array([0, 1])            # we know the two classes in advance\n    n_features = X_train.shape[1]\n    \n    # ------------------------------------------------------------------\n    # 1. Estimate per\u2013class means, variances, and the priors\n    # ------------------------------------------------------------------\n    means      = np.zeros((2, n_features))\n    variances  = np.zeros((2, n_features))\n    priors     = np.zeros(2)\n    \n    n_samples = len(y_train)\n    \n    for idx, c in enumerate(classes):\n        X_c = X_train[y_train == c]\n        \n        if X_c.size == 0:                 # class not present \u2013 still keep shapes valid\n            means[idx]     = 0.0\n            variances[idx] = 1.0\n            priors[idx]    = eps\n            continue\n        \n        means[idx]     = X_c.mean(axis=0)\n        variances[idx] = X_c.var(axis=0) + eps   # population var (+eps so it never hits 0)\n        priors[idx]    = X_c.shape[0] / n_samples\n    \n    priors[priors == 0] = eps            # guard log(0) in extremely unbalanced data\n    \n    # ------------------------------------------------------------------\n    # 2. Vectorised computation of log-posteriors for every test sample\n    # ------------------------------------------------------------------\n    # Shapes used:\n    #   means         : (2,  n_features)\n    #   variances     : (2,  n_features)\n    #   X_test        : (m,  n_features)\n    #   diff          : (m, 2, n_features)\n    #   log_likelihood: (m, 2, n_features)\n    diff = X_test[:, None, :] - means[None, :, :]\n    \n    log_likelihood = -0.5 * ( np.log(2.0 * np.pi * variances)[None, :, :]\n                              + (diff ** 2) / variances[None, :, :] )\n    \n    log_posteriors = np.sum(log_likelihood, axis=2) + np.log(priors)[None, :]\n    \n    # ------------------------------------------------------------------\n    # 3. Class with larger log-posterior wins\n    # ------------------------------------------------------------------\n    predictions = np.argmax(log_posteriors, axis=1)\n    \n    return predictions.tolist()\n```"}
{"task_id": 231, "completion_id": 0, "completion": "```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"\n    Determine whether a given two-card blackjack hand is a natural (blackjack).\n\n    A natural hand must satisfy BOTH conditions:\n        1. It contains exactly two cards.\n        2. Those two cards consist of one Ace (encoded as 1) and one ten-valued card\n           (10, J, Q, or K \u2013 all encoded as 10).\n\n    Parameters\n    ----------\n    hand : list[int]\n        A list with the numerical ranks of the cards (Ace \u2192 1, 10/J/Q/K \u2192 10).\n\n    Returns\n    -------\n    bool\n        True  \u2013 if `hand` is exactly [1, 10] in any order.\n        False \u2013 for every other combination/length.\n    \"\"\"\n    # Must be precisely two cards and, when converted to a set, equal {1, 10}.\n    return len(hand) == 2 and set(hand) == {1, 10}\n```"}
{"task_id": 232, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    # 1. Must be a 2-D NumPy array with at least one row and one column\n    if not isinstance(x, np.ndarray) or x.ndim != 2:\n        return False\n    if x.shape[0] == 0 or x.shape[1] == 0:\n        return False\n\n    # 2. All entries must be exactly 0 or 1\n    #    np.isin works for any numeric dtype (ints, floats, bools, etc.)\n    if not np.isin(x, (0, 1)).all():\n        return False\n\n    # 3. Each row must contain exactly one 1\n    if not (x.sum(axis=1) == 1).all():\n        return False\n\n    return True\n```"}
{"task_id": 234, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta:  float = 1.0,\n                                    seed: int | None = None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\"\"\"\n    # ---------------------------------------------------\n    # Basic input checks\n    # ---------------------------------------------------\n    if (not isinstance(successes, (list, tuple)) or\n        not isinstance(failures,  (list, tuple)) or\n        len(successes) == 0 or\n        len(successes) != len(failures)):\n        return -1\n    \n    # Convert to NumPy arrays (will also raise if elements are not numeric)\n    try:\n        succ_arr = np.asarray(successes, dtype=float)\n        fail_arr = np.asarray(failures,  dtype=float)\n    except Exception:\n        return -1\n    \n    # Counts must be non-negative\n    if (succ_arr < 0).any() or (fail_arr < 0).any():\n        return -1\n    \n    # ---------------------------------------------------\n    # Optional seeding for reproducibility\n    # ---------------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # ---------------------------------------------------\n    # Posterior parameters for every arm\n    # ---------------------------------------------------\n    alpha = prior_alpha + succ_arr\n    beta  = prior_beta  + fail_arr\n    \n    # ---------------------------------------------------\n    # Thompson sampling draw & argmax\n    # ---------------------------------------------------\n    samples = np.random.beta(alpha, beta)\n    chosen_arm = int(np.argmax(samples))   # tie breaks to smallest index\n    return chosen_arm\n```"}
{"task_id": 235, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (kept to respect stub, though the duplicate is harmless)\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as ``labels.max() + 1``.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n        number of labels and C is the number of classes.  Each row is a\n        one-hot vector corresponding to the associated label.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Basic validation\n    # ------------------------------------------------------------------\n    if labels.ndim != 1:\n        raise ValueError(\"`labels` must be a 1-D array.\")\n\n    # Make sure the array is of (signed/unsigned) integer type\n    if not np.issubdtype(labels.dtype, np.integer):\n        raise ValueError(\"`labels` must contain integer values.\")\n\n    if labels.size == 0:\n        # With no samples we can only proceed when n_classes is supplied.\n        if n_classes is None:\n            raise ValueError(\"Cannot infer number of classes from an empty array.\")\n        return np.zeros((0, n_classes), dtype=float)\n\n    # ------------------------------------------------------------------\n    # 2. Determine number of classes (C)\n    # ------------------------------------------------------------------\n    max_label = int(labels.max())\n    if max_label < 0:\n        raise ValueError(\"Negative class labels are not allowed.\")\n\n    if n_classes is None:\n        C = max_label + 1\n    else:\n        if n_classes < max_label + 1:\n            raise ValueError(\n                \"`n_classes` ({}) is smaller than max label + 1 ({}).\"\n                .format(n_classes, max_label + 1)\n            )\n        C = int(n_classes)\n\n    # ------------------------------------------------------------------\n    # 3. Build the one-hot matrix\n    # ------------------------------------------------------------------\n    N = labels.shape[0]\n    one_hot = np.zeros((N, C), dtype=float)\n    one_hot[np.arange(N), labels] = 1.0\n\n    return one_hot\n```"}
{"task_id": 236, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef minibatch(X: np.ndarray, batchsize: int = 256, shuffle: bool = True):\n    \"\"\"Create a generator that yields index mini-batches.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (N, *). The first axis holds the samples.\n    batchsize : int, default = 256\n        Desired mini-batch size. The last batch may be smaller.\n    shuffle : bool, default = True\n        Whether to shuffle sample indices before splitting.\n\n    Returns\n    -------\n    tuple\n        (mb_generator, n_batches)\n        mb_generator : generator yielding NumPy integer arrays \u2013 the indices\n            of each mini-batch in the order they are processed.\n        n_batches : int \u2013 total number of mini-batches.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Basic argument checks\n    if batchsize <= 0:\n        raise ValueError(\"`batchsize` must be a positive integer.\")\n    if X.ndim == 0:\n        raise ValueError(\"`X` must have at least one dimension (samples).\")\n    # ------------------------------------------------------------------\n    # Total number of samples\n    N = int(X.shape[0])\n\n    # Pre-compute number of mini-batches (ceiling division)\n    n_batches = (N + batchsize - 1) // batchsize   # == ceil(N / batchsize)\n\n    # Build an array of indices and shuffle in-place if requested\n    indices = np.arange(N, dtype=np.int64)\n    if shuffle:\n        # In-place random permutation\n        np.random.shuffle(indices)\n\n    # ------------------------------------------------------------------\n    # Inner generator (single\u2013pass)\n    def _generator():\n        for start in range(0, N, batchsize):\n            yield indices[start:start + batchsize]\n\n    # Return a *generator object* and the batch count\n    return _generator(), n_batches\n```"}
{"task_id": 237, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    total = 0          # running total assuming every Ace is 11\n    aces = 0           # number of Aces seen\n    \n    for card in hand:\n        if card == 'A':\n            aces += 1\n            total += 11            # start by counting Ace as 11\n        elif card in {'J', 'Q', 'K'}:\n            total += 10\n        else:                      # '2'\u2013'10'\n            total += int(card)\n\n    # Downgrade Aces from 11 to 1 as needed to avoid busting\n    while total > 21 and aces:\n        total -= 10                # 11 \u2192 1 means subtract 10\n        aces -= 1\n\n    return total if total <= 21 else 0\n```"}
{"task_id": 239, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import Tuple\n\ndef determine_padding(\n    filter_shape: Tuple[int, int],\n    output_shape: str = \"same\",\n) -> Tuple[Tuple[int, int], Tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution\n    with stride 1.\n\n    Parameters\n    ----------\n    filter_shape : (int, int)\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : {\"same\", \"valid\"}, default=\"same\"\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input.\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    ((int, int), (int, int))\n        ((pad_top, pad_bottom), (pad_left, pad_right))\n    \"\"\"\n    # --- \"valid\" convolution needs no padding ---------------------------------\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n\n    # --- \"same\" convolution (stride = 1) --------------------------------------\n    if output_shape == \"same\":\n        fh, fw = filter_shape\n\n        # total padding required in each dimension\n        total_pad_h = fh - 1      # height direction\n        total_pad_w = fw - 1      # width  direction\n\n        # split it into \"before\" (top/left) and \"after\" (bottom/right);\n        # put the smaller part on the top/left if an odd number is needed.\n        pad_top    = total_pad_h // 2\n        pad_bottom = total_pad_h - pad_top    # == ceil(total_pad_h / 2)\n\n        pad_left   = total_pad_w // 2\n        pad_right  = total_pad_w - pad_left   # == ceil(total_pad_w / 2)\n\n        return ((pad_top, pad_bottom), (pad_left, pad_right))\n\n    # --------------------------------------------------------------------------\n    raise ValueError(\"output_shape must be either 'same' or 'valid'\")\n```"}
{"task_id": 240, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef create_encoding_dicts(obs_ids: list,\n                          action_ids: list,\n                          obs_encoder: dict | None = None,\n                          act_encoder: dict | None = None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns\n    -------\n        dict\n            A dictionary that contains the keys 'obs2num', 'num2obs',\n            'act2num', and 'num2act'.\n    \"\"\"\n\n    def _build_maps(items: list, given_encoder: dict | None):\n        \"\"\"\n        Helper that creates a forward dictionary (sym\u2192int) and its inverse\n        (int\u2192sym) either from a supplied encoder or by enumerating *items*.\n        \"\"\"\n        # Use supplied encoder if present, otherwise enumerate items\n        if given_encoder is not None:\n            forward = dict(given_encoder)        # defensive copy\n        else:\n            forward = {item: idx for idx, item in enumerate(items)}\n\n        # Its inverse\n        inverse = {v: k for k, v in forward.items()}\n        return forward, inverse\n\n    # Observation mappings\n    obs2num, num2obs = _build_maps(obs_ids, obs_encoder) \\\n        if (obs_ids or obs_encoder) else ({}, {})\n\n    # Action mappings\n    act2num, num2act = _build_maps(action_ids, act_encoder) \\\n        if (action_ids or act_encoder) else ({}, {})\n\n    # --- Sanity checks: guarantee internal consistency --------------------\n    for obs in obs2num:\n        assert num2obs[obs2num[obs]] == obs, \"Inconsistent obs dictionaries\"\n    for act in act2num:\n        assert num2act[act2num[act]] == act, \"Inconsistent action dictionaries\"\n\n    return {\n        'obs2num': obs2num,\n        'num2obs': num2obs,\n        'act2num': act2num,\n        'num2act': num2act\n    }\n```"}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape. Must have length \u2265 2.\n        scale: Multiplicative scaling factor (default 0.5).\n\n    Returns:\n        A (possibly nested) python list whose first two dimensions are\n        orthogonal up to the given scaling factor, or \u20131 if `shape` is 1-D.\n    \"\"\"\n    # the algorithm only makes sense for matrices / higher-order tensors\n    if len(shape) < 2:\n        return -1\n\n    # ------------------------------------------------------------------\n    # 1. draw a random matrix  A \u2208 \u211d^{d0 \u00d7 (d1\u22efdn)}  with i.i.d. \ud835\udca9(0,1)\n    # ------------------------------------------------------------------\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    A = np.random.standard_normal((d0, d_rest))\n\n    # ------------------------------------------------------------------\n    # 2. full SVD (reduced form)     A = U \u03a3 V\u1d40\n    # ------------------------------------------------------------------\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # ------------------------------------------------------------------\n    # 3. pick the factor that has the same shape as A\n    # ------------------------------------------------------------------\n    Q = U if U.shape == A.shape else Vt\n\n    # ------------------------------------------------------------------\n    # 4. reshape back to the requested tensor shape and scale it\n    # ------------------------------------------------------------------\n    tensor = (scale * Q.reshape(shape)).tolist()\n    return tensor\n```"}
{"task_id": 242, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List\n\ndef ucb1_select(ev_estimates: List[float], pull_counts: List[int],\n                current_step: int, C: float = 1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    \n    # --- basic sanity checks (light-weight, no heavy validation) ----------\n    if len(ev_estimates) != len(pull_counts):\n        raise ValueError(\"`ev_estimates` and `pull_counts` must have the same length\")\n    if len(ev_estimates) == 0:\n        raise ValueError(\"At least one arm is required\")\n    \n    # 1) Step-1 of UCB1: play every arm at least once\n    for idx, pulls in enumerate(pull_counts):\n        if pulls == 0:\n            return idx\n    \n    # 2) All arms have been tried \u2192 compute UCB1 score\n    log_term = math.log(current_step + 1) * 2.0          # 2 * ln(T)\n    \n    best_idx   = 0\n    best_score = float(\"-inf\")\n    \n    for idx, (mean, pulls) in enumerate(zip(ev_estimates, pull_counts)):\n        exploration = C * math.sqrt(log_term / pulls)\n        score       = mean + exploration\n        \n        if score > best_score:          # strict > keeps first max (smallest idx)\n            best_score = score\n            best_idx   = idx\n    \n    return best_idx\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a tiny fully-connected Actor\u2013Critic network\n    whose weights are all ones and whose biases are all zeros.\n\n    Args:\n        state: List with three floats \u2013 the environment state.\n\n    Returns:\n        (action_probabilities, state_value) where\n            action_probabilities \u2013 list with three floats, each rounded to 4 decimals\n            state_value          \u2013 single float, rounded to 4 decimals\n    \"\"\"\n    # ----- shared feature extractor -----\n    s_sum = np.sum(state)              # s\u2080 + s\u2081 + s\u2082\n    z1 = max(0.0, s_sum)               # ReLU\n    z3 = 16.0 * z1                     # after three hidden layers (see task description)\n\n    # ----- actor head -----\n    # All three logits are identical (= 4\u00b7z\u2083 = 64\u00b7z\u2081), therefore soft-max \u21d2 uniform distribution.\n    action_probs = [round(1 / 3, 4)] * 3\n\n    # ----- critic head -----\n    state_value = round(64.0 * z1, 4)\n\n    return action_probs, state_value\n```"}
{"task_id": 244, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Real\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    Parameters\n    ----------\n    shape : tuple\n        Desired tensor shape (tuple of positive integers).\n    sparsity : float, optional\n        Probability that an entry is 1.0 (must lie in the interval [0,\u20061]).\n\n    Returns\n    -------\n    list\n        Nested Python list containing only 0.0 and 1.0, or -1 on invalid input.\n    \"\"\"\n    # ----------------- argument validation -----------------\n    # shape must be a non-empty tuple of positive integers\n    if (not isinstance(shape, tuple) or\n        len(shape) == 0 or\n        not all(isinstance(dim, int) and dim > 0 for dim in shape)):\n        return -1\n\n    # sparsity must be a real number in the closed interval [0, 1]\n    if not isinstance(sparsity, Real) or not (0.0 <= sparsity <= 1.0):\n        return -1\n\n    # ----------------- tensor generation -------------------\n    # Draw U(0,1) samples with np.random.rand and threshold\n    threshold = 1.0 - sparsity\n    random_draws = np.random.rand(*shape)\n    binary_tensor = (random_draws >= threshold).astype(float)\n\n    # ----------------- return as nested Python list --------\n    return binary_tensor.tolist()\n```"}
{"task_id": 247, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]],\n                           payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Return the highest expected reward (rounded to 4 decimals) and its arm\n    index for a Gaussian bandit, or -1 if the input is invalid.\"\"\"\n    \n    # ---------- basic list validation ----------\n    if not isinstance(payoff_dists, list) or not isinstance(payoff_probs, list):\n        return -1\n    K = len(payoff_dists)\n    if K == 0 or len(payoff_probs) != K:\n        return -1\n\n    # ---------- convert to numpy & structural checks ----------\n    try:\n        dists_arr = np.asarray(payoff_dists, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n\n    # Expect shape (K, 2)   -> (\u03bc_k , \u03c3_k\u00b2)\n    if dists_arr.ndim != 2 or dists_arr.shape[1] != 2 or dists_arr.shape[0] != K:\n        return -1\n\n    mus   = dists_arr[:, 0]\n    vars_ = dists_arr[:, 1]\n\n    # ---------- probabilities ----------\n    try:\n        probs = np.asarray(payoff_probs, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n\n    # ---------- semantic checks ----------\n    if np.any(vars_ <= 0) or np.any(probs < 0) or np.any(probs > 1):\n        return -1\n\n    # ---------- expected values ----------\n    exp_vals = mus * probs           # \ud835\udd3c[R_k] = p_k \u00b7 \u03bc_k (all-NumPy)\n\n    # ---------- find best ----------\n    best_arm = int(np.argmax(exp_vals))          # first max in case of ties\n    best_ev  = round(float(exp_vals[best_arm]), 4)\n\n    return best_ev, best_arm\n```"}
{"task_id": 248, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # kept to match the given stub\n\ndef baseline_prob_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray\n) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Unused by this baseline; included only for API completeness.\n    y_train : np.ndarray\n        One-dimensional array of binary labels (0 = negative, 1 = positive).\n    X_test : np.ndarray\n        Feature matrix whose rows require a probability prediction.\n\n    Returns\n    -------\n    list[float]\n        List of identical, 4-decimal-rounded probabilities.\n    \"\"\"\n    # Guard against empty y_train to avoid division-by-zero.\n    if y_train.size == 0:\n        positive_fraction = 0.0\n    else:\n        positive_fraction = y_train.sum() / len(y_train)\n\n    # Round to 4 decimal places.\n    positive_fraction = round(float(positive_fraction), 4)\n\n    # If X_test has zero rows, return an empty list.\n    n_rows_test = len(X_test)\n    if n_rows_test == 0:\n        return []\n\n    # Create and return a list filled with the constant probability.\n    return [positive_fraction] * n_rows_test\n```"}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _dense(x: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A small helper that applies a fully-connected layer to a 1-D input vector.\n    It copes with both common weight layouts:\n        \u2022  (in_dim , out_dim)   so   x @ w\n        \u2022  (out_dim, in_dim )   so   w @ x\n    \"\"\"\n    # try   x @ W   first (Keras / TF layout)\n    if x.shape[-1] == w.shape[0]:\n        return x @ w + b\n    # otherwise fall back to   W @ x   (PyTorch layout)\n    if x.shape[-1] == w.shape[1]:\n        return w @ x + b\n\n    raise ValueError(\"Weight matrix has incompatible shape.\")\n\n\ndef _relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\n\n\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    z = z - np.max(z)            # numerical stability\n    e = np.exp(z)\n    return e / e.sum()\n\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Convert input state and every parameter to NumPy arrays\n    # ------------------------------------------------------------------ #\n    x = np.asarray(state, dtype=np.float64)\n\n    W1 = np.asarray(params[\"W1\"], dtype=np.float64)\n    b1 = np.asarray(params[\"b1\"], dtype=np.float64)\n\n    W2 = np.asarray(params[\"W2\"], dtype=np.float64)\n    b2 = np.asarray(params[\"b2\"], dtype=np.float64)\n\n    W3 = np.asarray(params[\"W3\"], dtype=np.float64)\n    b3 = np.asarray(params[\"b3\"], dtype=np.float64)\n\n    Wa = np.asarray(params[\"Wa\"], dtype=np.float64)\n    ba = np.asarray(params[\"ba\"], dtype=np.float64)\n\n    Wc = np.asarray(params[\"Wc\"], dtype=np.float64)\n    bc = np.asarray(params[\"bc\"], dtype=np.float64)\n\n    # ------------------------------------------------------------------ #\n    # Shared trunk: three Dense layers with ReLU activations\n    # ------------------------------------------------------------------ #\n    x = _relu(_dense(x, W1, b1))\n    x = _relu(_dense(x, W2, b2))\n    hidden = _relu(_dense(x, W3, b3))          # final shared representation\n\n    # ------------------------------------------------------------------ #\n    # Actor head: logits \u2192 soft-max probabilities\n    # ------------------------------------------------------------------ #\n    logits = _dense(hidden, Wa, ba)\n    probs = _softmax(logits)\n\n    # ------------------------------------------------------------------ #\n    # Critic head: single scalar value\n    # ------------------------------------------------------------------ #\n    value = float(_dense(hidden, Wc, bc))       # ensure plain Python float\n\n    # ------------------------------------------------------------------ #\n    # Rounding to four decimal places\n    # ------------------------------------------------------------------ #\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n\n    return probs_rounded, value_rounded\n```"}
{"task_id": 251, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# --------------------------------------------------------------------------- #\n# Helper(s)\n# --------------------------------------------------------------------------- #\ndef _is_scalar(x):\n    \"\"\"Return True if *x* is a Python scalar (int/float or 0-d ndarray).\"\"\"\n    return np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0)\n\n\ndef _round4(x):\n    \"\"\"Round every numeric entry to 4 decimal places and keep structure.\"\"\"\n    if _is_scalar(x):\n        # Convert to Python float to avoid returning a 0-d ndarray\n        return round(float(x), 4)\n    return np.round(x, 4)            # returns 1-D NumPy array\n# --------------------------------------------------------------------------- #\n\n\n# --------------------------------------------------------------------------- #\n# Element-wise activations\n# --------------------------------------------------------------------------- #\ndef sigmoid(x):\n    \"\"\"Element-wise logistic sigmoid.\"\"\"\n    if _is_scalar(x):\n        return _round4(1 / (1 + np.exp(-x)))\n\n    x = np.asarray(x, dtype=float).ravel()          # ensure 1-D np.array\n    out = 1 / (1 + np.exp(-x))\n    return _round4(out)\n\n\ndef tanh(x):\n    \"\"\"Element-wise hyperbolic tangent.\"\"\"\n    if _is_scalar(x):\n        return _round4(np.tanh(x))\n\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.tanh(x)\n    return _round4(out)\n\n\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    if _is_scalar(x):\n        return _round4(max(0, x))\n\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.maximum(0, x)\n    return _round4(out)\n\n\ndef leaky_relu(x, alpha: float = 0.01):\n    \"\"\"Element-wise Leaky-ReLU with slope *alpha* for negative inputs.\"\"\"\n    if _is_scalar(x):\n        return _round4(x if x >= 0 else alpha * x)\n\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.where(x >= 0, x, alpha * x)\n    return _round4(out)\n# --------------------------------------------------------------------------- #\n\n\n# --------------------------------------------------------------------------- #\n# Softmax\n# --------------------------------------------------------------------------- #\ndef softmax(x):\n    \"\"\"Numerically stable softmax for a 1-D input.\"\"\"\n    if _is_scalar(x):                  # softmax of a scalar is always 1\n        return 1.0\n\n    x = np.asarray(x, dtype=float).ravel()\n    shift = x - np.max(x)              # numerical stability\n    exp_shift = np.exp(shift)\n    probs = exp_shift / np.sum(exp_shift)\n    return _round4(probs)\n# --------------------------------------------------------------------------- #\n\n\n# --------------------------------------------------------------------------- #\n# Activation lookup\n# --------------------------------------------------------------------------- #\ndef get_activation(name):\n    \"\"\"\n    Return the activation function associated with *name*.\n    \n    Parameters\n    ----------\n    name : str\n        One of {\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"softmax\"}.\n    \n    Returns\n    -------\n    Callable\n        The corresponding activation function.\n    \n    Raises\n    ------\n    ValueError\n        If *name* is not recognised.\n    \"\"\"\n    lookup = {\n        \"sigmoid\"    : sigmoid,\n        \"tanh\"       : tanh,\n        \"relu\"       : relu,\n        \"leaky_relu\" : leaky_relu,\n        \"softmax\"    : softmax,\n    }\n\n    try:\n        return lookup[name]\n    except KeyError:\n        raise ValueError('Invalid activation function.')\n# --------------------------------------------------------------------------- #\n```\n"}
{"task_id": 252, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    # Gradient: actual - predicted\n    grad = (actual - predicted).tolist()\n\n    # Hessian: a vector of ones with the same length as inputs\n    hess = np.ones_like(actual, dtype=float).tolist()\n\n    return grad, hess\n```"}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w        : 1-D weight vector (list or NumPy array).\n        alpha    : Regularisation strength (non-negative float).\n        l1_ratio : Fraction of L1 component (float in [0, 1]).\n        gradient : If True, return gradient, else return penalty value.\n\n    Returns\n        float  \u2013 if *gradient* is False  (penalty value),\n        list   \u2013 if *gradient* is True   (element-wise gradient).\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative.\")\n    if not (0.0 <= l1_ratio <= 1.0):\n        raise ValueError(\"l1_ratio must be in [0, 1].\")\n\n    # --- make sure w is a 1-D NumPy array -----------------------------------\n    w = np.asarray(w, dtype=float).ravel()\n\n    # --- compute -------------------------------------------------------------\n    if gradient:\n        # \u2207R(w) = \u03b1 \u00b7 [ \u03bb\u00b7sign(w) + (1\u2212\u03bb)\u00b7w ]\n        grad = alpha * (l1_ratio * np.sign(w) + (1.0 - l1_ratio) * w)\n        return list(np.round(grad, 4))\n    else:\n        # R(w) = \u03b1 \u00b7 [ \u03bb\u00b7||w||\u2081 + (1\u2212\u03bb)\u00b70.5\u00b7w\u1d40w ]\n        penalty = alpha * (\n            l1_ratio * np.linalg.norm(w, 1) +\n            (1.0 - l1_ratio) * 0.5 * np.dot(w, w)\n        )\n        return round(float(penalty), 4)\n```"}
{"task_id": 255, "completion_id": 0, "completion": "```python\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    An ace is *usable* if it can be counted as 11 without the hand busting.\n    Counting one ace as 11 instead of 1 adds 10 to the hand\u2019s total.\n\n    Args:\n        hand: List of integers representing card values (ace == 1).\n\n    Returns:\n        True if the hand has a usable ace, else False.\n    \"\"\"\n    # Fast check: we need at least one ace in the hand\n    if 1 not in hand:\n        return False\n\n    # Sum the hand with every ace valued at 1\n    total = sum(hand)\n\n    # If turning one ace from 1 \u2192 11 (i.e. +10) keeps total \u2264 21, it's usable\n    return total + 10 <= 21\n```"}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred\n    finite differences (double precision).\n\n    See the task description for a detailed specification.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Early exits / name canonicalisation ---------------------------- #\n    # ------------------------------------------------------------------ #\n    if param_name in (\"X\", \"y\"):              # not trainable\n        return None\n\n    real_name = param_name\n    if param_name == \"Ba\":\n        real_name = \"ba\"\n    elif param_name == \"Bx\":\n        real_name = \"bx\"\n\n    if real_name not in model.parameters:     # safety net\n        raise KeyError(f\"Parameter '{real_name}' not found in model.\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Convenience wrappers ------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    # Guarantee float64 everywhere\n    X = np.asarray(X, dtype=np.float64)\n    param: np.ndarray = model.parameters[real_name].astype(np.float64, copy=False)\n    model.parameters[real_name] = param           # insure reference is same\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    # (re-)initialise the model\u2019s hidden state if a reset method exists\n    def _reset_state_if_possible():\n        if hasattr(model, \"reset_state\") and callable(model.reset_state):\n            model.reset_state()\n\n    # Forward pass through the *whole* unrolled RNN and return the scalar loss\n    def _sequence_loss():\n        model.flush_gradients()\n        _reset_state_if_possible()\n\n        preds = []\n        for t in range(n_t):\n            # single-time-step forward\n            preds.append(model.forward(X[:, :, t]))\n        return float(loss_func(preds))        # enforce python scalar\n\n    # ------------------------------------------------------------------ #\n    # 2. Centred finite-difference loop -------------------------------- #\n    # ------------------------------------------------------------------ #\n    it = np.ndindex(param.shape)              # iterate over every element\n    for idx in it:\n        original_val = param[idx]\n\n        # +\u03b5 ------------------------------------------------------------ #\n        param[idx] = original_val + epsilon\n        loss_plus = _sequence_loss()\n\n        # \u2013\u03b5 ------------------------------------------------------------ #\n        param[idx] = original_val - epsilon\n        loss_minus = _sequence_loss()\n\n        # Numerical gradient ------------------------------------------- #\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n\n        # Restore original parameter value ----------------------------- #\n        param[idx] = original_val\n\n    # ------------------------------------------------------------------ #\n    # 3. Return transposed gradient (requirement of the task) ----------- #\n    # ------------------------------------------------------------------ #\n    return grads.T.astype(np.float64)\n\n\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray,\n                     X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    \n    # ---------- helpers -------------------------------------------------- #\n    def stump_predict(X, feature, thresh, polarity):\n        \"\"\"\n        Predict with a single decision stump.\n        polarity =  1 : x < thresh  -> -1     else 1\n        polarity = -1 : x < thresh  ->  1     else -1\n        \"\"\"\n        col = X[:, feature]\n        if polarity == 1:\n            return np.where(col < thresh, -1, 1)\n        else:                     # polarity == -1\n            return np.where(col < thresh, 1, -1)\n    \n    # ---------- sanity checks / init ------------------------------------ #\n    n_clf = max(1, int(n_clf))\n    m, n_features = X_train.shape\n    w = np.full(m, 1.0 / m)                     # initial sample weights\n    \n    stumps = []                                 # stores (feature, thresh, polarity, alpha)\n    eps = 1e-10                                 # to avoid division-by-zero\n    \n    # ---------- boosting ------------------------------------------------- #\n    for _ in range(n_clf):\n        best_feature, best_thresh, best_pol, min_err = None, None, None, np.inf\n        \n        # brute-force search over all possible decision stumps\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            thresholds = np.unique(values)      # candidate split points\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    pred = stump_predict(X_train, feature, thresh, polarity)\n                    err  = np.sum(w[pred != y_train])\n                    if err < min_err:\n                        best_feature, best_thresh, best_pol = feature, thresh, polarity\n                        min_err = err\n                        \n        # if the stump is worse than random, stop early\n        if min_err >= 0.5 - eps:\n            break\n        \n        # stump weight\n        min_err = max(min_err, eps)             # avoid log(0)\n        alpha = 0.5 * np.log((1 - min_err) / min_err)\n        \n        # save the stump\n        stumps.append((best_feature, best_thresh, best_pol, alpha))\n        \n        # update sample weights\n        predictions = stump_predict(X_train, best_feature, best_thresh, best_pol)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()                            # normalise\n    \n    # ---------- prediction for X_test ----------------------------------- #\n    if not stumps:                              # should only happen with degenerate data\n        return [-1] * len(X_test)\n    \n    agg = np.zeros(X_test.shape[0])\n    for feature, thresh, polarity, alpha in stumps:\n        agg += alpha * stump_predict(X_test, feature, thresh, polarity)\n    \n    return [1 if x >= 0 else -1 for x in agg]\n```"}
{"task_id": 258, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    # length check\n    if len(actual) != len(predicted):\n        return -1\n\n    # vectorise inputs\n    y_true = np.asarray(actual, dtype=float)\n    y_pred = np.asarray(predicted, dtype=float)\n\n    # hinge loss for each sample: max(0, 1 - y_i * y_hat_i)\n    per_sample_loss = np.maximum(0, 1 - y_true * y_pred)\n\n    # average and round\n    mean_loss = np.round(per_sample_loss.mean(), 4)\n\n    # np.round returns numpy.float64 \u2013 cast to native Python float\n    return float(mean_loss)\n```"}
{"task_id": 259, "completion_id": 0, "completion": "```python\nimport random\n\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Return *batch_size* different transitions sampled from the replay buffer.\n    \n    The sampling is deterministic because it is performed with a `random.Random`\n    instance that is always initialised with seed 42.\n    \n    If *batch_size* is larger than the number of stored transitions, -1\n    is returned instead.\n    \"\"\"\n    # Not enough elements in the buffer?\n    if batch_size > len(memory):\n        return -1\n    \n    # Deterministic RNG that does *not* affect the global random state\n    rng = random.Random(42)\n    \n    # random.sample already guarantees distinct elements and preserves\n    # the order they are generated in\n    return rng.sample(memory, batch_size)\n```"}
{"task_id": 260, "completion_id": 0, "completion": "```python\nfrom itertools import zip_longest     # only used for the size-check\nfrom math import floor\n\n\ndef tileswrap(ihtORsize: int,\n              numtilings: int,\n              floats: list[float],\n              wrapwidths: list[int | None],\n              ints: list[int] | None = None,\n              readonly: bool = False) -> list[int]:\n    \"\"\"Wrapping tile-coder that returns the index of the active tile in every\n    one of `numtilings` tilings.\n\n    The code follows the algorithm described in the task text verbatim:\n        1. quantise each float -> q_i\n        2. build coordinates for every tiling, applying wrapping if requested\n        3. hash the integer coordinate list with the fixed polynomial hash\n    \"\"\"\n    # ------------------------------------------------------------------ checks\n    if ihtORsize <= 0:\n        raise ValueError('ihtORsize must be a positive integer.')\n    if numtilings <= 0:\n        raise ValueError('numtilings must be a positive integer.')\n    if len(list(zip_longest(floats, wrapwidths))) != len(floats):\n        raise ValueError('wrapwidths must be the same length as floats.')\n    # make sure we really have an `ints` list\n    ints = ints or []\n\n    # ------------------------------------------------------------ step 1: q_i\n    q_vals = [int(floor(x * numtilings)) for x in floats]\n\n    active_tiles: list[int] = []\n\n    # ------------------------------------------------------ step 2 & 3: per t\n    for t in range(numtilings):\n        coords: list[int] = [t]            # 2a\n        b = t\n\n        for q, wrap in zip(q_vals, wrapwidths):       # 2b\n            offset = b % numtilings\n            c = (q + offset) // numtilings\n            if wrap and wrap > 0:                     # apply wrapping\n                c %= wrap\n            coords.append(c)\n            b += t * 2                                # update b\n\n        # 2c \u2013 append extra integer variables\n        coords.extend(ints)\n\n        # 3. deterministic polynomial hash mod ihtORsize\n        index = 0\n        for coord in coords:\n            index = (index * 31 + coord) % ihtORsize\n\n        active_tiles.append(index)\n\n    return active_tiles\n```\n\nThe routine\n\n\u2022 discretises every continuous input (`floats`) once,  \n\u2022 constructs the correct coordinate list for each tiling, applying optional\n  wrapping (`wrapwidths`),  \n\u2022 appends any additional integer variables (`ints`), and  \n\u2022 hashes the coordinates with a fixed polynomial hash so that each returned\n  value is an integer in `[0, ihtORsize)`.\n\nThe `readonly` flag is accepted for API compatibility but has no influence on\nthe (purely functional) computation."}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate an array initialised with Glorot/Xavier normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired tensor shape. Must have at least two dimensions so that\n        fan-in and fan-out are well defined.\n\n    Returns\n    -------\n    np.ndarray\n        Array of the requested shape whose elements are drawn independently\n        from \ud835\udca9(0, s\u00b2) with  s = \u221a[2 / (fan_in + fan_out)].\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"`shape` must have at least two dimensions.\")\n\n    # fan_in / fan_out --------------------------------------------------------\n    if len(shape) == 2:                                 # e.g. dense layer\n        fan_in, fan_out = shape\n    else:                                               # e.g. conv kernels\n        receptive_field = int(np.prod(shape[2:], dtype=int))\n        fan_in  = shape[1] * receptive_field            # in-channels \u22c5 RF\n        fan_out = shape[0] * receptive_field            # out-channels \u22c5 RF\n\n    # standard deviation ------------------------------------------------------\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n\n    # sample from N(0, std^2) -------------------------------------------------\n    return np.random.normal(loc=0.0, scale=std, size=shape).astype(float)\n```"}
{"task_id": 263, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef feedforward_predict(x, weights, activations):\n    \"\"\"Runs a forward pass through a fully-connected neural network.\n\n    Parameters\n    ----------\n    x : list[float] | numpy.ndarray\n        Input feature vector (bias term **not** included).\n    weights : list[list[list[float]]]\n        List of weight matrices; each inner list represents one 2-D weight\n        matrix where the first column contains the bias weights.\n    activations : list[str]\n        Activation functions per layer.  Allowed values: \"sigmoid\", \"relu\",\n        \"linear\".\n\n    Returns\n    -------\n    float | list[float]\n        Network output rounded to 4 decimal places.  A scalar is returned for\n        a single-neuron output layer, otherwise a list is returned.\n    \"\"\"\n    # --- helper -------------------------------------------------------------\n    def apply_activation(z, name):\n        \"\"\"Element-wise activation.\"\"\"\n        if name == \"sigmoid\":\n            return 1.0 / (1.0 + np.exp(-z))\n        elif name == \"relu\":\n            return np.maximum(0, z)\n        elif name == \"linear\":\n            return z\n        else:\n            raise ValueError(f\"Unsupported activation: {name}\")\n\n    # --- forward pass -------------------------------------------------------\n    a = np.asarray(x, dtype=float)                 # a^(0)\n    for W_list, act_name in zip(weights, activations):\n        W = np.asarray(W_list, dtype=float)        # W^(\u2113)\n        a_tilde = np.insert(a, 0, 1.0)             # prepend bias \u21d2 \u00e3^(\u2113-1)\n        z = W @ a_tilde                            # pre-activation z^(\u2113)\n        a = apply_activation(z, act_name)          # activation  a^(\u2113)\n\n    # --- formatting the output ---------------------------------------------\n    a = np.round(a, 4)\n    return float(a) if a.size == 1 else a.tolist()\n```"}
{"task_id": 265, "completion_id": 0, "completion": "```python\ndef king_loss_fn(x: int | float) -> float:\n    \"\"\"Compute King\u2019s piece-wise loss for a scalar value.\n\n    Segments:\n      \u2022 x \u2264 250             : L(x) = \u20130.25 * x + 82.50372665317208\n      \u2022 250 < x \u2264 600       : L(x) = 20.00372665317208\n      \u2022 600 < x \u2264 700       : L(x) = \u20130.2  * x + 140.00372665317207\n      \u2022 x > 700             : L(x) = 0.003726653172066108\n\n    Result is rounded to 4 decimal places.\n\n    Args:\n        x: Scalar input (int or float).\n\n    Returns:\n        Loss value rounded to four decimal places.\n    \"\"\"\n    if x <= 250:\n        value = -0.25 * x + 82.50372665317208\n    elif x <= 600:                    # 250 < x \u2264 600\n        value = 20.00372665317208\n    elif x <= 700:                    # 600 < x \u2264 700\n        value = -0.2 * x + 140.00372665317207\n    else:                             # x > 700\n        value = 0.003726653172066108\n\n    return round(value, 4)\n```"}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples ``(u, v)`` describing undirected edges that connect\n        vertices ``u`` and ``v``.\n        \u2022 Multiple occurrences of the same edge may be present.  \n        \u2022 Vertices that do not occur in ``V`` are ignored.  \n        \u2022 Self-loops (u == v) are ignored.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list ``G`` where ``G[i]`` contains all vertices adjacent to ``V[i]``.\n        \u2022 The outer list is in the same order as ``V``.  \n        \u2022 Inner lists contain *no duplicates* and are ordered by the vertices\u2019\n          order of appearance in ``V``.\n    \"\"\"\n    # Position lookup: vertex -> index in V  (O(|V|))\n    index = {vertex: i for i, vertex in enumerate(V)}\n\n    # Start with an empty set for the neighbours of every vertex\n    neighbours: List[set] = [set() for _ in V]\n\n    # Fill the neighbour sets  (O(|E|))\n    for u, v in E:\n        # Skip edges that reference unknown vertices\n        if u not in index or v not in index:\n            continue\n        # Skip self-loops\n        if u == v:\n            continue\n\n        ui, vi = index[u], index[v]\n        neighbours[ui].add(v)   # u \u2014 v\n        neighbours[vi].add(u)   # v \u2014 u\n\n    # Convert each neighbours set to a list whose order\n    # follows the global order in V by sorting on the index\n    G: List[List[Any]] = []\n    for nb_set in neighbours:\n        # sort according to appearance order in V\n        ordered_nb = sorted(nb_set, key=index.get)\n        G.append(ordered_nb)\n\n    return G\n```\n\nExplanation of the key steps\n1. Build `index`, a dictionary that maps every vertex in `V` to its position.  \n   This allows O(1) translation from a vertex label to its place in the final\n   structure and lets us sort neighbours according to the original order.\n\n2. Traverse the edge list `E` once and add both directions of every edge\n   (because the graph is undirected).  \n   Sets are used to ensure that duplicated/parallel edges do not create\n   duplicates in the neighbourhood.\n\n3. Finally turn every neighbour set into a list, ordering the vertices with\n   `sorted(..., key=index.get)` so that they appear in the order dictated by\n   `V`.\n\nThe resulting `G` satisfies all required constraints:\n\u2022 outer order identical to `V`,  \n\u2022 inner lists free of duplicates,  \n\u2022 inner order consistent with `V`,  \n\u2022 isolated vertices represented by an empty list `[]`."}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_decision_stump(\n    X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray\n) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Best stump (see task description).\n    \"\"\"\n    # make sure the vectors are 1-D np arrays\n    y = y.ravel().astype(int)\n    w = sample_weights.ravel().astype(float)\n\n    n_samples, n_features = X.shape\n\n    best_feature = -1\n    best_threshold = 0.0\n    best_polarity = 1\n    best_error = np.inf\n\n    for j in range(n_features):\n        # sort the j-th feature\n        sorted_idx = np.argsort(X[:, j])\n        f_vals = X[sorted_idx, j]\n        lbls = y[sorted_idx]\n        ws = w[sorted_idx]\n\n        pos_total = ws[lbls == 1].sum()\n        neg_total = ws[lbls == -1].sum()\n\n        cum_pos = 0.0  # cumulative positive weight   (x_ij  < \u03b8)\n        cum_neg = 0.0  # cumulative negative weight   (x_ij  < \u03b8)\n\n        i = 0\n        n = n_samples\n        while i < n:\n            # value that will act as a candidate threshold\n            theta = f_vals[i]\n\n            # weighted errors BEFORE adding the samples that equal `theta`\n            err_p_1 = (  # polarity  +1   (+1 left, -1 right)\n                cum_neg + (pos_total - cum_pos)\n            )\n            err_p_neg1 = (  # polarity  -1   (-1 left, +1 right)\n                cum_pos + (neg_total - cum_neg)\n            )\n\n            # update best stump if necessary\n            if err_p_1 < best_error - 1e-12:\n                best_error = err_p_1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = 1\n\n            if err_p_neg1 < best_error - 1e-12:\n                best_error = err_p_neg1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = -1\n\n            # move forward \u2013 add all samples that have feature value == theta\n            while i < n and f_vals[i] == theta:\n                if lbls[i] == 1:\n                    cum_pos += ws[i]\n                else:\n                    cum_neg += ws[i]\n                i += 1\n\n    return {\n        \"feature_index\": int(best_feature),\n        \"threshold\": round(best_threshold, 4),\n        \"polarity\": int(best_polarity),\n        \"weighted_error\": round(best_error, 4),\n    }\n```"}
{"task_id": 268, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations, permutations\nfrom itertools import combinations, permutations\nimport numpy as np\n\ndef random_unweighted_graph(n_vertices: int,\n                            edge_prob: float = 0.5,\n                            directed: bool = False) -> list[list[int]]:\n    \"\"\"Generate an un-weighted Erd\u0151s\u2013R\u00e9nyi G(n,p) random graph.\n\n    Parameters\n    ----------\n    n_vertices : int\n        Number of vertices (labelled 0 \u2026 n_vertices-1).\n    edge_prob  : float, default 0.5\n        Probability p that any admissible edge exists.\n    directed   : bool, default False\n        If True build a directed graph, otherwise an undirected one.\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix A where A[i][j] == 1  iff  edge i\u2192j exists.\n        The matrix contains only integers 0/1 and has shape\n        (n_vertices, n_vertices).  When n_vertices == 1, [[0]] is\n        returned.\n    \"\"\"\n    # --- trivial one-vertex case ------------------------------------------------\n    if n_vertices == 1:\n        return [[0]]\n\n    # Optional sanity checks (light-weight, can be removed if undesired)\n    if n_vertices <= 0:\n        raise ValueError(\"n_vertices must be a positive integer.\")\n    if not (0.0 <= edge_prob <= 1.0):\n        raise ValueError(\"edge_prob must be in the interval [0, 1].\")\n\n    # --- build empty adjacency matrix ------------------------------------------\n    # Use a NumPy array first for convenience, then convert to Python lists.\n    A = np.zeros((n_vertices, n_vertices), dtype=int)\n\n    # --- fill in edges ----------------------------------------------------------\n    rng_rand = np.random.rand          # local alias \u2013 slightly faster\n\n    if directed:\n        # Consider every ordered pair (i, j) with i != j\n        for i in range(n_vertices):\n            for j in range(n_vertices):\n                if i == j:\n                    continue                            # no self-loops\n                if rng_rand() <= edge_prob:\n                    A[i, j] = 1\n    else:\n        # Undirected: examine each unordered pair only once, keep matrix symmetric\n        for i in range(n_vertices):\n            for j in range(i + 1, n_vertices):\n                if rng_rand() <= edge_prob:\n                    A[i, j] = A[j, i] = 1\n\n    # --- return as nested Python list ------------------------------------------\n    return A.tolist()\n```"}
{"task_id": 269, "completion_id": 0, "completion": "```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\n\n    Parameters\n    ----------\n    state_dimensions : int\n        Length of the state (observation) vector.\n    action_dimensions : int\n        Number of possible discrete actions.\n\n    Returns\n    -------\n    int\n        Total count of trainable parameters (weights + biases).\n    \"\"\"\n    # First fully-connected layer: state_dimensions \u2192 164\n    layer1_params = state_dimensions * 164 + 164        # weights + biases\n\n    # Second fully-connected layer: 164 \u2192 action_dimensions\n    layer2_params = 164 * action_dimensions + action_dimensions\n\n    # Total trainable parameters\n    return layer1_params + layer2_params\n```"}
{"task_id": 270, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oracle_payoff(context: np.ndarray, thetas: np.ndarray) -> tuple[float, int]:\n    \"\"\"Return best (rounded) expected reward and corresponding arm index.\n\n    Parameters\n    ----------\n    context : np.ndarray, shape (D, K)\n        Column k holds the D\u2013dimensional context vector x_{t,k}.\n    thetas  : np.ndarray, shape (D, K)\n        Column k holds the parameter vector \u03b8_k for arm k.\n\n    Returns\n    -------\n    (best_reward, best_arm) : (float, int)\n        best_reward \u2013 maximal expected reward rounded to 4 decimals,\n        best_arm    \u2013 smallest index of an arm that attains best_reward\n                      after rounding.\n    \"\"\"\n    # basic shape check\n    if context.shape != thetas.shape:\n        raise ValueError(\"context and thetas must have identical shape (D, K).\")\n    if context.ndim != 2:\n        raise ValueError(\"context and thetas must be 2-D arrays (D, K).\")\n\n    # Expected reward of each arm k:  x_k^T \u03b8_k\n    # \u2192 element-wise multiply and sum over rows (axis 0)\n    rewards = np.sum(context * thetas, axis=0)          # shape (K,)\n\n    # Round to 4 decimals for comparison/output\n    rounded = np.round(rewards, 4)\n\n    best_reward = np.max(rounded)\n    # all arms that achieve the maximal (rounded) reward\n    best_indices = np.where(rounded == best_reward)[0]\n    best_arm = int(best_indices.min())                  # smallest index in case of ties\n\n    return float(best_reward), best_arm\n```\n\nThe function  \n1. computes each arm\u2019s expected reward (inner product),  \n2. rounds those rewards to four decimal places,  \n3. selects the largest rounded reward, breaking any ties by choosing the\n   smallest index, and  \n4. returns the rounded optimal reward together with its arm index."}
{"task_id": 271, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef xgboost_split_gain(\n    left_true: list[float],\n    left_pred: list[float],\n    right_true: list[float],\n    right_pred: list[float],\n) -> float:\n    \"\"\"Compute XGBoost split gain for squared-error loss.\"\"\"\n    \n    # Convert to NumPy arrays for vectorised maths\n    l_true  = np.asarray(left_true,  dtype=float)\n    l_pred  = np.asarray(left_pred,  dtype=float)\n    r_true  = np.asarray(right_true, dtype=float)\n    r_pred  = np.asarray(right_pred, dtype=float)\n\n    # Number of samples (Hessians: all ones)\n    H_L = l_true.size\n    H_R = r_true.size\n    \n    # Do not allow empty child (gain = 0 by definition/ convention)\n    if H_L == 0 or H_R == 0:\n        return 0.0\n    \n    # Gradients: g_i = y_pred - y_true\n    G_L = np.sum(l_pred - l_true)\n    G_R = np.sum(r_pred - r_true)\n    \n    # Parent aggregates\n    G_P = G_L + G_R\n    H_P = H_L + H_R\n\n    # Split gain formula\n    gain = 0.5 * (\n        (G_L ** 2) / H_L +\n        (G_R ** 2) / H_R -\n        (G_P ** 2) / H_P\n    )\n    \n    # Round to 4 decimals; guarantee +0.0 not \u22120.0\n    gain_rounded = round(gain, 4)\n    if gain_rounded == -0.0:\n        gain_rounded = 0.0\n        \n    return gain_rounded\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float (rounded to 4 decimals) or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n\n    n = len(feature)\n    if n <= 1:                          # nothing to split\n        return (None, 0.0)\n\n    # ------------------------------------------------------------------ #\n    # Helper : Shannon entropy for a multiset of class labels            #\n    # ------------------------------------------------------------------ #\n    def entropy(counter):\n        total = sum(counter.values())\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            h -= p * math.log2(p)\n        return h\n\n    # ------------------------------------------------------------------ #\n    # Sort by the feature \u2013 makes \u201c< threshold / \u2265 threshold\u201d contiguous #\n    # ------------------------------------------------------------------ #\n    pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feat  = [p[0] for p in pairs]\n    sorted_label = [p[1] for p in pairs]\n\n    if sorted_feat[0] == sorted_feat[-1]:          # all values identical\n        return (None, 0.0)\n\n    # initial class\u2013frequency tables\n    right_counts = Counter(sorted_label)           # all points start on the right\n    left_counts  = Counter()\n\n    parent_entropy = entropy(right_counts)\n\n    best_gain       = 0.0\n    best_threshold  = None\n    eps             = 1e-12                        # for floating-point ties\n\n    # ------------------------------------------------------------------ #\n    # Sweep once from left to right, updating class counts on the fly    #\n    # ------------------------------------------------------------------ #\n    for i in range(n - 1):\n        lbl = sorted_label[i]\n        left_counts[lbl]  += 1\n        right_counts[lbl] -= 1\n        if right_counts[lbl] == 0:\n            del right_counts[lbl]\n\n        # Only between *distinct* consecutive feature values a threshold is legal\n        if sorted_feat[i] != sorted_feat[i + 1]:\n            thr = (sorted_feat[i] + sorted_feat[i + 1]) / 2.0\n\n            n_left  = i + 1\n            n_right = n - n_left\n\n            gain = (parent_entropy\n                    - (n_left  / n) * entropy(left_counts)\n                    - (n_right / n) * entropy(right_counts))\n\n            # keep the smallest threshold in case of equal maximal gain\n            if gain > best_gain + eps:\n                best_gain, best_threshold = gain, thr\n            elif abs(gain - best_gain) <= eps and best_gain > 0.0:\n                best_threshold = min(best_threshold, thr)\n\n    if best_threshold is None:                     # no positive gain found\n        return (None, 0.0)\n\n    # Round as required\n    return (round(best_threshold, 4), round(best_gain, 4))\n```"}
{"task_id": 274, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef small_norm_clip(values):\n    \"\"\"Clip each element of *values* to the closed interval [-5, 5].\n\n    Args:\n        values (list[int | float] | np.ndarray): One-dimensional numeric data.\n\n    Returns:\n        list[float]: A Python list where every element has been limited to the\n            range [-5, 5] while preserving the original order.\n    \"\"\"\n    # NumPy array input \u2500 use vectorised clipping\n    if isinstance(values, np.ndarray):\n        if values.ndim != 1:\n            raise ValueError(\"Input array must be one-dimensional.\")\n        return np.clip(values, -5, 5).tolist()\n\n    # Python list (or other sequence) input \u2500 fall back to pure Python\n    if isinstance(values, (list, tuple)):\n        return [5 if x > 5 else (-5 if x < -5 else x) for x in values]\n\n    raise TypeError(\"Input must be a list or a one-dimensional NumPy array.\")\n```"}
{"task_id": 276, "completion_id": 0, "completion": "```python\nimport math\nimport string\nfrom collections import Counter\n\n\ndef additive_ngram_log_prob(\n    corpus: list[str],\n    sentence: str,\n    N: int,\n    k: float = 1.0,\n    unk: bool = True\n) -> float:\n    \"\"\"Return base-10 log-probability of *sentence* under an additive-smoothed\n    N-gram model that is trained on *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError(\"N must be a positive integer.\")\n    if k <= 0:\n        raise ValueError(\"k (smoothing parameter) must be > 0.\")\n\n    # ---------- helper ---------------------------------------------------- #\n    punct = set(string.punctuation)\n\n    def tokenize(text: str) -> list[str]:\n        \"\"\"Lower-case, white-space split, strip leading/trailing punctuation.\"\"\"\n        out = []\n        for tok in text.lower().split():\n            tok = tok.strip(string.punctuation)\n            if tok:                              # drop empty tokens\n                out.append(tok)\n        return out\n\n    # ---------- 1. build counts from training corpus ---------------------- #\n    vocab: set[str] = set()\n    sentences_tokens: list[list[str]] = []\n\n    total_tokens = 0  # needed for the unigram denominator\n\n    for sent in corpus:\n        toks = tokenize(sent)\n        padded = [\"<bol>\", *toks, \"<eol>\"]\n        sentences_tokens.append(padded)\n        vocab.update(padded)\n        total_tokens += len(padded)\n\n    # add special symbols that must always be in the vocabulary\n    vocab.update({\"<bol>\", \"<eol>\"})\n    if unk:\n        vocab.add(\"<unk>\")\n\n    V = len(vocab)  # vocabulary size\n\n    # ----- count N-grams (and histories) ---------------------------------- #\n    if N == 1:\n        unigram_counts = Counter()\n        for toks in sentences_tokens:\n            unigram_counts.update(toks)\n    else:\n        ngram_counts = Counter()\n        hist_counts = Counter()  # (N-1)-gram counts\n        for toks in sentences_tokens:\n            for i in range(len(toks) - N + 1):\n                ngram = tuple(toks[i : i + N])\n                ngram_counts[ngram] += 1\n                hist_counts[ngram[:-1]] += 1\n\n    # ---------- 2. process target sentence -------------------------------- #\n    tgt_tokens = tokenize(sentence)\n    if unk:\n        tgt_tokens = [t if t in vocab else \"<unk>\" for t in tgt_tokens]\n\n    tgt_tokens = [\"<bol>\", *tgt_tokens, \"<eol>\"]\n\n    # ---------- 3. compute sentence log-probability ----------------------- #\n    log_prob = 0.0\n\n    if N == 1:\n        denom = total_tokens + k * V\n        for w in tgt_tokens:\n            num = unigram_counts.get(w, 0) + k\n            log_prob += math.log10(num / denom)\n    else:\n        for i in range(len(tgt_tokens) - N + 1):\n            ngram = tuple(tgt_tokens[i : i + N])\n            history = ngram[:-1]\n\n            num = ngram_counts.get(ngram, 0) + k\n            denom = hist_counts.get(history, 0) + k * V\n            log_prob += math.log10(num / denom)\n\n    # ---------- 4. round and return --------------------------------------- #\n    return round(log_prob, 4)\n```"}
{"task_id": 277, "completion_id": 0, "completion": "```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    # Fetch the spaces (KeyError will propagate if keys are missing,\n    # as specified by the docstring.)\n    action_space = env[\"action_space\"]\n    observation_space = env[\"observation_space\"]\n\n    # Check if each space is either a built-in tuple or dict.\n    tuple_action = isinstance(action_space, (tuple, dict))\n    tuple_obs   = isinstance(observation_space, (tuple, dict))\n\n    return tuple_action, tuple_obs\n```"}
{"task_id": 278, "completion_id": 0, "completion": "```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    # We keep following branches until a leaf (a dict with key \"value\") is reached.\n    node = tree\n    while True:\n        # If we are at a leaf, return its value.\n        if 'value' in node:\n            return node['value']\n\n        # Extract the decision rule of the current internal node.\n        feature_i = node['feature_i']\n        threshold = node['threshold']\n\n        # Optional \u2013 ensure the requested feature exists.\n        if feature_i >= len(sample) or feature_i < 0:\n            raise IndexError(f\"Feature index {feature_i} is out of bounds for sample of length {len(sample)}\")\n\n        # Decide which branch to take and continue the loop.\n        if sample[feature_i] <= threshold:\n            node = node['true_branch']\n        else:\n            node = node['false_branch']\n```"}
{"task_id": 279, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error (MSE) loss, its gradient and accuracy.\n\n    Parameters\n    ----------\n    y_true : list[int] | numpy.ndarray\n        Ground-truth labels (0 or 1).\n    y_pred : list[float] | numpy.ndarray\n        Model predictions (any real numbers).\n\n    Returns\n    -------\n    tuple\n        (mse, gradient_list, accuracy), all rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorised math\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    n = y_true.size\n    diff = y_pred - y_true\n\n    # 1. MSE\n    mse = np.mean(diff ** 2)\n\n    # 2. Analytic gradient: 2/N * (y_pred - y_true)\n    grad = (2.0 / n) * diff\n\n    # 3. Classification accuracy with 0.5 threshold\n    pred_class = (y_pred >= 0.5).astype(int)\n    true_class = y_true.astype(int)\n    accuracy = np.mean(pred_class == true_class)\n\n    # Round everything to 4 decimals\n    mse = round(float(mse), 4)\n    grad_list = [round(float(g), 4) for g in grad.tolist()]\n    accuracy = round(float(accuracy), 4)\n\n    return mse, grad_list, accuracy\n```"}
{"task_id": 280, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool = False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector / matrix / tensor of any shape.\n    alpha : float\n        Non-negative regularization strength.\n    return_grad : bool, optional (default=False)\n        \u2022 False \u2192 return the scalar regularization value  \n        \u2022 True  \u2192 return the gradient (same shape as *w*)\n\n    Returns\n    -------\n    float or np.ndarray\n        Rounded to 4 decimal places as required.\n    \"\"\"\n    # --- basic validation ----------------------------------------------------\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative\")\n\n    # -------------------------------------------------------------------------\n    if return_grad:                       # gradient:  \u03b1 \u00b7 w\n        grad = alpha * w\n        return np.round(grad, 4)\n    else:                                 # value: 0.5 \u00b7 \u03b1 \u00b7 \u03a3 w_i\u00b2\n        value = 0.5 * alpha * np.sum(w ** 2)\n        return float(np.round(value, 4))\n```\n\nExample usage\n```\nw = np.array([1.0, -2.0,  3.0])\nalpha = 0.1\n\nprint(l2_regularization(w, alpha))           # 0.7\nprint(l2_regularization(w, alpha, True))     # [ 0.1 -0.2  0.3]\n```"}
{"task_id": 281, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np          # (left as-is to respect the given stub)\n\ndef L1Regularization(alpha: float):\n    \"\"\"Factory that returns a callable implementing L1 (Lasso) regularisation.\n\n    The resulting object *r* supports\n        value = r(w)          # penalty      (Python float)\n        grad  = r.grad(w)     # gradient     (NumPy array, same shape as w)\n\n    Parameters\n    ----------\n    alpha : float\n        Non-negative strength of the regularisation.  If alpha == 0 both the\n        penalty and its gradient are identically zero.\n    \"\"\"\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative.\")\n\n    class _L1:\n        __slots__ = (\"alpha\",)\n\n        def __init__(self, alpha_):\n            self.alpha = float(alpha_)\n\n        # ------------------------------------------------------------------ #\n        #  Penalty value\n        # ------------------------------------------------------------------ #\n        def __call__(self, w):\n            \"\"\"\n            Parameters\n            ----------\n            w : np.ndarray\n                Weight array (any shape)\n\n            Returns\n            -------\n            float\n                J_L1(w) = alpha * ||w||_1\n            \"\"\"\n            if self.alpha == 0.0:\n                return 0.0\n            # np.abs(w).sum() already gives a scalar NumPy value; cast to float.\n            return float(self.alpha * np.abs(w).sum())\n\n        # ------------------------------------------------------------------ #\n        #  Gradient\n        # ------------------------------------------------------------------ #\n        def grad(self, w):\n            \"\"\"\n            Gradient of the L1 penalty with respect to w.\n\n            Parameters\n            ----------\n            w : np.ndarray\n                Weight array\n\n            Returns\n            -------\n            np.ndarray\n                alpha * sign(w)  (same shape as w, zeros where w == 0)\n            \"\"\"\n            if self.alpha == 0.0:\n                # Ensure exact same shape/dtype as w\n                return np.zeros_like(w, dtype=w.dtype)\n\n            # np.sign gives -1, 0, +1 with same dtype as w (or float if w is bool)\n            return self.alpha * np.sign(w).astype(w.dtype, copy=False)\n\n        # ------------------------------------------------------------------ #\n        #  Nice string representation (optional, aids debugging)\n        # ------------------------------------------------------------------ #\n        def __repr__(self):\n            return f\"L1Regularization(alpha={self.alpha})\"\n\n    # Return an instance of the inner helper class\n    return _L1(alpha)\n```"}
{"task_id": 282, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import Union, List, Tuple\n\nNumber = Union[int, float]\n\ndef running_stats(data: List[Number]) -> Tuple[float, float, float]:\n    \"\"\"Compute sample mean, variance and standard deviation in one pass.\n\n    Parameters\n    ----------\n    data : list[int | float]\n        Stream of numeric observations (may be empty).\n\n    Returns\n    -------\n    (mean, variance, std) : tuple[float, float, float]\n        Sample statistics rounded to 4 decimal places.  For an empty\n        input all three numbers are 0.0.\n    \"\"\"\n    if not data:                                   # empty sequence\n        return 0.0, 0.0, 0.0\n\n    n = 0            # number of samples seen so far\n    mean = 0.0       # running mean\n    s = 0.0          # running sum of squared diffs from the mean\n\n    # Welford's online algorithm\n    for x in data:\n        n += 1\n        delta = x - mean\n        mean += delta / n\n        s += delta * (x - mean)\n\n    # Sample variance (unbiased, denominator n-1)\n    variance = s / (n - 1) if n > 1 else 0.0\n    std = math.sqrt(variance)\n\n    # Round to 4 decimal places as required\n    return round(mean, 4), round(variance, 4), round(std, 4)\n```"}
{"task_id": 283, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _stable_sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable sigmoid.\"\"\"\n    # For large negative values exp(x) overflows, so we treat the two regions separately.\n    pos_mask = x >= 0\n    neg_mask = ~pos_mask\n\n    out = np.empty_like(x, dtype=float)\n\n    # x >= 0  \u279c  \u03c3(x) = 1 / (1 + e^{-x})\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n\n    # x < 0   \u279c  \u03c3(x) = e^{x} / (1 + e^{x})\n    exp_x = np.exp(x[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    return out\n\n\ndef binary_log_loss_metrics(actual, predicted, regularization=1.0):\n    \"\"\"Compute Newton leaf value and split gain for binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        1-D array with true binary labels (0 or 1).\n    predicted : np.ndarray\n        1-D array with raw scores (before sigmoid).\n    regularization : float, default 1.0\n        L2-regularisation term \u03bb.\n\n    Returns\n    -------\n    list[float]\n        [leaf_value, gain] \u2013 each rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to NumPy arrays and ensure float datatype\n    y = np.asarray(actual, dtype=float).ravel()\n    f = np.asarray(predicted, dtype=float).ravel()\n\n    if y.shape != f.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same shape.\")\n\n    # Probability via numerically-stable sigmoid\n    p = _stable_sigmoid(f)\n\n    # First and second derivatives\n    g = p - y                 # gradient\n    h = p * (1.0 - p)         # Hessian\n\n    sum_g = g.sum()\n    sum_h = h.sum()\n\n    denom = sum_h + regularization\n\n    leaf_value = sum_g / denom\n    gain = 0.5 * (sum_g ** 2) / denom\n\n    # Round to 4 decimals\n    leaf_value = round(leaf_value, 4)\n    gain = round(gain, 4)\n\n    return [leaf_value, gain]\n```"}
{"task_id": 284, "completion_id": 0, "completion": "```python\ndef iht_get_index(\n    obj: object,\n    size: int,\n    table: dict,\n    overfull_count: list[int],\n    readonly: bool = False,\n) -> int | None:\n    \"\"\"Map *obj* to a stable integer index using a bounded index hash table.\n\n    Args:\n        obj:               Hashable object that should receive an index.\n        size:              Maximum number of *stored* entries.\n        table:             Mutable dictionary holding the current mapping.\n        overfull_count:    One-element list whose integer counts collisions.\n        readonly:          If True, never modify *table* or *overfull_count*.\n\n    Returns:\n        The integer index for *obj*, or None when *readonly* is True and the\n        object is not yet in *table*.\n    \"\"\"\n    # 1. Object already known \u2192 return stored index.\n    if obj in table:\n        return table[obj]\n\n    # 2. Read-only mode \u2192 unknown objects are ignored.\n    if readonly:\n        return None\n\n    # 3. Table still has room \u2192 insert new entry with the next consecutive index.\n    if len(table) < size:\n        index = len(table)            # next free consecutive index\n        table[obj] = index            # store mapping\n        return index\n\n    # 4. Table full \u2192 return a collision index and count the collision.\n    collision_index = hash(obj) % size\n    overfull_count[0] += 1\n    return collision_index\n```"}
{"task_id": 285, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list.  \n        If the feature dimensions of *x* and *y* differ, returns -1.\n    \"\"\"\n    # Ensure inputs are at most 2-D and reshape 1-D arrays to row vectors.\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    elif x.ndim != 2:\n        raise ValueError(\"Input x must be 1-D or 2-D.\")\n\n    if y.ndim == 1:\n        y = y.reshape(1, -1)\n    elif y.ndim != 2:\n        raise ValueError(\"Input y must be 1-D or 2-D.\")\n\n    # Check that feature dimensions match.\n    if x.shape[1] != y.shape[1]:\n        return -1\n\n    # Compute the Gram matrix and return as nested lists.\n    return (x @ y.T).tolist()\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef _validate_shape(shape: Tuple[int, int]) -> Tuple[int, int]:\n    \"\"\"Internal helper to validate the (rows, cols) shape tuple.\"\"\"\n    if (not isinstance(shape, tuple) or len(shape) != 2 or\n        not all(isinstance(x, int) and x >= 0 for x in shape)):\n        raise ValueError(\"shape must be a tuple of two non-negative integers.\")\n    return shape  # the same object is fine \u2013 just signal validation\n\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = _validate_shape(shape)\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = _validate_shape(shape)\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix whose elements are drawn uniformly from [low, high].\"\"\"\n    rows, cols = _validate_shape(shape)\n\n    # Use a dedicated RNG instance so we don't interfere with global state.\n    rng = random.Random(seed)\n    return [[rng.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Retrieve an initializer function by its name.\n\n    Args:\n        name: The name of the initializer to look up (e.g. \"zeros_init\").\n\n    Returns:\n        The corresponding callable weight initializer.\n\n    Raises:\n        ValueError: If `name` is not found or is not callable.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError(\"Invalid initialization function.\")\n    return obj\n```"}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(\n    X: np.ndarray,\n    pool_shape: tuple[int, int],\n    stride: tuple[int, int],\n    accum_grad: np.ndarray\n) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    pool_shape : (int, int)\n        Height and width of the pooling window  (p_h, p_w).\n    stride : (int, int)\n        Vertical and horizontal stride          (s_h, s_w).\n    accum_grad : np.ndarray\n        Gradient wrt. the pooled output. Shape must match the\n        forward-pass output.\n\n    Returns\n    -------\n    tuple[list, list]\n        (pooled_output, grad_input) \u2013 both rounded to 4 decimals and\n        converted to Python lists.\n    \"\"\"\n    # Unpack shapes and hyper-parameters\n    N, C, H, W   = X.shape\n    p_h, p_w     = pool_shape\n    s_h, s_w     = stride\n\n    # Output spatial dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n\n    # Forward pass ----------------------------------------------------------\n    pooled = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                h_start, h_end = i * s_h, i * s_h + p_h\n                for j in range(out_w):\n                    w_start, w_end = j * s_w, j * s_w + p_w\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, i, j] = window.mean()\n\n    # Back-propagation ------------------------------------------------------\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    scale = 1.0 / (p_h * p_w)         # each element gets an equal share\n\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                h_start, h_end = i * s_h, i * s_h + p_h\n                for j in range(out_w):\n                    w_start, w_end = j * s_w, j * s_w + p_w\n                    g = accum_grad[n, c, i, j] * scale\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += g\n\n    # Round to 4 decimals and convert to lists ------------------------------\n    pooled = np.round(pooled, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n\n    return pooled, grad_input\n```"}
{"task_id": 288, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef squared_error(actual, predicted):\n    \"\"\"Calculate the squared error or mean squared error.\n\n    If *actual* and *predicted* are scalars, return the squared difference.\n    If they are one-dimensional iterables (same length), return the mean squared\n    error.  Return -1 for length/type mismatches or empty iterables.\n\n    All results are rounded to 4 decimal places.\n    \"\"\"\n    # ------------------------------------------\n    # Helper: detect a numeric scalar (Python or NumPy)\n    numeric_scalar = (int, float, np.integer, np.floating)\n    is_scalar_a = isinstance(actual,  numeric_scalar)\n    is_scalar_p = isinstance(predicted, numeric_scalar)\n\n    # -------- case 1: both scalars -------------\n    if is_scalar_a and is_scalar_p:\n        return round((actual - predicted) ** 2, 4)\n\n    # If exactly one is scalar, we cannot proceed\n    if is_scalar_a ^ is_scalar_p:\n        return -1\n\n    # -------- case 2: both should be iterables -------------\n    # Guard against strings (they are iterable but not valid here)\n    if isinstance(actual, (str, bytes)) or isinstance(predicted, (str, bytes)):\n        return -1\n\n    # Convert to NumPy arrays\n    try:\n        a_arr = np.asarray(actual, dtype=float)\n        p_arr = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n\n    # Must be 1-D and of equal non-zero length\n    if a_arr.ndim != 1 or p_arr.ndim != 1:\n        return -1\n    if a_arr.size == 0 or p_arr.size == 0 or a_arr.size != p_arr.size:\n        return -1\n\n    mse = np.mean((a_arr - p_arr) ** 2)\n    return round(float(mse), 4)\n```"}
{"task_id": 289, "completion_id": 0, "completion": "```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    # If the list is empty, simply return an empty string.\n    if not entries:\n        return \"\"\n\n    # Build each line in the required format and join with newline characters.\n    formatted_lines = (f'{field}: \"{value}\"' for field, value in entries)\n    return \"\\n\".join(formatted_lines)\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\n\n_TOL = 1e-8  # Numerical tolerance used everywhere\n\n\ndef _values_equal(a, b):\n    \"\"\"Compare two prediction values with the required tolerance.\"\"\"\n    try:                       # works for scalars, lists and nd-arrays\n        return np.allclose(a, b, atol=_TOL, rtol=_TOL)\n    except TypeError:          # fall-back for objects that allclose cannot handle\n        return a == b\n\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\"\"\"\n    # Same node type?\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _values_equal(tree_a.value, tree_b.value)\n\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return (\n            tree_a.feature == tree_b.feature and\n            np.isclose(tree_a.threshold, tree_b.threshold, atol=_TOL, rtol=_TOL) and\n            compare_trees(tree_a.left, tree_b.left) and\n            compare_trees(tree_a.right, tree_b.right)\n        )\n\n    # One is a Node, the other a Leaf (different shape) \u2192 not equivalent\n    return False\n```"}
{"task_id": 291, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef pca_transform(X: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Project *X* onto its first *n_components* principal components.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (m, d).\n    n_components : int\n        Number of principal components to retain (1 \u2264 n_components \u2264 d).\n\n    Returns\n    -------\n    list[list[float]]\n        The transformed data rounded to 4 decimals, or -1 if *n_components*\n        is outside the valid range.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Basic validation\n    # ------------------------------------------------------------------ #\n    if not isinstance(X, np.ndarray):\n        # The task only specifies the n_components check; other checks are\n        # added for safety but could be omitted.\n        return -1\n    if X.ndim != 2:            # must be 2-D\n        return -1\n    m, d = X.shape\n    if not (1 <= n_components <= d):\n        return -1\n\n    # ------------------------------------------------------------------ #\n    # 2. Covariance matrix of X  (non-standardised PCA)\n    # ------------------------------------------------------------------ #\n    # np.cov centres the data internally and produces a (d\u00d7d) matrix\n    cov_mat = np.cov(X, rowvar=False)\n\n    # ------------------------------------------------------------------ #\n    # 3. Eigen-decomposition\n    # ------------------------------------------------------------------ #\n    eig_vals, eig_vecs = np.linalg.eig(cov_mat)   # eig_vecs: columns = eigenvectors\n    # The covariance matrix is symmetric \u21d2 eigenvalues/vectors are real,\n    # but we take the real part to be safe against tiny imaginaries.\n    eig_vals = eig_vals.real\n    eig_vecs = eig_vecs.real\n\n    # ------------------------------------------------------------------ #\n    # 4. Sort eigenpairs (descending eigenvalues)\n    # ------------------------------------------------------------------ #\n    order = np.argsort(eig_vals)[::-1]            # indices of sorted eigenvalues\n    eig_vecs = eig_vecs[:, order]                 # reorder eigenvectors accordingly\n\n    # ------------------------------------------------------------------ #\n    # 5. Keep first n_components & fix deterministic sign\n    # ------------------------------------------------------------------ #\n    V = eig_vecs[:, :n_components]                # shape: (d, n_components)\n    # Flip sign if first element is negative\n    first_elements_negative = V[0, :] < 0\n    V[:, first_elements_negative] *= -1\n\n    # ------------------------------------------------------------------ #\n    # 6. Project the data and format the output\n    # ------------------------------------------------------------------ #\n    transformed = X @ V                           # shape: (m, n_components)\n    transformed = np.round(transformed, 4)        # round to 4 decimals\n    return transformed.tolist()\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) with the two children as ordinary Python\n        lists, or -1 if the parent matrices do not have identical shapes.\n    \"\"\"\n    # Convert to NumPy arrays (no copy yet, so parents stay untouched)\n    p1 = np.asarray(parent1, dtype=float)\n    p2 = np.asarray(parent2, dtype=float)\n\n    # Shape validation --------------------------------------------------------\n    if p1.ndim != 2 or p2.ndim != 2 or p1.shape != p2.shape:\n        return -1                                          # shape mismatch\n\n    rows, cols = p1.shape\n    if not (0 <= cutoff <= cols):                         # sanity check\n        raise ValueError(f\"cutoff must be in [0, {cols}]\")\n\n    # -------------------------------------------------------------------------\n    # Create children.  Start with copies of the respective \u201cown\u201d parent and\n    # then overwrite the columns *from* cutoff with the other parent\u2019s columns.\n    # -------------------------------------------------------------------------\n    child1 = p1.copy()\n    child2 = p2.copy()\n\n    child1[:, cutoff:] = p2[:, cutoff:]                   # swap right side\n    child2[:, cutoff:] = p1[:, cutoff:]\n\n    # Convert back to plain Python lists and return\n    return child1.tolist(), child2.tolist()\n```"}
{"task_id": 293, "completion_id": 0, "completion": "```python\ndef map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces.  \n    If the agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    # Normalise the input: strip whitespace and make it lower-case\n    key = agent_name.strip().lower()\n\n    # Dictionary that encodes the mapping described in the prompt\n    mapping = {\n        \"dqn\":       (\"CartPole-v0\", \"unwrapped\"),\n        \"ddpg\":      (\"Pendulum-v0\", \"env\"),\n        \"a3c\":       (\"Pendulum-v0\", \"unwrapped\"),\n        \"a2c\":       (\"CartPole-v0\", \"env\"),\n        \"a2c_multi\": (\"CartPole-v0\", \"raw\"),\n        \"trpo\":      (\"Pendulum-v0\", \"unwrapped\"),\n    }\n\n    return mapping.get(key, -1)\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert the custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph with the keys\n          * 'is_directed' : bool\n          * '_V2I'        : dict  (vertex label -> consecutive index 0..n-1)\n          * '_G'          : list  (for each vertex index a list with tuples\n                                   (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary mapping every vertex label to a list of (neighbour, weight)\n        tuples.  Lists are sorted lexicographically by the neighbour label.\n        For undirected graphs every edge appears once in each direction.\n    \"\"\"\n    # ---------- trivial / empty graph cases ---------------------------------\n    if not G or not G.get('_V2I'):\n        return {}                       # no vertices at all\n\n    directed: bool = bool(G.get('is_directed', False))\n\n    # create an empty list for every vertex first (so isolated vertices stay in\n    # the result even if they have no incident edges)\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in G['_V2I']}\n\n    # helper structure to suppress duplicates in the *undirected* case\n    processed_undirected = set()        # stores unordered vertex-pairs\n\n    # iterate over all stored edges\n    for edge_list in G['_G']:\n        for u, v, w in edge_list:\n            if directed:\n                adj[u].append((v, w))\n            else:\n                # build a key that is independent of edge orientation\n                key = (u, v) if u <= v else (v, u)\n                if key in processed_undirected:\n                    continue            # already handled that undirected edge\n                processed_undirected.add(key)\n\n                # add the edge in *both* directions\n                adj[u].append((v, w))\n                adj[v].append((u, w))\n\n    # sort every neighbour list lexicographically by neighbour label\n    for neighbours in adj.values():\n        neighbours.sort(key=lambda t: str(t[0]))\n\n    return adj\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1 (or, for boolean arrays,\n        only ``True``/``False``), otherwise ``False``.\n    \"\"\"\n    # Convert any array-like input to a NumPy array\n    arr = np.asarray(x)\n\n    # Boolean arrays are by definition binary\n    if arr.dtype == np.bool_:\n        return True\n\n    # For all other dtypes, every element must be exactly 0 or 1\n    return np.isin(arr, (0, 1)).all()\n```"}
{"task_id": 297, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef min_max_scale(data: list[list[int | float]], \n                  feature_range: tuple = (0, 1)) -> list[list[float]]:\n    \"\"\"Scale every feature column of *data* to a given interval.\n\n    Parameters\n    ----------\n    data : list[list[int | float]]\n        Two-dimensional numerical data set where each inner list is a sample.\n    feature_range : tuple, optional\n        Desired range for the transformed data given as ``(lower, upper)``.\n        The default is ``(0, 1)``.\n\n    Returns\n    -------\n    list[list[float]]\n        Scaled data rounded to four decimal places.  \n        Returns ``-1`` if ``feature_range`` is invalid.\n    \"\"\"\n    # -----------------------------------------------------------\n    # 1. Validate the desired range\n    # -----------------------------------------------------------\n    if not isinstance(feature_range, tuple) or len(feature_range) != 2:\n        return -1                # malformed feature_range\n    a, b = feature_range\n    if a >= b:                   # user-defined lower must be < upper\n        return -1\n    \n    # -----------------------------------------------------------\n    # 2. Convert the input to a NumPy array (float)\n    # -----------------------------------------------------------\n    if data == []:               # empty input \u2192 nothing to scale\n        return []\n\n    arr = np.asarray(data, dtype=np.float64)\n    if arr.ndim == 1:            # allow passing a single sample\n        arr = arr.reshape(-1, 1)\n\n    # -----------------------------------------------------------\n    # 3. Column-wise min, max and range\n    # -----------------------------------------------------------\n    col_min = arr.min(axis=0)\n    col_max = arr.max(axis=0)\n    col_range = col_max - col_min\n\n    # Treat constant columns: range -> 1 so that (x-min)/range == 0\n    const_mask = col_range == 0\n    col_range[const_mask] = 1.0\n\n    # -----------------------------------------------------------\n    # 4. Apply the Min\u2013Max scaling formula\n    # -----------------------------------------------------------\n    scaled = (arr - col_min) / col_range            # in [0,1]\n    scaled = scaled * (b - a) + a                   # into [a,b]\n\n    # For constant columns explicitly set the result to lower bound `a`\n    if np.any(const_mask):\n        scaled[:, const_mask] = a\n\n    # -----------------------------------------------------------\n    # 5. Clean-up, round, convert back to a plain list\n    # -----------------------------------------------------------\n    scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0)\n    scaled = np.round(scaled, 4)\n\n    return scaled.tolist()\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom typing import List, Tuple, Dict\n\n\ndef _extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n    \"\"\"Return a list of all consecutive n-grams from *tokens*.\"\"\"\n    return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Notes\n    -----\n    \u2022 White-space is used for tokenisation.\n    \u2022 For N > 1 every sentence is padded with N\u22121 '<bol>' tokens in front\n      and a single '<eol>' token at the end.  Unigrams use no padding.\n    \u2022 If any N-gram (or its (N-1)-gram prefix) required for the query is\n      unseen in *corpus*, or if the query is too short, the function\n      returns float('-inf').\n    \u2022 The final result is rounded to 4 decimal places.\n    \"\"\"\n    # --- Guard conditions -------------------------------------------------\n    if N < 1:\n        raise ValueError(\"N must be \u2265 1\")\n    if not corpus:\n        return float(\"-inf\")\n\n    # ---------------------------------------------------------------------\n    # 1. Count N-grams in the training corpus (and prefixes for N > 1)\n    # ---------------------------------------------------------------------\n    if N == 1:\n        unigram_counts: Counter[str] = Counter()\n        total_tokens = 0\n\n        for sent in corpus:\n            tokens = sent.split()\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n\n    else:\n        ngram_counts: Counter[Tuple[str, ...]] = Counter()\n        prefix_counts: Counter[Tuple[str, ...]] = Counter()\n\n        for sent in corpus:\n            tokens = [\"<bol>\"] * (N - 1) + sent.split() + [\"<eol>\"]\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i : i + N])\n                prefix = tuple(tokens[i : i + N - 1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n\n    # ---------------------------------------------------------------------\n    # 2. Prepare the query sentence in exactly the same way\n    # ---------------------------------------------------------------------\n    if N == 1:\n        q_tokens = query.split()\n        if not q_tokens or total_tokens == 0:\n            return float(\"-inf\")\n    else:\n        q_tokens = [\"<bol>\"] * (N - 1) + query.split() + [\"<eol>\"]\n        if len(q_tokens) < N:\n            return float(\"-inf\")\n\n    # ---------------------------------------------------------------------\n    # 3. Compute the summed log-probability\n    # ---------------------------------------------------------------------\n    log_sum = 0.0\n\n    if N == 1:  # ---------- Unigram model --------------------------------\n        for tok in q_tokens:\n            cnt = unigram_counts.get(tok, 0)\n            if cnt == 0:\n                return float(\"-inf\")\n            prob = cnt / total_tokens\n            log_sum += math.log(prob)\n\n    else:       # ---------- N \u2265 2 ----------------------------------------\n        for i in range(len(q_tokens) - N + 1):\n            ngram = tuple(q_tokens[i : i + N])\n            prefix = tuple(q_tokens[i : i + N - 1])\n\n            cnt_ng = ngram_counts.get(ngram, 0)\n            cnt_pre = prefix_counts.get(prefix, 0)\n\n            if cnt_ng == 0 or cnt_pre == 0:\n                return float(\"-inf\")\n\n            prob = cnt_ng / cnt_pre\n            log_sum += math.log(prob)\n\n    # ---------------------------------------------------------------------\n    # 4. Round and return\n    # ---------------------------------------------------------------------\n    return round(log_sum, 4)\n```"}
{"task_id": 299, "completion_id": 0, "completion": "```python\nfrom __future__ import annotations\nimport numpy as np\n\ndef k_means(X: np.ndarray, k: int, max_iterations: int = 500) -> list[int]:\n    \"\"\"K-Means clustering from scratch (deterministic initialisation).\n\n    Args:\n        X: 2-D NumPy array of shape (m, n).\n        k:  Number of clusters (must satisfy 1 \u2264 k \u2264 m).\n        max_iterations: Hard limit on the number of iterations.\n\n    Returns:\n        List[int] of length m; the i-th entry is the cluster index of sample i.\n    \"\"\"\n    # --------------------- basic input checks ------------------------------\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be a NumPy array.\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array (m samples \u00d7 n features).\")\n    \n    m, n = X.shape\n    if not (1 <= k <= m):\n        raise ValueError(\"k must be in the range [1, m].\")\n\n    # work in float \u2013 means are not meaningful in integer dtype\n    X = X.astype(float, copy=False)\n\n    # --------------------- deterministic initial centroids -----------------\n    centroids = X[:k].copy()        # shape (k, n)\n\n    # ------------------------- main loop -----------------------------------\n    for _ in range(max_iterations):\n        # 1. Assign each sample to the closest centroid (euclidean distance).\n        #    We use squared distance to avoid an unnecessary sqrt.\n        diff = X[:, None, :] - centroids[None, :, :]   # shape (m, k, n)\n        distances = np.sum(diff**2, axis=2)            # shape (m, k)\n        labels = np.argmin(distances, axis=1)          # shape (m,)\n\n        # 2. Recompute centroids.\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):                           # non-empty cluster\n                new_centroids[j] = X[mask].mean(axis=0)\n            # else: keep the old centroid unchanged (= requirement)\n\n        # 3. Convergence test.\n        if np.allclose(new_centroids, centroids):\n            break\n        centroids = new_centroids\n\n    # ------------------------- return --------------------------------------\n    return labels.tolist()\n```"}
{"task_id": 300, "completion_id": 0, "completion": "```python\nimport heapq\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef top_k_elements(sequence: List[Number], k: int, order: str = \"largest\") -> List[Number]:\n    \"\"\"Return the k largest or k smallest elements of *sequence*.\n\n    The data stream is inspected exactly once and never keeps more than k\n    elements in memory by exploiting the binary\u2010heap semantics provided by\n    heapq.\n\n    Args:\n        sequence: Iterable of numbers.\n        k: How many extreme values to retain.\n        order: Either \"largest\" (keep the biggest values) or \"smallest\"\n               (keep the smallest values).\n\n    Returns:\n        A list of at most k numbers sorted according to *order*.\n\n    Raises:\n        ValueError: If *order* is neither \"largest\" nor \"smallest\".\n    \"\"\"\n    # -- Trivial cases -------------------------------------------------------\n    if k <= 0 or not sequence:\n        return []\n\n    if order not in {\"largest\", \"smallest\"}:\n        raise ValueError('order must be either \"largest\" or \"smallest\"')\n\n    n = len(sequence)\n    if k >= n:                                # we may simply sort everything\n        return sorted(sequence,\n                      reverse=(order == \"largest\"))\n\n    # -- Main single-pass, O(log k) algorithm --------------------------------\n    if order == \"largest\":\n        # Keep a min-heap that stores the current k largest items.\n        heap: List[Number] = []\n\n        for value in sequence:                # scan each element once\n            if len(heap) < k:\n                heapq.heappush(heap, value)\n            elif value > heap[0]:             # larger than the smallest kept\n                heapq.heapreplace(heap, value)\n\n        # heap contains the k largest values in arbitrary order \u2192 sort\n        return sorted(heap, reverse=True)\n\n    else:  # order == \"smallest\"\n        # Keep a *max*-heap built by storing the negated numbers.\n        heap: List[Number] = []\n\n        for value in sequence:\n            neg_val = -value                  # negate to simulate max-heap\n\n            if len(heap) < k:\n                heapq.heappush(heap, neg_val)\n            elif neg_val > heap[0]:           # smaller original value found\n                heapq.heapreplace(heap, neg_val)\n\n        # Convert back by re-negating and sort ascending.\n        return sorted([-x for x in heap])\n```"}
{"task_id": 301, "completion_id": 0, "completion": "```python\nclass Node:\n    \"\"\"\n    Simple binary-search-tree node that compares itself by ``val`` only.\n    \"\"\"\n    __slots__ = ('key', 'val', 'left', 'right')\n\n    def __init__(self, key, val):\n        self.key = key\n        self.val = val\n        self.left: 'Node | None' = None\n        self.right: 'Node | None' = None\n\n    # comparison operators \u2013 *only* the numeric value matters\n    def __lt__(self, other):   return self.val <  other.val\n    def __le__(self, other):   return self.val <= other.val\n    def __gt__(self, other):   return self.val >  other.val\n    def __ge__(self, other):   return self.val >= other.val\n\n\ndef inorder_keys(pairs):\n    \"\"\"Build a BST from (key, value) *pairs* and return keys from an in-order traversal.\n\n    Args:\n        pairs (list[tuple[Any, int | float]]): List of (key, numeric value) pairs.\n\n    Returns:\n        list: Keys in in-order (ascending by numeric value).  Empty list if *pairs* is empty.\n    \"\"\"\n    # ------------------------------------------------------------------ helpers\n    def insert(root: Node, node: Node) -> None:\n        \"\"\"Iteratively insert *node* into the BST whose root is *root*.\"\"\"\n        cur = root\n        while True:\n            if node < cur:                          # go left\n                if cur.left is None:\n                    cur.left = node\n                    return\n                cur = cur.left\n            else:                                   # go right  (>=)\n                if cur.right is None:\n                    cur.right = node\n                    return\n                cur = cur.right\n\n    def inorder(node: Node | None, out: list) -> None:\n        \"\"\"Recursive in-order traversal collecting *key*s.\"\"\"\n        if node is None:\n            return\n        inorder(node.left, out)\n        out.append(node.key)\n        inorder(node.right, out)\n    # --------------------------------------------------------------------------\n\n    # empty input \u2192 nothing to do\n    if not pairs:\n        return []\n\n    # 1. build the BST ----------------------------------------------------------\n    root = Node(*pairs[0])           # first pair is the root\n    for key, val in pairs[1:]:\n        insert(root, Node(key, val))\n\n    # 2. in-order traversal -----------------------------------------------------\n    result: list = []\n    inorder(root, result)\n    return result\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample (shape = [n_samples, n_features]).\n        k: Desired number of clusters (k \u2264 n_samples).\n\n    Returns:\n        Python list of length n_samples whose entries are integers in [0, k-1].\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Weighted adjacency matrix W  (w_ij = 1 / (1 + ||x_i - x_j||_2))\n    # ------------------------------------------------------------------\n    n_samples = X.shape[0]\n\n    # Pair-wise Euclidean distance matrix  (broadcasting \u2013 O(n\u00b2d) time, O(n\u00b2) memory)\n    diff = X[:, None, :] - X[None, :, :]        # shape (n, n, d)\n    dist = np.linalg.norm(diff, axis=2)         # shape (n, n)\n\n    W = 1.0 / (1.0 + dist)                      # weights\n    np.fill_diagonal(W, 0.0)                   # zero self-loops\n\n    # ------------------------------------------------------------------\n    # 2. Un-normalised graph Laplacian  L = D \u2212 W\n    # ------------------------------------------------------------------\n    degree = W.sum(axis=1)                      # shape (n,)\n    L = np.diag(degree) - W                    # shape (n, n)\n\n    # ------------------------------------------------------------------\n    # 3. k eigenvectors associated with the k smallest eigen-values\n    # ------------------------------------------------------------------\n    # L is symmetric => use eigh (eigenvalues are returned in ascending order)\n    eig_vals, eig_vecs = np.linalg.eigh(L)      # eig_vecs columns are eigenvectors\n    E = eig_vecs[:, :k]                         # projection matrix  (shape n \u00d7 k)\n\n    # ------------------------------------------------------------------\n    # 4. k-means on rows of E\n    #    \u2013 Initial centroids: first k rows of E (deterministic)\n    # ------------------------------------------------------------------\n    centroids = E[:k].copy()                    # shape (k, k)  (centroid i is row i of E)\n    labels = np.full(n_samples, -1, dtype=int)  # current assignments\n    max_iter = 100\n\n    rng = np.random.default_rng()               # for possible re-initialisation\n\n    for _ in range(max_iter):\n        # ----- Assignment step -----\n        # Compute distances of every point to every centroid\n        # distances[i, c] = ||E[i] - centroids[c]||_2\n        dists = np.linalg.norm(E[:, None, :] - centroids[None, :, :], axis=2)  # (n, k)\n        new_labels = dists.argmin(axis=1)\n\n        # Check for convergence\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # ----- Update step -----\n        for c in range(k):\n            mask = labels == c\n            if np.any(mask):                    # non-empty cluster\n                centroids[c] = E[mask].mean(axis=0)\n            else:                               # empty cluster -> re-initialise\n                random_idx = rng.integers(0, n_samples)\n                centroids[c] = E[random_idx]\n\n    # ------------------------------------------------------------------\n    # 5. Return labels as a Python list\n    # ------------------------------------------------------------------\n    return labels.tolist()\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Determine current and previous indices\n    prev_ix = max(ix - 1, 0)\n\n    # Extract previous and current elements\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n\n    # Fetch gold-standard outputs\n    gold_prev = golds.get(prev_label, \"\")\n    gold_curr = golds.get(curr_label, \"\")\n\n    # Header and footer lines\n    header = \"-\" * 25 + \" DEBUG \" + \"-\" * 25\n    footer = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    # Assemble all parts respecting the exact required layout\n    lines = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        f\"{prev_mine}\",\n        \"\",\n        f\"Theirs (prev) [{prev_label}]:\",\n        f\"{gold_prev}\",\n        \"\",\n        f\"Mine [{curr_label}]:\",\n        f\"{curr_mine}\",\n        \"\",\n        f\"Theirs [{curr_label}]:\",\n        f\"{gold_curr}{warn_str}\",\n        footer,\n    ]\n\n    return \"\\n\".join(lines)\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Posterior mean (MAP) of the coefficients in Bayesian linear regression.\n\n    Parameters\n    ----------\n    X : (N, M) array_like\n        Design matrix.\n    y : (N,) array_like\n        Targets.\n    alpha, beta : float\n        Hyper\u2013parameters of the inverse-gamma prior on \u03c3\u00b2 (kept for API\n        completeness \u2013 they do not enter the MAP of the weights).\n    mu : float or array_like\n        Prior mean for the weights.  Scalar values are broadcast.\n    V : None, float, 1-D or 2-D array_like\n        Prior scale.  None\u2192I, scalar\u2192s\u00b7I, 1-D\u2192diag(v), 2-D\u2192full matrix.\n    fit_intercept : bool\n        Whether to include a bias column of ones.\n\n    Returns\n    -------\n    list[float]\n        Posterior mean rounded to 4 decimals.\n    \"\"\"\n    # ---- 1. Basic checks & shapes ------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n\n    N, M = X.shape\n\n    # ---- 2. Prior scale V ---------------------------------------------------\n    if V is None:                                   # Identity\n        V_mat = np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:                         # Scalar\n            V_mat = float(V_arr) * np.eye(M)\n        elif V_arr.ndim == 1:                       # Diagonal specified\n            if V_arr.size != M:\n                raise ValueError(\"Diagonal length of V does not match #features.\")\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:                       # Full matrix\n            if V_arr.shape != (M, M):\n                raise ValueError(\"Shape of V does not match #features.\")\n            V_mat = V_arr\n        else:\n            raise ValueError(\"Unsupported shape for V.\")\n\n    # Inverse of V\n    V_inv = np.linalg.inv(V_mat)\n\n    # ---- 3. Prior mean \u03bc ----------------------------------------------------\n    if np.isscalar(mu):\n        mu_vec = np.full(M, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != M:\n            raise ValueError(\"Length of mu does not match #features.\")\n\n    # ---- 4. Posterior mean (MAP) -------------------------------------------\n    XtX = X.T @ X\n    Sigma_b = np.linalg.inv(V_inv + XtX)            # (V^{-1} + X\u1d40X)^{-1}\n    rhs = V_inv @ mu_vec + X.T @ y                 # V^{-1} \u03bc + X\u1d40 y\n    mu_b = Sigma_b @ rhs                            # Posterior mean\n\n    return np.round(mu_b, 4).tolist()\n```"}
{"task_id": 305, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Compute the element-wise logistic sigmoid in a numerically stable way.\n\n    The function accepts a scalar, list, tuple or NumPy array and returns a\n    Python list with each sigmoid value rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to NumPy array of floats\n    arr = np.asarray(x, dtype=float)\n    \n    # Allocate output array of same shape\n    out = np.empty_like(arr)\n    \n    # Masks for non-negative / negative values\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    \n    # Numerically-stable computation\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    \n    # Round to 4 decimals\n    out = np.round(out, 4)\n    \n    # Return as 1-D Python list (even for scalar input)\n    return out.ravel().tolist()\n```"}
{"task_id": 307, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef bootstrap_sample(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate a bootstrap sample (sampling with replacement) from the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, ...).\n    y : np.ndarray\n        Target vector/array of shape (n_samples, ...).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        (X_sample, y_sample) \u2013 both have exactly the same shapes as the\n        corresponding inputs.\n\n    Notes\n    -----\n    \u2022 Sampling is performed exclusively with NumPy\u2019s global RNG, so results are\n      reproducible if `np.random.seed(...)` is set beforehand.\n    \"\"\"\n    # Ensure the first dimension (number of samples) matches\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\n            \"X and y must have the same number of samples along axis 0 \"\n            f\"(got {X.shape[0]} and {y.shape[0]}).\"\n        )\n\n    n_samples = X.shape[0]\n\n    # Draw indices *with replacement*\n    indices = np.random.randint(0, n_samples, size=n_samples)\n\n    # Fancy-index both arrays; shapes are preserved\n    return X[indices], y[indices]\n```"}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        DFT length (number of frequency-domain samples).\n    n_filters : int, default 20\n        Number of Mel filters to construct.\n    fs : int, default 44_000\n        Sampling frequency of the signal in Hertz.\n    min_freq : int, default 0\n        Lower frequency limit of the filterbank in Hertz.\n    max_freq : int | None, optional\n        Upper frequency limit of the filterbank in Hertz.  If *None* the\n        Nyquist frequency *fs/2* is used.\n    normalize : bool, default True\n        Whether to scale each filter so that its (Mel-domain) area equals 1.\n\n    Returns\n    -------\n    list[list[float]]\n        The filterbank matrix with shape (n_filters, N//2 + 1).  All values are\n        rounded to four decimal places.\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # Helper conversions\n    # ------------------------------------------------------------------\n    def hz2mel(f_hz: np.ndarray | float) -> np.ndarray | float:\n        return 2595.0 * np.log10(1.0 + np.asarray(f_hz) / 700.0)\n\n    def mel2hz(mel: np.ndarray | float) -> np.ndarray | float:\n        return 700.0 * (10**(np.asarray(mel) / 2595.0) - 1.0)\n\n    # ------------------------------------------------------------------\n    # Sanity checks & parameter handling\n    # ------------------------------------------------------------------\n    if max_freq is None:\n        max_freq = fs / 2\n\n    if not (0 <= min_freq < max_freq <= fs / 2):\n        raise ValueError(\"min_freq must be >= 0, and max_freq must not \"\n                         \"exceed fs/2 with min_freq < max_freq.\")\n\n    # ------------------------------------------------------------------\n    # 1. Equally spaced points on the Mel axis (corners of the triangles)\n    # ------------------------------------------------------------------\n    mel_min, mel_max = hz2mel(min_freq), hz2mel(max_freq)\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel2hz(mel_points)                       # (n_filters + 2,)\n\n    # ------------------------------------------------------------------\n    # 2. Centre frequency of every DFT bin\n    # ------------------------------------------------------------------\n    bin_freqs = np.arange(N // 2 + 1) * (fs / N)         # (N//2 + 1,)\n\n    # ------------------------------------------------------------------\n    # 3. Build the triangular filters\n    # ------------------------------------------------------------------\n    F = np.zeros((n_filters, N // 2 + 1), dtype=float)\n\n    for i in range(n_filters):\n        f_left, f_center, f_right = hz_points[i:i + 3]\n\n        # Rising slope (left part of the triangle)\n        left_mask = np.logical_and(bin_freqs >= f_left, bin_freqs <= f_center)\n        F[i, left_mask] = (bin_freqs[left_mask] - f_left) / (f_center - f_left)\n\n        # Falling slope (right part of the triangle)\n        right_mask = np.logical_and(bin_freqs >= f_center, bin_freqs <= f_right)\n        F[i, right_mask] = (f_right - bin_freqs[right_mask]) / (f_right - f_center)\n\n        # Optional normalisation so the area in Mel space equals 1\n        if normalize:\n            F[i, :] *= 2.0 / (f_right - f_left)\n\n    # ------------------------------------------------------------------\n    # 4. Round to 4 decimal places and convert to plain Python list\n    # ------------------------------------------------------------------\n    F_rounded = np.round(F, 4).tolist()\n    return F_rounded\n```"}
{"task_id": 309, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: np.ndarray, w0: float, w: np.ndarray, v: np.ndarray) -> list[float]:\n    \"\"\"Return Factorization Machine predictions for all samples in *X*.\n    \n    Parameters\n    ----------\n    X  : (m, n) np.ndarray\n        Feature matrix with m samples and n features.\n    w0 : float\n        Global bias term.\n    w  : (n,) np.ndarray\n        Linear weight vector.\n    v  : (n, k) np.ndarray\n        Factor matrix whose columns contain the latent factors.\n    \n    Returns\n    -------\n    list[float]\n        FM predictions for each sample, rounded to four decimal places.\n    \"\"\"\n    # Linear part:  w0 + <w, x_i>\n    linear_term = X @ w                       # shape (m,)\n\n    # Interaction part:\n    # (X @ v)          -> shape (m, k)\n    # (X @ v) ** 2     -> (x_i^T v_f)^2\n    # (X**2) @ (v**2)  -> (x_i^{\u25e62})^T (v_f^{\u25e62})\n    xv        = X @ v\n    xv_sq     = xv ** 2\n    x_sq_v_sq = (X ** 2) @ (v ** 2)\n\n    interaction_term = 0.5 * np.sum(xv_sq - x_sq_v_sq, axis=1)  # shape (m,)\n\n    # Final prediction for every sample\n    y_hat = w0 + linear_term + interaction_term\n\n    # Round to 4 decimals and convert to Python list\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 310, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef root_mean_squared_error(actual: list[float] | np.ndarray, predicted: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Root Mean Squared Error (RMSE) between two numeric sequences.\n\n    The function returns ``-1`` if the two inputs do not have the same\n    non-zero length.\n\n    Args:\n        actual: A list or NumPy array containing the true values.\n        predicted: A list or NumPy array containing the predicted values.\n\n    Returns:\n        A float \u2013 the RMSE rounded to 4 decimal places, or ``-1`` on invalid\n        input.\n    \"\"\"\n    # Validate that both inputs are sized sequences of equal, non-zero length\n    try:\n        n_actual = len(actual)\n        n_pred   = len(predicted)\n    except TypeError:            # One (or both) objects is not a sized iterable\n        return -1\n\n    if n_actual == 0 or n_pred == 0 or n_actual != n_pred:\n        return -1\n\n    # Convert to NumPy arrays (floats) for vectorised computation\n    actual_arr    = np.asarray(actual, dtype=float)\n    predicted_arr = np.asarray(predicted, dtype=float)\n\n    # Compute RMSE\n    mse  = np.mean((actual_arr - predicted_arr) ** 2)\n    rmse = np.sqrt(mse)\n\n    # Round to 4 decimal places and return as Python float\n    return round(float(rmse), 4)\n```"}
{"task_id": 311, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef clip_non_negative(arr):\n    \"\"\"Replace every negative number in *arr* with 0.0.\n\n    Parameters\n    ----------\n    arr : list | numpy.ndarray\n        A list (possibly nested) or NumPy array containing numeric values.\n\n    Returns\n    -------\n    list\n        A Python list of the same shape where all negative values are replaced\n        by 0.0. All elements in the returned list are floats.\n    \"\"\"\n    # 1. Convert to a *new* NumPy array of floats.\n    #    Using np.array(..., copy=True) guarantees that we never modify the\n    #    original array in-place, even when *arr* is already a NumPy array.\n    arr_np = np.array(arr, dtype=float, copy=True)\n\n    # 2. Vectorised clipping: any value below 0 becomes 0.0\n    clipped = np.clip(arr_np, a_min=0.0, a_max=None)\n\n    # 3. Return as a (possibly nested) Python list of floats.\n    return clipped.tolist()\n```"}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # ---- special-case handling ---------------------------------------------\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # ---- choose the normalising divisor N ----------------------------------\n    # For the symmetric form we follow the textbook definition N = L-1.\n    # For the periodic form we mimic a window of length L+1 with the\n    # last sample dropped, which is equivalent to setting N = L.\n    N = window_len - 1 if symmetric else window_len\n\n    # ---- Blackman\u2013Harris coefficients --------------------------------------\n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n\n    # ---- sample indices -----------------------------------------------------\n    n = np.arange(window_len)\n\n    # ---- window calculation -------------------------------------------------\n    w = (a0\n         - a1 * np.cos(2 * np.pi * n / N)\n         + a2 * np.cos(4 * np.pi * n / N)\n         - a3 * np.cos(6 * np.pi * n / N))\n\n    # ---- round to 4 decimal places and return as a list ---------------------\n    return [round(float(x), 4) for x in w]\n```\n"}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Return E[log X_t] for X ~ Dir(gamma[d, :]), rounded to 4 decimals.\"\"\"\n    \n    # -------- digamma implementation ------------------------------------ #\n    def digamma(x):\n        \"\"\"\n        Digamma \u03c8(x) for positive real x.\n        Small arguments are stepped up with the recurrence\n            \u03c8(x) = \u03c8(x + 1) \u2212 1/x\n        until x \u2265 6, after which an asymptotic expansion is applied:\n            \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n        The combination is accurate to better than 1e-6 for all x > 0.\n        \"\"\"\n        res = 0.0\n        x = float(x)\n        while x < 6.0:     # step up to a comfortable range\n            res -= 1.0 / x\n            x += 1.0\n\n        inv  = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n\n        res += (math.log(x)\n                - 0.5 * inv\n                - inv2 / 12.0\n                + inv4 / 120.0\n                - inv6 / 252.0)\n        return res\n    # -------------------------------------------------------------------- #\n\n    # ensure `gamma` behaves like a 2-D array\n    g = np.asarray(gamma, dtype=float)\n\n    row      = g[d]\n    alpha_t  = row[t]\n    row_sum  = float(row.sum())\n\n    value = digamma(alpha_t) - digamma(row_sum)\n    return round(value, 4)\n```"}
{"task_id": 315, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _to_array(x):\n    \"\"\"Convert *x* to a NumPy float array.\"\"\"\n    return np.asarray(x, dtype=float)\n\n# --------------------------------------------------------------------------- #\n#                               Loss routines                                 #\n# --------------------------------------------------------------------------- #\ndef _mae(y_true, y_pred):\n    \"\"\"Mean Absolute Error.\"\"\"\n    y_true, y_pred = _to_array(y_true), _to_array(y_pred)\n    return float(np.round(np.mean(np.abs(y_true - y_pred)), 4))\n\n\ndef _mse(y_true, y_pred):\n    \"\"\"Mean Squared Error.\"\"\"\n    y_true, y_pred = _to_array(y_true), _to_array(y_pred)\n    return float(np.round(np.mean((y_true - y_pred) ** 2), 4))\n\n\ndef _bce(y_true, y_pred):\n    \"\"\"Binary Cross-Entropy / Log-loss.\"\"\"\n    y_true, y_pred = _to_array(y_true), _to_array(y_pred)\n    y_pred = np.clip(y_pred, 1e-15, 1.0 - 1e-15)          # avoid log(0)\n    loss = -np.mean(y_true * np.log(y_pred) +\n                    (1.0 - y_true) * np.log(1.0 - y_pred))\n    return float(np.round(loss, 4))\n\n\n# --------------------------------------------------------------------------- #\n#                                Dispatcher                                   #\n# --------------------------------------------------------------------------- #\ndef get_loss(name: str):\n    \"\"\"Return the loss function identified by *name* (case-insensitive).\n\n    Recognised aliases (underscores ignored):\n        MAE  \u2013  \"mae\", \"mean_absolute_error\"\n        MSE  \u2013  \"mse\", \"mean_squared_error\"\n        BCE  \u2013  \"cross_entropy\", \"log_loss\", \"ce\"\n\n    Raises\n    ------\n    ValueError\n        If *name* does not correspond to a supported loss.\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError(\"Invalid loss function.\")\n\n    # Canonicalise: lower-case and remove underscores\n    key = name.replace(\"_\", \"\").lower()\n\n    # Map canonical identifiers to the actual implementations\n    registry = {\n        \"mae\": _mae,\n        \"meanabsoluteerror\": _mae,\n\n        \"mse\": _mse,\n        \"meansquarederror\": _mse,\n\n        \"crossentropy\": _bce,\n        \"logloss\": _bce,\n        \"ce\": _bce,\n    }\n\n    try:\n        return registry[key]\n    except KeyError:\n        raise ValueError(\"Invalid loss function.\")\n```"}
{"task_id": 316, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef euclidean(x, y):\n    \"\"\"Compute the Euclidean (L2) distance between two equal-length vectors.\n\n    Args:\n        x (list | tuple | np.ndarray): First real vector.\n        y (list | tuple | np.ndarray): Second real vector.\n\n    Returns:\n        float | int: L2 distance rounded to four decimals, or -1 if lengths differ.\n    \"\"\"\n    # Quick length check (works for list/tuple/1-D ndarray alike)\n    if len(x) != len(y):\n        return -1\n\n    # Convert to NumPy arrays (does nothing if they already are)\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # Euclidean distance\n    dist = np.linalg.norm(x_arr - y_arr)\n\n    # Round to 4 decimal places and return\n    return round(dist, 4)\n```"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    # indices and data\n    prev_ix = max(ix - 1, 0)\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    header  = \"-\" * 25 + \" DEBUG \"     + \"-\" * 25\n    footer  = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    parts = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_pred,\n        \"\",                                   # blank line before Theirs (prev)\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        \"\",                                   # 1st blank line after prev block\n        \"\",                                   # 2nd blank line after prev block\n        f\"Mine [{curr_label}]:\",\n        curr_pred,\n        \"\",                                   # blank line before Theirs (curr)\n        f\"Theirs [{curr_label}]:\",\n        f\"{curr_gold}{warn_str}\",\n        footer\n    ]\n\n    return \"\\n\".join(parts)\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef _sign(z):\n    \"\"\"Return -1 for z < 0, +1 otherwise (i.e. sign(0)=+1).\"\"\"\n    return np.where(z < 0, -1, 1)\n\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples with labels \u20131 or +1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (\u20131 or +1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n\n    n_samples, n_features = X_train.shape\n\n    # ---------------------------------------------------------------------\n    # Initial weights\n    # ---------------------------------------------------------------------\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # To keep all learned stumps\n    stumps = []          # list of (j, t, p, alpha)\n\n    # ---------------------------------------------------------------------\n    # Boosting rounds\n    # ---------------------------------------------------------------------\n    for _ in range(n_estimators):\n        best_error = float(\"inf\")\n        best_j = best_t = best_p = None\n\n        # -----------------------------------------------------------------\n        # Search the best decision stump\n        # -----------------------------------------------------------------\n        for j in range(n_features):\n            xj = X_train[:, j]\n            idx = np.argsort(xj)\n            x_sorted = xj[idx]\n            y_sorted = y_train[idx]\n            w_sorted = w[idx]\n\n            # cumulative weights of positive / negative samples\n            pos_mask = (y_sorted == 1)\n            neg_mask = ~pos_mask\n\n            pos_cumsum = np.cumsum(w_sorted * pos_mask)\n            neg_cumsum = np.cumsum(w_sorted * neg_mask)\n\n            pos_total = pos_cumsum[-1]\n            neg_total = neg_cumsum[-1]\n\n            # left side includes first k samples (k = 0..n_samples)\n            pos_left = np.concatenate(([0.0], pos_cumsum))\n            neg_left = np.concatenate(([0.0], neg_cumsum))\n\n            # error if polarity p = +1   (left predicts \u22121, right +1)\n            err_p1 = pos_left + (neg_total - neg_left)\n\n            # error if polarity p = \u22121   (left predicts +1, right \u22121)\n            err_pn1 = neg_left + (pos_total - pos_left)\n\n            for polarity, errors in ((1, err_p1), (-1, err_pn1)):\n                k = int(np.argmin(errors))\n                err = errors[k]\n\n                if err < best_error - 1e-12:   # strict improvement\n                    best_error = err\n                    best_j = j\n                    best_p = polarity\n\n                    # choose threshold: between x_sorted[k-1] and x_sorted[k]\n                    if k == 0:\n                        best_t = x_sorted[0] - 1e-10\n                    elif k == len(x_sorted):\n                        best_t = x_sorted[-1] + 1e-10\n                    else:\n                        best_t = 0.5 * (x_sorted[k - 1] + x_sorted[k])\n\n        # -----------------------------------------------------------------\n        # Compute learner weight alpha\n        # -----------------------------------------------------------------\n        eps = max(best_error, 1e-10)         # avoid log(0)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n\n        # -----------------------------------------------------------------\n        # Update sample weights\n        # -----------------------------------------------------------------\n        preds = best_p * _sign(X_train[:, best_j] - best_t)\n        w *= np.exp(-alpha * y_train * preds)\n        w /= w.sum()                         # normalise\n\n        stumps.append((best_j, best_t, best_p, alpha))\n\n    # ---------------------------------------------------------------------\n    # Prediction on test data\n    # ---------------------------------------------------------------------\n    agg = np.zeros(X_test.shape[0])\n    for j, t, p, alpha in stumps:\n        agg += alpha * p * _sign(X_test[:, j] - t)\n\n    y_pred = _sign(agg).astype(int)\n    return y_pred.tolist()\n```"}
{"task_id": 319, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef stochastic_gradient_descent(\n    X: np.ndarray | list,\n    y: np.ndarray | list,\n    learning_rate: float = 0.01,\n    epochs: int = 1000\n) -> list[float]:\n    \"\"\"Learn a linear model with Stochastic Gradient Descent.\n\n    Args:\n        X: Feature matrix (m\u00d7n) \u2013 NumPy array or list-of-lists.\n        y: Target vector (m,) or (m,1) \u2013 NumPy array or list.\n        learning_rate: Learning rate (\u03b1).\n        epochs: Number of epochs (full passes over the dataset).\n\n    Returns:\n        Python list containing the learned parameters [\u03b8\u2080, \u03b8\u2081, \u2026, \u03b8_n],\n        rounded to 4 decimal places.\n    \"\"\"\n    # 1) Convert inputs to NumPy arrays\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()      # ensure shape (m,)\n\n    m, n = X.shape                              # m samples, n features\n\n    # 2) Add bias term (column of 1s as the first column)\n    X_b = np.hstack((np.ones((m, 1)), X))       # shape: (m, n+1)\n\n    # 3) Initialise weights \u03b8 with zeros\n    theta = np.zeros(n + 1, dtype=float)        # shape: (n+1,)\n\n    # 4) Stochastic Gradient Descent\n    for _ in range(epochs):\n        for i in range(m):                      # original order\n            xi = X_b[i]                         # feature row incl. bias\n            error = (theta @ xi) - y[i]         # scalar error\n            theta -= learning_rate * error * xi # in-place update\n\n    # 5) Round to 4 decimals and return as a plain Python list\n    return [round(val, 4) for val in theta.tolist()]\n```"}
{"task_id": 321, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport string\n\n\ndef genetic_algorithm(target: str,\n                      population_size: int,\n                      mutation_rate: float,\n                      iterations:   int) -> str:\n    \"\"\"Evolve a population of random strings towards *target* and\n    return the fittest string encountered.\"\"\"\n    # ------------------------------------------------------------------\n    # reproducibility\n    np.random.seed(42)\n\n    # ------------------------------------------------------------------\n    # character universe and helpers\n    chars = \" \" + string.ascii_lowercase + string.ascii_uppercase  # 53 symbols\n    char2idx = {c: i for i, c in enumerate(chars)}\n    idx2char = np.array(list(chars))\n\n    L = len(target)\n    target_idx = np.fromiter((char2idx[c] for c in target), dtype=np.int16,\n                             count=L)\n\n    def random_individual() -> str:\n        return \"\".join(np.random.choice(idx2char, size=L))\n\n    def fitness(population):\n        \"\"\"Return fitness np.array of shape (n,) for list/array of strings.\"\"\"\n        pop_idx = np.array([[char2idx[c] for c in s] for s in population],\n                           dtype=np.int16)\n        diff = np.abs(pop_idx - target_idx)  # shape (n, L)\n        score = 1.0 / (diff.sum(axis=1) + 1e-6)\n        return score\n\n    # ------------------------------------------------------------------\n    # 1. initialisation\n    population = [random_individual() for _ in range(population_size)]\n\n    best_individual = \"\"\n    best_score = -np.inf\n\n    # ------------------------------------------------------------------\n    # GA main loop\n    for _ in range(iterations):\n        # 2. evaluate fitness\n        fit = fitness(population)\n\n        # record best\n        idx_best = int(np.argmax(fit))\n        if fit[idx_best] > best_score:\n            best_score = float(fit[idx_best])\n            best_individual = population[idx_best]\n\n        # early\u2013stop if perfect\n        if best_individual == target:\n            break\n\n        # 3. selection (roulette-wheel, without replacement)\n        probs = fit / fit.sum()\n        parent_indices = np.random.choice(population_size,\n                                          size=population_size,\n                                          replace=False,\n                                          p=probs)\n        parents = [population[i] for i in parent_indices]\n\n        # 4.\u20135. crossover & mutation -> build next generation\n        children = []\n        for i in range(0, population_size - 1, 2):\n            p1, p2 = parents[i], parents[i + 1]\n\n            # crossover\n            cx_point = np.random.randint(1, L)  # 1 \u2026 L-1\n            c1 = p1[:cx_point] + p2[cx_point:]\n            c2 = p2[:cx_point] + p1[cx_point:]\n\n            # mutation\n            def mutate(child):\n                if mutation_rate == 0.0:\n                    return child\n                child_arr = np.frombuffer(child.encode('ascii'), dtype='S1')\n                mask = np.random.rand(L) < mutation_rate\n                if mask.any():\n                    child_arr[mask] = np.random.choice(idx2char, size=mask.sum())\n                return b\"\".join(child_arr).decode()\n\n            children.extend((mutate(c1), mutate(c2)))\n\n        # odd population_size: keep the last unpaired parent as is\n        if population_size % 2 == 1:\n            children.append(parents[-1])\n\n        # 6. replacement\n        population = children[:population_size]\n\n    return best_individual\n```"}
{"task_id": 322, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nEPS = 1e-15  # small constant for numerical stability\n\ndef binary_crossentropy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Compute the mean binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        Ground-truth labels containing only 0s and 1s.\n    predicted : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as `actual`.\n\n    Returns\n    -------\n    float\n        Mean BCE value rounded to four decimal places.\n    \"\"\"\n    # Ensure inputs have the same shape\n    if actual.shape != predicted.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same shape.\")\n\n    # Clip predictions to avoid log(0)\n    clipped_pred = np.clip(predicted, EPS, 1.0 - EPS)\n\n    # Binary cross-entropy computation\n    loss = -np.mean(\n        actual * np.log(clipped_pred) +\n        (1.0 - actual) * np.log(1.0 - clipped_pred)\n    )\n\n    # Round to 4 decimal places and return\n    return float(round(loss, 4))\n```"}
{"task_id": 324, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef bilinear_interpolate(X: np.ndarray, x: list[float], y: list[float]) -> list[list[float]]:\n    \"\"\"Estimate pixel values at arbitrary (x, y) positions using bilinear interpolation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Image array of shape (rows, cols, channels).\n    x : list[float]\n        Horizontal (column) coordinates.\n    y : list[float]\n        Vertical (row) coordinates.\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated values where each inner list corresponds to one pixel and\n        contains *channels* numbers rounded to 4 decimal places.  If the input\n        lists `x` and `y` differ in length the function must return an empty\n        list.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0.\u2006Input sanity check\n    # ------------------------------------------------------------------ #\n    if len(x) != len(y):          # unequal-length coordinate lists\n        return []\n    if len(x) == 0:               # nothing to do\n        return []\n\n    # ------------------------------------------------------------------ #\n    # 1.\u2006Make sure the image is 3-D (rows, cols, channels)\n    # ------------------------------------------------------------------ #\n    if X.ndim == 2:               # grayscale image supplied as 2-D\n        X = X[:, :, None]         # add a dummy channel dimension\n    rows, cols, ch = X.shape\n    Xf = X.astype(float)          # work in float\n\n    # ------------------------------------------------------------------ #\n    # 2.\u2006Vectorise the input coordinates\n    # ------------------------------------------------------------------ #\n    xs = np.asarray(x, dtype=float)\n    ys = np.asarray(y, dtype=float)\n\n    # Clip to the valid range first (border pixels are replicated)\n    xs = np.clip(xs, 0, cols - 1)\n    ys = np.clip(ys, 0, rows - 1)\n\n    # ------------------------------------------------------------------ #\n    # 3.\u2006Indices of the 4 surrounding pixels and distance weights\n    # ------------------------------------------------------------------ #\n    # Horizontal direction ------------------------------------------------\n    if cols > 1:\n        x0 = np.floor(xs).astype(int)\n        x0 = np.clip(x0, 0, cols - 2)   # make sure x0+1 is inside\n        x1 = x0 + 1\n        dx = xs - x0                    # fractional distance to the right\n    else:                               # single-column image\n        x0 = x1 = np.zeros_like(xs, dtype=int)\n        dx = np.zeros_like(xs)\n\n    # Vertical direction --------------------------------------------------\n    if rows > 1:\n        y0 = np.floor(ys).astype(int)\n        y0 = np.clip(y0, 0, rows - 2)   # make sure y0+1 is inside\n        y1 = y0 + 1\n        dy = ys - y0                    # fractional distance downward\n    else:                               # single-row image\n        y0 = y1 = np.zeros_like(ys, dtype=int)\n        dy = np.zeros_like(ys)\n\n    # ------------------------------------------------------------------ #\n    # 4.\u2006Gather the 4 neighbouring pixels for every query position\n    # ------------------------------------------------------------------ #\n    Ia = Xf[y0, x0]        # top-left\n    Ib = Xf[y0, x1]        # top-right\n    Ic = Xf[y1, x0]        # bottom-left\n    Id = Xf[y1, x1]        # bottom-right\n\n    # ------------------------------------------------------------------ #\n    # 5.\u2006Bilinear combination\n    # ------------------------------------------------------------------ #\n    wa = (1 - dx) * (1 - dy)     # top-left weight\n    wb = dx * (1 - dy)           # top-right\n    wc = (1 - dx) * dy           # bottom-left\n    wd = dx * dy                 # bottom-right\n\n    # Expand weights to shape (N, 1) so that broadcasting works\n    wa = wa[:, None]\n    wb = wb[:, None]\n    wc = wc[:, None]\n    wd = wd[:, None]\n\n    out = Ia * wa + Ib * wb + Ic * wc + Id * wd  # shape (N, ch)\n\n    # ------------------------------------------------------------------ #\n    # 6.\u2006Round to four decimal places and convert to plain list\n    # ------------------------------------------------------------------ #\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 326, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svm_predict(\n    X: list[list[float]],\n    support_vectors: list[list[float]],\n    lagr_multipliers: list[float],\n    support_vector_labels: list[int],\n    intercept: float,\n    kernel: str = \"rbf\",\n    power: int = 3,\n    gamma: float | None = None,\n    coef: float = 1.0,\n) -> list[int]:\n    \"\"\"Predict labels for a batch of samples using a pre-trained SVM.\"\"\"\n    # ------------------------------------------------------------------\n    # Convert all list-based inputs to NumPy arrays\n    # ------------------------------------------------------------------\n    X       = np.asarray(X, dtype=float)               # (n_query, n_features)\n    SV      = np.asarray(support_vectors, dtype=float) # (m,        n_features)\n    alpha   = np.asarray(lagr_multipliers, dtype=float)\n    y_sv    = np.asarray(support_vector_labels, dtype=float)\n    \n    if X.ndim != 2 or SV.ndim != 2:\n        raise ValueError(\"X and support_vectors must be 2-D (samples \u00d7 features).\")\n    if SV.shape[0] != alpha.size or alpha.size != y_sv.size:\n        raise ValueError(\n            \"Lengths of support_vectors, lagr_multipliers and \"\n            \"support_vector_labels must match.\"\n        )\n    \n    n_query, n_features = X.shape\n    if gamma is None:\n        gamma = 1.0 / n_features\n    \n    # ------------------------------------------------------------------\n    # Compute the kernel matrix K(X, SV)  \u2013  shape (n_query, m)\n    # ------------------------------------------------------------------\n    kernel = kernel.lower()\n    \n    if kernel == \"linear\":\n        # K(u,v) = u\u1d40v\n        K = X @ SV.T                                     # (n_query, m)\n    \n    elif kernel == \"polynomial\":\n        # K(u,v) = (\u03b3 u\u1d40v + coef) ** power\n        K = (gamma * (X @ SV.T) + coef) ** power         # (n_query, m)\n    \n    elif kernel == \"rbf\":\n        # K(u,v) = exp( -\u03b3 ||u \u2212 v||\u00b2 )\n        # Efficient pairwise squared distances:\n        X_sq  = np.sum(X**2, axis=1, keepdims=True)      # (n_query, 1)\n        SV_sq = np.sum(SV**2, axis=1)                    # (m,)\n        sq_dists = X_sq + SV_sq - 2.0 * (X @ SV.T)       # broadcasting\n        K = np.exp(-gamma * sq_dists)                    # (n_query, m)\n    \n    else:\n        raise ValueError(\"Unknown kernel '{}'. Choose from 'linear', 'polynomial', 'rbf'.\".format(kernel))\n    \n    # ------------------------------------------------------------------\n    # Decision function f(x) = \u03a3 \u03b1_i y_i K(x_i, x) + b\n    # ------------------------------------------------------------------\n    alpha_y = alpha * y_sv                               # (m,)\n    decision_values = K @ alpha_y + float(intercept)     # (n_query,)\n    \n    # ------------------------------------------------------------------\n    # Convert raw scores to class labels (\u22121 or 1)\n    # If score == 0 \u2192 predict 1\n    # ------------------------------------------------------------------\n    labels = np.where(decision_values >= 0.0, 1, -1)\n    \n    return labels.astype(int).tolist()\n```"}
{"task_id": 328, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Return a one-element list with the first word whose first character is\n    uppercase, or an empty list if no such word exists.\n    \"\"\"\n    for word in corpus:\n        # Ensure the word is non-empty before accessing word[0]\n        if word and word[0].isupper():\n            return [word]\n    return []\n    \n# ---------------------\n# Example usage & tests\n# ---------------------\nif __name__ == \"__main__\":\n    assert first_capitalized_word([]) == []\n    assert first_capitalized_word([\"hello\", \"world\"]) == []\n    assert first_capitalized_word([\"hello\", \"World\", \"Again\"]) == [\"World\"]\n    assert first_capitalized_word([\"Hello\", \"world\"]) == [\"Hello\"]\n    assert first_capitalized_word([\"\", \"!!\", \"Xylophone\"]) == [\"Xylophone\"]\n    print(\"All tests passed.\")\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dft_bins(N: int, fs: int = 44_000, *, positive_only: bool = True) -> list[float]:\n    \"\"\"Return the centre frequency (Hz) of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        If True, return only the non-negative frequencies (indices\n        0 \u2026 \u230aN/2\u230b).  If False, return all N frequency bins in the same\n        order as `numpy.fft.fftfreq`.  Default is True.\n\n    Returns\n    -------\n    list[float]\n        Frequencies rounded to 4 decimal places.  Empty list if `N`\n        or `fs` is not strictly positive.\n    \"\"\"\n    # Guard against non-positive inputs\n    if N <= 0 or fs <= 0:\n        return []\n\n    if positive_only:\n        # Non-negative part (same as np.fft.rfftfreq)\n        freqs = np.arange(0, N // 2 + 1) * (fs / N)\n    else:\n        # Full set (same ordering as np.fft.fftfreq)\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n\n    # Round to 4 decimal places and convert to a plain Python list\n    return np.round(freqs, 4).tolist()\n```"}
{"task_id": 330, "completion_id": 0, "completion": "```python\ndef update_kernel_settings(\n    parameters: dict,\n    hyperparameters: dict,\n    summary_dict: dict\n) -> tuple[dict, dict]:\n    \"\"\"Return *new* dicts where values found in ``summary_dict`` have been\n    merged into ``parameters`` and/or ``hyperparameters`` without mutating\n    the originals.\n\n    See task description for the exact semantics.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Copy the originals \u2013 never mutate user-supplied dicts\n    # ------------------------------------------------------------------ #\n    new_parameters      = parameters.copy()\n    new_hyperparameters = hyperparameters.copy()\n\n    # ------------------------------------------------------------------ #\n    # 2. \"Flatten\" summary_dict: merge the nested sub-dictionaries into a\n    #    single flat dict called `updates`.\n    # ------------------------------------------------------------------ #\n    updates = {}\n\n    for key, value in summary_dict.items():\n        if key == \"parameters\" and isinstance(value, dict):\n            # merge parameter updates\n            updates.update(value)\n        elif key == \"hyperparameters\" and isinstance(value, dict):\n            # merge hyper-parameter updates\n            updates.update(value)\n        else:\n            # any other top-level key is also an update\n            updates[key] = value\n\n    # ------------------------------------------------------------------ #\n    # 3. Apply the updates where the keys already exist; ignore unknowns\n    # ------------------------------------------------------------------ #\n    for key, value in updates.items():\n        if key in new_parameters:\n            new_parameters[key] = value\n        elif key in new_hyperparameters:\n            new_hyperparameters[key] = value\n        # silently ignore anything else\n\n    # ------------------------------------------------------------------ #\n    # 4. Return *new* dictionaries\n    # ------------------------------------------------------------------ #\n    return new_parameters, new_hyperparameters\n```\n\nThe function:\n\n1. Makes shallow copies of the original `parameters` and `hyperparameters`.\n2. Builds a flat `updates` dictionary by merging  \n   \u2022 `summary_dict[\"parameters\"]` (if present and a dict),  \n   \u2022 `summary_dict[\"hyperparameters\"]` (ditto), and  \n   \u2022 every other top-level key/value pair in `summary_dict`.\n3. Writes the updates into the copied dictionaries only when the key\n   already exists there; otherwise the key is ignored.\n4. Returns the two new dictionaries as a tuple."}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix whose element (i, j) is True/1 iff sample i\n        was out of bag for tree j, same shape as *predictions*.\n\n    Returns\n    -------\n    float\n        OOB mean squared error rounded to 4 decimal places, or -1 if it cannot\n        be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    preds   = np.asarray(predictions, dtype=float)\n    mask    = np.asarray(oob_mask).astype(bool)\n\n    if preds.shape != mask.shape:\n        raise ValueError(\"`predictions` and `oob_mask` must have the same shape.\")\n\n    # Number of OOB predictions for every sample\n    oob_counts = mask.sum(axis=1)\n\n    # Samples with at least one OOB estimate\n    valid_idx = oob_counts > 0\n    if not np.any(valid_idx):\n        return -1                                   # No OOB information\n\n    # Mean of OOB predictions for each sample\n    oob_sums  = (preds * mask).sum(axis=1)\n    oob_means = np.zeros_like(y_true)\n    oob_means[valid_idx] = oob_sums[valid_idx] / oob_counts[valid_idx]\n\n    # Squared errors on valid samples\n    sq_err = (oob_means[valid_idx] - y_true[valid_idx]) ** 2\n    mse    = sq_err.mean()\n\n    return round(float(mse), 4)\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree represented with plain nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of *transactions* an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a mapping:\n        {\"support\": <int>, \"children\": {item -> child-node}}\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # 1) Support per item (occurrence in distinct transactions)\n    # ---------------------------------------------------------------------\n    item_counter = Counter()\n    for trx in transactions:\n        for item in set(trx):          # duplicates inside a transaction do not matter\n            item_counter[item] += 1\n\n    # ---------------------------------------------------------------------\n    # 2) Keep only frequent items\n    # ---------------------------------------------------------------------\n    freq_items = {item: sup for item, sup in item_counter.items()\n                  if sup >= min_support}\n    if not freq_items:                         # nothing is frequent \u2192 empty FP-tree\n        return {\"support\": 0, \"children\": {}}\n\n    # ---------------------------------------------------------------------\n    # 3) Global ordering: by decreasing support, then alphabetically\n    # ---------------------------------------------------------------------\n    ordered = sorted(freq_items.items(),\n                     key=lambda pair: (-pair[1], pair[0]))\n    order_rank = {item: rank for rank, (item, _) in enumerate(ordered)}\n    frequent_set = set(freq_items)\n\n    # ---------------------------------------------------------------------\n    # 4) Build the tree\n    # ---------------------------------------------------------------------\n    root = {\"support\": 0, \"children\": {}}      # root support stays 0\n\n    for trx in transactions:\n        # keep only frequent, unique items and order them\n        filtered = [item for item in set(trx) if item in frequent_set]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda it: order_rank[it])\n\n        # walk / extend path, updating supports\n        node = root\n        for item in filtered:\n            children = node[\"children\"]\n            if item not in children:\n                children[item] = {\"support\": 0, \"children\": {}}\n            node = children[item]\n            node[\"support\"] += 1               # visited by this transaction\n\n    return root\n```"}
{"task_id": 333, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np     # (kept to match the given template)\n\ndef logistic_regression(X: np.ndarray, y: np.ndarray,\n                        alpha: float = 0.1,\n                        num_iter: int = 10_000) -> list[int]:\n    \"\"\"Train a logistic-regression classifier with batch gradient descent and\n    return the predicted labels for the *training* data.\n\n    Parameters\n    ----------\n    X : np.ndarray (m, n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Binary targets (0 / 1).\n    alpha : float, default=0.1\n        Learning-rate.\n    num_iter : int, default=10_000\n        Number of GD iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for the input samples.\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # 1. Z-score normalisation (column-wise) with numerical safety term.\n    # ------------------------------------------------------------------\n    eps   = 1e-8\n    mu    = X.mean(axis=0)                # (n,)\n    sigma = X.std(axis=0) + eps           # (n,)\n    Xn    = (X - mu) / sigma              # (m, n)\n\n    # ------------------------------------------------------------------\n    # 2. Bias augmentation.\n    # ------------------------------------------------------------------\n    m, n        = Xn.shape\n    X_aug       = np.hstack((np.ones((m, 1)), Xn))   # (m, n+1)\n\n    # ------------------------------------------------------------------\n    # 3. Parameter initialisation.\n    # ------------------------------------------------------------------\n    theta = np.zeros(n + 1)                              # (n+1,)\n\n    # ------------------------------------------------------------------\n    # 4. Batch Gradient Descent optimisation.\n    # ------------------------------------------------------------------\n    for _ in range(num_iter):\n        z      = X_aug @ theta               # (m,)\n        h      = 1 / (1 + np.exp(-z))        # sigmoid\n        grad   = (X_aug.T @ (h - y)) / m     # (n+1,)\n        theta -= alpha * grad\n\n    # ------------------------------------------------------------------\n    # 5. Prediction with 0.5 threshold.\n    # ------------------------------------------------------------------\n    probs  = 1 / (1 + np.exp(-X_aug @ theta))\n    preds  = (probs >= 0.5).astype(int)\n\n    # ------------------------------------------------------------------\n    # 6. Return as a plain Python list.\n    # ------------------------------------------------------------------\n    return preds.tolist()\n```"}
{"task_id": 334, "completion_id": 0, "completion": "```python\nimport math\n\ndef noam_learning_rate(step: int,\n                       model_dim: int = 512,\n                       scale_factor: float = 1.0,\n                       warmup_steps: int = 4000) -> float:\n    \"\"\"Return the learning-rate given by the Noam schedule.\n\n    lr = scale_factor * model_dim^{-0.5} *\n         min(step^{-0.5}, step * warmup_steps^{-1.5})\n\n    The result is rounded to 9 decimal places.\n    \"\"\"\n    # If step is smaller than 1 \u2192 learning-rate is defined as 0\n    if step < 1:\n        return 0.0\n\n    # Pre-compute constant parts\n    model_term   = model_dim ** -0.5          # model_dim^{-0.5}\n    warmup_term  = warmup_steps ** -1.5       # warmup_steps^{-1.5}\n\n    # Two possible scalings according to the schedule\n    decay_part   = step ** -0.5               # step^{-0.5}\n    warmup_part  = step * warmup_term         # step * warmup_steps^{-1.5}\n\n    lr = scale_factor * model_term * min(decay_part, warmup_part)\n\n    return round(lr, 9)\n```"}
{"task_id": 335, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef get_initializer(name: str):\n    \"\"\"Return a weight-initialisation callable associated with *name*.\n\n    The callable has the signature\n        initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray\n\n    Supported strategies (case-insensitive)\n        - \"zeros\"            \u2013 array of zeros          (scale ignored)\n        - \"ones\"             \u2013 ones  * scale\n        - \"normal\"           \u2013 N(0, scale)\n        - \"glorot_uniform\"   \u2013 U[\u2212limit, +limit] with\n                               limit = scale * sqrt(6 / (fan_in + fan_out))\n\n    Any other *name* raises ValueError.\n    \"\"\"\n    name_lc = name.lower()\n\n    # ----- Zeros -----------------------------------------------------------\n    if name_lc == \"zeros\":\n\n        def initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:  # scale ignored\n            return np.zeros(shape, dtype=float)\n\n        return initializer\n\n    # ----- Ones ------------------------------------------------------------\n    if name_lc == \"ones\":\n\n        def initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            return np.ones(shape, dtype=float) * scale\n\n        return initializer\n\n    # ----- Normal ----------------------------------------------------------\n    if name_lc == \"normal\":\n\n        def initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            return np.random.normal(loc=0.0, scale=scale, size=shape)\n\n        return initializer\n\n    # ----- Glorot / Xavier uniform ----------------------------------------\n    if name_lc == \"glorot_uniform\":\n\n        def initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            if len(shape) < 2:\n                raise ValueError(\"glorot_uniform requires at least 2-D shape\")\n            fan_in, fan_out = shape[0], shape[1]\n            limit = scale * np.sqrt(6.0 / (fan_in + fan_out))\n            return np.random.uniform(low=-limit, high=limit, size=shape)\n\n        return initializer\n\n    # ----------------------------------------------------------------------\n    raise ValueError(f\"Unknown initialisation strategy: {name}\")\n```"}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(\n    X: list[list[int | float]] | \"np.ndarray\",\n    Y: list[list[int | float]] | \"np.ndarray\" | None = None,\n    c0: int | float = 0,\n) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n    Y : 2-D list or NumPy array with shape (M, C) or None.  If None, Y = X.\n    c0 : additive constant of the kernel (default 0).\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u20111 for incompatible shapes.\n    \"\"\"\n    # ---------- helper: convert to 2-D float array ----------\n    def _to_2d_float_array(obj):\n        arr = np.asarray(obj, dtype=float)\n        if arr.ndim == 0:               # scalar \u2192 1\u00d71\n            arr = arr.reshape(1, 1)\n        elif arr.ndim == 1:             # 1-D \u2192 row-vector 1\u00d7C\n            arr = arr.reshape(1, -1)\n        return arr\n\n    # Empty X \u2192 return empty list immediately\n    if isinstance(X, (list, tuple)) and len(X) == 0:\n        return []\n\n    X_arr = _to_2d_float_array(X)\n\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        # Empty X but non-empty Y: still convert X correctly\n        if isinstance(Y, (list, tuple)) and len(Y) == 0:\n            Y_arr = _to_2d_float_array(Y)\n        else:\n            Y_arr = _to_2d_float_array(Y)\n\n    # Validation rule: same number of columns\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n\n    # Compute linear kernel matrix\n    K = X_arr @ Y_arr.T\n    if c0 != 0:\n        K = K + c0\n\n    # Round to 4 decimal places and return as plain Python lists\n    return np.round(K.astype(float), 4).tolist()\n```"}
{"task_id": 337, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nn_interpolate_2D(X: 'np.ndarray', x: list[float | int], y: list[float | int]) -> list:\n    \"\"\"Nearest-Neighbour interpolation on a 2-D regular grid.\n\n    Args:\n        X: Image of shape (rows, cols, channels).\n        x: List of x (column) coordinates.\n        y: List of y (row) coordinates.\n\n    Returns:\n        Python list with shape (k, channels) containing the interpolated\n        pixel values, where k == len(x) == len(y).\n    \"\"\"\n    # Fast exit for empty coordinate lists\n    if not x and not y:\n        return []\n\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length\")\n\n    rows, cols, _ = X.shape\n\n    # Convert to NumPy arrays\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # 1. Nearest-neighbour (banker's) rounding\n    x_idx = np.around(x_arr).astype(int)\n    y_idx = np.around(y_arr).astype(int)\n\n    # 2. Clip to valid image bounds\n    x_idx = np.clip(x_idx, 0, cols - 1)\n    y_idx = np.clip(y_idx, 0, rows - 1)\n\n    # 3. Gather pixels (vectorised fancy indexing)\n    out = X[y_idx, x_idx]          # shape: (k, channels)\n\n    # Return as regular Python list\n    return out.tolist()\n```"}
{"task_id": 338, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(X, y, reg_factor):\n    \"\"\"\n    Closed-form Ridge regression.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Design/feature matrix.\n    y : array-like of shape (n_samples,)\n        Target vector.\n    reg_factor : float, >= 0\n        L2 regularisation strength (\u03b1).\n\n    Returns\n    -------\n    list\n        Weight vector [bias, w1, \u2026, wd] rounded to 4 decimals.\n    \"\"\"\n    # Convert to NumPy arrays (float)\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()          # ensure 1-D\n\n    n_samples = X.shape[0]\n\n    # Add bias column (leading ones)\n    X_bar = np.hstack((np.ones((n_samples, 1)), X))\n\n    # (X\u0304\u1d40 X\u0304 + \u03b1 I) w = X\u0304\u1d40 y\n    d_plus_1 = X_bar.shape[1]\n    A = X_bar.T @ X_bar + reg_factor * np.eye(d_plus_1)\n    b = X_bar.T @ y\n\n    # Solve for w\n    w = np.linalg.solve(A, b)\n\n    # Round to 4 decimal places and return as a list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 339, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef most_common_label(y):\n    \"\"\"Return the most frequent label in *y*.\n\n    If *y* is empty, the function returns ``None``.\n    In case of a tie, the label whose first appearance comes earliest\n    in *y* is returned.\n    \"\"\"\n    if not y:                       # requirement 1\n        return None\n\n    counts = Counter(y)             # frequency of each label\n    max_freq = max(counts.values()) # highest frequency found\n\n    # requirement 2: earliest first appearance wins in a tie\n    for label in y:                 # preserves original order\n        if counts[label] == max_freq:\n            return label\n```"}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns:\n        Tuple of python lists rounded to 8 decimal places:\n        (updated_parameter, new_moment, new_variance)\n    \"\"\"\n    # Ensure all inputs are numpy arrays of float type (no copies if not needed)\n    param = np.asarray(param, dtype=np.float64)\n    grad  = np.asarray(grad,  dtype=np.float64)\n    m     = np.asarray(m,     dtype=np.float64)\n    v     = np.asarray(v,     dtype=np.float64)\n\n    # Update biased first and second raw moments\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * (grad ** 2)\n\n    # Bias-corrected moments\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n\n    # Parameter update (freeze if gradient is exactly zero everywhere)\n    if np.all(grad == 0):\n        param_t = param.copy()\n    else:\n        param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    # Helper: convert ndarray to nested python list rounded to 8 decimals\n    def to_rounded_list(arr: np.ndarray) -> list:\n        return np.round(arr, 8).tolist()\n\n    return (\n        to_rounded_list(param_t),\n        to_rounded_list(m_t),\n        to_rounded_list(v_t),\n    )\n```"}
{"task_id": 342, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n    \"\"\"Compute the Manhattan (L1) distance between two real vectors.\n\n    Args:\n        x: List of numbers representing the first vector.\n        y: List of numbers representing the second vector.\n\n    Returns:\n        The L1 distance rounded to 4 decimal places as a float. If the\n        vectors have different lengths, returns -1.\n    \"\"\"\n    # 1. Length check\n    if len(x) != len(y):\n        return -1\n\n    # 2. Vectorised absolute-difference sum\n    #    (works for both ints and floats)\n    dist = np.abs(np.asarray(x, dtype=float) - np.asarray(y, dtype=float)).sum()\n\n    # 3. Round to 4 decimal places and return as float\n    return float(round(dist, 4))\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists with the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Ensure rewards is a NumPy array\n    rewards = np.asarray(rewards)\n\n    # How many samples to retain (at least one)\n    n_samples = theta_samples.shape[0]\n    k = max(1, int(retain_prcnt * n_samples))\n\n    # Indices of the top-k rewards (higher reward \u21d2 better)\n    elite_idx = np.argsort(rewards)[::-1][:k]\n\n    # Elite parameter vectors\n    elite_samples = theta_samples[elite_idx]\n\n    # Per-dimension statistics\n    mean = np.mean(elite_samples, axis=0)\n    var  = np.var(elite_samples, axis=0)  # population variance (ddof=0)\n\n    # Round to 4 decimals and convert to Python lists\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded  = np.round(var, 4).tolist()\n\n    return mean_rounded, var_rounded\n```"}
{"task_id": 344, "completion_id": 0, "completion": "```python\nimport heapq\nimport itertools\nfrom collections import Counter\nfrom typing import Dict, List\n\n\nclass _Node:\n    \"\"\"Internal helper node for the Huffman tree.\"\"\"\n    __slots__ = (\"freq\", \"token\", \"left\", \"right\")\n\n    def __init__(self, freq: int, token: str = \"\", left: \"_Node | None\" = None, right: \"_Node | None\" = None) -> None:\n        self.freq = freq      # combined frequency\n        self.token = token    # leaf-token string or \"\" for internal node\n        self.left = left      # left child  (code bit 0)\n        self.right = right    # right child (code bit 1)\n\n\ndef huffman_encode(tokens: List[str]) -> Dict[str, str]:\n    \"\"\"Generate a Huffman code for the supplied tokens.\n\n    The function returns a dictionary mapping every unique token in *tokens*\n    to its binary, prefix-free Huffman code.\n    \"\"\"\n    # ---------- 0. Trivial cases -------------------------------------------------\n    if not tokens:                           # empty input\n        return {}\n    counter = Counter(tokens)                # frequency table\n    if len(counter) == 1:                    # only one symbol -> code \"0\"\n        return {next(iter(counter)): \"0\"}\n\n    # ---------- 1. Build initial heap -------------------------------------------\n    heap: list[tuple[int, str, int, _Node]] = []\n    seq = itertools.count()                  # unique tiebreaker for heap\n    for tok, freq in counter.items():\n        node = _Node(freq=freq, token=tok)\n        heapq.heappush(heap, (freq, tok, next(seq), node))\n\n    # ---------- 2. Bottom-up Huffman construction -------------------------------\n    while len(heap) > 1:\n        f1, _, _, n1 = heapq.heappop(heap)   # smallest\n        f2, _, _, n2 = heapq.heappop(heap)   # next smallest\n        parent = _Node(freq=f1 + f2, token=\"\", left=n1, right=n2)\n        # Internal nodes use \"\" as lexical key for deterministic ordering.\n        heapq.heappush(heap, (parent.freq, \"\", next(seq), parent))\n\n    # ---------- 3. Traverse tree to create codes --------------------------------\n    root = heap[0][3]\n    codes: Dict[str, str] = {}\n\n    def _walk(node: _Node, prefix: str) -> None:\n        if node.left is None and node.right is None:      # leaf\n            # Safety: if the tree degenerates (should not happen here),\n            # ensure non-empty code.\n            codes[node.token] = prefix or \"0\"\n            return\n        _walk(node.left, prefix + \"0\")\n        _walk(node.right, prefix + \"1\")\n\n    _walk(root, \"\")\n    return codes\n```"}
{"task_id": 345, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef bayesian_linear_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        mu0: np.ndarray,\n        omega0: np.ndarray) -> list[float]:\n    \"\"\"Predict targets for X_test using Bayesian linear regression.\n\n    The function adds an intercept, computes the posterior mean of the\n    weight vector with a Normal prior (\u03bc\u2080, \u03a9\u2080), and returns the predictions\n    rounded to four decimals.\n    \"\"\"\n    # --- 1. Add intercept (bias) column ------------------------------------\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test  = np.ones((X_test.shape[0],  1))\n    X_train_b  = np.hstack((ones_train, X_train))\n    X_test_b   = np.hstack((ones_test,  X_test))\n\n    # --- 2. Compute posterior mean \u03bc_n -------------------------------------\n    XtX            = X_train_b.T @ X_train_b\n    posterior_prec = XtX + omega0                         # (p+1, p+1)\n    rhs            = X_train_b.T @ y_train.ravel() + omega0 @ mu0  # (p+1,)\n\n    # Solve posterior_prec * \u03bc_n = rhs  (more stable than explicit inverse)\n    mu_n = np.linalg.solve(posterior_prec, rhs)           # (p+1,)\n\n    # --- 3. Predict on test data -------------------------------------------\n    y_pred = X_test_b @ mu_n                              # (n_test,)\n\n    # --- 4. Round to four decimals and return list -------------------------\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 346, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef entropy(y):\n    \"\"\"Compute the base-2 Shannon entropy of a vector of integer labels.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        One-dimensional iterable containing non-negative integer class labels.\n\n    Returns\n    -------\n    float\n        The entropy value rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a NumPy array for vectorised operations\n    y = np.asarray(y)\n    n_samples = y.size\n\n    # Empty input \u2192 entropy 0.0 (no uncertainty)\n    if n_samples == 0:\n        return 0.0\n\n    # Obtain class counts\n    _, counts = np.unique(y, return_counts=True)\n\n    # Relative frequencies (probabilities)\n    p = counts / n_samples\n\n    # Shannon entropy (base 2)\n    entropy_val = -np.sum(p * np.log2(p))\n\n    # Round to 4 decimal places\n    return round(float(entropy_val), 4)\n```"}
{"task_id": 347, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef wgan_gp_loss(\n        Y_fake: np.ndarray,\n        module: str,\n        Y_real: np.ndarray | None = None,\n        gradInterp: np.ndarray | None = None,\n        lambda_: float = 10.0) -> float:\n    \"\"\"Compute the WGAN-GP loss for the critic or the generator.\n\n    Args:\n        Y_fake (np.ndarray): Critic outputs on fake samples (shape: (n_ex, \u2026)).\n        module (str): \"C\" for critic loss, \"G\" for generator loss.\n        Y_real (np.ndarray | None, optional): Critic outputs on real samples.\n            Required when *module* == \"C\".\n        gradInterp (np.ndarray | None, optional): Gradients of the critic\n            outputs on the interpolated samples w.r.t. those samples.\n            Required when *module* == \"C\".\n        lambda_ (float, optional): Gradient-penalty coefficient \u03bb.\n\n    Returns:\n        float: Requested loss rounded to four decimals.\n    \"\"\"\n    module = module.upper()\n    if module not in {\"C\", \"G\"}:\n        raise ValueError('module must be either \"C\" or \"G\".')\n\n    # Ensure inputs are NumPy arrays of float type\n    Y_fake = np.asarray(Y_fake, dtype=np.float32)\n\n    # ---------- Generator loss ----------\n    if module == \"G\":\n        loss = -np.mean(Y_fake)\n        return round(float(loss), 4)\n\n    # ---------- Critic loss ----------\n    # Check required inputs\n    if Y_real is None or gradInterp is None:\n        raise ValueError(\"Y_real and gradInterp must be provided for critic loss.\")\n\n    Y_real = np.asarray(Y_real, dtype=np.float32)\n    gradInterp = np.asarray(gradInterp, dtype=np.float32)\n\n    # Mean scores\n    mean_fake = np.mean(Y_fake)\n    mean_real = np.mean(Y_real)\n\n    # Gradient norm \u2016\u2207x D(x)\u2016\u2082 per sample\n    # Collapse all but the first axis (which indexes the samples)\n    if gradInterp.ndim == 1:          # 1-D case (rare but possible)\n        grad_norm = np.abs(gradInterp)\n    else:\n        axes = tuple(range(1, gradInterp.ndim))\n        grad_norm = np.sqrt(np.sum(gradInterp ** 2, axis=axes))\n\n    # Gradient-penalty term\n    gp = np.mean((grad_norm - 1.0) ** 2)\n\n    # Critic loss\n    loss = mean_fake - mean_real + lambda_ * gp\n    return round(float(loss), 4)\n```"}
{"task_id": 348, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric_positive_definite(X: np.ndarray) -> bool:\n    \"\"\"Check whether a real matrix is symmetric and positive-definite.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A two-dimensional NumPy array representing the matrix to test.\n\n    Returns\n    -------\n    bool\n        True if *X* is symmetric and positive-definite, otherwise False.\n    \"\"\"\n    # Must be a NumPy array with exactly two dimensions\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        return False\n\n    n_rows, n_cols = X.shape\n    # Must be square\n    if n_rows != n_cols:\n        return False\n\n    # Require real\u2010valued entries\n    if not np.isrealobj(X):\n        return False\n\n    # Symmetry check (within numerical tolerance)\n    if not np.allclose(X, X.T, atol=1e-8, rtol=1e-8):\n        return False\n\n    # Eigenvalues of a symmetric matrix\n    try:\n        eigvals = np.linalg.eigvalsh(X)\n    except np.linalg.LinAlgError:\n        # Numerical failure in eigenvalue computation \u21d2 treat as non-PD\n        return False\n\n    # Positive-definiteness: all eigenvalues strictly greater than 1e-8\n    return np.all(eigvals > 1e-8)\n```"}
{"task_id": 350, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef polynomial_ridge_regression(X: list[float | int],\n                                y: list[float | int],\n                                degree: int,\n                                reg_factor: float) -> list[float]:\n    \"\"\"Fit a polynomial ridge regression model and return its coefficients.\n\n    Parameters\n    ----------\n    X : list of float or int\n        One-dimensional feature vector.\n    y : list of float or int\n        Target values.\n    degree : int\n        Highest polynomial degree (>=0).\n    reg_factor : float\n        Non\u2013negative ridge regularisation factor (\u03bb).\n\n    Returns\n    -------\n    list[float]\n        The (degree+1) learned coefficients, rounded to 4 decimal places.\n    \"\"\"\n    # --- Convert input to numpy arrays ---------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # --- Build the design / Vandermonde matrix ------------------------------\n    # Each row: [1, x, x^2, \u2026, x^degree]\n    Phi = np.vander(X_arr, N=degree + 1, increasing=True)\n\n    # --- Closed-form ridge solution -----------------------------------------\n    # w = (\u03a6\u1d40\u03a6 + \u03bbI)\u207b\u00b9 \u03a6\u1d40 y\n    XtX = Phi.T @ Phi\n    reg_matrix = reg_factor * np.eye(degree + 1)\n    Xt_y = Phi.T @ y_arr\n\n    # Solve (XtX + \u03bbI) w = Xt_y for w\n    w = np.linalg.solve(XtX + reg_matrix, Xt_y)\n\n    # --- Round to 4 decimal places and return -------------------------------\n    return [round(coef, 4) for coef in w]\n```"}
{"task_id": 351, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef max_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: int) -> list:\n    \"\"\"Perform 2-D max-pooling over a 4-D minibatch of images.\n\n    Args:\n        X: A 4-D NumPy array with shape (N, C, H, W).\n        pool_shape: (p_h, p_w) \u2013 height and width of the pooling window.\n        stride: The number of pixels the window moves vertically and horizontally.\n\n    Returns:\n        Nested Python list with shape (N, C, out_h, out_w) obtained via\n        ``numpy.ndarray.tolist()``.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError(\"Input must be a 4-D tensor of shape (N, C, H, W).\")\n\n    p_h, p_w = pool_shape\n    N, C, H, W = X.shape\n\n    # Calculate spatial dimensions of the pooled output\n    out_h = (H - p_h) // stride + 1\n    out_w = (W - p_w) // stride + 1\n\n    # Build a view that represents all pooling windows\n    # Resulting shape: (N, C, out_h, out_w, p_h, p_w)\n    shape_windows = (N, C, out_h, out_w, p_h, p_w)\n\n    # Original strides: (bytes to move 1 step along each axis)\n    sN, sC, sH, sW = X.strides\n\n    # Strides for the window view:\n    #   \u2013 Moving from one window to the next (out_h / out_w) advances by `stride`\n    #   \u2013 Inside a window (p_h / p_w) advances by 1 pixel\n    stride_windows = (sN,           # same image\n                      sC,           # same channel\n                      sH * stride,  # move down `stride` rows\n                      sW * stride,  # move right `stride` cols\n                      sH,           # move 1 row inside the window\n                      sW)           # move 1 col inside the window\n\n    windows = as_strided(\n        X,\n        shape=shape_windows,\n        strides=stride_windows,\n        writeable=False  # protect the original array\n    )\n\n    # Max over the last two axes (p_h, p_w)\n    pooled = windows.max(axis=(4, 5))\n\n    # Convert to nested Python list as required\n    return pooled.tolist()\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n                class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Convert the (possibly lazy) iterable to a list so we can compute its length\n    label_list = list(labels)\n\n    # Special case 1: empty input\n    if not label_list:                       # works for list/tuple/np.ndarray converted to list\n        return 0.0\n\n    total = len(label_list)\n\n    # Count occurrences of each label\n    counts = Counter(label_list)\n\n    # Special case 2: all labels identical (only one unique key)\n    if len(counts) == 1:\n        return 0.0\n\n    # Compute entropy\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        # skip p == 0 to avoid log(0); by convention it contributes 0\n        if p > 0.0:\n            entropy_val -= p * math.log2(p)\n\n    # Round to five decimal places, as required\n    return round(entropy_val, 5)\n```"}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c):\n    \"\"\"Returns the 4 individual paddings (top, bottom, left, right).\"\"\"\n    # Effective kernel size with dilation\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n\n    # Integer\n    if isinstance(pad, int):\n        return pad, pad, pad, pad\n\n    # 2-tuple\n    if isinstance(pad, tuple) and len(pad) == 2:\n        pr, pc = pad\n        return pr, pr, pc, pc\n\n    # 4-tuple\n    if isinstance(pad, tuple) and len(pad) == 4:\n        return pad\n\n    # 'same'  \u2192 smallest symmetric padding that keeps the\n    # output size equal to the input size (only strictly true for stride=1)\n    if pad == \"same\":\n        total_pad_r = max(eff_k_r - 1, 0)\n        total_pad_c = max(eff_k_c - 1, 0)\n\n        pr1 = total_pad_r // 2\n        pr2 = total_pad_r - pr1\n\n        pc1 = total_pad_c // 2\n        pc2 = total_pad_c - pc1\n        return pr1, pr2, pc1, pc2\n\n    # Fallback (undefined \u2013 per problem statement)\n    return 0, 0, 0, 0\n\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\"\"\"\n    # -----------------------------------------------------------\n    # Shapes & parameters\n    # -----------------------------------------------------------\n    n_ex, in_r, in_c, in_ch = X.shape\n    k_r, k_c, in_ch_w, out_ch = W.shape       # kernel dims\n\n    assert in_ch == in_ch_w, \"Input / kernel channels mismatch\"\n\n    dil = dilation                          # keep name short\n    eff_k_r = k_r + (k_r - 1) * dil         # effective kernel size (rows)\n    eff_k_c = k_c + (k_c - 1) * dil         # effective kernel size (cols)\n\n    # -----------------------------------------------------------\n    # Padding\n    # -----------------------------------------------------------\n    pr1, pr2, pc1, pc2 = _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c)\n\n    X_pad = np.pad(\n        X,\n        pad_width=((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)),\n        mode=\"constant\",\n    )\n\n    pad_r, pad_c = in_r + pr1 + pr2, in_c + pc1 + pc2\n\n    # -----------------------------------------------------------\n    # Output spatial dimensions\n    # -----------------------------------------------------------\n    out_r = (pad_r - eff_k_r) // stride + 1\n    out_c = (pad_c - eff_k_c) // stride + 1\n\n    # -----------------------------------------------------------\n    # im2col \u2013 build the big \u201cpatch\u201d matrix\n    # Resulting shape: (n_ex*out_r*out_c, k_r*k_c*in_ch)\n    # -----------------------------------------------------------\n    cols = np.empty(\n        (n_ex, out_r, out_c, k_r, k_c, in_ch), dtype=X.dtype\n    )\n\n    for kr in range(k_r):\n        row_start = kr * (dil + 1)\n        row_end = row_start + stride * out_r\n        row_slice = slice(row_start, row_end, stride)\n\n        for kc in range(k_c):\n            col_start = kc * (dil + 1)\n            col_end = col_start + stride * out_c\n            col_slice = slice(col_start, col_end, stride)\n\n            cols[:, :, :, kr, kc, :] = X_pad[:, row_slice, col_slice, :]\n\n    # reshape to 2-D matrix (im2col)\n    cols_2d = cols.reshape(n_ex * out_r * out_c, -1)  # (N*OH*OW, KR*KC*IC)\n\n    # -----------------------------------------------------------\n    # Reshape kernels and perform GEMM\n    # -----------------------------------------------------------\n    W_col = W.reshape(-1, out_ch)  # (KR*KC*IC, OC)\n\n    out_mat = cols_2d @ W_col      # (N*OH*OW, OC)\n\n    # -----------------------------------------------------------\n    # Back to 4-D tensor and to Python list\n    # -----------------------------------------------------------\n    out = out_mat.reshape(n_ex, out_r, out_c, out_ch)\n\n    return out.tolist()\n```"}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _deterministic_sign_flip(components: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Flip the sign of each component so that the first non\u2013zero loading is \u2265 0.\n    The operation is done in-place and the resulting array is also returned.\n    \"\"\"\n    for i in range(components.shape[1]):                      # iterate over columns\n        col = components[:, i]\n        # locate the first (in feature order) loading whose absolute value\n        # is not numerically zero\n        for v in col:\n            if abs(v) > 1e-12:\n                if v < 0:\n                    components[:, i] *= -1                    # flip the column\n                break                                         # go to next column\n    return components\n\n\ndef pca_transform(\n    data: np.ndarray,\n    n_components: int,\n    solver: str = \"svd\"\n) -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis with a deterministic sign convention.\"\"\"\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (samples \u00d7 features).\")\n    if n_components < 1:\n        raise ValueError(\"`n_components` must be at least 1.\")\n    if solver not in {\"svd\", \"eigen\"}:\n        raise ValueError(\"`solver` must be either 'svd' or 'eigen'.\")\n\n    n_samples, n_features = data.shape\n    n_components = min(n_components, n_features)              # cannot keep more than #features\n\n    # ------------------------------------------------------------------\n    # 1. Mean-centre the data\n    # ------------------------------------------------------------------\n    X = data.astype(float) - data.mean(axis=0, keepdims=True)\n\n    # ------------------------------------------------------------------\n    # 2. Obtain principal directions + explained variance\n    # ------------------------------------------------------------------\n    if solver == \"svd\":\n        # full_matrices=False gives Vt of shape (min(n_samples,n_features), n_features)\n        _, singular_vals, Vt = np.linalg.svd(X, full_matrices=False)\n        components = Vt.T[:, :n_components]                   # (n_features, n_components)\n        # eigenvalues from singular values\n        expl_var = (singular_vals**2) / (n_samples - 1)\n    else:                                                     # solver == \"eigen\"\n        cov = np.cov(X, rowvar=False, ddof=1)                 # (n_features, n_features)\n        eig_vals, eig_vecs = np.linalg.eigh(cov)              # ascending order\n        idx = eig_vals.argsort()[::-1]                        # descending\n        eig_vals, eig_vecs = eig_vals[idx], eig_vecs[:, idx]\n        components = eig_vecs[:, :n_components]               # (n_features, n_components)\n        expl_var = eig_vals                                   # already variance values\n\n    total_variance = expl_var.sum()\n    explained_variance_ratio = (expl_var / total_variance)[:n_components]\n\n    # ------------------------------------------------------------------\n    # 3. Deterministic sign fix\n    # ------------------------------------------------------------------\n    components = _deterministic_sign_flip(components)\n\n    # ------------------------------------------------------------------\n    # 4. Project the centred data\n    # ------------------------------------------------------------------\n    scores = X @ components                                   # (n_samples, n_components)\n\n    # Every column that was flipped in components is already consistent\n    # because we used the flipped components for the projection.\n\n    # ------------------------------------------------------------------\n    # 5. Round the FINAL results and convert to Python built-ins\n    # ------------------------------------------------------------------\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n\n    return scores_rounded, evr_rounded\n```"}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                    classification tree; otherwise treat it as regression.\n\n    Returns\n    -------\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    # Classification: pick the index of the maximum probability\n    if classifier:\n        # Make sure we can index the sequence (works for list, tuple, np.ndarray, etc.)\n        probs = list(leaf.value)\n\n        best_idx = 0\n        best_val = probs[0]\n        for idx, val in enumerate(probs[1:], start=1):\n            # Strict \">\" guarantees that the first (smallest index) wins ties,\n            # matching numpy.argmax's behaviour.\n            if val > best_val:\n                best_val = val\n                best_idx = idx\n        return best_idx\n\n    # Regression: just return the stored scalar\n    return leaf.value\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids, rounded to four decimals.\n    \"\"\"\n    # Ensure 2-D float array\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array\")\n\n    n_samples, n_features = data.shape\n    if K < 1 or K > n_samples:\n        raise ValueError(\"K must be between 1 and the number of samples\")\n\n    # Special case K == 1\n    if K == 1:\n        centroid = np.mean(data, axis=0, keepdims=True)\n        centroid = np.round(centroid, 4)\n        return centroid.tolist()\n\n    # 1. Deterministic initialisation with first K samples\n    centroids = data[:K].astype(float, copy=True)\n\n    tol = 1e-6\n    for _ in range(max_iters):\n        # 2a. Assign each sample to closest centroid (Euclidean)\n        #     Compute squared distances for efficiency\n        diff = data[:, None, :] - centroids[None, :, :]     # shape (n_samples, K, n_features)\n        dists = np.sum(diff ** 2, axis=2)                   # shape (n_samples, K)\n        labels = np.argmin(dists, axis=1)                   # shape (n_samples,)\n\n        # 2b. Recompute centroids\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:                                # only update if cluster non-empty\n                new_centroids[k] = members.mean(axis=0)\n\n        # 2c. Convergence test\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) <= tol):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # 3. Sort lexicographically and round to four decimals\n    sort_keys = tuple(centroids[:, i] for i in reversed(range(n_features)))\n    order = np.lexsort(sort_keys)\n    centroids_sorted = np.round(centroids[order], 4)\n\n    return centroids_sorted.tolist()\n```"}
{"task_id": 358, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef accuracy(y_true, y_pred):\n    \"\"\"Compute the classification accuracy between two equal-length sequences.\n\n    Args:\n        y_true (list | np.ndarray): Ground-truth class labels.\n        y_pred (list | np.ndarray): Predicted class labels.\n\n    Returns:\n        float | int: Accuracy rounded to 4 decimals, or -1 on length mismatch.\n    \"\"\"\n    # Ensure both inputs have the same length\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1\n\n    # Convert to NumPy arrays for efficient, element-wise comparison\n    y_true_arr = np.asarray(y_true)\n    y_pred_arr = np.asarray(y_pred)\n\n    # Count matches and compute accuracy\n    correct = np.sum(y_true_arr == y_pred_arr)\n    acc = correct / len(y_true_arr)\n\n    # Round to 4 decimal places and return\n    return round(float(acc), 4)\n```"}
{"task_id": 359, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy(y, y_pred, derivative=False):\n    \"\"\"Cross-entropy utility.\n\n    Parameters\n    ----------\n    y : numpy.ndarray\n        One\u2013hot encoded true labels, shape (n_samples, n_classes).\n    y_pred : numpy.ndarray\n        Predicted class probabilities, same shape as *y*.\n    derivative : bool, default False\n        If *True* return the gradient, else return the loss.\n\n    Returns\n    -------\n    float | list[list[float]]\n        Rounded loss (scalar) or gradient (nested list).\n    \"\"\"\n    # ensure input is NumPy array (in case Python lists are passed)\n    y       = np.asarray(y, dtype=float)\n    y_pred  = np.asarray(y_pred, dtype=float)\n\n    eps = np.finfo(float).eps          # tiny constant to avoid log(0)\n\n    if derivative:\n        # gradient of cross-entropy wrt soft-max output\n        grad = y_pred - y\n        # round to 4 decimals and convert to nested Python lists\n        return np.round(grad, 4).tolist()\n    else:\n        # cross-entropy loss (sum over samples, not mean)\n        loss = -np.sum(y * np.log(y_pred + eps))\n        # round to 4 decimals\n        return round(loss, 4)\n```"}
{"task_id": 360, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dct(frame: np.ndarray | list[float], orthonormal: bool = True) -> list[float]:\n    \"\"\"Compute the 1-D DCT-II of *frame* (na\u00efve implementation).\n\n    Parameters\n    ----------\n    frame : 1-D array-like\n        Real-valued input signal (length N).\n    orthonormal : bool, default=True\n        If True, apply orthonormal scaling (so the transform matrix is unitary).\n\n    Returns\n    -------\n    list[float]\n        DCT-II coefficients rounded to four decimal places.\n    \"\"\"\n    # Convert input to a 1-D NumPy array of floats\n    x = np.asarray(frame, dtype=float).ravel()          # ensures 1-D\n    if x.ndim != 1:\n        raise ValueError(\"Input *frame* must be one-dimensional.\")\n    N = x.size\n    if N == 0:\n        return []\n\n    # Indices\n    k = np.arange(N).reshape(-1, 1)      # column vector (N,1)\n    n = np.arange(N).reshape(1, -1)      # row    vector (1,N)\n\n    # Cosine kernel:  cos[ \u03c0 k (2n+1) / (2N) ]\n    cos_kn = np.cos(np.pi * k * (2 * n + 1) / (2 * N))   # shape (N,N)\n\n    # Raw (unnormalised) DCT-II coefficients\n    coeffs = 2.0 * cos_kn @ x            # matrix\u2013vector product  (N,)\n\n    # Optional orthonormal scaling\n    if orthonormal:\n        coeffs[0] *= 1 / np.sqrt(N)          # \u221a(1/N)\n        if N > 1:\n            coeffs[1:] *= np.sqrt(2 / N)     # \u221a(2/N)\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(coeffs, 4).tolist()\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]] | int:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row is divided by its own sum so that every row finally sums to 1.\n    If any row has a sum of exactly 0 normalisation is impossible and \u20131\n    is returned.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or \u20131 when a\n        zero-sum row is encountered.\n    \"\"\"\n    # 1. Convert to float NumPy array\n    arr = np.asarray(data, dtype=float)\n\n    # Ensure a proper 2-D structure\n    if arr.ndim != 2:\n        raise ValueError(\"Input must be a 2-D structure\")\n\n    # 2. Row sums\n    row_sums = arr.sum(axis=1)\n\n    # 3. Check for zero-sum rows\n    if np.any(row_sums == 0):\n        return -1\n\n    # 4. Row-wise normalisation (broadcasting)\n    stochastic = arr / row_sums[:, None]\n\n    # 5. Round to 4 decimals and convert back to Python list\n    return np.round(stochastic, 4).tolist()\n```\n\nExample usage\n-------------\n```python\n>>> row_stochastic_matrix([[1, 2], [3, 3]])\n[[0.3333, 0.6667], [0.5, 0.5]]\n\n>>> row_stochastic_matrix([[0, 0], [1, 2]])\n-1\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite\n    (`Tuple` or `Dict`) spaces the space is continuous only if **all** its\n    immediate sub-spaces are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: True if the action space is known to be composite.\n        tuple_obs:    True if the observation space is known to be composite.\n\n    Returns:\n        A tuple (cont_action, cont_obs) with the continuity result for the\n        action and observation spaces, respectively.\n    \"\"\"\n    def is_simple_box(space: Space) -> bool:\n        \"\"\"Return True only when `space` itself is a `Box`.\"\"\"\n        return isinstance(space, Box)\n\n    def all_subspaces_box(space: Space) -> bool:\n        \"\"\"Return True if every direct sub-space of `space` is a `Box`.\"\"\"\n        if isinstance(space, Tuple):\n            subspaces = space.spaces\n        elif isinstance(space, Dict):\n            subspaces = space.spaces.values()\n        else:  # Not a composite space\n            return False\n        return all(isinstance(s, Box) for s in subspaces)\n\n    # Evaluate action space\n    if tuple_action:\n        cont_action = all_subspaces_box(env.action_space)\n    else:\n        cont_action = is_simple_box(env.action_space)\n\n    # Evaluate observation space\n    if tuple_obs:\n        cont_obs = all_subspaces_box(env.observation_space)\n    else:\n        cont_obs = is_simple_box(env.observation_space)\n\n    return cont_action, cont_obs\n```"}
{"task_id": 364, "completion_id": 0, "completion": "```python\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function returns\n        1  if x > 0\n        0  if x == 0  (handles both 0 and -0.0)\n       -1  if x < 0\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        An integer: -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        # Covers x == 0 and the floating-point edge case -0.0\n        return 0\n```"}
{"task_id": 366, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X (np.ndarray): A 2-D array of shape (n_samples, n_features).\n\n    Returns:\n        list[list[float]]: n \u00d7 n matrix where entry (i, j) is the squared\n            \u2113\u2082 distance between samples i and j.  The diagonal entries are 0.0.\n    \"\"\"\n    # Ensure 2-D representation\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D NumPy array\")\n\n    n_samples = X.shape[0]\n\n    # Trivial single-sample case\n    if n_samples == 1:\n        return [[0.0]]\n\n    # Squared norms of each sample: \u2016x_i\u2016\u00b2\n    sq_norms = np.sum(X * X, axis=1)                       # shape: (n_samples,)\n\n    # Pair-wise squared distances using (\u2016x_i\u2016\u00b2 + \u2016x_j\u2016\u00b2 \u2212 2 x_i\u00b7x_j)\n    D = sq_norms[:, None] + sq_norms[None, :] - 2.0 * (X @ X.T)\n\n    # Numerical clean-up: tiny negative values \u2192 0\n    D[D < 0] = 0.0\n\n    # Exact zeros on the diagonal\n    np.fill_diagonal(D, 0.0)\n\n    # Return as a list of lists of floats\n    return D.tolist()\n```"}
{"task_id": 367, "completion_id": 0, "completion": "```python\nimport re\nimport ast\nfrom typing import Any, Dict, Optional\n\n\nclass KernelBase:\n    \"\"\"Minimal base class for all kernels.  Do NOT modify this class.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        self.params: Dict[str, Any] = kwargs\n\n    def set_params(self, state: Dict[str, Any]):\n        \"\"\"(Re)sets parameters from a previously stored summary dict.\"\"\"\n        hp = state.get(\"hyperparameters\", {})\n        self.params.update({k: v for k, v in hp.items() if k != \"id\"})\n        return self\n\n    def summary(self) -> Dict[str, Any]:\n        \"\"\"Dictionary description that can be fed back to *initialize_kernel*.\"\"\"\n        return {\"hyperparameters\": {\"id\": self.__class__.__name__, **self.params}}\n\n\nclass LinearKernel(KernelBase):\n    def __init__(self):\n        super().__init__()\n\n\nclass PolynomialKernel(KernelBase):\n    def __init__(self, degree: int = 3, coef0: float = 1.0):\n        super().__init__(degree=degree, coef0=coef0)\n\n\nclass RBFKernel(KernelBase):\n    def __init__(self, gamma: float = 1.0):\n        super().__init__(gamma=gamma)\n\n\n# --------------------------------------------------------------------------- #\n#                      The required factory implementation                    #\n# --------------------------------------------------------------------------- #\ndef _str_to_kwargs(arg_string: str) -> Dict[str, Any]:\n    \"\"\"\n    Convert the inside of the \"(\u2026)\" part of a textual kernel description\n    (\"degree=4, coef0=2\") to a dictionary of correctly-typed Python values.\n    \"\"\"\n    kwargs: Dict[str, Any] = {}\n    if not arg_string:\n        return kwargs\n\n    # split on commas that are NOT inside parentheses (good enough here)\n    for token in [t.strip() for t in arg_string.split(\",\") if t.strip()]:\n        if \"=\" not in token:\n            raise ValueError(f\"Malformed argument '{token}'.\")\n        key, value_txt = token.split(\"=\", 1)\n        key, value_txt = key.strip(), value_txt.strip()\n        try:\n            value = ast.literal_eval(value_txt)\n        except Exception:\n            # treat it as (unquoted) string\n            value = value_txt.strip(\"\\\"'\")\n        kwargs[key] = value\n    return kwargs\n\n\ndef initialize_kernel(param: Optional[Any] = None) -> KernelBase:\n    \"\"\"Create and return a kernel object from various representations.\n\n    Args:\n        param: None (default LinearKernel), an existing KernelBase instance, a\n            string description such as 'RBF(gamma=0.5)', or a dictionary with a\n            'hyperparameters' entry as produced by KernelBase.summary().\n\n    Returns:\n        KernelBase: the newly created or forwarded kernel instance.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. default \u2013 None  \u279c  LinearKernel()                               #\n    # ------------------------------------------------------------------ #\n    if param is None:\n        return LinearKernel()\n\n    # ------------------------------------------------------------------ #\n    # 2. already a kernel object                                         #\n    # ------------------------------------------------------------------ #\n    if isinstance(param, KernelBase):\n        return param\n\n    # ------------------------------------------------------------------ #\n    # Mapping (case-insensitive) of names or ids to classes              #\n    # ------------------------------------------------------------------ #\n    _KERNEL_MAP = {\n        \"linear\": LinearKernel,\n        \"linearkernel\": LinearKernel,\n        \"polynomial\": PolynomialKernel,\n        \"polynomialkernel\": PolynomialKernel,\n        \"rbf\": RBFKernel,\n        \"rbfkernel\": RBFKernel,\n    }\n\n    # ------------------------------------------------------------------ #\n    # 3. string description                                              #\n    # ------------------------------------------------------------------ #\n    if isinstance(param, str):\n        txt = param.strip()\n        match = re.fullmatch(r\"\\s*([A-Za-z_]\\w*)\\s*(?:\\((.*)\\))?\\s*\", txt)\n        if not match:  # cannot even parse name\n            raise ValueError(f\"Cannot parse kernel description '{param!s}'.\")\n        k_name, arg_string = match.group(1), match.group(2)\n        cls = _KERNEL_MAP.get(k_name.lower())\n        if cls is None:\n            raise NotImplementedError(f\"Unknown kernel '{k_name}'.\")\n        kwargs = _str_to_kwargs(arg_string or \"\")\n        return cls(**kwargs)\n\n    # ------------------------------------------------------------------ #\n    # 4. dictionary description                                          #\n    # ------------------------------------------------------------------ #\n    if isinstance(param, dict):\n        if \"hyperparameters\" not in param or not isinstance(\n            param[\"hyperparameters\"], dict\n        ):\n            raise ValueError('Dictionary must contain a \"hyperparameters\" key.')\n        hp: Dict[str, Any] = param[\"hyperparameters\"]\n        k_id = hp.get(\"id\")\n        if k_id is None:\n            raise ValueError('\"hyperparameters\" dictionary misses required \"id\".')\n        cls = _KERNEL_MAP.get(k_id.lower())\n        if cls is None:\n            raise NotImplementedError(f\"Unknown kernel id '{k_id}'.\")\n        kwargs = {k: v for k, v in hp.items() if k != \"id\"}\n        return cls(**kwargs)\n\n    # ------------------------------------------------------------------ #\n    # 5. unsupported type                                                #\n    # ------------------------------------------------------------------ #\n    raise TypeError(\n        f\"Unsupported parameter type '{type(param).__name__}' for initialize_kernel.\"\n    )\n```"}
{"task_id": 368, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minkowski(x: np.ndarray | list, y: np.ndarray | list, p: float) -> float:\n    \"\"\"Compute the Minkowski\u2013p distance between two vectors.\n\n    Args:\n        x (np.ndarray | list): First one-dimensional vector.\n        y (np.ndarray | list): Second one-dimensional vector (same length as x).\n        p (float): Order of the norm (must be > 0).\n\n    Returns:\n        float: The Minkowski distance rounded to 4 decimal places,\n               or -1 when the input validation fails.\n    \"\"\"\n    # --- Validation ---------------------------------------------------------\n    if p is None or p <= 0:\n        return -1\n\n    try:\n        x_arr = np.asarray(x, dtype=float)\n        y_arr = np.asarray(y, dtype=float)\n    except Exception:\n        # Conversion to ndarray failed\n        return -1\n\n    # Must be 1-D and have identical shapes\n    if x_arr.ndim != 1 or y_arr.ndim != 1 or x_arr.shape != y_arr.shape:\n        return -1\n\n    # --- Minkowski distance --------------------------------------------------\n    diff_p = np.abs(x_arr - y_arr) ** p\n    distance = np.sum(diff_p) ** (1.0 / p)\n\n    return round(float(distance), 4)\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError(\"`updates` must contain at least one tree output.\")\n\n    # Stack all tree outputs: shape \u2192 (n_trees, *) where * is (n_samples,) or (n_samples, n_classes)\n    stacked_updates = np.stack(updates, axis=0)\n\n    # Running prediction starts at zero and every tree is *subtracted* after learning-rate scaling\n    running_pred = -learning_rate * stacked_updates.sum(axis=0)\n\n    if regression:\n        # 1-D array, rounded to 4 decimal places\n        return np.round(running_pred.astype(float), 4)\n\n    # ----------  multi-class classification  ----------\n    # Soft-max (numerically stable)\n    logits = running_pred                          # shape (n_samples, n_classes)\n    logits -= logits.max(axis=1, keepdims=True)    # stability trick\n    exp_logits = np.exp(logits)\n    probas = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n\n    # Argmax \u2192 predicted class indices\n    return probas.argmax(axis=1)\n```"}
{"task_id": 370, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multivariate_gaussian(X: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> list[float]:\n    \"\"\"\n    Compute the multivariate normal (Gaussian) density for each sample in `X`.\n\n    Parameters\n    ----------\n    X   : array_like, shape (m, n)\n          Each row is a sample (observation) of dimension n.\n    mu  : array_like, shape (n,)\n          Mean vector of the distribution.\n    cov : array_like, shape (n, n)\n          Positive\u2013definite covariance matrix.\n\n    Returns\n    -------\n    list[float]\n          Densities of each sample rounded to 4 decimal places.\n    \"\"\"\n    # Ensure proper shapes\n    X  = np.atleast_2d(X)\n    mu = np.ravel(mu)\n    n  = mu.size                         # Dimensionality (n)\n\n    # Pre-compute constants\n    inv_cov = np.linalg.inv(cov)         # \u03a3\u207b\u00b9\n    det_cov = np.linalg.det(cov)         # |\u03a3|\n    denom   = np.sqrt((2.0 * np.pi) ** n * det_cov)   # (2\u03c0)^{n/2}|\u03a3|^{1/2}\n\n    # (x \u2212 \u03bc)\n    diff = X - mu                        # shape: (m, n)\n\n    # Mahalanobis distance term: (x\u2212\u03bc)^T \u03a3\u207b\u00b9 (x\u2212\u03bc)\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)  # shape: (m,)\n\n    # Density\n    densities = np.exp(-0.5 * mahal) / denom\n\n    # Round to 4 decimal places and return as list\n    return [round(d, 4) for d in densities]\n```"}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef conv1D(X: np.ndarray,\n           W: np.ndarray,\n           stride: int,\n           pad,\n           dilation: int = 0) -> list:\n    \"\"\"1-D cross-correlation (a.k.a. \u201cconvolution\u201d in DL frameworks).\n\n    Parameters\n    ----------\n    X : ndarray, shape (batch, signal_length, in_channels)\n        Input signals.\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n        Convolution kernels (not flipped \u2013 cross-correlation).\n    stride : int\n        Step size of the sliding window.\n    pad : int | tuple(int, int) | \"same\"\n        Zero padding to the left / right of the signal.\n    dilation : int, default 0\n        Number of zeros inserted *between* kernel elements\n        (0 \u21d2 normal, contiguous kernel).\n\n    Returns\n    -------\n    list\n        Convolved output with shape\n        (batch, output_length, out_channels), converted via ``tolist()``.\n    \"\"\"\n    # ------------- basic sanity checks -------------------------------------\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_ch).')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (k_width, in_ch, out_ch).')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Input-channel dimensions of X and W differ.')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer.')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative.')\n\n    batch, L_in, C_in = X.shape\n    K, _, C_out = W.shape\n    eff_K = (K - 1) * (dilation + 1) + 1      # effective receptive field\n\n    # ------------- determine padding amounts --------------------------------\n    if isinstance(pad, str):\n        pad = pad.lower()\n    if pad == \"same\":\n        # TensorFlow\u2019s SAME rule\n        out_len = int(np.ceil(L_in / stride))\n        total_pad = max(0, (out_len - 1) * stride + eff_K - L_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = p_right = int(pad)\n    elif (isinstance(pad, (tuple, list)) and len(pad) == 2\n          and all(isinstance(x, int) for x in pad)):\n        p_left, p_right = pad\n    else:\n        raise ValueError('pad must be int, 2-tuple or \"same\".')\n\n    if p_left < 0 or p_right < 0:\n        raise ValueError('Padding values must be non-negative.')\n\n    # ------------- pad the input signal -------------------------------------\n    X_pad = np.pad(X,\n                   ((0, 0), (p_left, p_right), (0, 0)),\n                   mode='constant')\n\n    L_pad = L_in + p_left + p_right\n\n    # ------------- output length --------------------------------------------\n    out_len = (L_pad - eff_K) // stride + 1\n    if out_len <= 0:\n        # Nothing fits \u2013 return an empty tensor with proper shape.\n        return np.empty((batch, 0, C_out)).tolist()\n\n    # ------------- build the index matrix for im2col ------------------------\n    start_idx = np.arange(out_len) * stride                     # (out_len,)\n    kernel_offset = np.arange(K) * (dilation + 1)               # (K,)\n    idx = start_idx[:, None] + kernel_offset[None, :]           # (out_len, K)\n\n    # ------------- gather sliding windows -----------------------------------\n    # Resulting shape: (batch, out_len, K, C_in)\n    windows = X_pad[:, idx, :]\n\n    # ------------- correlation via tensordot -------------------------------\n    # Match axes: windows(..., K, C_in)  with  W(K, C_in, C_out)\n    out = np.tensordot(windows, W, axes=([2, 3], [0, 1]))       # (batch, out_len, C_out)\n\n    return out.tolist()\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert to a 1-D NumPy array\n    y = np.asarray(y).ravel()\n\n    # Empty input \u2192 impurity 0.0\n    if y.size == 0:\n        return 0.0\n\n    # Count occurrences of each class\n    _, counts = np.unique(y, return_counts=True)\n\n    # Relative frequencies p_c\n    probs = counts / counts.sum()\n\n    # Gini impurity\n    impurity = 1.0 - np.sum(probs ** 2)\n\n    # Round to 4 decimal places and return as Python float\n    return float(np.round(impurity, 4))\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray,\n                         y: np.ndarray,\n                         learning_rate: float = 0.01,\n                         n_iterations: int = 10_000\n                         ) -> tuple[list[float], float]:\n    \"\"\"Train a multiple-linear\u2010regression model by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray (m \u00d7 n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Target vector.\n    learning_rate : float, default 0.01\n        Step size for gradient descent.\n    n_iterations : int, default 10_000\n        Number of gradient\u2010descent steps.\n\n    Returns\n    -------\n    (weights_list, bias) : tuple[list[float], float]\n        Learned parameters rounded to 4 decimal places.\n        If X and y have inconsistent sample sizes, returns -1.\n    \"\"\"\n    # --- Basic input checking -------------------------------------------------\n    if X.ndim != 2:\n        X = np.atleast_2d(X)\n    y = np.ravel(y)                        # ensure 1-D\n\n    m, n = X.shape\n    if m != y.shape[0] or m == 0:          # inconsistent or empty data\n        return -1\n\n    # --- Initialise parameters -----------------------------------------------\n    theta = np.zeros(n, dtype=float)       # weights\n    b = 0.0                                # bias\n\n    # --- Gradient Descent -----------------------------------------------------\n    for _ in range(n_iterations):\n        y_hat = X @ theta + b              # predictions, shape (m,)\n\n        error = y_hat - y                  # residuals\n        dw = (2 / m) * (X.T @ error)       # gradient w.r.t. weights\n        db = (2 / m) * np.sum(error)       # gradient w.r.t. bias\n\n        theta -= learning_rate * dw\n        b     -= learning_rate * db\n\n    # --- Rounding and output --------------------------------------------------\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return theta_rounded, b_rounded\n```"}
{"task_id": 375, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calc_pad_dims_2D(X_shape: tuple,\n                     out_dim: tuple,\n                     kernel_shape: tuple,\n                     stride: int,\n                     dilation: int = 0) -> tuple:\n    \"\"\"Return the amount of zero-padding needed on each side of a 4-D tensor.\n\n    Parameters\n    ----------\n    X_shape : tuple\n        Shape of the input tensor ``(n_ex, in_rows, in_cols, in_ch)``.\n    out_dim : tuple\n        Desired output height and width ``(out_rows, out_cols)``.\n    kernel_shape : tuple\n        Height and width of the convolution kernel ``(fr, fc)``.\n    stride : int\n        Convolution stride (the same value is used in both directions).\n    dilation : int, optional (default = 0)\n        Dilation factor (number of zeros inserted between kernel elements).\n\n    Returns\n    -------\n    tuple\n        A 4-tuple ``(top, bottom, left, right)``.\n\n    Raises\n    ------\n    ValueError\n        If an argument has a wrong type/size or the requested output\n        dimensions cannot be achieved with non-negative padding.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # -------------------------  argument checks  ---------------------- #\n    # ------------------------------------------------------------------ #\n    def _check_positive_int(name, value, minimum=1):\n        if not isinstance(value, int):\n            raise ValueError(f\"{name} must be an integer.\")\n        if value < minimum:\n            raise ValueError(f\"{name} must be >= {minimum} (got {value}).\")\n\n    # shapes ------------------------------------------------------------ #\n    if (not isinstance(X_shape, (tuple, list)) or\n            len(X_shape) != 4 or\n            not all(isinstance(x, int) for x in X_shape)):\n        raise ValueError(\"X_shape has to be a tuple of 4 integers \"\n                         \"(n_ex, in_rows, in_cols, in_ch).\")\n\n    if (not isinstance(out_dim, (tuple, list)) or\n            len(out_dim) != 2 or\n            not all(isinstance(x, int) for x in out_dim)):\n        raise ValueError(\"out_dim has to be a tuple (out_rows, out_cols).\")\n\n    if (not isinstance(kernel_shape, (tuple, list)) or\n            len(kernel_shape) != 2 or\n            not all(isinstance(x, int) for x in kernel_shape)):\n        raise ValueError(\"kernel_shape has to be a tuple (fr, fc).\")\n\n    # scalar parameters ------------------------------------------------- #\n    _check_positive_int(\"stride\", stride, 1)\n    _check_positive_int(\"dilation\", dilation, 0)\n\n    # unpack sizes ------------------------------------------------------ #\n    _, in_rows, in_cols, _ = X_shape\n    out_rows, out_cols = out_dim\n    fr, fc = kernel_shape\n\n    _check_positive_int(\"in_rows\", in_rows, 1)\n    _check_positive_int(\"in_cols\", in_cols, 1)\n    _check_positive_int(\"out_rows\", out_rows, 1)\n    _check_positive_int(\"out_cols\", out_cols, 1)\n    _check_positive_int(\"fr\", fr, 1)\n    _check_positive_int(\"fc\", fc, 1)\n\n    # ------------------------------------------------------------------ #\n    # -------------  helper: padding for one dimension  ---------------- #\n    # ------------------------------------------------------------------ #\n    def _pad_1d(in_size: int, out_size: int, k_size: int) -> tuple:\n        \"\"\"\n        Compute (pad_before, pad_after) for a single spatial dimension.\n        \"\"\"\n        # Effective filter size with dilation:\n        eff_k = k_size + (k_size - 1) * dilation   # same as k_size*(d+1) - d\n\n        # Total amount of padding needed to obtain the desired output:\n        # out_size = 1 + (in_size + total_pad - eff_k) // stride\n        total_pad = (out_size - 1) * stride - in_size + eff_k\n\n        if total_pad < 0:\n            raise ValueError(\"Requested output size is too small for the given \"\n                             \"input size, kernel, stride and dilation.\")\n\n        # Split the total padding \u2013 put the extra (odd) pixel on 'after'\n        pad_before = total_pad // 2\n        pad_after = total_pad - pad_before\n        return pad_before, pad_after\n\n    # compute per-dimension padding ------------------------------------- #\n    up_pad, down_pad = _pad_1d(in_rows, out_rows, fr)\n    left_pad, right_pad = _pad_1d(in_cols, out_cols, fc)\n\n    return up_pad, down_pad, left_pad, right_pad\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if and only if it is square and equal (within\n    numerical tolerance) to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists of numbers.\n\n    Returns:\n        True if *X* is symmetric, otherwise False.\n    \"\"\"\n    # Empty input \u2192 not a valid square matrix\n    if not X:\n        return False\n\n    n_rows = len(X)\n\n    # Verify each row has the same length as the number of rows\n    for row in X:\n        if len(row) != n_rows:\n            return False            # Not square (or ragged)\n\n    # Convert to NumPy array (now guaranteed to be square)\n    A = np.asarray(X, dtype=float)\n\n    # Compare with its transpose using a tight tolerance\n    return np.allclose(A, A.T, atol=1e-8, rtol=0.0)\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient that Gradient Boosting uses when minimising the\n    logistic (binomial-deviance) loss for binary classification.\n    \n    Parameters\n    ----------\n    y : list or 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    f : list or 1-D array-like of float\n        Current prediction scores.\n    \n    Returns\n    -------\n    list\n        Negative gradients for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n    \"\"\"\n    # Convert to numpy arrays\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n\n    # Validate labels\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n\n    # Convert labels from {0,1} to {-1,1}\n    y_prime = 2 * y_arr - 1        # shape (n_samples,)\n\n    # Compute negative gradient: g_i = y'_i / (1 + exp(y'_i * f_i))\n    g = y_prime / (1.0 + np.exp(y_prime * f_arr))\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(g, 4).tolist()\n```"}
{"task_id": 378, "completion_id": 0, "completion": "```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```"}
{"task_id": 379, "completion_id": 0, "completion": "```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Return the `key` values of the given priority-queue nodes in order.\n\n    Order definition:\n        1. Smaller ``priority`` ranks before larger ``priority``.\n        2. If ``priority`` ties, smaller ``entry_id`` ranks before larger\n           ``entry_id``.\n\n    The original input list is left untouched.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n               keys ``'key'``, ``'val'``, ``'priority'`` and ``'entry_id'``.\n\n    Returns:\n        A list of keys in their correct priority order.\n    \"\"\"\n    # Sort by priority first, then by entry_id for ties, and extract the keys.\n    return [n['key'] for n in sorted(nodes, key=lambda n: (n['priority'], n['entry_id']))]\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# SELU constants from the original paper\n_ALPHA = 1.6732632423543772848170429916717\n_SCALE = 1.0507009873554804934193349852946\n\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Works for scalars and any-shaped arrays/lists.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimals.  For array-like inputs the shape is\n        preserved and a Python list is returned; for scalar input a float\n        is returned.\n    \"\"\"\n    # convert input to ndarray\n    x_arr = np.asarray(x, dtype=np.float64)\n\n    # masks for the two SELU regions\n    pos_mask = x_arr > 0\n    neg_mask = ~pos_mask  # includes x <= 0\n\n    if order == 0:  # SELU itself\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE * x_arr[pos_mask]\n        out[neg_mask] = _SCALE * _ALPHA * (np.exp(x_arr[neg_mask]) - 1)\n\n    elif order == 1:  # first derivative\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n\n    elif order == 2:  # second derivative\n        out = np.empty_like(x_arr)\n        out[pos_mask] = 0.0\n        # for x <= 0 the 2nd derivative equals the first derivative there\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n\n    else:\n        raise ValueError(\"order must be 0, 1 or 2\")\n\n    # round to 4 decimals and convert back\n    rounded = np.round(out, 4)\n\n    # for scalar inputs return a scalar, else a (nested) Python list\n    return rounded.tolist()\n```"}
{"task_id": 382, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nimport numpy as np\nimport math\n\ndef gaussian_naive_bayes_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray) -> list:\n    \"\"\"Train a Gaussian Naive Bayes model on *X_train*, *y_train* and predict\n    the labels of *X_test*.\n\n    Every feature is assumed to follow an independent Gaussian distribution\n    inside each class.  The implementation follows the exact formulae given\n    in the task statement.  The returned value is a *Python list* whose i-th\n    element is the predicted label for the i-th row of *X_test*.\n    \"\"\"\n    # Safety cast to ndarray (in case lists are supplied)\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train)\n    X_test  = np.asarray(X_test,  dtype=float)\n\n    eps = 1e-4                        # small constant to avoid division by 0\n    classes = np.unique(y_train)      # sorted, guarantees tie-breaking rule\n    n_classes = classes.shape[0]\n    n_features = X_train.shape[1]\n\n    # ------------------------------------------------------------------\n    # 1) Estimate class-conditional parameters: prior, mean, variance\n    # ------------------------------------------------------------------\n    priors      = np.zeros(n_classes, dtype=float)        # P(Y = c)\n    means       = np.zeros((n_classes, n_features))       # \u03bc_{c, j}\n    variances   = np.zeros((n_classes, n_features))       # \u03c3\u00b2_{c, j}\n\n    for idx, c in enumerate(classes):\n        X_c              = X_train[y_train == c]\n        priors[idx]      = X_c.shape[0] / X_train.shape[0]\n        means[idx, :]    = X_c.mean(axis=0)\n        variances[idx, :] = X_c.var(axis=0)               # population variance (ddof=0)\n\n    log_priors = np.log(priors)                           # work in log-space\n    # ------------------------------------------------------------------\n    # 2) Compute log-posterior for every test sample and every class\n    # ------------------------------------------------------------------\n    n_test = X_test.shape[0]\n    log_posteriors = np.zeros((n_test, n_classes), dtype=float)\n\n    for idx in range(n_classes):\n        mu  = means[idx]          # shape (n_features,)\n        var = variances[idx]      # shape (n_features,)\n\n        # Terms of the Gaussian likelihood in log-space\n        log_coef   = -0.5 * np.log(2 * np.pi * var + eps)     # shape (n_features,)\n        inv_denom  = 1.0 / (2 * var + eps)                    # shape (n_features,)\n\n        # Broadcast over the test matrix: (n_test, n_features)\n        diff2      = (X_test - mu) ** 2\n\n        # log P(X | Y=c) per sample  -> sum over features\n        log_likelihood = (log_coef - diff2 * inv_denom).sum(axis=1)\n\n        # Add log-prior to obtain the (unnormalised) log-posterior\n        log_posteriors[:, idx] = log_priors[idx] + log_likelihood\n\n    # ------------------------------------------------------------------\n    # 3) Assign the class with the highest posterior probability\n    # ------------------------------------------------------------------\n    best_class_indices = np.argmax(log_posteriors, axis=1)    # np.argmax keeps first on ties\n    predicted_labels   = classes[best_class_indices]\n\n    # Return as a regular Python list\n    return predicted_labels.tolist()\n```"}
{"task_id": 383, "completion_id": 0, "completion": "```python\ndef find_bounds(y_i: int, y_j: int, alpha_i: float, alpha_j: float, C: float) -> list[float]:\n    \"\"\"Return the feasible interval [L, H] for alpha_j in SMO.\n\n    Parameters\n    ----------\n    y_i : int\n        Label of the i-th example (\u22121 or 1).\n    y_j : int\n        Label of the j-th example (\u22121 or 1).\n    alpha_i : float\n        Current Lagrange multiplier for the i-th example.\n    alpha_j : float\n        Current Lagrange multiplier for the j-th example.\n    C : float\n        Regularisation constant of the SVM.\n\n    Returns\n    -------\n    list[float]\n        A two-element list [L, H] with both values rounded to 4 decimals.\n    \"\"\"\n    if y_i != y_j:\n        L = max(0.0, alpha_j - alpha_i)\n        H = min(C, C - alpha_i + alpha_j)\n    else:  # y_i == y_j\n        L = max(0.0, alpha_i + alpha_j - C)\n        H = min(C, alpha_i + alpha_j)\n\n    return [round(L, 4), round(H, 4)]\n```"}
{"task_id": 384, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_eigvecs(L: list[list[int | float]] | np.ndarray, k: int):\n    \"\"\"\n    Calculate the eigenvectors that correspond to the k smallest eigenvalues\n    of a Laplacian matrix L.\n\n    Parameters\n    ----------\n    L : list[list[int | float]] | np.ndarray\n        Square Laplacian matrix.\n    k : int\n        Number of eigenvectors wanted.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Matrix (as a list of lists) whose columns are the eigenvectors\n        associated with the k smallest eigenvalues, rounded to 4 decimals.\n        Returns -1 if k is invalid (k > n or k <= 0) or L is not square.\n    \"\"\"\n    # Convert input to NumPy array of floats\n    L = np.asarray(L, dtype=float)\n\n    # Basic validations\n    if L.ndim != 2 or L.shape[0] != L.shape[1]:\n        return -1  # Not a square matrix\n    n = L.shape[0]\n    if k <= 0 or k > n:\n        return -1\n\n    # Compute eigenpairs (eigh exploits symmetry of Laplacian)\n    eigvals, eigvecs = np.linalg.eigh(L)\n\n    # Take the k eigenvectors for the k smallest eigenvalues\n    eigvecs = np.real(eigvecs[:, :k])  # discard (tiny) imaginary parts\n\n    # Post-processing: normalise, fix sign deterministically\n    for j in range(k):\n        v = eigvecs[:, j]\n        # Ensure unit L2 norm\n        norm = np.linalg.norm(v)\n        if norm != 0:\n            v /= norm\n        # Flip sign so first non-zero entry is positive\n        nz = np.flatnonzero(v)\n        if nz.size > 0 and v[nz[0]] < 0:\n            v *= -1\n        eigvecs[:, j] = v\n\n    # Round to 4 decimal places and return as regular list\n    eigvecs = np.round(eigvecs, 4)\n    return eigvecs.tolist()\n```"}
{"task_id": 385, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef lda_transform(X: list[list[int | float]], y: list[int], k: int) -> list[list[float]]:\n    \"\"\"\n    Project a labelled data set onto its first k Linear Discriminant\n    components and return the projected data rounded to 4 decimals.\n\n    Parameters\n    ----------\n    X : list of list (n_samples, n_features)\n        The input data matrix.\n    y : list (n_samples,)\n        Class labels for every sample.\n    k : int\n        Number of LDA directions that should be retained.\n\n    Returns\n    -------\n    list of list\n        The data projected onto the top-k LDA directions, rounded\n        to 4 decimal places.\n    \"\"\"\n\n    # ---- 1. Preparation ----------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n    classes, class_indices = np.unique(y, return_inverse=True)\n    c = len(classes)\n\n    # if fewer than two classes there are no discriminant directions\n    max_k = max(c - 1, 0)\n    k = min(k, max_k, n_features)\n    if k == 0:\n        # Return an array with 0 columns, converted to list-of-lists\n        return [[] for _ in range(n_samples)]\n\n    overall_mean = np.mean(X, axis=0)\n\n    # ---- 2. Scatter matrices ----------------------------------------------\n    Sw = np.zeros((n_features, n_features), dtype=float)  # within-class scatter\n    Sb = np.zeros((n_features, n_features), dtype=float)  # between-class scatter\n\n    for cls in classes:\n        cls_mask = y == cls\n        X_c = X[cls_mask]                      # samples in current class\n        mean_c = np.mean(X_c, axis=0)          # class mean\n\n        # within-class scatter\n        centered = X_c - mean_c\n        Sw += centered.T @ centered\n\n        # between-class scatter\n        mean_diff = (mean_c - overall_mean).reshape(-1, 1)\n        Sb += X_c.shape[0] * (mean_diff @ mean_diff.T)\n\n    # ---- 3. Solve the generalised eigenproblem -----------------------------\n    # Use Moore\u2013Penrose pseudo-inverse in case Sw is singular\n    Sw_inv = np.linalg.pinv(Sw)\n    eig_mat = Sw_inv @ Sb\n\n    eigvals, eigvecs = np.linalg.eig(eig_mat)\n    eigvals = eigvals.real\n    eigvecs = eigvecs.real\n\n    # ---- 4. Sort, pick k best, normalise, fix orientation ------------------\n    sorted_idx = np.argsort(eigvals)[::-1]          # descending order\n    W = eigvecs[:, sorted_idx[:k]]                  # (n_features, k)\n\n    for i in range(W.shape[1]):\n        vec = W[:, i]\n        vec /= np.linalg.norm(vec) + 1e-15          # normalise\n        if vec[0] < 0:                              # deterministic orientation\n            vec = -vec\n        W[:, i] = vec\n\n    # ---- 5. Project data and format output ---------------------------------\n    projected = X @ W                               # shape (n_samples, k)\n    projected = np.round(projected, 4)              # round to 4 decimals\n    return projected.tolist()\n```"}
{"task_id": 386, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        1-D array of true labels (0 or 1).\n    y_pred : np.ndarray\n        1-D array of predicted probabilities (between 0 and 1).\n\n    Returns\n    -------\n    float\n        The average BCE loss rounded to 4 decimal places.\n    \"\"\"\n    # Small constant to avoid log(0)\n    eps = 1e-15\n    \n    # 1) Clip predictions to [eps, 1 - eps]\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n    \n    # 2) Compute Binary Cross-Entropy\n    loss = -np.mean(\n        y_true * np.log(y_pred_clipped) +\n        (1 - y_true) * np.log(1 - y_pred_clipped)\n    )\n    \n    # 3) Round to 4 decimal places and return as Python float\n    return float(np.round(loss, 4))\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_boosting_1d(\n    x: list[float],\n    y: list[float],\n    n_estimators: int,\n    learning_rate: float = 0.1,\n) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    # --- data to numpy -------------------------------------------------------\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n    if n == 0:\n        return []\n\n    # --- initial (round-0) prediction ---------------------------------------\n    pred = np.full(n, y.mean())\n\n    # ------------------------------------------------------------------------\n    for _ in range(n_estimators):\n        residuals = y - pred  # negative gradients\n\n        # --------------------------------------------------------------------\n        # CASE 1 \u2013 all x are identical  \u2192  one-side stump with global mean\n        # --------------------------------------------------------------------\n        if np.all(x == x[0]):\n            stump_out = np.full(n, residuals.mean())\n\n        # --------------------------------------------------------------------\n        # CASE 2 \u2013 we can really split the data\n        # --------------------------------------------------------------------\n        else:\n            order = np.argsort(x)\n            xs = x[order]\n            rs = residuals[order]\n\n            # prefix sums for fast SSE computation\n            csum = np.cumsum(rs)\n            csum_sq = np.cumsum(rs ** 2)\n\n            total_sum = csum[-1]\n            total_sq = csum_sq[-1]\n\n            # left block sizes 1 \u2026 n-1\n            k_left = np.arange(1, n)\n            k_right = n - k_left\n\n            sum_left = csum[:-1]\n            sum_sq_left = csum_sq[:-1]\n\n            sum_right = total_sum - sum_left\n            sum_sq_right = total_sq - sum_sq_left\n\n            mean_left = sum_left / k_left\n            mean_right = sum_right / k_right\n\n            sse_left = sum_sq_left - k_left * mean_left**2\n            sse_right = sum_sq_right - k_right * mean_right**2\n            total_sse = sse_left + sse_right\n\n            # we may only split between *different* consecutive x values\n            valid = xs[:-1] != xs[1:]\n            if not np.any(valid):  # this should not happen (all-x-identical branch)\n                stump_out = np.full(n, residuals.mean())\n            else:\n                best_pos = np.argmin(np.where(valid, total_sse, np.inf))\n                thr = (xs[best_pos] + xs[best_pos + 1]) / 2.0\n\n                mu_left = mean_left[best_pos]\n                mu_right = mean_right[best_pos]\n\n                stump_out = np.where(x <= thr, mu_left, mu_right)\n\n        # --- update running prediction --------------------------------------\n        pred += learning_rate * stump_out\n\n    # round & return python list\n    return np.round(pred, 4).tolist()\n```"}
{"task_id": 389, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_laplacian(W):\n    \"\"\"Compute the (unnormalised) Laplacian matrix of an undirected graph.\n\n    Parameters\n    ----------\n    W : list[list[int | float]] | np.ndarray\n        Square weighted adjacency matrix of the graph.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The Laplacian matrix rounded to 4 decimal places and converted to a\n        Python list of lists. Returns -1 if the input is not a square matrix.\n    \"\"\"\n    # ---------- 1. Convert to numpy array & validate shape ----------\n    try:\n        A = np.asarray(W, dtype=float)      # force numeric type\n    except (ValueError, TypeError):\n        return -1                           # ragged / non-numeric input\n    \n    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n        return -1                           # not square\n    \n    # ---------- 2. Degree matrix ----------\n    degrees = A.sum(axis=1)                 # row sums\n    D = np.diag(degrees)                    # diagonal degree matrix\n    \n    # ---------- 3. Laplacian ----------\n    L = D - A\n    \n    # ---------- 4. Round & return ----------\n    L = np.round(L, 4)\n    return L.tolist()\n```"}
{"task_id": 391, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nn_interpolate_1D(X: np.ndarray, t: list[float]) -> list[list[float]]:\n    \"\"\"Nearest-neighbour interpolation for a 1-D (multi-channel) signal.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array of shape (in_length, in_channels) containing the sampled signal.\n        A 1-D array of shape (in_length,) is also accepted (treated as a single\n        channel).\n    t : list[float]\n        Coordinates at which to interpolate.\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated samples as a plain Python list with shape\n        (len(t), in_channels).\n    \"\"\"\n    # Ensure X has shape (in_length, in_channels)\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X[:, None]                       # promote to 2-D, single channel\n\n    in_len = X.shape[0]\n\n    # 1) Round coordinates using NumPy\u2019s `around` (ties \u2192 even)\n    idx = np.around(np.asarray(t, dtype=float)).astype(int)\n\n    # 2) Clip indices to the valid range\n    idx = np.clip(idx, 0, in_len - 1)\n\n    # 3) Gather and return as Python list\n    return X[idx].tolist()\n```"}
{"task_id": 392, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef preemphasis(x: \"list[float] | np.ndarray\", alpha: float) -> list[float]:\n    \"\"\"Apply a first-order pre-emphasis filter to a 1-D signal.\n\n    Args:\n        x: 1-D (or n-D) array-like sequence of numeric samples.\n        alpha: Pre-emphasis coefficient, 0 \u2264 alpha < 1.\n\n    Returns:\n        Filtered signal as a Python list, with every element rounded\n        to four decimal places.\n    \"\"\"\n    # -------- argument checks ------------------------------------------------\n    if not 0 <= alpha < 1:\n        raise ValueError(\"`alpha` must satisfy 0 \u2264 alpha < 1.\")\n    \n    # Convert to ndarray and flatten to 1-D\n    arr = np.asarray(x, dtype=float).ravel()\n\n    # Empty input \u2192 empty output\n    if arr.size == 0:\n        return []\n\n    # -------- core filter ----------------------------------------------------\n    y = np.empty_like(arr)\n    y[0] = arr[0]                       # first sample unchanged\n    if arr.size > 1:\n        y[1:] = arr[1:] - alpha * arr[:-1]\n\n    # -------- rounding & return ---------------------------------------------\n    return np.round(y, 4).tolist()\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np     # (kept twice as required by the template)\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 0.01.\n    rho : float, optional\n        Decay rate (\u03c1). Default is 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        Tuple ``(w_next, Eg_next)`` \u2013 both rounded to four decimal places and\n        converted to regular Python lists.\n    \"\"\"\n    \n    # 1. Initialise Eg if necessary\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    \n    epsilon = 1e-8\n    \n    # 2. Update the running average of squared gradients\n    Eg_next = rho * Eg + (1.0 - rho) * (grad ** 2)\n    \n    # 3. Parameter update\n    w_next = w - (learning_rate * grad) / np.sqrt(Eg_next + epsilon)\n    \n    # 4. Round to 4 decimal places and convert to Python lists\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    \n    return w_next_rounded, Eg_next_rounded\n```"}
{"task_id": 395, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef print_backward_result(beta: np.ndarray, b_prob: float) -> str:\n    \"\"\"Return a formatted string with a \u03b2-matrix and its probability.\n\n    The string has to look like:\n        **************************************************\n        Beta:\n        <beta as str(beta)>\n        Probability of sequence: <b_prob>\n    \"\"\"\n    header = '*' * 50                  # exactly 50 asterisks\n    beta_str = str(beta)               # default NumPy string representation\n    prob_str = f'Probability of sequence: {b_prob}'\n\n    return '\\n'.join([header, 'Beta:', beta_str, prob_str])\n```"}
{"task_id": 396, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_with_derivatives(x):\n    \"\"\"Compute tanh and its first two derivatives element-wise.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        1-D collection of real numbers.\n\n    Returns\n    -------\n    list[list[float]]\n        [\n            tanh(x),                     # element-wise tanh\n            1 - tanh(x)**2,              # 1st derivative\n            -2 * tanh(x) * (1 - tanh(x)**2)  # 2nd derivative\n        ]\n    All inner lists are rounded to 6 decimal places.\n    \"\"\"\n    # Work on a fresh NumPy copy so the original input is untouched\n    z = np.asarray(x, dtype=float).copy()\n\n    # tanh and its derivatives\n    t = np.tanh(z)\n    dt = 1.0 - t**2\n    d2t = -2.0 * t * dt\n\n    # Round to 6 decimals and convert to plain Python lists\n    out = [arr.round(6).tolist() for arr in (t, dt, d2t)]\n    return out\n```"}
{"task_id": 397, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_one_hot_matrix(n_examples: int, n_classes: int):\n    \"\"\"Create a random one-hot matrix.\n\n    Parameters\n    ----------\n    n_examples : int\n        Number of rows (examples) to generate.  May be 0.\n    n_classes : int\n        Size of the vocabulary / number of distinct classes (>0).\n\n    Returns\n    -------\n    numpy.ndarray\n        A matrix of shape (n_examples, n_classes) where every row is a one-hot\n        encoded vector chosen uniformly at random from all `n_classes` possible\n        class indices.\n    \"\"\"\n    # Empty matrix case --------------------------------------------------------\n    if n_examples == 0:\n        # Explicit dtype=float for consistency with the typical one-hot format\n        return np.zeros((0, n_classes), dtype=float)\n\n    # -------------------------------------------------------------------------\n    # Step 1: Pick `n_examples` class indices uniformly at random in\n    #         [0, n_classes-1].  Each index tells us where the 1 goes in that\n    #         row.  `np.random.randint` samples *with* replacement, as required.\n    # -------------------------------------------------------------------------\n    indices = np.random.randint(0, n_classes, size=n_examples)\n\n    # -------------------------------------------------------------------------\n    # Step 2: Convert the indices into a one-hot matrix.\n    #         The trick `np.eye(n_classes)[indices]` builds the identity matrix\n    #         (shape: (n_classes, n_classes)) and selects the desired rows.\n    #         The result therefore has shape (n_examples, n_classes).\n    # -------------------------------------------------------------------------\n    one_hot = np.eye(n_classes, dtype=float)[indices]\n\n    return one_hot\n```\n\nHow it works\n1. Draw `n_examples` random integers in the range `[0, n_classes \u2013 1]`.\n2. Use those integers to index the rows of the `n_classes \u00d7 n_classes` identity matrix, yielding a one-hot row for each integer.\n\nAll requirements hold:\n\u2022 Output type: `numpy.ndarray`.\n\u2022 Shape: `(n_examples, n_classes)`.\n\u2022 Elements are 0 or 1 (`dtype=float`).\n\u2022 Each row contains exactly one 1 (sum equals 1)."}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient element-wise.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive ELU hyper-parameter (default 0.1).\n        derivative: If True, return the gradient; otherwise return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for vectorised computation\n    x_arr = np.asarray(x, dtype=float)\n\n    # Masks for positive (x >= 0) and negative (x < 0) elements\n    pos_mask = x_arr >= 0\n    neg_mask = ~pos_mask\n\n    if not derivative:                                  # ----- ELU -----\n        out = x_arr.copy()\n        out[neg_mask] = alpha * (np.exp(x_arr[neg_mask]) - 1)\n    else:                                               # ---- d(ELU)/dx ----\n        out = np.ones_like(x_arr)\n        # ELU'(x) for x < 0  -->  ELU(x) + alpha = alpha * e^x\n        out[neg_mask] = alpha * np.exp(x_arr[neg_mask])\n\n    # Round to 4 decimal places and return as a Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 399, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # ----- validation --------------------------------------------------------\n    if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n        return -1\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n\n    # ----- pair-wise Euclidean distances -------------------------------------\n    # \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y  (broadcasted)\n    XX = np.sum(X * X, axis=1)[:, None]          # shape (m, 1)\n    YY = np.sum(Y * Y, axis=1)[None, :]          # shape (1, n)\n    d2 = XX + YY - 2 * X @ Y.T                   # squared distances\n    d2 = np.clip(d2, 0.0, None)                  # numerical safety\n    D = np.sqrt(d2)\n\n    # ----- rounding & conversion ---------------------------------------------\n    return np.round(D, 4).tolist()\n```"}
{"task_id": 400, "completion_id": 0, "completion": "```python\nimport math\n\ndef gaussian_cdf(x: float, mean: float, variance: float) -> float:\n    \"\"\"Compute \u03a6(x; \u03bc, \u03c3\u00b2) \u2013 the Gaussian CDF evaluated at *x*.\n\n    Parameters\n    ----------\n    x : float\n        The point at which to evaluate the CDF.\n    mean : float\n        The mean (\u03bc) of the Gaussian distribution.\n    variance : float\n        The variance (\u03c3\u00b2) of the Gaussian distribution. Must be non-negative.\n\n    Returns\n    -------\n    float\n        The probability that a Gaussian random variable with the given\n        parameters does not exceed *x*, rounded to six decimal places.\n    \"\"\"\n    # Validate input\n    if variance < 0:\n        raise ValueError(\"Variance must be non-negative.\")\n\n    # Degenerate (point-mass) case\n    if variance == 0:\n        if x < mean:\n            prob = 0.0\n        elif x > mean:\n            prob = 1.0\n        else:  # x == mean\n            prob = 0.5\n        return round(prob, 6)\n\n    # Proper Gaussian\n    sigma = math.sqrt(variance)\n    z = (x - mean) / (sigma * math.sqrt(2.0))\n    prob = 0.5 * (1.0 + math.erf(z))\n    return round(prob, 6)\n```"}
{"task_id": 401, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_one_hot(y: np.ndarray, num_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert a vector of integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        \u2022 Shape (N,)  : flat vector of labels  \n        \u2022 Shape (N,1) : column vector of labels  \n        \u2022 Shape (N,C) : already one-hot (every row sums to 1)\n    num_classes : int | None, optional\n        Desired number of classes (width of the output).  If ``None`` it is\n        inferred as ``max(y) + 1`` for label vectors, or the second dimension\n        of ``y`` when it is already one-hot.\n\n    Returns\n    -------\n    np.ndarray\n        Float array of shape (N, C) containing only 0.0 and 1.0.\n    \"\"\"\n    y = np.asarray(y)\n\n    # ------------------------------------------------------------------\n    # 1. Detect and immediately return already one-hot encoded matrices\n    # ------------------------------------------------------------------\n    if y.ndim == 2 and y.shape[1] > 1:\n        is_binary = np.logical_or(y == 0, y == 1).all()\n        sums_to_one = np.allclose(y.sum(axis=1), 1)\n        if is_binary and sums_to_one:\n            # Ensure float dtype but keep the original data unchanged\n            return y.astype(float, copy=False)\n\n    # ---------------------------------------------------------------\n    # 2. From here on *y* must be a label vector, shape (N,) or (N,1)\n    # ---------------------------------------------------------------\n    if y.ndim == 2 and y.shape[1] == 1:\n        y = y.ravel()                  # (N,1)  -> (N,)\n    elif y.ndim != 1:\n        raise ValueError(\n            \"Input must be a 1-D label vector, a (N,1) label matrix, \"\n            \"or an already one-hot encoded (N,C) matrix.\"\n        )\n\n    if not np.issubdtype(y.dtype, np.integer):\n        raise ValueError(\"Class labels must be integers.\")\n\n    if np.any(y < 0):\n        raise ValueError(\"Negative class labels are not supported.\")\n\n    # ----------------------------------------------------------\n    # 3. Decide how many classes the one-hot output should have\n    # ----------------------------------------------------------\n    inferred_classes = int(y.max()) + 1\n    C = num_classes if num_classes is not None else inferred_classes\n    if C <= y.max():\n        raise ValueError(\n            \"`num_classes` must be greater than the largest label \"\n            f\"({y.max()=}, {C=}).\"\n        )\n\n    # -----------------------------\n    # 4. Build the one-hot matrix\n    # -----------------------------\n    N = y.shape[0]\n    one_hot = np.zeros((N, C), dtype=float)\n    one_hot[np.arange(N), y] = 1.0\n\n    return one_hot\n```\n\nExplanation of the main steps  \n1. Detect an already one-hot encoded matrix (shape (N,C) with only 0/1 and rows summing to one) and return it immediately.  \n2. Otherwise reshape possible `(N,1)` input to `(N,)` and validate it contains non-negative integers.  \n3. Decide the number of classes (`C`): from `num_classes` if supplied, else `max(label)+1`.  \n4. Allocate an `(N,C)` zero matrix and set the appropriate positions to `1.0` using advanced indexing."}
{"task_id": 402, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_nb_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    eps: float = 1e-6\n) -> np.ndarray:\n    \"\"\"Predict labels for X_test using a Gaussian Naive Bayes classifier.\n\n    Parameters\n    ----------\n    X_train : np.ndarray of shape (N_train, M)\n        Training feature matrix.\n    y_train : np.ndarray of shape (N_train,)\n        Training labels.\n    X_test : np.ndarray of shape (N_test, M)\n        Test feature matrix whose labels are to be predicted.\n    eps : float, optional (default=1e-6)\n        Small value added to variances to avoid division by zero.\n\n    Returns\n    -------\n    np.ndarray of shape (N_test,)\n        Predicted labels for each row in X_test.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1.  Gather basic data\n    # ------------------------------------------------------------------\n    classes, y_inv = np.unique(y_train, return_inverse=True)   # preserves order\n    n_classes = classes.size\n    n_features = X_train.shape[1]\n\n    # ------------------------------------------------------------------\n    # 2.  Estimate priors, means and (regularised) variances\n    # ------------------------------------------------------------------\n    # Counts per class\n    class_counts = np.bincount(y_inv).astype(np.float64)   # shape (n_classes,)\n    priors = class_counts / class_counts.sum()             # P(y = c)\n    log_priors = np.log(priors)\n\n    # Allocate containers for means and variances\n    means = np.empty((n_classes, n_features), dtype=np.float64)\n    vars_ = np.empty_like(means)\n\n    for idx, c in enumerate(classes):\n        X_c = X_train[y_train == c]\n        means[idx] = X_c.mean(axis=0)\n        vars_[idx] = X_c.var(axis=0) + eps                # add eps for stability\n\n    # ------------------------------------------------------------------\n    # 3.  Compute log-posterior for every test sample & class\n    # ------------------------------------------------------------------\n    # Pre-compute constants that do not depend on a test sample\n    log_coeff = -0.5 * (np.log(2.0 * np.pi) + np.log(vars_))  # shape (n_classes, n_features)\n    # class-wise constant term (sum over features of log_coeff)\n    class_const = log_coeff.sum(axis=1)                       # shape (n_classes,)\n\n    # Expand dimensions to broadcast correctly:\n    #   X_test -> (N_test, 1, M)\n    #   means  -> (1, n_classes, M)\n    #   vars_  -> (1, n_classes, M)\n    diff = X_test[:, None, :] - means[None, :, :]             # (N_test, n_classes, M)\n    sq_term = -0.5 * (diff ** 2) / vars_[None, :, :]          # (N_test, n_classes, M)\n\n    # Log-likelihood per class: constant + sum over features of sq_term\n    log_lik = class_const[None, :] + sq_term.sum(axis=2)      # (N_test, n_classes)\n\n    # Log-posterior = log_prior + log_likelihood\n    log_post = log_priors[None, :] + log_lik                  # (N_test, n_classes)\n\n    # ------------------------------------------------------------------\n    # 4.  Pick the class with the highest posterior\n    # ------------------------------------------------------------------\n    y_pred = classes[np.argmax(log_post, axis=1)]\n    return y_pred\n```"}
{"task_id": 405, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport re\n\n\n_FLOAT = r\"[+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?\"      # generic float regexp\n\n\ndef _parse_leaky(param: str) -> float:\n    \"\"\"\n    Parse   leaky relu(alpha=<float>)\n    \"\"\"\n    pat = rf\"^leaky\\s*relu(?:\\(\\s*alpha\\s*=\\s*({_FLOAT})\\s*\\))?$\"\n    m = re.fullmatch(pat, param, flags=re.I)\n    if not m:                                         # no match -> really unknown\n        raise ValueError(f\"Unknown activation: {param}\")\n    return float(m.group(1)) if m.group(1) is not None else 0.01\n\n\ndef _parse_affine(param: str) -> tuple[float, float]:\n    \"\"\"\n    Parse   affine(slope=<float>, intercept=<float>)\n    (order inside the parentheses may be swapped, white-spaces ignored)\n    \"\"\"\n    m = re.fullmatch(r\"affine\\((.*)\\)\", param, flags=re.I)\n    if not m:\n        raise ValueError(f\"Unknown activation: {param}\")\n\n    inside = m.group(1)\n    pieces = [p.strip() for p in inside.split(\",\") if p.strip()]\n    kv = {}\n    for p in pieces:\n        if \"=\" not in p:\n            raise ValueError(f\"Unknown activation: {param}\")\n        k, v = [s.strip().lower() for s in p.split(\"=\", 1)]\n        if not re.fullmatch(_FLOAT, v, flags=re.I):\n            raise ValueError(f\"Unknown activation: {param}\")\n        kv[k] = float(v)\n\n    if (\"slope\" not in kv) or (\"intercept\" not in kv):\n        raise ValueError(f\"Unknown activation: {param}\")\n\n    return kv[\"slope\"], kv[\"intercept\"]\n\n\ndef apply_activation(x, param=None):\n    \"\"\"Apply a chosen activation to the input vector.\n\n    Args:\n        x (list[float] | np.ndarray): Input data. Anything that can be\n            converted to a NumPy array of floats is accepted.\n        param (str | None, optional): Description of the activation. See the\n            task specification for the accepted values. If *None* the\n            identity function is applied.\n\n    Returns:\n        list[float]: Result of applying the activation element-wise, rounded\n            to six decimals and converted to a standard Python *list*.\n    \"\"\"\n    # convert to ndarray\n    arr = np.asarray(x, dtype=float)\n\n    # decide which activation\n    if param is None or (isinstance(param, str) and param.strip().lower() == \"identity\"):\n        res = arr\n\n    elif isinstance(param, str) and param.strip().lower() == \"relu\":\n        res = np.maximum(0, arr)\n\n    elif isinstance(param, str) and param.strip().lower() == \"tanh\":\n        res = np.tanh(arr)\n\n    elif isinstance(param, str) and param.strip().lower() == \"sigmoid\":\n        res = 1.0 / (1.0 + np.exp(-arr))\n\n    elif isinstance(param, str) and param.lower().startswith(\"leaky\"):\n        alpha = _parse_leaky(param.strip().lower())\n        res = np.where(arr > 0, arr, alpha * arr)\n\n    elif isinstance(param, str) and param.lower().startswith(\"affine\"):\n        slope, intercept = _parse_affine(param.strip().lower())\n        res = slope * arr + intercept\n\n    else:\n        raise ValueError(f\"Unknown activation: {param}\")\n\n    # round & convert to list\n    res = np.round(res, 6)\n    if res.ndim == 0:\n        return [float(res)]           # keep the output a list in scalar case\n    return res.tolist()\n```"}
{"task_id": 406, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(\n    X_train: \"np.ndarray\",\n    y_train: \"np.ndarray\",\n    X_test:  \"np.ndarray\",\n    k: int = 5\n) -> list[int]:\n    \"\"\"k-Nearest Neighbors classifier.\"\"\"\n    # --- basic checks --------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test,  dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        raise ValueError(\"X_train and X_test must be 2-D arrays\")\n    if X_train.shape[0] != y_train.shape[0]:\n        raise ValueError(\"X_train and y_train have incompatible shapes\")\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer\")\n\n    n_train = X_train.shape[0]\n    k = min(k, n_train)                      # do not ask for more neighbours\n    # --- pairwise (squared) Euclidean distances -----------------------------\n    # shape -> (n_test, n_train)\n    diff   = X_test[:, None, :] - X_train[None, :, :]\n    dists  = np.einsum(\"ijk,ijk->ij\", diff, diff)   # faster than **2 + sum(...)\n    # --- majority vote for every test sample --------------------------------\n    predictions: list[int] = []\n\n    for row in dists:                # row: distances from one test sample\n        nn_idx = np.argpartition(row, k-1)[:k]  # indices of k closest\n        nearest_labels = y_train[nn_idx]\n        vote_count = np.bincount(nearest_labels)  # counts per label\n        predictions.append(int(vote_count.argmax()))\n\n    return predictions\n```"}
{"task_id": 410, "completion_id": 0, "completion": "```python\nimport re\ndef parse_activation(act_str: str):\n    \"\"\"Parse an activation-function description string.\n\n    The string can be one of the following forms:\n        \u2022 \"ReLU\"\n        \u2022 \"Sigmoid\"\n        \u2022 \"Tanh\"\n        \u2022 \"Affine(slope=<number>, intercept=<number>)\"\n        \u2022 \"Leaky ReLU(alpha=<number>)\"\n\n    Args:\n        act_str: Description of the activation function.\n\n    Returns:\n        A dictionary with key \"type\" and, when applicable, its parameters.\n        If the input format is invalid, return -1.\n    \"\"\"\n    # ---------- helpers -----------------------------------------------------\n    # A number: optional sign,  digits with optional decimal part   OR   .digits\n    number_pat = r'[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)'\n    \n    # Pre-compile the patterns that exactly match the canonical forms\n    patterns = [\n        (re.compile(r'^ReLU$'),\n         lambda _: {\"type\": \"ReLU\"}),\n\n        (re.compile(r'^Sigmoid$'),\n         lambda _: {\"type\": \"Sigmoid\"}),\n\n        (re.compile(r'^Tanh$'),\n         lambda _: {\"type\": \"Tanh\"}),\n\n        (re.compile(rf'^Affine\\(slope=({number_pat}), intercept=({number_pat})\\)$'),\n         lambda m: {\"type\": \"Affine\",\n                    \"slope\": float(m.group(1)),\n                    \"intercept\": float(m.group(2))}),\n\n        (re.compile(rf'^Leaky ReLU\\(alpha=({number_pat})\\)$'),\n         lambda m: {\"type\": \"Leaky ReLU\",\n                    \"alpha\": float(m.group(1))}),\n    ]\n\n    # ---------- main matching loop ------------------------------------------\n    for pat, builder in patterns:\n        m = pat.match(act_str)\n        if m:\n            return builder(m)\n\n    # Nothing matched -> invalid\n    return -1\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Integral, Real\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute simple statistics of action / observation trajectories.\"\"\"\n    \n    def is_iterable(x):\n        \"\"\"Return True if *x* is a list/tuple/ndarray with length.\"\"\"\n        return isinstance(x, (list, tuple, np.ndarray)) and not np.isscalar(x)\n    \n    def analyse(collection):\n        \"\"\"\n        Inspect a list of (possibly-vector) values and return\n        tuple_flag  \u2026 at least one element is an iterable\n        multi_dim   \u2026 dimensionality > 1\n        continuous  \u2026 at least one value is a non-integer float\n        n_per_dim   \u2026 list with #unique values per dimension\n        dim         \u2026 dimensionality\n        ids         \u2026 list with sorted unique ids per dimension\n        \"\"\"\n        if len(collection) == 0:\n            raise ValueError(\"Empty trajectory supplied.\")\n        \n        tuple_flag = any(is_iterable(x) for x in collection)\n        \n        # find dimensionality\n        if tuple_flag:\n            # find first iterable element to determine length\n            for sample in collection:\n                if is_iterable(sample):\n                    dim = len(sample)\n                    break\n        else:\n            dim = 1\n        \n        multi_dim = dim > 1\n        \n        # prepare containers\n        uniq_sets = [set() for _ in range(dim)]\n        continuous = False\n        \n        for sample in collection:\n            if tuple_flag:\n                # convert to 1-D sequence\n                values = list(sample)\n                if len(values) != dim:\n                    raise ValueError(\"Inconsistent dimensionality in data.\")\n            else:\n                values = [sample]\n            \n            for d, val in enumerate(values):\n                uniq_sets[d].add(val)\n                \n                # discrete / continuous test\n                if not continuous:          # quit test as soon as True\n                    if isinstance(val, (Real, np.floating, np.integer)):\n                        if isinstance(val, (Integral, np.integer)):\n                            continue        # definitely discrete\n                        # real number \u2013 check if fractional\n                        if not float(val).is_integer():\n                            continuous = True\n                    else:\n                        # anything non-numeric \u2013 treat as continuous\n                        continuous = True\n        \n        ids = [sorted(list(s)) for s in uniq_sets]\n        n_per_dim = [len(s) for s in uniq_sets]\n        \n        return tuple_flag, multi_dim, continuous, n_per_dim, dim, ids\n    \n    # ---------- process both collections ----------\n    (tuple_a, multi_a, cont_a, n_a, dim_a, ids_a) = analyse(actions)\n    (tuple_o, multi_o, cont_o, n_o, dim_o, ids_o) = analyse(observations)\n    \n    return {\n        'tuple_actions':        tuple_a,\n        'tuple_observations':   tuple_o,\n        'multidim_actions':     multi_a,\n        'multidim_observations':multi_o,\n        'continuous_actions':   cont_a,\n        'continuous_observations': cont_o,\n        'n_actions_per_dim':    n_a,\n        'action_dim':           dim_a,\n        'n_obs_per_dim':        n_o,\n        'obs_dim':              dim_o,\n        'action_ids':           ids_a,\n        'obs_ids':              ids_o\n    }\n```\n\nThe helper function `analyse` performs all required computations on one trajectory, and the main `env_stats` function simply calls it for both the actions and the observations and arranges the returned values in the requested dictionary format."}
{"task_id": 412, "completion_id": 0, "completion": "```python\ndef get_metric(name: str):\n    \"\"\"Return an evaluation metric function by *name*.\n\n    Supported names:\n    - 'accuracy'\n    - 'precision'\n    - 'recall'\n    - 'f1'\n\n    If the name is invalid, the function returns -1.\n    The returned metric functions accept two equal-length iterables\n    (y_true, y_pred) of 0s and 1s and return the metric value\n    rounded to 4 decimal places.\n    \"\"\"\n\n    def _accuracy(y_true, y_pred):\n        total = 0\n        correct = 0\n        for t, p in zip(y_true, y_pred):\n            total += 1\n            if t == p:\n                correct += 1\n        if total == 0:\n            return 0.0\n        return round(correct / total, 4)\n\n    def _precision(y_true, y_pred):\n        tp = fp = 0\n        for t, p in zip(y_true, y_pred):\n            if p == 1:\n                if t == 1:\n                    tp += 1\n                else:\n                    fp += 1\n        denom = tp + fp\n        if denom == 0:\n            return 0.0\n        return round(tp / denom, 4)\n\n    def _recall(y_true, y_pred):\n        tp = fn = 0\n        for t, p in zip(y_true, y_pred):\n            if t == 1:\n                if p == 1:\n                    tp += 1\n                else:\n                    fn += 1\n        denom = tp + fn\n        if denom == 0:\n            return 0.0\n        return round(tp / denom, 4)\n\n    def _f1(y_true, y_pred):\n        tp = fp = fn = 0\n        for t, p in zip(y_true, y_pred):\n            if t == 1:\n                if p == 1:\n                    tp += 1\n                else:\n                    fn += 1\n            elif p == 1:\n                fp += 1\n\n        prec_denom = tp + fp\n        rec_denom = tp + fn\n\n        precision = tp / prec_denom if prec_denom else 0.0\n        recall = tp / rec_denom if rec_denom else 0.0\n\n        if precision == 0 and recall == 0:\n            return 0.0\n\n        f1_value = 2 * precision * recall / (precision + recall)\n        return round(f1_value, 4)\n\n    metrics = {\n        'accuracy': _accuracy,\n        'precision': _precision,\n        'recall': _recall,\n        'f1': _f1\n    }\n\n    return metrics.get(name, -1)\n```"}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\nclass _TreeNode:\n    \"\"\"A single node in a CART regression tree.\"\"\"\n    __slots__ = (\"is_leaf\", \"value\", \"feature\", \"threshold\", \"left\", \"right\")\n\n    def __init__(self, *, value=None, feature=None, threshold=None,\n                 left=None, right=None, is_leaf=False):\n        self.is_leaf = is_leaf\n        self.value = value            # float, mean of y in this node\n        self.feature = feature        # int\n        self.threshold = threshold    # float\n        self.left = left              # _TreeNode\n        self.right = right            # _TreeNode\n\n\nclass _CARTRegressor:\n    \"\"\"Very small, depth\u2013limited CART regressor (MSE criterion).\"\"\"\n\n    def __init__(self, max_depth: int = 3):\n        self.max_depth = max_depth\n        self.root: _TreeNode | None = None\n\n    # ---------- public helpers ----------\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.root = self._grow(X, y, depth=0)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        # vectorised traversal\n        preds = np.empty(X.shape[0], dtype=float)\n        for i, x in enumerate(X):\n            preds[i] = self._predict_row(self.root, x)\n        return preds\n\n    # ---------- private helpers ----------\n    def _predict_row(self, node: _TreeNode, x: np.ndarray) -> float:\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.value\n\n    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n        n_samples, n_feats = X.shape\n        # leaf if depth limit, pure node, or not enough points\n        if depth >= self.max_depth or n_samples <= 1 or np.all(y == y[0]):\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n\n        # current impurity (sum of squared errors)\n        best_sse = np.var(y) * n_samples\n        best_feat, best_thr, best_split = None, None, None\n\n        for feat in range(n_feats):\n            x_col = X[:, feat]\n            # sort to evaluate splits fast\n            sorted_idx = np.argsort(x_col)\n            x_sorted, y_sorted = x_col[sorted_idx], y[sorted_idx]\n\n            # unique candidate thresholds \u2013 mid-points between distinct values\n            uniq_vals = np.unique(x_sorted)\n            if uniq_vals.size == 1:       # cannot split on constant column\n                continue\n            thr_candidates = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n\n            # cumulative sums for fast left/right statistics\n            y_cumsum = np.cumsum(y_sorted)\n            y_sq_cumsum = np.cumsum(y_sorted ** 2)\n\n            for idx, thr in enumerate(thr_candidates, start=1):\n                # position where x <= thr  (since x_sorted is ascending)\n                # find index of last value <= thr \u21d2 here it is idx because\n                # thr lies between uniq_vals[idx-1] and uniq_vals[idx]\n                n_left = np.searchsorted(x_sorted, thr, side=\"right\")\n                # quick skip if split would put all points on one side\n                if n_left == 0 or n_left == n_samples:\n                    continue\n\n                # left stats\n                sum_y_left = y_cumsum[n_left - 1]\n                sum_yy_left = y_sq_cumsum[n_left - 1]\n                n_l = n_left\n                mse_left = sum_yy_left - (sum_y_left ** 2) / n_l\n\n                # right stats\n                sum_y_right = y_cumsum[-1] - sum_y_left\n                sum_yy_right = y_sq_cumsum[-1] - sum_yy_left\n                n_r = n_samples - n_l\n                mse_right = sum_yy_right - (sum_y_right ** 2) / n_r\n\n                sse_split = mse_left + mse_right\n                if sse_split < best_sse - 1e-7:   # tolerance to avoid tiny gains\n                    best_sse = sse_split\n                    best_feat = feat\n                    best_thr = thr\n                    best_split = n_left, sorted_idx  # store index break\n\n        # no useful split found \u2192 leaf\n        if best_feat is None:\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n\n        # build children\n        split_idx, sorted_idx = best_split\n        x_sorted_global = X[sorted_idx, best_feat]\n        mask_left = x_sorted_global <= best_thr\n        left_idx = sorted_idx[mask_left]\n        right_idx = sorted_idx[~mask_left]\n\n        left_child = self._grow(X[left_idx], y[left_idx], depth + 1)\n        right_child = self._grow(X[right_idx], y[right_idx], depth + 1)\n\n        return _TreeNode(feature=best_feat,\n                         threshold=best_thr,\n                         left=left_child,\n                         right=right_child,\n                         value=float(y.mean()),   # not used but nice to store\n                         is_leaf=False)\n\n\ndef gbdt_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    n_estimators: int = 10,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor (simplified).\"\"\"\n    # --- initial prediction ---\n    init_pred = y_train.mean()\n    y_hat_train = np.full_like(y_train, init_pred, dtype=float)\n    y_hat_test = np.full(X_test.shape[0], init_pred, dtype=float)\n\n    # --- iterative boosting ---\n    for _ in range(n_estimators):\n        residuals = y_train - y_hat_train              # negative gradient\n        tree = _CARTRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n\n        # update predictions\n        y_hat_train += learning_rate * tree.predict(X_train)\n        y_hat_test += learning_rate * tree.predict(X_test)\n\n    # --- output ---\n    return np.round(y_hat_test, 4).tolist()\n```"}
{"task_id": 414, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef exponential_scheduler(step: int,\n                          initial_lr: float = 0.01,\n                          stage_length: int = 500,\n                          staircase: bool = False,\n                          decay: float = 0.1) -> float:\n    \"\"\"Return the exponentially decayed learning-rate for a given optimisation step.\n\n    The learning-rate is computed as:\n\n        lr = initial_lr * decay ** curr_stage\n\n    where ``curr_stage`` is either the real-valued ratio ``step / stage_length``\n    (smooth schedule) or its floor value (stair-case schedule).\n\n    Returns the learning-rate rounded to 6 decimal places or \u20131 if any\n    argument is invalid.\n    \"\"\"\n    # ---------- validation ----------\n    # step must be a non-negative integer\n    if step < 0:\n        return -1\n    # stage_length must be a positive integer\n    if not isinstance(stage_length, int) or stage_length <= 0:\n        return -1\n    # decay must be strictly positive\n    if decay <= 0:\n        return -1\n\n    # ---------- stage computation ----------\n    if staircase:\n        curr_stage = step // stage_length      # integer division (floor)\n    else:\n        curr_stage = step / stage_length       # smooth decay\n\n    # ---------- learning-rate computation ----------\n    lr = initial_lr * (decay ** curr_stage)\n\n    # ---------- rounding ----------\n    return round(float(lr), 6)\n```"}
{"task_id": 415, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dct2(frame: list[int | float]) -> list[float]:\n    \"\"\"Un-normalised 1-D Discrete Cosine Transform (type II).\n\n    Parameters\n    ----------\n    frame : list[int | float]\n        Real-valued input samples (length N).\n\n    Returns\n    -------\n    list[float]\n        Length-N list with the DCT-II coefficients, each rounded\n        to 4 decimal places.\n    \"\"\"\n    # ---- handle the trivial case ------------------------------------------------\n    if not frame:                       # empty input \u21d2 empty output\n        return []\n\n    # ---- numpy set-up -----------------------------------------------------------\n    x = np.asarray(frame, dtype=float)       # (N,) input vector\n    N = x.size\n    n = np.arange(N).reshape(-1, 1)          # column vector  (N,1)\n    k = np.arange(N).reshape(1,  -1)         # row    vector  (1,N)\n\n    # ---- core DCT-II computation -----------------------------------------------\n    angles = (np.pi / N) * (n + 0.5) * k     # (N,N) matrix of the angles\n    cosines = np.cos(angles)                 # (N,N) cos matrix\n\n    coeffs = (x.reshape(-1, 1) * cosines).sum(axis=0)  # \u03a3_n x_n\u00b7cos(...), shape (N,)\n\n    # ---- round to 4 decimal places and convert to plain Python floats -----------\n    return [round(float(c), 4) for c in coeffs]\n```"}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Evaluate the multivariate normal (Gaussian) probability-density function for\n    every sample contained in `X`.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)  or  (n_samples,)\n        Data points at which the PDF will be evaluated.  If `X` is one\u2013dimensional\n        it is treated as having shape (n_samples, 1).\n    mean : array_like, shape (n_features,)\n        Mean vector (\u03bc) of the distribution.\n    cov : array_like, shape (n_features, n_features)\n        Positive-definite covariance matrix (\u03a3) of the distribution.\n\n    Returns\n    -------\n    list\n        A list whose *i-th* element is the PDF value for `X[i]`, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n\n    # If X is 1-D treat it as (n_samples, 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    n_samples, n_features = X.shape\n\n    # Basic shape validations\n    if mean.shape[0] != n_features:\n        raise ValueError(\"Mean vector length must equal the number of features.\")\n    if cov.shape != (n_features, n_features):\n        raise ValueError(\"Covariance matrix shape must be (n_features, n_features).\")\n\n    # Inverse and determinant of covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite (determinant > 0).\")\n\n    # Constant coefficient 1 / sqrt((2\u03c0)^d * det(\u03a3))\n    coeff = 1.0 / math.sqrt(((2 * math.pi) ** n_features) * det_cov)\n\n    # Mahalanobis distance for every sample: (x-\u03bc)^T \u03a3^{-1} (x-\u03bc)\n    diff = X - mean                      # shape: (n_samples, n_features)\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)  # shape: (n_samples,)\n\n    # Exponent part and the final pdf values\n    exponent = -0.5 * mahal\n    pdf_vals = coeff * np.exp(exponent)\n\n    # Round to 4 decimal places and return as a regular Python list\n    return [round(val, 4) for val in pdf_vals.tolist()]\n```"}
{"task_id": 417, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pca(data: np.ndarray, k: int = 1) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Perform Principal Component Analysis and return the projected data and its reconstruction.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int, default=1\n        Number of principal components to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        * First element  \u2013 the data expressed in the *k*-dimensional PCA space.\n        * Second element \u2013 the data reconstructed back in the original space.\n\n        Both matrices are rounded to 4 decimal places before being converted\n        to ordinary Python lists.\n    \"\"\"\n    # --- 1. centre the data --------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D NumPy array.\")\n    n_samples, n_features = data.shape\n    mean = data.mean(axis=0, keepdims=True)          # shape (1, n_features)\n    Xc = data - mean                                 # centred data\n\n    # --- 2. unbiased covariance matrix ---------------------------------------\n    if n_samples > 1:\n        cov = Xc.T @ Xc / (n_samples - 1)            # shape (n_features, n_features)\n    else:                                            # edge case: single sample\n        cov = np.zeros((n_features, n_features), dtype=Xc.dtype)\n\n    # --- 3. eigen-decomposition (covariance is symmetric) --------------------\n    eig_vals, eig_vecs = np.linalg.eigh(cov)         # ascending order\n\n    # --- 4. sort eigenvectors by descending eigenvalue magnitude -------------\n    idx = np.argsort(eig_vals)[::-1]                 # descending indices\n    eig_vecs = eig_vecs[:, idx]\n\n    # keep only the first k (allow k >= n_features)\n    k = min(max(int(k), 0), n_features)\n    eig_vecs = eig_vecs[:, :k]\n\n    # --- 5. deterministic orientation ----------------------------------------\n    for j in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, j]\n        nz = np.flatnonzero(vec)\n        if nz.size and vec[nz[0]] < 0:               # flip sign if first non-zero is negative\n            eig_vecs[:, j] *= -1\n\n    # --- 6. projection onto the selected eigenvectors ------------------------\n    low_dim = Xc @ eig_vecs                          # shape (n_samples, k)\n\n    # --- 7. reconstruction in the original space -----------------------------\n    recon = low_dim @ eig_vecs.T + mean              # add the mean back\n\n    # --- 8. round to 4 decimal places ----------------------------------------\n    low_dim_rounded = np.round(low_dim, 4).tolist()\n    recon_rounded   = np.round(recon,   4).tolist()\n\n    # --- 9. return -----------------------------------------------------------\n    return low_dim_rounded, recon_rounded\n```"}
{"task_id": 418, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(\n    X: np.ndarray | list[list[int | float]]\n) -> list[list[float]]:\n    \"\"\"Compute and return the sample covariance matrix of *X*.\n\n    Args:\n        X: A 2-D NumPy array or a (nested) Python list whose rows correspond\n           to samples and columns correspond to features.\n\n    Returns:\n        The sample covariance matrix as a nested Python list, with every\n        element rounded to four decimal places.\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # 1. Convert input to a float NumPy array (also handles plain Python lists)\n    # ---------------------------------------------------------------------\n    X = np.asarray(X, dtype=float)\n\n    # If X is 1-D, treat it as a single-feature data set (n_samples \u00d7 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    if X.ndim != 2:\n        raise ValueError(\"Input data must be 1-D or 2-D (samples \u00d7 features).\")\n\n    n_samples, n_features = X.shape\n\n    # ---------------------------------------------------------------------\n    # 2. Handle the degenerate case n_samples < 2\n    # ---------------------------------------------------------------------\n    if n_samples < 2:\n        return np.zeros((n_features, n_features), dtype=float).tolist()\n\n    # ---------------------------------------------------------------------\n    # 3. Center the data and compute the (unbiased) sample covariance\n    #    cov = (X_c.T @ X_c) / (n_samples - 1)\n    # ---------------------------------------------------------------------\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n\n    # ---------------------------------------------------------------------\n    # 4. Round to 4 decimal places and return as nested Python list\n    # ---------------------------------------------------------------------\n    return np.round(cov_matrix, 4).tolist()\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (kept twice to respect the given header)\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,          # kept for signature completeness (not used)\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression \u2013 MAP predictive mean (rounded to 4 dp).\"\"\"\n    \n    # 1. Ensure 2-D/1-D numpy arrays\n    X      = np.asarray(X,      dtype=float)\n    y      = np.asarray(y,      dtype=float).ravel()\n    X_new  = np.asarray(X_new,  dtype=float)\n\n    # 2. (Optional) intercept handling\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test  = np.ones((X_new.shape[0], 1))\n        X      = np.hstack((ones_train, X))\n        X_new  = np.hstack((ones_test,  X_new))\n\n    N, D = X.shape   # D \u2013 number of parameters\n\n    # 3. Prior mean \u03bc\n    if np.isscalar(mu):\n        mu_vec = np.full(D, mu, dtype=float)\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != D:\n            raise ValueError(\"`mu` has incompatible length.\")\n    \n    # 4. Prior covariance V  \u2794  precision V_inv\n    if V is None:                  # Identity\n        V_inv = np.eye(D)\n    elif np.isscalar(V):           # Scalar multiple of I\n        if V == 0:\n            raise ValueError(\"Scalar `V` must be non-zero.\")\n        V_inv = np.eye(D) / float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:        # Diagonal given as 1-D array\n            if np.any(V_arr == 0):\n                raise ValueError(\"Zero on the diagonal of `V` is not allowed.\")\n            V_inv = np.diag(1.0 / V_arr)\n        else:                      # Full square matrix\n            if V_arr.shape != (D, D):\n                raise ValueError(\"`V` shape incompatible with number of features.\")\n            V_inv = np.linalg.inv(V_arr)\n\n    # 5. Posterior covariance \u03a3_p and mean \u03bc_p\n    XtX   = X.T @ X\n    A     = V_inv + XtX                   # Posterior precision\n    Sigma_p = np.linalg.inv(A)            # Posterior covariance\n    rhs     = V_inv @ mu_vec + X.T @ y\n    mu_p    = Sigma_p @ rhs               # Posterior mean (MAP estimator)\n\n    # 6. Predictive MAP means for X_new\n    y_pred = X_new @ mu_p\n\n    # 7. Round to 4 decimal places and return as Python list\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 420, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n\n    Notes\n    -----\n    If *column* is outside the valid range, two empty lists are returned.\n    \"\"\"\n    # Basic validation: X must be a 2-D ndarray and column must be in range\n    if (\n        not isinstance(X, np.ndarray)\n        or X.ndim != 2\n        or not (0 <= column < X.shape[1])\n    ):\n        # Return two empty Python lists when the column index is invalid\n        return [], []\n\n    # Vectorised comparison (does NOT modify X)\n    left_mask_np = X[:, column] < value          # strictly less than\n    right_mask_np = X[:, column] >= value        # greater than or equal to\n\n    # Convert to plain Python lists\n    return left_mask_np.tolist(), right_mask_np.tolist()\n```"}
{"task_id": 421, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density\u2010Based Spatial Clustering (DBSCAN) from scratch.\n\n    Args\n    ----\n    data : np.ndarray, shape = (n_samples, n_features)\n        The data to cluster.\n    eps : float\n        Maximum distance at which two points are considered neighbours.\n    min_samples : int\n        Minimum number of neighbours (including the point itself) that makes a\n        point a *core* point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise is labelled -1, while clusters\n        are numbered 1, 2, \u2026 in the order they are first encountered during the\n        left-to-right scan over *data*.\n    \"\"\"\n    if eps <= 0:\n        raise ValueError(\"eps must be positive.\")\n    if min_samples <= 0:\n        raise ValueError(\"min_samples must be a positive integer.\")\n\n    n_samples = data.shape[0]\n    labels = np.zeros(n_samples, dtype=int)          #  0 = unvisited/unlabelled\n    cluster_id = 0                                   # start counting clusters at 1\n    eps_sq = eps * eps                               # use squared distance\n\n    # ------------------------------------------------------------------\n    # Helper: neighbourhood search\n    # ------------------------------------------------------------------\n    def region_query(idx: int) -> np.ndarray:\n        \"\"\"Return indices of all points within *eps* of point *idx* (incl. itself).\"\"\"\n        diff = data - data[idx]\n        dists_sq = np.sum(diff * diff, axis=1)\n        return np.flatnonzero(dists_sq <= eps_sq)\n\n    # ------------------------------------------------------------------\n    # Helper: expand the current cluster starting from a core point\n    # ------------------------------------------------------------------\n    def expand_cluster(seed_idx: int, neighbours: np.ndarray, cid: int) -> None:\n        \"\"\"Flood-fill the cluster *cid* starting from the core point *seed_idx*.\"\"\"\n        labels[seed_idx] = cid\n        # We work with a Python list so that we can keep appending new neighbours.\n        # Use `list(neighbours)` to make an independent copy.\n        search_queue = list(neighbours)\n\n        i = 0\n        while i < len(search_queue):\n            point_idx = search_queue[i]\n            if labels[point_idx] == -1:          # previously marked noise \u2013 now density-reachable\n                labels[point_idx] = cid\n            elif labels[point_idx] == 0:         # unvisited\n                labels[point_idx] = cid\n                neighbour_pts = region_query(point_idx)\n                if neighbour_pts.size >= min_samples:     # point is a *core*\n                    # Add new neighbours to the queue (avoid repeated indices\n                    # only for efficiency, correctness is unaffected).\n                    for n in neighbour_pts:\n                        if labels[n] == 0 or labels[n] == -1:\n                            search_queue.append(n)\n            # advance to next element in the queue\n            i += 1\n\n    # ------------------------------------------------------------------\n    # Main loop \u2013 scan every point in the order given\n    # ------------------------------------------------------------------\n    for point_idx in range(n_samples):\n        if labels[point_idx] != 0:          # already processed\n            continue\n\n        neighbours = region_query(point_idx)\n        if neighbours.size < min_samples:\n            labels[point_idx] = -1          # mark as noise\n        else:\n            cluster_id += 1\n            expand_cluster(point_idx, neighbours, cluster_id)\n\n    return labels.tolist()\n```\n\nHow it works\n-------------\n1. Each point starts as *unvisited* (label 0).\n2. We iterate through the dataset once (left \u2192 right).\n3. If a point has fewer than `min_samples` neighbours within `eps`, it is marked `-1`\n   (noise) **for now**.\n4. Otherwise it is a core point; a new cluster ID is created and we *expand* the\n   cluster:\n   \u2022 Every neighbour is added to the cluster.  \n   \u2022 If any neighbour itself becomes a core point, all of *its* neighbours are\n     appended to the search queue, realising *density reachability*.\n5. After the scan ends every sample is labelled either with a positive cluster\n   ID or with `-1` (noise)."}
{"task_id": 422, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _as_python(obj):\n    \"\"\"\n    Helper that converts a NumPy scalar/array into regular Python\n    scalars / (nested) lists after rounding to 4 decimals.\n    \"\"\"\n    obj = np.round(obj, 4)  # round first\n    # scalar \u2192 plain float\n    if np.isscalar(obj) or obj.shape == ():      # covers 0-dim arrays\n        return float(obj)\n    # array \u2192 nested Python lists\n    return obj.tolist()\n\n\ndef softplus(x: \"np.ndarray | list | tuple | float | int\", order: int = 0):\n    \"\"\"SoftPlus activation function and its derivatives.\n\n    Parameters\n    ----------\n    x : float | int | list | tuple | np.ndarray\n        Input data on which SoftPlus or its derivatives will be computed.\n    order : int, optional\n        0  -> SoftPlus(x)\n        1  -> SoftPlus'(x)\n        2  -> SoftPlus''(x)\n        Any other value makes the function return -1.\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimal places. The shape mirrors the input. If the\n        order is invalid, returns -1.\n    \"\"\"\n    # --- convert the input to a NumPy array for vectorised computation ---\n    x = np.asarray(x, dtype=float)\n\n    # --- select the required expression based on *order* ---------------\n    if order == 0:\n        # SoftPlus: ln(1 + e^x)  \u2013 use logaddexp for numerical stability\n        res = np.logaddexp(0.0, x)\n    elif order == 1:\n        # First derivative: e^x / (1 + e^x)  \u2013 sigmoid(x)\n        res = 1.0 / (1.0 + np.exp(-x))\n    elif order == 2:\n        # Second derivative: e^x / (1 + e^x)^2\n        sig = 1.0 / (1.0 + np.exp(-x))   # sigmoid\n        res = sig * (1.0 - sig)          # stable form\n    else:\n        return -1\n\n    # --- round & convert to regular Python containers ------------------\n    return _as_python(res)\n```\n\nExample usage\n--------------\n```python\n>>> softplus(0)\n0.6931\n>>> softplus([0, 1, 2], order=1)\n[0.5, 0.7311, 0.8808]\n>>> softplus((0, 1, 2), order=2)\n[0.25, 0.1966, 0.1049]\n>>> softplus(3.2, order=5)\n-1\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute Root Mean Squared Logarithmic Error (RMSLE).\n\n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or -1 if validation fails.\n    \"\"\"\n    # 1. Basic validation \u2013 same length and non-empty\n    if not isinstance(actual, (list, tuple, np.ndarray)) or \\\n       not isinstance(predicted, (list, tuple, np.ndarray)) or \\\n       len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n\n    # 2. Convert to numpy arrays and ensure numeric dtype\n    try:\n        actual_arr    = np.asarray(actual,    dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        return -1  # Non-numeric value encountered\n\n    # 3. Check non-negativity\n    if np.any(actual_arr < 0) or np.any(predicted_arr < 0):\n        return -1\n\n    # 4. Compute RMSLE\n    log_diff = np.log1p(predicted_arr) - np.log1p(actual_arr)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n\n    # 5. Return rounded result\n    return round(float(rmsle), 4)\n```"}
{"task_id": 424, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    # Convert to NumPy array for vectorised operations\n    labels = np.asarray(y)\n    n = labels.size                       # total number of samples\n\n    # Frequencies of each distinct label\n    _, counts = np.unique(labels, return_counts=True)\n    probs = counts / n                    # relative frequencies p_i\n\n    # Shannon entropy in bits\n    h = -np.sum(probs * np.log2(probs))\n\n    # Round to 4 decimal places and return as native Python float\n    return float(np.round(h, 4))\n```"}
{"task_id": 425, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef f_entropy(x: list[int] | np.ndarray) -> float:\n    \"\"\"\n    Shannon (natural-log) entropy of a 1-D collection of non-negative integers.\n\n    Parameters\n    ----------\n    x : list[int] | np.ndarray\n        Input data. May be a Python list or a NumPy 1-D array containing\n        non-negative integers.\n\n    Returns\n    -------\n    float\n        Shannon entropy  H = -\u03a3 p_i ln(p_i)  rounded to 4 decimal places.\n        If the input contains only one distinct value (or is empty),\n        the function returns 0.0.\n    \"\"\"\n    # Convert to 1-D NumPy array\n    arr = np.asarray(x, dtype=int).ravel()\n\n    # Empty input \u2192 zero entropy\n    if arr.size == 0:\n        return 0.0\n\n    # Frequencies of the distinct symbols\n    unique, counts = np.unique(arr, return_counts=True)\n\n    # Only one symbol present \u2192 zero entropy\n    if unique.size == 1:\n        return 0.0\n\n    # Convert counts to probabilities (float division)\n    probs = counts.astype(float) / arr.size\n\n    # Shannon entropy (natural logarithm)\n    entropy = -np.sum(probs * np.log(probs))\n\n    # Round to 4 decimal places and return plain Python float\n    return round(float(entropy), 4)\n```"}
{"task_id": 426, "completion_id": 0, "completion": "```python\ndef classification_accuracy(y_true: list, y_pred: list) -> float:\n    \"\"\"Compute the accuracy between the true and predicted class labels.\n\n    Args:\n        y_true (list): Ground-truth class labels.\n        y_pred (list): Predicted class labels produced by a model.\n\n    Returns:\n        float: The accuracy score rounded to 4 decimal places.\n               Returns -1 when the inputs have different lengths or when\n               either of the inputs is empty.\n    \"\"\"\n    # Validate inputs\n    if not y_true or not y_pred or len(y_true) != len(y_pred):\n        return -1\n\n    # Count correct predictions\n    correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n    total = len(y_true)\n\n    accuracy = correct / total\n    return round(accuracy, 4)\n```"}
{"task_id": 427, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef identity_activation(z):\n    \"\"\"Compute identity activation and its derivative.\n\n    Args:\n        z (list | tuple | numpy.ndarray): Input data (any dimensionality).\n\n    Returns:\n        tuple[list, list]: (activation, derivative) \u2013\n            both are regular Python lists mirroring the shape of *z*.\n    \"\"\"\n    # Convert the input to a NumPy array (no copy if already an ndarray)\n    z_arr = np.asarray(z)\n\n    # Identity activation: f(z) = z\n    activation = z_arr.copy()          # copy to avoid aliasing side-effects\n\n    # Derivative of identity: f'(z) = 1 (element-wise)\n    derivative = np.ones_like(z_arr)\n\n    # Convert the results back to (nested) Python lists\n    return activation.tolist(), derivative.tolist()\n```"}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine\u2010sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places. Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    # -------- special-case handling ----------------------------------------\n    if window_len <= 0:\n        return []\n    if window_len == 1:               # the only valid coefficient is 1.0\n        return [1.0]\n\n    # -------- choose build length ------------------------------------------\n    # Periodic window needs one extra point before discarding the last.\n    build_len = window_len if symmetric else window_len + 1\n\n    # -------- actual Hann formula ------------------------------------------\n    n = np.arange(build_len, dtype=float)\n    denom = build_len - 1\n    window = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / denom)\n\n    # -------- periodic case: drop last (duplicated) sample -----------------\n    if not symmetric:\n        window = window[:-1]\n\n    # -------- round to 4 decimals and return plain Python list -------------\n    window = np.round(window, 4)\n\n    return window.tolist()\n```"}
{"task_id": 429, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        2-D array of shape (D, K); row i is the success probabilities of all K\n        arms when the active context is i.\n    context : np.ndarray\n        One-hot vector of length D identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward\n        (rounded to four decimals) and the index of the arm that achieves it.\n        If *context* is not a valid one-hot vector the function returns (-1, -1).\n    \"\"\"\n    # ---------- sanity checks for the one-hot context ----------\n    # must be a 1-D array\n    if context.ndim != 1:\n        return (-1, -1)\n\n    D = context_probs.shape[0]\n    # length must match the number of contexts\n    if len(context) != D:\n        return (-1, -1)\n\n    # valid one-hot => exactly one element equal to 1 and the rest equal to 0\n    # (allow both int and float representations of 0/1)\n    if np.count_nonzero(context) != 1:\n        return (-1, -1)\n\n    # ensure every entry is either 0 or 1\n    unique_vals = np.unique(context)\n    if not np.all(np.isin(unique_vals, [0, 1])):\n        return (-1, -1)\n\n    # ---------- identify the active context ----------\n    ctx_idx = int(np.argmax(context))          # the index of the single 1\n    row = context_probs[ctx_idx]               # probabilities for that context\n\n    # ---------- find the best arm ----------\n    optimal_arm = int(np.argmax(row))          # first index of the maximum value\n    optimal_rwd = float(np.round(row[optimal_arm], 4))\n\n    return (optimal_rwd, optimal_arm)\n```"}
{"task_id": 430, "completion_id": 0, "completion": "```python\nfrom typing import List, Dict, Set, Tuple, Optional\n\ndef topological_sort(vertices: List, edges: List[Tuple]) -> Optional[List]:\n    \"\"\"Return a deterministic topological ordering of a directed graph.\n\n    The function performs a depth-first search (DFS) that\n    1. starts the search in *vertices* order, left-to-right;\n    2. explores every vertex\u2019s outgoing neighbours in the same left-to-right\n       order induced by *vertices*.\n\n    If during the DFS a back-edge (an edge to a vertex that is still on the\n    recursion stack) is encountered, the graph contains a directed cycle and\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    vertices : list\n        List of vertex identifiers (must be hashable).\n    edges : list[tuple]\n        List of directed edges (u, v) with u \u2192 v.\n\n    Returns\n    -------\n    list\n        A list containing the vertices in deterministic topological order, or\n    None\n        if the graph contains at least one directed cycle.\n    \"\"\"\n    # ------------------------------------------------------------\n    # Build a position map so we can sort neighbours once up-front\n    # according to the order in which they appear in *vertices*.\n    # ------------------------------------------------------------\n    position: Dict = {v: i for i, v in enumerate(vertices)}\n\n    # Adjacency list with every vertex present (even isolated ones)\n    adj: Dict = {v: [] for v in vertices}\n    for u, v in edges:\n        # If the input may contain unknown vertices\n        # you could raise an error instead of \"continue\".\n        if u in position and v in position:\n            adj[u].append(v)\n\n    # Sort each neighbour list so that DFS visits them deterministically.\n    for u in adj:\n        adj[u].sort(key=position.__getitem__)\n\n    # ------------------------------------------------------------\n    # DFS with three-colour marking:\n    #   0 = unvisited, 1 = currently on recursion stack, 2 = finished\n    # ------------------------------------------------------------\n    state: Dict = {v: 0 for v in vertices}\n    topo: List = []                      # post-order list\n\n    def dfs(node) -> bool:\n        \"\"\"Return False on cycle detection, True otherwise.\"\"\"\n        state[node] = 1                  # mark as \u2018visiting\u2019\n        for nei in adj[node]:\n            if state[nei] == 0:          # tree edge\n                if not dfs(nei):\n                    return False\n            elif state[nei] == 1:        # back edge \u21d2 cycle\n                return False\n            # state[nei] == 2  \u2192 cross/forward edge, fine\n        state[node] = 2                  # finished\n        topo.append(node)                # post-order append\n        return True\n\n    # Run DFS, always starting with the left-to-right vertex order\n    for v in vertices:\n        if state[v] == 0:\n            if not dfs(v):\n                return None              # cycle found\n\n    topo.reverse()                       # convert post-order to topo order\n    return topo\n```"}
{"task_id": 431, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef dist(x1, x2):\n    \"\"\"Compute the Euclidean distance between two 1-D vectors.\n\n    Parameters\n    ----------\n    x1, x2 : list | tuple | np.ndarray\n        1-D sequences/arrays containing numeric (int/float) values.\n\n    Returns\n    -------\n    float\n        L2 distance rounded to 4 decimal places, or -1 if the vectors\n        have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays (this also validates numeric types)\n    try:\n        a = np.asarray(x1, dtype=float).ravel()\n        b = np.asarray(x2, dtype=float).ravel()\n    except (TypeError, ValueError):\n        # Non-numeric data encountered\n        raise ValueError(\"Both vectors must contain numeric data only.\")\n\n    # Ensure both vectors are 1-D and of equal length\n    if a.size != b.size:\n        return -1\n\n    # Compute Euclidean (L2) distance\n    dist_val = np.linalg.norm(a - b)\n\n    # Round to 4 decimal places and return as native Python float\n    return float(round(dist_val, 4))\n```\n"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute Xavier/Glorot fan-in and fan-out values for a weight tensor.\n\n    Args:\n        shape: Tensor shape. For 4-D convolutional kernels the expected layout\n               is (out_channels, in_channels, kernel_height, kernel_width).\n\n    Returns:\n        A tuple (fan_in, fan_out) as floats.\n    \"\"\"\n    # Ensure we are working with a sequence we can index\n    if len(shape) == 4:          # 2-D convolutional kernel\n        out_c, in_c, k_h, k_w = shape[:4]\n        receptive_field_size = k_h * k_w\n        fan_in  = in_c  * receptive_field_size\n        fan_out = out_c * receptive_field_size\n    else:                        # Dense / embedding / other tensors\n        fan_in, fan_out = shape[0], shape[1]\n\n    # Return as floats, as requested\n    return float(fan_in), float(fan_out)\n```"}
{"task_id": 434, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(logits: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute a numerically-stable, row-wise softmax.\n\n    Parameters\n    ----------\n    logits : np.ndarray\n        1-D array of shape (n,) or 2-D array of shape (m, n)\n        containing the raw scores.\n\n    Returns\n    -------\n    list[list[float]]\n        Row-wise softmax probabilities, each rounded to 6 decimals.\n        Even for a 1-D input, a list containing a single list is returned.\n    \"\"\"\n    # Ensure we are working with a NumPy float array\n    logits = np.asarray(logits, dtype=np.float64)\n    \n    # Treat a 1-D input as a single row\n    if logits.ndim == 1:\n        logits = logits[np.newaxis, :]\n\n    # Numerically-stable shift: subtract the row-wise max\n    shifted = logits - np.max(logits, axis=1, keepdims=True)\n\n    # Exponentiate and normalize\n    exp_vals = np.exp(shifted)\n    probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n\n    # Round to 6 decimals and convert to plain Python lists\n    return np.round(probs, 6).tolist()\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into\n    a plain Python list.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Convert inputs to NumPy arrays of type float\n    # ------------------------------------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # Ensure X is two-dimensional\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(-1, 1)\n\n    m = y_arr.shape[0]                         # number of samples\n    y_mean = y_arr.mean()                      # scalar target mean\n\n    # ------------------------------------------------------------------\n    # 2. Handle the special cases\n    # ------------------------------------------------------------------\n    if n_estimators <= 0 or learning_rate == 0:\n        return np.round(np.full(m, y_mean), 4).tolist()\n\n    # ------------------------------------------------------------------\n    # 3. Initial prediction is the mean of y\n    # ------------------------------------------------------------------\n    current_pred = np.full(m, y_mean, dtype=float)\n\n    # Pre-compute the design matrix with an intercept term\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.concatenate((ones, X_arr), axis=1)  # shape (m, d+1)\n\n    # ------------------------------------------------------------------\n    # 4. Gradient-boosting iterations\n    # ------------------------------------------------------------------\n    for _ in range(n_estimators):\n        # Residuals of the current model\n        residuals = y_arr - current_pred\n\n        # Ordinary Least Squares fit to the residuals\n        # beta has shape (d+1,)\n        beta, _, _, _ = np.linalg.lstsq(X_design, residuals, rcond=None)\n\n        # Weak learner predictions\n        h_pred = X_design @ beta                 # shape (m,)\n\n        # Update the overall prediction\n        current_pred += learning_rate * h_pred\n\n    # ------------------------------------------------------------------\n    # 5. Return rounded predictions as a plain Python list\n    # ------------------------------------------------------------------\n    return np.round(current_pred, 4).tolist()\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n\n    arr = np.asarray(x, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n\n    if order == 0:\n        res = sig\n    elif order == 1:\n        res = sig * (1.0 - sig)\n    else:                      # order == 2\n        res = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n\n    res = np.round(res, 4)     # round to 4 decimal places\n\n    # Preserve original type/shape convention\n    if res.shape == ():        # scalar input \u2192 Python float\n        return float(res)\n    return res.tolist()         # list or ndarray input \u2192 Python list\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef to_categorical(\n    y: list[int] | np.ndarray, \n    num_classes: int | None = None\n) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *y*, or ``-1`` when the given\n        *num_classes* is smaller than required.\n    \"\"\"\n    # Convert the input to a 1-D NumPy array of integers\n    y_arr = np.asarray(y, dtype=int).ravel()\n\n    # Handle the trivial empty-input case early\n    if y_arr.size == 0:\n        inferred_classes = 0 if num_classes is None else num_classes\n        # produce [] no matter what \u2013 an empty one-hot matrix\n        return [] if inferred_classes == 0 else [[] for _ in range(0)]\n\n    # Maximum label present\n    max_label = int(y_arr.max())\n\n    # Determine / validate num_classes\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        # impossible to encode \u2013 not enough columns\n        return -1\n\n    # Build the one-hot matrix\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n\n    # Return as a plain Python list of lists\n    return one_hot.tolist()\n```"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\nimport math\nfrom collections import defaultdict\nfrom typing import Hashable, List, Tuple\n\ndef apriori_frequent_itemsets(\n    transactions: List[List[Hashable]], min_sup: float\n) -> List[Tuple]:\n    \"\"\"Find all frequent item-sets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The data base: one list per transaction, containing *hashable* items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction in (0, 1].\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets ordered first by their length and then\n        lexicographically inside each length block; inside every tuple the items\n        themselves appear in ascending (lexicographic) order.\n    \"\"\"\n    # ------------ basic checks ------------------------------------------------\n    if not (0 < min_sup <= 1):\n        raise ValueError(\"min_sup must be in the range (0, 1].\")\n\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return []\n\n    # Turn each transaction into a set \u2013 duplicates inside a transaction do not\n    # influence support counting and membership checks become O(1).\n    transactions = [set(t) for t in transactions]\n\n    # Convenience helper -------------------------------------------------------\n    def is_frequent(count: int) -> bool:\n        \"\"\"True if empirical support \u2265 min_sup.\"\"\"\n        # Faster than division, but still exact:  count / n \u2265 min_sup   <=>  \n        # count \u2265 min_sup * n\n        return count >= min_sup * n_transactions - 1e-12\n\n    # -------------------------------------------------------------------------\n    # 1-item candidate counting\n    # -------------------------------------------------------------------------\n    item_counts: defaultdict[Hashable, int] = defaultdict(int)\n    for t in transactions:\n        for item in t:\n            item_counts[item] += 1\n\n    L1 = [tuple([item]) for item, cnt in item_counts.items() if is_frequent(cnt)]\n    L1.sort()  # alphabetical order for length-1 block\n\n    frequent_itemsets: list[tuple] = []\n    frequent_itemsets.extend(L1)\n\n    # Frequent (k-1)-itemsets of the previous iteration\n    prev_L = L1\n    k = 2\n\n    while prev_L:\n        # ---------------------------------------------------------------------\n        # Candidate generation: self-join + Apriori subset pruning\n        # ---------------------------------------------------------------------\n        prev_L_len = len(prev_L)\n        prev_L_set = set(prev_L)                # for O(1) subset membership test\n        Ck: set[tuple] = set()                  # size-k candidates\n\n        for i in range(prev_L_len):\n            for j in range(i + 1, prev_L_len):\n                a, b = prev_L[i], prev_L[j]\n                # Join if the first k-2 items are identical\n                if a[:-1] != b[:-1]:\n                    break                       # because prev_L is sorted\n                candidate = tuple(sorted(set(a) | set(b)))  # union\n                if len(candidate) != k:\n                    continue\n\n                # Subset pruning: all (k-1)-subsets of candidate must be frequent\n                if all(tuple(subset) in prev_L_set\n                       for subset in itertools.combinations(candidate, k - 1)):\n                    Ck.add(candidate)\n\n        if not Ck:\n            break\n\n        # ---------------------------------------------------------------------\n        # Count supports of candidates\n        # ---------------------------------------------------------------------\n        candidate_counts = defaultdict(int)\n        # Convert candidates to frozenset once for fast \u2286 test inside loop\n        cand_sets = {cand: frozenset(cand) for cand in Ck}\n\n        for t in transactions:\n            for cand, cand_set in cand_sets.items():\n                if cand_set.issubset(t):\n                    candidate_counts[cand] += 1\n\n        # ---------------------------------------------------------------------\n        # Keep only the frequent candidates -> Lk\n        # ---------------------------------------------------------------------\n        Lk = [cand for cand, cnt in candidate_counts.items() if is_frequent(cnt)]\n        Lk.sort()            # lexicographic order inside size-k block\n\n        if not Lk:\n            break\n\n        frequent_itemsets.extend(Lk)\n        prev_L = Lk\n        k += 1\n\n    return frequent_itemsets\n```"}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]\n                          ) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class\u2013probability predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Shape (n_estimators, n_samples, n_classes).  Each innermost list is a\n        valid probability distribution (sums to 1).\n\n    Returns\n    -------\n    tuple\n        (\n          averaged_probs,      # list[list[float]] shape (n_samples, n_classes)\n          predicted_labels     # list[int]          length == n_samples\n        )\n        `averaged_probs` are rounded to 4 decimal places; `predicted_labels`\n        are obtained with arg-max on the rounded probabilities.  Ties are\n        resolved by the smallest class index (numpy\u2019s default behaviour).\n    \"\"\"\n    # --- Basic validation ----------------------------------------------------\n    if not predictions:\n        raise ValueError(\"`predictions` must contain at least one estimator.\")\n\n    # Convert to a NumPy array of shape (n_estimators, n_samples, n_classes)\n    arr = np.asarray(predictions, dtype=float)\n\n    if arr.ndim != 3:\n        raise ValueError(\"`predictions` must be 3-dimensional \"\n                         \"(n_estimators, n_samples, n_classes).\")\n\n    # --- 1. Average over estimators -----------------------------------------\n    mean_probs = arr.mean(axis=0)            # shape -> (n_samples, n_classes)\n\n    # --- 2. Round to 4 decimal places ---------------------------------------\n    mean_probs = np.round(mean_probs, 4)\n\n    # --- 3. Arg-max per sample ----------------------------------------------\n    predicted = np.argmax(mean_probs, axis=1)   # shape -> (n_samples,)\n\n    # Convert results back to regular Python lists (as required)\n    averaged_probs_list = mean_probs.tolist()\n    predicted_labels_list = predicted.tolist()\n\n    return averaged_probs_list, predicted_labels_list\n```\n\nExplanation of the steps\n1. The nested list is cast to a NumPy array for easy vectorised operations.\n2. `mean(axis=0)` computes the average probability vector for each sample across all estimators.\n3. `np.round(..., 4)` performs rounding to four decimal places.\n4. `np.argmax` (after rounding) selects the index of the maximum probability per sample, breaking ties in favour of the lowest index.\n5. The results are converted back to plain Python lists to satisfy the output constraints."}
{"task_id": 441, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a 1-D Hamming window.\n    \n    Parameters\n    ----------\n    window_len : int\n        Number of samples that must be returned.\n    symmetric : bool, optional\n        If True  \u2013 return the symmetric window (for FIR design).\n        If False \u2013 return the periodic window (for FFT analysis).\n\n    Returns\n    -------\n    list[float]\n        The requested window as ordinary Python floats rounded\n        to 4 decimal places.\n    \"\"\"\n    # ---- special-case handling -------------------------------------------\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]          # a single point is always 1\n    \n    # ---- choose the length to *compute* ----------------------------------\n    # For the periodic variant we create one extra point and drop it later\n    N = window_len if symmetric else window_len + 1\n    \n    # ---- generate the symmetric Hamming window of length N ---------------\n    n = np.arange(N, dtype=float)\n    w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n    \n    # ---- make it periodic if requested -----------------------------------\n    if not symmetric:\n        w = w[:-1]            # discard the last sample\n    \n    # ---- round to 4 decimals and convert to a plain Python list ----------\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 442, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport heapq\n\n\ndef knn_kdtree(X_train: np.ndarray, y_train: np.ndarray,\n               X_test: np.ndarray, k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbours classification accelerated by a (hand-written) KD-Tree.\"\"\"\n    # -------------------------------------------------------------------------\n    # 1. KD-TREE CONSTRUCTION\n    # -------------------------------------------------------------------------\n    n_train, n_features = X_train.shape\n    indices_all = np.arange(n_train)\n\n    def build(node_indices: np.ndarray):\n        \"\"\"\n        Recursively builds a balanced KD-Tree.\n\n        Every inner node stores:\n            (split_dim, split_val, point_index, left_child, right_child)\n        A leaf is represented by None.\n        \"\"\"\n        if node_indices.size == 0:\n            return None\n        # choose split dimension = one with largest variance\n        pts = X_train[node_indices]\n        split_dim = np.argmax(pts.var(axis=0))\n        # split by median\n        order = np.argsort(pts[:, split_dim])\n        ordered_idx = node_indices[order]\n        mid = ordered_idx.size // 2\n        point_idx = ordered_idx[mid]                # point kept in this node\n        left_child = build(ordered_idx[:mid])\n        right_child = build(ordered_idx[mid + 1:])\n        split_val = X_train[point_idx, split_dim]    # hyper-plane value\n        return (split_dim, split_val, point_idx, left_child, right_child)\n\n    root = build(indices_all)\n\n    # -------------------------------------------------------------------------\n    # 2. QUERY \u2011 k NEAREST NEIGHBOURS\n    # -------------------------------------------------------------------------\n    predictions: list[int] = []\n\n    def knn_search(node, query_point, heap):\n        \"\"\"\n        Depth-first search with back-tracking & pruning.\n\n        'heap' is a max-heap (implemented as a min-heap of negative distances)\n        that stores at most k entries (-dist_sq, index).\n        \"\"\"\n        if node is None:\n            return\n        split_dim, split_val, p_idx, left, right = node\n\n        # Decide which branch to visit first\n        if query_point[split_dim] <= split_val:\n            near, far = left, right\n        else:\n            near, far = right, left\n\n        # Go down the nearer side\n        knn_search(near, query_point, heap)\n\n        # Visit current node (its stored point)\n        dist_sq = float(np.sum((query_point - X_train[p_idx]) ** 2))\n        heapq.heappush(heap, (-dist_sq, p_idx))\n        if len(heap) > k:               # keep only k closest\n            heapq.heappop(heap)\n\n        # Is it necessary to examine the far side?\n        worst_sq = -heap[0][0] if len(heap) == k else float(\"inf\")\n        plane_dist_sq = (query_point[split_dim] - split_val) ** 2\n        if plane_dist_sq < worst_sq:\n            knn_search(far, query_point, heap)\n\n    # -------------------------------------------------------------------------\n    # 3. CLASSIFICATION OF ALL TEST SAMPLES\n    # -------------------------------------------------------------------------\n    for q in X_test:\n        heap: list[tuple[float, int]] = []    # empty heap for this query\n        knn_search(root, q, heap)\n\n        # majority vote\n        neighbour_indices = [idx for _, idx in heap]\n        vote = np.argmax(np.bincount(y_train[neighbour_indices]))\n        predictions.append(int(vote))\n\n    return predictions\n```"}
{"task_id": 443, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Number\n\ndef _is_numeric(value) -> bool:\n    \"\"\"\n    Decide whether *value* should be treated as \u201cnumeric\u201d **for this task**.\n    Booleans are deliberately excluded although they are subclasses of int.\n    \"\"\"\n    return isinstance(value, Number) and not isinstance(value, bool)\n\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold):\n    \"\"\"Split *X* into two NumPy matrices according to feature *feature_i* and *threshold*.\n\n    A row is sent to the *left* subset iff\n        \u2022 threshold is numeric  ->  X[row, feature_i] >= threshold\n        \u2022 threshold is non-numeric -> X[row, feature_i] == threshold\n\n    Args:\n        X: 2-D NumPy array. Each row is a sample, each column a feature.\n        feature_i: Index of the column that is tested.\n        threshold: Value that determines the split (numeric or categorical).\n\n    Returns:\n        (left_subset, right_subset): tuple of NumPy arrays with the same\n        number of columns as *X*.\n    \"\"\"\n    # Ensure input is a NumPy array\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D NumPy array\")\n\n    col = X[:, feature_i]\n\n    if _is_numeric(threshold):\n        mask = col >= threshold\n    else:\n        mask = col == threshold\n\n    left_subset  = X[mask]\n    right_subset = X[~mask]\n\n    # Shapes are automatically correct; guarantee 2-D even when empty.\n    if left_subset.size == 0:\n        left_subset = left_subset.reshape(0, X.shape[1])\n    if right_subset.size == 0:\n        right_subset = right_subset.reshape(0, X.shape[1])\n\n    return left_subset, right_subset\n```"}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications given in the task description.\n    It returns -1 on invalid input, otherwise a nested list containing the\n    kernel matrix rounded to four decimal places.\n    \"\"\"\n    # ------------------ helper: convert input to proper 2-D numpy array -------\n    def _to_2d_array(mat):\n        try:\n            arr = np.asarray(mat, dtype=float)\n        except Exception:\n            return None\n        # Reject ragged or 1-D structures\n        if arr.ndim != 2 or arr.shape[1] == 0:\n            return None\n        return arr\n\n    # ------------------ convert / validate X ----------------------------------\n    X_arr = _to_2d_array(X)\n    if X_arr is None:\n        return -1\n    n_samples, n_features = X_arr.shape\n\n    # ------------------ convert / validate Y ----------------------------------\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = _to_2d_array(Y)\n        if Y_arr is None or Y_arr.shape[1] != n_features:\n            return -1\n\n    # ------------------ handle / validate sigma ------------------------------\n    if sigma is None:                                 # default\n        sigma_scalar = np.sqrt(n_features / 2.0)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None                               # isotropic\n    elif isinstance(sigma, (int, float, np.floating)):  # scalar provided\n        sigma_scalar = float(sigma)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None                               # isotropic\n    else:                                              # list / array\n        try:\n            sigma_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return -1\n        if sigma_vec.ndim != 1 or sigma_vec.shape[0] != n_features:\n            return -1\n        if np.any(sigma_vec <= 0):\n            return -1\n        sigma_scalar = None                            # anisotropic\n\n    # ------------------ compute the kernel matrix ----------------------------\n    if sigma_vec is None:  # isotropic case\n        diff = X_arr[:, None, :] - Y_arr[None, :, :]       # (N, M, C)\n        dist2 = np.sum(diff ** 2, axis=-1) / (sigma_scalar ** 2)\n    else:               # anisotropic case\n        diff = (X_arr[:, None, :] - Y_arr[None, :, :]) / sigma_vec\n        dist2 = np.sum(diff ** 2, axis=-1)\n\n    K = np.exp(-0.5 * dist2)\n\n    # ------------------ round & return as nested Python list -----------------\n    return np.round(K, 4).tolist()\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best 1-level split (decision stump) by information gain.\"\"\"\n    # ------------------------------------------------------------------\n    # Basic preparation\n    # ------------------------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n\n    # Map the labels to 0, \u2026 , (n_classes-1) so that we can use bincount\n    classes, y_indices = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n\n    # ------------------------------------------------------------------\n    # Helper functions\n    # ------------------------------------------------------------------\n    def entropy_from_counts(counts: np.ndarray) -> float:\n        \"\"\"Shannon entropy (base 2) from a 1-D array with class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts[counts > 0] / total\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(counts: np.ndarray) -> int:\n        \"\"\"Most frequent label, ties resolved by smaller label value.\"\"\"\n        max_count = counts.max()\n        # classes are already sorted, so np.where returns indices in order\n        winner_idx = np.where(counts == max_count)[0][0]\n        return int(classes[winner_idx])\n\n    # ------------------------------------------------------------------\n    # Compute parent entropy and check the trivial case\n    # ------------------------------------------------------------------\n    total_counts = np.bincount(y_indices, minlength=n_classes)\n    parent_entropy = entropy_from_counts(total_counts)\n\n    if parent_entropy == 0.0:                     # all samples same label\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n\n    # ------------------------------------------------------------------\n    # Search the best split\n    # ------------------------------------------------------------------\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n    eps = 1e-12                                   # tolerance for float ties\n\n    for j in range(n_features):\n        col = X[:, j]\n        order = np.argsort(col)                   # sort by feature value\n        sorted_vals = col[order]\n        sorted_y_idx = y_indices[order]\n\n        # cumulative counts of classes for the left partition\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # iterate over possible split points (between two distinct values)\n        for i in range(n_samples - 1):\n            # move sample i to the left side\n            left_counts[sorted_y_idx[i]] += 1\n\n            # if the next value is identical, no threshold here\n            if sorted_vals[i] == sorted_vals[i + 1]:\n                continue\n\n            # candidate threshold: midpoint between the two consecutive values\n            thr = (sorted_vals[i] + sorted_vals[i + 1]) / 2.0\n\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n\n            right_counts = total_counts - left_counts\n\n            # entropies\n            h_left = entropy_from_counts(left_counts)\n            h_right = entropy_from_counts(right_counts)\n\n            p_left = n_left / n_samples\n            p_right = 1.0 - p_left\n\n            gain = parent_entropy - p_left * h_left - p_right * h_right\n\n            # ------------------------------------------------------------------\n            # Update the best split following the tie-breaking rules\n            # ------------------------------------------------------------------\n            better_gain = gain > best_gain + eps\n            same_gain = abs(gain - best_gain) <= eps\n\n            update = False\n            if better_gain:\n                update = True\n            elif same_gain:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and (best_threshold is None or thr < best_threshold - eps):\n                    update = True\n\n            if update:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_label = majority_label(left_counts)\n                best_right_label = majority_label(right_counts)\n\n    # ------------------------------------------------------------------\n    # Final answer: did we actually improve the information gain?\n    # ------------------------------------------------------------------\n    if best_gain <= eps:                          # no improvement found\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n\n    return (\n        best_feature,\n        round(best_threshold, 4),\n        best_left_label,\n        best_right_label,\n    )\n```"}
{"task_id": 447, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one(shape, **kwargs):\n    \"\"\"Create a nested Python list filled with ones.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired shape of the output list. Works exactly as the first argument of\n        ``numpy.ones``.\n    **kwargs : dict\n        Optional keyword arguments forwarded to ``numpy.ones`` (e.g. ``dtype``).\n\n    Returns\n    -------\n    list\n        A (possibly nested) Python list containing only ones that mirrors the\n        provided shape.\n    \"\"\"\n    return np.ones(shape, **kwargs).tolist()\n```"}
{"task_id": 448, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples_train, n_features_train).\n        (Only its length is relevant for this baseline.)\n    y_train : np.ndarray\n        One-dimensional array of training targets of length n_samples_train.\n    X_test : np.ndarray\n        Test feature matrix of shape (n_samples_test, n_features_test).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every row in ``X_test`` rounded to 4 decimal places.\n        If ``y_train`` is empty, an empty list is returned.\n    \"\"\"\n    # Convert to NumPy array in case a different sequence type is passed\n    y_train = np.asarray(y_train)\n\n    # Special rule: nothing to learn from, so return empty list\n    if y_train.size == 0:\n        return []\n\n    # 1. Compute the arithmetic mean of the training targets\n    mean_val = np.mean(y_train)\n\n    # 2. Create a vector filled with that mean (one entry per test sample)\n    preds = np.full(X_test.shape[0], mean_val, dtype=float)\n\n    # 3. Round to the nearest 4th decimal\n    preds = np.round(preds, 4)\n\n    # 4. Return as a plain Python list\n    return preds.tolist()\n```"}
{"task_id": 449, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Train a (simplified) Gaussian Discriminant Analysis model and predict labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary training labels (0/1), length n_samples.\n    X_test : np.ndarray\n        Test samples, shape (k_samples, n_features).\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for every row in `X_test`.\n    \"\"\"\n    # --- learning ------------------------------------------------------------\n    X0, X1 = X_train[y_train == 0], X_train[y_train == 1]\n\n    # Class means\n    Mu0 = X0.mean(axis=0)\n    Mu1 = X1.mean(axis=0)\n\n    # (Optional \u2013 not used later, included for faithfulness to the rule)\n    X_sub_Mu = np.vstack([X0 - Mu0, X1 - Mu1])\n    m = X_train.shape[1]                                # n_features\n    Sigma = (1.0 / m) * (X_sub_Mu.T @ X_sub_Mu)         # noqa: F841  (unused)\n\n    # Normal vector of the separating hyper-plane (unit length)\n    normal_vec = Mu1 - Mu0\n    norm = np.linalg.norm(normal_vec)\n    # If the means coincide, we keep w = 0 (all predictions -> 0)\n    w = normal_vec / norm if norm != 0 else np.zeros_like(normal_vec)\n\n    b = -np.dot(w.T, (Mu0 + Mu1) / 2.0)\n\n    # Determine on which half-space class-1 mean lies\n    sign = int(np.dot(w.T, Mu1) + b > 0)\n\n    # --- prediction ----------------------------------------------------------\n    scores = X_test @ w + b\n    preds = (scores > 0).astype(int) * sign            # apply the sign convention\n\n    return preds.astype(int).tolist()\n```"}
{"task_id": 450, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(preds: np.ndarray) -> list[int]:\n    \"\"\"Aggregate ensemble predictions using majority voting.\n\n    Parameters\n    ----------\n    preds : np.ndarray\n        A 2-D NumPy array of shape (n_samples, n_estimators) whose entries are\n        *non-negative integer* class labels predicted by the individual\n        estimators.\n\n    Returns\n    -------\n    list[int]\n        The chosen class label for every sample.  In case of a tie the smallest\n        label is returned (NumPy\u2019s argmax behaviour).\n    \"\"\"\n    if preds.ndim != 2:\n        raise ValueError(\"`preds` must be a 2-D array of shape \"\n                         \"(n_samples, n_estimators).\")\n\n    # For every sample (row) count occurrences of each label and pick\n    # the label with the highest count (argmax -> smallest label wins ties).\n    winners = [int(np.bincount(row).argmax()) for row in preds]\n\n    return winners\n```"}
{"task_id": 451, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\nfrom collections import Counter\nfrom typing import Optional, List\nimport numpy as np\n\n\ndef knn_kdtree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    k: int,\n) -> list[int]:\n    \"\"\"Classify *X_test* with a KD-Tree based exact k-NN search (Euclidean).\n\n    When two or more labels are equally frequent among the *k* neighbours,\n    the smallest label is returned.\n\n    If *X_train* is empty or *k* \u2264 0 an empty list is returned.\n    \"\"\"\n\n    # ------------------------------------------------------------------ helpers\n    class _KDNode:\n        \"\"\"Light-weight KD-Tree node.\"\"\"\n        __slots__ = (\"point\", \"label\", \"axis\", \"left\", \"right\")\n\n        def __init__(\n            self,\n            point: np.ndarray,\n            label: int,\n            axis: int,\n            left: Optional[\"._KDNode\"],\n            right: Optional[\"._KDNode\"],\n        ):\n            self.point = point\n            self.label = label\n            self.axis = axis\n            self.left = left\n            self.right = right\n\n    def _build(points: np.ndarray, labels: np.ndarray, depth: int = 0) -> Optional[_KDNode]:\n        \"\"\"Recursively build a balanced KD-Tree.\"\"\"\n        if points.shape[0] == 0:\n            return None\n        axis = depth % points.shape[1]\n        # median split\n        idx = np.argsort(points[:, axis])\n        mid = len(idx) // 2\n        return _KDNode(\n            point=points[idx[mid]],\n            label=int(labels[idx[mid]]),\n            axis=axis,\n            left=_build(points[idx[:mid]], labels[idx[:mid]], depth + 1),\n            right=_build(points[idx[mid + 1 :]], labels[idx[mid + 1 :]], depth + 1),\n        )\n\n    def _knn_search(node: Optional[_KDNode], query: np.ndarray, heap: List[tuple]):\n        \"\"\"Fill *heap* with the k closest points to *query* found below *node*.\"\"\"\n        if node is None:\n            return\n\n        # determine which branch to visit first\n        diff = query[node.axis] - node.point[node.axis]\n        near_branch, far_branch = (node.left, node.right) if diff < 0 else (node.right, node.left)\n\n        # 1. explore the nearer side\n        _knn_search(near_branch, query, heap)\n\n        # 2. process current node\n        dist_sq = float(np.sum((node.point - query) ** 2))\n        if len(heap) < k:\n            heapq.heappush(heap, (-dist_sq, node.label))\n        elif dist_sq < -heap[0][0]:  # closer than the worst so far\n            heapq.heapreplace(heap, (-dist_sq, node.label))\n\n        # 3. explore the farther side if necessary\n        #    (either not enough neighbours collected yet or hypersphere crosses plane)\n        if len(heap) < k or diff * diff < -heap[0][0]:\n            _knn_search(far_branch, query, heap)\n\n    # ------------------------------------------------------------------- guard\n    if k <= 0 or X_train.size == 0:\n        return []\n\n    # make sure we are working with numpy arrays\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n\n    # clip k to #training points, avoids unnecessary work in queries\n    k = min(k, X_train.shape[0])\n\n    # build the KD-Tree\n    root = _build(X_train, y_train)\n\n    # ----------------------------------------------------------------- queries\n    predictions: list[int] = []\n    for query in X_test:\n        heap: list[tuple[float, int]] = []  # (\u2212dist\u00b2, label)\n        _knn_search(root, query, heap)\n\n        # vote\n        labels = [lbl for _, lbl in heap]\n        counts = Counter(labels)\n        max_votes = max(counts.values())\n        # smallest label among those with maximal votes\n        chosen = min(lbl for lbl, cnt in counts.items() if cnt == max_votes)\n        predictions.append(chosen)\n\n    return predictions\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numbers\nimport numpy as np\nfrom typing import List\n\n\ndef divide_on_feature(\n    X: np.ndarray | List[list], feature_i: int, threshold\n) -> List[list]:\n    \"\"\"Split *X* into two subsets according to one feature and a threshold.\n\n    For a numeric *threshold* (int / float) the first subset contains every\n    sample whose value in column *feature_i* is >= threshold, otherwise the\n    first subset contains every sample whose value equals *threshold*.\n\n    The second subset gets all remaining samples.  The original row order is\n    preserved and both subsets are returned as regular Python lists.\n    \"\"\"\n    # --------------------------------------------------------------------- #\n    # Decide whether we are dealing with a numeric split (int/float) or not\n    # (exclude bool which is a subclass of int):\n    is_numeric = isinstance(threshold, numbers.Real) and not isinstance(\n        threshold, bool\n    )\n\n    left, right = [], []  # left == first subset, right == second subset\n\n    # Iterate through the samples in *X* (works for both ndarray and list):\n    for row in X:\n        value = row[feature_i]\n\n        if is_numeric:\n            goes_left = value >= threshold\n        else:\n            goes_left = value == threshold\n\n        (left if goes_left else right).append(row)\n\n    # Convert the subsets to regular Python lists (as requested)\n    left_out = np.asarray(left, dtype=object).tolist() if left else []\n    right_out = np.asarray(right, dtype=object).tolist() if right else []\n\n    return [left_out, right_out]\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch (see doc-string above).\"\"\"\n    # --------  basic validation  ------------------------------------------------\n    # gamma within [0, 1]\n    if not (0.0 <= gamma <= 1.0):\n        return -1\n\n    # Q and Q_next must be 2-D numpy arrays of identical shape\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.shape != Q_next.shape or Q.ndim != 2:\n        return -1\n\n    batch_size, n_actions = Q.shape\n\n    # actions, rewards, dones lengths must match batch size\n    if not (len(actions) == len(rewards) == len(dones) == batch_size):\n        return -1\n\n    # check every action index is valid\n    if any(a < 0 or a >= n_actions for a in actions):\n        return -1\n\n    # --------  target computation  ---------------------------------------------\n    # copy because we must not overwrite the original array\n    targets = Q.copy()\n\n    # compute target for every sample and replace only the chosen action\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target\n\n    # --------  rounding & conversion to plain python list  ---------------------\n    targets = np.round(targets, 4).tolist()\n    return targets\n```"}
{"task_id": 454, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_dcgan_labels(batch_size: int) -> tuple[list[list[int]], list[list[int]], list[list[int]]]:\n    \"\"\"Generate the fixed one-hot label arrays used in DCGAN training.\n\n    The function returns (valid_half, fake_half, valid_full), each\n    converted to a plain Python list via ``tolist()``.\n\n    Args:\n        batch_size: A positive, even integer indicating the total number\n            of samples in one training batch.\n\n    Returns:\n        Tuple containing three Python lists with one-hot vectors.\n    \"\"\"\n    # Half-batch size (guaranteed to be an integer because `batch_size` is even)\n    half = batch_size // 2\n\n    # Create label arrays\n    valid_half = np.tile([1, 0], (half, 1))\n    fake_half  = np.tile([0, 1], (half, 1))\n    valid_full = np.tile([1, 0], (batch_size, 1))\n\n    # Convert to Python lists as requested\n    return valid_half.tolist(), fake_half.tolist(), valid_full.tolist()\n```"}
{"task_id": 455, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\nimport numpy as np\nfrom collections import Counter, namedtuple\n\n\n# ----------------------------------------------------------------------\n# KD-Tree helpers\n# ----------------------------------------------------------------------\n_KDNode = namedtuple(\n    \"_KDNode\", [\"point\", \"label\", \"axis\", \"left\", \"right\"]\n)\n\n\ndef _build_kdtree(points, labels, depth=0):\n    \"\"\"Returns root node of a balanced KD-Tree built from the data\"\"\"\n    if len(points) == 0:\n        return None\n\n    n_features = points.shape[1]\n    axis = depth % n_features            # cycle through axes\n    # median split (argpartition runs in linear time)\n    median_idx = len(points) // 2\n    idx = np.argpartition(points[:, axis], median_idx)\n    idx_med = idx[median_idx]\n\n    # pivot\n    point = points[idx_med]\n    label = labels[idx_med]\n\n    # recursively build children\n    left = _build_kdtree(points[idx[:median_idx]],\n                         labels[idx[:median_idx]],\n                         depth + 1)\n    right = _build_kdtree(points[idx[median_idx + 1:]],\n                          labels[idx[median_idx + 1:]],\n                          depth + 1)\n    return _KDNode(point, label, axis, left, right)\n\n\ndef _kdtree_search(node, query, k, heap):\n    \"\"\"Best-first traversal that keeps the k closest points in *heap*.\n\n    *heap* is a max-heap implemented through a min-heap that stores\n    (-distance, label).  The root of the heap is therefore the farthest\n    neighbour currently kept.\n    \"\"\"\n    if node is None:\n        return\n\n    # --- visit current node ------------------------------------------------\n    dist = float(np.sum((node.point - query) ** 2))  # squared Euclidean\n    if len(heap) < k:\n        heapq.heappush(heap, (-dist, node.label))\n    elif dist < -heap[0][0]:\n        heapq.heapreplace(heap, (-dist, node.label))\n\n    # --- determine which branch to visit first ----------------------------\n    axis = node.axis\n    diff = query[axis] - node.point[axis]\n\n    first, second = (node.left, node.right) if diff < 0 else (node.right,\n                                                              node.left)\n    _kdtree_search(first, query, k, heap)\n\n    # could the other branch still contain a closer point?\n    if len(heap) < k or diff * diff < -heap[0][0]:\n        _kdtree_search(second, query, k, heap)\n\n\n# ----------------------------------------------------------------------\n# Majority vote helper\n# ----------------------------------------------------------------------\ndef _vote(labels):\n    \"\"\"Return the majority label; tie is resolved in favour of the\n    numerically smallest label.\"\"\"\n    counts = Counter(labels)\n    max_count = max(counts.values())\n    winners = [lab for lab, c in counts.items() if c == max_count]\n    return min(winners)\n\n\n# ----------------------------------------------------------------------\n# Public API\n# ----------------------------------------------------------------------\ndef knn_classifier(X_train, y_train, X_query, k, method=\"naive\"):\n    \"\"\"k-Nearest Neighbours classifier supporting three search strategies.\n\n    Parameters\n    ----------\n    X_train : (n_samples, n_features) array_like\n    y_train : (n_samples,) array_like of int\n    X_query : (n_queries, n_features) array_like\n    k       : int\n    method  : {\"naive\", \"heap\", \"kdtree\"}\n\n    Returns\n    -------\n    list[int] \u2013 predicted labels for the queries\n    \"\"\"\n    # --- sanitise inputs ---------------------------------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n\n    n_train, n_feat = X_train.shape\n    if k <= 0:\n        raise ValueError(\"k must be >= 1\")\n    k_eff = min(k, n_train)          # cannot take more neighbours than points\n\n    preds = []\n\n    # ----------------------------------------------------------------------\n    # Strategy 1 \u2013 Na\u00efve linear scan\n    # ----------------------------------------------------------------------\n    if method == \"naive\":\n        for q in X_query:\n            # squared distances, no sqrt needed\n            dists = np.sum((X_train - q) ** 2, axis=1)\n            idx = np.argsort(dists)[:k_eff]\n            preds.append(_vote(y_train[idx]))\n\n    # ----------------------------------------------------------------------\n    # Strategy 2 \u2013 Heap (keep only k smallest distances)\n    # ----------------------------------------------------------------------\n    elif method == \"heap\":\n        for q in X_query:\n            heap = []                           # max-heap via negative distances\n            # iterate once over all training samples\n            for xi, yi in zip(X_train, y_train):\n                d = float(np.sum((xi - q) ** 2))\n                if len(heap) < k_eff:\n                    heapq.heappush(heap, (-d, yi))\n                elif d < -heap[0][0]:\n                    heapq.heapreplace(heap, (-d, yi))\n            preds.append(_vote([lab for _, lab in heap]))\n\n    # ----------------------------------------------------------------------\n    # Strategy 3 \u2013 KD-Tree\n    # ----------------------------------------------------------------------\n    elif method == \"kdtree\":\n        # build tree once\n        root = _build_kdtree(X_train, y_train)\n        for q in X_query:\n            heap = []\n            _kdtree_search(root, q, k_eff, heap)\n            preds.append(_vote([lab for _, lab in heap]))\n    else:\n        raise ValueError(\"Unknown method: choose 'naive', 'heap' or 'kdtree'.\")\n\n    return preds\n```"}
{"task_id": 456, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the unbiased sample covariance (or cross-covariance) matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        (n_samples \u00d7 n_features) data matrix.\n    Y : np.ndarray | None, optional\n        (n_samples \u00d7 m_features) data matrix. If None, the covariance\n        of X with itself is returned.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Covariance matrix rounded to 4 decimals, or -1 for invalid input.\n    \"\"\"\n    # Ensure input is an ndarray\n    X = np.asarray(X)\n    n_samples = X.shape[0]\n\n    # Rule 1: need at least two observations\n    if n_samples < 2:\n        return -1\n\n    # If Y is not provided, use X\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y)\n        # Rule 2: Y must have the same number of rows\n        if Y.shape[0] != n_samples:\n            return -1\n\n    # Center the matrices\n    Xc = X - X.mean(axis=0, keepdims=True)\n    Yc = Y - Y.mean(axis=0, keepdims=True)\n\n    # Unbiased estimator: divide by (n - 1)\n    cov = (Xc.T @ Yc) / (n_samples - 1)\n\n    # Round to 4 decimals and convert to list of lists\n    return np.round(cov, 4).tolist()\n```"}
{"task_id": 457, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_penalty(weights, alpha):\n    \"\"\"Compute the Elastic-Net penalty for a set of weights.\n\n    Parameters\n    ----------\n    weights : list | tuple | np.ndarray\n        1-D collection of numerical weights.\n    alpha : float\n        Mixing parameter. Must satisfy 0 \u2264 alpha \u2264 1.  If outside this\n        range the function returns -1.\n\n    Returns\n    -------\n    float | int\n        Elastic-Net penalty rounded to 4 decimals, or -1 if *alpha* is\n        outside the valid interval.\n    \"\"\"\n    # 1. Validate alpha\n    if not (0.0 <= alpha <= 1.0):\n        return -1\n\n    # 2. Convert weights to numpy array (handles list, tuple, ndarray)\n    w = np.asarray(weights, dtype=float)\n\n    # 3. Compute L1 and L2 components\n    l2_term = 0.5 * alpha * np.sum(w ** 2)\n    l1_term = (1.0 - alpha) * np.sum(np.abs(w))\n\n    penalty = l2_term + l1_term\n\n    # 4. Round to 4 decimal places and return\n    return float(round(penalty, 4))\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef to_categorical(\n    labels: list | tuple | \"np.ndarray\",\n    num_classes: int | None = None,\n) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists (shape: (len(labels), num_classes)) containing\n        0/1 integers, or -1 when:\n            \u2022 any label is negative, or\n            \u2022 `num_classes` is provided but smaller than `max(labels)+1`.\n    \"\"\"\n    # --- Step 1: make a flat Python list ---------------------------------\n    try:\n        labels_list = list(labels)\n    except TypeError:\n        return -1  # labels not iterable\n\n    if not labels_list:                                   # empty input\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes >= 0 else -1\n\n    # --- Step 2: basic validation ----------------------------------------\n    max_label = -1\n    for lbl in labels_list:\n        # Accept Python int or NumPy integer (but reject float/bool/other)\n        if not isinstance(lbl, (int, np.integer)) or lbl < 0:\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n\n    # --- Step 3: determine num_classes -----------------------------------\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n\n    # --- Step 4: build one-hot matrix ------------------------------------\n    one_hot: list[list[int]] = [\n        [1 if class_idx == lbl else 0 for class_idx in range(num_classes)]\n        for lbl in labels_list\n    ]\n    return one_hot\n```"}
{"task_id": 459, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport heapq\n\n\ndef knn_validate_heap(X_train: \"np.ndarray\",\n                      Y_train: \"np.ndarray\",\n                      X_val: \"np.ndarray\",\n                      Y_val: \"np.ndarray\",\n                      k: int) -> tuple[float, list[int]]:\n    \"\"\"Validate a k-NN classifier using a heap-based neighbour selection.\"\"\"\n    predicted = []\n\n    # Pre\u2013cast once for speed\n    Y_train = np.asarray(Y_train, dtype=int)\n    Y_val   = np.asarray(Y_val,   dtype=int)\n\n    for x in X_val:\n        # ---- 1. distances --------------------------------------------------\n        diff   = X_train - x\n        dists  = np.einsum('ij,ij->i', diff, diff)          # squared Eucl. distances\n\n        # ---- 2. keep k closest via max-heap -------------------------------\n        heap: list[tuple[float, int]] = []                  # (-dist, idx)\n        for idx, dist in enumerate(dists):\n            nd = -dist                                       # negate for max-heap\n            if len(heap) < k:\n                heapq.heappush(heap, (nd, idx))\n            elif nd > heap[0][0]:                            # new dist < current worst\n                heapq.heapreplace(heap, (nd, idx))\n\n        # ---- 3. majority vote --------------------------------------------\n        neighbour_indices = [idx for _, idx in heap]\n        votes             = Y_train[neighbour_indices]\n        pred_label        = int(np.bincount(votes).argmax())\n        predicted.append(pred_label)\n\n    # ---- 4. accuracy -------------------------------------------------------\n    accuracy = round((np.array(predicted) == Y_val).mean(), 4)\n    return accuracy, predicted\n```"}
{"task_id": 460, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list | int:\n    \"\"\"Compute the Pearson correlation matrix between the columns of *X* and *Y*.\n\n    If *Y* is omitted the auto-correlation \u03c1(X,X) is returned.\n    If any column of *X* or *Y* has a population standard deviation of zero\n    the correlation is undefined and the function returns -1.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        (n_samples, n_features_X) data matrix.\n    Y : np.ndarray | None, optional\n        (n_samples, n_features_Y) data matrix; must have the same number of\n        rows as *X* when supplied.\n\n    Returns\n    -------\n    list | int\n        Correlation matrix (Python list of lists, rounded to 4 decimals)\n        or -1 if the computation is impossible.\n    \"\"\"\n    # ---- basic input checks -------------------------------------------------\n    if X.ndim != 2:\n        raise ValueError(\"X must be two-dimensional\")\n    n_samples = X.shape[0]\n\n    if Y is None:\n        Y = X\n    else:\n        if Y.ndim != 2:\n            raise ValueError(\"Y must be two-dimensional\")\n        if Y.shape[0] != n_samples:\n            raise ValueError(\"X and Y must have the same number of rows\")\n\n    # ---- centre the data ----------------------------------------------------\n    Xc = X - np.mean(X, axis=0, keepdims=True)\n    Yc = Y - np.mean(Y, axis=0, keepdims=True)\n\n    # ---- population standard deviations -------------------------------------\n    std_X = np.sqrt(np.mean(Xc ** 2, axis=0))        # shape (p,)\n    std_Y = np.sqrt(np.mean(Yc ** 2, axis=0))        # shape (q,)\n\n    # any zero standard deviation \u2192 undefined correlation\n    if np.any(std_X == 0) or np.any(std_Y == 0):\n        return -1\n\n    # ---- covariance matrix ---------------------------------------------------\n    cov = (Xc.T @ Yc) / n_samples                    # shape (p, q)\n\n    # ---- correlation matrix --------------------------------------------------\n    corr = cov / (std_X[:, None] * std_Y[None, :])   # broadcasting\n\n    # ---- round & convert to pure python --------------------------------------\n    corr_rounded = np.round(corr, 4).tolist()\n    return corr_rounded\n```"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    # convert to numpy once\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # constant prediction if no boosting iterations requested\n    if n_estimators <= 0:\n        return np.round(np.full_like(y_arr, y_arr.mean()), 4).tolist()\n\n    # initial prediction: mean of targets\n    y_pred = np.full_like(y_arr, y_arr.mean(), dtype=float)\n\n    # pre-compute unique feature values, sorted\n    uniq_x = np.unique(x_arr)\n\n    for _ in range(n_estimators):\n        # residuals\n        residuals = y_arr - y_pred\n        if np.allclose(residuals, 0.0):\n            break                                    # perfect fit\n\n        # ---- search for best split -------------------------------------------------\n        best_sse = np.inf\n        best_t = None\n        best_left_mean = best_right_mean = 0.0\n\n        # all possible thresholds: all but the largest unique x\n        thresholds = uniq_x[:-1]\n        for t in thresholds:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue                            # invalid split, skip\n\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n\n            m_left, m_right = r_left.mean(), r_right.mean()\n            sse = np.sum((r_left - m_left) ** 2) + np.sum((r_right - m_right) ** 2)\n\n            if sse < best_sse:                      # first minimum kept if tie\n                best_sse = sse\n                best_t = t\n                best_left_mean, best_right_mean = m_left, m_right\n\n        # ---- build stump prediction ------------------------------------------------\n        if best_t is None:                          # no valid split\n            stump_pred = np.full_like(residuals, residuals.mean())\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_left_mean, best_right_mean)\n\n        # ---- update ensemble prediction --------------------------------------------\n        y_pred += learning_rate * stump_pred\n\n    # final rounded predictions\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 462, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef cepstral_lifter(mfccs: np.ndarray, D: int) -> np.ndarray:\n    \"\"\"Apply a sinusoidal cepstral lifter to a matrix of MFCC coefficients.\n\n    Args:\n        mfccs: NumPy array of shape (G, C) containing MFCC coefficients.\n        D: Non-negative integer lifter parameter.  \n           A value of 0 or 1 returns the input unchanged.\n\n    Returns:\n        NumPy array with the same shape and dtype as *mfccs* containing the\n        liftered coefficients.\n    \"\"\"\n    # ----- basic argument checks ------------------------------------------------\n    if D < 0:\n        raise ValueError(\"D has to be a non-negative integer.\")\n    if mfccs.ndim != 2:\n        raise ValueError(\"mfccs must be a 2-D array of shape (G, C).\")\n\n    # For D == 0 or 1 the lifter reduces to 1, so just return a copy\n    if D <= 1:\n        return mfccs.copy()\n\n    # ----- build lifter ---------------------------------------------------------\n    G, C = mfccs.shape\n    n = np.arange(C, dtype=mfccs.dtype)     # 0 \u2026 C-1, same dtype as input\n    lifter = 1.0 + (D / 2.0) * np.sin(np.pi * n / D)\n\n    # Ensure lifter has the same dtype as the input to preserve dtype after *\n    lifter = lifter.astype(mfccs.dtype, copy=False)\n\n    # ----- apply lifter with broadcasting --------------------------------------\n    # mfccs has shape (G, C), lifter has shape (C,) \u2192 broadcasting along rows\n    return mfccs * lifter\n```"}
{"task_id": 463, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List\n\ndef compute_ball_radius(centroid: List[float], X: List[List[float]]) -> float:\n    \"\"\"Return the Ball-tree node radius, i.e. the largest Euclidean distance\n    between *centroid* and any point in *X*, rounded to 4 decimals.\n\n    Parameters\n    ----------\n    centroid : List[float]\n        The node\u2019s centroid (a single point).\n    X : List[List[float]]\n        Points belonging to the node.\n\n    Returns\n    -------\n    float\n        The radius rounded to 4 decimal places. 0.0 if *X* is empty.\n    \"\"\"\n    if not X:                          # Empty node \u21d2 radius is 0\n        return 0.0\n\n    # Compute squared distances to avoid an extra sqrt inside the loop\n    max_sq_dist = 0.0\n    for point in X:\n        sq_dist = sum((c - p) ** 2 for c, p in zip(centroid, point))\n        if sq_dist > max_sq_dist:\n            max_sq_dist = sq_dist\n\n    radius = math.sqrt(max_sq_dist)\n    return round(radius, 4)\n```"}
{"task_id": 464, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_validate(X_train: np.ndarray,\n                 Y_train: np.ndarray,\n                 X_val: np.ndarray,\n                 Y_val: np.ndarray,\n                 k: int) -> tuple[float, list[int]]:\n    \"\"\"k-Nearest Neighbours (k-NN) validation helper.\"\"\"\n    # Ensure everything is a NumPy array of the right shape/dtype\n    X_train = np.asarray(X_train, dtype=float)\n    X_val   = np.asarray(X_val,   dtype=float)\n    Y_train = np.asarray(Y_train, dtype=int)\n    Y_val   = np.asarray(Y_val,   dtype=int)\n\n    n_val = X_val.shape[0]\n    predictions = []\n\n    # Compute the full (squared) Euclidean distance matrix:\n    #     dist[i, j] = || X_val[i] - X_train[j] ||\u00b2\n    # Shape: (n_val, n_train)\n    diff = X_val[:, None, :] - X_train[None, :, :]\n    dists = np.sum(diff ** 2, axis=2)\n\n    # For every validation sample, find its k nearest neighbours\n    for i in range(n_val):\n        # Indices of the k smallest distances\n        nn_idx = np.argpartition(dists[i], k)[:k]\n        nn_labels = Y_train[nn_idx]\n\n        # Majority vote (smallest label wins in a tie)\n        label_counts = np.bincount(nn_labels)\n        pred_label = int(np.argmax(label_counts))\n        predictions.append(pred_label)\n\n    # Classification accuracy\n    correct = np.sum(np.array(predictions) == Y_val)\n    accuracy = round(correct / n_val if n_val else 0.0, 4)\n\n    return accuracy, predictions\n```"}
{"task_id": 465, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef rbf_kernel(gamma: float, **kwargs):\n    \"\"\"Create and return a Gaussian RBF kernel function.\n\n    The returned function takes two 1-D vectors and computes the Gaussian\n    Radial Basis Function (RBF) similarity between them.\n\n    Args:\n        gamma: A non-negative real number that controls the width of the RBF\n               kernel. A larger gamma makes the kernel decay faster.\n        **kwargs: Optional keyword arguments kept for interface compatibility\n                   (not used in the computation).\n\n    Returns:\n        Function ``f(x1, x2)`` such that ``f`` returns the value of\n        ``exp( -gamma * ||x1 - x2||^2 )`` rounded to four decimal places.\n    \"\"\"\n    # ========  Write your code below ========\n    if gamma < 0:\n        raise ValueError(\"gamma must be non-negative\")\n\n    def f(x1, x2):\n        \"\"\"Gaussian RBF similarity between two 1-D vectors.\"\"\"\n        # Convert inputs to 1-D NumPy arrays\n        a = np.asarray(x1, dtype=float).ravel()\n        b = np.asarray(x2, dtype=float).ravel()\n\n        # Both vectors must have the same length\n        if a.shape != b.shape:\n            raise ValueError(\"Input vectors must have the same length\")\n\n        # When gamma == 0 the similarity is always 1\n        if gamma == 0:\n            return 1.0\n\n        diff = a - b\n        dist_sq = np.dot(diff, diff)          # ||x1 - x2||\u00b2\n        sim = np.exp(-gamma * dist_sq)        # RBF kernel\n        return round(float(sim), 4)           # Round to 4 decimal places\n\n    return f\n    # ========  End of your code ========\n```"}
{"task_id": 466, "completion_id": 0, "completion": "```python\nimport string\nimport string\n\n# Build the translation table once so it can be reused efficiently by every\n# call to ``strip_punctuation``.\n_PUNC_TABLE: dict[int, None] = str.maketrans('', '', string.punctuation)\n\ndef strip_punctuation(line: str) -> str:\n    \"\"\"Remove punctuation characters from a string.\n\n    The function deletes all characters listed in ``string.punctuation`` and\n    trims surrounding whitespace.\n\n    Args:\n        line (str): Text that may contain punctuation symbols.\n\n    Returns:\n        str: Text with punctuation removed and stripped of leading/trailing\n        whitespace.\n    \"\"\"\n    # Delete punctuation using translate, then strip outer whitespace.\n    return line.translate(_PUNC_TABLE).strip()\n```"}
{"task_id": 467, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef regularization(\n    weights: list[float | int],\n    C: float = 0.01,\n    norm: str = \"l2\",\n) -> tuple[float, list[float]] | int:\n    \"\"\"Return the regularisation penalty and gradient for a list of weights.\n\n    Parameters\n    ----------\n    weights : list[float | int]\n        Vector of model weights.\n    C : float, default 0.01\n        Regularisation strength.\n    norm : str, {\"l1\", \"l2\"}, default \"l2\"\n        Type of regularisation to apply.\n\n    Returns\n    -------\n    tuple[float, list[float]]\n        (penalty, gradient) \u2014 every value rounded to 4 decimals.\n        If *norm* is unsupported, returns -1.\n    \"\"\"\n    if norm not in {\"l1\", \"l2\"}:\n        return -1\n\n    w = np.asarray(weights, dtype=float)\n\n    if norm == \"l1\":\n        penalty = C * np.sum(np.abs(w))\n        grad = C * np.sign(w)\n    else:  # norm == \"l2\"\n        penalty = 0.5 * C * np.sum(w ** 2)\n        grad = C * w\n\n    # Round to the 4th decimal place\n    penalty = round(float(penalty), 4)\n    grad = [round(float(g), 4) for g in grad]\n\n    return penalty, grad\n```"}
{"task_id": 468, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iters: int = 10000) -> list[int]:\n    \"\"\"Train a binary logistic-regression model with gradient descent and\n    predict labels for the test set.\n\n    Args:\n        X_train: Training feature matrix of shape (m, n).\n        y_train: Training labels \u2013 either shape (m,) or (m, 1) containing 0/1.\n        X_test:  Test feature matrix of shape (k, n).\n        learning_rate: Step size used by gradient descent (default 0.1).\n        num_iters:      Number of optimisation iterations (default 10000).\n\n    Returns:\n        A list of length *k* holding the predicted class (0 or 1) for every\n        test sample, in order.\n    \"\"\"\n    # Ensure y is column-shaped (m, 1)\n    if y_train.ndim == 1:\n        y = y_train.reshape(-1, 1)\n    else:\n        y = y_train.copy()\n\n    m, n = X_train.shape\n\n    # Add bias column (of 1s) to training and test matrices\n    Xb_train = np.hstack((np.ones((m, 1)), X_train))\n    Xb_test  = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # Parameter vector (weights + bias) initialised to zeros\n    W = np.zeros((n + 1, 1))\n\n    # Sigmoid helper\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Batch gradient-descent optimisation\n    for _ in range(num_iters):\n        z = Xb_train @ W           # (m, 1)\n        h = sigmoid(z)             # (m, 1)\n        dW = (Xb_train.T @ (h - y)) / m\n        W -= learning_rate * dW\n\n    # Predict probabilities for test set, then threshold at 0.5\n    probs = sigmoid(Xb_test @ W)       # (k, 1)\n    preds = (probs >= 0.5).astype(int) # (k, 1)\n\n    return preds.ravel().tolist()\n```"}
{"task_id": 469, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (duplicate import kept intentionally, harmless)\n\ndef linear_kernel(**kwargs) -> callable:\n    \"\"\"Factory returning a linear-kernel (dot-product) callable.\n\n    Any supplied keyword arguments are ignored so that the signature stays\n    compatible with more elaborate kernel factories.\n    \"\"\"\n    def f(x1, x2):\n        \"\"\"\n        Parameters\n        ----------\n        x1, x2 : 1-D list-like or np.ndarray\n            Input feature vectors.\n\n        Returns\n        -------\n        float\n            Dot product of the two vectors, or \u20111 if their lengths differ.\n        \"\"\"\n        # Convert inputs to 1-D float ndarrays\n        x1_arr = np.asarray(x1, dtype=float).ravel()\n        x2_arr = np.asarray(x2, dtype=float).ravel()\n\n        # Length check\n        if x1_arr.size != x2_arr.size:\n            return -1\n\n        # Compute dot product and return a pure Python float\n        return float(np.inner(x1_arr, x2_arr))\n\n    return f\n```"}
{"task_id": 470, "completion_id": 0, "completion": "```python\nimport os\n\n\ndef get_scriptdir(file_path: str) -> str:\n    \"\"\"Return the absolute directory that contains *file_path*.\n\n    The function normalises *file_path* by resolving any symbolic links\n    and removing ``..`` / ``.`` path segments, then returns only the\n    directory component of this canonical path.\n\n    Args:\n        file_path: Path string pointing to a file.  May be absolute or\n                   relative and may contain symlinks or redundant path\n                   elements.\n\n    Returns:\n        The absolute directory path that contains *file_path*.  The\n        returned path never ends with a path separator (unless it is the\n        root directory).\n    \"\"\"\n    # 1. Convert to an absolute, canonical path (resolves symlinks, \"..\", \".\")\n    canonical_path = os.path.realpath(file_path)\n\n    # 2. Strip the final component \u2013 i.e. keep only the directory part\n    directory = os.path.dirname(canonical_path)\n\n    # 3. Normalise once more so that possible trailing separators\n    #    (other than those required for the root itself) disappear.\n    #    os.path.normpath never removes the separator from the root.\n    directory = os.path.normpath(directory)\n\n    return directory\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One-step tabular Expected-SARSA TD(0) update.\"\"\"\n    \n    n_actions = len(q_table[0])                 # |A|\n    \n    # ---------- compute the expectation term ----------\n    if next_state is None:                      # terminal transition\n        expected_q = 0.0\n    else:\n        next_row = q_table[next_state]\n        greedy_idx = max(range(n_actions), key=next_row.__getitem__)   # first max\n        \n        # \u03b5-soft probabilities\n        base_prob = epsilon / n_actions\n        expected_q = 0.0\n        for a in range(n_actions):\n            prob = base_prob\n            if a == greedy_idx:                 # greedy action gets extra mass\n                prob += (1.0 - epsilon)\n            expected_q += prob * next_row[a]\n    \n    # ---------- TD target ----------\n    target = reward + gamma * expected_q\n    \n    # ---------- Q update ----------\n    q_table[state][action] += lr * (target - q_table[state][action])\n    \n    # ---------- round everything to 4 decimals and return ----------\n    rounded_q = [[round(v, 4) for v in row] for row in q_table]\n    return rounded_q\n```"}
{"task_id": 472, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x: list | list[list]):\n    \"\"\"Return the softmax probabilities and their element-wise gradient.\n\n    Parameters\n    ----------\n    x : list | list[list]\n        A one- or two-dimensional numeric list.\n\n    Returns\n    -------\n    tuple[list, list]\n        (probabilities, gradient) \u2013 both with the same shape as *x*,\n        rounded to 4 decimal places.\n    \"\"\"\n    # convert the input to a NumPy array for vectorised math\n    arr = np.asarray(x, dtype=float)\n    \n    # subtract the maximum along the last axis for numerical stability\n    shifted = arr - arr.max(axis=-1, keepdims=True)\n    \n    exp_vals = np.exp(shifted)\n    probs = exp_vals / exp_vals.sum(axis=-1, keepdims=True)\n    \n    grad = probs * (1.0 - probs)          # element-wise (diagonal) gradient\n    \n    # round to 4 decimals and convert back to plain Python lists\n    probs_out = np.round(probs, 4).tolist()\n    grad_out  = np.round(grad, 4).tolist()\n    \n    return probs_out, grad_out\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\nimport numpy as np          # duplicated on purpose \u2013 harmless\nimport random               # duplicated on purpose \u2013 harmless\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray,\n                                                    np.ndarray,\n                                                    np.ndarray,\n                                                    np.ndarray]:\n    \"\"\"Generate a 2-D toy data set consisting of `m` rectangular clusters.\n\n    Each class i (0-based index) is located at\n        x \u2208 [(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710]\n        y \u2208 [((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710]\n    and is assigned the label  (i \u2212 0.5)\u00b72  (\u2192 \u22121, +1, 3, 5, \u2026).\n\n    All returned coordinates are rounded to 4 decimals.\n    \"\"\"\n    if m < 2:\n        raise ValueError(\"`m` (number of classes) must be at least 2.\")\n    if seed is not None:\n        # Make both the Python stdlib RNG and NumPy RNG deterministic\n        random.seed(seed)\n        np.random.seed(seed)\n\n    X_train_parts, X_val_parts = [], []\n    Y_train_parts, Y_val_parts = [], []\n\n    for i in range(m):\n        col = i // 2             # horizontal position (0, 1, 2, \u2026)\n        row = i % 2              # 0 for bottom row, 1 for top row\n\n        # Coordinate ranges\n        x_min, x_max = (col + 0.1) * 10, (col + 0.9) * 10\n        y_min, y_max = (row * interval + 0.1) * 10, (row * interval + 0.9) * 10\n\n        # Training samples\n        x_tr = np.random.uniform(x_min, x_max, n_train)\n        y_tr = np.random.uniform(y_min, y_max, n_train)\n        X_train_parts.append(np.column_stack((x_tr, y_tr)))\n\n        # Validation samples\n        x_va = np.random.uniform(x_min, x_max, n_val)\n        y_va = np.random.uniform(y_min, y_max, n_val)\n        X_val_parts.append(np.column_stack((x_va, y_va)))\n\n        # Labels\n        label = int((i - 0.5) * 2)         # \u22121, +1, 3, 5, \u2026\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n\n    # Concatenate parts for all classes\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val   = np.round(np.vstack(X_val_parts),   4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val   = np.concatenate(Y_val_parts)\n\n    return X_train, X_val, Y_train, Y_val\n```"}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t,\n                learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step (see doc-string above).\"\"\"\n    # Convert inputs to numpy arrays for uniform maths (scalars are fine too)\n    grad_arr = np.asarray(grad, dtype=float)\n\n    # Initialise moments if this is the first call\n    if m_prev is None:\n        m_prev = np.zeros_like(grad_arr, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad_arr, dtype=float)\n\n    # ---- 1. Update the (un-biased) first and second moments -------------\n    m_new = b1 * m_prev + (1.0 - b1) * grad_arr\n    v_new = b2 * v_prev + (1.0 - b2) * (grad_arr ** 2)\n\n    # ---- 2. Bias correction --------------------------------------------\n    m_hat = m_new / (1.0 - b1 ** t)\n    v_hat = v_new / (1.0 - b2 ** t)\n\n    # ---- 3. Parameter update -------------------------------------------\n    w_new = np.asarray(w, dtype=float) - learning_rate * m_hat / (\n        np.sqrt(v_hat) + eps\n    )\n\n    # Keep the same (scalar / array) \"shape type\" as the input parameter w\n    if np.isscalar(w):\n        w_new = w_new.item()          # back to Python float\n        m_new = m_new.item()\n        v_new = v_new.item()\n\n    return w_new, m_new, v_new\n```\n\nThe function exactly follows the Adam update equations:\n\n    m_t = \u03b2\u2081 m_{t\u22121} + (1\u2212\u03b2\u2081) grad\n    v_t = \u03b2\u2082 v_{t\u22121} + (1\u2212\u03b2\u2082) grad\u00b2\n    m\u0302_t = m_t / (1\u2212\u03b2\u2081\u1d57)\n    v\u0302_t = v_t / (1\u2212\u03b2\u2082\u1d57)\n    w_new = w \u2212 \u03b1 \u00b7 m\u0302_t / (\u221av\u0302_t + \u03b5)\n\nIt works for scalars as well as arbitrary-shaped NumPy arrays and honours the provided hyper-parameters."}
{"task_id": 477, "completion_id": 0, "completion": "```python\nimport heapq\nfrom typing import List, Tuple\n\n\ndef astar(grid: List[List[int]], start: Tuple[int, int], dest: Tuple[int, int]) -> int:\n    \"\"\"A* search on a 2-D grid using 4-directional movement.\n\n    Args:\n        grid: 2-D list with 0 = free cell, 1 = obstacle.\n        start: (row, col) start position.\n        dest:  (row, col) destination position.\n\n    Returns:\n        Length of the shortest path (number of moves) or \u20111 if unreachable.\n    \"\"\"\n    # --- basic checks ------------------------------------------------------\n    if not grid or not grid[0]:\n        return -1\n    rows, cols = len(grid), len(grid[0])\n    sr, sc = start\n    dr, dc = dest\n\n    # start/dest inside grid and on free cells?\n    if not (0 <= sr < rows and 0 <= sc < cols and\n            0 <= dr < rows and 0 <= dc < cols):\n        return -1\n    if grid[sr][sc] == 1 or grid[dr][dc] == 1:\n        return -1\n    if start == dest:\n        return 0\n\n    # --- helpers -----------------------------------------------------------\n    def heuristic(r: int, c: int) -> int:\n        \"\"\"Manhattan distance to destination.\"\"\"\n        return abs(r - dr) + abs(c - dc)\n\n    # (f, g, r, c) tuples will be stored in the priority queue\n    open_heap: List[Tuple[int, int, int, int]] = []\n    g_cost = {start: 0}               # best cost discovered so far for each cell\n    heapq.heappush(open_heap, (heuristic(sr, sc), 0, sr, sc))\n\n    # 4-directional movement\n    directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n\n    # --- main loop ---------------------------------------------------------\n    while open_heap:\n        f, g, r, c = heapq.heappop(open_heap)\n\n        # Outdated entry? skip it.\n        if g != g_cost.get((r, c), float(\"inf\")):\n            continue\n\n        if (r, c) == dest:\n            return g  # reached destination\n\n        for drc, dcc in directions:\n            nr, nc = r + drc, c + dcc\n            if not (0 <= nr < rows and 0 <= nc < cols):\n                continue\n            if grid[nr][nc] == 1:\n                continue\n\n            ng = g + 1  # cost to neighbour\n            if ng < g_cost.get((nr, nc), float(\"inf\")):\n                g_cost[(nr, nc)] = ng\n                heapq.heappush(open_heap, (ng + heuristic(nr, nc), ng, nr, nc))\n\n    return -1  # destination never reached\n```"}
{"task_id": 478, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_std_dev(X):\n    \"\"\"Return the population standard deviation for every feature in *X*.\n\n    Args:\n        X (list[list[int | float]] | np.ndarray): 2-D data set where rows are\n            samples and columns are features.\n\n    Returns:\n        list[float]: Standard deviation of each feature (population version),\n            rounded to 4 decimal places.  If *X* is empty, [] is returned.\n    \"\"\"\n    # 1. Convert input to a NumPy array of floats\n    arr = np.asarray(X, dtype=float)\n\n    # 2. Handle empty input ([], [[]], or np.array([]))\n    if arr.size == 0:\n        return []\n\n    # 3. Ensure the data are 2-D: (n_samples, n_features)\n    if arr.ndim == 1:                    # e.g., [1, 2, 3] \u2192 a single-feature set\n        arr = arr.reshape(-1, 1)\n\n    n_samples = arr.shape[0]\n\n    # 4. Compute population variance feature-wise\n    mean = np.sum(arr, axis=0) / n_samples          # feature means\n    sq_diffs = (arr - mean) ** 2                    # squared deviations\n    variance = np.sum(sq_diffs, axis=0) / n_samples # \u03c3\u00b2 = \u03a3(x-\u03bc)\u00b2 / n\n\n    # 5. Population standard deviation and rounding\n    std_dev = np.sqrt(variance)\n    std_rounded = np.round(std_dev, 4)\n\n    # 6. Return as regular Python list\n    return std_rounded.tolist()\n```"}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    # Reproducibility ----------------------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Basic set-up -------------------------------------------------------------\n    if not rewards:                                 # no time-steps -> return priors\n        return [round(ev_prior, 4)]\n\n    n_arms = len(rewards[0])\n    values = np.full(n_arms, float(ev_prior))       # current value estimates V_i\n    counts = np.zeros(n_arms, dtype=int)            # pull counters  C_i\n\n    # Main interaction loop ----------------------------------------------------\n    for row in rewards:\n        # 1. Pick an arm -------------------------------------------------------\n        if np.random.random() < epsilon:            # exploration\n            arm = np.random.randint(n_arms)\n        else:                                       # exploitation  (argmax breaks ties by the\n            arm = int(np.argmax(values))            # first / smallest index)\n\n        # 2. Observe reward ----------------------------------------------------\n        r = row[arm]\n\n        # 3. Update estimates --------------------------------------------------\n        counts[arm] += 1\n        # incremental mean: V_i \u2190 V_i + (r \u2212 V_i)/C_i\n        values[arm] += (r - values[arm]) / counts[arm]\n\n    # -------------------------------------------------------------------------\n    return [round(v, 4) for v in values]\n```"}
{"task_id": 480, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef mel_spectrogram(\n    x: \"np.ndarray\",\n    window_duration: float = 0.025,\n    stride_duration: float = 0.01,\n    mean_normalize: bool = True,\n    window: str = \"hamming\",\n    n_filters: int = 20,\n    center: bool = True,\n    alpha: float = 0.95,\n    fs: int = 44000,\n):\n    \"\"\"Return the Mel spectrogram of *x* together with the energy of every frame.\"\"\"\n    # --------------------------------------------------------------------- #\n    # 0. Checks / helpers\n    # --------------------------------------------------------------------- #\n    if x.ndim != 1:\n        raise ValueError(\"x must be one-dimensional\")\n\n    def hz2mel(hz):\n        return 2595.0 * np.log10(1.0 + hz / 700.0)\n\n    def mel2hz(mel):\n        return 700.0 * (10.0 ** (mel / 2595.0) - 1.0)\n\n    # --------------------------------------------------------------------- #\n    # 1. Pre-emphasis\n    # --------------------------------------------------------------------- #\n    x = np.asarray(x, dtype=np.float64)\n    if alpha != 0.0:\n        x = np.append(x[0], x[1:] - alpha * x[:-1])\n\n    # --------------------------------------------------------------------- #\n    # 2. Framing\n    # --------------------------------------------------------------------- #\n    frame_len = int(round(window_duration * fs))\n    frame_step = int(round(stride_duration * fs))\n\n    if center:\n        pad = frame_len // 2\n        x = np.pad(x, (pad, pad), mode=\"constant\")\n\n    num_frames = 1 + int(np.floor((len(x) - frame_len) / frame_step))\n    if num_frames <= 0:\n        raise ValueError(\"Not enough samples for even one frame\")\n\n    # as_strided framing (no copy until the `copy()` at the end)\n    shape = (num_frames, frame_len)\n    strides = (x.strides[0] * frame_step, x.strides[0])\n    frames = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides).copy()\n\n    # --------------------------------------------------------------------- #\n    # 3. Windowing\n    # --------------------------------------------------------------------- #\n    win_name = window.lower()\n    if win_name == \"hamming\":\n        win = np.hamming(frame_len)\n    elif win_name == \"hann\":\n        win = np.hanning(frame_len)\n    elif win_name == \"blackman_harris\":\n        n = np.arange(frame_len)\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        win = (\n            a0\n            - a1 * np.cos(2 * np.pi * n / (frame_len - 1))\n            + a2 * np.cos(4 * np.pi * n / (frame_len - 1))\n            - a3 * np.cos(6 * np.pi * n / (frame_len - 1))\n        )\n    else:\n        raise ValueError(\"window must be 'hamming', 'hann' or 'blackman_harris'\")\n    frames *= win\n\n    # --------------------------------------------------------------------- #\n    # 4. Power spectrum\n    # --------------------------------------------------------------------- #\n    N = frame_len\n    fft_frames = np.fft.rfft(frames, n=N)\n    power_spec = (np.abs(fft_frames) ** 2) / N  # shape : (G, N//2+1)\n\n    energy_per_frame = power_spec.sum(axis=1)  # before any projection\n\n    # --------------------------------------------------------------------- #\n    # 5. Mel filter-bank\n    # --------------------------------------------------------------------- #\n    mel_low, mel_high = 0.0, hz2mel(fs / 2.0)\n    mel_points = np.linspace(mel_low, mel_high, n_filters + 2)\n    hz_points = mel2hz(mel_points)\n\n    bin_idxs = np.floor((N * hz_points) / fs).astype(int)\n    bin_idxs = np.clip(bin_idxs, 0, N // 2)  # safety\n\n    fbank = np.zeros((n_filters, N // 2 + 1))\n    for m in range(1, n_filters + 1):\n        left, center, right = bin_idxs[m - 1], bin_idxs[m], bin_idxs[m + 1]\n\n        if center - left != 0:\n            up = (np.arange(left, center) - left) / (center - left)\n            fbank[m - 1, left:center] = up\n\n        if right - center != 0:\n            down = (right - np.arange(center, right)) / (right - center)\n            fbank[m - 1, center:right] = down\n\n    # --------------------------------------------------------------------- #\n    # 6. Filter-bank projection\n    # --------------------------------------------------------------------- #\n    filter_energies = power_spec @ fbank.T  # (G, n_filters)\n\n    # --------------------------------------------------------------------- #\n    # 7. Mean normalization\n    # --------------------------------------------------------------------- #\n    if mean_normalize:\n        filter_energies -= filter_energies.mean(axis=0, keepdims=True)\n\n    # --------------------------------------------------------------------- #\n    # 8. Guard against exact zeros\n    # --------------------------------------------------------------------- #\n    eps = np.finfo(float).eps\n    filter_energies[filter_energies == 0.0] = eps\n    energy_per_frame[energy_per_frame == 0.0] = eps\n\n    # --------------------------------------------------------------------- #\n    # 9. Rounding & conversion to lists\n    # --------------------------------------------------------------------- #\n    filter_energies = np.round(filter_energies, 4).tolist()\n    energy_per_frame = np.round(energy_per_frame, 4).tolist()\n\n    return filter_energies, energy_per_frame\n```"}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Safety checks / trivial case\n    # ------------------------------------------------------------------\n    if data.size == 0:               # empty input \u2192 nothing to do\n        return []\n\n    n_samples = data.shape[0]\n\n    # ------------------------------------------------------------------\n    # Pre\u2013compute \u03b5\u2013neighbourhoods for every point\n    # ------------------------------------------------------------------\n    eps_sq = eps * eps                              # compare squared dists\n    neighbours: list[np.ndarray] = []\n\n    # (n, d) - (1, d) broadcasting; numpy\u2019s einsum is cheap for 2-D arrays\n    for i in range(n_samples):\n        diff = data - data[i]                       # (n, d)\n        dist_sq = np.einsum(\"ij,ij->i\", diff, diff) # squared \u21132 distance\n        neighbours.append(np.where(dist_sq <= eps_sq)[0])\n\n    # ------------------------------------------------------------------\n    # Main DBSCAN\n    # ------------------------------------------------------------------\n    UNASSIGNED = -1\n    labels   = np.full(n_samples, UNASSIGNED, dtype=int)\n    visited  = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n\n        visited[point_idx] = True\n        neigh = neighbours[point_idx]\n\n        # --------------------------------------------------------------\n        # Not enough neighbours \u2192 noise (may be changed later to border)\n        # --------------------------------------------------------------\n        if neigh.size < min_samples:\n            labels[point_idx] = UNASSIGNED\n            continue\n\n        # --------------------------------------------------------------\n        # Found a new core \u2192 start new cluster and expand it\n        # --------------------------------------------------------------\n        labels[point_idx] = cluster_id\n        stack = list(neigh)          # depth-first expansion\n\n        while stack:\n            current = stack.pop()\n\n            if not visited[current]:\n                visited[current] = True\n                neigh_current = neighbours[current]\n                if neigh_current.size >= min_samples:   # current is a core\n                    # add unvisited neighbours to the stack\n                    stack.extend(neigh_current.tolist())\n\n            # Assign the point to the cluster if it is not yet in any cluster\n            if labels[current] == UNASSIGNED:\n                labels[current] = cluster_id\n\n        # finished expanding the cluster\n        cluster_id += 1\n\n    return labels.tolist()\n```\n\nExplanation of the essential steps\n\u2022 \u03b5\u2013neighbourhoods are pre-computed once so they can be reused while clusters are expanded (cost: O(n\u00b2) for n samples).  \n\u2022 Each sample is processed exactly once in discovery order.  \n\u2022 When we enter `expandCluster` (implemented via `stack`) we keep adding reachable core points\u2019 neighbours to that stack, thereby labelling every density-reachable point with the current `cluster_id`.  \n\u2022 Points initially marked as noise can still be relabelled as border points if they later become reachable from a core point.  \n\u2022 Cluster indices start at 0 and increase exactly in the order in which the first core point of every cluster is encountered during the linear scan through the data."}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence of non-negative integer labels.\n    num_classes : int or None, optional\n        Number of classes (columns in the output).  If None, it is inferred\n        as max(y) + 1.  Every label must satisfy\n        0 <= label < num_classes, otherwise ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (len(y), num_classes) with dtype=int.\n    \"\"\"\n    # Convert input to a 1-D NumPy array\n    y_arr = np.asarray(y)\n    if y_arr.ndim == 0:                      # scalar \u2192 length-1 vector\n        y_arr = y_arr.reshape(1)\n    if y_arr.ndim != 1:\n        raise ValueError(\"`y` must be a 1-D sequence of integer labels.\")\n    # Ensure integer type\n    try:\n        y_int = y_arr.astype(int, casting='safe', copy=False)\n    except TypeError:\n        raise ValueError(\"`y` must contain integer values only.\")\n    # Empty input edge-case\n    n_samples = y_int.size\n    if n_samples == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        if num_classes <= 0:\n            raise ValueError(\"`num_classes` must be a positive integer.\")\n        return np.zeros((0, num_classes), dtype=int)\n\n    # Infer / validate num_classes\n    max_label = int(y_int.max())\n    min_label = int(y_int.min())\n    if num_classes is None:\n        if min_label < 0:\n            raise ValueError(\"Negative labels are not allowed.\")\n        num_classes = max_label + 1\n    else:\n        if num_classes <= 0:\n            raise ValueError(\"`num_classes` must be a positive integer.\")\n        if min_label < 0 or max_label >= num_classes:\n            raise ValueError(\n                \"All labels must satisfy 0 <= label < num_classes.\"\n            )\n\n    # Create the one-hot matrix and fill the appropriate positions with 1\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    rows = np.arange(n_samples)\n    one_hot[rows, y_int] = 1\n    return one_hot\n```"}
{"task_id": 483, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef to_categorical(labels, num_classes=None):\n    \"\"\"Convert integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    labels : sequence of int or 1-D np.ndarray\n        The class labels to transform.\n    num_classes : int, optional\n        Desired length of the one-hot vectors; if omitted the function uses\n        ``max(labels) + 1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of ``labels``.\n    \"\"\"\n    # Convert input to a 1-D NumPy array of ints\n    labels = np.asarray(labels, dtype=int).ravel()\n\n    # Infer number of classes if not given\n    if num_classes is None:\n        num_classes = int(labels.max()) + 1\n\n    # Build the one-hot list of lists\n    one_hot = [[0] * num_classes for _ in range(len(labels))]\n    for i, lbl in enumerate(labels):\n        one_hot[i][int(lbl)] = 1\n\n    return one_hot\n```"}
{"task_id": 484, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef lasso_regression_predict(\n    X_train: list[float | int],\n    y_train: list[float | int],\n    X_test: list[float | int],\n    degree: int,\n    reg_factor: float = 0.1,\n    n_iterations: int = 1000,\n) -> list[float]:\n    \"\"\"Fit a one-dimensional Lasso regression model (coordinate descent)\n    and predict on `X_test`.\"\"\"\n    # ------------------------------------------------------------------\n    # Helper\n    # ------------------------------------------------------------------\n    def soft_threshold(rho: float, lam: float) -> float:\n        \"\"\"Soft\u2013thresholding operator S(\u03c1, \u03bb).\"\"\"\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n\n    # ------------------------------------------------------------------\n    # 1. Convert to numpy arrays\n    # ------------------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=float).ravel()\n    y_train = np.asarray(y_train, dtype=float).ravel()\n    X_test = np.asarray(X_test, dtype=float).ravel()\n\n    n_samples = X_train.size\n    n_features = degree + 1  # incl. bias\n\n    # ------------------------------------------------------------------\n    # 2. Polynomial feature expansion (with bias term)\n    # ------------------------------------------------------------------\n    # Design matrices  \u03a6_train  and  \u03a6_test\n    Phi_train = np.column_stack([X_train ** k for k in range(n_features)])\n    Phi_test = np.column_stack([X_test ** k for k in range(n_features)])\n\n    # ------------------------------------------------------------------\n    # 3. Coordinate-descent optimisation\n    # ------------------------------------------------------------------\n    #   z_j  = \u03a3_i \u03c6_{ij}\u00b2   (does not change, so compute once)\n    z = (Phi_train ** 2).sum(axis=0)\n\n    w = np.zeros(n_features)           # weight vector initialised at 0\n    y_pred = Phi_train @ w             # current predictions  (all zeros)\n\n    for _ in range(n_iterations):\n        for j in range(n_features):\n            # Remove current contribution of feature j from the predictions\n            #   residual_i = y_i \u2013 (\u0177_i - \u03c6_{ij} w_j)\n            residual = y_train - (y_pred - Phi_train[:, j] * w[j])\n            rho_j = (Phi_train[:, j] * residual).sum()\n\n            if j == 0:                                # bias -> no \u2113\u2081 penalty\n                new_w_j = rho_j / z[j] if z[j] != 0 else 0.0\n            else:\n                new_w_j = soft_threshold(rho_j, reg_factor) / z[j] if z[j] != 0 else 0.0\n\n            # Update prediction vector efficiently\n            if new_w_j != w[j]:\n                y_pred += Phi_train[:, j] * (new_w_j - w[j])\n                w[j] = new_w_j\n\n    # ------------------------------------------------------------------\n    # 4. Predict on X_test and return rounded values\n    # ------------------------------------------------------------------\n    y_test_pred = Phi_test @ w\n    return [float(np.round(val, 4)) for val in y_test_pred]\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n    # ------------------------------------------------------------------ #\n    # Helper : build the tree ------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    def build(x: np.ndarray, y: np.ndarray, depth: int):\n        # stopping conditions\n        if (depth == max_depth) or (x.size < 2) or (np.all(x == x[0])):\n            return {\"leaf\": True, \"value\": float(y.mean())}\n\n        # sort by feature value\n        order = np.argsort(x)\n        xs, ys = x[order], y[order]\n        n = xs.size\n\n        # prefix sums for O(n) split search\n        csum = np.cumsum(ys)\n        csum2 = np.cumsum(ys ** 2)\n        total_sum, total_sum2 = csum[-1], csum2[-1]\n\n        best_sse, best_idx = np.inf, None\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:                # identical feature value -> skip\n                continue\n            n_l, n_r = i + 1, n - (i + 1)\n\n            sum_l, sum_r = csum[i], total_sum - csum[i]\n            sum2_l, sum2_r = csum2[i], total_sum2 - csum2[i]\n\n            sse_l = sum2_l - (sum_l ** 2) / n_l\n            sse_r = sum2_r - (sum_r ** 2) / n_r\n            sse = sse_l + sse_r\n\n            if sse < best_sse:\n                best_sse, best_idx = sse, i\n\n        # no admissible split -> make leaf\n        if best_idx is None:\n            return {\"leaf\": True, \"value\": float(y.mean())}\n\n        # split threshold = midpoint between consecutive feature values\n        thr = (xs[best_idx] + xs[best_idx + 1]) / 2.0\n\n        left_mask = x <= thr\n        right_mask = ~left_mask\n\n        return {\n            \"leaf\": False,\n            \"thr\": float(thr),\n            \"left\":  build(x[left_mask],  y[left_mask],  depth + 1),\n            \"right\": build(x[right_mask], y[right_mask], depth + 1),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Helper : prediction ---------------------------------------------- #\n    # ------------------------------------------------------------------ #\n    def predict_one(node, x_val: float):\n        while not node[\"leaf\"]:\n            node = node[\"left\"] if x_val <= node[\"thr\"] else node[\"right\"]\n        return node[\"value\"]\n\n    # ------------------------------------------------------------------ #\n    # Main routine ------------------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    if len(X_train) == 0:\n        return []\n\n    X_arr = np.asarray(X_train, dtype=float)\n    y_arr = np.asarray(y_train, dtype=float)\n    tree = build(X_arr, y_arr, depth=0)\n\n    return [predict_one(tree, float(x)) for x in X_test]\n```"}
{"task_id": 486, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Compute the Mean Squared Error (MSE) between two numeric sequences.\n\n    The function returns -1 if the two inputs do not have the same\n    non-zero length, otherwise it returns the MSE rounded to 4 decimals.\n\n    Args:\n        y_true: Ground-truth values (list, tuple or NumPy array).\n        y_pred: Predicted values (list, tuple or NumPy array).\n\n    Returns:\n        float: The MSE rounded to 4 decimal places, or -1 on error.\n    \"\"\"\n    # Convert inputs to NumPy arrays of float type\n    try:\n        a_true = np.asarray(y_true, dtype=float).ravel()\n        a_pred = np.asarray(y_pred, dtype=float).ravel()\n    except (TypeError, ValueError):\n        # Non-numeric data encountered\n        return -1\n\n    # Validate lengths\n    if a_true.size == 0 or a_true.size != a_pred.size:\n        return -1\n\n    # Calculate MSE\n    mse = np.mean((a_true - a_pred) ** 2)\n\n    # Round to 4 decimal places and return as Python float\n    return float(round(mse, 4))\n```"}
{"task_id": 487, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef k_means(X: np.ndarray, k: int, max_iters: int = 300, tol: float = 1e-4) -> list[int]:\n    \"\"\"Classical K-Means clustering (from scratch, NumPy only).\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Data matrix.\n    k : int\n        Number of clusters (1 \u2264 k \u2264 n_samples).\n    max_iters : int, optional\n        Hard iteration cap (default 300).\n    tol : float, optional\n        Convergence threshold on centroid displacement (default 1e-4).\n\n    Returns\n    -------\n    list[int]\n        Cluster index (0 \u2026 k-1) for every sample, in the original order.\n    \"\"\"\n    # --- Initialisation ----------------------------------------------------\n    X = np.asarray(X, dtype=float)            # ensure we work with float values\n    centroids = X[:k].copy()                  # first k samples -> initial centroids\n    labels = np.zeros(X.shape[0], dtype=int)  # will hold cluster indices\n\n    # --- Lloyd\u2019s iterative refinement -------------------------------------\n    for _ in range(max_iters):\n        # 1) Assignment step \u2013 pair-wise Euclidean distances\n        #    distances shape = (n_samples, k)\n        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # 2) Update step \u2013 recompute centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            members = labels == j\n            if np.any(members):                       # avoid empty-cluster error\n                new_centroids[j] = X[members].mean(axis=0)\n\n        # 3) Convergence check\n        if np.linalg.norm(new_centroids - centroids) < tol:\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # --- Result ------------------------------------------------------------\n    return labels.tolist()\n```"}
{"task_id": 488, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_variance(X):\n    \"\"\"Return the population variance of every feature (column) in X.\n\n    Parameters\n    ----------\n    X : list[list[int | float]] | np.ndarray\n        A 2-D data set where each row is a sample and each column is a feature.\n\n    Returns\n    -------\n    list[float] | int\n        A list containing the variance of each feature rounded to four\n        decimals.  Returns -1 if the input is empty, has <1 sample,\n        or is not strictly two-dimensional.\n    \"\"\"\n    # 1. Basic validation / conversion\n    try:\n        arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1                         # cannot be converted to numeric array\n\n    # 2. Dimensionality and emptiness checks\n    if arr.ndim != 2 or arr.shape[0] < 1:\n        return -1\n\n    # 3. Population variance: \u03c3\u00b2 = (1/n) \u03a3 (x_i - x\u0304)\u00b2\n    mean = arr.mean(axis=0)\n    var  = np.mean((arr - mean) ** 2, axis=0)\n\n    # 4. Round to 4 decimals and convert to plain Python list\n    return np.round(var, 4).tolist()\n```"}
{"task_id": 489, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_nominal(one_hot):\n    \"\"\"Convert a batch of one-hot encoded vectors to nominal class labels.\n\n    Parameters\n    ----------\n    one_hot : list[list[int]] | numpy.ndarray\n        A 2-D structure where each row is expected to be a one-hot encoded\n        vector (all zeros except a single 1).\n\n    Returns\n    -------\n    list[int] | int\n        List with the class index for every sample when the input is valid;\n        otherwise the integer \u20111.\n    \"\"\"\n    # Convert to numpy array for uniform processing\n    arr = np.asarray(one_hot)\n\n    # A valid batch must be strictly 2-D\n    if arr.ndim != 2:\n        return -1\n\n    labels = []\n\n    for row in arr:\n        # All elements must be either 0 or 1\n        if not np.all((row == 0) | (row == 1)):\n            return -1\n\n        # Exactly one \u20181\u2019 per row\n        ones_idx = np.flatnonzero(row)\n        if ones_idx.size != 1:\n            return -1\n\n        labels.append(int(ones_idx[0]))\n\n    return labels\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Convert the input to a NumPy array (this also copes with list/tuple inputs)\n    arr = np.asarray(x)\n\n    # Handle the empty-vector case early\n    if arr.size == 0:\n        return []\n\n    # Ensure we are working with a 1-D view of the data\n    arr = arr.ravel()\n\n    # Build the diagonal matrix and convert it back to a Python list of lists\n    diag_matrix = np.diag(arr).tolist()\n\n    return diag_matrix\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nfrom itertools import combinations\n\n# ----------  FP-tree data structure -----------------------------------------\nclass _FPNode:                       # very small helper class\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item, parent):\n        self.item     = item         # item label, None for the root\n        self.count    = 0            # support counter\n        self.parent   = parent       # parent node\n        self.children = {}           # item  -> _FPNode\n        self.link     = None         # next node that contains the same item\n\n\n# ----------  FP-tree construction ------------------------------------------\ndef _build_fp_tree(trans_iterable, min_sup):\n    \"\"\"\n    Build an FP-tree from `trans_iterable` and return (root, header_table).\n    `trans_iterable` must yield tuples   (items_in_transaction , multiplicity)\n    in which every `items_in_transaction` is already ordered according to a\n    global frequency order and contains *only* items that are frequent in the\n    projected data set.\n    \"\"\"\n    # header_table : item -> [overall_support , first_node_in_linked_list]\n    header = {}\n    for items, cnt in trans_iterable:         # 1st scan \u2013 frequency counting\n        for itm in items:\n            header.setdefault(itm, [0, None])[0] += cnt\n    # eliminate infrequent items inside this (sub-)database\n    header = {itm: val for itm, val in header.items() if val[0] >= min_sup}\n    if not header:\n        return None, None                     # nothing left \u2013 stop here\n\n    # global order in this (sub-)database:  descending support, tie\u2192lexicographic\n    order = sorted(header.items(),\n                   key=lambda x: (-x[1][0], x[0]))\n    rank  = {itm: idx for idx, (itm, _) in enumerate(order)}\n\n    # second scan \u2013 real tree construction\n    root = _FPNode(None, None)\n    for items, cnt in trans_iterable:\n        ordered = [i for i in sorted(items, key=lambda x: rank.get(x, 10**9))\n                   if i in header]             # keep only still-frequent items\n        if not ordered:\n            continue\n        cur = root\n        for itm in ordered:\n            nxt = cur.children.get(itm)\n            if nxt is None:                    # create new node\n                nxt = _FPNode(itm, cur)\n                cur.children[itm] = nxt\n                # link into header table\n                if header[itm][1] is None:\n                    header[itm][1] = nxt\n                else:\n                    last = header[itm][1]\n                    while last.link is not None:\n                        last = last.link\n                    last.link = nxt\n            nxt.count += cnt\n            cur = nxt\n    return root, header\n\n\n# ----------  Mining ---------------------------------------------------------\ndef _is_single_path(node):\n    \"\"\"return list of nodes if the subtree under `node` is a single path\"\"\"\n    path = []\n    while node:\n        if len(node.children) > 1:\n            return None\n        if len(node.children) == 0:\n            return path\n        nxt = next(iter(node.children.values()))\n        path.append(nxt)\n        node = nxt\n    return path\n\n\ndef _mine(tree_root, header, min_sup, prefix, out):\n    \"\"\"\n    Recursively mine an FP-tree;  write every frequent pattern (as a tuple\n    sorted lexicographically) into set `out`.\n    `prefix` is a list that already belongs to (super-)patterns.\n    \"\"\"\n    # process items in *ascending* lexicographic order to obtain deterministic\n    # output that is easy to sort later\n    for itm in sorted(header):\n        new_prefix = prefix + [itm]\n        out.add(tuple(sorted(new_prefix)))\n\n        # ---------- build conditional pattern base --------------------------\n        cond_transactions = []\n        node = header[itm][1]\n        while node:\n            path = []\n            parent = node.parent\n            while parent and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                cond_transactions.append((tuple(reversed(path)), node.count))\n            node = node.link\n\n        # ---------- build conditional FP-tree -------------------------------\n        cond_tree, cond_header = _build_fp_tree(cond_transactions, min_sup)\n        if cond_tree is None:\n            continue                          # nothing to mine deeper\n\n        # ---------- optimisation: single path -------------------------------\n        single_path = _is_single_path(cond_tree)\n        if single_path is not None:           # linear tree \u21d2 enumerate directly\n            items_on_path = [n.item for n in single_path]\n            for r in range(1, len(items_on_path)+1):\n                for comb in combinations(items_on_path, r):\n                    out.add(tuple(sorted(new_prefix + list(comb))))\n        else:                                 # general case \u2013 mine recursively\n            _mine(cond_tree, cond_header, min_sup, new_prefix, out)\n\n\n# ----------  Public interface ----------------------------------------------\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    # 1st pass over the original database \u2013 global frequency\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n\n    freq = {k: v for k, v in freq.items() if v >= min_sup}\n    if not freq:                              # nothing is frequent\n        return []\n\n    # global order for the *original* tree\n    order = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    rank  = {itm: idx for idx, (itm, _) in enumerate(order)}\n\n    # prepare (ordered, filtered) transactions with multiplicity = 1\n    preprocessed = []\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda x: rank[x])\n        preprocessed.append((tuple(filtered), 1))\n\n    # build initial FP-tree and mine it\n    root, header = _build_fp_tree(preprocessed, min_sup)\n    patterns = set()\n    _mine(root, header, min_sup, [], patterns)\n\n    # final, deterministic sorting as required by the task description\n    result = [tuple(sorted(p)) for p in patterns]\n    result.sort(key=lambda x: (len(x), x))\n    return result\n```\n\nThe implementation strictly follows the FP-Growth procedure:\n\n1. Counts global item frequencies and discards infrequent ones.\n2. Builds the FP-tree while maintaining node links in the header table.\n3. Recursively mines conditional FP-trees, using the single-path shortcut whenever possible.\n4. Collects every frequent item-set in a set (to avoid duplicates), finally returning them\n   as a list that is\n      \u2022 internally lexicographically sorted,\n      \u2022 externally sorted by length and then lexicographically,\n   exactly as demanded."}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a polynomial feature matrix up to *degree*.\n\n    Parameters\n    ----------\n    X : list of list\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n        If degree < 0 the function returns -1.\n\n    Returns\n    -------\n    list of list\n        Enriched feature matrix whose columns are all monomials of the\n        original variables having total degree \u2264 *degree*, ordered\n        (i)   by increasing total degree,\n        (ii)  lexicographically within the same degree.\n        The first column is 1 (degree-0 term).\n    \"\"\"\n    # ---- Guard clause --------------------------------------------------------\n    if degree < 0:\n        return -1\n\n    # ---- Convert to ndarray for convenience ---------------------------------\n    X_arr = np.asarray(X, dtype=float)        # ensures 2-D float array\n    if X_arr.ndim != 2:\n        raise ValueError(\"X must be a 2-D array-like structure\")\n\n    n_samples, n_features = X_arr.shape\n\n    # ---- Build list of index tuples specifying each monomial ----------------\n    # ()  -> constant term\n    # (0,)             -> x0\n    # (0, 0)           -> x0^2\n    # (0, 1)           -> x0 x1\n    # ...\n    index_tuples: list[tuple[int, ...]] = [()]   # degree-0 term first\n\n    for p in range(1, degree + 1):               # ascending total degree\n        for combo in combinations_with_replacement(range(n_features), p):\n            index_tuples.append(combo)           # lexicographic order\n\n    # ---- Compute each column -------------------------------------------------\n    n_terms = len(index_tuples)\n    out = np.empty((n_samples, n_terms), dtype=float)\n\n    for j, comb in enumerate(index_tuples):\n        if not comb:                             # constant column\n            out[:, j] = 1.0\n        else:\n            col = np.ones(n_samples, dtype=float)\n            for idx in comb:                     # multiply the chosen features\n                col *= X_arr[:, idx]\n            out[:, j] = col\n\n    return out.tolist()\n```"}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    If *y* is empty return -1.\n\n    Parameters\n    ----------\n    y : Sequence[int | float] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        MSE rounded to four decimal places, or -1 when *y* is empty.\n    \"\"\"\n    # Convert to NumPy array to unify computations\n    arr = np.asarray(y, dtype=float).ravel()          # ensures 1-D\n    \n    # Empty input \u2192 -1\n    if arr.size == 0:\n        return -1\n\n    # Mean-squared error\n    diff = arr - arr.mean()\n    mse_val = (diff ** 2).mean()\n\n    # Round to four decimal places and return as plain Python float\n    return float(round(mse_val, 4))\n```"}
{"task_id": 494, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\n\ndef aggregate_predictions(\n    preds: Union[List[List[Union[int, float]]], np.ndarray],\n    classifier: bool = True,\n) -> List[Union[int, float]]:\n    \"\"\"Aggregate ensemble predictions.\n\n    Parameters\n    ----------\n    preds : list[list[int | float]] | np.ndarray\n        Each row contains the predictions of one base model and each\n        column corresponds to a data sample.\n    classifier : bool, default True\n        True  -> classification  (majority voting)\n        False -> regression      (mean of predictions)\n\n    Returns\n    -------\n    list[int | float]\n        Aggregated predictions.  For regression results are rounded to\n        4 decimal places.\n    \"\"\"\n    # Convert to NumPy array for easier manipulation\n    arr = np.asarray(preds)\n\n    if arr.ndim != 2:\n        raise ValueError(\"`preds` must be a 2-D structure (n_models, n_samples).\")\n\n    if classifier:                      # ---- classification: majority vote ----\n        # Function that returns the majority label for a column\n        def _majority(col: np.ndarray) -> int:\n            values, counts = np.unique(col, return_counts=True)\n            # np.argmax breaks ties by the first occurrence; because `values`\n            # is returned in ascending order, this favours the smallest label.\n            return int(values[np.argmax(counts)])\n\n        # Apply along the *model* axis (axis=0 \u21d2 operate per column/sample)\n        majority_per_sample = np.apply_along_axis(_majority, 0, arr)\n        return majority_per_sample.tolist()\n\n    else:                               # ---- regression: simple averaging ----\n        mean_per_sample = np.round(arr.mean(axis=0), 4)\n        return mean_per_sample.tolist()\n```"}
{"task_id": 495, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef standardize(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Standardize every column of a 2-D NumPy array.\n\n    Each feature (column) will have mean 0 and variance 1 after the\n    transformation.  Columns with zero variance become all zeros.\n\n    The input array is **not** modified in place.\n\n    Args:\n        X: 2-D NumPy array (shape = n_samples \u00d7 n_features).\n\n    Returns:\n        Standardized dataset as a Python list rounded to 4 decimals.\n    \"\"\"\n    # Compute column-wise mean and (population) standard deviation\n    means = X.mean(axis=0)\n    stds  = X.std(axis=0)          # ddof=0 by default\n    \n    # Avoid division by zero: temporarily replace 0 std with 1\n    safe_stds = np.where(stds == 0, 1, stds)\n    \n    # Standardize: (X - mean) / std  (broadcasting over rows)\n    Z = (X - means) / safe_stds\n    \n    # For columns that originally had std == 0, set them to 0\n    Z[:, stds == 0] = 0.0\n    \n    # Round to 4 decimals and convert to regular Python list\n    return np.round(Z, 4).tolist()\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list/array or a scalar.\n        slope: Coefficient multiplying x (default 1.0).\n        intercept: Constant term added to the product (default 0.0).\n\n    Returns:\n        (y, grad, grad2): tuple of three lists (all rounded to 4 decimals)\n            y[i]     = slope * x[i] + intercept\n            grad[i]  = slope\n            grad2[i] = 0\n    \"\"\"\n    # Turn the input into a 1-D NumPy array (scalar \u2192 length-1 array)\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim == 0:          # scalar case\n        arr = arr.reshape(1)\n\n    # Affine activation and its derivatives\n    y     = slope * arr + intercept\n    grad  = np.full(arr.shape, slope, dtype=float)\n    grad2 = np.zeros(arr.shape, dtype=float)\n\n    # Round to 4 decimal places and convert to Python lists\n    y     = np.round(y,     4).tolist()\n    grad  = np.round(grad,  4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n\n    return y, grad, grad2\n```"}
{"task_id": 497, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef exponential_activation(x, order: int = 0):\n    \"\"\"Compute the exponential activation or one of its first two derivatives.\n\n    Parameters\n    ----------\n    x : list | numpy.ndarray\n        A (possibly nested) list or NumPy array containing real numbers.\n    order : int, optional\n        Indicates what to compute:\n        * 0 \u2013 activation itself (default)\n        * 1 \u2013 first derivative\n        * 2 \u2013 second derivative\n\n    Returns\n    -------\n    list | int\n        Python list mirroring the shape of *x* with every entry replaced by\n        the corresponding result rounded to 6 decimals, or -1 for invalid\n        *order*.\n    \"\"\"\n    # Accept only the three supported orders\n    if order not in (0, 1, 2):\n        return -1\n\n    # Convert input to NumPy array for vectorized operations\n    arr = np.asarray(x, dtype=float)\n\n    # For the exponential function, the value and all its higher-order\n    # derivatives are the same (e^x), so we just compute it once.\n    result = np.exp(arr)\n\n    # Round every element to 6 decimal places\n    result = np.round(result, 6)\n\n    # Return as a (possibly nested) Python list\n    return result.tolist()\n```"}
{"task_id": 498, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(x: np.ndarray | list | int | float,\n               alpha: float = 0.3,\n               order: int = 0) -> list:\n    \"\"\"Compute Leaky ReLU, its first derivative, or its second derivative.\n\n    Args:\n        x: Values to be transformed. Accepts NumPy array, list, or scalar.\n        alpha: Slope used when an element is non-positive (default 0.3).\n        order: 0 \u2013 return the function value;\n               1 \u2013 return the first derivative;\n               2 \u2013 return the second derivative.\n\n    Returns:\n        Python list with the same shape as *x* (scalars wrapped in a\n        one-element list) that contains the requested quantity.\n        Returns an empty list if *order* is not 0, 1, or 2.\n    \"\"\"\n    # Validate requested order\n    if order not in (0, 1, 2):\n        return []\n\n    # Convert input to a NumPy array (do not modify the original object)\n    arr = np.asarray(x, dtype=float)\n\n    # Compute according to the requested order\n    if order == 0:          # Leaky ReLU value\n        result = np.where(arr > 0, arr, alpha * arr)\n    elif order == 1:        # First derivative\n        result = np.where(arr > 0, 1.0, alpha)\n    else:                   # order == 2, second derivative is zero\n        result = np.zeros_like(arr)\n\n    # Convert the NumPy result to the required list format\n    if result.ndim == 0:    # scalar \u2192 wrap in a list\n        return [result.item()]\n    return result.tolist()\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places.  \n        If *k* is outside the valid range, return \u22121.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Basic checks\n    # ------------------------------------------------------------------ #\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D NumPy array.\")\n        \n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_features):\n        return -1\n\n    data = data.astype(float, copy=False)          # work in float\n\n    # ------------------------------------------------------------------ #\n    # 1. Standardise the features  (zero mean, unit *population* variance)\n    # ------------------------------------------------------------------ #\n    means = data.mean(axis=0)                      # shape (n_features,)\n    stds  = data.std(axis=0, ddof=0)               # population \u03c3  (ddof = 0)\n\n    # avoid division by zero \u2192 keep centred column at all zeros\n    stds_safe = stds.copy()\n    stds_safe[stds_safe == 0] = 1.0\n\n    Z = (data - means) / stds_safe                # standardised data\n\n    # ------------------------------------------------------------------ #\n    # 2. Sample covariance matrix  (divide by n \u2212 1 \u2192 Bessel correction)\n    # ------------------------------------------------------------------ #\n    if n_samples > 1:\n        cov = Z.T @ Z / (n_samples - 1)            # shape (n_features, n_features)\n    else:                                          # degenerate case n = 1\n        cov = np.zeros((n_features, n_features))\n\n    # ------------------------------------------------------------------ #\n    # 3. Eigen-decomposition (covariance matrix is symmetric)\n    # ------------------------------------------------------------------ #\n    eigvals, eigvecs = np.linalg.eigh(cov)         # ascending order\n\n    # ------------------------------------------------------------------ #\n    # 4. Sort eigenvalues / eigenvectors in *descending* order\n    # ------------------------------------------------------------------ #\n    order = eigvals.argsort()[::-1]\n    eigvals = eigvals[order]\n    eigvecs = eigvecs[:, order]                    # columns are eigenvectors\n\n    # ------------------------------------------------------------------ #\n    # 5. Fix sign ambiguity of each eigenvector\n    # ------------------------------------------------------------------ #\n    for i in range(eigvecs.shape[1]):\n        v = eigvecs[:, i]\n        if v[np.argmax(np.abs(v))] < 0:            # make dominant entry positive\n            eigvecs[:, i] = -v\n\n    # ------------------------------------------------------------------ #\n    # 6. Project data onto the first k principal components\n    # ------------------------------------------------------------------ #\n    components = eigvecs[:, :k]                    # shape (n_features, k)\n    projected  = Z @ components                    # shape (n_samples,  k)\n\n    # ------------------------------------------------------------------ #\n    # 7. Round & return as a regular Python list\n    # ------------------------------------------------------------------ #\n    return np.round(projected, 4).tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom math import inf\n\ndef _fit_best_stump(X: np.ndarray, residuals: np.ndarray):\n    \"\"\"Find the decision stump (feature, threshold, left_val, right_val)\n    that minimises the squared error w.r.t. the provided residuals.\"\"\"\n    n_samples, n_features = X.shape\n    best_feature, best_thr = None, None\n    best_left_val, best_right_val = None, None\n    best_sse = inf\n\n    for j in range(n_features):\n        f = X[:, j]\n        order = np.argsort(f)\n        f_sorted = f[order]\n        r_sorted = residuals[order]\n\n        # prefix sums for fast variance / SSE computation\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n\n        for i in range(1, n_samples):            # split is between i-1 and i\n            if f_sorted[i] == f_sorted[i - 1]:\n                continue                          # identical value \u2013 no real split\n            n_left = i\n            n_right = n_samples - i\n\n            sum_left = cumsum_r[i - 1]\n            sum_right = total_sum - sum_left\n\n            sum2_left = cumsum_r2[i - 1]\n            sum2_right = total_sum2 - sum2_left\n\n            mean_left = sum_left / n_left\n            mean_right = sum_right / n_right\n\n            sse_left = sum2_left - (sum_left ** 2) / n_left\n            sse_right = sum2_right - (sum_right ** 2) / n_right\n            sse = sse_left + sse_right\n\n            if sse < best_sse:\n                best_sse = sse\n                best_feature = j\n                best_thr = 0.5 * (f_sorted[i] + f_sorted[i - 1])  # midpoint\n                best_left_val = mean_left\n                best_right_val = mean_right\n\n    return best_feature, best_thr, best_left_val, best_right_val\n\n\ndef gradient_boosting_classifier(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test:  list[list[float]],\n    n_estimators: int = 20,\n    learning_rate: float = 0.1\n) -> list[int]:\n    \"\"\"Tiny gradient\u2013boosted binary classifier (decision-stump base learners).\"\"\"\n    # Convert to numpy arrays\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n\n    n_train = X_train.shape[0]\n\n    # 1) Initial constant predictor\n    c0 = y_train.mean()\n    F_train = np.full(n_train, c0, dtype=float)\n    F_test  = np.full(X_test.shape[0], c0, dtype=float)\n\n    stumps = []\n\n    # 2) Boosting iterations\n    for _ in range(n_estimators):\n        residuals = y_train - F_train          # r_i = y_i - F(x_i)\n\n        feat, thr, left_val, right_val = _fit_best_stump(X_train, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n\n        # Predictions of the new stump on train & test\n        pred_train = np.where(X_train[:, feat] <= thr, left_val, right_val)\n        pred_test  = np.where(X_test[:,  feat] <= thr, left_val, right_val)\n\n        # Update ensemble scores\n        F_train += learning_rate * pred_train\n        F_test  += learning_rate * pred_test\n\n    # 3) Turn final scores into class labels (threshold at 0.5)\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred\n```"}
{"task_id": 501, "completion_id": 0, "completion": "```python\ndef constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        Current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    # The learning rate stays unchanged, irrespective of `step`.\n    return initial_lr\n```"}
{"task_id": 502, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hard_sigmoid(z: np.ndarray, order: int = 0) -> list[float]:\n    \"\"\"Compute Hard-Sigmoid activation or its derivatives.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array.\n    order : int, optional\n        0 \u2013 activation (default)\n        1 \u2013 first derivative\n        2 \u2013 second derivative\n\n    Returns\n    -------\n    list[float]\n        Rounded results (4 decimals) as a Python list.\n        Returns an empty list for unsupported *order* values.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return []\n\n    z = np.asarray(z, dtype=float)        # make sure we have a float array\n\n    if order == 0:                        # Hard-Sigmoid\n        out = np.where(z < -2.5, 0.0,\n              np.where(z > 2.5, 1.0, 0.2 * z + 0.5))\n\n    elif order == 1:                      # first derivative\n        out = np.where((-2.5 <= z) & (z <= 2.5), 0.2, 0.0)\n\n    else:                                 # order == 2 \u2192 second derivative\n        out = np.zeros_like(z)\n\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 503, "completion_id": 0, "completion": "```python\nfrom copy import deepcopy\n\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    \"\"\"Return a deep-copied version of *hparams* with selective updates applied.\n\n    Only the keys that already exist in *hparams* are overwritten by *updates*.\n    The original inputs are never modified (purely functional).\n\n    Parameters\n    ----------\n    hparams : dict | None\n        Original hyper-parameter dictionary.  If None, the function returns -1.\n    updates : dict | None\n        Dictionary containing new values.  Keys not present in *hparams* are\n        ignored.  If None, no changes are applied.\n\n    Returns\n    -------\n    dict | int\n        A deep-copied (and possibly updated) dictionary, or \u22121 if *hparams* is\n        None.\n    \"\"\"\n    # 1. Guard against a missing hyper-parameter dictionary\n    if hparams is None:\n        return -1\n\n    # 2. Work on a deep copy to keep the original untouched\n    result = deepcopy(hparams)\n\n    # 3. Apply only valid updates\n    if updates is not None:\n        for key, value in updates.items():\n            if key in result:\n                result[key] = value\n\n    return result\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM (probabilities only).\"\"\"\n    # Empty mini-batch \u2500> nothing to do\n    if X.size == 0:\n        return []\n\n    m = X.shape[0]       # mini-batch size\n\n    # ------------------------------------------------------------------\n    # Helper: logistic sigmoid\n    # ------------------------------------------------------------------\n    def _sigmoid(a: np.ndarray) -> np.ndarray:\n        return 1. / (1. + np.exp(-a))\n\n    # ------------------------------------------------------------------\n    # Positive phase  (v0 = X  \u2192  h0)\n    # ------------------------------------------------------------------\n    h_prob = _sigmoid(X @ W + hbias)          # h0 probabilities\n    pos_grad = X.T @ h_prob                   # X\u1d40 \u00b7 h0_prob\n\n    # ------------------------------------------------------------------\n    # Gibbs sampling: k full steps  (h \u2192 v \u2192 h)  using probabilities\n    # ------------------------------------------------------------------\n    for _ in range(k):\n        v_prob = _sigmoid(h_prob @ W.T + vbias)   # p(v | h)\n        h_prob = _sigmoid(v_prob @ W + hbias)     # p(h | v)\n\n    # At this point: v_prob = v_k_prob, h_prob = h_k_prob\n    neg_grad = v_prob.T @ h_prob              # v_k_prob\u1d40 \u00b7 h_k_prob\n\n    # ------------------------------------------------------------------\n    # Weight update\n    # ------------------------------------------------------------------\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n\n    # ------------------------------------------------------------------\n    # Return as plain Python list rounded to 4 decimals\n    # ------------------------------------------------------------------\n    return np.round(W_new, 4).tolist()\n```"}
{"task_id": 508, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List\n\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    \"\"\"Return all values whose keys contain a given substring.\n\n    The original insertion order of the dictionary is preserved.\n\n    Args:\n        params: Dictionary mapping parameter names (strings) to parameter values (any type).\n        name: Substring to search for inside the dictionary keys.\n\n    Returns:\n        List of values whose corresponding keys contain *name* (order preserved).  \n        Returns an empty list if no key matches.\n    \"\"\"\n    # Since Python 3.7, dict preserves insertion order; we can therefore\n    # simply iterate through the items in their original order and collect\n    # values whose keys contain the specified substring.\n    return [value for key, value in params.items() if name in key]\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n\n    # Length mismatch \u279c return -1\n    if x_arr.size != y_arr.size:\n        return -1.0\n\n    # Chebyshev distance = max(|x_i \u2212 y_i|)\n    dist = np.max(np.abs(x_arr - y_arr))\n\n    # Round to four decimal places and cast to built-in float\n    return float(round(dist, 4))\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Returns the mini-batch average of the binary cross-entropy reconstruction\n    loss plus the analytical KL divergence, rounded to 6 decimal places.\n    \"\"\"\n    # Numerical-stability clipping for the logits\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n\n    # 1. Reconstruction (binary cross-entropy) loss\n    recon_element = (\n        -y * np.log(y_pred_clipped) - (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    )                       # shape: (batch_size, n_features)\n    recon_loss = recon_element.sum(axis=1)   # per sample\n\n    # 2. KL divergence between q(t|x)=N(mu,diag(sigma^2)) and p(t)=N(0,I)\n    kl_element = (\n        1.0 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    )                       # shape: (batch_size, latent_dim)\n    kl_loss = -0.5 * kl_element.sum(axis=1)  # per sample\n\n    # Total loss per sample and then mini-batch mean\n    total_loss = recon_loss + kl_loss\n    batch_mean_loss = total_loss.mean()\n\n    # Return value rounded to 6 decimal places\n    return round(float(batch_mean_loss), 6)\n```"}
{"task_id": 511, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_bernoulli_mab(n_arms: int = 10) -> list[float]:\n    \"\"\"\n    Build a random Bernoulli multi-armed bandit environment.\n\n    Parameters\n    ----------\n    n_arms : int, default=10\n        Number of arms in the bandit.  If `n_arms` is less than 1, an\n        empty list is returned.\n\n    Returns\n    -------\n    list[float]\n        List of success probabilities (rounded to 4 decimal places)\n        whose pre-rounding values sum to 1.\n    \"\"\"\n    # Edge-case: invalid number of arms\n    if n_arms < 1:\n        return []\n\n    # 1. Draw independent samples from U[0,1)\n    samples = np.random.rand(n_arms)\n\n    # 2. Normalise so that the probabilities sum to 1\n    probs = samples / samples.sum()\n\n    # 3. Round each probability to 4 decimal places and convert to list\n    return np.round(probs, 4).tolist()\n```"}
{"task_id": 512, "completion_id": 0, "completion": "```python\ndef pooling_shape(pool_shape: tuple[int, int],\n                  image_shape: tuple[int, int, int, int],\n                  stride: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Calculate the spatial output dimensions of a 2-D no-padding pooling layer.\n\n    out_h = (h - k_h) / s_h + 1\n    out_w = (w - k_w) / s_w + 1\n\n    Return (out_h, out_w) when both are integers and the kernel fits,\n    otherwise return -1.\n    \"\"\"\n    # Unpack arguments\n    try:\n        k_h, k_w = pool_shape\n        _, _, h,  w = image_shape\n        s_h, s_w = stride\n    except ValueError:            # wrong tuple lengths\n        return -1\n\n    # Basic sanity checks --------------------------------------------------\n    if (k_h <= 0 or k_w <= 0 or               # kernel sizes must be positive\n        s_h <= 0 or s_w <= 0):                # strides must be positive\n        return -1\n\n    # Kernel must fit completely inside the input\n    if k_h > h or k_w > w:\n        return -1\n\n    # Compute numerators\n    num_h = h - k_h\n    num_w = w - k_w\n\n    # Divisibility check (no padding \u21d2 must be exact)\n    if num_h % s_h or num_w % s_w:\n        return -1\n\n    # Integer outputs\n    out_h = num_h // s_h + 1\n    out_w = num_w // s_w + 1\n\n    return (out_h, out_w)\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples \u00d7 n_features) given as a list of lists.\n        w0: Global bias term.\n        w: Linear weights (length = n_features).\n        V: Latent factor matrix (n_features \u00d7 k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    # Convert everything to numpy arrays\n    X = np.asarray(X, dtype=float)        # shape: (N, n)\n    w = np.asarray(w, dtype=float)        # shape: (n,)\n    V = np.asarray(V, dtype=float)        # shape: (n, k)\n    \n    # Linear part: w0 + X\u00b7w\n    linear_term = w0 + X @ w              # shape: (N,)\n    \n    # Pair-wise interaction part\n    XV = X @ V                            # shape: (N, k)      \u2013  \u03a3_j V_{j,f} x_j\n    X2_V2 = (X ** 2) @ (V ** 2)           # shape: (N, k)      \u2013  \u03a3_j V_{j,f}\u00b2 x_j\u00b2\n    interaction_term = 0.5 * np.sum(XV**2 - X2_V2, axis=1)  # shape: (N,)\n    \n    # Final prediction\n    y_hat = linear_term + interaction_term\n    \n    # Round to 4 decimal places and return as Python list\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 514, "completion_id": 0, "completion": "```python\ndef switch_phase(current_phase: str,\n                 set_train: bool | None = None,\n                 set_test: bool | None = None) -> str:\n    \"\"\"Update the training/testing phase.\n    \n    See task description for full specification.\n    \"\"\"\n    # --------------------------------------------------\n    # 1. Validate the current phase\n    # --------------------------------------------------\n    if current_phase not in (\"training\", \"testing\"):\n        return -1\n\n    # --------------------------------------------------\n    # 2. Validate the switches\n    # --------------------------------------------------\n    # At most one switch may be supplied\n    if set_train is not None and set_test is not None:\n        return -1\n\n    # If supplied, the switch must be a real bool (not int, etc.)\n    if set_train is not None and not isinstance(set_train, bool):\n        return -1\n    if set_test is not None and not isinstance(set_test, bool):\n        return -1\n\n    # --------------------------------------------------\n    # 3. Determine the new phase\n    # --------------------------------------------------\n    if set_train is not None:                 # Rule 3a\n        return \"training\" if set_train else \"testing\"\n\n    if set_test is not None:                  # Rule 3b\n        return \"testing\" if set_test else \"training\"\n\n    # Rule 4: no switches supplied \u2192 toggle\n    return \"testing\" if current_phase == \"training\" else \"training\"\n```"}
{"task_id": 515, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict\n\ndef flatten_parameters(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Return a flattened (dot\u2010separated) copy of a potentially nested\n    parameter dictionary.\n\n    The function is purely functional: the *input dictionary is never\n    mutated*; instead a brand-new flat dictionary is produced.\n\n    Examples\n    --------\n    >>> params = {\n    ...     \"optimizer\": {\n    ...         \"type\": \"SGD\",\n    ...         \"config\": {\"lr\": 0.01, \"momentum\": 0.9}\n    ...     },\n    ...     \"epochs\": 25\n    ... }\n    >>> flatten_parameters(params)\n    {'optimizer.type': 'SGD',\n     'optimizer.config.lr': 0.01,\n     'optimizer.config.momentum': 0.9,\n     'epochs': 25}\n    \"\"\"\n    def _recurse(current: Dict[str, Any], prefix: str, out: Dict[str, Any]) -> None:\n        \"\"\"Recursive helper that populates `out` in-place.\"\"\"\n        for key, value in current.items():\n            full_key = f\"{prefix}.{key}\" if prefix else key\n            if isinstance(value, dict):\n                _recurse(value, full_key, out)          # dive deeper\n            else:\n                out[full_key] = value                   # leaf node\n\n    flat: Dict[str, Any] = {}\n    _recurse(params, prefix=\"\", out=flat)\n    return flat\n```\n\nExplanation of the approach\n1. A private helper `_recurse` traverses the dictionary depth-first.\n2. `prefix` keeps the already-seen key path (empty at the top level).\n3. When the current value is a dictionary we call `_recurse` again, appending\n   the current key to the prefix with a dot.\n4. When the value is *not* a dictionary we have reached a leaf, so we store\n   it in the output dictionary under its fully qualified key.\n5. Because a brand-new dictionary `flat` is filled and finally returned, the\n   original `params` remains untouched.\n\nThe function is fully recursive, handles arbitrarily deep nesting, and\nreturns an empty dictionary when the input is empty."}
{"task_id": 516, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef euclidean_distance(a, b):\n    \"\"\"Compute the Euclidean distance between two numeric vectors.\n\n    The vectors may be provided as Python lists or NumPy arrays and may contain\n    integers or floats.  If the vectors have different numbers of total\n    elements, the function must return -1.\n\n    Args:\n        a (list[int | float] | np.ndarray): First vector.\n        b (list[int | float] | np.ndarray): Second vector.\n\n    Returns:\n        float: Euclidean distance rounded to 4 decimal places, or -1 if the\n               vectors are of unequal length.\n    \"\"\"\n    # Convert both inputs to 1-D NumPy arrays of floats\n    a_arr = np.asarray(a, dtype=float).ravel()\n    b_arr = np.asarray(b, dtype=float).ravel()\n\n    # Check if they have the same total number of elements\n    if a_arr.size != b_arr.size:\n        return -1\n\n    # Compute Euclidean (\u21132) distance\n    distance = np.linalg.norm(a_arr - b_arr)\n\n    # Return the distance rounded to 4 decimal places\n    return round(float(distance), 4)\n```"}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int | float | list | tuple | numpy.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        If `z` is a scalar a Python float is returned, otherwise a (nested)\n        list whose elements are the sigmoid of the corresponding entries in\n        `z`, rounded to four decimal places.\n    \"\"\"\n    # Convert to numpy array for unified, vectorised processing\n    arr = np.asarray(z, dtype=np.float64)\n\n    # Allocate output array of the same shape\n    out = np.empty_like(arr)\n\n    # Numerically stable sigmoid:\n    # For non-negative x  : \u03c3(x) = 1 / (1 + exp(-x))\n    # For negative x      : \u03c3(x) = exp(x) / (1 + exp(x))\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])           # this is safe because arr[neg_mask] < 0\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    # Round to four decimal places\n    out = np.round(out, 4)\n\n    # Preserve the input structure\n    if out.ndim == 0:            # scalar case (also covers 0-D ndarray)\n        return float(out)\n    else:                        # list/tuple/ndarray case\n        return out.tolist()\n```\n\nExample usage\n```python\nprint(sigmoid([-1, 0, 1]))   # \u279c [0.2689, 0.5, 0.7311]\nprint(sigmoid(800))          # \u279c 1.0      (no overflow)\nprint(sigmoid(-800))         # \u279c 0.0      (no overflow)\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\nfrom functools import wraps\n\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D NumPy array whose second\n    dimension is larger than one, it is assumed to be one-hot encoded and is\n    replaced by the index of the maximum value along axis 1 before the wrapped\n    *function* is executed.\n    \"\"\"\n\n    def _decode(arr: np.ndarray) -> np.ndarray:\n        \"\"\"Return a 1-D label array if *arr* looks like one-hot, else *arr*.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.ndim == 2 and arr.shape[1] > 1:\n            return arr.argmax(axis=1)\n        return arr\n\n    @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        actual_decoded    = _decode(actual)\n        predicted_decoded = _decode(predicted)\n        return function(actual_decoded, predicted_decoded, *args, **kwargs)\n\n    return wrapper\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).\n    The derivative is computed element-wise as z * (1 - z) and rounded\n    to 4 decimal places.\n\n    Args:\n        z: float, list, or NumPy ndarray holding sigmoid outputs.\n\n    Returns:\n        float if *z* is a scalar; otherwise a (nested) list with the same\n        shape as *z*, each value rounded to 4 decimals.\n    \"\"\"\n    # Empty-list shortcut\n    if z == []:\n        return []\n\n    # Convert to NumPy array for vectorised computation\n    arr = np.asarray(z, dtype=float)\n\n    # Element-wise gradient, rounded to 4 decimals\n    grad = np.round(arr * (1.0 - arr), 4)\n\n    # Scalar \u2192 float, anything else \u2192 nested list\n    return float(grad) if grad.ndim == 0 else grad.tolist()\n```"}
{"task_id": 521, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_grad(z):\n    \"\"\"Element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    z : scalar, list, or np.ndarray\n        Input data.\n\n    Returns\n    -------\n    int or list\n        0/1 values that match the shape of `z`.\n    \"\"\"\n    # --- Scalar cases --------------------------------------------------------\n    # \u2022 Plain Python numbers                     \u2192 np.isscalar(z) is True\n    # \u2022 0-d NumPy arrays (shape == ())           \u2192 z.ndim == 0\n    if np.isscalar(z) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return int(z > 0)\n\n    # --- Array / list cases ---------------------------------------------------\n    arr = np.asarray(z)          # works for nested lists, tuples, ndarrays \u2026\n    grad = (arr > 0).astype(int) # boolean \u2192 int(0/1)\n\n    # Must return a Python list, not an ndarray\n    return grad.tolist()\n```"}
{"task_id": 522, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef standard_normal(weight_shape):\n    \"\"\"Return a tensor of the given shape filled with N(0,1) samples.\n\n    The NumPy random seed is reset to 42 on every call, the generated\n    values are rounded to 4 decimal places and the final result is\n    converted to a (nested) Python list.\n\n    Parameters\n    ----------\n    weight_shape : int | tuple[int, ...] | list[int]\n        Desired tensor shape.\n\n    Returns\n    -------\n    list\n        Nested Python list containing the rounded samples.\n    \"\"\"\n    # 1. Re-seed NumPy\u2019s RNG for reproducibility\n    np.random.seed(42)\n\n    # 2. Normalise `weight_shape` to a tuple that NumPy understands\n    if isinstance(weight_shape, int):\n        shape = (weight_shape,)          # e.g. 3 -> (3,)\n    else:\n        shape = tuple(weight_shape)      # list/tuple -> tuple\n\n    # 3. Draw samples from the standard normal distribution\n    arr = np.random.standard_normal(shape)\n\n    # 4. Round every entry to four decimal places\n    arr = np.round(arr, 4)\n\n    # 5. Convert to a pure Python (nested) list and return\n    return arr.tolist()\n```"}
{"task_id": 523, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_normal(weight_shape: tuple[int, ...]) -> list:\n    \"\"\"Initialise a weight tensor using the He normal (Kaiming normal) distribution.\n\n    Args:\n        weight_shape: Shape of the desired weight tensor. For a dense layer this\n            is usually (fan_in, fan_out). For a 2-D convolution it is\n            (kernel_h, kernel_w, in_channels, out_channels).\n\n    Returns:\n        Nested Python lists containing the initialised weights, rounded to four\n        decimal places.\n    \"\"\"\n    # ------------------------- determine fan-in --------------------------\n    if len(weight_shape) == 2:                                   # dense layer\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:                                 # conv2d kernel\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:                                                        # fall-back\n        fan_in = weight_shape[0]\n\n    # ------------------------- draw samples -----------------------------\n    std = (2.0 / fan_in) ** 0.5                                  # He \u03c3\n    weights = np.random.randn(*weight_shape) * std               # N(0, \u03c3\u00b2)\n\n    # ------------------------- round & return ---------------------------\n    rounded = np.round(weights, 4)                               # four decimals\n    return rounded.tolist()\n```"}
{"task_id": 525, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(\n        X: np.ndarray,\n        y: np.ndarray,\n        lambda_: float = 0.0) -> tuple[list[float], float]:\n    \"\"\"Fit a Ridge (L2-regularised) linear regression model in closed form.\n\n    Args:\n        X: (m, n) design matrix.\n        y: (m,) target vector (or (m, 1)).\n        lambda_: Non-negative regularisation strength.\n\n    Returns\n    -------\n        weights: list of length n with the coefficients (rounded to 4 dp).\n        bias:    scalar intercept term (rounded to 4 dp).\n    \"\"\"\n    # ---- basic sanity checks -------------------------------------------------\n    if lambda_ < 0:\n        raise ValueError(\"lambda_ must be non-negative\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array\")\n    \n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()          # ensure 1-D shape (m,)\n    m, n = X.shape\n    if y.shape[0] != m:\n        raise ValueError(\"X and y have incompatible lengths\")\n\n    # ---- 1. mean-centre ------------------------------------------------------\n    x_mean = X.mean(axis=0)                         # (n,)\n    y_mean = y.mean()                               # scalar\n    Xc = X - x_mean                                 # centred design matrix\n    yc = y - y_mean                                 # centred targets\n\n    # ---- 2. closed-form weights ---------------------------------------------\n    A = Xc.T @ Xc                                   # (n, n)\n    A += lambda_ * np.eye(n)                        # ridge term \u03bbI\n    w = np.linalg.pinv(A) @ Xc.T @ yc               # (n,)\n\n    # ---- 3. recover (unpenalised) bias --------------------------------------\n    b = y_mean - x_mean @ w                         # scalar\n\n    # ---- 4. rounding & output ----------------------------------------------\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return w_rounded, b_rounded\n```"}
{"task_id": 527, "completion_id": 0, "completion": "```python\nimport os\nfrom typing import List, Union\n\n\ndef extract_archive(file_path: str,\n                    path: str = '.',\n                    archive_format: Union[str, List[str], None] = 'auto') -> bool:\n    \"\"\"Checks whether *file_path* belongs to an allowed archive format.\n\n    The helper never raises and never performs real extraction.\n\n    Args:\n        file_path: Full path (or name) of the archive file.\n        path:     Destination folder \u2013 ignored in this simplified version.\n        archive_format:   \u2022 'auto' \u2500\u2192 ['tar', 'zip']\n                          \u2022 'tar' / 'zip'  \u2192 only that format\n                          \u2022 list           \u2192 that list\n                          \u2022 None / []      \u2192 no format allowed\n\n    Returns:\n        True  \u2013 extension matches one of the allowed formats\n        False \u2013 otherwise or on any error\n    \"\"\"\n    try:\n        # ------------------------------------------------------------------ #\n        # 1. Normalise the *archive_format* argument to a list of strings.    #\n        # ------------------------------------------------------------------ #\n        if archive_format is None:\n            allowed_formats: List[str] = []\n        elif isinstance(archive_format, str):\n            if archive_format.lower() == 'auto':\n                allowed_formats = ['tar', 'zip']\n            else:\n                allowed_formats = [archive_format]\n        else:  # assume an iterable (e.g. list/tuple)\n            allowed_formats = list(archive_format)  # makes a shallow copy\n\n        # Empty list  \u2192 everything disabled.\n        if not allowed_formats:\n            return False\n\n        # ------------------------------------------------------------------ #\n        # 2. Map each supported format name to the list of recognised suffixes#\n        # ------------------------------------------------------------------ #\n        format_to_suffixes = {\n            'tar': ['.tar', '.tar.gz', '.tgz', '.tar.bz', '.tar.bz2', '.tbz'],\n            'zip': ['.zip'],\n        }\n\n        # ------------------------------------------------------------------ #\n        # 3. Case-insensitive comparison of the filename with all suffixes.   #\n        # ------------------------------------------------------------------ #\n        fname = os.path.basename(str(file_path)).lower()\n\n        for fmt in allowed_formats:\n            # Only evaluate formats that the helper understands.\n            suffixes = format_to_suffixes.get(str(fmt).lower())\n            if not suffixes:\n                continue  # unknown/unsupported format name\n            for suf in suffixes:\n                if fname.endswith(suf):\n                    return True\n\n        return False\n    except Exception:\n        # Absolutely everything must fail gracefully.\n        return False\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_boundary_grid(\n    X: list[list[int | float]],\n    W: list[int | float],\n    b: float,\n    grid_n: int = 100,\n) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) receives the label  1 if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n    and \u22121 otherwise.  The returned list has shape (grid_n \u00d7 grid_n) and\n    contains those labels for a regular grid that spans the bounding box\n    of *X*.\n\n    Args:\n        X: 2-D data set \u2013 list of [x\u2081 , x\u2082] pairs (shape n\u00d72).\n        W: Length-2 sequence [w\u2081 , w\u2082] with the classifier\u2019s weights.\n        b: Bias term of the classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Python list of lists with shape (grid_n \u00d7 grid_n) whose entries\n        are exclusively 1 or \u22121.\n    \"\"\"\n    # --- basic validation -------------------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError(\"X must be a 2-D array-like with exactly two columns.\")\n    if len(W) != 2:\n        raise ValueError(\"W must contain exactly two elements.\")\n    if grid_n <= 0:\n        raise ValueError(\"grid_n must be a positive integer.\")\n\n    # --- build the 1-D plotting axes --------------------------------------\n    x1_min, x1_max = X_arr[:, 0].min(), X_arr[:, 0].max()\n    x2_min, x2_max = X_arr[:, 1].min(), X_arr[:, 1].max()\n\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n\n    # --- generate the full grid (shape grid_n \u00d7 grid_n) -------------------\n    x1_grid, x2_grid = np.meshgrid(x1_plot, x2_plot, indexing=\"xy\")\n\n    # --- apply the linear decision rule -----------------------------------\n    w1, w2 = float(W[0]), float(W[1])\n    decision_values = w1 * x1_grid + w2 * x2_grid + b\n    labels = np.where(decision_values >= 0, 1, -1)\n\n    # --- convert to a plain Python list and return ------------------------\n    return labels.tolist()\n```"}
{"task_id": 529, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef generate_progress_bar(current: int, target: int | None, width: int = 30) -> str:\n    \"\"\"Return an ASCII progress bar string similar to Keras' *Progbar*.\n\n    Parameters\n    ----------\n    current : int\n        The index of the most recently completed step.\n    target : int | None\n        The total number of steps, or ``None`` if it is unknown.\n    width : int, default=30\n        The length (in characters) of the bar itself (everything situated\n        between the two square brackets).\n\n    Returns\n    -------\n    str\n        A textual progress bar that fully complies with the specification\n        given in the task description (all spaces, brackets, arrows and dots\n        have to be placed exactly).\n    \"\"\"\n    # Case 1: target is unknown \u2013 just produce the counter + '/Unknown'\n    if target is None:\n        return f\"{current:>7d}/Unknown\"\n\n    # Case 2: target is known \u2013 build the real bar.\n    # Number of digits required for printing `target`\n    d = len(str(target))\n    # Number of bar slots that should already be filled\n    prog_width = min(int(width * current / target), width)\n\n    # Bar construction\n    bar_chars = []\n\n    if prog_width > 0:\n        # All but the last filled slot are '='\n        bar_chars.append(\"=\" * (prog_width - 1))\n\n        # The last filled slot depends on whether we are finished\n        if current < target:\n            bar_chars.append(\">\")\n        else:\n            bar_chars.append(\"=\")\n\n    # Remaining (unfilled) slots are '.'\n    bar_chars.append(\".\" * (width - prog_width))\n\n    bar = \"\".join(bar_chars)\n\n    # Assemble full string with prefix, bar and trailing bracket\n    return f\"{current:>{d}d}/{target} [{bar}]\"\n```"}
{"task_id": 530, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (left untouched, as required by the template)\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates the mean Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as *y_true* and values in (0, 1).\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"`y_true` and `y_pred` must have the same shape.\")\n\n    # Clip predictions for numerical stability\n    eps = 1e-7\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n\n    # BCE loss for each sample\n    loss = -(y_true * np.log(y_pred_clipped) +\n             (1 - y_true) * np.log(1 - y_pred_clipped))\n\n    # Mean loss across the batch\n    mean_loss = np.mean(loss)\n\n    # Round to 4 decimal places and return as Python float\n    return float(np.round(mean_loss, 4))\n```"}
{"task_id": 531, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef generalized_cosine(window_len: int, coefs: list[float], symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a generalized cosine window.\n\n    Args\n    ----\n    window_len : int\n        Desired length L of the window (must be > 0).\n    coefs : list[float]\n        Coefficients a_k, k = 0 \u2026 K.\n    symmetric : bool, optional\n        \u2022 True  \u2192 symmetric window (exactly L samples from \u2013\u03c0 to  \u03c0)  \n        \u2022 False \u2192 periodic  window (L + 1 samples from \u2013\u03c0 to  \u03c0, last one\n          discarded).  Default is False.\n\n    Returns\n    -------\n    list[float]\n        The window rounded to four decimal places.\n    \"\"\"\n    # ---------------- sanity checks -----------------\n    if window_len <= 0:\n        raise ValueError(\"window_len must be a positive integer.\")\n    if not coefs:\n        raise ValueError(\"coefs list must contain at least one value.\")\n\n    # ---------------- phase vector \u03b8_n ---------------\n    n = np.arange(window_len)\n\n    if symmetric:\n        # exactly L equally\u2013spaced samples on [\u2013\u03c0 , \u03c0]\n        denom = window_len - 1 if window_len > 1 else 1  # avoid division by zero for L=1\n    else:\n        # L+1 samples, last one dropped  \u2192 spacing = 2\u03c0/L\n        denom = window_len\n\n    theta = -np.pi + (2.0 * np.pi * n) / denom          # shape (L,)\n\n    # ---------------- window computation -------------\n    k = np.arange(len(coefs)).reshape(-1, 1)            # shape (K+1, 1)\n    a = np.asarray(coefs, dtype=float).reshape(-1, 1)   # same shape as k\n    w = np.sum(a * np.cos(k * theta), axis=0)           # shape (L,)\n\n    # ------------- round to 4 decimals & return ------\n    # Using round() to remove the possible -0.0 artefact\n    return [round(float(val), 4) for val in w]\n```\n\nExample usage:\n\n```python\n# 5-point periodic Hamming window (a0 = 0.54, a1 = \u20130.46)\nprint(generalized_cosine(5, [0.54, -0.46], symmetric=False))\n# [-0.046 ,  0.397 ,  1.   ,  0.397 , -0.046 ]\n\n# 5-point symmetric Blackman window (a0 = 0.42, a1 = 0.5, a2 = 0.08)\nprint(generalized_cosine(5, [0.42, 0.5, 0.08], symmetric=True))\n# [0.    , 0.3623, 1.    , 0.3623, 0.    ]\n```"}
{"task_id": 532, "completion_id": 0, "completion": "```python\ndef count_fc_parameters(layers: list[int]) -> int:\n    \"\"\"Calculate total trainable parameters (weights + biases) in a fully-connected network.\n\n    Args:\n        layers: List of integers where each element represents the number of\n            neurons in the respective layer (input layer first).\n\n    Returns:\n        An integer \u2013 total count of trainable parameters. Returns 0 when fewer\n        than two layers are supplied.\n    \"\"\"\n    # Fewer than two layers means no connections, hence no parameters\n    if len(layers) < 2:\n        return 0\n\n    total_params = 0\n    # Iterate over consecutive layer sizes\n    for n_in, n_out in zip(layers[:-1], layers[1:]):\n        total_params += n_in * n_out  # weights\n        total_params += n_out         # biases\n\n    return total_params\n```"}
{"task_id": 533, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_likelihood(x, mean, sigma):\n    \"\"\"Compute the probability density of a normal (Gaussian) distribution.\n    \n    Parameters\n    ----------\n    x : float | list[float] | np.ndarray\n        Point(s) where the density will be evaluated.\n    mean : float\n        The mean (\u00b5) of the distribution.\n    sigma : float\n        The standard deviation (\u03c3) of the distribution (\u03c3 > 0).\n    \n    Returns\n    -------\n    float | list[float]\n        PDF value(s) rounded to four decimal places.  A float is returned\n        when `x` is a scalar, otherwise a list is returned.\n    \"\"\"\n    # Detect whether the caller supplied a scalar or an array-like object\n    is_scalar = np.isscalar(x)\n    \n    # Convert input to a NumPy array for vectorised computation\n    x_arr = np.asarray(x, dtype=float)\n    \n    # Gaussian (normal) PDF formula\n    coeff = 1.0 / (sigma * np.sqrt(2.0 * np.pi))\n    exponent = -((x_arr - mean) ** 2) / (2.0 * sigma ** 2)\n    pdf_vals = coeff * np.exp(exponent)\n    \n    # Round to four decimal places\n    pdf_vals = np.round(pdf_vals, 4)\n    \n    # Return in the required format\n    if is_scalar:          # original input was a single number\n        return float(pdf_vals)\n    else:                  # list / NumPy array input\n        return pdf_vals.tolist()\n```"}
{"task_id": 534, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_(Z):\n    \"\"\"Compute the logistic sigmoid activation element-wise.\n\n    Args:\n        Z: A scalar, 1-D list/array, or 2-D list/array of real numbers.\n\n    Returns:\n        float  \u2013 if `Z` is a Python scalar;\n        list   \u2013 for every other kind of input (shape preserved),\n                 with every element rounded to 4 decimals.\n    \"\"\"\n    # Treat genuine Python scalars or 0-D NumPy arrays as scalar input\n    if np.isscalar(Z) or (isinstance(Z, np.ndarray) and Z.ndim == 0):\n        s = 1.0 / (1.0 + np.exp(-float(Z)))         # sigmoid\n        return round(float(s), 4)                   # scalar \u2192 float\n\n    # Everything else: convert to NumPy array, apply sigmoid element-wise\n    arr = np.asarray(Z, dtype=float)                # handles lists & ndarrays\n    sig = 1.0 / (1.0 + np.exp(-arr))               # vectorised sigmoid\n    sig = np.round(sig, 4)                          # round to 4 decimals\n    return sig.tolist()                             # list output\n```\n\nExample usage\n```python\nprint(sigmoid_(0))                 # 0.5\nprint(sigmoid_(3.2))               # 0.9608\n\nprint(sigmoid_([0, 1, -1]))        # [0.5, 0.7311, 0.2689]\n\nmat = [[-2, 0, 2],\n       [10, -10, 0.5]]\nprint(sigmoid_(mat))\n# [[0.1192, 0.5, 0.8808], [0.9999, 0.0, 0.6225]]\n\nimport numpy as np\na = np.array([[1.5, -3.5], [0, 4]])\nprint(sigmoid_(a))\n# [[0.8176, 0.0293], [0.5, 0.9820]]\n```"}
{"task_id": 536, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression(X_train, y_train, X_test, method=\"normal\", alpha=0.01, num_iter=1000):\n    \"\"\"Linear regression by Normal Equation or batch Gradient Descent.\"\"\"\n    \n    # --------- 1. Convert inputs to numpy arrays and add bias term ----------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test , dtype=float)\n    y_train = np.asarray(y_train, dtype=float).ravel()      # ensure 1-D\n    \n    m, n = X_train.shape\n    \n    X_train = np.hstack((np.ones((m, 1)), X_train))         # shape (m, n+1)\n    X_test  = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    \n    # ----------------- 2. Fit parameters \u03b8 -------------------------------\n    if method.lower() == \"normal\":\n        # \u03b8 = (X\u1d40X)\u207b\u00b9 X\u1d40 y\n        XtX_inv = np.linalg.inv(X_train.T @ X_train)\n        theta   = XtX_inv @ X_train.T @ y_train\n    else:\n        # Batch Gradient Descent\n        theta = np.zeros(n + 1)                             # initialise\n        for _ in range(num_iter):\n            error   = X_train @ theta - y_train             # (m,)\n            grad    = (X_train.T @ error) / m               # (n+1,)\n            theta  -= alpha * grad\n    \n    # ----------------- 3. Predict on X_test -------------------------------\n    predictions = X_test @ theta                            # (k,)\n    \n    # ----------------- 4. Round and return -------------------------------\n    return np.round(predictions, 4).tolist()\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (from scratch).\"\"\"\n    # ------------------------------------------------------------------\n    # 1. Split the training set by class and estimate parameters\n    # ------------------------------------------------------------------\n    eps = 1e-9                      # to avoid division by zero\n    classes = np.array([0, 1])      # binary classifier: class 0 or 1\n    \n    # Masks selecting the rows that belong to each class\n    mask0 = y_train == 0\n    mask1 = y_train == 1\n    \n    # Means and (non-zero) standard deviations of every feature per class\n    mu0  = X_train[mask0].mean(axis=0)\n    mu1  = X_train[mask1].mean(axis=0)\n    std0 = X_train[mask0].std(axis=0, ddof=0) + eps\n    std1 = X_train[mask1].std(axis=0, ddof=0) + eps\n    \n    # ------------------------------------------------------------------\n    # 2. Class priors  P(class)\n    # ------------------------------------------------------------------\n    n_samples = len(y_train)\n    prior0 = mask0.sum() / n_samples\n    prior1 = mask1.sum() / n_samples\n    \n    # Pre-compute parts of the Gaussian log-pdf that do not depend on x\n    log_prior = np.log(np.array([prior0, prior1]))\n    log_coeff0 = -0.5 * np.log(2 * np.pi) - np.log(std0)\n    log_coeff1 = -0.5 * np.log(2 * np.pi) - np.log(std1)\n    inv_var0 = 1.0 / (2 * std0 ** 2)\n    inv_var1 = 1.0 / (2 * std1 ** 2)\n    \n    # ------------------------------------------------------------------\n    # 3. Predict every sample in X_test\n    # ------------------------------------------------------------------\n    predictions = []\n    for x in X_test:\n        # log-likelihoods for class 0 and 1\n        ll0 = (log_coeff0 - ((x - mu0) ** 2) * inv_var0).sum()\n        ll1 = (log_coeff1 - ((x - mu1) ** 2) * inv_var1).sum()\n        \n        # log posterior = log prior + log likelihood\n        log_post0 = log_prior[0] + ll0\n        log_post1 = log_prior[1] + ll1\n        \n        # choose the class with the higher log posterior\n        pred = 0 if log_post0 >= log_post1 else 1\n        predictions.append(pred)\n    \n    return predictions\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Model predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    if AL.shape != Y.shape:\n        raise ValueError(\"AL and Y must have the same shape.\")\n    \n    # flatten to 1-D (works for (m,) or (1, m))\n    AL = AL.ravel()\n    Y  = Y.ravel()\n    m  = AL.size\n\n    # numerical stability: clip probabilities\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n\n    # cross-entropy\n    cost = -np.sum(Y * np.log(AL_clipped) +\n                   (1 - Y) * np.log(1 - AL_clipped)) / m\n\n    return float(cost)\n```"}
{"task_id": 540, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef derivative_sigmoid(Z: np.ndarray) -> list:\n    \"\"\"Return the element-wise derivative of the sigmoid function.\n\n    Args:\n        Z: NumPy array (any shape) containing numeric values.\n\n    Returns:\n        A Python list with the same shape as *Z*, containing the\n        derivative of the sigmoid evaluated at each element, rounded to\n        4 decimal places.\n    \"\"\"\n    # ---- numerically-stable sigmoid ---------------------------------\n    # For large negative numbers exp(-z) may overflow, so we use a\n    # branch that keeps all exponentials on numbers with small magnitude.\n    pos_mask = Z >= 0\n    neg_mask = ~pos_mask\n\n    # Initialize an empty array with same shape as Z\n    sigmoid = np.empty_like(Z, dtype=np.float64)\n\n    # z >= 0  -->  \u03c3(z) = 1 / (1 + e^{-z})\n    sigmoid[pos_mask] = 1.0 / (1.0 + np.exp(-Z[pos_mask]))\n\n    # z < 0   -->  \u03c3(z) = e^{z} / (1 + e^{z})\n    exp_z = np.exp(Z[neg_mask])\n    sigmoid[neg_mask] = exp_z / (1.0 + exp_z)\n    # -----------------------------------------------------------------\n\n    # Derivative: \u03c3(z) * (1 - \u03c3(z))\n    derivative = sigmoid * (1.0 - sigmoid)\n\n    # Round to 4 decimal places and return as plain Python list\n    derivative_rounded = np.round(derivative, 4)\n\n    return derivative_rounded.tolist()\n```"}
{"task_id": 541, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef return_input_grads(y, y_hat):\n    \"\"\"Compute the gradient of Binary Cross-Entropy loss w.r.t. each logit.\n\n    Parameters\n    ----------\n    y : array-like\n        Ground-truth binary labels (0 or 1). Shape can be scalar, 1-D or 2-D.\n    y_hat : array-like\n        Predicted probabilities (after sigmoid) with the same shape as y.\n\n    Returns\n    -------\n    list\n        Element-wise gradient (y_hat \u2212 y), rounded to 4 decimals and returned\n        as a regular Python list with the original shape.\n    \"\"\"\n    # Convert inputs to float NumPy arrays\n    y_arr     = np.asarray(y, dtype=float)\n    y_hat_arr = np.asarray(y_hat, dtype=float)\n\n    # Ensure identical shapes (no implicit broadcasting)\n    if y_arr.shape != y_hat_arr.shape:\n        raise ValueError(\"y and y_hat must have the same shape.\")\n    \n    # Element-wise gradient\n    grads = y_hat_arr - y_arr\n    \n    # Round to 4 decimal places\n    grads = np.round(grads, 4)\n    \n    # Return as a regular Python list (or scalar) with the same shape\n    return grads.tolist()\n```"}
{"task_id": 542, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef rmsprop_update(\n    params: list[np.ndarray],\n    grads: list[np.ndarray],\n    s: list[np.ndarray] | None = None,\n    alpha: float = 0.01,\n    beta: float = 0.9,\n    epsilon: float = 1e-8,\n) -> tuple[list[np.ndarray], list[np.ndarray]]:\n    \"\"\"Performs a single RMSProp optimisation step.\n    \n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current parameter tensors \u03b8.\n    grads : list[np.ndarray]\n        Corresponding gradients g.\n    s : list[np.ndarray] | None, optional\n        Running averages of squared gradients. If None/empty,\n        they are initialised with zeros of the same shape as grads.\n    alpha : float, optional\n        Learning-rate (default 0.01).\n    beta : float, optional\n        Decay factor for the running average (default 0.9).\n    epsilon : float, optional\n        Small constant for numerical stability (default 1e-8).\n    \n    Returns\n    -------\n    tuple[list[np.ndarray], list[np.ndarray]]\n        new_params : list with updated (and 6-dp rounded) parameters.\n        new_s      : list with updated running averages.\n    \"\"\"\n    \n    # If s is missing, empty or None \u2192 create zero tensors\n    if not s:          # covers None and empty list\n        s = [np.zeros_like(g, dtype=float) for g in grads]\n    elif len(s) != len(grads):\n        raise ValueError(\"Length of `s` must match `grads` (or be None/empty).\")\n    \n    new_params: list[np.ndarray] = []\n    new_s: list[np.ndarray] = []\n    \n    for \u03b8, g, s_prev in zip(params, grads, s):\n        # Update the running average of squared gradients\n        s_next = beta * s_prev + (1.0 - beta) * np.square(g)\n        \n        # Parameter update\n        denom = np.sqrt(s_next) + epsilon\n        \u03b8_next = \u03b8 - alpha * g / denom\n        \n        # Store results\n        new_s.append(s_next)\n        new_params.append(np.round(\u03b8_next, 6))\n    \n    return new_params, new_s\n```"}
{"task_id": 543, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef cal_gini_index(data: list) -> float:\n    \"\"\"Calculate the Gini index of a list of class labels.\n\n    Args:\n        data: A list of hashable values representing class labels.\n\n    Returns:\n        The Gini index rounded to 4 decimal places. If the input list is empty\n        return 0.0.\n    \"\"\"\n    # Return 0.0 for an empty list as specified\n    if not data:\n        return 0.0\n\n    total = len(data)\n    counts = Counter(data)\n\n    # Compute the sum of squared class proportions\n    squared_sum = sum((freq / total) ** 2 for freq in counts.values())\n\n    gini = 1.0 - squared_sum\n\n    # Round to 4 decimal places\n    return round(gini, 4)\n```"}
{"task_id": 544, "completion_id": 0, "completion": "```python\ndef split_tree(data: list, fea: int, value: float):\n    \"\"\"Split a data set into two ordered subsets according to a threshold.\n\n    Args:\n        data (list): List of samples. Each sample is a list or tuple of feature values.\n        fea (int): Index of the feature used for the split.\n        value (float): Threshold for the split.\n\n    Returns:\n        tuple[list, list]: Two lists (set1, set2) where set1 contains the samples\n        with feature value >= threshold and set2 contains the rest.\n    \"\"\"\n    # Pre-allocate the result containers that will keep the original order\n    set1, set2 = [], []\n\n    # Iterate over the data once; do NOT touch the original list itself\n    for sample in data:\n        # Guard against an invalid column index (optional but helpful)\n        if fea >= len(sample) or fea < -len(sample):\n            raise IndexError(f\"Feature index {fea} out of range for sample {sample}\")\n\n        # Direct the sample to the appropriate subset\n        if sample[fea] >= value:\n            set1.append(sample)\n        else:\n            set2.append(sample)\n\n    return set1, set2\n```"}
{"task_id": 545, "completion_id": 0, "completion": "```python\ndef tree_stats(root: \"node | None\") -> tuple[int, int]:\n    \"\"\"Return the number of leaf nodes and the maximum depth of the tree.\n\n    A **leaf** is a node whose ``results`` attribute is *not* ``None``.\n\n    Parameters\n    ----------\n    root : node | None\n        Root of the decision tree.  If *None*, the tree is considered empty.\n\n    Returns\n    -------\n    tuple[int, int]\n        (leaf_count, max_depth)\n    \"\"\"\n\n    def _traverse(cur: \"node | None\") -> tuple[int, int]:\n        \"\"\"\n        Recursively compute (leaf_count, max_depth) for the subtree rooted\n        at *cur*.\n        \"\"\"\n        # Empty subtree\n        if cur is None:\n            return 0, 0\n\n        # Leaf node\n        if cur.results is not None:\n            return 1, 1\n\n        # Internal node \u2015 recurse once for both children\n        left_leaves, left_depth  = _traverse(cur.left)\n        right_leaves, right_depth = _traverse(cur.right)\n\n        # Combine statistics from the two subtrees\n        leaf_count = left_leaves + right_leaves\n        max_depth  = 1 + max(left_depth, right_depth)\n\n        return leaf_count, max_depth\n\n    return _traverse(root)\n```"}
{"task_id": 546, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, Dict, List\n\n\ndef build_tree(data: List[List[Any]]) -> Dict[str, Any]:\n    \"\"\"Build a binary classification tree (CART) using the Gini criterion.\n\n    A sample is a list whose last element is the class label, the preceding\n    elements are feature values.  Categorical and numerical values are both\n    compared by ordinary equality.\n\n    The tree is represented as nested dictionaries:\n        Leaf   -> {\"label\": class_label}\n        Branch -> {\"feature\": feature_index,\n                   \"value\":   split_value,\n                   \"left\":    subtree for samples with feature == value,\n                   \"right\":   subtree for samples with feature != value}\n\n    If *data* is empty the empty dict is returned.\n    \"\"\"\n    # ------------------------------------------------------------------ helpers\n    def gini(subset: List[List[Any]]) -> float:\n        \"\"\"Gini impurity of *subset*.\"\"\"\n        n = len(subset)\n        if n == 0:\n            return 0.0\n        counts = Counter(row[-1] for row in subset)\n        return 1.0 - sum((cnt / n) ** 2 for cnt in counts.values())\n\n    def majority_label(subset: List[List[Any]]) -> Any:\n        \"\"\"Most frequent class, ties are broken by first\u2010appearance order.\"\"\"\n        counts: Counter = Counter()\n        first_seen: Dict[Any, int] = {}\n        for idx, row in enumerate(subset):\n            label = row[-1]\n            counts[label] += 1\n            if label not in first_seen:\n                first_seen[label] = idx\n        best_count = max(counts.values())\n        candidates = [lbl for lbl, c in counts.items() if c == best_count]\n        return min(candidates, key=lambda lbl: first_seen[lbl])\n\n    def sorted_values(values):\n        \"\"\"Return *values* sorted, robust to incomparable mixed types.\"\"\"\n        try:\n            return sorted(values)\n        except TypeError:\n            # fall back to string sorting when types differ\n            return sorted(values, key=lambda v: str(v))\n\n    # -------------------------------------------------------------- recursion\n    def grow(subset: List[List[Any]]) -> Dict[str, Any]:\n        \"\"\"Recursively build the tree for *subset*.\"\"\"\n        # If pure, make a leaf immediately\n        if gini(subset) == 0.0:\n            return {\"label\": subset[0][-1]}\n\n        current_impurity = gini(subset)\n        best_gain = 0.0\n        best_feature = None\n        best_value = None\n        best_left = best_right = None\n\n        n_features = len(subset[0]) - 1\n\n        for feat_idx in range(n_features):\n            unique_vals = {row[feat_idx] for row in subset}\n            for val in sorted_values(unique_vals):        # deterministic order\n                left = [row for row in subset if row[feat_idx] == val]\n                right = [row for row in subset if row[feat_idx] != val]\n                if not left or not right:                 # ignore empty splits\n                    continue\n\n                weighted = (len(left) / len(subset)) * gini(left) + \\\n                           (len(right) / len(subset)) * gini(right)\n                gain = current_impurity - weighted\n\n                if gain > best_gain:                      # strict '>' keeps tie-breaking rule\n                    best_gain = gain\n                    best_feature, best_value = feat_idx, val\n                    best_left, best_right = left, right\n\n        # No improvement \u2192 leaf with majority label\n        if best_gain <= 0.0:\n            return {\"label\": majority_label(subset)}\n\n        # Otherwise create a decision node\n        return {\n            \"feature\": best_feature,\n            \"value\":   best_value,\n            \"left\":    grow(best_left),\n            \"right\":   grow(best_right),\n        }\n\n    # ------------------------------------------------------------------ driver\n    return grow(data) if data else {}\n```"}
{"task_id": 547, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_step(A: list[list[float]],\n                 B: list[list[float]],\n                 S: list[float],\n                 obs: list[int],\n                 t: int) -> list[float] | int:\n    \"\"\"Compute the forward probability vector \u03b1_t for a Hidden-Markov model.\n\n    All returned numbers are rounded to 4 decimals.  If any input is\n    malformed or t is out of bounds, the function returns -1.\n    \"\"\"\n    # ---------- basic conversion & sanity checks ----------\n    try:\n        A = np.asarray(A, dtype=float)\n        B = np.asarray(B, dtype=float)\n        S = np.asarray(S, dtype=float)\n    except Exception:\n        return -1                               # non-numeric entries\n\n    # shapes --------------------------------------------------------------\n    if A.ndim != 2 or A.shape[0] == 0 or A.shape[0] != A.shape[1]:\n        return -1                               # A must be square (n\u00d7n)\n    n = A.shape[0]\n\n    if B.ndim != 2 or B.shape[0] != n or B.shape[1] == 0:\n        return -1                               # B must be n\u00d7m\n    m = B.shape[1]\n\n    if S.ndim != 1 or S.shape[0] != n:\n        return -1                               # S length must be n\n\n    # obs and t -----------------------------------------------------------\n    if not isinstance(obs, (list, tuple)) or len(obs) == 0:\n        return -1\n    if not all(isinstance(k, int) for k in obs):\n        return -1\n    if any(k < 0 or k >= m for k in obs):\n        return -1                               # illegal symbol index\n\n    if not isinstance(t, int) or t < 0 or t >= len(obs):\n        return -1                               # t outside 0 \u2026 len(obs)-1\n\n    # ---------- forward recursion ---------------------------------------\n    alpha = S * B[:, obs[0]]                    # \u03b1\u2080\n\n    for step in range(1, t + 1):                # \u03b1\u2081 \u2026 \u03b1_t\n        alpha = alpha @ A                       # (\u03b1_{step-1})\u1d40 A\n        alpha = alpha * B[:, obs[step]]         # element-wise multiply\n\n    # ---------- rounding & output ---------------------------------------\n    return [round(float(x), 4) for x in alpha]\n```"}
{"task_id": 548, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_xi(A: list[list[float]],\n               B: list[list[float]],\n               pi: list[float],\n               obs: list[int],\n               t: int) -> list[list[float]]:\n    \"\"\"Return the \u03be (xi) matrix for a single time step in a discrete HMM.\n\n    \u03be_t(i,j) = P(q_t=i , q_{t+1}=j | obs, \u03bb)\n\n    All entries of the returned matrix are rounded to 4 decimals.\n    If t is outside 0 \u2026 len(obs)-2 the function returns -1.\n    \"\"\"\n    T = len(obs)\n    if t < 0 or t >= T - 1:\n        return -1                       # invalid index\n\n    # numpy copies of the parameters\n    A  = np.asarray(A,  dtype=float)    # (N , N)\n    B  = np.asarray(B,  dtype=float)    # (N , M)\n    pi = np.asarray(pi, dtype=float)    # (N ,)\n\n    N  = A.shape[0]\n\n    # ---------- forward pass (\u03b1) ----------\n    alpha = np.zeros((T, N))\n    alpha[0] = pi * B[:, obs[0]]        # \u03b1_0\n\n    for k in range(1, T):\n        alpha[k] = (alpha[k-1] @ A) * B[:, obs[k]]\n\n    # ---------- backward pass (\u03b2) ----------\n    beta = np.zeros((T, N))\n    beta[T-1] = np.ones(N)              # \u03b2_{T-1}\n\n    for k in range(T-2, -1, -1):\n        beta[k] = A @ (B[:, obs[k+1]] * beta[k+1])\n\n    # ---------- \u03be_t (numerator) ----------\n    alpha_t   = alpha[t]                       # (N ,)\n    beta_t1   = beta[t+1]                      # (N ,)\n    emit_next = B[:, obs[t+1]] * beta_t1       # B[j,o_{t+1}] * \u03b2_{t+1}(j)  (N ,)\n\n    # element-wise multiplication \u2013 broadcasts to (N , N)\n    numer = alpha_t[:, None] * A * emit_next[None, :]\n\n    denom = numer.sum()\n    xi_t  = numer / denom                      # normalise to probabilities\n\n    return np.round(xi_t, 4).tolist()\n```\n\nExplanation of the main steps\n1. Forward probabilities \u03b1 are computed up to time T\u22121:\n       \u03b1_0(i) = \u03c0_i \u00b7 B(i, o_0)\n       \u03b1_k(j) = [\u03a3_i \u03b1_{k\u22121}(i) \u00b7 A(i,j)] \u00b7 B(j, o_k)\n\n2. Back-ward probabilities \u03b2 are computed from the end to the start:\n       \u03b2_{T\u22121}(i) = 1\n       \u03b2_k(i)     = \u03a3_j A(i,j) \u00b7 B(j, o_{k+1}) \u00b7 \u03b2_{k+1}(j)\n\n3. For the requested time step t the unnormalised joint probability of\n   being in state i at t and j at t+1 is\n       \u03b1_t(i) \u00b7 A(i,j) \u00b7 B(j, o_{t+1}) \u00b7 \u03b2_{t+1}(j)\n\n4. Dividing by the sum of all i, j gives the required \u03be_t matrix, which\n   is finally rounded to 4 decimals and returned as a regular Python list.\n\nThe implementation avoids any external dependencies apart from NumPy and\nhandles illegal time indices by returning \u20131 exactly as specified."}
{"task_id": 549, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cal_dis(x, y):\n    \"\"\"Compute the squared Euclidean distance between two vectors.\n\n    Parameters\n    ----------\n    x : list, tuple, or np.ndarray\n        First vector of numbers.\n    y : list, tuple, or np.ndarray\n        Second vector of numbers.\n\n    Returns\n    -------\n    float\n        Squared Euclidean distance rounded to 4 decimal places, or -1 if the\n        vectors are of different lengths.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays of floats\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n    \n    # Check that the two vectors have the same length\n    if x_arr.size != y_arr.size:\n        return -1\n    \n    # Compute squared Euclidean distance\n    dist_sq = np.sum((x_arr - y_arr) ** 2)\n    \n    # Round to 4 decimal places and return as a Python float\n    return float(round(dist_sq, 4))\n```"}
{"task_id": 551, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef _gini(labels: np.ndarray) -> float:\n    \"\"\"Gini impurity of a 1-D array of class labels.\"\"\"\n    n = len(labels)\n    if n == 0:\n        return 0.0\n    counts = Counter(labels)\n    return 1.0 - sum((cnt / n) ** 2 for cnt in counts.values())\n\n\ndef best_gini_split(X: np.ndarray,\n                    y: np.ndarray,\n                    epsilon: float = 1e-3) -> tuple[int, object, float]:\n    \"\"\"Find the best equality split (feature == value) w.r.t. Gini impurity.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Shape (n_samples, n_features).  Elements may be numeric or objects.\n    y : np.ndarray\n        Shape (n_samples, ).\n    epsilon : float, optional\n        Required minimum impurity decrease.\n\n    Returns\n    -------\n    tuple\n        (best_feature_index, best_split_value, best_gini_value)\n        or (-1, None, parent_gini) if no valid split is found.\n    \"\"\"\n    n_samples, n_features = X.shape\n    parent_gini = _gini(y)\n\n    best_gini = float('inf')\n    best_feature = -1\n    best_value = None\n\n    for feat_idx in range(n_features):\n        col = X[:, feat_idx]\n        # np.unique works with numeric and object dtypes\n        for val in np.unique(col):\n            mask_left = col == val\n            n_left = np.sum(mask_left)\n            n_right = n_samples - n_left\n\n            # Skip degenerate splits (all left or all right)\n            if n_left == 0 or n_right == 0:\n                continue\n\n            g_left = _gini(y[mask_left])\n            g_right = _gini(y[~mask_left])\n\n            weighted_gini = (n_left * g_left + n_right * g_right) / n_samples\n\n            if weighted_gini < best_gini:\n                best_gini = weighted_gini\n                best_feature = feat_idx\n                best_value = val\n\n    # Check if the best split is good enough\n    if best_feature == -1 or parent_gini - best_gini < epsilon:\n        return -1, None, float(parent_gini)\n\n    return best_feature, best_value, float(best_gini)\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm with scaling\n    to obtain numerically stable \u03b1, \u03b2 and finally \u03b3\u209c.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' -> 1-D initial state distribution (N,)\n                    'A'  -> state transition matrix      (N,N)\n                    'B'  -> emission matrix              (N,M)\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to four decimals.  If t is out of range an\n                     empty list is returned.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Basic sanity checks\n    # ------------------------------------------------------------------ #\n    if t < 0 or t >= len(observations):\n        return []\n\n    # Convert HMM parameters to numpy arrays for vectorised math\n    pi = np.asarray(hmm[\"pi\"], dtype=float)           # (N,)\n    A  = np.asarray(hmm[\"A\"],  dtype=float)           # (N,N)\n    B  = np.asarray(hmm[\"B\"],  dtype=float)           # (N,M)\n\n    N          = pi.shape[0]                          # #states\n    T          = len(observations)                    # length of sequence\n    obs_idx    = observations                         # just a shorter name\n\n    # ------------------------------------------------------------------ #\n    # Forward pass (\u03b1) with scaling                                      #\n    # ------------------------------------------------------------------ #\n    alpha      = np.zeros((T, N), dtype=float)\n    c          = np.zeros(T,      dtype=float)        # scaling factors\n\n    # t = 0\n    alpha[0] = pi * B[:, obs_idx[0]]\n    c[0]     = 1.0 / alpha[0].sum() if alpha[0].sum() != 0 else 1.0\n    alpha[0] *= c[0]\n\n    # t = 1 .. T-1\n    for tt in range(1, T):\n        alpha[tt]  = (alpha[tt-1] @ A) * B[:, obs_idx[tt]]\n        denom      = alpha[tt].sum()\n        c[tt]      = 1.0 / denom if denom != 0 else 1.0\n        alpha[tt] *= c[tt]\n\n    # ------------------------------------------------------------------ #\n    # Backward pass (\u03b2) with the same scaling factors                    #\n    # ------------------------------------------------------------------ #\n    beta = np.zeros((T, N), dtype=float)\n\n    # t = T-1\n    beta[-1]  = 1.0\n    beta[-1] *= c[-1]\n\n    # t = T-2 .. 0\n    for tt in range(T-2, -1, -1):\n        beta[tt]  = (A @ (B[:, obs_idx[tt+1]] * beta[tt+1]))\n        beta[tt] *= c[tt]\n\n    # ------------------------------------------------------------------ #\n    # Posterior state distribution \u03b3\u209c                                   #\n    # ------------------------------------------------------------------ #\n    gamma_t = alpha[t] * beta[t]\n    total   = gamma_t.sum()\n    if total == 0:                                     # avoid division by 0\n        return [0.0 for _ in range(N)]\n    gamma_t /= total\n\n    # Round to 4 decimal places and return as a Python list\n    return [round(float(p), 4) for p in gamma_t]\n```"}
{"task_id": 553, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    alpha: float = 0.01,\n    max_iter: int = 1000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model with batch gradient descent and\n    predict class labels for a test set.\"\"\"\n    # ------------------------------------------------------------------\n    # 1. Add bias column\n    # ------------------------------------------------------------------\n    n_train = X_train.shape[0]\n    n_test  = X_test.shape[0]\n\n    X_train_b = np.hstack((np.ones((n_train, 1)), X_train))\n    X_test_b  = np.hstack((np.ones((n_test, 1)),  X_test))\n\n    # ------------------------------------------------------------------\n    # 2. Initialise weights (including bias) with zeros\n    # ------------------------------------------------------------------\n    w = np.zeros(X_train_b.shape[1], dtype=float)\n\n    # ------------------------------------------------------------------\n    # Helper: numerically\u2013stable sigmoid\n    # ------------------------------------------------------------------\n    def _sigmoid(z):\n        # Split to avoid overflow when z is very negative/positive\n        positive = z >= 0\n        negative = ~positive\n        out = np.empty_like(z, dtype=float)\n\n        out[positive] = 1.0 / (1.0 + np.exp(-z[positive]))\n        exp_z = np.exp(z[negative])\n        out[negative] = exp_z / (1.0 + exp_z)\n        return out\n\n    # ------------------------------------------------------------------\n    # 3. Batch gradient ascent on log-likelihood\n    # ------------------------------------------------------------------\n    for _ in range(max_iter):\n        preds = _sigmoid(X_train_b @ w)                 # \u03c3(Xw)\n        gradient = X_train_b.T @ (y_train - preds)      # X\u1d40(y-\u03c3)\n        gradient /= n_train                             # average over samples\n        w += alpha * gradient                           # update step\n\n    # ------------------------------------------------------------------\n    # 4. Predict on the test set\n    # ------------------------------------------------------------------\n    probs = _sigmoid(X_test_b @ w)\n    predictions = (probs >= 0.5).astype(int)\n\n    # Ensure pure Python ints in the returned list\n    return [int(p) for p in predictions]\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum number of points (inclusive) needed for a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (1, 2, \u2026) for clustered points; noise is \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (n_samples, n_features)\")\n\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n\n    # ------------------------------------------------------------------\n    # 1. Pair-wise distance matrix (Euclidean).\n    # ------------------------------------------------------------------\n    # Broadcasting builds an (n, n, n_features) tensor of differences\n    # whose L2-norm along the last axis gives the distance matrix.\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.linalg.norm(diff, axis=-1)          # shape (n, n)\n\n    # ------------------------------------------------------------------\n    # 2. \u03b5-neighbourhood list for every sample.\n    # ------------------------------------------------------------------\n    neighbours = [np.where(dist_mat[i] <= eps)[0]     # inclusive of i itself\n                  for i in range(n_samples)]\n\n    # ------------------------------------------------------------------\n    # 3. Identify core points.\n    # ------------------------------------------------------------------\n    core_mask = np.array([len(idx) >= min_samples for idx in neighbours],\n                         dtype=bool)\n\n    # ------------------------------------------------------------------\n    # 4. Cluster expansion (BFS).\n    # ------------------------------------------------------------------\n    labels = np.zeros(n_samples, dtype=int)           # 0 \u2192 unlabeled\n    current_label = 0\n\n    for point in range(n_samples):\n        # Skip non-core or already labelled points\n        if not core_mask[point] or labels[point] != 0:\n            continue\n\n        # Start a new cluster\n        current_label += 1\n        queue = deque([point])\n        labels[point] = current_label\n\n        # Breadth-first search\n        while queue:\n            p = queue.popleft()\n            for nbr in neighbours[p]:\n                if labels[nbr] == 0:                  # not yet assigned\n                    labels[nbr] = current_label\n                    if core_mask[nbr]:\n                        queue.append(nbr)\n\n    # ------------------------------------------------------------------\n    # 5. Mark remaining unlabeled points as noise (\u22121).\n    # ------------------------------------------------------------------\n    labels[labels == 0] = -1\n    return labels.tolist()\n```"}
{"task_id": 559, "completion_id": 0, "completion": "```python\nfrom typing import List, Optional\n\n\nclass Node:\n    \"\"\"Binary-tree node (only leaves hold a value).\"\"\"\n\n    def __init__(self, val: Optional[int] = None,\n                 right: \"Optional[Node]\" = None,\n                 left: \"Optional[Node]\" = None):\n        self.val = val\n        self.right = right\n        self.left = left\n\n\ndef leaf_values(root: Optional[Node]) -> List[int]:\n    \"\"\"Return all leaf-node values from left to right.\"\"\"\n    if root is None:                      # Empty tree\n        return []\n\n    # A node is a leaf when it has *no* children.\n    if root.left is None and root.right is None:\n        # By the task definition leaf nodes always store a value.\n        return [root.val]\n\n    # Otherwise collect leaves from both sub-trees.\n    leaves: List[int] = []\n    leaves.extend(leaf_values(root.left))\n    leaves.extend(leaf_values(root.right))\n    return leaves\n```\n\nExplanation (concise):\n1. An empty tree immediately yields `[]`.\n2. If the current node has no children it is a leaf, so return its value in a single-element list.\n3. For internal nodes recursively gather leaf values from the left sub-tree first and then from the right, concatenate, and return."}
{"task_id": 560, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\n\n\ndef k_nearest_neighbors(\n    points: list[list[float]], query: list[float], k: int = 3\n) -> list[int]:\n    \"\"\"Find the *k* nearest neighbours of *query* among *points* using a k-d tree.\"\"\"\n    # --------------------------------------------------------------------- #\n    #  Small helpers                                                        #\n    # --------------------------------------------------------------------- #\n    if k <= 0 or not points:\n        return []\n\n    n_points = len(points)\n    dim = len(points[0])\n    k = min(k, n_points)  # never need more than *n_points* neighbours\n\n    def sq_dist(p, q):\n        \"\"\"Squared Euclidean distance between two points.\"\"\"\n        return sum((pi - qi) ** 2 for pi, qi in zip(p, q))\n\n    # --------------------------------------------------------------------- #\n    #  Build a balanced k-d tree                                            #\n    #  Each node is a *dict* with keys: idx, axis, left, right              #\n    # --------------------------------------------------------------------- #\n    indices = list(range(n_points))  # remember original indices\n\n    def build(idxs, depth=0):\n        if not idxs:\n            return None\n        axis = depth % dim\n        idxs.sort(key=lambda i: points[i][axis])  # sort by splitting axis\n        mid = len(idxs) // 2\n        node_idx = idxs[mid]\n\n        return {\n            \"idx\": node_idx,\n            \"axis\": axis,\n            \"left\": build(idxs[:mid], depth + 1),\n            \"right\": build(idxs[mid + 1 :], depth + 1),\n        }\n\n    root = build(indices)\n\n    # --------------------------------------------------------------------- #\n    #  k-NN search (branch-and-bound)                                       #\n    #  We keep a *max*-heap of current best candidates:                     #\n    #      heap item = (\u2013distance, \u2013index)                                  #\n    #  \u2013distance makes the largest distance (worst of the best) sit at top  #\n    # --------------------------------------------------------------------- #\n    best = []  # max-heap of size \u2264 k\n\n    def maybe_add(idx):\n        \"\"\"Insert point *idx* into the candidate heap if it improves the set.\"\"\"\n        d = sq_dist(points[idx], query)\n        item = (-d, -idx)  # negative values to emulate a max-heap\n        if len(best) < k:\n            heapq.heappush(best, item)\n        else:\n            # Compare with current worst candidate (heap[0])\n            if item > best[0]:  # smaller distance OR same distance & smaller idx\n                heapq.heapreplace(best, item)\n\n    def search(node):\n        if node is None:\n            return\n\n        idx = node[\"idx\"]\n        axis = node[\"axis\"]\n        point = points[idx]\n\n        # Visit current node\n        maybe_add(idx)\n\n        # Choose side to search first (near branch)\n        diff = query[axis] - point[axis]\n        near, far = (\"left\", \"right\") if diff < 0 else (\"right\", \"left\")\n\n        search(node[near])\n\n        # Decide whether we also need to explore the far branch\n        if len(best) < k or diff * diff < -best[0][0]:\n            search(node[far])\n\n    search(root)\n\n    # --------------------------------------------------------------------- #\n    #  Extract and sort final neighbour list                                #\n    # --------------------------------------------------------------------- #\n    result = [(-d, -i) for d, i in best]  # convert back to positive values\n    result.sort(key=lambda x: (x[0], x[1]))  # by distance, then by index\n    return [idx for _, idx in result]\n```\n\nThe function:\n\n1. Builds a balanced k-d tree from the supplied points using simple `dict` nodes.\n2. Performs a branch-and-bound k-nearest-neighbour search while maintaining a size-`k` max-heap of best candidates.\n3. Returns the indices of the `k` (or fewer) closest points, ordered by ascending distance to the query and breaking ties by the smaller index.\n\nExample usage\n-------------\n```python\npoints = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]\nquery  = [9, 2]\nk = 2\nprint(k_nearest_neighbors(points, query, k))   # -> [4, 5]\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef gmm_em(data: np.ndarray,\n           k: int,\n           max_iter: int = 200,\n           epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Simple EM implementation for a full-covariance Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array with shape (N, D); N samples, D features.\n    k : int\n        Number of mixture components (clusters).\n    max_iter : int, optional\n        Maximum number of EM iterations (default 200).\n    epsilon : float, optional\n        Convergence tolerance on the maximum absolute change of the mixture\n        weights between two successive iterations (default 1 e-4).\n\n    Returns\n    -------\n    list[int]\n        Length-N list of final cluster labels (0 \u2026 k-1) in deterministic,\n        left-to-right order (ascending first coordinate of the component mean).\n\n    Notes\n    -----\n    \u2022 If k == 1 all samples receive label 0 and the algorithm stops\n      immediately.\n    \u2022 A tiny diagonal regulariser (1 e-6) is added to every covariance\n      estimate to guarantee positive-definiteness and numerical stability.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    #  Basic checks and early exit                                       #\n    # ------------------------------------------------------------------ #\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (N samples, D features).\")\n\n    n_samples, n_features = data.shape\n\n    if k < 1:\n        raise ValueError(\"`k` must be a positive integer.\")\n    if k == 1:\n        return [0] * n_samples\n\n    # ------------------------------------------------------------------ #\n    #  Helper: log multivariate normal                                   #\n    # ------------------------------------------------------------------ #\n    _LOG_2PI = n_features * math.log(2.0 * math.pi)\n\n    def _log_gaussian(x: np.ndarray,\n                      mean: np.ndarray,\n                      cov: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log N(x | mean, cov) for all rows of `x`.\n        x : (N, D)\n        mean : (D,)\n        cov : (D, D)\n        Returns\n        -------\n        log_prob : (N,)\n        \"\"\"\n        # Solution of cov^{-1} * (x - mean).T for every sample\n        diff = x - mean\n        try:\n            L = np.linalg.cholesky(cov)\n            # Solve L * y = diff^T  -> y\n            y = np.linalg.solve(L, diff.T)\n            # Mahalanobis distance\n            maha = np.sum(y ** 2, axis=0)\n            log_det = 2.0 * np.sum(np.log(np.diag(L)))\n        except np.linalg.LinAlgError:\n            # Fallback: slower but more stable\n            inv_cov = np.linalg.inv(cov)\n            maha = np.einsum(\"ij,ij->i\", diff, diff @ inv_cov)\n            sign, log_det = np.linalg.slogdet(cov)\n            if sign <= 0:\n                log_det = np.log(np.abs(np.linalg.det(cov) + 1e-12))\n\n        return -0.5 * (_LOG_2PI + log_det + maha)\n\n    # ------------------------------------------------------------------ #\n    #  Initialisation                                                    #\n    # ------------------------------------------------------------------ #\n    rng = np.random.default_rng()\n    # Randomly choose k different samples as initial means\n    init_idx = rng.choice(n_samples, size=k, replace=False)\n    means = data[init_idx]\n\n    # Mixture weights (uniform)\n    weights = np.full(k, 1.0 / k)\n\n    # Start with the global empirical covariance for every component\n    global_cov = np.cov(data.T, bias=True)\n    if global_cov.shape == ():       # handled when D==1\n        global_cov = global_cov.reshape(1, 1)\n    covs = np.array([global_cov.copy() for _ in range(k)])\n\n    reg_eps = 1e-6 * np.eye(n_features)  # regulariser\n\n    # ------------------------------------------------------------------ #\n    #  EM loop                                                           #\n    # ------------------------------------------------------------------ #\n    for _ in range(max_iter):\n        # ------------------------ E-step ------------------------------ #\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            # log \u03c0_k + log N(x | \u03bc_k, \u03a3_k)\n            log_resp[:, j] = np.log(weights[j] + 1e-16) + \\\n                             _log_gaussian(data, means[j], covs[j])\n\n        # Normalise in log-domain to avoid under/overflow\n        log_resp_max = np.max(log_resp, axis=1, keepdims=True)\n        resp = np.exp(log_resp - log_resp_max)\n        resp_sum = resp.sum(axis=1, keepdims=True)\n        resp /= resp_sum  # shape (N, k)\n\n        # Effective number of points per component\n        nk = resp.sum(axis=0)  # shape (k,)\n\n        # ------------------------ M-step ------------------------------ #\n        prev_weights = weights.copy()\n        weights = nk / n_samples\n\n        # Means\n        means = (resp.T @ data) / nk[:, None]  # shape (k, D)\n\n        # Covariances\n        for j in range(k):\n            diff = data - means[j]\n            # weighted outer products and regularisation\n            cov = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n            covs[j] = cov + reg_eps\n\n        # -------------------- Convergence test ------------------------ #\n        if np.max(np.abs(weights - prev_weights)) < epsilon:\n            break\n\n    # ------------------------------------------------------------------ #\n    #  Final assignment                                                  #\n    # ------------------------------------------------------------------ #\n    # Responsibilities for the last parameter set\n    log_resp = np.empty((n_samples, k))\n    for j in range(k):\n        log_resp[:, j] = np.log(weights[j] + 1e-16) + \\\n                         _log_gaussian(data, means[j], covs[j])\n\n    labels = np.argmax(log_resp, axis=1)  # (N,)\n\n    # ------------------------------------------------------------------ #\n    #  Deterministic re-labelling (left \u2192 right)                         #\n    # ------------------------------------------------------------------ #\n    ordering = np.argsort(means[:, 0])           # ascending first coordinate\n    remap = np.zeros(k, dtype=int)\n    remap[ordering] = np.arange(k)               # old id -> new id\n\n    labels = remap[labels]                       # apply remap\n    return labels.tolist()\n```"}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef spectral_clustering(\n    data: np.ndarray,\n    n_cluster: int,\n    gamma: float = 2.0,\n    method: str = \"unnormalized\",\n) -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A Python list containing the cluster label (0-based) for each sample.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Trivial case\n    # ------------------------------------------------------------------ #\n    N = data.shape[0]\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n\n    if method not in {\"unnormalized\", \"normalized\"}:\n        raise ValueError(\"`method` must be either 'unnormalized' or 'normalized'\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Similarity matrix W using the Gaussian (RBF) kernel\n    # ------------------------------------------------------------------ #\n    # pairwise squared Euclidean distances\n    sq_norms = np.sum(data ** 2, axis=1)\n    sq_dists = (\n        sq_norms[:, None] + sq_norms[None, :] - 2.0 * data @ data.T\n    )  # shape (N, N)\n    sq_dists = np.maximum(sq_dists, 0.0)  # numerical stability\n    W = np.exp(-gamma * sq_dists)\n    np.fill_diagonal(W, 0.0)  # remove self-loops, standard in spectral clustering\n\n    # ------------------------------------------------------------------ #\n    # 2. Laplacian construction\n    # ------------------------------------------------------------------ #\n    degree = np.sum(W, axis=1)  # shape (N,)\n    if method == \"unnormalized\":\n        L = np.diag(degree) - W\n    else:  # symmetric normalized Laplacian\n        # Avoid division by zero\n        d_inv_sqrt = 1.0 / np.sqrt(degree + 1e-12)\n        L = np.eye(N) - (d_inv_sqrt[:, None] * W * d_inv_sqrt[None, :])\n\n    # ------------------------------------------------------------------ #\n    # 3. Eigen-decomposition & embedding\n    # ------------------------------------------------------------------ #\n    eigvals, eigvecs = np.linalg.eigh(L)  # L is symmetric\n    U = eigvecs[:, :n_cluster]  # eigenvectors for the smallest eigenvalues\n\n    if method == \"normalized\":\n        # Row-normalize\n        row_norm = np.linalg.norm(U, axis=1, keepdims=True)\n        U = U / (row_norm + 1e-12)\n\n    embedding = U  # shape (N, n_cluster)\n\n    # ------------------------------------------------------------------ #\n    # 4. Deterministic k-means in the embedding space\n    # ------------------------------------------------------------------ #\n    centroids = embedding[:n_cluster].copy()  # deterministic initialisation\n    labels = np.full(N, -1, dtype=int)\n\n    for _ in range(300):  # hard stop to guarantee termination\n        # Assign step\n        dists = ((embedding[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n\n        # Check for convergence\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n\n        # Update step\n        for k in range(n_cluster):\n            members = embedding[labels == k]\n            if len(members) > 0:  # normal case\n                centroids[k] = members.mean(axis=0)\n            # else: keep centroid unchanged (deterministic tie-break)\n\n    # ------------------------------------------------------------------ #\n    # 5. Relabel clusters so that the cluster containing the\n    #    smallest original index becomes 0, next 1, etc.\n    # ------------------------------------------------------------------ #\n    unique_old = np.unique(labels)\n    # pair (old_label, smallest index of data point with that label)\n    order = sorted((l, np.where(labels == l)[0][0]) for l in unique_old)\n    relabel_map = {old: new for new, (old, _) in enumerate(order)}\n    final_labels = [int(relabel_map[l]) for l in labels]\n\n    return final_labels\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array with N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate.\n        max_iter: Maximum number of full passes over the training data.\n\n    Returns:\n        (w, b)  \u2013 w is the learned weight vector as a Python list,\n                  b is the bias term; both rounded to 4 decimals.\n    \"\"\"\n    # Ensure proper NumPy types/shapes\n    X = np.asarray(X_data, dtype=float)\n    y = np.asarray(y_data, dtype=float).flatten()\n    N, d = X.shape\n\n    # 1. initialise alpha and bias\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n\n    # 2. Gram matrix  G[i, j] = X_i \u00b7 X_j\n    G = X @ X.T        # shape (N, N)\n\n    # 3. training loop\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y * G[:, i])      # \u03a3_j \u03b1_j y_j G[j, i]\n            if y[i] * (g + b) <= 0:              # misclassified (or on margin)\n                alpha[i] += eta                  # update \u03b1_i\n                b += eta * y[i]                  # update bias\n                updated = True\n        if not updated:                          # no update in pass \u2192 converged\n            break\n\n    # 4. recover weight vector  w = \u03a3_i \u03b1_i y_i x_i\n    w_vec = X.T @ (alpha * y)                    # shape (d,)\n\n    # 5. round and convert to requested output types\n    w_rounded = [round(float(val), 4) for val in w_vec]\n    b_rounded = round(float(b), 4)\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n\n    while True:\n        # 1. Leaf check \u2013 identical for both task types\n        if hasattr(node, \"y\"):\n            return node.y\n\n        # 2. Internal\u2013node handling\n        if task_type == \"classification\":\n            feat_val = features[node.label]\n\n            # Find the first child whose x matches `feat_val`\n            next_node = None\n            for child in node.child:\n                if hasattr(child, \"x\") and child.x == feat_val:\n                    next_node = child\n                    break\n\n            # Default to the second child if no match\n            if next_node is None:\n                next_node = node.child[1]\n\n        else:  # task_type == 'regression'\n            feat_val = features[node.label]\n            # <= goes left (index 0); > goes right (index 1)\n            next_node = node.child[0] if feat_val <= node.s else node.child[1]\n\n        node = next_node\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"\n    Shannon entropy H(labels) in bits.\n    labels are integers starting from 0.\n    \"\"\"\n    if labels.size == 0:\n        return 0.0\n    # np.bincount is very fast for non-negative integers\n    counts = np.bincount(labels)\n    probs = counts[counts > 0] / labels.size        # skip zero counts\n    return float(-np.sum(probs * np.log2(probs)))\n\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain_rounded_to_6_decimals)\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Base entropy of the label distribution\n    base_entropy = _entropy(y)\n\n    best_gain = -math.inf\n    best_index = 0\n\n    for idx in range(n_features):\n        column = X[:, idx]\n        # unique values of the feature and their frequencies\n        vals, counts = np.unique(column, return_counts=True)\n\n        cond_entropy = 0.0\n        for val, cnt in zip(vals, counts):\n            subset_labels = y[column == val]\n            cond_entropy += (cnt / n_samples) * _entropy(subset_labels)\n\n        info_gain = base_entropy - cond_entropy\n\n        # tie-breaking: choose the smallest index\n        if info_gain > best_gain + 1e-12:\n            best_gain = info_gain\n            best_index = idx\n        elif abs(info_gain - best_gain) <= 1e-12 and idx < best_index:\n            best_index = idx\n\n    # Avoid returning \u22120.0\n    if abs(best_gain) < 1e-12:\n        best_gain = 0.0\n\n    return best_index, round(best_gain, 6)\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef detect_outliers(data: np.ndarray,\n                    n_trees: int,\n                    sample_size: int,\n                    epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation (deterministic).\n\n    Args\n    ----\n    data        : 2-D NumPy array (n_samples, n_features)\n    n_trees     : number of random isolation trees\n    sample_size : subsample size used to grow every tree\n    epsilon     : fraction (0 \u2264 \u03b5 < 1) of observations regarded as outliers\n\n    Returns\n    -------\n    A sorted list with the zero-based indices of the detected outliers.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    #  Preliminary checks / constants\n    # ------------------------------------------------------------------ #\n    np.random.seed(42)                        # global, deterministic seed\n    n_samples, n_features = data.shape\n\n    # no outliers requested\n    if epsilon == 0 or n_samples == 0:\n        return []\n\n    # size of every subsample actually drawn\n    subsz = min(sample_size, n_samples)\n    # height limit for every tree\n    height_limit = math.ceil(math.log2(subsz)) if subsz > 1 else 0\n\n    # ------------------------------------------------------------------ #\n    #  Helper functions\n    # ------------------------------------------------------------------ #\n    def c_factor(n: int) -> float:\n        \"\"\"Expected path length for an unsuccessful search in a BST.\"\"\"\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    class Node:\n        __slots__ = (\"is_leaf\", \"size\",\n                     \"feat\", \"thr\", \"left\", \"right\")\n\n        def __init__(self, *, is_leaf: bool, size: int,\n                     feat=None, thr=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.size = size\n            self.feat = feat\n            self.thr = thr\n            self.left = left\n            self.right = right\n\n    # -------------  tree building  ------------------------------------ #\n    def build_tree(idxs: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build one isolation tree.\"\"\"\n        if depth >= height_limit or idxs.size <= 1:\n            return Node(is_leaf=True, size=idxs.size)\n\n        # random feature and its range\n        feat = np.random.randint(n_features)\n        col = data[idxs, feat]\n        f_min, f_max = col.min(), col.max()\n\n        # if all values equal => cannot split\n        if f_min == f_max:\n            return Node(is_leaf=True, size=idxs.size)\n\n        # random split point\n        thr = np.random.uniform(f_min, f_max)\n\n        # partition\n        left_mask = col < thr\n        right_mask = ~left_mask\n        if not left_mask.any() or not right_mask.any():\n            # degenerate split, fallback to leaf\n            return Node(is_leaf=True, size=idxs.size)\n\n        left_child = build_tree(idxs[left_mask], depth + 1)\n        right_child = build_tree(idxs[right_mask], depth + 1)\n        return Node(is_leaf=False, size=idxs.size,\n                    feat=feat, thr=thr,\n                    left=left_child, right=right_child)\n\n    # -------------  path length evaluation  --------------------------- #\n    def path_length(x: np.ndarray, node: Node, depth: int = 0) -> float:\n        \"\"\"Path length of one sample through one tree.\"\"\"\n        if node.is_leaf:\n            return depth + c_factor(node.size)\n\n        if x[node.feat] < node.thr:\n            return path_length(x, node.left, depth + 1) \\\n                if node.left is not None else depth + c_factor(node.size)\n        else:\n            return path_length(x, node.right, depth + 1) \\\n                if node.right is not None else depth + c_factor(node.size)\n\n    # ------------------------------------------------------------------ #\n    #  Grow the forest\n    # ------------------------------------------------------------------ #\n    forest: list[Node] = []\n    for _ in range(n_trees):\n        sample_indices = np.random.choice(n_samples, subsz, replace=False)\n        forest.append(build_tree(sample_indices, depth=0))\n\n    # ------------------------------------------------------------------ #\n    #  Compute mean path length for every observation\n    # ------------------------------------------------------------------ #\n    path_sum = np.zeros(n_samples, dtype=float)\n    for tree in forest:\n        for i in range(n_samples):\n            path_sum[i] += path_length(data[i], tree)\n\n    h_bar = path_sum / n_trees\n\n    # ------------------------------------------------------------------ #\n    #  Anomaly scores  s(x) = 2^{-(h_bar / \u03c6)}\n    #  with  \u03c6 = 2 ln(n-1) - 2 (n-1) / n\n    # ------------------------------------------------------------------ #\n    phi = 0.0\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:                                   # single point (corner case)\n        phi = 1.0                           # arbitrary non-zero to avoid /0\n\n    scores = np.power(2.0, -h_bar / phi)\n\n    # ------------------------------------------------------------------ #\n    #  Pick the \u03b5 \u00b7 n_samples largest scores \u2192 outliers\n    # ------------------------------------------------------------------ #\n    k = int(math.floor(epsilon * n_samples))\n    if k == 0:\n        return []\n\n    largest_k_idx = np.argsort(scores)[-k:]      # indices of k largest\n    return sorted(largest_k_idx.tolist())\n```"}
{"task_id": 567, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(predictions: list[list[int | float]]) -> list[int]:\n    \"\"\"Ensemble majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float]]\n        A 2-D list where each row contains the predictions made by one tree\n        and each column groups the votes for a single sample.\n\n    Returns\n    -------\n    list[int]\n        The final class label for every sample after majority voting.  In a\n        tie, the smallest label is chosen.\n    \"\"\"\n    # Convert the input to a NumPy array for convenient column operations\n    votes = np.asarray(predictions)\n\n    # Number of samples equals the number of columns\n    n_samples = votes.shape[1]\n\n    final_labels: list[int] = []\n\n    # Process one column (i.e., one sample) at a time\n    for col in range(n_samples):\n        # Unique labels in this column and their corresponding counts\n        labels, counts = np.unique(votes[:, col], return_counts=True)\n\n        # Highest vote count for this sample\n        max_count = counts.max()\n\n        # Labels tied with the maximum count\n        tied_labels = labels[counts == max_count]\n\n        # Choose the smallest label among ties (or the unique winner)\n        chosen_label = int(tied_labels.min())\n\n        final_labels.append(chosen_label)\n\n    return final_labels\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Train a Maximum-Entropy classifier with Generalised Iterative Scaling\n    on (categorical) training data and return the label predictions for\n    X_test.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # helpers -----------------------------------------------------------\n    # ------------------------------------------------------------------\n    def active_sum(x_row, y_label):\n        \"\"\"\n        Sum of the weights that are active for (x_row , y_label).\n        Each (feature_index, feature_value, y_label) that matches x_row\n        contributes its current weight.\n        \"\"\"\n        s = 0.0\n        for i, val in enumerate(x_row):\n            idx = feat2id.get((i, val, y_label))\n            if idx is not None:\n                s += weights[idx]\n        return s\n\n    # ------------------------------------------------------------------\n    # build the feature set and empirical expectations -----------------\n    # ------------------------------------------------------------------\n    N, d = X_train.shape\n    labels = list(set(int(y) for y in y_train))          # all classes\n    C = d                                                # \u03a3_j f_j(x,y) is constant (=d)\n\n    feat_counts = defaultdict(int)                       # empirical counts\n    for x_row, y in zip(X_train, y_train):\n        y = int(y)\n        for i, val in enumerate(x_row):\n            feat_counts[(i, val, y)] += 1\n\n    n_feats = len(feat_counts)\n    feat2id = {}                                         # map feature \u2192 column in weight vector\n    emp_exp = np.zeros(n_feats, dtype=float)             # empirical expectations\n    for idx, (feat, cnt) in enumerate(feat_counts.items()):\n        feat2id[feat] = idx\n        emp_exp[idx] = cnt / N                           # 1/N \u03a3_i f_j(x_i , y_i)\n\n    # ------------------------------------------------------------------\n    # training with GIS -------------------------------------------------\n    # ------------------------------------------------------------------\n    weights = np.zeros(n_feats, dtype=float)\n    tiny = 1e-12                                         # to avoid div/0\n\n    for _ in range(n_iter):\n\n        mod_exp = np.zeros(n_feats, dtype=float)         # model expectations\n\n        # expectation under the current model\n        for x_row in X_train:\n            # raw scores for every label\n            raw = {y: active_sum(x_row, y) for y in labels}\n            max_raw = max(raw.values())                  # for numerical stability\n\n            # normalised probabilities P(y|x)\n            exp_scores = {y: math.exp(raw[y] - max_raw) for y in labels}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n\n            # accumulate expectations\n            for y, p in probs.items():\n                if p == 0.0:\n                    continue\n                for i, val in enumerate(x_row):\n                    idx = feat2id.get((i, val, y))\n                    if idx is not None:\n                        mod_exp[idx] += p\n\n        mod_exp /= N                                     # 1/N \u03a3_i \u03a3_y P(y|x_i) f_j(x_i , y)\n\n        # weight update\n        max_change = 0.0\n        for j in range(n_feats):\n            delta = (1.0 / C) * math.log((emp_exp[j] + tiny) / (mod_exp[j] + tiny))\n            weights[j] += delta\n            if abs(delta) > max_change:\n                max_change = abs(delta)\n\n        if max_change < epsilon:\n            break\n\n    # ------------------------------------------------------------------\n    # prediction --------------------------------------------------------\n    # ------------------------------------------------------------------\n    preds = []\n    for x_row in X_test:\n        best_label, best_score = None, -float(\"inf\")\n        for y in labels:\n            score = active_sum(x_row, y)\n            if score > best_score:\n                best_score = score\n                best_label = y\n        preds.append(int(best_label))\n\n    return preds\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate per-word document-frequency statistics.\n\n    The function returns\n        1. a list of (word, ratio) pairs where *ratio* is the fraction of\n           documents in which the word occurs (rounded to 4 decimals),\n           sorted from the most to the least frequent word;\n        2. a set that contains only the *cut_off* most frequent words\n           (all words if *cut_off* is None).\n\n    Args:\n        data: list/iterable of documents.  Each document is a tuple\n              ``(label, words)`` where *words* is any iterable of strings.\n        cut_off: maximal number of top words to put into the returned set, or\n                 ``None`` for *all* words.\n\n    Returns:\n        Tuple[list[(word, ratio)], set[str]]\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Basic checks / early exit\n    # ------------------------------------------------------------------ #\n    n_docs = len(data)\n    if n_docs == 0:                       # empty input \u2192 empty result\n        return [], set()\n\n    # ------------------------------------------------------------------ #\n    # 2. Count in how many documents each word appears\n    # ------------------------------------------------------------------ #\n    doc_counter: Counter[str] = Counter()\n\n    for _, words in data:\n        doc_counter.update(set(words))    # use a set \u2192 count each word once/-doc\n\n    # ------------------------------------------------------------------ #\n    # 3. Build (word, ratio) list (rounded to 4 decimals)\n    # ------------------------------------------------------------------ #\n    freq_list = [\n        (word, round(count / n_docs, 4))\n        for word, count in doc_counter.items()\n    ]\n\n    # Sort by ratio (descending).  Secondary sort on the word itself simply\n    # guarantees deterministic output when two ratios are identical.\n    freq_list.sort(key=lambda pair: (-pair[1], pair[0]))\n\n    # ------------------------------------------------------------------ #\n    # 4. Build the \u201ctop\u2010k\u201d set\n    # ------------------------------------------------------------------ #\n    if cut_off is None:\n        top_k_set = {word for word, _ in freq_list}\n    elif cut_off <= 0:\n        top_k_set = set()\n    else:\n        # If cut_off is larger than #words slicing will just give all of them.\n        top_k_set = {word for word, _ in freq_list[:cut_off]}\n\n    return freq_list, top_k_set\n```"}
{"task_id": 570, "completion_id": 0, "completion": "```python\nfrom collections import defaultdict\nfrom typing import List, Tuple, Set\n\n\ndef naive_bayes_predict(train_data: List[Tuple[str, List[str]]],\n                        keywords: Set[str],\n                        test_samples: List[List[str]]) -> List[str]:\n    \"\"\"Unsmooothed Bernoulli Naive Bayes classifier that uses only *keywords*\n    as binary features (present / absent).  No class prior and no smoothing.\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # 1.  TRAINING  \u2013  document-level counts  count_c(w)\n    # ------------------------------------------------------------------\n    # counts[label][word]  : #documents of that label that contain *word*\n    counts = defaultdict(lambda: defaultdict(int))\n    # N_docs[label]        : #documents of that label\n    N_docs = defaultdict(int)\n    # to get deterministic tie-breaking keep order of first appearance\n    class_order: list[str] = []\n\n    for label, tokens in train_data:\n        if label not in N_docs:\n            class_order.append(label)          # remember first appearance\n        N_docs[label] += 1\n\n        token_set = set(tokens)                # Bernoulli \u2192 ignore multiples\n        for w in keywords:\n            if w in token_set:\n                counts[label][w] += 1\n\n    # ------------------------------------------------------------------\n    # 2.  CONDITIONAL PROBABILITIES  P(w | c)  (no smoothing)\n    # ------------------------------------------------------------------\n    # probs[label][word] = P(word | label)\n    probs: dict[str, dict[str, float]] = {}\n\n    for label in N_docs:\n        Nc = N_docs[label]\n        probs[label] = {}\n        for w in keywords:\n            probs[label][w] = counts[label][w] / Nc    # may be 0.0\n\n    # ------------------------------------------------------------------\n    # 3.  PREDICTION\n    # ------------------------------------------------------------------\n    predictions: list[str] = []\n\n    for tokens in test_samples:\n        tok_set = set(tokens)\n\n        best_label = None\n        best_likelihood = -1.0     # likelihoods are in [0,1]\n\n        for label in class_order:  # deterministic order\n            likelihood = 1.0\n            for w in keywords:\n                pwc = probs[label][w]\n                if w in tok_set:\n                    likelihood *= pwc\n                else:\n                    likelihood *= (1.0 - pwc)\n\n                # Early stop: 0 stays 0, but it is cheap enough to continue.\n\n            if likelihood > best_likelihood:\n                best_likelihood = likelihood\n                best_label = label\n\n        # (All-zero edge case: fall back to first class.)\n        if best_label is None:\n            best_label = class_order[0]\n\n        predictions.append(best_label)\n\n    return predictions\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Training data.\n    y : np.ndarray, shape (n_samples,)\n        Class labels (+1 or \u22121).\n    alpha : np.ndarray, shape (n_samples,)\n        Optimised Lagrange multipliers.\n\n    Returns\n    -------\n    (w_list, b) : tuple[list[float], float]\n        w_list : the weight vector rounded to 4 decimals (Python list)\n        b       : the bias term rounded to 4 decimals (float)\n    \"\"\"\n    # Element-wise product \u03b1 \u2299 y   (shape: n_samples,)\n    alpha_y = alpha * y\n\n    # Weight vector:  w = X\u1d40 (\u03b1 \u2299 y)      (shape: n_features,)\n    w = X.T @ alpha_y\n\n    # Bias: b = 1/n \u03a3_i (y_i \u2212 w\u1d40 x_i)\n    predictions = X @ w                    # w\u1d40 x_i for every sample (shape: n_samples,)\n    b = np.mean(y - predictions)\n\n    # Rounding\n    w_list = [round(float(w_i), 4) for w_i in w]\n    b = round(float(b), 4)\n\n    return w_list, b\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local-Outlier-Factor (LOF) for every sample in `data`\n    and returns\n        (list_of_LOF_scores_rounded_to_4_decimals ,\n         ascending_list_of_indices_with_LOF>epsilon)\n\n    Parameters\n    ----------\n    data : array-like, shape = (n_samples, n_features) or (n_samples,)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold that decides which samples are flagged\n              as outliers        (default 1.5)\n\n    Notes\n    -----\n    \u2022  NO external ML / scientific libraries are used \u2013 only `numpy`.\n    \u2022  All LOF scores are rounded with `numpy.round(score, 4)`.\n    \"\"\"\n\n    # ---- 0. Sanity checks ----------------------------------------------------\n    data = np.asarray(data, dtype=float)\n    if data.ndim == 1:                             # make 1-D data 2-D\n        data = data[:, None]\n    n_samples = data.shape[0]\n\n    if not (1 <= k < n_samples):\n        raise ValueError(\"k must satisfy 1 \u2264 k < n_samples (got k = %d)\" % k)\n\n    # ---- 1. Full pair-wise Euclidean distance matrix ------------------------\n    # shape : (n_samples, n_samples)\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.sqrt(np.sum(diff**2, axis=2))\n\n    # ---- 2. k-distance and k nearest neighbours -----------------------------\n    k_distances   = np.empty(n_samples)            # k-distance(p)\n    neighbours    = np.empty((n_samples, k), int)  # N_k(p)\n\n    for i in range(n_samples):\n        idx_sorted = np.argsort(dist_mat[i])       # includes index i (dist = 0)\n        neigh_idx  = idx_sorted[1:k+1]             # skip itself\n        neighbours[i]  = neigh_idx\n        k_distances[i] = dist_mat[i, neigh_idx[-1]]\n\n    # ---- 3 & 4. Reachability distances and Local Reachability Density -------\n    lrd = np.empty(n_samples)\n    for p in range(n_samples):\n        reach_dist_sum = 0.0\n        for q in neighbours[p]:\n            reach_dist = max(k_distances[q], dist_mat[p, q])\n            reach_dist_sum += reach_dist\n        lrd[p] = k / reach_dist_sum\n\n    # ---- 5. Local Outlier Factor --------------------------------------------\n    lof = np.empty(n_samples)\n    for p in range(n_samples):\n        lrd_sum = lrd[neighbours[p]].sum()\n        lof[p]  = (lrd_sum / k) / lrd[p]\n\n    # ---- 6. Rounding & outlier indices --------------------------------------\n    lof_rounded = np.round(lof, 4).tolist()\n    outlier_idx = [i for i, score in enumerate(lof) if score > epsilon]\n\n    return (lof_rounded, outlier_idx)\n```\n\nThe routine follows the exact mathematical steps of the LOF algorithm:\n\n1. full pair-wise Euclidean distances  \n2. extraction of each point\u2019s k-distance and its k nearest neighbours  \n3. reach-dist\u2096(p,q)=max(k-distance(q),\u2006dist(p,q))  \n4. LRD\u2096(p)=k / \u03a3 reach-dist\u2096(p,q)  \n5. LOF\u2096(p)= (\u03a3 LRD\u2096(q)/k) / LRD\u2096(p)  \n\nAll LOF scores are rounded to four decimals for the return value, while the raw (un-rounded) scores are used for the outlier decision (`score > epsilon`)."}
{"task_id": 573, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_train(X_train: np.ndarray,\n                     y_train: np.ndarray,\n                     eta: float = 0.1,\n                     n_iter: int = 1000) -> tuple[list[float], float, list[int]]:\n    \"\"\"Implement the classical Rosenblatt Perceptron learning rule.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training patterns of shape (m, n).\n    y_train : np.ndarray\n        Target labels (+1 or -1) of length m.\n    eta : float, optional\n        Learning rate, by default 0.1.\n    n_iter : int, optional\n        Maximum number of passes over the data, by default 1000.\n\n    Returns\n    -------\n    tuple[list[float], float, list[int]]\n        (learned weight vector as list, bias term, list with #errors per epoch),\n        all real\u2010valued outputs rounded to 4 decimal places.\n    \"\"\"\n    # Cast inputs to NumPy arrays (ensures sliceability & dot products)\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float)\n\n    m, n = X.shape                      # m = #patterns, n = #features\n    w = np.zeros(n, dtype=float)        # weight vector initialised to 0\n    b = 0.0                             # bias initialised to 0\n    error_history: list[int] = []       # keeps track of errors per epoch\n\n    for _ in range(n_iter):\n        error_count = 0\n        for xi, yi in zip(X, y):        # iterate in given order\n            if yi * (np.dot(xi, w) + b) <= 0.0:\n                w += eta * yi * xi      # weight update\n                b += eta * yi           # bias update\n                error_count += 1\n\n        error_history.append(int(error_count))\n        if error_count == 0:            # convergence achieved\n            break\n\n    # Round outputs to the nearest 4th decimal place\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded, error_history\n```"}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the current global\n        random state is used unchanged.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays with shapes\n        (m\u00b7n_train, 2), (m\u00b7n_val, 2), (m\u00b7n_train,) and (m\u00b7n_val,)\n        respectively.  Every coordinate is rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    X_tr, X_va = [], []\n    y_tr, y_va = [], []\n\n    for c in range(m):\n        base = 10 * (c // 2)          # 10\u00b7\u230ac/2\u230b\n\n        # Training points for class c\n        for _ in range(n_train):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_tr.append((x, y))\n            y_tr.append(c)\n\n        # Validation points for class c\n        for _ in range(n_val):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_va.append((x, y))\n            y_va.append(c)\n\n    # Convert to NumPy arrays and round coordinates\n    X_train = np.round(np.asarray(X_tr, dtype=float), 4)\n    X_val   = np.round(np.asarray(X_va, dtype=float), 4)\n    y_train = np.asarray(y_tr, dtype=int)\n    y_val   = np.asarray(y_va, dtype=int)\n\n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 577, "completion_id": 0, "completion": ""}
{"task_id": 578, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple\n\ndef make_pairs(corpus: List) -> List[Tuple]:\n    \"\"\"Return every consecutive pair (bigram) found in *corpus*.\n\n    Args:\n        corpus: A list containing any kind of elements.\n\n    Returns:\n        A list of tuples where each tuple is a pair of consecutive\n        elements from *corpus*.\n    \"\"\"\n    # If corpus has fewer than 2 items, an empty list is returned automatically.\n    return [(corpus[i], corpus[i + 1]) for i in range(len(corpus) - 1)]\n```\n\nExplanation\n-----------\n\u2022 `range(len(corpus) - 1)` iterates over every index except the last one.  \n\u2022 For each index `i`, the tuple `(corpus[i], corpus[i + 1])` captures the\n  current element and its immediate successor.  \n\u2022 When `len(corpus) < 2`, `range(-1)` produces no indices, so the comprehension\n  yields an empty list, satisfying the requirement automatically."}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Deterministic Lloyd-style K-Means clustering.\n\n    Args:\n        X        : (m, d) array with the input points.\n        k        : number of clusters, 1 \u2264 k \u2264 m.\n        max_iters: maximum amount of Lloyd iterations.\n\n    Returns\n    -------\n        labels  : list with length m, the cluster index of every point.\n        centres : list with k inner lists \u2013 the final centres, each\n                  coordinate rounded to 4 decimals.\n    \"\"\"\n    # --- basic checks / preparation -----------------------------------------\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"`X` must be a 2-D array (m, d).\")\n    m, d = X.shape\n    if not (1 <= k <= m):\n        raise ValueError(\"`k` must satisfy 1 \u2264 k \u2264 number of points.\")\n    if max_iters <= 0:\n        raise ValueError(\"`max_iters` must be positive.\")\n\n    # --- initialisation ------------------------------------------------------\n    centres = X[:k].copy()                # (k, d) \u2013 first k points, deterministic\n    labels  = np.full(m, -1, dtype=int)   # previous assignment (initially none)\n\n    # --- Lloyd iterations ----------------------------------------------------\n    for _ in range(max_iters):\n        # Assignment step -----------------------------------------------------\n        # squared Euclidean distances \u2013 avoids the costly square root,\n        # argmin still gives the same result and breaks ties towards\n        # the smaller index automatically.\n        diff   = X[:, None, :] - centres[None, :, :]   # (m, k, d)\n        dist2  = np.sum(diff ** 2, axis=2)             # (m, k)\n        new_labels = np.argmin(dist2, axis=1)          # (m,)\n\n        # Convergence test ----------------------------------------------------\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update step ---------------------------------------------------------\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):                           # centre received points\n                centres[i] = X[mask].mean(axis=0)\n            # else: centre keeps its previous position (per task statement)\n\n    # --- rounding of the final centres ---------------------------------------\n    centres_rounded = [[round(val, 4) for val in centre] for centre in centres]\n\n    return labels.tolist(), centres_rounded\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch GD.\"\"\"\n    \n    m, n = X.shape                          # m \u2026 samples, n \u2026 features\n    \n    # Return the initial parameters if nothing can change\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n\n    w = np.zeros(n, dtype=float)            # weights w1 \u2026 wn\n    b = 0.0                                 # bias\n\n    for _ in range(iterations):\n        y_hat = X @ w + b                   # predictions\n        r = y - y_hat                       # residuals\n\n        # sign(w) with the special rule sign(0) == -1\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n\n        # Gradients\n        grad_w = (-2 * X.T @ r) + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * r.sum()\n\n        # Mean gradients and parameter update\n        w -= learning_rate * (grad_w / m)\n        b -= learning_rate * (grad_b / m)\n\n    # Round the results to four decimal places and return as Python types\n    weights_rounded = [round(float(val), 4) for val in w]\n    bias_rounded = round(float(b), 4)\n\n    return (weights_rounded, bias_rounded)\n```"}
{"task_id": 582, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nearest_neighbor_graph(X):\n    \"\"\"Build the undirected k-nearest-neighbour adjacency matrix.\n\n    Parameters\n    ----------\n    X : Sequence[Sequence[float]]\n        2-D array-like object of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix W where W[i][j] is 1 iff an undirected edge exists\n        between sample i and sample j, otherwise 0.\n    \"\"\"\n    # Convert to numpy array\n    X = np.asarray(X, dtype=float)\n    n = X.shape[0]\n\n    # Trivial case ------------------------------------------------------------\n    if n <= 1:\n        return [[0]] if n == 1 else []\n\n    # Determine k -------------------------------------------------------------\n    k = min(int(np.sqrt(n)), 10)\n    if k == 0:      # Happens only when n==1, already handled\n        k = 1\n\n    # Pair-wise squared Euclidean distances ----------------------------------\n    # (squared distances are enough for ranking purposes)\n    diff = X[:, None, :] - X[None, :, :]         # shape (n, n, d)\n    dist2 = np.sum(diff ** 2, axis=-1)           # shape (n, n)\n\n    # Directed k-NN adjacency -------------------------------------------------\n    directed = np.zeros((n, n), dtype=bool)\n\n    for i in range(n):\n        # argsort puts smaller distances first; self-index i is distance 0\n        order = np.argsort(dist2[i])\n        # Skip the point itself and take first k neighbours\n        neighbours = [idx for idx in order if idx != i][:k]\n        directed[i, neighbours] = True\n\n    # Symmetrise --------------------------------------------------------------\n    undirected = (directed | directed.T).astype(int)\n\n    # Return as list of lists of ints ----------------------------------------\n    return undirected.tolist()\n```\n\nExplanation of the key steps\n1. `k = min(\u230a\u221an\u230b, 10)` as required.\n2. Pair-wise squared distances (`dist2`) are used only for ordering, saving the costly square root.\n3. For every point `i` the indices are sorted, `i` itself is skipped and the first `k` neighbours are marked in a boolean adjacency matrix `directed`.\n4. The final undirected adjacency is obtained by a logical OR with its transpose.\n5. Converted to a plain Python list of lists of integers (0/1) before returning."}
{"task_id": 584, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef make_initial_state(state: list[list[int | float]]) -> list[list[list[int | float]]]:\n    \"\"\"Stack a single 2-D Atari frame four times to form the initial state.\n\n    Args:\n        state: 2-D list (height \u00d7 width) representing the grayscale frame.\n\n    Returns:\n        3-D list with shape (height \u00d7 width \u00d7 4) where the same frame is\n        repeated along the last axis.\n    \"\"\"\n    # Convert the 2-D Python list to a NumPy array\n    frame = np.asarray(state)\n\n    # Add a new last axis (shape becomes H \u00d7 W \u00d7 1) and repeat it 4 times\n    stacked_state = np.repeat(frame[..., np.newaxis], 4, axis=-1)\n\n    # Convert back to a regular nested list before returning\n    return stacked_state.tolist()\n```"}
{"task_id": 585, "completion_id": 0, "completion": "```python\ndef cmp(a: object, b: object) -> int:\n    \"\"\"Compares two values and returns their ordering.\n\n    The function mimics Python 2's ``cmp``. It returns:\n        \u2022  1  if ``a`` is greater than ``b``\n        \u2022  0  if ``a`` equals ``b``\n        \u2022 -1  if ``a`` is less than ``b``\n\n    Args:\n        a: Any Python object supporting comparison operations.\n        b: Any Python object supporting comparison operations.\n\n    Returns:\n        int: -1, 0, or 1 indicating the comparison result.\n    \"\"\"\n    # True and False are treated as 1 and 0 respectively, so\n    # (a > b) - (a < b) yields exactly the desired mapping.\n    return (a > b) - (a < b)\n```"}
{"task_id": 586, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef lowess_predict(x: np.ndarray, y: np.ndarray, x_query: float, window: int) -> float:\n    \"\"\"Single-point LOWESS prediction (linear local model).\n\n    Parameters\n    ----------\n    x, y : np.ndarray\n        One-dimensional training data (same length).\n    x_query : float\n        Abscissa at which to predict.\n    window : int\n        Number of nearest neighbours to use (2 \u2264 window \u2264 len(x)).\n\n    Returns\n    -------\n    float\n        LOWESS prediction rounded to 4 decimals.\n    \"\"\"\n    # ---- basic validation --------------------------------------------------\n    if x.shape[0] != y.shape[0]:\n        raise ValueError(\"x and y must have the same length.\")\n    n = x.shape[0]\n    if not (2 <= window <= n):\n        raise ValueError(\"window must satisfy 2 \u2264 window \u2264 len(x).\")\n\n    # ---- helper to normalise a vector to [0,1] -----------------------------\n    def _normalise(arr):\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx == mn:                        # constant array -> all zeros\n            return np.zeros_like(arr, dtype=float), mn, mx\n        return (arr - mn) / (mx - mn), mn, mx\n\n    # ---- 1. normalise ------------------------------------------------------\n    x_norm, x_min, x_max = _normalise(x)\n    y_norm, y_min, y_max = _normalise(y)\n    # normalise the query point with the *same* transformation as x\n    if x_max == x_min:\n        xq_norm = 0.0\n    else:\n        xq_norm = (x_query - x_min) / (x_max - x_min)\n\n    # ---- 2. choose nearest neighbours --------------------------------------\n    distances = np.abs(x_norm - xq_norm)\n    neighbour_idx = np.argpartition(distances, window - 1)[:window]\n    # ensure we work with them in a consistent order (optional)\n    neighbour_idx = neighbour_idx[np.argsort(distances[neighbour_idx])]\n    d = distances[neighbour_idx]\n    x_sel = x_norm[neighbour_idx]\n    y_sel = y_norm[neighbour_idx]\n\n    # ---- 3. tricubic weights ----------------------------------------------\n    d_max = d.max()\n    if d_max == 0:                      # all neighbours at the query point\n        w = np.ones_like(d)\n    else:\n        u = d / d_max\n        w = (1 - u**3)**3\n        w[u > 1] = 0.0\n\n    # ---- 4. weighted linear regression ------------------------------------\n    w_sum = w.sum()\n    # Weighted means and denominator for the slope\n    wx = w * x_sel\n    wy = w * y_sel\n\n    x_bar = wx.sum() / w_sum\n    y_bar = wy.sum() / w_sum\n    b_den = (w * (x_sel - x_bar) ** 2).sum()        # \u03a3 w (x - x\u0304)\u00b2\n\n    if b_den == 0:                   # ill-conditioned -> slope = 0\n        y_hat_norm = y_bar\n    else:\n        b = (w * (x_sel - x_bar) * (y_sel - y_bar)).sum() / b_den\n        a = y_bar - b * x_bar\n        y_hat_norm = a + b * xq_norm\n\n    # ---- 5. back-transform to original y scale ----------------------------\n    if y_max == y_min:\n        y_hat = y_min                 # y was constant\n    else:\n        y_hat = y_hat_norm * (y_max - y_min) + y_min\n\n    # ---- 6. round and return ----------------------------------------------\n    return float(np.round(y_hat, 4))\n```"}
{"task_id": 587, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Any, List, Tuple\n\ndef manage_replay_memory(operations: List[Tuple]) -> List[Any]:\n    \"\"\"Execute a sequence of replay-buffer operations.\n\n    The buffer is a plain list that starts out empty.\n    \"\"\"\n    # deterministic randomness\n    random.seed(0)\n\n    buffer: List[Any] = []      # the replay memory\n    outputs: List[Any] = []     # collected results\n\n    for op in operations:\n        cmd = op[0]\n\n        if cmd == 'append':\n            # ('append', transition)\n            buffer.append(op[1])\n\n        elif cmd == 'pop':\n            # ('pop',)\n            val = buffer.pop() if buffer else None\n            outputs.append(val)\n\n        elif cmd == 'sample':\n            # ('sample', batch_size)\n            batch_size = op[1]\n            if batch_size > len(buffer):\n                outputs.append([])\n            else:\n                outputs.append(random.sample(buffer, batch_size))\n\n        elif cmd == 'size':\n            # ('size',)\n            outputs.append(len(buffer))\n\n        else:\n            raise ValueError(f\"Unsupported command: {cmd}\")\n\n    return outputs\n```"}
{"task_id": 588, "completion_id": 0, "completion": "```python\nfrom typing import Optional\n\nclass Node:\n    \"\"\"A simple binary tree node.\"\"\"\n    def __init__(self,\n                 data=None,\n                 split_variable=None,\n                 split_variable_value=None,\n                 left: Optional['Node'] = None,\n                 right: Optional['Node'] = None,\n                 depth: int = 0,\n                 criterion_value=None):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.criterion_value = criterion_value\n\n\ndef tree_max_depth(root: Optional[Node]) -> int:\n    \"\"\"Computes the maximum depth of a binary tree.\n\n    The depth of the root node is defined to be 0.\n    The depth of an empty tree is defined to be -1.\n    \"\"\"\n    # Base case: empty tree\n    if root is None:\n        return -1\n\n    # Recursively compute the maximum depth of the left and right sub-trees\n    left_depth = tree_max_depth(root.left)\n    right_depth = tree_max_depth(root.right)\n\n    # The depth of the current node is one more than the max depth of its children\n    return max(left_depth, right_depth) + 1\n```"}
{"task_id": 590, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np                   # <- duplicated on purpose, see template\n\ndef minesweeper_step(ground: list[list[float | int]],\n                     flags : list[list[bool]]\n                    ) -> tuple[list[tuple[int, int]],\n                               list[tuple[int, int]]]:\n    \"\"\"Return the squares that can be revealed (`clicks`) and the squares that\n    have to be bombs (`new_flags`) after **one** logical pass.\n\n    The deduction rules implemented are the standard Minesweeper rules:\n        \u2022 value == 0  \u2192  every hidden, un-flagged neighbour is safe.\n        \u2022 value == 8  \u2192  every hidden, un-flagged neighbour is a bomb.\n        \u2022 otherwise\n              bombs       = # already flagged neighbours\n              unexplored  = set(hidden & un-flagged neighbours)\n\n              \u2013 if bombs == value\n                    \u21d2 all squares in *unexplored* are safe   (clicks)\n              \u2013 elif value \u2013 bombs == len(unexplored)\n                    \u21d2 all squares in *unexplored* are bombs  (new_flags)\n\n    Only the *current* board state is taken into account; newly deduced\n    clicks/flags are **not** fed back into the analysis within the same call.\n    \"\"\"\n    rows  = len(ground)\n    cols  = len(ground[0]) if rows else 0\n\n    # Helper that yields neighbour coordinates (8-connected)\n    def neighbours(r: int, c: int):\n        for dr in (-1, 0, 1):\n            for dc in (-1, 0, 1):\n                if dr == 0 and dc == 0:\n                    continue\n                nr, nc = r + dr, c + dc\n                if 0 <= nr < rows and 0 <= nc < cols:\n                    yield nr, nc\n\n    clicks_set    : set[tuple[int, int]] = set()\n    new_flags_set : set[tuple[int, int]] = set()\n\n    for r in range(rows):\n        for c in range(cols):\n            # only look at *revealed* squares that are not themselves flagged\n            if np.isnan(ground[r][c]) or flags[r][c]:\n                continue\n\n            value        = int(ground[r][c])\n            unexplored   = []         # hidden & (not flagged)\n            bombs        = 0          # already flagged neighbours\n\n            for nr, nc in neighbours(r, c):\n                if flags[nr][nc]:\n                    bombs += 1\n                elif np.isnan(ground[nr][nc]):   # hidden AND not flagged\n                    unexplored.append((nr, nc))\n\n            # Rule application ------------------------------------------------\n            if value == 0:\n                clicks_set.update(unexplored)           # everything is safe\n\n            elif value == 8:\n                new_flags_set.update(unexplored)        # everything is a bomb\n\n            else:\n                if bombs == value:\n                    clicks_set.update(unexplored)\n\n                elif value - bombs == len(unexplored):\n                    new_flags_set.update(unexplored)\n\n    # Produce the requested, **sorted**, duplicate-free lists\n    clicks     = sorted(clicks_set)\n    new_flags  = sorted(new_flags_set)\n\n    return clicks, new_flags\n```\n"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Forward pass of a fully\u2013connected ReLU network with a linear output layer.\n\n    Args:\n        weights : Three-level nested list holding all weight matrices.\n        x       : Input features (without bias).\n\n    Returns:\n        Network prediction rounded to four decimals.\n        Returns a float for a single output neuron, otherwise a list of floats.\n    \"\"\"\n    # current layer input as a NumPy array\n    h = np.asarray(x, dtype=float)\n\n    n_layers = len(weights)\n\n    for L, layer in enumerate(weights):\n        # shape: (n_neurons, prev_layer_size + 1)\n        W = np.asarray(layer, dtype=float)\n\n        # prepend bias input \u201c1\u201d\n        h_aug = np.concatenate(([1.0], h))\n\n        # linear combination\n        z = W @ h_aug     # shape: (n_neurons,)\n\n        # activation: ReLU for hidden layers, identity for output layer\n        h = np.maximum(0, z) if L < n_layers - 1 else z\n\n    # round to 4 decimals\n    if h.size == 1:\n        return round(float(h[0]), 4)\n    else:\n        return [round(v, 4) for v in h.tolist()]\n```"}
{"task_id": 592, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef analyze_cell(clicked: np.ndarray, flags: np.ndarray, row: int, col: int) -> tuple[list[tuple[int, int]], int]:\n    \"\"\"Return unopened neighbouring cells and flagged-mine count for a board position.\"\"\"\n    n_rows, n_cols = clicked.shape\n\n    # quick exit for a 1\u00d71 board\n    if n_rows == 1 and n_cols == 1:\n        return [], 0\n\n    unopened: list[tuple[int, int]] = []\n    flagged_count = 0\n\n    # bounding box that surrounds the eight possible neighbours\n    r0, r1 = max(0, row - 1), min(n_rows - 1, row + 1)\n    c0, c1 = max(0, col - 1), min(n_cols - 1, col + 1)\n\n    for r in range(r0, r1 + 1):\n        for c in range(c0, c1 + 1):\n            if (r, c) == (row, col):           # skip the centre cell itself\n                continue\n            if not clicked[r, c]:              # still unrevealed \u2013 add to list\n                unopened.append((r, c))\n            if flags[r, c]:                    # count flagged neighbours\n                flagged_count += 1\n\n    unopened.sort()                            # lexicographic sort (row, then col)\n    return unopened, flagged_count\n```"}
{"task_id": 593, "completion_id": 0, "completion": "```python\nfrom copy import deepcopy\nfrom typing import List, Tuple, Set, FrozenSet\n\ndef deduce_mines(constraints: List[List]) -> Tuple[List[str], List[str]]:\n    \"\"\"Simplify a list of Minesweeper constraints.\n\n    Each *constraint* is a pair  [variables, value]  meaning\n    'exactly *value* of the listed *variables* hide a mine'.\n\n    The function applies, in a loop, the following rules\n\n        1. solved constraints          (all-safe / all-mine)\n        2. propagation of knowledge    (delete known vars, adjust value)\n        3. subset rule                 (S1\u2282S2  \u21d2  S2 := S2\\S1 ,  v2 := v2\u2212v1)\n        4. duplicate removal\n\n    until no more information can be gained.\n\n    It returns two alphabetically sorted lists:\n        (certainly_safe_variables, certainly_mine_variables)\n    \"\"\"\n    # -------------------------------------------------------------------------\n    # helper functions\n    # -------------------------------------------------------------------------\n    def remove_duplicates(cons: List[Tuple[FrozenSet, int]]\n                          ) -> List[Tuple[Set[str], int]]:\n        \"\"\"Drop identical (set,value) pairs, keep only one copy.\"\"\"\n        unique = set(cons)          # a set of tuples is enough\n        return [(set(vs), val) for (vs, val) in unique]\n\n    def propagate(cons: List[Tuple[Set[str], int]],\n                  safe: Set[str], mines: Set[str]\n                  ) -> List[Tuple[Set[str], int]]:\n        \"\"\"Delete already classified vars, adjust values for known mines.\"\"\"\n        new_cons: List[Tuple[Set[str], int]] = []\n\n        for varset, val in cons:\n            # mines inside this constraint already accounted for\n            mine_cnt = len(varset & mines)\n            val -= mine_cnt\n            varset = varset - mines - safe        # drop all known vars\n\n            if not varset:                        # empty constraint\n                # if val != 0 an inconsistency would have been found \u2013 ignore\n                continue\n            new_cons.append((varset, val))\n\n        return new_cons\n\n    def apply_subset_rule(cons: List[Tuple[Set[str], int]]\n                          ) -> Tuple[List[Tuple[Set[str], int]], bool]:\n        \"\"\"Apply subset rule once.  Return (new_constraints, changed?).\"\"\"\n        changed = False\n        n = len(cons)\n        for i in range(n):\n            Si, vi = cons[i]\n            for j in range(n):\n                if i == j:\n                    continue\n                Sj, vj = cons[j]\n                if Si < Sj:                       # strict subset\n                    # replace Cj by the reduced constraint\n                    cons[j] = (Sj - Si, vj - vi)\n                    changed = True\n                    break            # restart scan \u2013 structure changed\n            if changed:\n                break\n\n        return cons, changed\n\n    # -------------------------------------------------------------------------\n    # set-up\n    # -------------------------------------------------------------------------\n    # Copy input and convert variable lists to sets (easier handling)\n    constraints = [(set(vars_), val) for vars_, val in deepcopy(constraints)]\n    safe:  Set[str] = set()\n    mines: Set[str] = set()\n\n    # -------------------------------------------------------------------------\n    # main fix-point loop\n    # -------------------------------------------------------------------------\n    changed_globally = True\n    while changed_globally:\n        changed_globally = False\n\n        # --------------------------------------------------------------\n        # 1. solved constraints  (all safe / all mine)\n        # --------------------------------------------------------------\n        newly_safe:  Set[str] = set()\n        newly_mine:  Set[str] = set()\n\n        for varset, val in constraints:\n            if val == 0:                         # all safe\n                newly_safe |= varset\n            elif val == len(varset):             # all mines\n                newly_mine |= varset\n\n        # remember only really new information\n        if newly_safe - safe or newly_mine - mines:\n            safe  |= newly_safe\n            mines |= newly_mine\n            changed_globally = True\n\n        # --------------------------------------------------------------\n        # 2. propagate knowledge\n        # --------------------------------------------------------------\n        constraints = propagate(constraints, safe, mines)\n\n        # --------------------------------------------------------------\n        # 3. subset rule (repeat until nothing more changes)\n        # --------------------------------------------------------------\n        subset_changed = True\n        while subset_changed:\n            constraints, subset_changed = apply_subset_rule(constraints)\n            if subset_changed:\n                changed_globally = True\n                # after each structural change duplicates may arise\n                constraints = remove_duplicates([\n                    (set(vs), val) for vs, val in constraints\n                ])\n\n        # --------------------------------------------------------------\n        # 4. duplicate removal (final for this outer iteration)\n        # --------------------------------------------------------------\n        before = len(constraints)\n        constraints = remove_duplicates([\n            (set(vs), val) for vs, val in constraints\n        ])\n        if len(constraints) != before:\n            changed_globally = True\n\n    # -------------------------------------------------------------------------\n    # result\n    # -------------------------------------------------------------------------\n    return (sorted(safe), sorted(mines))\n```"}
{"task_id": 594, "completion_id": 0, "completion": "```python\ndef maze_to_graph(maze: list[list[int]]) -> dict[tuple[int, int], list[tuple[int, int]]]:\n    \"\"\"Convert a binary maze into an adjacency-list graph.\n\n    A 0 in the maze denotes an open cell, while 1 denotes a wall.  \n    Two open cells are adjacent if they share an edge (up, down, left, right).\n\n    Args:\n        maze: Rectangular 2-D list containing only 0s and 1s.\n\n    Returns:\n        A dictionary whose keys are coordinates (row, col) for every open\n        cell, and whose values are *sorted* lists of orthogonally adjacent\n        open-cell coordinates.  If the maze has no open cells the function\n        returns an empty dictionary.\n    \"\"\"\n    if not maze:                     # Empty outer list \u2794 no cells at all\n        return {}\n\n    rows, cols = len(maze), len(maze[0])\n    graph: dict[tuple[int, int], list[tuple[int, int]]] = {}\n\n    # Pre-compute the set of open cells for O(1) membership tests.\n    open_cells = {(r, c) for r in range(rows) for c in range(cols) if maze[r][c] == 0}\n\n    if not open_cells:               # No 0s in the maze\n        return {}\n\n    # Directions: up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    for r, c in open_cells:\n        neighbours: list[tuple[int, int]] = []\n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            if (nr, nc) in open_cells:      # within bounds & open\n                neighbours.append((nr, nc))\n\n        # Sort to satisfy deterministic order requirement\n        neighbours.sort()\n        graph[(r, c)] = neighbours\n\n    return graph\n```"}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering \u2013 return **centroids only**.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    k : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids (lexicographically), each coordinate rounded to 4 decimals.\n    \"\"\"\n    # ------ basic sanity check ------------------------------------------------\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-dimensional\")\n\n    X = X.astype(float, copy=False)           # ensure float for arithmetic\n    n_samples, n_features = X.shape\n    if k > n_samples:\n        raise ValueError(\"k cannot be larger than number of samples\")\n    \n    # ------ 1. deterministic initialisation ----------------------------------\n    centroids = X[:k].copy()                 # shape (k, n_features)\n\n    # ------ 2. main loop ------------------------------------------------------\n    for _ in range(max_iters):\n        # 2.a assign to nearest centroid (squared Euclidean distance)\n        #    distances: (n_samples, k)\n        distances = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # 2.b update centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):                          # cluster non-empty\n                new_centroids[j] = X[mask].mean(axis=0)\n            # else: keep old centroid unchanged\n\n        # 3. early stopping\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 1e-4):\n            break\n\n    # ------ 4. final formatting ----------------------------------------------\n    # sort lexicographically\n    centroids_list = centroids.tolist()\n    centroids_list.sort()                    # python\u2019s default list sort is lexicographic\n\n    # round each coordinate to 4 decimals\n    rounded = [[round(val, 4) for val in centre] for centre in centroids_list]\n    return rounded\n```"}
{"task_id": 596, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef convert_maze_to_graph(maze: np.ndarray) -> dict:\n    \"\"\"Convert a binary maze to an adjacency-list representation.\n\n    Each free cell (value 1) becomes a vertex.  Two vertices share an\n    (undirected, un-weighted) edge iff the corresponding cells are adjacent\n    in the four-neighbourhood (up, down, left, right).\n\n    A dictionary mapping every free cell coordinate (row, col) to a *sorted*\n    list of adjacent free cells is returned.  If the maze contains no free\n    cells, an empty dictionary is returned.\n    \"\"\"\n    # Empty array \u21d2 no vertices\n    if maze.size == 0:\n        return {}\n\n    rows, cols = maze.shape\n\n    # All coordinates whose value is 1\n    free_cells = np.argwhere(maze == 1)\n    if free_cells.size == 0:\n        return {}\n\n    # Build adjacency list with sets first (to avoid duplicates)\n    adjacency = {tuple(cell): set() for cell in free_cells}\n\n    # 4-neighbour directions: up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    for r, c in free_cells:\n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            # Check bounds and whether neighbour is free\n            if 0 <= nr < rows and 0 <= nc < cols and maze[nr, nc] == 1:\n                adjacency[(r, c)].add((nr, nc))\n                adjacency[(nr, nc)].add((r, c))   # ensure undirected edge\n\n    # Convert neighbour sets to *sorted* lists\n    return {cell: sorted(neighbours) for cell, neighbours in adjacency.items()}\n```"}
{"task_id": 597, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef best_split_mse(X: list | np.ndarray, y: list | np.ndarray) -> list:\n    \"\"\"Return the best (feature, threshold) split for a regression tree.\n\n    A split is evaluated with the weighted mean\u2013squared error (population\n    variance) of the two resulting groups.  Only thresholds that coincide\n    with an existing feature value are examined.\n\n    Parameters\n    ----------\n    X : list | np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : list | np.ndarray\n        Target vector of shape (n_samples,).\n\n    Returns\n    -------\n    list\n        [best_feature_index, best_threshold, best_mse] where *best_mse*\n        is rounded to 4 decimals.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # convert to numpy arrays (copy is cheap, simplifies the rest)\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n_samples, n_features = X.shape\n\n    # keep some pre-allocated work space for speed\n    best_feature = None\n    best_threshold = None\n    best_mse = np.inf\n\n    # tiny tolerance used for \u201cequality\u201d\n    eps = 1e-12\n\n    # ------------------------------------------------------------------\n    for j in range(n_features):\n        x_col = X[:, j]\n\n        # sort the column together with y \u2013 stable sort keeps order of\n        # identical x values, which is handy but not strictly required\n        order = np.argsort(x_col, kind=\"mergesort\")\n        xs = x_col[order]\n        ys = y[order]\n\n        # cumulative sums for constant-time left/right statistics\n        csum = np.cumsum(ys, dtype=float)\n        csum_sq = np.cumsum(ys ** 2, dtype=float)\n        total_sum = csum[-1]\n        total_sq_sum = csum_sq[-1]\n\n        # possible split positions: *after* the last occurrence of each\n        # unique value  ->  where the next value differs\n        change_pos = np.where(np.diff(xs) != 0)[0]        # indices 0 \u2026 n-2\n        if change_pos.size == 0:                          # no valid split in this feature\n            continue\n\n        for idx in change_pos:\n            left_n = idx + 1\n            right_n = n_samples - left_n\n            if right_n == 0:          # shouldn\u2019t happen because of diff!=0, but be safe\n                continue\n\n            # ---- left side statistics\n            sum_left = csum[idx]\n            sq_left = csum_sq[idx]\n            mean_left = sum_left / left_n\n            var_left = sq_left / left_n - mean_left ** 2    # population variance\n\n            # ---- right side statistics\n            sum_right = total_sum - sum_left\n            sq_right = total_sq_sum - sq_left\n            mean_right = sum_right / right_n\n            var_right = sq_right / right_n - mean_right ** 2\n\n            # ---- weighted MSE of the split\n            mse = (left_n / n_samples) * var_left + (right_n / n_samples) * var_right\n\n            # ---- keep best according to tie-breaking rules\n            update = False\n            if mse < best_mse - eps:\n                update = True\n            elif abs(mse - best_mse) <= eps:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and xs[idx] < best_threshold - eps:\n                    update = True\n\n            if update:\n                best_feature = j\n                best_threshold = xs[idx]\n                best_mse = mse\n\n    # ------------------------------------------------------------------\n    return [int(best_feature),\n            float(best_threshold),\n            round(float(best_mse), 4)]\n```"}
{"task_id": 598, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef standardize(data: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Standardise a numeric data matrix feature-wise (z-scores).\n\n    Each column of *data* is centred to mean 0 and scaled to variance 1\n    (population, i.e. ddof=0).  Constant columns (\u03c3 == 0) are set to 0.\n    Any NaN/Inf produced during the computation are replaced by 0.\n    The result is rounded to 4 decimals and returned as a plain list.\n\n    Args:\n        data: 2-D list (n_samples \u00d7 n_features) of numbers.\n\n    Returns:\n        list[list[float]]: Standardised matrix with the same shape.\n    \"\"\"\n    # Gracefully handle completely empty input\n    if not data:\n        return []\n\n    # Convert to NumPy array of floats\n    arr = np.asarray(data, dtype=float)\n\n    # If shape is (n_samples,) -> treat as single feature column\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n\n    # Compute feature means and (population) standard deviations\n    mean = arr.mean(axis=0)\n    std = arr.std(axis=0, ddof=0)\n\n    # Avoid division by zero: temporarily set zeros to 1\n    std_safe = std.copy()\n    std_safe[std_safe == 0] = 1.0\n\n    # Z-score transformation\n    z = (arr - mean) / std_safe\n\n    # Put back zeros for constant columns\n    z[:, std == 0] = 0.0\n\n    # Replace NaN and \u00b1Inf with 0\n    z = np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Round to 4 decimals\n    z = np.round(z, 4)\n\n    return z.tolist()\n```"}
