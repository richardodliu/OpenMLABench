{"task_id": 1, "completion_id": 0, "completion": "To solve the problem of computing the dot product of a matrix and a vector, we need to ensure that the number of columns in the matrix matches the length of the vector. If this condition is met, we can proceed to calculate the dot product for each row of the matrix with the vector. Here's how you can implement this in Python:\n\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the matrix 'a' is empty\n    if not a:\n        return -1\n    \n    # Get the number of columns in the matrix 'a'\n    num_columns = len(a[0])\n    \n    # Check if the number of columns in 'a' matches the length of vector 'b'\n    if num_columns != len(b):\n        return -1\n    \n    # Initialize the result list\n    result = []\n    \n    # Iterate over each row in the matrix 'a'\n    for row in a:\n        # Calculate the dot product of the current row with vector 'b'\n        dot_product = sum(row[i] * b[i] for i in range(num_columns))\n        # Append the result to the result list\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nvector = [1, 0, -1]\n\nprint(matrix_dot_vector(matrix, vector))  # Output: [-2, -2, -2]\n```\n\n### Explanation:\n\n1. **Input Validation**: \n   - First, we check if the matrix `a` is empty. If it is, we return `-1` because an empty matrix cannot be dotted with a vector.\n   - We then determine the number of columns in the matrix by checking the length of the first row.\n\n2. **Dimension Check**:\n   - We compare the number of columns in the matrix with the length of the vector `b`. If they do not match, the dot product is not defined, and we return `-1`.\n\n3. **Dot Product Calculation**:\n   - We initialize an empty list `result` to store the dot products.\n   - For each row in the matrix, we calculate the dot product with the vector `b` using a list comprehension and the `sum` function. The dot product for a row is computed as the sum of element-wise products of the row and the vector.\n   - We append each calculated dot product to the `result` list.\n\n4. **Return the Result**:\n   - Finally, we return the `result` list, which contains the dot products of each row of the matrix with the vector.\n\nThis implementation efficiently computes the matrix-vector dot product while handling dimension mismatches gracefully."}
{"task_id": 2, "completion_id": 0, "completion": "To compute the transpose of a given matrix in Python, you can use a simple function that iterates over the rows and columns of the matrix and rearranges them. Here's how you can implement the `transpose_matrix` function:\n\n```python\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if the input matrix is empty\n    if not a or not a[0]:\n        return []\n\n    # Get the number of rows and columns in the original matrix\n    num_rows = len(a)\n    num_cols = len(a[0])\n\n    # Initialize the transposed matrix with the dimensions swapped\n    transposed = [[0] * num_rows for _ in range(num_cols)]\n\n    # Fill the transposed matrix\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed[j][i] = a[i][j]\n\n    return transposed\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\ntransposed_matrix = transpose_matrix(matrix)\nprint(transposed_matrix)\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the input matrix is empty or if it has no columns. If so, it returns an empty list.\n- **Matrix Dimensions**: It calculates the number of rows and columns in the input matrix.\n- **Initialize Transposed Matrix**: A new matrix is initialized with dimensions swapped (i.e., the number of rows becomes the number of columns and vice versa).\n- **Transpose Logic**: The function iterates over each element of the input matrix and assigns it to the appropriate position in the transposed matrix.\n- **Return**: Finally, the transposed matrix is returned.\n\nThis function will correctly transpose any rectangular matrix of integers or floats."}
{"task_id": 3, "completion_id": 0, "completion": "To accomplish the task of reshaping a matrix using Python and NumPy, we need to ensure that the total number of elements in the original matrix matches the total number of elements in the desired new shape. If they don't match, the matrix cannot be reshaped, and we should return an empty list. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Calculate the total number of elements in the original matrix\n    original_elements = sum(len(row) for row in a)\n    \n    # Calculate the total number of elements required for the new shape\n    new_elements = new_shape[0] * new_shape[1]\n    \n    # Check if reshaping is possible\n    if original_elements != new_elements:\n        return []\n    \n    # Convert the input list to a NumPy array\n    np_array = np.array(a)\n    \n    # Reshape the array using NumPy\n    reshaped_array = np_array.reshape(new_shape)\n    \n    # Convert the reshaped NumPy array back to a list\n    reshaped_list = reshaped_array.tolist()\n    \n    return reshaped_list\n\n# Example usage:\noriginal_matrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (3, 2)\nprint(reshape_matrix(original_matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n```\n\n### Explanation:\n1. **Element Count Check**: We first calculate the total number of elements in the original matrix and compare it with the number of elements required for the new shape. If they don't match, reshaping is not possible, and we return an empty list.\n\n2. **Conversion to NumPy Array**: We convert the input list to a NumPy array to leverage NumPy's reshaping capabilities.\n\n3. **Reshape**: We use NumPy's `reshape` method to reshape the array to the desired shape.\n\n4. **Convert Back to List**: Finally, we convert the reshaped NumPy array back to a list using the `tolist()` method, as required by the task constraints.\n\nThis approach ensures that the matrix is reshaped correctly if possible, and handles cases where reshaping is not feasible by returning an empty list."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this task, we need to write a function that calculates the mean of a matrix either by row or by column, depending on the specified mode. Here's how you can implement this function:\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        return [sum(row) / len(row) for row in matrix]\n    \n    elif mode == 'column':\n        # Calculate mean for each column\n        num_columns = len(matrix[0])\n        num_rows = len(matrix)\n        \n        # Initialize a list to store the sum of each column\n        column_sums = [0] * num_columns\n        \n        # Sum up each column\n        for row in matrix:\n            for col_index in range(num_columns):\n                column_sums[col_index] += row[col_index]\n        \n        # Calculate mean for each column\n        return [column_sum / num_rows for column_sum in column_sums]\n\n# Example usage:\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(calculate_matrix_mean(matrix, 'row'))    # Output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix, 'column')) # Output: [4.0, 5.0, 6.0]\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the mode is either 'row' or 'column'. If not, it raises a `ValueError`.\n- **Row Mean Calculation**: If the mode is 'row', it calculates the mean for each row by summing the elements of the row and dividing by the number of elements.\n- **Column Mean Calculation**: If the mode is 'column', it first calculates the sum of each column by iterating over each row and adding the respective column values. Then, it divides each column sum by the number of rows to get the mean.\n- **Example Usage**: The example demonstrates how to use the function with a sample matrix and both modes."}
{"task_id": 5, "completion_id": 0, "completion": "To perform scalar multiplication of a matrix, you need to multiply each element of the matrix by the given scalar. Below is the implementation of the `scalar_multiply` function:\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    # Create a new matrix to store the result\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Create a new row to store the multiplied values\n        new_row = []\n        \n        # Iterate over each element in the row\n        for element in row:\n            # Multiply the element by the scalar and append to the new row\n            new_row.append(element * scalar)\n        \n        # Append the new row to the result matrix\n        result.append(new_row)\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nscalar = 2\n\nresult = scalar_multiply(matrix, scalar)\nprint(result)\n```\n\n### Explanation:\n- The function `scalar_multiply` takes two arguments: `matrix`, which is a list of lists containing integers or floats, and `scalar`, which is an integer or float.\n- It initializes an empty list `result` to store the resulting matrix.\n- It iterates over each row in the input matrix, and for each row, it creates a new list `new_row`.\n- It then iterates over each element in the current row, multiplies it by the scalar, and appends the result to `new_row`.\n- After processing all elements in the row, `new_row` is appended to `result`.\n- Finally, the function returns the `result` matrix, which contains the scalar-multiplied values."}
{"task_id": 6, "completion_id": 0, "completion": "To calculate the eigenvalues of a 2x2 matrix, we can use the characteristic equation of the matrix. For a 2x2 matrix:\n\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThe characteristic equation is given by:\n\n\\[ \\text{det}(A - \\lambda I) = 0 \\]\n\nWhere \\( I \\) is the identity matrix and \\( \\lambda \\) represents the eigenvalues. The determinant of \\( A - \\lambda I \\) is:\n\n\\[ \\begin{vmatrix} a - \\lambda & b \\\\ c & d - \\lambda \\end{vmatrix} = (a - \\lambda)(d - \\lambda) - bc \\]\n\nThis simplifies to the quadratic equation:\n\n\\[ \\lambda^2 - (a + d)\\lambda + (ad - bc) = 0 \\]\n\nThe solutions to this quadratic equation are the eigenvalues, which can be found using the quadratic formula:\n\n\\[ \\lambda = \\frac{-(b) \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\n\nIn our case, \\( a = 1 \\), \\( b = -(a + d) \\), and \\( c = ad - bc \\).\n\nHere's the Python function to calculate the eigenvalues:\n\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the coefficients of the characteristic polynomial\n    trace = a + d\n    determinant = a * d - b * c\n    \n    # Calculate the eigenvalues using the quadratic formula\n    term = math.sqrt(trace**2 - 4 * determinant)\n    eigenvalue1 = (trace + term) / 2\n    eigenvalue2 = (trace - term) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\nmatrix = [[4, 2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)\nprint(eigenvalues)  # Output: [5.0, 2.0]\n```\n\nThis function calculates the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest. The `math.sqrt` function is used to compute the square root needed for the quadratic formula."}
{"task_id": 7, "completion_id": 0, "completion": "To solve the problem of transforming a matrix \\( A \\) using the operation \\( T^{-1}AS \\), we need to ensure that matrices \\( T \\) and \\( S \\) are invertible. If they are not, the function should return -1. If they are invertible, we perform the matrix multiplication and return the result rounded to the nearest 4th decimal.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert lists to numpy arrays\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible\n    try:\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)  # Not needed, but checking invertibility\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Perform the matrix transformation T^{-1}AS\n    transformed_matrix = T_inv @ A_np @ S_np\n    \n    # Round the results to the nearest 4th decimal\n    transformed_matrix_rounded = np.round(transformed_matrix, 4)\n    \n    # Convert the result back to a list\n    return transformed_matrix_rounded.tolist()\n\n# Example usage:\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[2, 0], [0, 2]]\n\nresult = transform_matrix(A, T, S)\nprint(result)\n```\n\n### Explanation:\n1. **Matrix Conversion**: Convert the input lists to NumPy arrays for easier manipulation and computation.\n2. **Invertibility Check**: Ensure that matrices \\( T \\) and \\( S \\) are square and attempt to compute their inverses. If either matrix is not invertible, return -1.\n3. **Matrix Transformation**: Compute the product \\( T^{-1}AS \\) using NumPy's matrix multiplication.\n4. **Rounding**: Round the elements of the resulting matrix to the nearest 4th decimal.\n5. **Return**: Convert the NumPy array back to a list using the `tolist()` method and return it.\n\nThis function handles the matrix transformation while ensuring the constraints are met."}
{"task_id": 8, "completion_id": 0, "completion": "To calculate the inverse of a 2x2 matrix, we need to use the formula for the inverse of a 2x2 matrix. Given a matrix:\n\n\\[\n\\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\]\n\nThe inverse of this matrix is:\n\n\\[\n\\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{bmatrix}\n\\]\n\nThe matrix is invertible if and only if the determinant \\(ad - bc\\) is not zero. If the determinant is zero, the matrix is not invertible, and we should return `None`.\n\nHere's how you can implement this in Python:\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    determinant = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse using the formula\n    inverse_matrix = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    return inverse_matrix\n\n# Example usage:\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)\nprint(inverse)  # Output: [[0.6, -0.7], [-0.2, 0.4]]\n```\n\nThis function first calculates the determinant of the matrix. If the determinant is zero, it returns `None`, indicating that the matrix is not invertible. Otherwise, it calculates the inverse using the formula and returns the resulting matrix."}
{"task_id": 9, "completion_id": 0, "completion": "To multiply two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix. If this condition is not met, the matrices cannot be multiplied, and the function should return -1. If the matrices can be multiplied, the function should return the resulting matrix.\n\nHere's the implementation of the `matrixmul` function:\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if matrices can be multiplied\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n```\n\n### Explanation:\n\n1. **Dimension Check**: The function first checks if the number of columns in matrix `a` is equal to the number of rows in matrix `b`. If not, it returns `-1`.\n\n2. **Result Matrix Initialization**: It initializes a result matrix with dimensions equal to the number of rows in `a` and the number of columns in `b`, filled with zeros.\n\n3. **Matrix Multiplication**: It uses three nested loops to perform the multiplication:\n   - The outer loop iterates over the rows of matrix `a`.\n   - The middle loop iterates over the columns of matrix `b`.\n   - The innermost loop iterates over the columns of matrix `a` (or equivalently, the rows of matrix `b`) to calculate the dot product for each element in the result matrix.\n\n4. **Return Result**: Finally, it returns the resulting matrix after multiplication."}
{"task_id": 10, "completion_id": 0, "completion": "To calculate the covariance matrix for a given set of vectors, we need to follow these steps:\n\n1. **Calculate the Mean**: Compute the mean of each feature (each inner list).\n2. **Center the Data**: Subtract the mean of each feature from the corresponding feature values.\n3. **Compute the Covariance**: For each pair of features, calculate the covariance by taking the dot product of the centered data and dividing by the number of observations minus one.\n\nHere's the implementation of the function `calculate_covariance_matrix`:\n\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors or not all(len(v) == len(vectors[0]) for v in vectors):\n        raise ValueError(\"All vectors must have the same number of observations.\")\n    \n    num_features = len(vectors)\n    num_observations = len(vectors[0])\n    \n    # Calculate means of each feature\n    means = [sum(feature) / num_observations for feature in vectors]\n    \n    # Center the data by subtracting the mean\n    centered_data = [\n        [value - means[i] for value in feature]\n        for i, feature in enumerate(vectors)\n    ]\n    \n    # Calculate the covariance matrix\n    covariance_matrix = [\n        [\n            sum(centered_data[i][k] * centered_data[j][k] for k in range(num_observations)) / (num_observations - 1)\n            for j in range(num_features)\n        ]\n        for i in range(num_features)\n    ]\n    \n    return covariance_matrix\n\n# Test cases to verify the correctness of the implementation\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2D data\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    expected1 = [\n        [1.0, 1.0],\n        [1.0, 1.0]\n    ]\n    assert calculate_covariance_matrix(vectors1) == expected1, \"Test case 1 failed\"\n\n    # Test case 2: 3D data\n    vectors2 = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    expected2 = [\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0]\n    ]\n    assert calculate_covariance_matrix(vectors2) == expected2, \"Test case 2 failed\"\n\n    # Test case 3: More complex data\n    vectors3 = [\n        [2.1, 2.5, 3.6, 4.0],\n        [8.0, 10.0, 12.0, 14.0]\n    ]\n    expected3 = [\n        [0.9166666666666666, 2.833333333333333],\n        [2.833333333333333, 8.333333333333334]\n    ]\n    assert calculate_covariance_matrix(vectors3) == expected3, \"Test case 3 failed\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\n### Explanation:\n\n- **Input Validation**: The function first checks if the input is valid, ensuring all vectors have the same number of observations.\n- **Mean Calculation**: The mean of each feature is calculated.\n- **Centering the Data**: Each feature is centered by subtracting its mean.\n- **Covariance Calculation**: The covariance between each pair of features is calculated using the centered data.\n\nThe test cases provided verify the correctness of the implementation by comparing the output of the function with the expected covariance matrices for different sets of input vectors."}
{"task_id": 11, "completion_id": 0, "completion": "To solve a system of linear equations using the Jacobi method, we need to iteratively update the solution vector until it converges or until a specified number of iterations is reached. The Jacobi method is an iterative algorithm that updates each component of the solution vector independently using the values from the previous iteration.\n\nHere's how you can implement the Jacobi method in Python:\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Number of variables\n    num_vars = len(b)\n    \n    # Initial guess (zero vector)\n    x = np.zeros(num_vars)\n    \n    # Iterate n times\n    for _ in range(n):\n        # Create a new array for the next iteration\n        x_new = np.zeros(num_vars)\n        \n        for i in range(num_vars):\n            # Sum the known values\n            sum_ = sum(A[i][j] * x[j] for j in range(num_vars) if j != i)\n            \n            # Update the ith variable\n            x_new[i] = (b[i] - sum_) / A[i][i]\n        \n        # Round the new solution to 4 decimal places\n        x = np.round(x_new, 4)\n    \n    # Return the solution as a list\n    return x.tolist()\n\n# Example usage:\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\nn = 25\n\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\n### Explanation:\n\n1. **Initialization**: We start with an initial guess for the solution vector `x`, which is a zero vector in this case.\n\n2. **Iteration**: For each iteration, we compute a new solution vector `x_new`:\n   - For each variable `i`, we calculate the sum of the products of the coefficients and the current values of the other variables.\n   - We then update the `i-th` variable using the formula derived from the Jacobi method: \n     \\[\n     x_i^{(k+1)} = \\frac{b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}}{a_{ii}}\n     \\]\n   - We round each component of the new solution vector to four decimal places.\n\n3. **Return**: After completing the specified number of iterations, we return the solution vector as a list.\n\nThis implementation assumes that the matrix `A` is diagonally dominant, which is a common requirement for the convergence of the Jacobi method. If `A` is not diagonally dominant, the method may not converge."}
{"task_id": 12, "completion_id": 0, "completion": "To approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobi method, we need to iteratively apply rotations to zero out the off-diagonal elements. Here's a Python function that performs this task:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure the input is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input matrix must be 2x2.\"\n\n    # Initialize the rotation matrix\n    U = np.eye(2)\n    V = np.eye(2)\n    S = np.copy(A)\n\n    # Jacobi method for 2x2 matrix\n    for _ in range(10):  # Limit iterations to ensure convergence\n        # Calculate the off-diagonal element\n        off_diag = S[0, 1]\n\n        if np.abs(off_diag) < 1e-10:\n            break\n\n        # Calculate the rotation angle\n        tau = (S[1, 1] - S[0, 0]) / (2 * off_diag)\n        t = np.sign(tau) / (np.abs(tau) + np.sqrt(1 + tau**2))\n        c = 1 / np.sqrt(1 + t**2)\n        s = c * t\n\n        # Create the rotation matrix\n        J = np.array([[c, s], [-s, c]])\n\n        # Apply the rotation\n        S = J.T @ S @ J\n        U = U @ J\n        V = V @ J.T\n\n    # Singular values are the diagonal elements of S\n    singular_values = (S[0, 0], S[1, 1])\n\n    # Round the results to the nearest 4th decimal\n    singular_values = tuple(round(sv, 4) for sv in singular_values)\n\n    return singular_values\n\n# Example usage:\nA = np.array([[4, 1], [1, 3]])\nprint(svd_2x2_singular_values(A))\n```\n\n### Explanation:\n1. **Initialization**: We start with identity matrices for `U` and `V`, and a copy of `A` for `S`.\n2. **Jacobi Iteration**: We perform a fixed number of iterations (10 in this case) to approximate the SVD. The off-diagonal element is reduced by applying rotations.\n3. **Rotation Matrix**: The rotation matrix `J` is calculated using the angle derived from the off-diagonal element.\n4. **Update Matrices**: We update `S`, `U`, and `V` using the rotation matrix.\n5. **Singular Values**: After convergence, the singular values are the diagonal elements of `S`.\n6. **Rounding**: The singular values are rounded to the nearest 4th decimal.\n\nThis function should give you a good approximation of the singular values for a 2x2 matrix using the Jacobi method."}
{"task_id": 13, "completion_id": 0, "completion": "To calculate the determinant of a 4x4 matrix using Laplace's Expansion, we can recursively compute the determinant of smaller matrices (3x3, 2x2) until we reach the base case of a 2x2 matrix. Here's how you can implement this in Python:\n\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def determinant_3x3(matrix: list[list[int|float]]) -> float:\n        # Calculate the determinant of a 3x3 matrix\n        return (matrix[0][0] * (matrix[1][1] * matrix[2][2] - matrix[1][2] * matrix[2][1])\n                - matrix[0][1] * (matrix[1][0] * matrix[2][2] - matrix[1][2] * matrix[2][0])\n                + matrix[0][2] * (matrix[1][0] * matrix[2][1] - matrix[1][1] * matrix[2][0]))\n\n    def determinant_2x2(matrix: list[list[int|float]]) -> float:\n        # Calculate the determinant of a 2x2 matrix\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n\n    def minor(matrix: list[list[int|float]], row: int, col: int) -> list[list[int|float]]:\n        # Generate the minor matrix by removing the specified row and column\n        return [ [matrix[i][j] for j in range(len(matrix)) if j != col] \n                 for i in range(len(matrix)) if i != row ]\n\n    # Calculate the determinant of the 4x4 matrix using Laplace's Expansion\n    determinant = 0\n    for col in range(4):\n        # Get the minor matrix for the current element\n        minor_matrix = minor(matrix, 0, col)\n        # Calculate the cofactor\n        cofactor = ((-1) ** col) * matrix[0][col]\n        # Recursively calculate the determinant of the minor matrix\n        if len(minor_matrix) == 3:\n            minor_determinant = determinant_3x3(minor_matrix)\n        elif len(minor_matrix) == 2:\n            minor_determinant = determinant_2x2(minor_matrix)\n        else:\n            raise ValueError(\"Unexpected minor matrix size\")\n        # Add the contribution of the current element to the determinant\n        determinant += cofactor * minor_determinant\n\n    return determinant\n\n# Example usage:\nmatrix_4x4 = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix_4x4))  # Output: 0.0\n```\n\nThis code defines a function `determinant_4x4` that calculates the determinant of a 4x4 matrix using Laplace's Expansion. It uses helper functions `determinant_3x3` and `determinant_2x2` to calculate the determinants of 3x3 and 2x2 matrices, respectively. The `minor` function generates the minor matrix by removing a specified row and column. The main function iterates over the first row of the matrix, calculates the cofactor for each element, and recursively computes the determinant of the minor matrix, summing up the contributions to get the final determinant."}
{"task_id": 14, "completion_id": 0, "completion": "To perform linear regression using the normal equation, we need to compute the coefficients using the formula:\n\n\\[ \\theta = (X^T X)^{-1} X^T y \\]\n\nHere's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Compute the coefficients using the normal equation\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the result back to a list and return\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [1, 2, 3]\n# print(linear_regression_normal_equation(X, y))\n```\n\n### Explanation:\n\n1. **Input Conversion**: The input lists `X` and `y` are converted to NumPy arrays for efficient matrix operations.\n\n2. **Intercept Term**: A column of ones is added to `X` to account for the intercept term in the linear regression model. This is done using `np.hstack`.\n\n3. **Normal Equation**: The coefficients are computed using the normal equation formula. The matrix operations are performed using NumPy's `@` operator for matrix multiplication and `np.linalg.inv` for matrix inversion.\n\n4. **Rounding**: The computed coefficients are rounded to four decimal places using `np.round`.\n\n5. **Output Conversion**: The rounded coefficients are converted back to a list before returning.\n\nThis function will return the coefficients of the linear regression model, including the intercept term, rounded to four decimal places."}
{"task_id": 15, "completion_id": 0, "completion": "To implement linear regression using gradient descent, we need to iteratively update the coefficients to minimize the cost function, which is typically the mean squared error for linear regression. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Number of training examples\n    m = len(y)\n    \n    # Initialize theta (coefficients) with zeros\n    theta = np.zeros(X.shape[1])\n    \n    # Perform gradient descent\n    for _ in range(iterations):\n        # Calculate the predictions\n        predictions = X.dot(theta)\n        \n        # Calculate the errors\n        errors = predictions - y\n        \n        # Calculate the gradient\n        gradient = (1/m) * X.T.dot(errors)\n        \n        # Update the coefficients\n        theta -= alpha * gradient\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Return the coefficients as a list\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = np.array([[1, 1], [1, 2], [1, 3]])\n# y = np.array([1, 2, 3])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\n### Explanation:\n\n1. **Initialization**: We start by initializing the coefficients (`theta`) to zeros. The number of coefficients is equal to the number of features in `X`.\n\n2. **Gradient Descent Loop**: For a specified number of iterations, we perform the following steps:\n   - **Predictions**: Compute the predicted values by multiplying the feature matrix `X` with the coefficients `theta`.\n   - **Errors**: Calculate the difference between the predicted values and the actual target values `y`.\n   - **Gradient**: Compute the gradient of the cost function with respect to the coefficients. This involves taking the dot product of the transpose of `X` with the errors, scaled by the number of examples `m`.\n   - **Update Coefficients**: Adjust the coefficients by subtracting the product of the learning rate `alpha` and the gradient.\n\n3. **Rounding**: After completing the iterations, we round the coefficients to four decimal places.\n\n4. **Return**: Finally, we convert the coefficients to a list and return them.\n\nThis implementation assumes that the input feature matrix `X` already includes a column of ones for the intercept term. If not, you should add it before calling the function."}
{"task_id": 16, "completion_id": 0, "completion": "To implement feature scaling using both standardization and min-max normalization, we need to calculate the mean and standard deviation for standardization, and the minimum and maximum values for min-max normalization. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    \n    # Rounding to the nearest 4th decimal\n    standardized_data = np.round(standardized_data, 4)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4)\n    \n    # Convert to lists\n    standardized_list = standardized_data.tolist()\n    min_max_normalized_list = min_max_normalized_data.tolist()\n    \n    return standardized_list, min_max_normalized_list\n\n# Example usage:\n# data = np.array([[1, 2], [3, 4], [5, 6]])\n# standardized, min_max_normalized = feature_scaling(data)\n# print(\"Standardized Data:\", standardized)\n# print(\"Min-Max Normalized Data:\", min_max_normalized)\n```\n\n### Explanation:\n1. **Standardization**: This process transforms the data to have a mean of 0 and a standard deviation of 1. It is done by subtracting the mean and dividing by the standard deviation for each feature.\n\n2. **Min-Max Normalization**: This process scales the data to a fixed range, usually [0, 1]. It is done by subtracting the minimum value and dividing by the range (max - min) for each feature.\n\n3. **Rounding**: The results are rounded to the nearest 4th decimal using `np.round`.\n\n4. **Conversion to List**: The scaled data is converted from NumPy arrays to Python lists using the `tolist()` method.\n\nThis function will return two lists: one for standardized data and one for min-max normalized data, both rounded to four decimal places."}
{"task_id": 17, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Means clustering algorithm in Python, using the specified inputs and constraints:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Step 1: Assign each point to the nearest centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        closest_centroids = np.argmin(distances, axis=1)\n        \n        # Step 2: Update centroids to be the mean of points assigned to them\n        new_centroids = []\n        for i in range(k):\n            cluster_points = points[closest_centroids == i]\n            if len(cluster_points) > 0:\n                new_centroid = np.mean(cluster_points, axis=0)\n            else:\n                # If a centroid has no points assigned, keep it unchanged\n                new_centroid = centroids[i]\n            new_centroids.append(new_centroid)\n        \n        new_centroids = np.array(new_centroids)\n        \n        # Check for convergence (if centroids do not change)\n        if np.allclose(centroids, new_centroids, atol=1e-4):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    \n    return final_centroids\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nk = 2\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\n### Explanation:\n1. **Initialization**: Convert the list of points and initial centroids into NumPy arrays for efficient computation.\n\n2. **Assignment Step**: Calculate the distance from each point to each centroid and assign each point to the nearest centroid.\n\n3. **Update Step**: Compute the new centroids as the mean of all points assigned to each centroid. If a centroid has no points assigned, it remains unchanged.\n\n4. **Convergence Check**: If the centroids do not change significantly (within a tolerance of `1e-4`), the algorithm stops early.\n\n5. **Rounding**: The final centroids are rounded to the nearest fourth decimal before returning.\n\nThis implementation efficiently partitions the points into clusters and updates the centroids iteratively until convergence or the maximum number of iterations is reached."}
{"task_id": 18, "completion_id": 0, "completion": "To implement K-Fold Cross-Validation, we need to divide the dataset into `k` folds and generate train-test splits for each fold. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): Feature dataset.\n    y (np.ndarray): Target dataset.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    random_seed (int): Seed for random number generator. Default is None.\n\n    Returns:\n    list: A list of tuples, each containing train indices and test indices for each fold.\n    \"\"\"\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.arange(len(X))\n    \n    fold_sizes = np.full(k, len(X) // k, dtype=int)\n    fold_sizes[:len(X) % k] += 1  # Distribute the remainder across the first folds\n\n    current = 0\n    splits = []\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate([indices[:start], indices[stop:]])\n        splits.append((train_indices, test_indices))\n        current = stop\n\n    return splits\n\n# Example usage:\n# X = np.array([...])  # Your feature data\n# y = np.array([...])  # Your target data\n# splits = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\n# for train_indices, test_indices in splits:\n#     X_train, X_test = X[train_indices], X[test_indices]\n#     y_train, y_test = y[train_indices], y[test_indices]\n```\n\n### Explanation:\n1. **Shuffle**: If `shuffle` is `True`, the indices of the dataset are shuffled using `np.random.permutation`. A `random_seed` can be set for reproducibility.\n2. **Fold Sizes**: The dataset is divided into `k` folds. The `fold_sizes` array determines the size of each fold, ensuring that any remainder is distributed across the first few folds.\n3. **Splitting**: For each fold, the test indices are selected, and the remaining indices are used for training. This is done by slicing the shuffled indices array.\n4. **Return**: The function returns a list of tuples, each containing train and test indices for each fold.\n\nThis implementation allows you to easily perform K-Fold Cross-Validation on your dataset, providing train-test splits for model evaluation."}
{"task_id": 19, "completion_id": 0, "completion": "To implement Principal Component Analysis (PCA) from scratch, we need to follow these steps:\n\n1. **Standardize the dataset**: This involves centering the data by subtracting the mean of each feature and scaling to unit variance.\n\n2. **Compute the covariance matrix**: This matrix represents how much the dimensions vary from the mean with respect to each other.\n\n3. **Find the eigenvalues and eigenvectors**: These are used to identify the principal components.\n\n4. **Sort the eigenvectors by the largest eigenvalues**: The eigenvectors corresponding to the largest eigenvalues are the principal components.\n\n5. **Select the top `k` principal components**: Return these as the result.\n\nHere's the implementation of the PCA function:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the dataset\n    # Subtract the mean of each feature\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Step 4: Sort the eigenvectors by the largest eigenvalues\n    # Get the indices of the sorted eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Round the results to the nearest 4th decimal\n    principal_components_rounded = np.round(principal_components, 4)\n    \n    # Convert the result to a list of lists for the output\n    return principal_components_rounded.tolist()\n\n# Example usage:\n# data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n# k = 1\n# print(pca(data, k))\n```\n\n### Explanation:\n\n- **Standardization**: We subtract the mean and divide by the standard deviation for each feature to ensure that each feature contributes equally to the analysis.\n\n- **Covariance Matrix**: This is calculated using `np.cov` with `rowvar=False` to indicate that each column represents a variable.\n\n- **Eigenvalues and Eigenvectors**: We use `np.linalg.eigh` because the covariance matrix is symmetric, which is more efficient than `np.linalg.eig`.\n\n- **Sorting**: We sort the eigenvectors based on the eigenvalues in descending order to get the principal components.\n\n- **Rounding**: The results are rounded to four decimal places as specified.\n\nThis function will return the top `k` principal components as a list of lists, with each list representing a principal component."}
{"task_id": 20, "completion_id": 0, "completion": "To implement a decision tree learning algorithm using entropy and information gain, we need to follow these steps:\n\n1. Calculate the entropy of the dataset.\n2. Calculate the information gain for each attribute.\n3. Choose the attribute with the highest information gain to split the dataset.\n4. Recursively apply the process to the subsets of the dataset created by the split until a stopping condition is met (e.g., all examples have the same classification or no attributes are left to split).\n\nHere's the implementation:\n\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the given examples.\"\"\"\n    # Count the frequency of each class in the target attribute\n    class_counts = Counter(example[target_attr] for example in examples)\n    total_examples = len(examples)\n    \n    # Calculate entropy\n    entropy_value = 0.0\n    for count in class_counts.values():\n        probability = count / total_examples\n        entropy_value -= probability * math.log2(probability)\n    \n    return entropy_value\n\ndef information_gain(examples, attr, target_attr):\n    \"\"\"Calculate the information gain of splitting the examples by the given attribute.\"\"\"\n    # Calculate the entropy of the whole dataset\n    total_entropy = entropy(examples, target_attr)\n    \n    # Group examples by the attribute values\n    attr_values = set(example[attr] for example in examples)\n    weighted_entropy = 0.0\n    \n    for value in attr_values:\n        subset = [example for example in examples if example[attr] == value]\n        subset_entropy = entropy(subset, target_attr)\n        weighted_entropy += (len(subset) / len(examples)) * subset_entropy\n    \n    # Information gain is the difference between the total entropy and the weighted entropy\n    return total_entropy - weighted_entropy\n\ndef majority_class(examples, target_attr):\n    \"\"\"Return the most common class in the examples.\"\"\"\n    class_counts = Counter(example[target_attr] for example in examples)\n    return class_counts.most_common(1)[0][0]\n\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"Recursively build a decision tree using the given examples and attributes.\"\"\"\n    # If all examples have the same classification, return that classification\n    if len(set(example[target_attr] for example in examples)) == 1:\n        return examples[0][target_attr]\n    \n    # If no attributes are left, return the majority class\n    if not attributes:\n        return majority_class(examples, target_attr)\n    \n    # Find the best attribute to split on\n    gains = [(attr, information_gain(examples, attr, target_attr)) for attr in attributes]\n    best_attr, best_gain = max(gains, key=lambda x: x[1])\n    \n    # Create a new decision tree node\n    tree = {best_attr: {}}\n    \n    # Split the examples by the best attribute and recursively build the tree\n    attr_values = set(example[best_attr] for example in examples)\n    for value in attr_values:\n        subset = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(subset, [attr for attr in attributes if attr != best_attr], target_attr)\n        tree[best_attr][value] = subtree\n    \n    return tree\n\n# Example usage:\nexamples = [\n    {'attribute1': 'A', 'attribute2': 'B', 'target': 'Yes'},\n    {'attribute1': 'A', 'attribute2': 'C', 'target': 'No'},\n    {'attribute1': 'B', 'attribute2': 'B', 'target': 'Yes'},\n    {'attribute1': 'B', 'attribute2': 'C', 'target': 'No'},\n]\n\nattributes = ['attribute1', 'attribute2']\ntarget_attr = 'target'\n\ndecision_tree = learn_decision_tree(examples, attributes, target_attr)\nprint(decision_tree)\n```\n\nThis code defines a function `learn_decision_tree` that builds a decision tree using entropy and information gain. It recursively splits the dataset based on the attribute with the highest information gain until all examples have the same classification or no attributes are left to split."}
{"task_id": 21, "completion_id": 0, "completion": "To implement the Pegasos Kernel SVM algorithm, we need to define the kernel functions, compute the kernel matrix, and iteratively update the alpha coefficients and bias. Below is the implementation of the deterministic Pegasos algorithm for kernel SVM:\n\n```python\nimport numpy as np\n\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n\ndef rbf_kernel(x1, x2, sigma):\n    return np.exp(-np.linalg.norm(x1 - x2)**2 / (2 * (sigma ** 2)))\n\ndef compute_kernel_matrix(data, kernel, sigma):\n    n_samples = data.shape[0]\n    kernel_matrix = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if kernel == 'linear':\n                kernel_matrix[i, j] = linear_kernel(data[i], data[j])\n            elif kernel == 'rbf':\n                kernel_matrix[i, j] = rbf_kernel(data[i], data[j], sigma)\n    return kernel_matrix\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    \n    kernel_matrix = compute_kernel_matrix(data, kernel, sigma)\n    \n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        \n        for i in range(n_samples):\n            margin = labels[i] * (np.sum(alpha * labels * kernel_matrix[:, i]) + bias)\n            if margin < 1:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i] + learning_rate * labels[i]\n            else:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i]\n        \n        bias = np.sum(alpha * labels) / n_samples\n    \n    # Round results to the nearest 4th decimal\n    alpha = np.round(alpha, 4)\n    bias = round(bias, 4)\n    \n    return alpha.tolist(), bias\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4]])\n# labels = np.array([1, -1, 1])\n# alpha, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.01, iterations=100, sigma=1.0)\n# print(\"Alpha coefficients:\", alpha)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n1. **Kernel Functions**: We define `linear_kernel` and `rbf_kernel` functions to compute the kernel values between two samples.\n2. **Kernel Matrix**: The `compute_kernel_matrix` function calculates the kernel matrix for the entire dataset based on the chosen kernel.\n3. **Pegasos Algorithm**: The main function `pegasos_kernel_svm` iteratively updates the alpha coefficients using the deterministic approach (using all samples in each iteration) and computes the bias.\n4. **Output**: The alpha coefficients and bias are rounded to the nearest 4th decimal and returned as a list.\n\nThis implementation should work for both linear and RBF kernels, and it performs binary classification using the Pegasos algorithm."}
{"task_id": 22, "completion_id": 0, "completion": "To implement the sigmoid activation function in Python, we can use the mathematical formula for the sigmoid function, which is:\n\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n\nHere's the complete implementation of the function, including rounding the result to four decimal places:\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    # Calculate the sigmoid function\n    result = 1 / (1 + math.exp(-z))\n    # Round the result to four decimal places\n    return round(result, 4)\n```\n\nThis function takes a single float input `z`, computes the sigmoid of `z`, and returns the result rounded to four decimal places. The `math.exp` function is used to calculate the exponential part of the sigmoid formula."}
{"task_id": 23, "completion_id": 0, "completion": "To implement the softmax activation function, we need to follow these steps:\n\n1. Compute the exponential of each score.\n2. Sum all the exponential values.\n3. Divide each exponential value by the sum of all exponential values to get the softmax value.\n4. Round each softmax value to four decimal places.\n\nHere's the complete implementation:\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Compute the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Sum all the exponential values\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax values and round them to four decimal places\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_values\n\n# Example usage:\nscores = [1.0, 2.0, 3.0]\nprint(softmax(scores))  # Output: [0.0900, 0.2447, 0.6652]\n```\n\nThis function takes a list of scores, computes the softmax values, and returns them rounded to four decimal places. The example usage demonstrates how to call the function and print the result."}
{"task_id": 24, "completion_id": 0, "completion": "To implement the function `single_neuron_model`, we need to perform the following steps:\n\n1. Calculate the weighted sum of inputs for each feature vector.\n2. Apply the sigmoid activation function to the weighted sum to get the predicted probabilities.\n3. Compute the mean squared error between the predicted probabilities and the true labels.\n4. Return the predicted probabilities and the mean squared error, both rounded to four decimal places.\n\nHere's the implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Calculate the predicted probabilities using the sigmoid activation function\n    predicted_probabilities = []\n    for feature_vector in features:\n        # Calculate the weighted sum\n        weighted_sum = sum(f * w for f, w in zip(feature_vector, weights)) + bias\n        # Apply sigmoid activation function\n        probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(probability)\n    \n    # Convert to numpy array for easier manipulation\n    predicted_probabilities = np.array(predicted_probabilities)\n    \n    # Calculate the mean squared error\n    labels_array = np.array(labels)\n    mse = np.mean((predicted_probabilities - labels_array) ** 2)\n    \n    # Round the results to four decimal places\n    predicted_probabilities = np.round(predicted_probabilities, 4).tolist()\n    mse = round(mse, 4)\n    \n    return predicted_probabilities, mse\n\n# Example usage:\nfeatures = [[0.5, 1.5], [1.0, -1.0], [0.0, 0.0]]\nlabels = [1, 0, 1]\nweights = [0.2, -0.3]\nbias = 0.1\n\npredicted_probabilities, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probabilities)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis code defines a function `single_neuron_model` that takes feature vectors, labels, weights, and bias as input, computes the predicted probabilities using a sigmoid activation function, and calculates the mean squared error. The results are rounded to four decimal places and returned."}
{"task_id": 25, "completion_id": 0, "completion": "To implement a single neuron with sigmoid activation and backpropagation, we need to follow these steps:\n\n1. **Forward Pass**: Compute the neuron's output using the sigmoid activation function.\n2. **Loss Calculation**: Calculate the Mean Squared Error (MSE) loss.\n3. **Backward Pass**: Compute the gradients of the loss with respect to the weights and bias.\n4. **Update Parameters**: Adjust the weights and bias using the gradients and the learning rate.\n5. **Repeat**: Perform the above steps for a specified number of epochs.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n\n        # Calculate MSE loss\n        errors = predictions - labels\n        mse = np.mean(errors ** 2)\n        mse_values.append(round(mse, 4))\n\n        # Backward pass\n        # Gradient of the loss w.r.t. predictions\n        d_loss_d_predictions = 2 * errors / len(labels)\n        \n        # Gradient of predictions w.r.t. linear output\n        d_predictions_d_linear_output = predictions * (1 - predictions)\n        \n        # Gradient of the loss w.r.t. linear output\n        d_loss_d_linear_output = d_loss_d_predictions * d_predictions_d_linear_output\n        \n        # Gradient of the loss w.r.t. weights\n        d_loss_d_weights = np.dot(features.T, d_loss_d_linear_output)\n        \n        # Gradient of the loss w.r.t. bias\n        d_loss_d_bias = np.sum(d_loss_d_linear_output)\n\n        # Update weights and bias\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n\n    # Round weights and bias to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n\n    return weights.tolist(), bias, mse_values\n\n# Example usage:\n# features = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n# labels = np.array([0, 1, 0])\n# initial_weights = np.array([0.1, 0.2])\n# initial_bias = 0.3\n# learning_rate = 0.01\n# epochs = 1000\n# updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n# print(updated_weights, updated_bias, mse_values)\n```\n\nThis function will simulate a single neuron with sigmoid activation, perform backpropagation to update the weights and bias, and return the updated values along with the MSE for each epoch. The results are rounded to four decimal places as required."}
{"task_id": 26, "completion_id": 0, "completion": "To implement basic autograd operations with automatic differentiation, we need to define a `Value` class that can perform operations like addition, multiplication, and ReLU activation, while also keeping track of the computation graph to compute gradients. Here's how you can implement this:\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    a = Value(2.0)\n    b = Value(-3.0)\n    c = Value(10.0)\n    d = a * b + c\n    e = d.relu()\n    e.backward()\n\n    print(f\"a: {a}, b: {b}, c: {c}, d: {d}, e: {e}\")\n```\n\n### Explanation:\n- **Initialization**: Each `Value` object stores its data, gradient, and a backward function to compute gradients. It also keeps track of its children in the computation graph and the operation that produced it.\n- **Addition and Multiplication**: These operations create new `Value` objects and define backward functions to propagate gradients.\n- **ReLU Activation**: This operation applies the ReLU function and defines a backward function that propagates gradients only if the output is positive.\n- **Backward Propagation**: The `backward` method computes gradients by traversing the computation graph in reverse topological order.\n- **Example Usage**: Demonstrates how to use the `Value` class to perform operations and compute gradients.\n\nThis implementation provides a basic framework for automatic differentiation using a computation graph, similar to what is done in libraries like PyTorch."}
{"task_id": 27, "completion_id": 0, "completion": "To compute the transformation matrix \\( P \\) from basis \\( B \\) to basis \\( C \\) in \\( \\mathbb{R}^3 \\), we need to follow these steps:\n\n1. **Understand the Problem**: We are given two sets of basis vectors, \\( B \\) and \\( C \\). Each set consists of three vectors that form a basis for \\( \\mathbb{R}^3 \\). We need to find the matrix \\( P \\) such that when we multiply a vector expressed in basis \\( B \\) by \\( P \\), we get the vector expressed in basis \\( C \\).\n\n2. **Matrix Representation**: \n   - Let \\( B \\) be a matrix where each column is a basis vector from the basis \\( B \\).\n   - Let \\( C \\) be a matrix where each column is a basis vector from the basis \\( C \\).\n\n3. **Transformation Matrix**: The transformation matrix \\( P \\) from basis \\( B \\) to basis \\( C \\) is given by:\n   \\[\n   P = C \\cdot B^{-1}\n   \\]\n   Here, \\( B^{-1} \\) is the inverse of the matrix \\( B \\).\n\n4. **Implementation**: We will use NumPy to handle matrix operations, including inversion and multiplication. Finally, we will round the results to the nearest 4th decimal and convert the result back to a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert lists to numpy arrays\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    \n    # Compute the inverse of B\n    B_inv = np.linalg.inv(B_matrix)\n    \n    # Compute the transformation matrix P\n    P = np.dot(C_matrix, B_inv)\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the result back to a list of lists\n    return P_rounded.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\nprint(transform_basis(B, C))\n```\n\n### Explanation:\n- **NumPy Arrays**: We convert the input lists to NumPy arrays for easy matrix operations.\n- **Inverse Calculation**: We use `np.linalg.inv()` to compute the inverse of matrix \\( B \\).\n- **Matrix Multiplication**: We use `np.dot()` to perform matrix multiplication.\n- **Rounding**: We use `np.round()` to round the elements of the matrix to the nearest 4th decimal.\n- **Conversion to List**: Finally, we convert the NumPy array back to a list of lists using the `tolist()` method.\n\nThis function will compute the transformation matrix \\( P \\) from basis \\( B \\) to basis \\( C \\) and return it as a list of lists with each element rounded to four decimal places."}
{"task_id": 28, "completion_id": 0, "completion": "To compute the Singular Value Decomposition (SVD) of a 2x2 matrix using eigenvalues and eigenvectors, we can follow these steps:\n\n1. Compute the eigenvalues and eigenvectors of \\( A^T A \\).\n2. Use the eigenvalues to determine the singular values.\n3. Compute the matrix \\( V \\) using the eigenvectors of \\( A^T A \\).\n4. Compute the matrix \\( U \\) using the relationship \\( U = AVS^{-1} \\).\n\nHere's the Python function implementing these steps:\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Ensure A is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input matrix must be 2x2.\"\n    \n    # Compute A^T A\n    AtA = np.dot(A.T, A)\n    \n    # Compute eigenvalues and eigenvectors of A^T A\n    eigenvalues, V = np.linalg.eigh(AtA)\n    \n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    V = V[:, idx]\n    \n    # Compute singular values (square roots of eigenvalues)\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Construct the Sigma matrix\n    S = np.diag(singular_values)\n    \n    # Compute U using the relationship U = AVS^{-1}\n    U = np.dot(A, np.dot(V, np.linalg.inv(S)))\n    \n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert matrices to lists\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return U_list, S_list, V_list\n\n# Example usage:\nA = np.array([[3, 1], [1, 3]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis function computes the SVD of a 2x2 matrix \\( A \\) and returns the matrices \\( U \\), \\( S \\), and \\( V \\) as lists, with all values rounded to the nearest fourth decimal. The function uses the eigen decomposition of \\( A^T A \\) to find the singular values and the matrix \\( V \\), and then computes \\( U \\) using the relationship \\( U = AVS^{-1} \\)."}
{"task_id": 29, "completion_id": 0, "completion": "To accomplish the task of shuffling two numpy arrays while maintaining their corresponding order, we can use numpy's random permutation functionality. Here's how you can implement the `shuffle_data` function:\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Set the seed for reproducibility if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random permutation of indices based on the length of X\n    indices = np.random.permutation(X.shape[0])\n    \n    # Shuffle X and y using the generated indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert the shuffled arrays to lists and return them\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n# print(X_shuffled)\n# print(y_shuffled)\n```\n\n### Explanation:\n1. **Seed Setting**: If a seed is provided, it is set using `np.random.seed(seed)` to ensure reproducibility of the shuffle.\n2. **Random Permutation**: `np.random.permutation(X.shape[0])` generates a random permutation of indices based on the number of samples in `X`.\n3. **Shuffling**: The arrays `X` and `y` are shuffled using the generated indices, ensuring that the order between `X` and `y` is maintained.\n4. **Conversion to List**: The shuffled numpy arrays are converted to Python lists using the `tolist()` method before returning.\n\nThis function will shuffle the data while maintaining the correspondence between `X` and `y`, and it will return the shuffled data as lists."}
{"task_id": 30, "completion_id": 0, "completion": "To implement a batch iterator function that yields batches from a numpy array `X` and optionally from a numpy array `y`, we can follow these steps:\n\n1. Determine the number of samples in `X`.\n2. Iterate over the data in steps of `batch_size`.\n3. For each step, slice the arrays `X` and `y` (if provided) to get the current batch.\n4. Convert the batch arrays to lists using the `tolist()` method.\n5. Yield the batches as lists.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[start_idx:end_idx]\n        \n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\n\n# Example usage:\n# X = np.random.rand(100, 10)  # 100 samples, 10 features\n# y = np.random.randint(0, 2, 100)  # 100 samples, binary labels\n# for batch in batch_iterator(X, y, batch_size=20):\n#     print(batch)\n```\n\n### Explanation:\n\n- **`num_samples = X.shape[0]`**: This line determines the number of samples in the dataset `X`.\n- **`range(0, num_samples, batch_size)`**: This loop iterates over the dataset in increments of `batch_size`.\n- **`end_idx = min(start_idx + batch_size, num_samples)`**: This ensures that the end index does not exceed the number of samples.\n- **`X_batch = X[start_idx:end_idx]`**: This slices the `X` array to get the current batch.\n- **`y_batch = y[start_idx:end_idx]`**: This slices the `y` array to get the current batch, if `y` is provided.\n- **`tolist()`**: This method converts the numpy arrays to Python lists, as required by the output constraints.\n- **`yield`**: This statement returns the current batch and pauses the function, allowing it to resume from this point in the next iteration."}
{"task_id": 31, "completion_id": 0, "completion": "To accomplish the task of dividing a dataset based on a feature threshold, we can iterate over the dataset and separate the samples into two subsets based on whether they meet the specified condition. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset into two subsets based on a feature threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset, where each row is a sample and each column is a feature.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value to divide the dataset.\n\n    Returns:\n    tuple: Two lists containing the samples that meet the condition and those that do not.\n    \"\"\"\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Subset where feature value is greater than or equal to the threshold\n    subset_meet_condition = X[X[:, feature_i] >= threshold]\n    \n    # Subset where feature value is less than the threshold\n    subset_not_meet_condition = X[X[:, feature_i] < threshold]\n    \n    # Convert subsets to lists using tolist()\n    return subset_meet_condition.tolist(), subset_not_meet_condition.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# feature_i = 0\n# threshold = 4\n# subset_meet, subset_not_meet = divide_on_feature(X, feature_i, threshold)\n# print(\"Subset meeting condition:\", subset_meet)\n# print(\"Subset not meeting condition:\", subset_not_meet)\n```\n\n### Explanation:\n- **Parameters**:\n  - `X`: The dataset represented as a NumPy array, where each row is a sample and each column is a feature.\n  - `feature_i`: The index of the feature used for division.\n  - `threshold`: The threshold value for dividing the dataset.\n\n- **Logic**:\n  - We use NumPy's array slicing and boolean indexing to efficiently separate the samples into two subsets.\n  - `subset_meet_condition` contains samples where the specified feature's value is greater than or equal to the threshold.\n  - `subset_not_meet_condition` contains samples where the specified feature's value is less than the threshold.\n\n- **Output**:\n  - The function returns two lists, each containing the samples that meet or do not meet the condition, respectively. The conversion to lists is done using the `tolist()` method of NumPy arrays."}
{"task_id": 32, "completion_id": 0, "completion": "To generate polynomial features for a given dataset, we can use combinations with replacement to create all possible polynomial combinations of the features up to the specified degree. Here's how you can implement the `polynomial_features` function:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Get the number of samples and features\n    n_samples, n_features = X.shape\n    \n    # Generate combinations of feature indices with replacement\n    combinations = list(combinations_with_replacement(range(n_features), degree))\n    \n    # Initialize the output array with the correct shape\n    n_output_features = len(combinations)\n    X_poly = np.empty((n_samples, n_output_features))\n    \n    # Compute the polynomial features\n    for i, comb in enumerate(combinations):\n        X_poly[:, i] = np.prod(X[:, comb], axis=1)\n    \n    # Return the result as a list\n    return X_poly.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# degree = 2\n# print(polynomial_features(X, degree))\n```\n\n### Explanation:\n1. **Input Handling**: The function first ensures that the input `X` is a numpy array. This is important for consistent behavior when performing operations on the array.\n\n2. **Combinations Generation**: Using `combinations_with_replacement`, we generate all possible combinations of feature indices up to the specified degree. This allows us to create polynomial terms like \\(x_1^2\\), \\(x_1x_2\\), etc.\n\n3. **Output Array Initialization**: We initialize an empty numpy array `X_poly` to store the polynomial features. The number of columns in this array is equal to the number of combinations generated.\n\n4. **Polynomial Feature Calculation**: For each combination, we calculate the product of the corresponding features and store it in the output array.\n\n5. **Return as List**: Finally, we convert the numpy array to a list using the `tolist()` method and return it.\n\nThis function will generate polynomial features for the input dataset up to the specified degree, allowing for more complex models to be built using linear regression or other algorithms that can benefit from polynomial features."}
{"task_id": 33, "completion_id": 0, "completion": "To accomplish the task of generating random subsets of a dataset, we can use numpy's random sampling capabilities. Here's a Python function that implements the required functionality:\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)  # Set the random seed for reproducibility\n    subsets = []\n    n_samples = X.shape[0]  # Number of samples in the dataset\n\n    for _ in range(n_subsets):\n        if replacements:\n            # Sample indices with replacement\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            # Sample indices without replacement\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        \n        # Create subsets using the sampled indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Append the subset as a tuple to the list\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y = np.array([0, 1, 0, 1])\n# subsets = get_random_subsets(X, y, n_subsets=3, replacements=True)\n# print(subsets)\n```\n\n### Explanation:\n- **Seed**: We set a seed for the random number generator to ensure reproducibility of the results.\n- **Sampling**: Depending on the `replacements` parameter, we either sample with or without replacement using `np.random.choice`.\n- **Subsets Creation**: For each subset, we use the sampled indices to create `X_subset` and `y_subset`.\n- **Return Format**: Each subset is converted to a list using the `tolist()` method before being appended to the `subsets` list.\n\nThis function will return a list of tuples, where each tuple contains a subset of `X` and `y` as lists."}
{"task_id": 34, "completion_id": 0, "completion": "To perform one-hot encoding of nominal values, we can use the `numpy` library to create a one-hot encoded matrix. Here's how you can implement the `to_categorical` function:\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Initialize a zero matrix of shape (len(x), n_col)\n    one_hot = np.zeros((len(x), n_col), dtype=int)\n    \n    # Set the appropriate indices to 1\n    one_hot[np.arange(len(x)), x] = 1\n    \n    # Convert the numpy array to a list and return\n    return one_hot.tolist()\n\n# Example usage:\nx = np.array([0, 1, 2, 1, 0])\nprint(to_categorical(x))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n```\n\n### Explanation:\n1. **Determine `n_col`:** If `n_col` is not provided, it is determined by finding the maximum value in the array `x` and adding 1. This ensures that we have a column for each possible integer value in `x`.\n\n2. **Initialize the matrix:** A zero matrix of shape `(len(x), n_col)` is created. This matrix will hold the one-hot encoded values.\n\n3. **Set indices:** Using `np.arange(len(x))` and the values in `x`, we set the appropriate positions in the matrix to 1, effectively creating the one-hot encoding.\n\n4. **Convert to list:** Finally, the numpy array is converted to a Python list using the `tolist()` method, as required by the output constraints."}
{"task_id": 35, "completion_id": 0, "completion": "To convert a 1D numpy array into a diagonal matrix, you can use the `np.diag` function, which creates a diagonal matrix from a given array. After creating the diagonal matrix, you can convert it to a Python list using the `tolist()` method. Here's how you can implement the function:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D numpy array\n    diagonal_matrix = np.diag(x)\n    \n    # Convert the numpy array to a Python list\n    return diagonal_matrix.tolist()\n\n# Example usage:\n# x = np.array([1, 2, 3])\n# print(make_diagonal(x))\n# Output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```\n\nThis function takes a 1D numpy array `x` as input, creates a diagonal matrix using `np.diag`, and then converts the resulting 2D numpy array into a Python list using the `tolist()` method."}
{"task_id": 36, "completion_id": 0, "completion": "To calculate the accuracy score, you need to determine the proportion of correctly predicted labels out of the total number of labels. Here's how you can implement the `accuracy_score` function:\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    # Ensure that y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if the lengths of the arrays are the same\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    \n    # Calculate the accuracy score\n    accuracy = correct_predictions / len(y_true)\n    \n    return accuracy\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0])\n# y_pred = np.array([1, 0, 0, 1, 0])\n# print(accuracy_score(y_true, y_pred))  # Output: 0.8\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are the same. If not, it raises a `ValueError` because accuracy cannot be calculated if the arrays are of different lengths.\n\n2. **Correct Predictions**: It uses `np.sum(y_true == y_pred)` to count the number of elements where the true label matches the predicted label.\n\n3. **Accuracy Calculation**: The accuracy is calculated as the ratio of correct predictions to the total number of predictions.\n\nThis function will return the accuracy score as a float, representing the proportion of correct predictions."}
{"task_id": 37, "completion_id": 0, "completion": "To calculate the correlation matrix for a given dataset using numpy, we can utilize the `numpy.corrcoef` function, which computes the correlation coefficients between rows of the input arrays. Here's how you can implement the function:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    # If Y is not provided, calculate the correlation matrix of X with itself\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        # Concatenate X and Y along the columns to calculate the correlation matrix between them\n        combined = np.concatenate((X, Y), axis=1)\n        correlation_matrix = np.corrcoef(combined, rowvar=False)\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Convert the numpy array to a list using tolist() method\n    return correlation_matrix.tolist()\n\n# Example usage:\n# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n# print(calculate_correlation_matrix(X))\n# print(calculate_correlation_matrix(X, Y))\n```\n\n### Explanation:\n- **Input Handling**: The function accepts two parameters, `X` and an optional `Y`. If `Y` is not provided, the function calculates the correlation matrix of `X` with itself.\n- **Correlation Calculation**: The `np.corrcoef` function is used to compute the correlation matrix. The `rowvar=False` argument specifies that variables are represented by columns, which is typical for datasets.\n- **Rounding**: The correlation matrix is rounded to the nearest 4th decimal using `np.round`.\n- **Output Format**: The result is converted to a Python list using the `tolist()` method, as required by the task constraints.\n\nThis function should work efficiently for calculating the correlation matrix for both single and paired datasets."}
{"task_id": 38, "completion_id": 0, "completion": "To implement the AdaBoost fit method, we need to follow these steps:\n\n1. Initialize sample weights.\n2. For each classifier, find the best threshold for each feature that minimizes the weighted error.\n3. Calculate the error and update the weights.\n4. Store the classifier parameters.\n\nHere's the implementation:\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    # Initialize weights\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize variables to store the best classifier parameters\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_error = float('inf')\n        best_alpha = None\n\n        # Iterate over each feature\n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            # Sort feature values and corresponding labels and weights\n            sorted_indices = np.argsort(feature_values)\n            sorted_feature_values = feature_values[sorted_indices]\n            sorted_labels = y[sorted_indices]\n            sorted_weights = weights[sorted_indices]\n\n            # Try different thresholds\n            for i in range(1, n_samples):\n                threshold = (sorted_feature_values[i - 1] + sorted_feature_values[i]) / 2\n                for polarity in [1, -1]:\n                    # Make predictions\n                    predictions = np.ones(n_samples)\n                    if polarity == 1:\n                        predictions[feature_values < threshold] = -1\n                    else:\n                        predictions[feature_values > threshold] = -1\n\n                    # Calculate weighted error\n                    error = np.sum(sorted_weights[predictions != sorted_labels])\n\n                    # Update best parameters if error is lower\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_polarity = polarity\n\n        # Calculate alpha (classifier weight)\n        best_alpha = 0.5 * math.log((1 - best_error) / (best_error + 1e-10))\n\n        # Update weights\n        predictions = np.ones(n_samples)\n        if best_polarity == 1:\n            predictions[X[:, best_feature] < best_threshold] = -1\n        else:\n            predictions[X[:, best_feature] > best_threshold] = -1\n\n        weights *= np.exp(-best_alpha * y * predictions)\n        weights /= np.sum(weights)\n\n        # Store the classifier parameters\n        classifiers.append({\n            'feature': best_feature,\n            'threshold': round(best_threshold, 4),\n            'polarity': best_polarity,\n            'alpha': round(best_alpha, 4)\n        })\n\n    return classifiers\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 1, -1, -1])\n# n_clf = 3\n# print(adaboost_fit(X, y, n_clf))\n```\n\nThis code implements the AdaBoost fit method by iterating over features and thresholds to find the best weak classifier at each step. It updates the sample weights based on the classifier's performance and stores the parameters of each classifier. The results are rounded to the nearest 4th decimal as required."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the log-softmax function, we need to first compute the softmax values and then take the logarithm of those values. The log-softmax function is useful for numerical stability, especially when dealing with large numbers, as it avoids overflow issues that can occur with the exponential function.\n\nHere's the implementation of the log-softmax function:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Compute the maximum score for numerical stability\n    max_score = np.max(scores_array)\n    \n    # Compute the shifted scores by subtracting the max score\n    shifted_scores = scores_array - max_score\n    \n    # Compute the exponentials of the shifted scores\n    exp_scores = np.exp(shifted_scores)\n    \n    # Compute the sum of the exponentials\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Compute the log-softmax values\n    log_softmax_values = shifted_scores - np.log(sum_exp_scores)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    \n    # Convert the numpy array to a list and return\n    return rounded_log_softmax_values.tolist()\n\n# Example usage:\nscores = [3.0, 1.0, 0.2]\nprint(log_softmax(scores))\n```\n\n### Explanation:\n1. **Numerical Stability**: We subtract the maximum score from each score before computing the exponentials. This helps prevent large numbers in the exponentiation step, which can lead to overflow.\n\n2. **Exponentiation and Summation**: We compute the exponentials of the shifted scores and then sum them up.\n\n3. **Log-Softmax Calculation**: The log-softmax values are computed by subtracting the logarithm of the sum of exponentials from the shifted scores.\n\n4. **Rounding**: We round the results to the nearest 4th decimal for precision.\n\n5. **Conversion to List**: Finally, we convert the numpy array to a Python list using the `tolist()` method before returning the result."}
{"task_id": 40, "completion_id": 0, "completion": "To implement the `Dense` class, we need to extend the `Layer` class and provide implementations for the methods specified in the task. Below is the complete implementation of the `Dense` class:\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.initialize()\n\n    def initialize(self):\n        # Initialize weights and biases\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        # Initialize optimizers (placeholders for now)\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def parameters(self):\n        # Return the total number of trainable parameters\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        # Compute the output of the layer\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        # Calculate the gradient with respect to the input\n        grad_input = np.dot(accum_grad, self.W.T)\n        \n        # Update weights and biases if trainable\n        if self.trainable:\n            # Placeholder for optimizer update rule\n            self.W -= 0.01 * np.dot(self.layer_input.T, accum_grad)\n            self.w0 -= 0.01 * np.sum(accum_grad, axis=0, keepdims=True)\n        \n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        # Return the shape of the output\n        return (self.n_units,)\n```\n\n### Explanation:\n\n1. **Initialization (`__init__`)**:\n   - The constructor initializes the number of units (`n_units`) and optionally the input shape (`input_shape`).\n   - It sets placeholders for weights (`W`), biases (`w0`), and optimizers.\n   - Calls the `initialize` method to set up weights and biases.\n\n2. **Weight Initialization (`initialize`)**:\n   - Weights `W` are initialized using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`.\n   - Biases `w0` are initialized to zero.\n   - Optimizers are placeholders for now.\n\n3. **Parameter Count (`parameters`)**:\n   - Returns the total number of trainable parameters, which is the sum of the number of elements in `W` and `w0`.\n\n4. **Forward Pass (`forward_pass`)**:\n   - Computes the output by performing a dot product between the input `X` and the weight matrix `W`, then adds the bias `w0`.\n   - Rounds the output to the nearest 4th decimal and converts it to a list.\n\n5. **Backward Pass (`backward_pass`)**:\n   - Calculates the gradient with respect to the input.\n   - If the layer is trainable, updates the weights and biases using a simple gradient descent update rule (placeholder for optimizer).\n   - Rounds the gradient input to the nearest 4th decimal and converts it to a list.\n\n6. **Output Shape (`output_shape`)**:\n   - Returns the shape of the output, which is `(self.n_units,)`.\n\nThis implementation provides a basic structure for a fully connected layer in a neural network, with placeholders for more advanced optimization techniques."}
{"task_id": 41, "completion_id": 0, "completion": "To implement a simple 2D convolutional layer in Python, we need to consider the input matrix, the convolutional kernel, the padding, and the stride. The convolution operation involves sliding the kernel over the input matrix, applying the kernel to each sub-region of the input matrix, and producing an output matrix.\n\nHere's how you can implement the `simple_conv2d` function:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Get dimensions of the input matrix and kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate the dimensions of the output matrix\n    output_height = ((input_height - kernel_height + 2 * padding) // stride) + 1\n    output_width = ((input_width - kernel_width + 2 * padding) // stride) + 1\n    \n    # Initialize the output matrix with zeros\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Apply padding to the input matrix\n    if padding > 0:\n        padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    else:\n        padded_input = input_matrix\n    \n    # Perform the convolution operation\n    for y in range(output_height):\n        for x in range(output_width):\n            # Calculate the starting position of the current \"slice\" of the input\n            start_y = y * stride\n            start_x = x * stride\n            \n            # Extract the current slice of the input\n            input_slice = padded_input[start_y:start_y + kernel_height, start_x:start_x + kernel_width]\n            \n            # Perform element-wise multiplication and sum the result\n            output_value = np.sum(input_slice * kernel)\n            \n            # Assign the result to the output matrix, rounded to 4 decimal places\n            output_matrix[y, x] = round(output_value, 4)\n    \n    # Convert the output matrix to a list and return\n    return output_matrix.tolist()\n\n# Example usage:\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 1\nstride = 1\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)\n```\n\n### Explanation:\n1. **Padding**: We use `np.pad` to add zero-padding around the input matrix if the padding is greater than zero.\n2. **Output Dimensions**: The dimensions of the output matrix are calculated based on the input dimensions, kernel size, padding, and stride.\n3. **Convolution Operation**: We iterate over each position in the output matrix, extract the corresponding slice from the padded input matrix, perform element-wise multiplication with the kernel, sum the results, and store the rounded value in the output matrix.\n4. **Rounding**: Each value in the output matrix is rounded to four decimal places before being stored.\n5. **Return**: The output matrix is converted to a list using `tolist()` and returned.\n\nThis implementation should work for any valid input matrix, kernel, padding, and stride values."}
{"task_id": 42, "completion_id": 0, "completion": "To implement the ReLU activation function in Python, you can use a simple conditional statement to check if the input is greater than zero. If it is, return the input; otherwise, return zero. Here's the complete implementation:\n\n```python\ndef relu(z: float) -> float:\n    return max(0, z)\n```\n\n### Explanation:\n- The `relu` function takes a single argument `z`, which is a float.\n- The `max` function is used to return the greater of two values: `0` and `z`. This effectively implements the ReLU function, which outputs `z` if `z` is greater than zero, and `0` otherwise."}
{"task_id": 43, "completion_id": 0, "completion": "To implement the Ridge Regression loss function, we need to calculate the Mean Squared Error (MSE) between the predicted values and the true labels, and add a regularization term that penalizes large coefficients. The regularization term is proportional to the square of the L2 norm of the coefficients, scaled by the regularization parameter `alpha`.\n\nHere's the implementation of the `ridge_loss` function:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate predictions\n    y_pred = X.dot(w)\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate regularization term (L2 norm of the coefficients)\n    regularization_term = alpha * np.sum(w ** 2)\n    \n    # Calculate Ridge loss\n    ridge_loss_value = mse + regularization_term\n    \n    # Round the result to the nearest 4th decimal\n    return round(ridge_loss_value, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# w = np.array([0.5, -0.5])\n# y_true = np.array([1, 2])\n# alpha = 0.1\n# print(ridge_loss(X, w, y_true, alpha))\n```\n\n### Explanation:\n1. **Predictions**: We calculate the predicted values `y_pred` by performing a dot product between the feature matrix `X` and the coefficients `w`.\n2. **Mean Squared Error (MSE)**: We compute the MSE by taking the mean of the squared differences between the true labels `y_true` and the predicted values `y_pred`.\n3. **Regularization Term**: The regularization term is calculated as `alpha * np.sum(w ** 2)`, which penalizes large values in the coefficient vector `w`.\n4. **Ridge Loss**: The Ridge loss is the sum of the MSE and the regularization term.\n5. **Rounding**: The final result is rounded to the nearest 4th decimal using `round()`.\n\nThis function provides a straightforward implementation of the Ridge Regression loss, combining both the error term and the regularization term to prevent overfitting."}
{"task_id": 44, "completion_id": 0, "completion": "To implement the Leaky ReLU activation function, we need to define a function that applies the Leaky ReLU formula. The Leaky ReLU function is defined as:\n\n\\[ \\text{Leaky ReLU}(z) = \n\\begin{cases} \nz, & \\text{if } z \\geq 0 \\\\\n\\alpha \\cdot z, & \\text{if } z < 0 \n\\end{cases} \\]\n\nHere's how you can implement this in Python:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Apply the Leaky ReLU activation function to the input value z.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The result of applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\n```\n\n### Explanation:\n- The function `leaky_relu` takes two parameters: `z` (the input value) and `alpha` (the slope for negative inputs, with a default value of 0.01).\n- It checks if `z` is greater than or equal to 0. If so, it returns `z` itself.\n- If `z` is less than 0, it returns `alpha * z`, which applies the slope to the negative input.\n\nThis function will return a float value, which is the result of applying the Leaky ReLU activation function to the input `z`."}
{"task_id": 45, "completion_id": 0, "completion": "To implement the linear kernel function, we need to compute the dot product of the two input vectors `x1` and `x2`. The dot product is a fundamental operation in linear algebra, and it can be efficiently computed using NumPy's `dot` function. Here's how you can implement the `kernel_function`:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    # Ensure the inputs are numpy arrays\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    \n    # Compute the dot product\n    return np.dot(x1, x2)\n```\n\n### Explanation:\n- **Input Conversion**: The function first converts the input vectors `x1` and `x2` to NumPy arrays to ensure compatibility with NumPy operations.\n- **Dot Product Calculation**: The `np.dot` function is used to compute the dot product of the two vectors, which is the essence of the linear kernel.\n- **Return Value**: The result of the dot product is returned as a float, representing the linear kernel value.\n\nThis function assumes that `x1` and `x2` are one-dimensional arrays or lists of the same length. If they are not, NumPy will raise an error, which is useful for debugging and ensuring correct input dimensions."}
{"task_id": 46, "completion_id": 0, "completion": "To implement the precision metric, we need to calculate the number of true positives and false positives from the given `y_true` and `y_pred` arrays. Precision is defined as the ratio of true positives to the sum of true positives and false positives. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (TP)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false positives (FP)\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if true_positives + false_positives == 0:\n        return 0.0  # Avoid division by zero; precision is undefined in this case\n    else:\n        precision_value = true_positives / (true_positives + false_positives)\n        return precision_value\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n\nprint(precision(y_true, y_pred))  # Output: 0.75\n```\n\n### Explanation:\n- **True Positives (TP)**: These are the cases where both `y_true` and `y_pred` are 1.\n- **False Positives (FP)**: These are the cases where `y_true` is 0 but `y_pred` is 1.\n- The precision is calculated as `TP / (TP + FP)`.\n- If there are no positive predictions (`TP + FP == 0`), precision is set to 0.0 to avoid division by zero, as precision is undefined in such cases."}
{"task_id": 47, "completion_id": 0, "completion": "To implement the three variants of gradient descent using Mean Squared Error (MSE) as the loss function, we need to define a function that can handle the different methods: Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef compute_mse(X, y, weights):\n    predictions = X.dot(weights)\n    errors = predictions - y\n    mse = np.mean(errors ** 2)\n    return mse\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    m = len(y)\n    \n    for iteration in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            indices = np.arange(m)\n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            indices = np.random.randint(0, m, 1)\n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            indices = np.random.choice(m, batch_size, replace=False)\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n        \n        X_batch = X[indices]\n        y_batch = y[indices]\n        \n        predictions = X_batch.dot(weights)\n        errors = predictions - y_batch\n        gradients = 2 / len(y_batch) * X_batch.T.dot(errors)\n        \n        weights -= learning_rate * gradients\n        \n        # Optionally, you can print the MSE to monitor the training process\n        # mse = compute_mse(X, y, weights)\n        # print(f\"Iteration {iteration+1}/{n_iterations}, MSE: {mse:.4f}\")\n    \n    # Round the weights to the nearest 4th decimal\n    weights = np.round(weights, 4)\n    \n    # Return the weights as a list\n    return weights.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([7, 8, 9])\n# weights = np.array([0.1, 0.2])\n# learning_rate = 0.01\n# n_iterations = 1000\n# batch_size = 2\n# method = 'mini-batch'\n# result = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method)\n# print(result)\n```\n\n### Explanation:\n- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n- **Stochastic Gradient Descent (SGD)**: Uses a single randomly chosen data point to compute the gradient.\n- **Mini-Batch Gradient Descent**: Uses a randomly chosen subset of the data (of size `batch_size`) to compute the gradient.\n- The function updates the weights iteratively based on the chosen method.\n- The weights are rounded to the nearest 4th decimal place before returning them as a list.\n\nThis implementation allows you to specify the method of gradient descent you want to use by setting the `method` parameter to `'batch'`, `'stochastic'`, or `'mini-batch'`."}
{"task_id": 48, "completion_id": 0, "completion": "To implement the Reduced Row Echelon Form (RREF) function, we need to follow the Gaussian elimination process with additional steps to ensure each leading entry in a row is 1 and all other elements in the column containing the leading 1 are zeros. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the input matrix to a numpy array for easier manipulation\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    row_index = 0\n\n    for col_index in range(cols):\n        # Find the pivot row\n        pivot_row = None\n        for r in range(row_index, rows):\n            if A[r, col_index] != 0:\n                pivot_row = r\n                break\n\n        if pivot_row is None:\n            continue\n\n        # Swap the current row with the pivot row\n        A[[row_index, pivot_row]] = A[[pivot_row, row_index]]\n\n        # Normalize the pivot row\n        pivot_value = A[row_index, col_index]\n        A[row_index] = A[row_index] / pivot_value\n\n        # Eliminate all other entries in the current column\n        for r in range(rows):\n            if r != row_index:\n                factor = A[r, col_index]\n                A[r] = A[r] - factor * A[row_index]\n\n        # Move to the next row\n        row_index += 1\n\n    # Convert the numpy array back to a list\n    return A.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nrref_matrix = rref(matrix)\nprint(rref_matrix)\n```\n\n### Explanation:\n1. **Initialization**: Convert the input matrix to a numpy array for easier manipulation. This allows us to use numpy's array operations for row swapping and scaling.\n\n2. **Pivot Finding**: For each column, find the first row below the current row index that has a non-zero entry. This row becomes the pivot row.\n\n3. **Row Swapping**: Swap the current row with the pivot row to bring the pivot to the current row position.\n\n4. **Row Normalization**: Normalize the pivot row so that the pivot element becomes 1.\n\n5. **Column Elimination**: Eliminate all other entries in the current column by subtracting an appropriate multiple of the pivot row from each other row.\n\n6. **Row Advancement**: Move to the next row and repeat the process for the next column.\n\n7. **Return**: Convert the numpy array back to a list using `tolist()` method and return it.\n\nThis implementation handles matrices with zero rows and reducible matrices, ensuring the output is in Reduced Row Echelon Form."}
{"task_id": 49, "completion_id": 0, "completion": "To implement the Adam optimization algorithm, we need to maintain and update the first and second moment estimates of the gradients. Here's how you can implement the `adam_optimizer` function:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)  # First moment vector\n    v = np.zeros_like(x)  # Second moment vector\n    t = 0  # Time step\n\n    for _ in range(num_iterations):\n        t += 1\n        g = np.array(grad(x), dtype=np.float64)  # Compute gradient\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n\n        # Update parameters\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return np.sum(x ** 2)\n\ndef grad(x):\n    return 2 * x\n\n# Initial parameters\nx0 = [1.0, 2.0, 3.0]\n\n# Run Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0)\nprint(optimized_params)\n```\n\n### Explanation:\n- **Initialization**: We start by initializing the parameters `x`, the first moment vector `m`, and the second moment vector `v`.\n- **Gradient Calculation**: For each iteration, we compute the gradient `g` using the provided gradient function `grad`.\n- **Moment Updates**: We update the first and second moment estimates using the exponential decay rates `beta1` and `beta2`.\n- **Bias Correction**: We apply bias correction to the moment estimates to account for their initialization at zero.\n- **Parameter Update**: We update the parameters using the corrected moment estimates, the learning rate, and a small constant `epsilon` for numerical stability.\n- **Return**: After completing the specified number of iterations, we round the optimized parameters to the nearest 4th decimal and return them as a list.\n\nThis implementation should work for optimizing any differentiable function given its gradient."}
{"task_id": 50, "completion_id": 0, "completion": "To implement Lasso Regression using Gradient Descent, we need to update the weights and bias iteratively by considering both the gradient of the loss function and the L1 penalty. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Initialize weights and bias\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    \n    # Gradient Descent\n    for iteration in range(max_iter):\n        # Calculate predictions\n        y_pred = np.dot(X, w) + b\n        \n        # Calculate gradients\n        dw = -(1/n_samples) * np.dot(X.T, (y - y_pred)) + alpha * np.sign(w)\n        db = -(1/n_samples) * np.sum(y - y_pred)\n        \n        # Update weights and bias\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        \n        # Calculate the loss (Mean Squared Error with L1 penalty)\n        loss = (1/(2*n_samples)) * np.sum((y - y_pred)**2) + alpha * np.sum(np.abs(w))\n        \n        # Check for convergence\n        if np.linalg.norm(dw) < tol and np.abs(db) < tol:\n            print(f\"Converged after {iteration} iterations.\")\n            break\n    \n    # Round results to the nearest 4th decimal\n    w = np.round(w, 4)\n    b = round(b, 4)\n    \n    # Return weights and bias as a list\n    return w.tolist(), b\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# alpha = 0.1\n# learning_rate = 0.01\n# max_iter = 1000\n# tol = 1e-4\n# weights, bias = l1_regularization_gradient_descent(X, y, alpha, learning_rate, max_iter, tol)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the weights `w` and bias `b` to zeros.\n\n2. **Gradient Calculation**: For each iteration, we calculate the gradient of the loss function with respect to the weights and bias. The gradient for the weights includes the L1 penalty term `alpha * np.sign(w)`.\n\n3. **Weight and Bias Update**: We update the weights and bias using the calculated gradients and the learning rate.\n\n4. **Loss Calculation**: The loss function includes both the mean squared error and the L1 penalty term.\n\n5. **Convergence Check**: We check for convergence by monitoring the norm of the weight gradient and the absolute change in bias. If both are below the tolerance level `tol`, we stop the iterations.\n\n6. **Rounding**: Finally, we round the weights and bias to the nearest 4th decimal and return them as a list.\n\nThis implementation should help in feature selection by shrinking some of the feature coefficients to zero due to the L1 penalty."}
{"task_id": 51, "completion_id": 0, "completion": "To solve the problem of calculating the Optimal String Alignment (OSA) distance between two strings, we can use dynamic programming. The OSA distance is similar to the Levenshtein distance but includes an additional operation: transposing two adjacent characters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Get the lengths of the source and target strings\n    len_source = len(source)\n    len_target = len(target)\n    \n    # Create a 2D array to store the distances\n    # (len_source + 1) x (len_target + 1) because we include the empty prefix\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Initialize the base cases\n    for i in range(len_source + 1):\n        dp[i][0] = i  # Cost of deleting all characters from source\n    for j in range(len_target + 1):\n        dp[0][j] = j  # Cost of inserting all characters into source\n    \n    # Fill the dp table\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            # If the characters are the same, no new cost is added\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            \n            # Calculate the minimum cost considering all operations\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,      # Deletion\n                dp[i][j - 1] + 1,      # Insertion\n                dp[i - 1][j - 1] + cost # Substitution\n            )\n            \n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)  # Transposition\n    \n    # The OSA distance is the value in the bottom-right corner of the matrix\n    return dp[len_source][len_target]\n\n# Example usage:\ns1 = \"caper\"\ns2 = \"acer\"\nprint(OSA(s1, s2))  # Output: 2\n```\n\n### Explanation:\n- **Initialization**: We initialize a 2D array `dp` where `dp[i][j]` represents the OSA distance between the first `i` characters of `source` and the first `j` characters of `target`.\n- **Base Cases**: The first row and column are initialized to represent the cost of converting an empty string to a prefix of the other string, which is simply the length of that prefix.\n- **Dynamic Programming Transition**: For each pair of indices `(i, j)`, we calculate the minimum cost considering:\n  - Deletion from `source`\n  - Insertion into `source`\n  - Substitution between `source[i-1]` and `target[j-1]`\n  - Transposition of adjacent characters if applicable\n- **Result**: The OSA distance is found at `dp[len_source][len_target]`, which gives the minimum number of edits required to transform the entire `source` string into the `target` string."}
{"task_id": 52, "completion_id": 0, "completion": "Certainly! Below is the implementation of the recall metric for binary classification:\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP) and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    if TP + FN == 0:\n        return 0.0\n    \n    recall_value = TP / (TP + FN)\n    \n    # Return recall rounded to three decimal places\n    return round(recall_value, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1, 0]\n# y_pred = [1, 0, 0, 1, 0, 1, 1]\n# print(recall(y_true, y_pred))  # Output: 0.667\n```\n\n### Explanation:\n- **True Positives (TP)**: These are the cases where the model correctly predicts the positive class (1).\n- **False Negatives (FN)**: These are the cases where the model incorrectly predicts the negative class (0) for an actual positive instance.\n- **Recall Formula**: Recall is calculated as \\( \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\).\n- **Handling Division by Zero**: If there are no actual positive instances (i.e., \\( \\text{TP} + \\text{FN} = 0 \\)), the recall is set to 0.0 to avoid division by zero.\n- **Rounding**: The recall value is rounded to three decimal places for precision."}
{"task_id": 53, "completion_id": 0, "completion": "To implement the self-attention mechanism, we need to compute the attention scores using the query (Q), key (K), and value (V) matrices. The self-attention mechanism involves the following steps:\n\n1. Compute the dot product of the query matrix Q with the key matrix K^T (transpose of K) to get the attention scores.\n2. Scale the attention scores by dividing by the square root of the dimension of the key vectors (often denoted as `d_k`).\n3. Apply a softmax function to the scaled attention scores to obtain the attention weights.\n4. Multiply the attention weights with the value matrix V to get the final self-attention output.\n\nHere's the implementation of the self-attention mechanism:\n\n```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    # Step 1: Compute the dot product of Q and K^T\n    attention_scores = np.dot(Q, K.T)\n    \n    # Step 2: Scale the attention scores\n    d_k = K.shape[-1]  # Dimension of the key vectors\n    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n    \n    # Step 3: Apply softmax to get attention weights\n    attention_weights = np.exp(scaled_attention_scores)\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Step 4: Compute the self-attention output\n    attention_output = np.dot(attention_weights, V)\n    \n    # Round the results to the nearest 4th decimal\n    attention_output = np.round(attention_output, 4)\n    \n    # Return the result as a python list\n    return attention_output.tolist()\n\n# Example usage:\n# Q, K, V are numpy arrays with the same number of rows (sequence length)\n# and the number of columns representing the feature dimension.\nQ = np.array([[1, 0, 1], [0, 1, 0]])\nK = np.array([[1, 0, 1], [0, 1, 0]])\nV = np.array([[1, 2], [3, 4]])\n\noutput = self_attention(Q, K, V)\nprint(output)\n```\n\nThis code defines the `self_attention` function, which takes query, key, and value matrices as input and returns the self-attention output as a list. The attention scores are computed, scaled, and normalized using the softmax function, and the final output is rounded to the nearest 4th decimal before being converted to a list."}
{"task_id": 54, "completion_id": 0, "completion": "To implement a simple RNN cell, we need to iterate over the sequence of input vectors, updating the hidden state at each step using the provided weight matrices and bias vector. We'll use the tanh activation function for the hidden state updates. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier matrix operations\n    input_sequence = np.array(input_sequence)\n    h_t = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Iterate over each input vector in the sequence\n    for x_t in input_sequence:\n        # Compute the new hidden state\n        h_t = np.tanh(np.dot(x_t, Wx) + np.dot(h_t, Wh) + b)\n    \n    # Round the final hidden state to 4 decimal places\n    h_t_rounded = np.round(h_t, 4)\n    \n    # Convert the final hidden state to a list and return\n    return h_t_rounded.tolist()\n\n# Example usage:\n# input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n# initial_hidden_state = [0.0, 0.0]\n# Wx = [[0.1, 0.2], [0.3, 0.4]]\n# Wh = [[0.5, 0.6], [0.7, 0.8]]\n# b = [0.1, 0.2]\n# final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n# print(final_hidden_state)\n```\n\n### Explanation:\n1. **Input Conversion**: We convert all inputs to numpy arrays for efficient matrix operations.\n2. **Iteration**: We iterate over each input vector in the sequence, updating the hidden state using the formula:\n   \\[\n   h_t = \\tanh(x_t \\cdot Wx + h_{t-1} \\cdot Wh + b)\n   \\]\n   where \\(x_t\\) is the current input vector, \\(h_{t-1}\\) is the previous hidden state, \\(Wx\\) is the input-to-hidden weight matrix, \\(Wh\\) is the hidden-to-hidden weight matrix, and \\(b\\) is the bias vector.\n3. **Rounding**: After processing the entire sequence, we round the final hidden state to four decimal places.\n4. **Return**: The final hidden state is converted to a list using `tolist()` and returned.\n\nThis implementation should work for any sequence of input vectors, initial hidden state, weight matrices, and bias vector, as long as their dimensions are compatible."}
{"task_id": 55, "completion_id": 0, "completion": "To implement a 2D translation matrix and apply it to a set of points, we can use numpy to handle the matrix operations efficiently. The translation matrix for 2D space is defined as:\n\n\\[\n\\begin{bmatrix}\n1 & 0 & tx \\\\\n0 & 1 & ty \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nThis matrix is used to translate a point \\([x, y]\\) to a new point \\([x', y']\\) by adding \\(tx\\) to the x-coordinate and \\(ty\\) to the y-coordinate. The points are represented in homogeneous coordinates as \\([x, y, 1]\\).\n\nHere's how you can implement the `translate_object` function:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create the translation matrix\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Convert points to homogeneous coordinates by adding a 1 to each point\n    homogeneous_points = np.array([point + [1] for point in points])\n    \n    # Apply the translation matrix to each point\n    translated_points = homogeneous_points.dot(translation_matrix.T)\n    \n    # Convert back to 2D coordinates and return as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\n### Explanation:\n1. **Translation Matrix**: We define a 3x3 translation matrix that includes the translation distances \\(tx\\) and \\(ty\\).\n\n2. **Homogeneous Coordinates**: Each point is converted to homogeneous coordinates by appending a 1, making it \\([x, y, 1]\\).\n\n3. **Matrix Multiplication**: We multiply each point by the translation matrix using the dot product. This operation applies the translation to each point.\n\n4. **Convert Back to 2D**: After the transformation, we convert the points back to 2D by taking only the first two columns of the result.\n\n5. **Return as List**: Finally, we convert the numpy array back to a Python list using the `tolist()` method.\n\nThis function will return a new list of points that have been translated by the specified distances in the x and y directions."}
{"task_id": 56, "completion_id": 0, "completion": "To calculate the Kullback-Leibler (KL) divergence between two normal distributions, we can use the following formula:\n\nGiven two normal distributions:\n- \\( P \\sim N(\\mu_P, \\sigma_P^2) \\)\n- \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\)\n\nThe KL divergence from \\( Q \\) to \\( P \\) is given by:\n\n\\[\nD_{KL}(P \\parallel Q) = \\log\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n\\]\n\nThis formula is derived from the general definition of KL divergence for continuous distributions and is specific to normal distributions.\n\nHere's the implementation in Python:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: The KL divergence D_KL(P || Q).\n    \"\"\"\n    # Ensure the standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    # Calculate the KL divergence using the formula\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n    \n    return kl_div\n\n# Example usage:\n# mu_p, sigma_p = 0, 1\n# mu_q, sigma_q = 1, 2\n# print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n```\n\nThis function takes the means and standard deviations of two normal distributions as input and returns the KL divergence between them. Note that the standard deviations must be positive, as they represent the spread of the distributions."}
{"task_id": 57, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gauss-Seidel method for solving a system of linear equations:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Ensure A is a numpy array\n    A = np.array(A, dtype=float)\n    # Ensure b is a numpy array\n    b = np.array(b, dtype=float)\n    \n    # Number of variables\n    num_vars = len(b)\n    \n    # Initialize x with zeros if x_ini is not provided\n    if x_ini is None:\n        x = np.zeros(num_vars)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform n iterations\n    for _ in range(n):\n        for i in range(num_vars):\n            # Calculate the sum for the current row\n            sum_ = b[i]\n            for j in range(num_vars):\n                if j != i:\n                    sum_ -= A[i][j] * x[j]\n            \n            # Update the value of x[i] using the most recent values\n            x[i] = sum_ / A[i][i]\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the result as a Python list\n    return x.tolist()\n\n# Example usage:\nA = [[4, 1, 2],\n     [3, 5, 1],\n     [1, 1, 3]]\nb = [4, 7, 3]\nn = 10\nx_ini = [0, 0, 0]\n\nsolution = gauss_seidel(A, b, n, x_ini)\nprint(solution)\n```\n\n### Explanation:\n- **Initialization**: If no initial guess is provided, the solution vector `x` is initialized to zeros.\n- **Iteration**: For each iteration, the solution vector `x` is updated using the most recent values available. The update for each element `x[i]` is based on the formula derived from the Gauss-Seidel method.\n- **Rounding**: After completing the iterations, the solution vector is rounded to the nearest 4th decimal.\n- **Output**: The solution vector is returned as a Python list using `tolist()`.\n\nThis implementation assumes that the matrix `A` is diagonally dominant, which is a common requirement for the convergence of the Gauss-Seidel method."}
{"task_id": 58, "completion_id": 0, "completion": "To implement the Gaussian Elimination method with partial pivoting, we need to transform the matrix \\( A \\) into an upper triangular form and then perform backward substitution to find the solution vector \\( x \\). Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = len(b)\n    \n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Partial pivoting: find the row with the largest element in the current column\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        \n        # Swap the current row with the row having the largest element\n        if i != max_row:\n            A[[i, max_row]] = A[[max_row, i]]\n            b[[i, max_row]] = b[[max_row, i]]\n        \n        # Eliminate the entries below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j] -= factor * b[i]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    x = np.round(x, 4).tolist()\n    \n    return x\n\n# Example usage:\nA = [[3, 2, -4], [2, 3, 3], [5, -3, 1]]\nb = [3, 15, 14]\nsolution = gaussian_elimination(A, b)\nprint(solution)\n```\n\n### Explanation:\n1. **Partial Pivoting**: This step ensures numerical stability by swapping rows to make the largest element in the current column the pivot element.\n2. **Forward Elimination**: This process transforms the matrix \\( A \\) into an upper triangular form by eliminating the entries below the pivot.\n3. **Backward Substitution**: Once the matrix is in upper triangular form, we solve for the variables starting from the last row upwards.\n4. **Rounding and Conversion**: The solution vector is rounded to the nearest 4th decimal and converted to a Python list using `tolist()`.\n\nThis implementation should work for any system of linear equations that can be solved using Gaussian Elimination."}
{"task_id": 59, "completion_id": 0, "completion": "To implement the Long Short-Term Memory (LSTM) network, we need to define the forward pass that processes a sequence of inputs and updates the hidden and cell states at each time step. Below is the complete implementation of the `LSTM` class:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        hidden_states = []\n\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)\n            combined = np.vstack((h_t, x_t))\n\n            # Forget gate\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n\n            # Input gate\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n\n            # Candidate cell state\n            c_hat_t = self.tanh(np.dot(self.Wc, combined) + self.bc)\n\n            # Update cell state\n            c_t = f_t * c_t + i_t * c_hat_t\n\n            # Output gate\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n\n            # Update hidden state\n            h_t = o_t * self.tanh(c_t)\n\n            # Store hidden state\n            hidden_states.append(h_t)\n\n        # Convert hidden states to list and round to 4 decimal places\n        hidden_states = np.array(hidden_states).reshape(x.shape[0], -1).round(4).tolist()\n        final_hidden_state = h_t.round(4).tolist()\n        final_cell_state = c_t.round(4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage:\n# lstm = LSTM(input_size=3, hidden_size=2)\n# x = np.random.randn(5, 3)  # Sequence of 5 time steps, each with 3 features\n# initial_hidden_state = np.zeros((2, 1))\n# initial_cell_state = np.zeros((2, 1))\n# hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n```\n\n### Explanation:\n- **Initialization**: The weights and biases for the forget gate, input gate, candidate cell state, and output gate are initialized randomly. Biases are initialized to zero.\n- **Forward Pass**: For each time step in the input sequence:\n  - The input and previous hidden state are concatenated.\n  - The forget gate, input gate, candidate cell state, and output gate are computed using the respective weights and biases.\n  - The cell state is updated using the forget gate and input gate.\n  - The hidden state is updated using the output gate and the updated cell state.\n  - The hidden state for each time step is stored.\n- **Output**: The hidden states for all time steps, the final hidden state, and the final cell state are returned, rounded to 4 decimal places and converted to lists."}
{"task_id": 60, "completion_id": 0, "completion": "To implement the TF-IDF (Term Frequency-Inverse Document Frequency) calculation, we need to follow these steps:\n\n1. **Term Frequency (TF)**: Calculate how frequently a term appears in a document. This is usually the number of times a term appears in a document divided by the total number of terms in the document.\n\n2. **Inverse Document Frequency (IDF)**: Calculate how important a term is. This is done by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. We will use smoothing to avoid division by zero.\n\n3. **TF-IDF**: Multiply the TF and IDF values for each term.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    # Handle empty corpus\n    if not corpus:\n        return []\n\n    # Calculate document frequency for each term in the query\n    df = {term: 0 for term in query}\n    for document in corpus:\n        unique_terms = set(document)\n        for term in query:\n            if term in unique_terms:\n                df[term] += 1\n\n    # Calculate IDF with smoothing\n    num_documents = len(corpus)\n    idf = {term: np.log((num_documents + 1) / (df[term] + 1)) + 1 for term in query}\n\n    # Calculate TF-IDF for each document\n    tf_idf_scores = []\n    for document in corpus:\n        doc_length = len(document)\n        tf_idf_doc = []\n        for term in query:\n            # Calculate term frequency\n            term_count = document.count(term)\n            tf = term_count / doc_length if doc_length > 0 else 0\n            # Calculate TF-IDF\n            tf_idf = tf * idf[term]\n            tf_idf_doc.append(round(tf_idf, 5))\n        tf_idf_scores.append(tf_idf_doc)\n\n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"this\", \"is\", \"a\", \"sample\", \"document\"],\n    [\"this\", \"document\", \"is\", \"another\", \"example\"],\n    [\"example\", \"document\", \"with\", \"different\", \"words\"]\n]\nquery = [\"this\", \"document\", \"example\", \"words\"]\n\ntf_idf_scores = compute_tf_idf(corpus, query)\nprint(tf_idf_scores)\n```\n\n### Explanation:\n\n- **Document Frequency (DF)**: We count how many documents contain each term in the query.\n- **Inverse Document Frequency (IDF)**: We apply smoothing by adding 1 to both the numerator and denominator in the IDF formula to prevent division by zero.\n- **Term Frequency (TF)**: We calculate the frequency of each term in the document.\n- **TF-IDF Calculation**: We multiply the TF and IDF values for each term and round the result to five decimal places.\n- **Handling Edge Cases**: The implementation handles empty corpus, documents with no words, and terms not present in the corpus gracefully.\n\nThis implementation should be robust and handle various edge cases effectively."}
{"task_id": 61, "completion_id": 0, "completion": "To implement the F-Score calculation for binary classification, we need to compute Precision and Recall first, and then use them to calculate the F-Score. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        return 0.0\n    \n    f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n    \n    return round(f_score, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n# y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n# beta = 1.0\n# print(f_score(y_true, y_pred, beta))  # Output: F-Score rounded to three decimal places\n```\n\n### Explanation:\n- **True Positives (TP)**: The number of instances where the model correctly predicted the positive class.\n- **False Positives (FP)**: The number of instances where the model incorrectly predicted the positive class.\n- **False Negatives (FN)**: The number of instances where the model failed to predict the positive class.\n\n- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. It is calculated as `TP / (TP + FP)`.\n- **Recall**: The ratio of correctly predicted positive observations to all the actual positives. It is calculated as `TP / (TP + FN)`.\n\n- **F-Score**: A weighted harmonic mean of Precision and Recall. The formula is `(1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)`. When `beta` is 1, it becomes the F1-Score, which is the harmonic mean of Precision and Recall.\n\nThe function handles cases where Precision and Recall are zero to avoid division by zero errors."}
{"task_id": 62, "completion_id": 0, "completion": "To implement a simple RNN with Backpropagation Through Time (BPTT), we need to define the methods for initializing weights, performing the forward pass, and executing the backward pass. Below is the complete implementation:\n\n```python\nimport numpy as np\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    W_xh = np.random.randn(hidden_size, input_size) * 0.01\n    W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n    W_hy = np.random.randn(output_size, hidden_size) * 0.01\n    b_h = np.zeros((hidden_size, 1))\n    b_y = np.zeros((output_size, 1))\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    h_prev = np.zeros((hidden_size, 1))\n    outputs = []\n    last_inputs = []\n    last_hiddens = [h_prev]\n\n    for x in input_sequence:\n        x = x.reshape(-1, 1)  # Ensure x is a column vector\n        h_next = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h_prev) + b_h)\n        y = np.dot(W_hy, h_next) + b_y\n        outputs.append(y)\n        last_inputs.append(x)\n        last_hiddens.append(h_next)\n        h_prev = h_next\n\n    return outputs, last_inputs, last_hiddens\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n\n    dh_next = np.zeros_like(last_hiddens[0])\n\n    for t in reversed(range(len(input_sequence))):\n        y = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        dy = y - y_true\n\n        dW_hy += np.dot(dy, last_hiddens[t+1].T)\n        db_y += dy\n\n        dh = np.dot(W_hy.T, dy) + dh_next\n        dh_raw = (1 - last_hiddens[t+1] ** 2) * dh\n\n        db_h += dh_raw\n        dW_xh += np.dot(dh_raw, last_inputs[t].T)\n        dW_hh += np.dot(dh_raw, last_hiddens[t].T)\n\n        dh_next = np.dot(W_hh.T, dh_raw)\n\n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h -= learning_rate * db_h\n    b_y -= learning_rate * db_y\n\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef entrypoint(input_sequence, expected_output, input_size, hidden_size, output_size, learning_rate=0.01, epochs=100, random_seed=42):\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n    return np.round(np.array(outputs), 5).tolist()\n```\n\n### Explanation:\n1. **Weight Initialization**: The weights are initialized to small random values, and biases are initialized to zero. This helps in breaking symmetry and ensuring that the network learns effectively.\n\n2. **Forward Pass**: The RNN processes each input in the sequence, updating the hidden state and computing the output. The hidden state is updated using the current input and the previous hidden state.\n\n3. **Backward Pass (BPTT)**: The gradients are computed by iterating backward through the sequence. The loss is calculated using the difference between the predicted and expected outputs. Gradients are accumulated for each weight and bias, and the weights are updated using these gradients.\n\n4. **Entrypoint Function**: This function initializes the weights, performs training over multiple epochs, and returns the final outputs rounded to five decimal places."}
{"task_id": 63, "completion_id": 0, "completion": "Here's the implementation of the Conjugate Gradient method for solving linear systems:\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Initialize the solution vector\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    \n    # Compute the initial residual\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n        \n        # Check for convergence\n        if np.sqrt(rs_new) < tol:\n            break\n        \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n    \n    # Round the solution to 8 decimal places and convert to list\n    return np.round(x, 8).tolist()\n\n# Example usage:\n# A = np.array([[4, 1], [1, 3]])\n# b = np.array([1, 2])\n# n = 1000\n# x0 = np.array([0, 0])\n# solution = conjugate_gradient(A, b, n, x0)\n# print(solution)\n```\n\n### Explanation:\n- **Initialization**: The function initializes the solution vector `x`. If no initial guess `x0` is provided, it defaults to a zero vector.\n- **Residual and Direction**: The initial residual `r` is calculated as `b - Ax`, and the initial direction `p` is set to `r`.\n- **Iteration**: The algorithm iteratively updates the solution vector `x`, the residual `r`, and the direction `p` using the Conjugate Gradient method formulas.\n- **Convergence Check**: The loop breaks if the residual norm is below the specified tolerance `tol`.\n- **Return**: The solution vector `x` is rounded to 8 decimal places and converted to a list before returning.\n\nThis implementation efficiently solves large, sparse, symmetric, positive-definite linear systems using the Conjugate Gradient method."}
{"task_id": 64, "completion_id": 0, "completion": "To implement the Gini Impurity calculation, we need to determine the probability of each class in the list and use these probabilities to compute the impurity. The formula for Gini Impurity is:\n\n\\[ Gini = 1 - \\sum (p_i^2) \\]\n\nwhere \\( p_i \\) is the probability of class \\( i \\). Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each class\n    unique, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the probabilities of each class\n    probabilities = counts / counts.sum()\n    \n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage:\n# y = [1, 1, 2, 2, 3]\n# print(gini_impurity(y))  # Output: 0.64\n```\n\n### Explanation:\n1. **Counting Occurrences**: We use `np.unique` with `return_counts=True` to get the unique classes and their counts in the list `y`.\n2. **Calculating Probabilities**: We divide the counts by the total number of elements to get the probability of each class.\n3. **Computing Gini Impurity**: We apply the formula \\( 1 - \\sum (p_i^2) \\) to calculate the impurity.\n4. **Rounding**: Finally, we round the result to three decimal places for precision.\n\nThis function will efficiently compute the Gini Impurity for any list of class labels provided."}
{"task_id": 65, "completion_id": 0, "completion": "To convert a dense matrix into the Compressed Row Sparse (CSR) format, we need to extract the non-zero elements, their column indices, and the row pointers that indicate the start of each row in the values array. Here's how you can implement this conversion in Python:\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # Start with 0 for the first row\n\n    for row in dense_matrix:\n        for col_index, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_index)\n        # Append the cumulative count of non-zero elements up to the current row\n        row_pointers.append(len(values))\n\n    return values, column_indices, row_pointers\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [22, 0, 0, 0],\n    [0, 0, 0, 0],\n    [0, 7, 0, 5]\n]\n\nvalues, column_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointers:\", row_pointers)\n```\n\n### Explanation:\n- **Values Array**: This list contains all the non-zero elements of the matrix in row-major order.\n- **Column Indices Array**: This list contains the column indices corresponding to each non-zero element in the values array.\n- **Row Pointer Array**: This list starts with 0 and contains the cumulative count of non-zero elements up to each row, indicating where each row starts in the values array.\n\nThe example usage demonstrates how the function works with a sample dense matrix, printing out the CSR format components."}
{"task_id": 66, "completion_id": 0, "completion": "To compute the orthogonal projection of a vector \\( v \\) onto another vector \\( L \\), we can use the formula:\n\n\\[ \\text{proj}_L(v) = \\left( \\frac{v \\cdot L}{L \\cdot L} \\right) L \\]\n\nWhere:\n- \\( v \\cdot L \\) is the dot product of vectors \\( v \\) and \\( L \\).\n- \\( L \\cdot L \\) is the dot product of vector \\( L \\) with itself.\n- The result is a scalar that scales the vector \\( L \\).\n\nHere's the implementation of the `orthogonal_projection` function:\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_vL = sum(v_i * L_i for v_i, L_i in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_LL = sum(L_i * L_i for L_i in L)\n    \n    # Calculate the scalar for the projection\n    scalar = dot_vL / dot_LL\n    \n    # Calculate the projection vector\n    projection = [round(scalar * L_i, 3) for L_i in L]\n    \n    return projection\n\n# Example usage:\nv = [3, 4]\nL = [1, 2]\nprint(orthogonal_projection(v, L))  # Output: [2.2, 4.4]\n```\n\n### Explanation:\n1. **Dot Product Calculation**: We calculate the dot product of \\( v \\) and \\( L \\) using a generator expression inside the `sum` function.\n2. **Scalar Calculation**: The scalar is computed by dividing the dot product of \\( v \\) and \\( L \\) by the dot product of \\( L \\) with itself.\n3. **Projection Calculation**: We multiply each component of \\( L \\) by the scalar to get the projection vector, rounding each component to three decimal places.\n4. **Return**: The function returns the projection vector as a list."}
{"task_id": 67, "completion_id": 0, "completion": "To implement the Compressed Column Sparse Matrix (CSC) format, we need to traverse the dense matrix column by column, collecting non-zero values and their corresponding row indices, and marking the start of each column in the values list. Here's how you can implement this:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    column_pointer = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n\n    return (values, row_indices, column_pointer)\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3],\n    [4, 0, 0],\n    [0, 5, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row indices:\", row_indices)\nprint(\"Column pointer:\", column_pointer)\n```\n\n### Explanation:\n- **Values**: This list stores all non-zero elements of the matrix in column-major order.\n- **Row indices**: This list stores the row index for each non-zero element in the `values` list.\n- **Column pointer**: This list indicates the starting index of each column in the `values` list. The last element in `column_pointer` is the total number of non-zero elements, which helps in determining the range of indices for the last column.\n\nThe function iterates over each column and within each column, it iterates over each row to find non-zero elements. It appends these elements to the `values` list and their row indices to the `row_indices` list. The `column_pointer` list is updated to mark the start of each new column in the `values` list."}
{"task_id": 68, "completion_id": 0, "completion": "To compute the column space of a matrix using row echelon form, we need to identify the independent columns of the matrix. These columns correspond to the pivot columns in the row echelon form. Here's how you can implement the `matrix_image` function in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the input matrix to a NumPy array\n    A = np.array(A, dtype=float)\n    \n    # Get the number of rows and columns\n    rows, cols = A.shape\n    \n    # Perform Gaussian elimination to get the row echelon form\n    for i in range(min(rows, cols)):\n        # Find the pivot row\n        pivot_row = i + np.argmax(np.abs(A[i:, i]))\n        \n        # Swap the current row with the pivot row\n        A[[i, pivot_row]] = A[[pivot_row, i]]\n        \n        # If the pivot element is zero, continue to the next column\n        if A[i, i] == 0:\n            continue\n        \n        # Normalize the pivot row\n        A[i] = A[i] / A[i, i]\n        \n        # Eliminate the current column entries below the pivot\n        for j in range(i + 1, rows):\n            A[j] = A[j] - A[j, i] * A[i]\n    \n    # Identify the pivot columns\n    pivot_columns = []\n    for i in range(min(rows, cols)):\n        if np.any(A[i, :]):\n            pivot_columns.append(i)\n    \n    # Extract the independent columns from the original matrix\n    basis_vectors = A[:, pivot_columns]\n    \n    # Round the basis vectors to 8 decimal places\n    basis_vectors = np.round(basis_vectors, 8)\n    \n    # Convert the basis vectors to a list and return\n    return basis_vectors.tolist()\n\n# Example usage:\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix_image(A))\n```\n\n### Explanation:\n1. **Gaussian Elimination**: We perform Gaussian elimination to transform the matrix into row echelon form. This involves row swapping, normalization, and elimination to make the matrix upper triangular.\n\n2. **Pivot Columns**: After obtaining the row echelon form, the pivot columns (columns with leading 1s) are identified. These columns correspond to the independent columns of the original matrix.\n\n3. **Extract Basis Vectors**: The independent columns are extracted from the original matrix to form the basis of the column space.\n\n4. **Rounding and Conversion**: The basis vectors are rounded to 8 decimal places and converted to a list format for the final output.\n\nThis function will return the basis vectors that span the column space of the input matrix `A`."}
{"task_id": 69, "completion_id": 0, "completion": "To calculate the R-squared value, we need to follow these steps:\n\n1. Calculate the total sum of squares (TSS), which measures the total variance in the dependent variable:\n   \\[\n   \\text{TSS} = \\sum (y_{\\text{true}} - \\bar{y})^2\n   \\]\n   where \\(\\bar{y}\\) is the mean of the true values.\n\n2. Calculate the residual sum of squares (RSS), which measures the variance that is not explained by the model:\n   \\[\n   \\text{RSS} = \\sum (y_{\\text{true}} - y_{\\text{pred}})^2\n   \\]\n\n3. Compute the R-squared value using the formula:\n   \\[\n   R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n   \\]\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    # Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(r_squared(y_true, y_pred))  # Output should be 0.948\n```\n\nThis function will compute the R-squared value for the given true and predicted values, providing a measure of how well the model explains the variability of the dependent variable. The result is rounded to three decimal places as specified."}
{"task_id": 70, "completion_id": 0, "completion": "To solve the task of calculating the average brightness of a grayscale image represented as a 2D matrix, we need to implement the function `calculate_brightness(img)`. The function should handle several edge cases and return the average brightness rounded to two decimal places. Let's break down the steps needed to implement this function:\n\n1. **Check for an empty image matrix**: If the input matrix `img` is empty, return `-1`.\n\n2. **Check for inconsistent row lengths**: Ensure that all rows in the matrix have the same length. If they don't, return `-1`.\n\n3. **Check for valid pixel values**: Ensure all pixel values are within the range 0 to 255. If any value is outside this range, return `-1`.\n\n4. **Calculate the average brightness**: If all checks pass, calculate the average brightness by summing all pixel values and dividing by the total number of pixels. Round the result to two decimal places.\n\nHere's the implementation of the function:\n\n```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Get the number of columns in the first row\n    num_cols = len(img[0])\n    \n    # Initialize variables to calculate the sum of pixel values and count of pixels\n    total_brightness = 0\n    total_pixels = 0\n    \n    for row in img:\n        # Check for inconsistent row lengths\n        if len(row) != num_cols:\n            return -1\n        \n        for pixel in row:\n            # Check if pixel values are within the valid range\n            if not (0 <= pixel <= 255):\n                return -1\n            \n            # Accumulate the total brightness and count the pixels\n            total_brightness += pixel\n            total_pixels += 1\n    \n    # Calculate the average brightness\n    average_brightness = total_brightness / total_pixels\n    \n    # Return the average brightness rounded to two decimal places\n    return round(average_brightness, 2)\n\n# Example usage:\n# img = [\n#     [0, 255, 128],\n#     [64, 128, 192],\n#     [255, 0, 64]\n# ]\n# print(calculate_brightness(img))  # Output should be the average brightness\n```\n\nThis function will correctly handle the specified edge cases and compute the average brightness of the image when the input is valid."}
{"task_id": 71, "completion_id": 0, "completion": "To implement the `rmse` function, we need to ensure it handles various edge cases and computes the RMSE correctly. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are valid numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Inputs must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate the RMSE\n    error = y_true - y_pred\n    squared_error = np.square(error)\n    mean_squared_error = np.mean(squared_error)\n    root_mean_square_error = np.sqrt(mean_squared_error)\n    \n    # Return the RMSE rounded to three decimal places\n    return round(root_mean_square_error, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(rmse(y_true, y_pred))  # Output: 0.612\n```\n\n### Explanation:\n1. **Type Checking**: The function first checks if both `y_true` and `y_pred` are numpy arrays. If not, it raises a `TypeError`.\n\n2. **Empty Arrays**: It checks if either of the arrays is empty and raises a `ValueError` if so.\n\n3. **Shape Matching**: It ensures both arrays have the same shape, raising a `ValueError` if they don't.\n\n4. **RMSE Calculation**:\n   - Compute the error between the true and predicted values.\n   - Square the errors.\n   - Calculate the mean of the squared errors.\n   - Take the square root of the mean squared error to get the RMSE.\n\n5. **Rounding**: The final RMSE value is rounded to three decimal places before being returned.\n\nThis implementation ensures robustness by handling common edge cases and providing clear error messages when inputs are invalid."}
{"task_id": 72, "completion_id": 0, "completion": "To implement the Jaccard Index for binary classification, we need to calculate the intersection and union of the true and predicted labels. The intersection is the number of elements where both `y_true` and `y_pred` are 1, and the union is the number of elements where either `y_true` or `y_pred` is 1.\n\nHere's the implementation of the `jaccard_index` function:\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Convert inputs to numpy arrays if they aren't already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    \n    # Handle edge case where union is zero\n    if union == 0:\n        return 1.0 if intersection == 0 else 0.0\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0]\n# y_pred = [1, 0, 0, 1, 1]\n# print(jaccard_index(y_true, y_pred))  # Output: 0.5\n```\n\n### Explanation:\n1. **Intersection Calculation**: We use a logical AND operation to find elements where both `y_true` and `y_pred` are 1, and then sum these elements to get the intersection count.\n\n2. **Union Calculation**: We use a logical OR operation to find elements where either `y_true` or `y_pred` is 1, and then sum these elements to get the union count.\n\n3. **Edge Case Handling**: If the union is zero (which can happen if both arrays contain only zeros), we return 1.0 if the intersection is also zero (indicating perfect overlap of empty sets), otherwise 0.0.\n\n4. **Rounding**: The result is rounded to three decimal places for precision.\n\nThis function should work correctly for typical binary classification tasks, including edge cases where there is no overlap or both arrays contain only zeros."}
{"task_id": 73, "completion_id": 0, "completion": "To calculate the Dice Score for binary classification, we need to compare two binary arrays: `y_true` (the ground truth) and `y_pred` (the predicted labels). The Dice Score is a measure of overlap between these two sets, and it is particularly useful in scenarios like image segmentation where we want to measure how well the predicted segmentation matches the true segmentation.\n\nHere's how you can implement the `dice_score` function in Python:\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Convert inputs to numpy arrays if they aren't already\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Calculate the intersection\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the sum of the true and predicted elements\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Calculate the Dice Score\n    # Handle the edge case where both sums are zero\n    if sum_true + sum_pred == 0:\n        return 1.0 if intersection == 0 else 0.0\n    \n    dice = (2.0 * intersection) / (sum_true + sum_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0])\n# y_pred = np.array([0, 1, 1, 1, 0])\n# print(dice_score(y_true, y_pred))  # Output: 0.8\n```\n\n### Explanation:\n\n1. **Convert to Numpy Arrays**: Ensure that both `y_true` and `y_pred` are numpy arrays. This allows us to perform element-wise operations easily.\n\n2. **Calculate Intersection**: The intersection of `y_true` and `y_pred` is calculated by taking the element-wise product and summing the result. This gives the count of true positives.\n\n3. **Calculate Sums**: Compute the sum of elements in `y_true` and `y_pred` to get the total number of true positives and predicted positives, respectively.\n\n4. **Handle Edge Cases**: If both `sum_true` and `sum_pred` are zero, it means there are no true or predicted positives. In this case, if the intersection is also zero, the Dice Score should be 1.0 (perfect overlap of empty sets), otherwise 0.0.\n\n5. **Compute Dice Score**: Use the formula to compute the Dice Score and round it to three decimal places for precision.\n\nThis implementation should handle typical cases as well as edge cases where there are no positives in either or both arrays."}
{"task_id": 74, "completion_id": 0, "completion": "To implement the function `create_row_hv`, we need to follow the principles of Hyperdimensional Computing (HDC). In HDC, we typically use high-dimensional binary or bipolar vectors to represent data. The process involves creating hypervectors for each feature and its value, binding them together, and then bundling all these bound hypervectors to form a composite hypervector for the entire row.\n\nHere's a step-by-step implementation:\n\n```python\nimport numpy as np\n\ndef create_random_hv(dim, seed):\n    \"\"\"Generate a random bipolar hypervector of given dimensionality using a seed.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\n\ndef bind_hvs(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors using element-wise summation followed by thresholding.\"\"\"\n    sum_hv = np.sum(hvs, axis=0)\n    # Thresholding: convert sum_hv to bipolar (-1, 1) based on the sign\n    return np.where(sum_hv >= 0, 1, -1)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row using HDC.\"\"\"\n    bound_hvs = []\n    \n    for feature, value in row.items():\n        # Create hypervector for the feature name\n        feature_hv = create_random_hv(dim, random_seeds[feature])\n        \n        # Create hypervector for the feature value using the same seed for reproducibility\n        value_hv = create_random_hv(dim, random_seeds[feature] + hash(value))\n        \n        # Bind feature and value hypervectors\n        bound_hv = bind_hvs(feature_hv, value_hv)\n        \n        # Collect the bound hypervector\n        bound_hvs.append(bound_hv)\n    \n    # Bundle all bound hypervectors to create the composite hypervector\n    composite_hv = bundle_hvs(bound_hvs)\n    \n    # Return the composite hypervector as a list\n    return composite_hv.tolist()\n\n# Example usage:\nrow = {'feature1': 'value1', 'feature2': 'value2'}\ndim = 10000\nrandom_seeds = {'feature1': 42, 'feature2': 43}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\n\n### Explanation:\n1. **Random Hypervector Generation**: We generate random bipolar hypervectors using a seed for reproducibility. The `create_random_hv` function uses `np.random.choice` to create a vector of `-1` and `1`.\n\n2. **Binding Hypervectors**: The `bind_hvs` function performs element-wise multiplication to bind two hypervectors, representing the association between a feature and its value.\n\n3. **Bundling Hypervectors**: The `bundle_hvs` function sums the bound hypervectors and applies a threshold to convert the summed vector into a bipolar hypervector.\n\n4. **Composite Hypervector**: The `create_row_hv` function iterates over each feature-value pair in the row, creates and binds hypervectors, and bundles them to form the composite hypervector.\n\nThis implementation ensures that the hypervectors are reproducible and that the composite hypervector accurately represents the dataset row using HDC principles."}
{"task_id": 75, "completion_id": 0, "completion": "To generate a confusion matrix for a binary classification problem, we need to count the occurrences of true positives, true negatives, false positives, and false negatives. Here's how you can implement the `confusion_matrix` function:\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize counts for the confusion matrix\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    \n    # Iterate over each pair of true and predicted labels\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    \n    # Construct the confusion matrix\n    confusion_matrix = [\n        [true_positive, false_negative],\n        [false_positive, true_negative]\n    ]\n    \n    return confusion_matrix\n\n# Example usage:\ndata = [\n    [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0]\n]\nprint(confusion_matrix(data))\n```\n\n### Explanation:\n- **True Positive (TP)**: The model correctly predicts the positive class.\n- **True Negative (TN)**: The model correctly predicts the negative class.\n- **False Positive (FP)**: The model incorrectly predicts the positive class (Type I error).\n- **False Negative (FN)**: The model incorrectly predicts the negative class (Type II error).\n\nThe confusion matrix is structured as follows:\n```\n[[TP, FN],\n [FP, TN]]\n```\n\nThis function iterates over the list of `[y_true, y_pred]` pairs, updates the counts for each category, and finally returns the confusion matrix as a 2x2 list of lists."}
{"task_id": 76, "completion_id": 0, "completion": "To implement the `cosine_similarity` function, we need to calculate the cosine of the angle between two vectors. This is done using the dot product of the vectors divided by the product of their magnitudes. Here's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Ensure the vectors are not empty or have zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot be empty or have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the result rounded to three decimal places\n    return round(cosine_sim, 3)\n\n# Example usage:\n# v1 = np.array([1, 2, 3])\n# v2 = np.array([4, 5, 6])\n# print(cosine_similarity(v1, v2))  # Output: 0.974\n```\n\n### Explanation:\n1. **Shape Check**: We first check if the input vectors have the same shape. If not, we raise a `ValueError`.\n\n2. **Dot Product**: We calculate the dot product of the two vectors using `np.dot`.\n\n3. **Magnitude Calculation**: We calculate the magnitude (or norm) of each vector using `np.linalg.norm`.\n\n4. **Zero Magnitude Check**: We ensure neither vector has zero magnitude, as this would make the cosine similarity undefined. If either vector has zero magnitude, we raise a `ValueError`.\n\n5. **Cosine Similarity Calculation**: We compute the cosine similarity using the formula: \n   \\[\n   \\text{cosine similarity} = \\frac{\\text{dot product}}{\\text{magnitude of } v1 \\times \\text{magnitude of } v2}\n   \\]\n\n6. **Rounding**: Finally, we round the result to three decimal places before returning it."}
{"task_id": 77, "completion_id": 0, "completion": "To implement the `performance_metrics` function, we need to calculate the confusion matrix and derive the required performance metrics from it. Here's how you can achieve this:\n\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Initialize confusion matrix components\n    true_positive = false_positive = true_negative = false_negative = 0\n    \n    # Calculate confusion matrix components\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positive += 1\n        elif a == 0 and p == 1:\n            false_positive += 1\n        elif a == 0 and p == 0:\n            true_negative += 1\n        elif a == 1 and p == 0:\n            false_negative += 1\n    \n    # Confusion matrix\n    confusion_matrix = [[true_positive, false_positive], [false_negative, true_negative]]\n    \n    # Calculate metrics\n    total = len(actual)\n    accuracy = (true_positive + true_negative) / total\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) > 0 else 0\n    negative_predictive_value = true_negative / (true_negative + false_negative) if (true_negative + false_negative) > 0 else 0\n    \n    # Round metrics to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\nactual = [1, 0, 1, 1, 0, 0, 1, 0]\npredicted = [1, 0, 1, 0, 0, 0, 1, 1]\nresult = performance_metrics(actual, predicted)\nprint(result)\n```\n\n### Explanation:\n1. **Confusion Matrix Calculation**: We iterate over the `actual` and `predicted` lists simultaneously and count the occurrences of true positives, false positives, true negatives, and false negatives.\n\n2. **Metrics Calculation**:\n   - **Accuracy**: The ratio of correctly predicted instances to the total instances.\n   - **Precision**: The ratio of true positives to the sum of true positives and false positives.\n   - **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n   - **F1 Score**: The harmonic mean of precision and recall.\n   - **Specificity**: The ratio of true negatives to the sum of true negatives and false positives.\n   - **Negative Predictive Value**: The ratio of true negatives to the sum of true negatives and false negatives.\n\n3. **Rounding**: Each metric is rounded to three decimal places for the final output.\n\nThis function will provide a comprehensive set of performance metrics for evaluating a binary classification model."}
{"task_id": 78, "completion_id": 0, "completion": "To accomplish the task of calculating various descriptive statistics for a given dataset, we can utilize the NumPy library for efficient numerical operations and the SciPy library for calculating the mode. Below is the implementation of the `descriptive_statistics` function:\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Ensure data is a NumPy array for consistency\n    data = np.array(data)\n    \n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate median\n    median = np.median(data)\n    \n    # Calculate mode\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 0 else None\n    \n    # Calculate variance\n    variance = np.var(data, ddof=1)  # ddof=1 for sample variance\n    \n    # Calculate standard deviation\n    standard_deviation = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    \n    # Calculate percentiles\n    percentiles = np.percentile(data, [25, 50, 75])\n    percentile_25 = percentiles[0]\n    percentile_50 = percentiles[1]\n    percentile_75 = percentiles[2]\n    \n    # Calculate interquartile range (IQR)\n    interquartile_range = percentile_75 - percentile_25\n    \n    # Prepare the result dictionary\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,  # Mode is not rounded as it is a data value\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return result\n\n# Example usage:\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]\nstatistics = descriptive_statistics(data)\nprint(statistics)\n```\n\n### Explanation:\n- **Mean**: Calculated using `np.mean`.\n- **Median**: Calculated using `np.median`.\n- **Mode**: Calculated using `stats.mode` from the SciPy library. The mode is not rounded because it is a value from the dataset.\n- **Variance**: Calculated using `np.var` with `ddof=1` to get the sample variance.\n- **Standard Deviation**: Calculated using `np.std` with `ddof=1` for sample standard deviation.\n- **Percentiles**: Calculated using `np.percentile` for the 25th, 50th, and 75th percentiles.\n- **Interquartile Range (IQR)**: Calculated as the difference between the 75th and 25th percentiles.\n\nThe function returns a dictionary with all the required statistics, rounding the calculated values to four decimal places where applicable."}
{"task_id": 79, "completion_id": 0, "completion": "To calculate the probability of achieving exactly \\( k \\) successes in \\( n \\) independent Bernoulli trials, each with probability \\( p \\) of success, we can use the Binomial distribution formula:\n\n\\[ P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1-p)^{n-k} \\]\n\nWhere:\n- \\(\\binom{n}{k}\\) is the binomial coefficient, calculated as \\(\\frac{n!}{k!(n-k)!}\\).\n- \\( p^k \\) is the probability of success raised to the power of the number of successes.\n- \\( (1-p)^{n-k} \\) is the probability of failure raised to the power of the number of failures.\n\nHere's the Python function implementing this calculation:\n\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient\n    binom_coeff = math.comb(n, k)\n    \n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# print(binomial_probability(10, 3, 0.5))  # Example call to the function\n```\n\nThis function uses Python's `math.comb` to calculate the binomial coefficient, which is available in Python 3.8 and later. If you're using an earlier version of Python, you can calculate the binomial coefficient using `math.factorial`:\n\n```python\ndef binomial_coefficient(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n```\n\nReplace `math.comb(n, k)` with `binomial_coefficient(n, k)` if needed."}
{"task_id": 80, "completion_id": 0, "completion": "To calculate the probability density function (PDF) of the normal distribution, we can use the following formula:\n\n\\[ \\text{PDF}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\n\nWhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nHere's the Python function implementing this formula:\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the PDF using the normal distribution formula\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf_value = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(exponent)\n    \n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf_value, 5)\n\n# Example usage:\n# print(normal_pdf(0, 0, 1))  # Should return approximately 0.39894\n```\n\nThis function calculates the PDF of the normal distribution for a given value \\( x \\), mean \\( \\mu \\), and standard deviation \\( \\sigma \\), and returns the result rounded to 5 decimal places. You can test the function with different values to verify its correctness."}
{"task_id": 81, "completion_id": 0, "completion": "To calculate the probability of observing exactly \\( k \\) events in a fixed interval using the Poisson distribution, we can use the following formula:\n\n\\[ P(k; \\lambda) = \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\]\n\nWhere:\n- \\( P(k; \\lambda) \\) is the probability of observing \\( k \\) events.\n- \\( \\lambda \\) (lam) is the average rate of occurrences.\n- \\( e \\) is the base of the natural logarithm.\n- \\( k! \\) is the factorial of \\( k \\).\n\nHere's the implementation of the function in Python:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    # Calculate the probability using the Poisson formula\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage:\n# print(poisson_probability(3, 2.5))  # Example call to the function\n```\n\nThis function uses the `math` module to calculate the exponential and factorial values needed for the Poisson formula. The result is then rounded to 5 decimal places as specified."}
{"task_id": 82, "completion_id": 0, "completion": "To calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values, you can use the following Python function:\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray.\")\n    \n    if img.ndim != 2:\n        raise ValueError(\"Input must be a 2D array representing a grayscale image.\")\n    \n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    contrast = max_pixel_value - min_pixel_value\n    \n    return contrast\n\n# Example usage:\n# img = np.array([[0, 50, 100], [150, 200, 255]])\n# print(calculate_contrast(img))  # Output: 255\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input is a `numpy.ndarray` and if it is a 2D array. This ensures that the input is a valid grayscale image.\n\n2. **Calculate Maximum and Minimum Pixel Values**: Using `np.max()` and `np.min()`, the function finds the maximum and minimum pixel values in the image.\n\n3. **Calculate Contrast**: The contrast is calculated as the difference between the maximum and minimum pixel values.\n\n4. **Return Contrast**: The function returns the calculated contrast value.\n\nThis function is efficient and leverages NumPy's capabilities to handle array operations, making it suitable for processing images."}
{"task_id": 83, "completion_id": 0, "completion": "Certainly! To calculate the dot product of two vectors using NumPy, you can utilize the `numpy.dot()` function, which efficiently computes the dot product of two arrays. Here's how you can implement the `calculate_dot_product` function:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Ensure the input vectors are 1D numpy arrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError(\"Both inputs must be numpy arrays.\")\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must have the same length.\")\n    \n    # Calculate the dot product using numpy's dot function\n    dot_product = np.dot(vec1, vec2)\n    \n    return dot_product\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if both inputs are 1D NumPy arrays and have the same length. This ensures that the dot product can be computed correctly.\n- **Dot Product Calculation**: The `np.dot()` function is used to compute the dot product of the two vectors.\n- **Return Value**: The function returns the dot product as a single number.\n\nYou can use this function by passing two 1D NumPy arrays to it, and it will return their dot product. Here's an example of how to use the function:\n\n```python\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])\n\nresult = calculate_dot_product(vec1, vec2)\nprint(\"Dot Product:\", result)  # Output: Dot Product: 32\n```\n\nThis example demonstrates the dot product calculation for two vectors `[1, 2, 3]` and `[4, 5, 6]`, which results in `32`."}
{"task_id": 84, "completion_id": 0, "completion": "To implement the Phi Transformation for polynomial features, we will generate all polynomial combinations of the input features up to the specified degree. This involves creating combinations of the input features raised to various powers, which allows models to capture nonlinear relationships in the data.\n\nHere's the complete implementation of the `phi_transform` function:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    # Initialize the result list\n    transformed_data = []\n\n    # Iterate over each data point\n    for x in data:\n        # Start with the bias term (1.0)\n        features = [1.0]\n\n        # Generate polynomial features for each degree from 1 to the specified degree\n        for d in range(1, degree + 1):\n            # Generate combinations with replacement of the current degree\n            for combo in combinations_with_replacement([x], d):\n                # Calculate the product of the combination\n                product = np.prod(combo)\n                # Append the product to the features list\n                features.append(product)\n\n        # Round each feature to 8 decimal places\n        features = [round(f, 8) for f in features]\n        # Append the features to the transformed data\n        transformed_data.append(features)\n\n    return transformed_data\n\n# Example usage:\ndata = [2.0, 3.0]\ndegree = 2\nprint(phi_transform(data, degree))\n```\n\n### Explanation:\n\n1. **Input Validation**: If the degree is less than 0, the function returns an empty list as specified.\n\n2. **Feature Generation**:\n   - For each data point `x` in the input list, we start with a bias term of `1.0`.\n   - We then generate polynomial features for each degree from 1 to the specified degree.\n   - Using `combinations_with_replacement`, we generate all combinations of the data point `x` raised to the current degree `d`.\n   - We calculate the product of each combination to form the polynomial feature.\n\n3. **Rounding**: Each feature is rounded to 8 decimal places to meet the output constraints.\n\n4. **Output**: The function returns a nested list where each inner list contains the polynomial features of the corresponding data point.\n\nThis implementation allows you to transform a list of numerical data into polynomial features up to a specified degree, which can be useful for fitting nonlinear models."}
{"task_id": 85, "completion_id": 0, "completion": "To implement the Positional Encoding layer for Transformers, we need to calculate the positional encodings using sine and cosine functions as specified in the original Transformer architecture. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the positional encoding matrix\n    pos_enc = np.zeros((position, d_model), dtype=np.float32)\n    \n    # Calculate the positional encodings\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            # Calculate the angle rates\n            angle_rate = pos / np.power(10000, (2 * i) / np.float32(d_model))\n            # Apply sine to even indices\n            pos_enc[pos, i] = np.sin(angle_rate)\n            # Apply cosine to odd indices\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(angle_rate)\n    \n    # Convert the dtype to np.float16\n    pos_enc = pos_enc.astype(np.float16)\n    \n    # Convert the numpy array to a list and return\n    return pos_enc.tolist()\n\n# Example usage:\n# pos_encoding(50, 512)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `position` is 0 or `d_model` is less than or equal to 0, returning -1 in such cases as specified.\n\n2. **Matrix Initialization**: A zero matrix of shape `(position, d_model)` is initialized with `np.float32` to ensure precision during calculations.\n\n3. **Positional Encoding Calculation**:\n   - For each position `pos` and each dimension `i`, the angle rate is calculated using the formula: \n     \\[\n     \\text{angle\\_rate} = \\frac{\\text{pos}}{10000^{\\frac{2i}{d\\_model}}}\n     \\]\n   - Sine is applied to even indices and cosine to odd indices.\n\n4. **Type Conversion**: The matrix is converted to `np.float16` to meet the output constraints.\n\n5. **Return**: The positional encoding matrix is converted to a list using `tolist()` and returned.\n\nThis implementation follows the standard approach used in Transformer models for positional encoding, ensuring that the encodings capture the relative positions in the input sequence effectively."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting: training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n\n    # Check for underfitting: both training and test accuracy are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n\n    # If neither condition is met, the model is a good fit\n    return 0  # Good fit\n```\n\nThis function evaluates the quality of a machine learning model's fit by comparing the training and test accuracies. It returns `1` for overfitting, `-1` for underfitting, and `0` for a good fit based on the specified conditions."}
{"task_id": 87, "completion_id": 0, "completion": "To implement the Adam optimizer update step function, we need to follow the Adam optimization algorithm, which involves updating the moving averages of the gradients and their squares, applying bias correction, and then updating the parameters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to 5 decimal places and convert to list\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n```\n\n### Explanation:\n1. **Moving Averages Update**:\n   - `m` is updated as the exponential moving average of the gradients.\n   - `v` is updated as the exponential moving average of the squared gradients.\n\n2. **Bias Correction**:\n   - `m_hat` and `v_hat` are the bias-corrected versions of `m` and `v`, respectively. This step is crucial to counteract the initialization bias.\n\n3. **Parameter Update**:\n   - The parameter is updated using the corrected moving averages, the learning rate, and a small constant `epsilon` for numerical stability.\n\n4. **Output Formatting**:\n   - The updated parameter, `m`, and `v` are rounded to 5 decimal places and converted to lists before returning, as specified in the task.\n\nThis implementation handles both scalar and array inputs due to the use of NumPy operations, which are element-wise by default."}
{"task_id": 88, "completion_id": 0, "completion": "To implement a simplified GPT-2-like text generation function, we need to follow the architecture components you've outlined. Here's a step-by-step implementation:\n\n1. **Token Embeddings**: Convert input tokens into dense vector representations using a learned embedding matrix.\n2. **Positional Embeddings**: Add positional information to the token embeddings to retain the order of tokens.\n3. **Multi-head Attention**: Implement a simplified version of multi-head self-attention to allow the model to focus on different parts of the input sequence.\n4. **Feed-Forward Network**: Process the attention outputs through a dense layer to transform the data.\n5. **Layer Normalization**: Apply layer normalization to stabilize the training process.\n\nHere's the complete code for the simplified GPT-2-like text generation function:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    norm_x = (x - mean) / np.sqrt(variance + eps)\n    return g * norm_x + b\n\ndef multi_head_attention(x, n_head):\n    # Simplified single-head attention for demonstration\n    q = k = v = x  # In practice, these would be different linear transformations\n    scores = np.dot(q, k.T) / np.sqrt(x.shape[-1])\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    return np.dot(weights, v)\n\ndef feed_forward(x):\n    # Simplified feed-forward network\n    return np.tanh(np.dot(x, np.random.rand(x.shape[-1], x.shape[-1])))\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    encoded_prompt = encoder.encode(prompt)\n    context = np.array(encoded_prompt)\n\n    for _ in range(n_tokens_to_generate):\n        # Token Embeddings\n        token_embeddings = params['wte'][context]\n\n        # Positional Embeddings\n        position_embeddings = params['wpe'][:len(context)]\n\n        # Combine token and positional embeddings\n        x = token_embeddings + position_embeddings\n\n        # Multi-head Attention\n        x = multi_head_attention(x, hparams['n_head'])\n\n        # Feed-Forward Network\n        x = feed_forward(x)\n\n        # Layer Normalization\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n\n        # Simplified token prediction (random choice for demonstration)\n        next_token = np.random.choice(len(params['wte']))\n        context = np.append(context, next_token)\n\n    generated_text = encoder.decode(context)\n    return generated_text\n\n# Example usage\nprompt = \"hello\"\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)\nprint(generated_text)\n```\n\n### Explanation:\n- **Token and Positional Embeddings**: We use random embeddings for simplicity. In practice, these would be learned parameters.\n- **Multi-head Attention**: A simplified version is implemented here, focusing on the core idea of attention.\n- **Feed-Forward Network**: A simple transformation using `tanh` is applied.\n- **Layer Normalization**: Normalizes the output to stabilize the process.\n- **Token Generation**: For demonstration, the next token is chosen randomly. In a real model, this would be based on the output logits.\n\nThis code provides a basic framework to understand the components of GPT-2. In a real-world scenario, these components would be more complex and involve learned parameters."}
{"task_id": 89, "completion_id": 0, "completion": "To implement the Pattern Weaver's Code using a simplified self-attention mechanism, we need to follow these steps:\n\n1. **Softmax Function**: This function will compute the attention scores for each crystal based on its relationship with every other crystal.\n\n2. **Self-Attention Mechanism**: For each crystal, calculate its relationship with every other crystal, compute the attention scores using the softmax function, and derive the final weighted pattern for each crystal.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))  # Subtract max for numerical stability\n        return exp_values / np.sum(exp_values)\n    \n    # Initialize the output list\n    enhanced_patterns = []\n    \n    # Iterate over each crystal to compute its enhanced pattern\n    for i in range(n):\n        # Compute the relationship (dot product) of the i-th crystal with all other crystals\n        relationships = np.dot(crystal_values[i], crystal_values.T)\n        \n        # Compute attention scores using the softmax function\n        attention_scores = softmax(relationships)\n        \n        # Compute the weighted pattern for the i-th crystal\n        weighted_pattern = np.dot(attention_scores, crystal_values)\n        \n        # Append the result rounded to 4 decimal places\n        enhanced_patterns.append(np.round(weighted_pattern, 4))\n    \n    return enhanced_patterns\n\n# Example usage:\nn = 3\ncrystal_values = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndimension = 3\nresult = pattern_weaver(n, crystal_values, dimension)\nprint(result)\n```\n\n### Explanation:\n\n- **Softmax Function**: This function computes the softmax of a given array. It first stabilizes the computation by subtracting the maximum value from the input array to prevent overflow issues. Then, it calculates the exponential of each element and normalizes them by dividing by the sum of all exponentials.\n\n- **Self-Attention Mechanism**: For each crystal, we calculate its relationship with all other crystals using the dot product. The attention scores are then computed using the softmax function. Finally, the weighted pattern is derived by taking the dot product of the attention scores with the crystal values.\n\n- **Output**: The result is a list of enhanced patterns for each crystal, with each value rounded to the fourth decimal place.\n\nThis code provides a simplified version of the self-attention mechanism, which is a core component of many modern neural network architectures, such as the Transformer model."}
{"task_id": 90, "completion_id": 0, "completion": "To implement the BM25 ranking function, we need to calculate the BM25 score for each document in the corpus given a query. BM25 is a probabilistic information retrieval model that considers term frequency, document length, and inverse document frequency. Here's how you can implement it:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Calculate average document length\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    \n    # Calculate document frequencies for each term in the corpus\n    doc_freqs = Counter()\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            doc_freqs[term] += 1\n    \n    # Calculate IDF for each term in the query\n    N = len(corpus)\n    idf = {}\n    for term in query:\n        # Using the BM25 IDF formula\n        n_qi = doc_freqs.get(term, 0)\n        idf[term] = np.log((N - n_qi + 0.5) / (n_qi + 0.5) + 1)\n    \n    # Calculate BM25 score for each document\n    scores = []\n    for doc in corpus:\n        score = 0.0\n        doc_len = len(doc)\n        term_freqs = Counter(doc)\n        \n        for term in query:\n            if term in term_freqs:\n                f_qi = term_freqs[term]\n                numerator = f_qi * (k1 + 1)\n                denominator = f_qi + k1 * (1 - b + b * (doc_len / avg_doc_len))\n                score += idf[term] * (numerator / denominator)\n        \n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage:\ncorpus = [\n    [\"this\", \"is\", \"a\", \"sample\", \"document\"],\n    [\"this\", \"document\", \"is\", \"another\", \"example\"],\n    [\"example\", \"document\", \"with\", \"different\", \"words\"]\n]\nquery = [\"document\", \"example\"]\n\nbm25_scores = calculate_bm25_scores(corpus, query)\nprint(bm25_scores)\n```\n\n### Explanation:\n1. **Average Document Length**: We calculate the average length of documents in the corpus to normalize document length effects.\n\n2. **Document Frequencies**: We count how many documents contain each term to compute the inverse document frequency (IDF).\n\n3. **IDF Calculation**: The IDF is calculated using the BM25 formula, which adjusts the traditional IDF to account for term saturation.\n\n4. **BM25 Score Calculation**: For each document, we compute the BM25 score by iterating over the query terms, calculating the term frequency, and applying the BM25 formula.\n\n5. **Output**: The scores are rounded to three decimal places as required.\n\nThis implementation should provide a robust BM25 ranking for documents in the corpus based on the given query."}
{"task_id": 91, "completion_id": 0, "completion": "To calculate the F1 score, we need to first compute precision and recall. Precision is the ratio of true positive predictions to the total number of positive predictions, while recall is the ratio of true positive predictions to the total number of actual positive instances. The F1 score is the harmonic mean of precision and recall.\n\nHere's the implementation of the `calculate_f1_score` function:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counts for true positives, false positives, and false negatives\n    true_positive = 0\n    false_positive = 0\n    false_negative = 0\n\n    # Calculate true positives, false positives, and false negatives\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positive += 1\n        elif true == 0 and pred == 1:\n            false_positive += 1\n        elif true == 1 and pred == 0:\n            false_negative += 1\n\n    # Calculate precision and recall\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n\n    # Calculate F1 score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n\n    # Return the F1 score rounded to three decimal places\n    return round(f1_score, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n# y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n# print(calculate_f1_score(y_true, y_pred))  # Output: 0.667\n```\n\nThis function iterates over the true and predicted labels, counts the true positives, false positives, and false negatives, and then calculates precision, recall, and the F1 score. The result is rounded to three decimal places before being returned."}
{"task_id": 92, "completion_id": 0, "completion": "To tackle this task, we'll follow the steps outlined in the function `power_grid_forecast`. We'll use Python's `numpy` library for numerical operations and `scipy.stats.linregress` for linear regression. Here's the complete implementation:\n\n```python\nimport math\nimport numpy as np\nfrom scipy.stats import linregress\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended_data = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption_data[i - 1] - fluctuation)\n    \n    # Step 2: Perform linear regression on the detrended data\n    days = np.arange(1, 11)\n    slope, intercept, _, _, _ = linregress(days, detrended_data)\n    \n    # Step 3: Predict day 15's base consumption\n    day_15_base_consumption = slope * 15 + intercept\n    \n    # Step 4: Add the day 15 fluctuation back\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_total_consumption = day_15_base_consumption + day_15_fluctuation\n    \n    # Step 5: Round, then add a 5% safety margin (rounded up)\n    day_15_total_consumption_rounded = math.ceil(day_15_total_consumption)\n    safety_margin = math.ceil(day_15_total_consumption_rounded * 0.05)\n    final_consumption = day_15_total_consumption_rounded + safety_margin\n    \n    # Step 6: Return the final integer\n    return final_consumption\n\n# Example usage:\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]  # Example data\nprint(power_grid_forecast(consumption_data))\n```\n\n### Explanation:\n1. **Detrending the Data**: We subtract the known fluctuation from each day's consumption data to isolate the linear trend.\n2. **Linear Regression**: Using `scipy.stats.linregress`, we fit a linear model to the detrended data to find the slope and intercept.\n3. **Prediction**: We use the linear model to predict the base consumption for day 15.\n4. **Adding Fluctuation**: We calculate the fluctuation for day 15 and add it back to the predicted base consumption.\n5. **Safety Margin**: We round the total consumption and add a 5% safety margin, ensuring the result is an integer.\n6. **Return**: The function returns the final integer value, which includes the safety margin.\n\nThis code will help ensure the colony's energy needs are met with a buffer for unexpected increases in consumption."}
{"task_id": 93, "completion_id": 0, "completion": "To calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values, you can implement the function as follows:\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute differences between the true and predicted values\n    absolute_errors = np.abs(y_true - y_pred)\n    \n    # Calculate the mean of these absolute differences\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    # Return the result rounded to 3 decimal places\n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(mae(y_true, y_pred))  # Output: 0.5\n```\n\n### Explanation:\n1. **Absolute Differences**: The function first calculates the absolute differences between each pair of true and predicted values using `np.abs(y_true - y_pred)`.\n2. **Mean Calculation**: It then computes the mean of these absolute differences using `np.mean(absolute_errors)`.\n3. **Rounding**: Finally, the result is rounded to 3 decimal places using the `round` function before being returned.\n\nThis implementation assumes that `y_true` and `y_pred` are NumPy arrays of the same length. If they are not, you should handle such cases appropriately, possibly by raising an error or returning a specific value."}
{"task_id": 94, "completion_id": 0, "completion": "To implement the multi-head attention mechanism, we need to define the functions `compute_qkv`, `self_attention`, and `multi_head_attention`. Here's how you can implement these functions:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the Q, K, V matrices using the input matrix X and weight matrices W_q, W_k, W_v.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention for a single head.\n    \"\"\"\n    # Calculate the dot product between Q and K^T\n    scores = np.dot(Q, K.T)\n    \n    # Scale the scores by the square root of the dimension of K\n    d_k = K.shape[1]\n    scores /= np.sqrt(d_k)\n    \n    # Apply softmax to get the attention weights\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    \n    # Compute the output as the weighted sum of V\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute the multi-head attention by splitting the Q, K, V matrices into multiple heads.\n    \"\"\"\n    # Get the dimension of the input\n    m, d_model = Q.shape\n    \n    # Ensure the dimension is divisible by the number of heads\n    assert d_model % n_heads == 0, \"The dimension must be divisible by the number of heads.\"\n    \n    # Calculate the dimension of each head\n    d_head = d_model // n_heads\n    \n    # Split Q, K, V into multiple heads\n    Q_heads = np.split(Q, n_heads, axis=-1)\n    K_heads = np.split(K, n_heads, axis=-1)\n    V_heads = np.split(V, n_heads, axis=-1)\n    \n    # Compute the attention for each head and concatenate the results\n    attention_outputs = [self_attention(Q_h, K_h, V_h) for Q_h, K_h, V_h in zip(Q_heads, K_heads, V_heads)]\n    \n    # Concatenate the outputs from each head\n    concatenated_output = np.concatenate(attention_outputs, axis=-1)\n    \n    return concatenated_output\n\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n```\n\n### Explanation:\n1. **`compute_qkv` Function**: This function computes the query (Q), key (K), and value (V) matrices by multiplying the input matrix `X` with the respective weight matrices `W_q`, `W_k`, and `W_v`.\n\n2. **`self_attention` Function**: This function calculates the self-attention for a single head. It computes the dot product of Q and the transpose of K, scales the scores, applies the softmax function to get attention weights, and finally computes the output as the weighted sum of V.\n\n3. **`multi_head_attention` Function**: This function splits the Q, K, and V matrices into multiple heads, computes the attention for each head using the `self_attention` function, and concatenates the results to form the final output.\n\nThe `entrypoint` function initializes the input data and weight matrices, computes Q, K, V, and returns the result of the multi-head attention rounded to four decimal places."}
{"task_id": 95, "completion_id": 0, "completion": "To calculate the Phi coefficient, we need to understand that it is a measure of association for two binary variables. It is similar to the Pearson correlation coefficient but specifically for binary data. The formula for the Phi coefficient is:\n\n\\[\n\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{1\\cdot}n_{0\\cdot}n_{\\cdot1}n_{\\cdot0})}}\n\\]\n\nWhere:\n- \\( n_{11} \\) is the count of cases where both variables are 1.\n- \\( n_{00} \\) is the count of cases where both variables are 0.\n- \\( n_{10} \\) is the count of cases where the first variable is 1 and the second is 0.\n- \\( n_{01} \\) is the count of cases where the first variable is 0 and the second is 1.\n- \\( n_{1\\cdot} \\) is the total number of cases where the first variable is 1.\n- \\( n_{0\\cdot} \\) is the total number of cases where the first variable is 0.\n- \\( n_{\\cdot1} \\) is the total number of cases where the second variable is 1.\n- \\( n_{\\cdot0} \\) is the total number of cases where the second variable is 0.\n\nHere's how you can implement the function in Python:\n\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"The lists x and y must have the same length.\")\n    \n    # Calculate the contingency table values\n    n11 = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 1)\n    n00 = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 0)\n    n10 = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 0)\n    n01 = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 1)\n    \n    # Calculate the marginals\n    n1_ = n11 + n10\n    n0_ = n00 + n01\n    n_1 = n11 + n01\n    n_0 = n00 + n10\n    \n    # Calculate the denominator\n    denominator = (n1_ * n0_ * n_1 * n_0) ** 0.5\n    \n    if denominator == 0:\n        raise ValueError(\"Denominator is zero, cannot compute Phi coefficient.\")\n    \n    # Calculate the Phi coefficient\n    phi = (n11 * n00 - n10 * n01) / denominator\n    \n    return round(phi, 4)\n```\n\nThis function first checks if the input lists are of the same length. It then calculates the necessary counts for the contingency table and the marginals. Finally, it computes the Phi coefficient using the formula and returns it rounded to four decimal places. If the denominator is zero, it raises an error to avoid division by zero."}
{"task_id": 96, "completion_id": 0, "completion": "To implement the Hard Sigmoid activation function, we need to follow its definition, which is a piecewise linear approximation of the standard sigmoid function. The Hard Sigmoid function can be defined as:\n\n\\[ \\text{hard\\_sigmoid}(x) = \\max(0, \\min(1, 0.2x + 0.5)) \\]\n\nThis function linearly scales the input \\( x \\) and then clamps the result between 0 and 1. Here's how you can implement it in Python:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))\n\n# Example usage:\n# print(hard_sigmoid(-5))  # Output: 0.0\n# print(hard_sigmoid(0))   # Output: 0.5\n# print(hard_sigmoid(5))   # Output: 1.0\n```\n\nThis function first scales the input \\( x \\) by multiplying it by 0.2 and then adding 0.5. The result is then clamped between 0 and 1 using the `max` and `min` functions. This ensures that the output is always within the range [0, 1], which is characteristic of sigmoid-like functions."}
{"task_id": 97, "completion_id": 0, "completion": "To implement the ELU (Exponential Linear Unit) activation function, we need to handle both positive and negative input values. The ELU function is defined as follows:\n\n- For \\( x > 0 \\), the function returns \\( x \\).\n- For \\( x \\leq 0 \\), the function returns \\( \\alpha \\times (\\exp(x) - 1) \\).\n\nHere's the complete implementation of the ELU function, including rounding the result to the fourth decimal place:\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(elu(1.0))  # Output: 1.0\n# print(elu(-1.0)) # Output: -0.6321\n```\n\nThis function checks the input value \\( x \\) and applies the appropriate formula based on whether \\( x \\) is positive or non-positive. The result is then rounded to four decimal places before being returned."}
{"task_id": 98, "completion_id": 0, "completion": "To implement the PReLU activation function, we need to handle both positive and negative input values. For positive values, the function behaves like the standard ReLU, returning the input value itself. For negative values, it returns the input value multiplied by a learnable parameter, `alpha`. Here's how you can implement this:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x\n```\n\n### Explanation:\n- **Positive Input (`x > 0`)**: The function returns `x`, similar to the standard ReLU.\n- **Negative Input (`x <= 0`)**: The function returns `alpha * x`, allowing for a small, learnable slope for negative values.\n\nThis implementation is efficient and straightforward, providing the flexibility to adjust the `alpha` parameter as needed for different applications or during model training."}
{"task_id": 99, "completion_id": 0, "completion": "To implement the Softplus activation function, we need to ensure that the computation is numerically stable, especially for large or small values of `x`. The formula for the Softplus function is `log(1 + e^x)`. However, directly computing `e^x` can lead to overflow for large values of `x`. To handle this, we can use a numerically stable approach by leveraging the properties of logarithms and exponentials.\n\nHere's the implementation:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    # Use a numerically stable approach to compute softplus\n    if x > 100:\n        # For large x, e^x is very large, so log(1 + e^x) \u2248 x\n        result = x\n    elif x < -100:\n        # For small x, e^x is very small, so log(1 + e^x) \u2248 log(1) = 0\n        result = 0\n    else:\n        # Use the standard formula for moderate values of x\n        result = math.log1p(math.exp(x))\n    \n    # Round the result to 4 decimal places\n    return round(result, 4)\n\n# Example usage:\n# print(softplus(1.0))  # Expected output: 1.3133\n# print(softplus(-1.0)) # Expected output: 0.3133\n# print(softplus(1000)) # Expected output: 1000.0\n# print(softplus(-1000))# Expected output: 0.0\n```\n\n### Explanation:\n- **Numerical Stability**: For large values of `x`, `e^x` becomes very large, and `log(1 + e^x)` is approximately `x`. For very small values of `x`, `e^x` is close to zero, and `log(1 + e^x)` is approximately `log(1) = 0`.\n- **math.log1p**: This function computes `log(1 + e^x)` in a numerically stable way, especially for small values of `x`.\n- **Rounding**: The result is rounded to 4 decimal places to meet the output constraints."}
{"task_id": 100, "completion_id": 0, "completion": "To implement the Softsign activation function, we need to compute the value using the formula:\n\n\\[ \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\]\n\nThis function smoothly maps input values to a range between -1 and 1. Below is the implementation of the Softsign function in Python, ensuring the output is rounded to the fourth decimal place:\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)\n\n# Example usage:\n# print(softsign(0.5))  # Output: 0.3333\n# print(softsign(-0.5)) # Output: -0.3333\n```\n\nThis function calculates the Softsign value and rounds it to four decimal places before returning it. You can test the function with different input values to see how it behaves."}
{"task_id": 101, "completion_id": 0, "completion": "To implement the GRPO objective function, we need to consider the following components:\n\n1. **Likelihood Ratios (`rhos`)**: These are the ratios of the new policy probabilities to the old policy probabilities.\n2. **Advantage Estimates (`A`)**: These are the advantage values for each action taken.\n3. **Clipping Mechanism**: This is used to limit the change in policy updates to ensure stability.\n4. **KL Divergence Penalty**: This penalizes the divergence between the new policy and a reference policy to maintain stability.\n\nThe GRPO objective function can be computed by combining these elements. Here's how you can implement it:\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute the clipped objective\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    objective = np.minimum(rhos * A, clipped_rhos * A)\n    \n    # Compute the KL divergence penalty\n    kl_divergence = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    \n    # Compute the final GRPO objective\n    grpo_value = np.mean(objective) - beta * kl_divergence\n    \n    # Round the result to the nearest 6th decimal\n    return round(grpo_value, 6)\n\n# Example usage:\n# rhos = [1.1, 0.9, 1.05]\n# A = [0.5, 0.2, 0.3]\n# pi_theta_old = [0.2, 0.3, 0.5]\n# pi_theta_ref = [0.25, 0.25, 0.5]\n# print(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\n### Explanation:\n- **Likelihood Ratios (`rhos`)**: These are used to scale the advantage estimates.\n- **Clipping**: The `np.clip` function ensures that the likelihood ratios do not deviate too much from 1, which helps in stabilizing the training.\n- **KL Divergence**: This is calculated between the old policy and the reference policy to penalize large deviations.\n- **Objective Calculation**: The objective is the mean of the minimum between the scaled advantages and the clipped advantages, minus the KL divergence penalty.\n- **Rounding**: The final result is rounded to six decimal places as specified."}
{"task_id": 102, "completion_id": 0, "completion": "To implement the Swish activation function, we need to compute the value using the formula:\n\n\\[ \\text{Swish}(x) = x \\cdot \\text{sigmoid}(x) \\]\n\nwhere the sigmoid function is defined as:\n\n\\[ \\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\]\n\nHere's the complete implementation of the Swish activation function in Python, with results rounded to the nearest 4th decimal:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\n\n# Example usage:\n# print(swish(1.0))  # Output: 0.7311\n# print(swish(-1.0)) # Output: -0.2689\n```\n\nThis function calculates the Swish activation value for a given input `x` and rounds the result to the nearest 4th decimal. You can test the function with different input values to see how it behaves."}
{"task_id": 103, "completion_id": 0, "completion": "To implement the SELU activation function, we need to apply the formula for SELU, which is defined as:\n\n\\[ \\text{SELU}(x) = \\text{scale} \\times \\left( \\begin{cases} \nx & \\text{if } x > 0 \\\\\n\\alpha \\times (\\exp(x) - 1) & \\text{if } x \\leq 0 \n\\end{cases} \\right) \\]\n\nGiven the constants `alpha` and `scale`, we can implement this function in Python. Here's the complete code:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    \n    return round(selu_value, 4)\n\n# Example usage:\n# print(selu(1.0))  # Output: 1.0507\n# print(selu(-1.0)) # Output: -1.1113\n```\n\n### Explanation:\n- **Positive Input**: For \\( x > 0 \\), the SELU function simply scales the input by the `scale` factor.\n- **Negative Input**: For \\( x \\leq 0 \\), the SELU function applies the ELU transformation scaled by `alpha` and then multiplies by `scale`.\n- **Rounding**: The result is rounded to the nearest 4th decimal using Python's `round()` function to meet the output constraints."}
{"task_id": 104, "completion_id": 0, "completion": "To implement the prediction function for binary classification using Logistic Regression, we need to compute the class probabilities using the sigmoid function and then convert these probabilities into binary predictions based on a threshold of 0.5. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the sigmoid function for the input z.\n\n    Args:\n        z: Input array\n\n    Returns:\n        Sigmoid of z\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    z = np.dot(X, weights) + bias\n    \n    # Compute probabilities using the sigmoid function\n    probabilities = sigmoid(z)\n    \n    # Convert probabilities to binary predictions using a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return predictions as a list\n    return predictions.tolist()\n\n# Example usage:\n# X = np.array([[0.2, 0.4], [0.5, 0.6]])\n# weights = np.array([0.1, 0.2])\n# bias = -0.1\n# print(predict_logistic(X, weights, bias))\n```\n\n### Explanation:\n1. **Sigmoid Function**: The `sigmoid` function computes the sigmoid of the input `z`, which is used to convert the linear combination of features into probabilities.\n\n2. **Linear Combination**: The linear combination `z` is calculated using the dot product of the input feature matrix `X` and the model weights `weights`, plus the bias term.\n\n3. **Probability Calculation**: The probabilities are computed using the sigmoid function applied to `z`.\n\n4. **Binary Predictions**: The probabilities are converted to binary predictions by checking if they are greater than or equal to 0.5. If so, the prediction is 1; otherwise, it is 0.\n\n5. **Output**: The predictions are returned as a Python list using the `tolist()` method."}
{"task_id": 105, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for Softmax regression, we need to follow these steps:\n\n1. Initialize the model parameters.\n2. Compute the predictions using the Softmax function.\n3. Calculate the Cross Entropy loss.\n4. Compute the gradient of the loss with respect to the parameters.\n5. Update the parameters using the gradient descent rule.\n6. Repeat the process for a specified number of iterations.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef softmax(Z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the softmax of each row of the input array.\"\"\"\n    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the cross-entropy loss.\"\"\"\n    n_samples = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(n_samples), y_true])\n    return np.sum(log_likelihood) / n_samples\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = np.max(y) + 1  # Assuming y contains class labels starting from 0\n\n    # Initialize parameters\n    B = np.zeros((n_features, n_classes))\n    losses = []\n\n    for _ in range(iterations):\n        # Compute predictions\n        Z = np.dot(X, B)\n        y_pred = softmax(Z)\n\n        # Compute loss\n        loss = cross_entropy_loss(y, y_pred)\n        losses.append(round(loss, 4))\n\n        # Compute gradient\n        y_one_hot = np.eye(n_classes)[y]\n        gradient = np.dot(X.T, (y_pred - y_one_hot)) / n_samples\n\n        # Update parameters\n        B -= learning_rate * gradient\n\n    # Round the parameters to 4 decimal places and convert to list\n    B_rounded = np.round(B, 4).tolist()\n\n    return B_rounded, losses\n\n# Example usage:\n# X = np.array([[...], [...], ...])  # Feature matrix\n# y = np.array([...])  # Labels\n# learning_rate = 0.01\n# iterations = 1000\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n```\n\n### Explanation:\n\n- **Softmax Function**: Computes the probabilities for each class. It normalizes the input values to a probability distribution.\n- **Cross Entropy Loss**: Measures the difference between the true labels and the predicted probabilities.\n- **Gradient Descent**: Updates the parameters by moving in the direction of the negative gradient of the loss function.\n- **One-hot Encoding**: Converts class labels into a one-hot encoded format for easier computation of gradients.\n\nThis implementation assumes that the labels `y` are integers starting from 0 up to `n_classes - 1`. Adjust the code if your labels are in a different format."}
{"task_id": 106, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for logistic regression, we need to follow these steps:\n\n1. Initialize the model parameters (weights and bias).\n2. Implement the sigmoid function to map predictions to probabilities.\n3. Compute the Binary Cross Entropy loss.\n4. Update the model parameters using gradient descent.\n5. Collect and return the loss values over iterations.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross Entropy loss.\"\"\"\n    # Clip predictions to avoid log(0)\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Initialize parameters\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Collect loss values\n    loss_values = []\n    \n    for _ in range(iterations):\n        # Compute linear model\n        linear_model = np.dot(X, weights) + bias\n        \n        # Apply sigmoid function\n        y_pred = sigmoid(linear_model)\n        \n        # Compute loss\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n        \n        # Compute gradients\n        dw = np.dot(X.T, (y_pred - y)) / n_samples\n        db = np.mean(y_pred - y)\n        \n        # Update parameters\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    \n    # Round weights and bias to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    \n    # Return the optimized coefficients and loss values\n    return weights.tolist(), bias, loss_values\n\n# Example usage:\n# X = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]])\n# y = np.array([0, 1, 0])\n# learning_rate = 0.01\n# iterations = 1000\n# weights, bias, loss_values = train_logreg(X, y, learning_rate, iterations)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n# print(\"Loss values:\", loss_values)\n```\n\nThis code initializes the weights and bias, computes predictions using the sigmoid function, calculates the Binary Cross Entropy loss, updates the parameters using gradient descent, and collects the loss values over the specified number of iterations. The results are rounded to the nearest 4th decimal place as required."}
{"task_id": 107, "completion_id": 0, "completion": "To implement masked self-attention, we need to follow these steps:\n\n1. Compute the attention scores by taking the dot product of the query (Q) and key (K) matrices.\n2. Apply the mask to the attention scores to prevent attending to certain positions (e.g., future positions in sequence generation).\n3. Apply the softmax function to the masked attention scores to obtain the attention weights.\n4. Use the attention weights to compute the weighted sum of the value (V) matrix.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Step 1: Compute attention scores\n    d_k = K.shape[-1]  # Dimension of the key vectors\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Step 2: Apply the mask\n    scores = np.where(mask, scores, -np.inf)\n    \n    # Step 3: Apply softmax to get attention weights\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Step 4: Compute the weighted sum of the value vectors\n    output = np.dot(attention_weights, V)\n    \n    # Return the result as a Python list\n    return output.tolist()\n\n# Example usage:\n# X: Input data matrix\n# W_q, W_k, W_v: Weight matrices for Q, K, V\n# mask: Attention mask matrix\n# X, W_q, W_k, W_v, and mask should be defined with appropriate dimensions before calling these functions.\n```\n\n### Explanation:\n- **compute_qkv**: This function computes the query, key, and value matrices by multiplying the input matrix `X` with the respective weight matrices `W_q`, `W_k`, and `W_v`.\n- **masked_attention**: This function computes the masked self-attention:\n  - It calculates the attention scores by taking the dot product of `Q` and the transpose of `K`, scaled by the square root of the dimension of the key vectors (`d_k`).\n  - The mask is applied to the scores, setting masked positions to negative infinity to ensure they have no influence after the softmax.\n  - The softmax function is applied to the masked scores to obtain normalized attention weights.\n  - The attention weights are used to compute a weighted sum of the value vectors, resulting in the final output.\n- The result is converted to a Python list using the `tolist()` method before returning."}
{"task_id": 108, "completion_id": 0, "completion": "To measure the disorder in a basket of apples based on their colors, we can use the concept of entropy from information theory. Entropy is a measure of uncertainty or disorder in a set of data. In this context, it will help us quantify the variety of apple colors.\n\nHere's how you can implement the function using entropy:\n\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    \n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # Round the result to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Example usage:\nprint(disorder([0, 0, 0, 0]))          # Output: 0.0\nprint(disorder([1, 1, 0, 0]))          # Output: 1.0\nprint(disorder([0, 1, 2, 3]))          # Output: 2.0\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Output: 2.0\nprint(disorder([0, 0, 0, 0, 0, 1, 2, 3]))  # Output: 1.75\n```\n\n### Explanation:\n- **Entropy Calculation**: The entropy is calculated using the formula \\( H(X) = -\\sum p(x) \\log_2 p(x) \\), where \\( p(x) \\) is the probability of each color.\n- **Counter**: We use `collections.Counter` to count the occurrences of each color.\n- **Probability**: The probability of each color is calculated as the count of that color divided by the total number of apples.\n- **Entropy**: The entropy is summed up for each color's probability, providing a measure of disorder.\n- **Rounding**: The result is rounded to the nearest 4th decimal as required.\n\nThis approach ensures that the disorder measure increases with the variety of colors, satisfying the conditions specified in the task."}
{"task_id": 109, "completion_id": 0, "completion": "To implement Layer Normalization for sequence data, we need to normalize the input tensor across the feature dimension for each sequence. This involves computing the mean and variance for each sequence's features, normalizing the features, and then applying scaling (`gamma`) and shifting (`beta`) parameters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Ensure X, gamma, and beta are numpy arrays\n    X = np.array(X)\n    gamma = np.array(gamma)\n    beta = np.array(beta)\n    \n    # Get the shape of the input tensor\n    batch_size, seq_length, feature_dim = X.shape\n    \n    # Initialize the output tensor\n    normalized_X = np.zeros_like(X)\n    \n    # Perform layer normalization for each sequence in the batch\n    for i in range(batch_size):\n        for j in range(seq_length):\n            # Extract the features for the current sequence\n            features = X[i, j, :]\n            \n            # Compute mean and variance for the current sequence's features\n            mean = np.mean(features)\n            variance = np.var(features)\n            \n            # Normalize the features\n            normalized_features = (features - mean) / np.sqrt(variance + epsilon)\n            \n            # Apply scaling and shifting\n            normalized_X[i, j, :] = gamma * normalized_features + beta\n    \n    # Round the normalized values to 5 decimal places\n    normalized_X = np.round(normalized_X, 5)\n    \n    # Convert the result to a list and return\n    return normalized_X.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 3, 4)  # Example input tensor with shape (batch_size=2, seq_length=3, feature_dim=4)\n# gamma = np.ones(4)  # Scaling parameter with shape (feature_dim=4)\n# beta = np.zeros(4)  # Shifting parameter with shape (feature_dim=4)\n# normalized_X = layer_normalization(X, gamma, beta)\n# print(normalized_X)\n```\n\n### Explanation:\n1. **Input Tensor**: The input tensor `X` is a 3D array with dimensions `[batch_size, seq_length, feature_dim]`.\n2. **Normalization**: For each sequence in the batch, compute the mean and variance of the features, normalize the features using these statistics, and apply the scaling (`gamma`) and shifting (`beta`) parameters.\n3. **Epsilon**: A small constant `epsilon` is added to the variance to prevent division by zero.\n4. **Output**: The normalized tensor is rounded to 5 decimal places and converted to a list before returning.\n\nThis implementation ensures that each sequence's features are normalized independently, which is the essence of layer normalization."}
{"task_id": 110, "completion_id": 0, "completion": "To compute the METEOR score, we need to follow the steps outlined in the METEOR metric, which includes calculating unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. Below is the implementation of the `meteor_score` function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the reference and candidate sentences\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Calculate unigram matches\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    \n    matches = sum(min(ref_counter[word], cand_counter[word]) for word in cand_counter)\n    \n    # Calculate precision and recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    \n    # Calculate F-mean\n    if precision + recall > 0:\n        f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    else:\n        f_mean = 0\n    \n    # Calculate fragmentation penalty\n    # Fragmentation is calculated based on the number of chunks\n    chunks = 0\n    last_match_index = -1\n    for i, word in enumerate(cand_tokens):\n        if word in ref_counter and ref_counter[word] > 0:\n            if last_match_index == -1 or i != last_match_index + 1:\n                chunks += 1\n            last_match_index = i\n            ref_counter[word] -= 1\n    \n    frag_penalty = gamma * (chunks / matches) if matches > 0 else 1\n    \n    # Calculate METEOR score\n    meteor = f_mean * (1 - frag_penalty)\n    \n    return round(meteor, 3)\n\n# Example usage\nreference = \"the cat is on the mat\"\ncandidate = \"the cat is on mat\"\nscore = meteor_score(reference, candidate)\nprint(\"METEOR Score:\", score)\n```\n\n### Explanation:\n1. **Tokenization**: The reference and candidate sentences are split into tokens (words).\n2. **Unigram Matches**: We count the number of unigram matches between the reference and candidate using `Counter`.\n3. **Precision and Recall**: Precision is the ratio of matched unigrams to the total unigrams in the candidate. Recall is the ratio of matched unigrams to the total unigrams in the reference.\n4. **F-mean**: This is a harmonic mean of precision and recall, weighted by `alpha`.\n5. **Fragmentation Penalty**: This penalty is based on the number of chunks (disjoint sequences of matched words) in the candidate translation.\n6. **METEOR Score**: The final score is calculated by combining the F-mean and the fragmentation penalty.\n\nThe function returns the METEOR score rounded to three decimal places."}
{"task_id": 111, "completion_id": 0, "completion": "To compute the Pointwise Mutual Information (PMI), we need to use the formula:\n\n\\[ \\text{PMI}(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x) \\cdot P(y)} \\right) \\]\n\nWhere:\n- \\( P(x, y) \\) is the probability of the joint occurrence of events \\( x \\) and \\( y \\).\n- \\( P(x) \\) is the probability of occurrence of event \\( x \\).\n- \\( P(y) \\) is the probability of occurrence of event \\( y \\).\n\nGiven the joint occurrence count, individual counts, and total number of samples, we can compute these probabilities as follows:\n- \\( P(x, y) = \\frac{\\text{joint\\_counts}}{\\text{total\\_samples}} \\)\n- \\( P(x) = \\frac{\\text{total\\_counts\\_x}}{\\text{total\\_samples}} \\)\n- \\( P(y) = \\frac{\\text{total\\_counts\\_y}}{\\text{total\\_samples}} \\)\n\nLet's implement the function:\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Calculate probabilities\n    p_x_y = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Compute PMI\n    if p_x_y == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')  # PMI is undefined if any probability is zero\n    \n    pmi = np.log2(p_x_y / (p_x * p_y))\n    \n    # Return the PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\n# joint_counts = 10\n# total_counts_x = 50\n# total_counts_y = 30\n# total_samples = 100\n# print(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\n### Explanation:\n- We first calculate the probabilities \\( P(x, y) \\), \\( P(x) \\), and \\( P(y) \\) using the provided counts and total samples.\n- We check if any of these probabilities are zero, which would make PMI undefined. In such cases, we return negative infinity (`float('-inf')`) to indicate that PMI cannot be computed.\n- We compute the PMI using the formula and return the result rounded to 3 decimal places."}
{"task_id": 112, "completion_id": 0, "completion": "To implement Min-Max Normalization, we need to scale each value in the list to the range [0, 1] using the formula:\n\n\\[ \\text{normalized\\_value} = \\frac{\\text{value} - \\text{min\\_value}}{\\text{max\\_value} - \\text{min\\_value}} \\]\n\nHere's the Python function that performs Min-Max Normalization on a list of integers:\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_value = min(x)\n    max_value = max(x)\n    \n    # Handle the case where all values are the same\n    if min_value == max_value:\n        return [0.0] * len(x)\n    \n    normalized_values = [\n        round((value - min_value) / (max_value - min_value), 4) for value in x\n    ]\n    \n    return normalized_values\n\n# Example usage:\n# x = [10, 20, 30, 40, 50]\n# print(min_max(x))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```\n\n### Explanation:\n1. **Check for Empty List**: If the input list is empty, return an empty list.\n2. **Calculate Min and Max**: Determine the minimum and maximum values in the list.\n3. **Handle Uniform List**: If all values are the same (i.e., `min_value == max_value`), return a list of zeros since normalization is undefined in this case.\n4. **Normalize Values**: For each value in the list, apply the normalization formula and round the result to 4 decimal places.\n5. **Return Result**: Return the list of normalized values."}
{"task_id": 113, "completion_id": 0, "completion": "To implement a simple residual block with a shortcut connection using NumPy, we need to follow these steps:\n\n1. **Matrix Multiplication**: Multiply the input array with the first weight matrix.\n2. **ReLU Activation**: Apply the ReLU activation function to the result of the first matrix multiplication.\n3. **Second Matrix Multiplication**: Multiply the result of the ReLU activation with the second weight matrix.\n4. **Second ReLU Activation**: Apply the ReLU activation function again.\n5. **Shortcut Connection**: Add the original input to the result of the second ReLU activation.\n6. **Final ReLU Activation**: Apply the ReLU activation function to the result of the shortcut connection.\n7. **Output**: Round the final result to 4 decimal places and convert it to a list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First layer: matrix multiplication and ReLU activation\n    z1 = np.dot(x, w1)\n    a1 = relu(z1)\n    \n    # Second layer: matrix multiplication and ReLU activation\n    z2 = np.dot(a1, w2)\n    a2 = relu(z2)\n    \n    # Add the shortcut connection (original input)\n    shortcut = x + a2\n    \n    # Final ReLU activation\n    output = relu(shortcut)\n    \n    # Round the output to 4 decimal places and convert to list\n    return np.round(output, 4).tolist()\n\n# Example usage:\n# x = np.array([1.0, 2.0, 3.0])\n# w1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n# w2 = np.array([[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]])\n# result = residual_block(x, w1, w2)\n# print(result)\n```\n\n### Explanation:\n- **ReLU Function**: The ReLU (Rectified Linear Unit) function is defined as `relu(x) = max(0, x)`. It sets all negative values to zero.\n- **Matrix Multiplication**: We use `np.dot()` for matrix multiplication.\n- **Shortcut Connection**: The original input `x` is added to the output of the second layer before applying the final ReLU activation.\n- **Output Formatting**: The final output is rounded to 4 decimal places using `np.round()` and converted to a list using `tolist()`.\n\nThis implementation assumes that the dimensions of `x`, `w1`, and `w2` are compatible for matrix multiplication. Make sure to adjust the dimensions of the weight matrices according to your specific use case."}
{"task_id": 114, "completion_id": 0, "completion": "To implement Global Average Pooling for a 3D NumPy array, you need to compute the average of each feature map across its spatial dimensions (height and width). Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    # Ensure the input is a 3D array\n    if x.ndim != 3:\n        raise ValueError(\"Input must be a 3D array with shape (height, width, channels)\")\n    \n    # Compute the average across the height and width for each channel\n    # The result will be a 1D array with the average of each channel\n    return np.mean(x, axis=(0, 1))\n\n# Example usage:\n# Suppose we have a feature map of shape (height=4, width=4, channels=3)\nfeature_maps = np.random.rand(4, 4, 3)\npooled_output = global_avg_pool(feature_maps)\nprint(pooled_output)  # Output will be a 1D array of shape (3,)\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the input is a 3D array. If not, it raises a `ValueError`.\n- **Global Average Pooling**: The `np.mean` function is used with `axis=(0, 1)` to compute the mean across the height and width dimensions for each channel. This effectively collapses the spatial dimensions, resulting in a 1D array where each element is the average of the corresponding feature map.\n- **Output**: The function returns a 1D array with the average values for each channel."}
{"task_id": 115, "completion_id": 0, "completion": "To implement batch normalization for a 4D NumPy array in the BCHW format, we need to normalize the input across the batch and spatial dimensions for each channel. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Ensure the input is a 4D array\n    assert X.ndim == 4, \"Input must be a 4D array in BCHW format\"\n    \n    # Get the dimensions\n    B, C, H, W = X.shape\n    \n    # Reshape X to combine batch and spatial dimensions\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(C, -1)\n    \n    # Calculate mean and variance for each channel\n    mean = np.mean(X_reshaped, axis=1, keepdims=True)\n    variance = np.var(X_reshaped, axis=1, keepdims=True)\n    \n    # Normalize\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    \n    # Reshape back to original dimensions\n    X_normalized = X_normalized.reshape(C, B, H, W).transpose(1, 0, 2, 3)\n    \n    # Apply scale (gamma) and shift (beta)\n    X_scaled_shifted = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(X_scaled_shifted, 4).tolist()\n\n# Example usage:\n# X = np.random.rand(2, 3, 4, 4)  # Example input\n# gamma = np.ones((3,))\n# beta = np.zeros((3,))\n# result = batch_normalization(X, gamma, beta)\n# print(result)\n```\n\n### Explanation:\n1. **Reshape and Transpose**: We first transpose the input to bring the channel dimension to the front and then reshape it to combine the batch and spatial dimensions. This allows us to compute the mean and variance across these dimensions for each channel.\n\n2. **Mean and Variance Calculation**: We compute the mean and variance for each channel using the reshaped array.\n\n3. **Normalization**: We normalize the data using the computed mean and variance, ensuring numerical stability with the epsilon value.\n\n4. **Reshape Back**: After normalization, we reshape the data back to its original dimensions.\n\n5. **Scale and Shift**: We apply the scale (`gamma`) and shift (`beta`) parameters to the normalized data.\n\n6. **Output Formatting**: Finally, we round the output to 4 decimal places and convert it to a list using `tolist()`.\n\nThis implementation ensures that the batch normalization is correctly applied to each channel across the batch and spatial dimensions."}
{"task_id": 116, "completion_id": 0, "completion": "To compute the derivative of a polynomial term of the form \\( c \\cdot x^n \\), we can use the power rule from calculus. The power rule states that the derivative of \\( x^n \\) is \\( n \\cdot x^{n-1} \\). Therefore, the derivative of \\( c \\cdot x^n \\) is \\( c \\cdot n \\cdot x^{n-1} \\).\n\nHere's how you can implement this in Python:\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    # Calculate the derivative using the power rule\n    derivative_value = c * n * x**(n - 1)\n    # Return the result rounded to 4 decimal places\n    return round(derivative_value, 4)\n\n# Example usage:\n# For the term 3 * x^2, the derivative at x = 2 is 3 * 2 * 2^(2-1) = 12\nprint(poly_term_derivative(3, 2, 2))  # Output: 12.0\n```\n\n### Explanation:\n- **Power Rule**: The derivative of \\( c \\cdot x^n \\) is calculated as \\( c \\cdot n \\cdot x^{n-1} \\).\n- **Rounding**: The result is rounded to 4 decimal places using Python's `round()` function to meet the output constraints.\n- **Example**: The example provided shows how to use the function to compute the derivative of \\( 3 \\cdot x^2 \\) at \\( x = 2 \\), which results in 12.0."}
{"task_id": 117, "completion_id": 0, "completion": "To compute an orthonormal basis for a set of 2D vectors using the Gram-Schmidt process, we need to ensure that the vectors are orthogonal and normalized. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    def normalize(v):\n        norm = np.linalg.norm(v)\n        if norm < tol:\n            return None\n        return v / norm\n\n    def project(u, v):\n        return (np.dot(v, u) / np.dot(u, u)) * u\n\n    orthonormal_vectors = []\n    \n    for v in vectors:\n        v = np.array(v)\n        for u in orthonormal_vectors:\n            v = v - project(u, v)\n        \n        normalized_v = normalize(v)\n        if normalized_v is not None:\n            orthonormal_vectors.append(normalized_v)\n    \n    # Round the vectors to 4 decimal places and convert to list\n    orthonormal_vectors = [np.round(vec, 4).tolist() for vec in orthonormal_vectors]\n    \n    return orthonormal_vectors\n\n# Example usage:\nvectors = [[1, 1], [1, 0], [0, 1]]\northonormal_basis_result = orthonormal_basis(vectors)\nprint(orthonormal_basis_result)\n```\n\n### Explanation:\n1. **Normalization**: The `normalize` function checks if the norm of a vector is above the tolerance level and normalizes it if so. If the norm is below the tolerance, it returns `None`, indicating that the vector is effectively zero and should not be included in the basis.\n\n2. **Projection**: The `project` function computes the projection of vector `v` onto vector `u`.\n\n3. **Gram-Schmidt Process**: For each vector `v` in the input list, we subtract its projection onto each of the already computed orthonormal vectors. This ensures orthogonality.\n\n4. **Adding to Basis**: After orthogonalizing `v`, we normalize it and add it to the list of orthonormal vectors if it is not effectively zero.\n\n5. **Output Formatting**: The resulting orthonormal vectors are rounded to 4 decimal places and converted to lists before being returned.\n\nThis implementation will give you a set of orthonormal vectors that span the same subspace as the input vectors, suitable for applications like feature orthogonalization in machine learning."}
{"task_id": 118, "completion_id": 0, "completion": "To compute the cross product of two 3-dimensional vectors, we can utilize NumPy's built-in functionality for vector operations. Here's how you can implement the function:\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Ensure the input vectors are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product using numpy's cross function\n    cross_prod = np.cross(a, b)\n    \n    # Round the result to 4 decimal places\n    cross_prod_rounded = np.round(cross_prod, 4)\n    \n    # Convert the result to a list and return\n    return cross_prod_rounded.tolist()\n\n# Example usage:\n# vector_a = [1, 2, 3]\n# vector_b = [4, 5, 6]\n# result = cross_product(vector_a, vector_b)\n# print(result)  # Output: [-3.0, 6.0, -3.0]\n```\n\n### Explanation:\n1. **Input Conversion**: The input vectors `a` and `b` are converted to NumPy arrays to facilitate vector operations.\n2. **Cross Product Calculation**: The `np.cross()` function is used to compute the cross product of the two vectors.\n3. **Rounding**: The resulting vector is rounded to 4 decimal places using `np.round()`.\n4. **Conversion to List**: The rounded NumPy array is converted to a list using the `tolist()` method before returning.\n\nThis implementation ensures that the cross product is calculated accurately and the output meets the specified constraints."}
{"task_id": 119, "completion_id": 0, "completion": "To solve a system of linear equations using Cramer's Rule, we need to calculate the determinant of the coefficient matrix \\( A \\) and the determinants of matrices obtained by replacing each column of \\( A \\) with the constant vector \\( b \\). Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Check if A is a square matrix\n    n, m = A.shape\n    if n != m:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, the system has no unique solution\n    if np.isclose(det_A, 0):\n        return -1\n    \n    # Initialize the solution vector\n    x = np.zeros(n)\n    \n    # Iterate over each column to calculate the solution using Cramer's Rule\n    for i in range(n):\n        # Create a copy of A to modify\n        A_i = np.copy(A)\n        \n        # Replace the i-th column of A with vector b\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the modified matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the i-th component of the solution vector\n        x[i] = det_A_i / det_A\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the solution vector as a list\n    return x.tolist()\n\n# Example usage:\n# A = [[2, -1, 0], [1, 2, -1], [3, 1, 2]]\n# b = [1, 2, 3]\n# print(cramers_rule(A, b))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the matrix \\( A \\) is square. If not, it raises a `ValueError`.\n\n2. **Determinant Calculation**: The determinant of \\( A \\) is calculated using `np.linalg.det()`. If the determinant is zero, the function returns `-1`, indicating no unique solution.\n\n3. **Cramer's Rule Application**: For each column \\( i \\) of \\( A \\), the function creates a modified matrix \\( A_i \\) by replacing the \\( i \\)-th column with the vector \\( b \\). It then calculates the determinant of \\( A_i \\) and uses it to compute the \\( i \\)-th component of the solution vector \\( x \\).\n\n4. **Rounding and Output**: The solution vector \\( x \\) is rounded to the nearest 4th decimal and returned as a list using the `tolist()` method.\n\nThis implementation efficiently solves the system of linear equations using Cramer's Rule, adhering to the specified constraints."}
{"task_id": 120, "completion_id": 0, "completion": "To calculate the Bhattacharyya distance between two discrete probability distributions, we need to follow these steps:\n\n1. **Validate Input**: Ensure that both input lists `p` and `q` are of the same length and are not empty. If they are not, return `0.0`.\n\n2. **Calculate the Bhattacharyya Coefficient**: This is done by summing the square roots of the products of corresponding probabilities from the two distributions.\n\n3. **Compute the Bhattacharyya Distance**: The distance is derived from the coefficient using the formula:\n   \\[\n   D_B(p, q) = -\\ln(\\text{BC}(p, q))\n   \\]\n   where \\(\\text{BC}(p, q)\\) is the Bhattacharyya coefficient.\n\n4. **Round the Result**: The final distance should be rounded to four decimal places.\n\nHere's the implementation in Python:\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the input lists are of the same length and not empty\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = sum(np.sqrt(p_i * q_i) for p_i, q_i in zip(p, q))\n    \n    # Calculate the Bhattacharyya distance\n    # Use np.log to handle the logarithm\n    try:\n        distance = -np.log(bc)\n    except ValueError:\n        # In case of a math domain error, return 0.0\n        return 0.0\n    \n    # Round the result to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\n# p = [0.1, 0.4, 0.5]\n# q = [0.2, 0.3, 0.5]\n# print(bhattacharyya_distance(p, q))  # Output should be rounded to 4 decimal places\n```\n\n### Explanation:\n\n- **Input Validation**: The function first checks if the lists `p` and `q` are of the same length and not empty. If not, it returns `0.0`.\n\n- **Bhattacharyya Coefficient Calculation**: The coefficient is calculated using a generator expression that iterates over pairs of elements from `p` and `q`, computes the square root of their product, and sums these values.\n\n- **Distance Calculation**: The natural logarithm of the Bhattacharyya coefficient is taken, and the negative of this value gives the distance. We handle potential math domain errors (e.g., if `bc` is zero or negative) by returning `0.0`.\n\n- **Rounding**: The result is rounded to four decimal places to meet the output constraints."}
{"task_id": 121, "completion_id": 0, "completion": "To solve the task of computing the element-wise sum of two vectors, we need to ensure that both vectors have the same length. If they do, we can proceed with the element-wise addition; otherwise, we return -1 to indicate incompatible dimensions. Here's the implementation:\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [a[i] + b[i] for i in range(len(a))]\n    \n    return result\n```\n\n### Explanation:\n1. **Length Check**: We first check if the lengths of the two vectors `a` and `b` are equal. If they are not, we return `-1` to indicate that the vectors cannot be summed element-wise due to incompatible dimensions.\n\n2. **Element-wise Sum**: If the lengths are equal, we use a list comprehension to iterate over the indices of the vectors and compute the sum of corresponding elements. The result is stored in a new list `result`.\n\n3. **Return Result**: Finally, we return the `result` list containing the element-wise sums.\n\nThis function handles both integer and float types, as specified in the type hints, and ensures that the operation is performed only when valid."}
{"task_id": 122, "completion_id": 0, "completion": "To implement the policy gradient estimator using the REINFORCE algorithm, we need to compute the gradient of the log-policy weighted by the return for each time step in the episodes. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    # Initialize the gradient to zero\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Compute the return for each time step\n        G = 0\n        returns = []\n        for _, _, reward in reversed(episode):\n            G = reward + G\n            returns.insert(0, G)\n        \n        # Compute the gradient for the episode\n        for t, (state, action, _) in enumerate(episode):\n            # Compute the policy using softmax\n            policy = softmax(theta[state, :])\n            \n            # Compute the gradient of the log-policy\n            grad_log_policy = np.zeros_like(theta)\n            grad_log_policy[state, action] = 1\n            grad_log_policy[state, :] -= policy\n            \n            # Update the gradient with the return\n            gradient += grad_log_policy * returns[t]\n    \n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round the results to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    \n    # Return the gradient as a list\n    return gradient.tolist()\n\n# Example usage:\ntheta = np.array([[0.2, 0.5, 0.3], [0.1, 0.7, 0.2]])\nepisodes = [\n    [(0, 1, 1.0), (1, 2, 2.0)],\n    [(0, 0, 0.5), (1, 1, 1.5)]\n]\ngradient = compute_policy_gradient(theta, episodes)\nprint(gradient)\n```\n\n### Explanation:\n1. **Softmax Function**: Computes the softmax probabilities for the actions given the state.\n2. **Return Calculation**: For each episode, calculate the return `G` for each time step by summing rewards from the current step to the end of the episode.\n3. **Gradient Calculation**: For each time step in the episode, compute the gradient of the log-policy and multiply it by the return. Accumulate these gradients over all episodes.\n4. **Average Gradient**: Divide the accumulated gradient by the number of episodes to get the average gradient.\n5. **Rounding**: Round the gradient values to the nearest 4th decimal.\n6. **Output**: Convert the gradient to a list using `tolist()` for the final output."}
{"task_id": 123, "completion_id": 0, "completion": "To calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer, we need to compute the number of floating-point operations (FLOPs) for both types of layers and then determine the percentage savings. Here's how you can implement this in Python:\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    n_experts (int): Number of experts in the MoE layer.\n    k_active (int): Number of active experts (sparsity).\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n\n    Returns:\n    float: Percentage of computational cost savings.\n    \"\"\"\n    # FLOPs for a dense layer\n    flops_dense = d_in * d_out\n\n    # FLOPs for an MoE layer\n    # Each active expert processes the input independently\n    flops_moe = k_active * (d_in * d_out)\n\n    # Calculate savings\n    savings = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    # Round the result to the nearest 1th decimal\n    savings_rounded = round(savings, 1)\n\n    return savings_rounded\n\n# Example usage\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 2048\n\nsavings = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational cost savings: {savings}%\")\n```\n\n### Explanation:\n1. **Dense Layer FLOPs**: The number of FLOPs for a dense layer is calculated as the product of the input dimension (`d_in`) and the output dimension (`d_out`).\n\n2. **MoE Layer FLOPs**: In an MoE layer, only a subset of experts (`k_active`) are active for each input. Each active expert processes the input independently, so the total FLOPs for the MoE layer is `k_active * (d_in * d_out)`.\n\n3. **Savings Calculation**: The savings are calculated as the difference between the FLOPs of the dense layer and the MoE layer, divided by the FLOPs of the dense layer, and then converted to a percentage.\n\n4. **Rounding**: The result is rounded to the nearest 1th decimal as per the output constraints.\n\nThis function will give you the percentage of computational cost savings when using an MoE layer compared to a dense layer, based on the specified parameters."}
{"task_id": 124, "completion_id": 0, "completion": "To implement the Noisy Top-K gating function, we need to follow these steps:\n\n1. Compute the logits by multiplying the input matrix `X` with the weight matrix `W_g`.\n2. Add noise to the logits using the pre-sampled noise matrix `N` and the noise weight matrix `W_noise`.\n3. Apply the Top-K operation to select the top `k` values for each row in the noisy logits.\n4. Compute the gating probabilities using a softmax function over the selected top-k values.\n5. Round the results to the nearest 4th decimal and return the result as a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Step 1: Compute the logits\n    logits = np.dot(X, W_g)\n    \n    # Step 2: Add noise to the logits\n    noisy_logits = logits + np.dot(N, W_noise)\n    \n    # Step 3: Apply Top-K operation\n    # Get the indices of the top-k values for each row\n    topk_indices = np.argpartition(noisy_logits, -k, axis=1)[:, -k:]\n    \n    # Create a mask for the top-k values\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    np.put_along_axis(mask, topk_indices, True, axis=1)\n    \n    # Step 4: Compute the gating probabilities using softmax\n    # Apply the mask to get only the top-k values\n    topk_values = np.where(mask, noisy_logits, -np.inf)\n    \n    # Compute softmax probabilities\n    exp_values = np.exp(topk_values - np.max(topk_values, axis=1, keepdims=True))\n    gating_probs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n    \n    # Step 5: Round the results to the nearest 4th decimal\n    rounded_probs = np.round(gating_probs, 4)\n    \n    # Return the result as a Python list\n    return rounded_probs.tolist()\n\n# Example usage:\n# X = np.array([[0.1, 0.2], [0.3, 0.4]])\n# W_g = np.array([[0.5, 0.6], [0.7, 0.8]])\n# W_noise = np.array([[0.1, 0.2], [0.3, 0.4]])\n# N = np.array([[0.01, 0.02], [0.03, 0.04]])\n# k = 1\n# result = noisy_topk_gating(X, W_g, W_noise, N, k)\n# print(result)\n```\n\nThis code will compute the noisy top-k gating probabilities for the given input matrix `X`, weight matrices `W_g` and `W_noise`, pre-sampled noise `N`, and sparsity constraint `k`. The results are rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 125, "completion_id": 0, "completion": "To implement a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing, we need to follow these steps:\n\n1. Compute the gating scores using the gating weight matrix.\n2. Apply softmax to the gating scores to get the gating probabilities.\n3. Select the top-k experts for each token based on the gating probabilities.\n4. Compute the output of the selected experts.\n5. Aggregate the results weighted by the normalized gating probabilities.\n\nHere's the implementation:\n\n```python\nimport numpy as np\nnp.random.seed(42)\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute gating scores\n    gating_scores = np.dot(x, Wg)\n    \n    # Apply softmax to get gating probabilities\n    gating_probs = softmax(gating_scores)\n    \n    # Initialize output tensor\n    output = np.zeros((x.shape[0], We.shape[2]))\n    \n    # Process each token\n    for i in range(x.shape[0]):\n        # Get top-k experts for the current token\n        top_k_indices = np.argsort(gating_probs[i])[-top_k:]\n        \n        # Compute the weighted sum of the top-k experts\n        for idx in top_k_indices:\n            expert_output = np.dot(x[i], We[idx])\n            output[i] += gating_probs[i, idx] * expert_output\n    \n    # Round results to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Return as a list\n    return output.tolist()\n\n# Example usage\nx = np.random.rand(5, 10)  # Input tensor with 5 tokens and 10 features\nWe = np.random.rand(3, 10, 8)  # 3 experts, each with a 10x8 weight matrix\nWg = np.random.rand(10, 3)  # Gating weight matrix for 3 experts\nn_experts = 3\ntop_k = 2\n\nresult = moe(x, We, Wg, n_experts, top_k)\nprint(result)\n```\n\n### Explanation:\n- **Softmax Function**: Computes the softmax probabilities for the gating scores.\n- **Gating Scores**: Calculated by multiplying the input tensor `x` with the gating weight matrix `Wg`.\n- **Top-k Selection**: For each token, the indices of the top-k experts are selected based on the gating probabilities.\n- **Expert Output**: For each selected expert, the transformation is applied to the input token, and the result is weighted by the gating probability.\n- **Aggregation**: The weighted outputs of the top-k experts are summed to produce the final output for each token.\n- **Rounding and Conversion**: The final output is rounded to the nearest 4th decimal and converted to a list using `tolist()`."}
{"task_id": 126, "completion_id": 0, "completion": "To implement Group Normalization, we need to divide the channels into groups, normalize each group, and then apply the learned scale and shift parameters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n    \n    # Ensure the number of groups divides the number of channels evenly\n    assert C % num_groups == 0, \"Number of channels must be divisible by num_groups\"\n    \n    # Calculate the number of channels per group\n    channels_per_group = C // num_groups\n    \n    # Reshape the input tensor to (B, num_groups, channels_per_group, H, W)\n    X = X.reshape(B, num_groups, channels_per_group, H, W)\n    \n    # Compute the mean and variance for each group\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X, axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize the input tensor\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Reshape gamma and beta to (1, num_groups, channels_per_group, 1, 1) for broadcasting\n    gamma = gamma.reshape(1, num_groups, channels_per_group, 1, 1)\n    beta = beta.reshape(1, num_groups, channels_per_group, 1, 1)\n    \n    # Apply scale (gamma) and shift (beta)\n    X_scaled_shifted = gamma * X_normalized + beta\n    \n    # Reshape back to the original shape (B, C, H, W)\n    X_scaled_shifted = X_scaled_shifted.reshape(B, C, H, W)\n    \n    # Round the results to the nearest 4th decimal\n    X_scaled_shifted = np.round(X_scaled_shifted, 4)\n    \n    # Convert the result to a Python list\n    return X_scaled_shifted.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 4, 2, 2)  # Example input tensor\n# gamma = np.ones((4,))  # Example gamma\n# beta = np.zeros((4,))  # Example beta\n# num_groups = 2  # Example number of groups\n# result = group_normalization(X, gamma, beta, num_groups)\n# print(result)\n```\n\nThis function performs Group Normalization on a 4D input tensor, applying the learned scale and shift parameters, and returns the result as a Python list with values rounded to the nearest 4th decimal. Make sure to adjust the example usage according to your specific input tensor, gamma, beta, and number of groups."}
{"task_id": 127, "completion_id": 0, "completion": "To solve this problem, we can use a gradient descent algorithm to find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\). Gradient descent is an iterative optimization algorithm used to find the minimum of a function. Here's how you can implement it in Python:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function f(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    # Define the derivative of the function f(x)\n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    # Initialize the current position\n    current_x = start_x\n    for _ in range(max_iters):\n        # Calculate the gradient at the current position\n        gradient = df(current_x)\n        \n        # Update the current position\n        new_x = current_x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(new_x - current_x) < tolerance:\n            break\n        \n        # Update current_x\n        current_x = new_x\n    \n    # Return the x value where the minimum is found, rounded to 4 decimal places\n    return round(current_x, 4)\n\n# Example usage:\n# start_x = np.random.uniform(-10, 10)  # Random starting point\n# print(find_treasure(start_x))\n```\n\n### Explanation:\n1. **Function Definition**: We define the function \\( f(x) = x^4 - 3x^3 + 2 \\) and its derivative \\( f'(x) = 4x^3 - 9x^2 \\).\n\n2. **Gradient Descent Loop**: We start from an initial guess `start_x` and iteratively update the position using the gradient descent update rule:\n   \\[\n   x_{\\text{new}} = x_{\\text{current}} - \\text{learning\\_rate} \\times f'(x_{\\text{current}})\n   \\]\n\n3. **Convergence Check**: The loop continues until the change in `x` is smaller than the specified `tolerance` or the maximum number of iterations `max_iters` is reached.\n\n4. **Result**: The function returns the `x` value where the minimum is found, rounded to four decimal places.\n\nThis implementation should effectively find the minimum of the function starting from any random initial position."}
{"task_id": 128, "completion_id": 0, "completion": "To implement the Dynamic Tanh (DyT) function, we need to create a transformation inspired by the Tanh function that can be used in Transformer architectures without requiring normalization. The function will take an input array `x` and parameters `alpha`, `gamma`, and `beta` to perform the transformation. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the dynamic tanh transformation\n    # The transformation is inspired by the Tanh function and uses the parameters alpha, gamma, and beta\n    # to adjust the squashing behavior and enable stable training.\n    \n    # Calculate the dynamic tanh\n    dy_tanh = alpha * np.tanh(gamma * x + beta)\n    \n    # Round the results to the nearest 4th decimal\n    dy_tanh_rounded = np.round(dy_tanh, 4)\n    \n    # Convert the numpy array to a list\n    result_list = dy_tanh_rounded.tolist()\n    \n    return result_list\n\n# Example usage:\n# x = np.array([0.1, 0.2, 0.3])\n# alpha = 1.0\n# gamma = 1.0\n# beta = 0.0\n# print(dynamic_tanh(x, alpha, gamma, beta))\n```\n\n### Explanation:\n1. **Dynamic Tanh Calculation**: The function applies a transformation similar to the Tanh function, scaled by `alpha` and adjusted by `gamma` and `beta`. This allows for flexible squashing behavior.\n\n2. **Rounding**: The results are rounded to the nearest 4th decimal using `np.round`.\n\n3. **Conversion to List**: The final output is converted to a Python list using the `tolist()` method, which is required by the task description.\n\nThis implementation should work for any input array `x` and parameters `alpha`, `gamma`, and `beta`, providing a normalization-free activation function suitable for Transformer architectures."}
