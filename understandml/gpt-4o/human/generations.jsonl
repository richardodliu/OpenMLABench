{"task_id": 1, "completion_id": 0, "completion": "To solve the task of computing the dot product of a matrix and a vector, we need to ensure that the number of columns in the matrix matches the length of the vector. If this condition is met, we can proceed to calculate the dot product for each row of the matrix with the vector. Here's the implementation:\n\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the matrix and vector dimensions are compatible\n    if not a or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result list\n    result = []\n    \n    # Compute the dot product for each row in the matrix\n    for row in a:\n        dot_product = sum(row[i] * b[i] for i in range(len(b)))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nvector = [1, 0, -1]\n\nprint(matrix_dot_vector(matrix, vector))  # Output: [-2, -2, -2]\n```\n\n### Explanation:\n1. **Dimension Check**: The function first checks if the matrix `a` is non-empty and if the number of columns in the matrix matches the length of the vector `b`. If not, it returns `-1`.\n\n2. **Dot Product Calculation**: If the dimensions are compatible, the function iterates over each row of the matrix. For each row, it calculates the dot product with the vector using a list comprehension and the `sum` function.\n\n3. **Result Construction**: The dot product of each row with the vector is appended to the `result` list, which is returned at the end.\n\nThis implementation efficiently computes the matrix-vector dot product while ensuring dimension compatibility."}
{"task_id": 2, "completion_id": 0, "completion": "To compute the transpose of a given matrix, you can iterate over the columns of the original matrix and construct rows for the transposed matrix. Here's how you can implement the `transpose_matrix` function:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if the input matrix is empty\n    if not a or not a[0]:\n        return []\n\n    # Get the number of rows and columns in the original matrix\n    num_rows = len(a)\n    num_cols = len(a[0])\n\n    # Initialize the transposed matrix with dimensions swapped\n    transposed = [[0] * num_rows for _ in range(num_cols)]\n\n    # Fill the transposed matrix\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed[j][i] = a[i][j]\n\n    return transposed\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\ntransposed_matrix = transpose_matrix(matrix)\nprint(transposed_matrix)\n```\n\n### Explanation:\n- The function first checks if the input matrix is empty or has empty rows, returning an empty list if so.\n- It calculates the number of rows and columns in the original matrix.\n- It initializes the transposed matrix with dimensions swapped (i.e., the number of rows becomes the number of columns and vice versa).\n- It iterates over each element of the original matrix, placing it in the correct position in the transposed matrix.\n- Finally, it returns the transposed matrix."}
{"task_id": 3, "completion_id": 0, "completion": "To accomplish the task of reshaping a matrix using Python and NumPy, we need to ensure that the total number of elements in the original matrix matches the total number of elements in the desired new shape. If they don't match, the matrix cannot be reshaped, and we should return an empty list. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Calculate the total number of elements in the original matrix\n    original_elements = sum(len(row) for row in a)\n    \n    # Calculate the total number of elements required for the new shape\n    new_elements = new_shape[0] * new_shape[1]\n    \n    # Check if reshaping is possible\n    if original_elements != new_elements:\n        return []\n    \n    # Convert the input list to a NumPy array\n    np_array = np.array(a)\n    \n    # Reshape the array using NumPy\n    reshaped_array = np_array.reshape(new_shape)\n    \n    # Convert the reshaped NumPy array back to a list\n    reshaped_list = reshaped_array.tolist()\n    \n    return reshaped_list\n\n# Example usage:\noriginal_matrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (3, 2)\nprint(reshape_matrix(original_matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n```\n\n### Explanation:\n1. **Element Count Check**: We first calculate the total number of elements in the original matrix and compare it with the number of elements required by the new shape. If they don't match, reshaping is not possible, and we return an empty list.\n\n2. **Conversion to NumPy Array**: We convert the input list to a NumPy array to leverage NumPy's reshaping capabilities.\n\n3. **Reshape**: We use NumPy's `reshape` method to reshape the array to the desired shape.\n\n4. **Convert Back to List**: Finally, we convert the reshaped NumPy array back to a list using the `tolist()` method, as required by the task constraints."}
{"task_id": 4, "completion_id": 0, "completion": "To accomplish the task of calculating the mean of a matrix either by row or by column, we can define a function `calculate_matrix_mean` that takes a matrix and a mode as input. The function will iterate over the matrix based on the specified mode and compute the mean accordingly. Here's how you can implement this:\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        return [sum(row) / len(row) for row in matrix]\n    \n    elif mode == 'column':\n        # Calculate mean for each column\n        num_columns = len(matrix[0])\n        num_rows = len(matrix)\n        \n        # Initialize a list to store the sum of each column\n        column_sums = [0] * num_columns\n        \n        # Sum up each column\n        for row in matrix:\n            for col_index in range(num_columns):\n                column_sums[col_index] += row[col_index]\n        \n        # Calculate mean for each column\n        return [column_sum / num_rows for column_sum in column_sums]\n\n# Example usage:\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(calculate_matrix_mean(matrix, 'row'))    # Output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix, 'column')) # Output: [4.0, 5.0, 6.0]\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the mode is either 'row' or 'column'. If not, it raises a `ValueError`.\n- **Row Mean Calculation**: If the mode is 'row', the function calculates the mean for each row by summing the elements of the row and dividing by the number of elements.\n- **Column Mean Calculation**: If the mode is 'column', the function calculates the mean for each column. It first initializes a list to store the sum of each column, iterates over each row to accumulate the sums, and then divides each sum by the number of rows to get the mean.\n- **Example Usage**: The example demonstrates how to use the function to calculate means by row and by column for a given matrix."}
{"task_id": 5, "completion_id": 0, "completion": "To perform scalar multiplication of a matrix, you need to multiply each element of the matrix by the given scalar. Below is the implementation of the `scalar_multiply` function that accomplishes this task:\n\n```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    # Create a new matrix to store the result\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Create a new row to store the result of the current row\n        new_row = []\n        \n        # Iterate over each element in the row\n        for element in row:\n            # Multiply the element by the scalar and append to the new row\n            new_row.append(element * scalar)\n        \n        # Append the new row to the result matrix\n        result.append(new_row)\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nscalar = 2\n\nresult = scalar_multiply(matrix, scalar)\nprint(result)\n```\n\n### Explanation:\n- The function `scalar_multiply` takes two arguments: `matrix` and `scalar`.\n- It initializes an empty list `result` to store the resulting matrix.\n- It iterates over each row of the input matrix.\n- For each row, it creates a new list `new_row` to store the results of multiplying each element by the scalar.\n- It iterates over each element in the current row, multiplies it by the scalar, and appends the result to `new_row`.\n- After processing all elements in the row, it appends `new_row` to the `result` matrix.\n- Finally, it returns the `result` matrix, which contains the scalar-multiplied values.\n\nThis function will work for matrices containing integers and/or floating-point numbers, and it will handle both integer and floating-point scalars."}
{"task_id": 6, "completion_id": 0, "completion": "To calculate the eigenvalues of a 2x2 matrix, we can use the characteristic equation of the matrix. For a 2x2 matrix:\n\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThe characteristic equation is given by:\n\n\\[ \\lambda^2 - \\text{tr}(A)\\lambda + \\text{det}(A) = 0 \\]\n\nwhere \\(\\text{tr}(A) = a + d\\) is the trace of the matrix and \\(\\text{det}(A) = ad - bc\\) is the determinant of the matrix. The solutions to this quadratic equation are the eigenvalues of the matrix.\n\nHere's how you can implement this in Python:\n\n```python\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the trace and determinant of the matrix\n    trace = a + d\n    determinant = a * d - b * c\n    \n    # Calculate the discriminant of the characteristic equation\n    discriminant = trace**2 - 4 * determinant\n    \n    # Calculate the two eigenvalues using the quadratic formula\n    eigenvalue1 = (trace + discriminant**0.5) / 2\n    eigenvalue2 = (trace - discriminant**0.5) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\nmatrix = [[4, 2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)\nprint(eigenvalues)  # Output: [5.0, 2.0]\n```\n\nThis function first extracts the elements of the matrix, calculates the trace and determinant, and then uses the quadratic formula to find the eigenvalues. Finally, it returns the eigenvalues sorted from highest to lowest."}
{"task_id": 7, "completion_id": 0, "completion": "To solve the task of transforming a matrix \\( A \\) using the operation \\( T^{-1}AS \\), we need to ensure that matrices \\( T \\) and \\( S \\) are invertible. If they are not invertible, the function should return -1. If they are invertible, we perform the matrix transformation and return the result rounded to the nearest 4th decimal.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert lists to numpy arrays\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by checking their determinants\n    if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n        return -1\n    \n    # Calculate the inverse of T\n    T_inv_np = np.linalg.inv(T_np)\n    \n    # Perform the matrix transformation T^{-1}AS\n    transformed_matrix_np = T_inv_np @ A_np @ S_np\n    \n    # Round the results to the nearest 4th decimal\n    transformed_matrix_np = np.round(transformed_matrix_np, 4)\n    \n    # Convert the numpy array back to a list\n    transformed_matrix_list = transformed_matrix_np.tolist()\n    \n    return transformed_matrix_list\n```\n\n### Explanation:\n1. **Convert Lists to Numpy Arrays**: We first convert the input lists \\( A \\), \\( T \\), and \\( S \\) into numpy arrays for easier matrix operations.\n\n2. **Check for Square Matrices**: We ensure that \\( T \\) and \\( S \\) are square matrices, as only square matrices can be invertible.\n\n3. **Check Invertibility**: We check the invertibility of \\( T \\) and \\( S \\) by verifying that their determinants are non-zero. If either matrix is not invertible, we return -1.\n\n4. **Matrix Transformation**: If both matrices are invertible, we compute the inverse of \\( T \\) and perform the matrix multiplication \\( T^{-1}AS \\).\n\n5. **Rounding**: We round the elements of the resulting matrix to the nearest 4th decimal.\n\n6. **Convert Back to List**: Finally, we convert the numpy array back to a list using the `tolist()` method and return it.\n\nThis implementation ensures that the matrix transformation is performed correctly and adheres to the constraints provided."}
{"task_id": 8, "completion_id": 0, "completion": "To calculate the inverse of a 2x2 matrix, we need to ensure that the matrix is invertible. A 2x2 matrix is invertible if its determinant is non-zero. The formula for the inverse of a 2x2 matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is:\n\n\\[\n\\text{Inverse} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\\]\n\nHere's the Python function to calculate the inverse of a 2x2 matrix:\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    determinant = a * d - b * c\n    \n    # If the determinant is zero, the matrix is not invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse using the formula\n    inverse_matrix = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    return inverse_matrix\n\n# Example usage:\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)\nprint(inverse)  # Output: [[0.6, -0.7], [-0.2, 0.4]]\n```\n\nThis function first calculates the determinant of the matrix. If the determinant is zero, it returns `None`, indicating that the matrix is not invertible. Otherwise, it computes the inverse using the formula provided and returns the resulting matrix."}
{"task_id": 9, "completion_id": 0, "completion": "To multiply two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix. If this condition is not met, the matrices cannot be multiplied, and we should return -1. Otherwise, we can proceed with the multiplication.\n\nHere's how you can implement the `matrixmul` function:\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if the matrices can be multiplied\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# a = [[1, 2, 3], [4, 5, 6]]\n# b = [[7, 8], [9, 10], [11, 12]]\n# print(matrixmul(a, b))  # Output: [[58, 64], [139, 154]]\n```\n\n### Explanation:\n\n1. **Check Compatibility**: The function first checks if the number of columns in matrix `a` is equal to the number of rows in matrix `b`. If not, it returns `-1`.\n\n2. **Initialize Result Matrix**: A result matrix is initialized with dimensions equal to the number of rows in `a` and the number of columns in `b`. All elements are initially set to zero.\n\n3. **Matrix Multiplication**: The function uses three nested loops to perform the multiplication:\n   - The outer loop iterates over the rows of matrix `a`.\n   - The middle loop iterates over the columns of matrix `b`.\n   - The innermost loop iterates over the elements of the current row of `a` and the current column of `b`, performing the dot product and accumulating the result in the appropriate position in the result matrix.\n\nThis implementation efficiently multiplies two matrices and handles the case where multiplication is not possible by returning `-1`."}
{"task_id": 10, "completion_id": 0, "completion": "To calculate the covariance matrix for a given set of vectors, we need to follow these steps:\n\n1. Calculate the mean of each feature.\n2. Compute the covariance between each pair of features.\n3. Construct the covariance matrix using these computed covariances.\n\nHere's the implementation of the function `calculate_covariance_matrix` along with some test cases:\n\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    \n    # Convert the list of lists into a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Number of features\n    num_features = data.shape[0]\n    \n    # Calculate the mean of each feature\n    means = np.mean(data, axis=1)\n    \n    # Initialize the covariance matrix\n    covariance_matrix = np.zeros((num_features, num_features))\n    \n    # Calculate the covariance between each pair of features\n    for i in range(num_features):\n        for j in range(num_features):\n            covariance_matrix[i][j] = np.mean((data[i] - means[i]) * (data[j] - means[j]))\n    \n    return covariance_matrix.tolist()\n\n# Test cases\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2D data\n    vectors1 = [\n        [2.1, 2.5, 3.6, 4.0],\n        [8.0, 10.0, 12.0, 14.0]\n    ]\n    expected1 = [\n        [0.9166666666666666, 2.833333333333333],\n        [2.833333333333333, 8.666666666666666]\n    ]\n    result1 = calculate_covariance_matrix(vectors1)\n    assert np.allclose(result1, expected1), f\"Test case 1 failed: {result1}\"\n\n    # Test case 2: 3D data\n    vectors2 = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    expected2 = [\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0]\n    ]\n    result2 = calculate_covariance_matrix(vectors2)\n    assert np.allclose(result2, expected2), f\"Test case 2 failed: {result2}\"\n\n    # Test case 3: Identical features\n    vectors3 = [\n        [1, 1, 1],\n        [1, 1, 1]\n    ]\n    expected3 = [\n        [0.0, 0.0],\n        [0.0, 0.0]\n    ]\n    result3 = calculate_covariance_matrix(vectors3)\n    assert np.allclose(result3, expected3), f\"Test case 3 failed: {result3}\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\n### Explanation:\n\n- **Data Input**: The function takes a list of lists, where each inner list represents a feature with its observations.\n- **Numpy Usage**: We use NumPy for efficient numerical operations. The input list of lists is converted to a NumPy array for easier manipulation.\n- **Mean Calculation**: The mean of each feature is calculated using `np.mean`.\n- **Covariance Calculation**: The covariance between each pair of features is calculated using the formula for covariance.\n- **Test Cases**: We provide test cases to verify the correctness of the implementation. The `np.allclose` function is used to compare floating-point results for equality within a tolerance.\n\nThis implementation should correctly compute the covariance matrix for any valid input of vectors."}
{"task_id": 11, "completion_id": 0, "completion": "To solve a system of linear equations using the Jacobi method, we need to iteratively update the solution vector until we reach the specified number of iterations. The Jacobi method is an iterative algorithm that updates each component of the solution vector independently using the values from the previous iteration.\n\nHere's how you can implement the Jacobi method in Python:\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Number of variables\n    num_vars = len(b)\n    \n    # Initial guess (zero vector)\n    x = np.zeros(num_vars)\n    \n    # Iterate n times\n    for _ in range(n):\n        # Create a new array for the next iteration\n        x_new = np.zeros(num_vars)\n        \n        for i in range(num_vars):\n            # Sum the known values\n            sum_ = sum(A[i][j] * x[j] for j in range(num_vars) if j != i)\n            \n            # Update the ith variable\n            x_new[i] = (b[i] - sum_) / A[i][i]\n        \n        # Round the new solution to 4 decimal places\n        x = np.round(x_new, 4)\n    \n    # Return the solution as a list\n    return x.tolist()\n\n# Example usage:\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\nn = 25\n\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\n### Explanation:\n\n1. **Initialization**: We start with an initial guess for the solution vector `x`, which is a zero vector of the same length as `b`.\n\n2. **Iteration**: For each iteration, we compute a new solution vector `x_new`:\n   - For each variable `i`, we calculate the sum of the products of the coefficients `A[i][j]` and the current solution `x[j]` for all `j` except `i`.\n   - We then update `x_new[i]` using the formula derived from the linear equation: \\((b[i] - \\text{sum}) / A[i][i]\\).\n\n3. **Rounding**: After computing the new solution vector for the current iteration, we round each element to four decimal places.\n\n4. **Update**: We update `x` with `x_new` for the next iteration.\n\n5. **Return**: After completing the specified number of iterations, we return the solution vector as a list.\n\nThis implementation assumes that the matrix `A` is diagonally dominant, which is a common requirement for the Jacobi method to converge. If `A` is not diagonally dominant, the method may not converge to the correct solution."}
{"task_id": 12, "completion_id": 0, "completion": "To approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobi method, we need to iteratively apply rotations to zero out the off-diagonal elements of the matrix. This process will help us find the singular values of the matrix. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure the input is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input must be a 2x2 matrix.\"\n    \n    # Initialize U and V as identity matrices\n    U = np.eye(2)\n    V = np.eye(2)\n    \n    # Tolerance for convergence\n    tol = 1e-10\n    max_iterations = 100\n    iteration = 0\n    \n    # Jacobi rotation to zero out the off-diagonal elements\n    while np.abs(A[0, 1]) > tol and iteration < max_iterations:\n        iteration += 1\n        \n        # Calculate the Jacobi rotation\n        if A[0, 1] != 0:\n            tau = (A[1, 1] - A[0, 0]) / (2 * A[0, 1])\n            t = np.sign(tau) / (np.abs(tau) + np.sqrt(1 + tau**2))\n            c = 1 / np.sqrt(1 + t**2)\n            s = c * t\n        else:\n            c = 1\n            s = 0\n        \n        # Apply the rotation to A\n        J = np.array([[c, s], [-s, c]])\n        A = J.T @ A @ J\n        \n        # Accumulate the rotations into U and V\n        U = U @ J\n        V = V @ J.T\n    \n    # The singular values are the absolute values of the diagonal elements of A\n    singular_values = np.abs(np.diag(A))\n    \n    # Sort the singular values in descending order\n    singular_values.sort()\n    singular_values = singular_values[::-1]\n    \n    # Round the singular values to the nearest 4th decimal\n    singular_values = tuple(np.round(singular_values, 4))\n    \n    return singular_values\n\n# Example usage:\nA = np.array([[4, 3], [3, 2]])\nsingular_values = svd_2x2_singular_values(A)\nprint(singular_values)\n```\n\n### Explanation:\n1. **Initialization**: We start with identity matrices for `U` and `V`, which will accumulate the rotations.\n2. **Jacobi Rotation**: We iteratively apply a Jacobi rotation to zero out the off-diagonal elements of the matrix `A`. The rotation is determined by the values of the matrix.\n3. **Convergence**: The loop continues until the off-diagonal element is sufficiently small (below a tolerance) or a maximum number of iterations is reached.\n4. **Singular Values**: After convergence, the singular values are the absolute values of the diagonal elements of the transformed matrix `A`.\n5. **Sorting and Rounding**: The singular values are sorted in descending order and rounded to the nearest 4th decimal.\n\nThis method provides an approximation of the singular values for a 2x2 matrix using the Jacobi method."}
{"task_id": 13, "completion_id": 0, "completion": "To calculate the determinant of a 4x4 matrix using Laplace's Expansion, we can break down the problem into smaller subproblems by recursively calculating the determinants of 3x3 matrices. Here's how you can implement this in Python:\n\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    def determinant_3x3(m: list[list[int | float]]) -> float:\n        # Calculate the determinant of a 3x3 matrix\n        return (\n            m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1])\n            - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0])\n            + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n        )\n\n    def get_minor(m: list[list[int | float]], i: int, j: int) -> list[list[int | float]]:\n        # Get the minor of the matrix by removing the i-th row and j-th column\n        return [row[:j] + row[j+1:] for row in (m[:i] + m[i+1:])]\n\n    # Calculate the determinant of the 4x4 matrix using Laplace's expansion\n    det = 0\n    for col in range(4):\n        minor = get_minor(matrix, 0, col)\n        cofactor = ((-1) ** col) * matrix[0][col] * determinant_3x3(minor)\n        det += cofactor\n\n    return det\n\n# Example usage:\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix))\n```\n\n### Explanation:\n\n1. **determinant_3x3 Function**: This function calculates the determinant of a 3x3 matrix using the standard formula for 3x3 determinants.\n\n2. **get_minor Function**: This helper function generates the minor of a matrix by removing the specified row and column. This is used to create the 3x3 matrices needed for the recursive calculation.\n\n3. **determinant_4x4 Function**: This is the main function that calculates the determinant of the 4x4 matrix. It uses Laplace's expansion along the first row. For each element in the first row, it calculates the cofactor, which is the product of the element, its sign (determined by the position), and the determinant of its minor.\n\nThis implementation is recursive and leverages the smaller 3x3 determinant calculations to solve the larger 4x4 determinant problem."}
{"task_id": 14, "completion_id": 0, "completion": "To perform linear regression using the normal equation, we need to compute the coefficients using the formula:\n\n\\[ \\theta = (X^T X)^{-1} X^T y \\]\n\nHere's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Compute the coefficients using the normal equation\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the result back to a list and return\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [1, 2, 3]\n# print(linear_regression_normal_equation(X, y))\n```\n\n### Explanation:\n\n1. **Input Conversion**: Convert the input lists `X` and `y` into NumPy arrays for efficient matrix operations.\n\n2. **Intercept Term**: Add a column of ones to `X` to include the intercept term in the model. This is done using `np.hstack` to concatenate a column of ones with the original feature matrix.\n\n3. **Normal Equation**: Calculate the coefficients using the normal equation formula. The `@` operator is used for matrix multiplication in NumPy.\n\n4. **Rounding**: Use `np.round` to round the coefficients to four decimal places.\n\n5. **Output**: Convert the rounded coefficients back to a list before returning them.\n\nThis function will compute the linear regression coefficients using the normal equation method and ensure the results are rounded to four decimal places."}
{"task_id": 15, "completion_id": 0, "completion": "To implement linear regression using gradient descent, we need to iteratively update the coefficients to minimize the cost function, which is typically the mean squared error for linear regression. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Number of training examples\n    m = len(y)\n    \n    # Initialize coefficients (theta) to zeros\n    theta = np.zeros(X.shape[1])\n    \n    for _ in range(iterations):\n        # Calculate the predictions\n        predictions = X.dot(theta)\n        \n        # Calculate the error\n        error = predictions - y\n        \n        # Calculate the gradient\n        gradient = (1/m) * X.T.dot(error)\n        \n        # Update the coefficients\n        theta -= alpha * gradient\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Return the coefficients as a list\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = np.array([[1, 1], [1, 2], [1, 3]])\n# y = np.array([1, 2, 3])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the coefficients (`theta`) to zeros. The number of coefficients is equal to the number of features in `X`.\n\n2. **Gradient Descent Loop**: For each iteration, we:\n   - Compute the predictions by multiplying the feature matrix `X` with the coefficients `theta`.\n   - Calculate the error by subtracting the actual target values `y` from the predictions.\n   - Compute the gradient, which is the derivative of the cost function with respect to the coefficients. This is done by taking the dot product of the transpose of `X` and the error, scaled by the number of examples `m`.\n   - Update the coefficients by subtracting the product of the learning rate `alpha` and the gradient.\n\n3. **Rounding**: After completing the iterations, we round the coefficients to four decimal places.\n\n4. **Return**: Finally, we convert the coefficients to a list using the `tolist()` method and return them.\n\nThis function will perform linear regression using gradient descent and return the coefficients rounded to four decimal places."}
{"task_id": 16, "completion_id": 0, "completion": "To implement feature scaling using both standardization and min-max normalization, we need to calculate the necessary statistics for each feature and apply the respective transformations. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization: (x - mean) / std\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    standardized_data = (data - means) / stds\n    \n    # Min-Max Normalization: (x - min) / (max - min)\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    min_max_normalized_data = (data - mins) / (maxs - mins)\n    \n    # Round the results to the nearest 4th decimal\n    standardized_data = np.round(standardized_data, 4)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4)\n    \n    # Convert numpy arrays to lists\n    standardized_list = standardized_data.tolist()\n    min_max_normalized_list = min_max_normalized_data.tolist()\n    \n    return standardized_list, min_max_normalized_list\n\n# Example usage:\n# data = np.array([[1, 2], [3, 4], [5, 6]])\n# standardized, min_max_normalized = feature_scaling(data)\n# print(\"Standardized:\", standardized)\n# print(\"Min-Max Normalized:\", min_max_normalized)\n```\n\n### Explanation:\n1. **Standardization**: For each feature, subtract the mean and divide by the standard deviation. This centers the data around zero with a unit variance.\n2. **Min-Max Normalization**: For each feature, subtract the minimum value and divide by the range (max - min). This scales the data to a range of [0, 1].\n3. **Rounding**: Use `np.round()` to round the results to the nearest 4th decimal.\n4. **Conversion to List**: Use `tolist()` to convert the NumPy arrays to Python lists, as required by the task.\n\nThis function will return two lists containing the scaled data using both methods."}
{"task_id": 17, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Means clustering algorithm in Python, following the specified requirements:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Step 1: Assign each point to the nearest centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        closest_centroids = np.argmin(distances, axis=1)\n        \n        # Step 2: Update centroids to be the mean of points assigned to them\n        new_centroids = np.array([points[closest_centroids == i].mean(axis=0) if np.any(closest_centroids == i) else centroids[i] for i in range(k)])\n        \n        # Check for convergence (if centroids do not change)\n        if np.allclose(new_centroids, centroids, atol=1e-4):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(c, 4)) for c in centroids]\n    \n    return final_centroids\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nk = 2\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\n### Explanation:\n1. **Initialization**: Convert the list of points and initial centroids into NumPy arrays for efficient computation.\n\n2. **Assignment Step**: For each point, compute the distance to each centroid and assign the point to the nearest centroid.\n\n3. **Update Step**: Calculate the new centroids as the mean of all points assigned to each centroid. If a centroid has no points assigned, it remains unchanged.\n\n4. **Convergence Check**: If the centroids do not change significantly (within a tolerance of `1e-4`), the algorithm stops early.\n\n5. **Rounding**: The final centroids are rounded to the nearest fourth decimal before returning.\n\nThis implementation efficiently handles the k-Means clustering process and adheres to the specified constraints."}
{"task_id": 18, "completion_id": 0, "completion": "Certainly! Below is a Python function that implements K-Fold Cross-Validation. This function takes a dataset `X` and labels `y`, divides them into `k` folds, and returns a list of train-test indices for each fold. It also includes options to shuffle the data and set a random seed for reproducibility.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Perform K-Fold Cross-Validation on the given dataset.\n\n    Parameters:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    random_seed (int): Random seed for reproducibility. Default is None.\n\n    Returns:\n    list: A list of tuples, each containing train indices and test indices for each fold.\n    \"\"\"\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.arange(len(X))\n    \n    fold_sizes = np.full(k, len(X) // k, dtype=int)\n    fold_sizes[:len(X) % k] += 1  # Distribute the remainder across the first folds\n    \n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate([indices[:start], indices[stop:]])\n        folds.append((train_indices, test_indices))\n        current = stop\n    \n    return folds\n\n# Example usage:\n# X = np.array([...])  # Feature matrix\n# y = np.array([...])  # Target vector\n# folds = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\n# for train_idx, test_idx in folds:\n#     print(\"Train indices:\", train_idx)\n#     print(\"Test indices:\", test_idx)\n```\n\n### Explanation:\n- **Shuffle**: The function shuffles the indices of the dataset if `shuffle` is set to `True`. This ensures that the data is randomly distributed across the folds.\n- **Random Seed**: Setting a `random_seed` ensures that the shuffling is reproducible, which is useful for debugging and consistent results.\n- **Fold Sizes**: The dataset is divided into `k` folds, and the remainder is distributed across the first few folds to ensure all data is used.\n- **Indices**: For each fold, the function calculates the train and test indices and appends them to the `folds` list.\n\nThis function can be used to generate train-test splits for K-Fold Cross-Validation, which is a common technique in machine learning for assessing model performance."}
{"task_id": 19, "completion_id": 0, "completion": "To implement Principal Component Analysis (PCA) from scratch, we need to follow these steps:\n\n1. **Standardize the dataset**: Center the data by subtracting the mean of each feature and scale it by dividing by the standard deviation.\n2. **Compute the covariance matrix**: Calculate the covariance matrix of the standardized data.\n3. **Find eigenvalues and eigenvectors**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n4. **Sort eigenvectors by eigenvalues**: Sort the eigenvectors based on the descending order of their corresponding eigenvalues.\n5. **Select the top k eigenvectors**: Choose the top k eigenvectors to form the principal components.\n\nHere's the implementation of the PCA function:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the dataset\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Step 4: Sort eigenvectors by eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Round the results to the nearest 4th decimal\n    principal_components_rounded = np.round(principal_components, 4)\n    \n    # Convert to list of lists for the output\n    return principal_components_rounded.tolist()\n\n# Example usage:\n# data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n# k = 2\n# print(pca(data, k))\n```\n\nThis function will return the top `k` principal components as a list of lists, with each component rounded to the nearest 4th decimal. You can test the function with your own data and specify the number of principal components you want to extract."}
{"task_id": 20, "completion_id": 0, "completion": "To implement a decision tree learning algorithm using entropy and information gain, we need to follow these steps:\n\n1. Calculate the entropy of the dataset.\n2. Calculate the information gain for each attribute.\n3. Choose the attribute with the highest information gain to split the dataset.\n4. Recursively apply the above steps to the subsets created by the split until a stopping condition is met (e.g., all examples have the same classification, or no attributes are left to split).\n\nHere's a Python implementation of the decision tree learning algorithm:\n\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the given examples.\"\"\"\n    # Count the frequency of each class label in the target attribute\n    label_counts = Counter(example[target_attr] for example in examples)\n    total_examples = len(examples)\n    \n    # Calculate entropy\n    entropy_value = 0.0\n    for count in label_counts.values():\n        probability = count / total_examples\n        entropy_value -= probability * math.log2(probability)\n    \n    return entropy_value\n\ndef information_gain(examples, attr, target_attr):\n    \"\"\"Calculate the information gain of splitting the examples by the given attribute.\"\"\"\n    # Calculate the entropy of the whole dataset\n    total_entropy = entropy(examples, target_attr)\n    \n    # Group examples by the attribute values\n    attr_values = set(example[attr] for example in examples)\n    weighted_entropy = 0.0\n    \n    for value in attr_values:\n        subset = [example for example in examples if example[attr] == value]\n        subset_entropy = entropy(subset, target_attr)\n        weighted_entropy += (len(subset) / len(examples)) * subset_entropy\n    \n    # Information gain is the reduction in entropy\n    return total_entropy - weighted_entropy\n\ndef majority_class(examples, target_attr):\n    \"\"\"Return the most common class label in the examples.\"\"\"\n    label_counts = Counter(example[target_attr] for example in examples)\n    return label_counts.most_common(1)[0][0]\n\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"Recursively build a decision tree using the given examples and attributes.\"\"\"\n    # If all examples have the same classification, return that classification\n    if len(set(example[target_attr] for example in examples)) == 1:\n        return examples[0][target_attr]\n    \n    # If no attributes are left, return the majority class\n    if not attributes:\n        return majority_class(examples, target_attr)\n    \n    # Choose the best attribute to split on\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    \n    # Create a subtree for each value of the best attribute\n    tree = {best_attr: {}}\n    attr_values = set(example[best_attr] for example in examples)\n    \n    for value in attr_values:\n        subset = [example for example in examples if example[best_attr] == value]\n        # If the subset is empty, use the majority class of the parent node\n        if not subset:\n            tree[best_attr][value] = majority_class(examples, target_attr)\n        else:\n            # Recursively build the subtree\n            remaining_attributes = [attr for attr in attributes if attr != best_attr]\n            tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n    \n    return tree\n\n# Example usage:\nexamples = [\n    {'attribute1': 'A', 'attribute2': 'B', 'target': 'Yes'},\n    {'attribute1': 'A', 'attribute2': 'C', 'target': 'No'},\n    {'attribute1': 'B', 'attribute2': 'B', 'target': 'Yes'},\n    {'attribute1': 'B', 'attribute2': 'C', 'target': 'No'},\n]\n\nattributes = ['attribute1', 'attribute2']\ntarget_attr = 'target'\n\ndecision_tree = learn_decision_tree(examples, attributes, target_attr)\nprint(decision_tree)\n```\n\nThis code defines a function `learn_decision_tree` that builds a decision tree using entropy and information gain. It recursively splits the dataset based on the attribute with the highest information gain until a stopping condition is met. The resulting decision tree is represented as a nested dictionary."}
{"task_id": 21, "completion_id": 0, "completion": "To implement the Pegasos Kernel SVM algorithm, we need to define the kernel functions, compute the kernel matrix, and iteratively update the alpha coefficients and bias. Below is the implementation of the deterministic Pegasos algorithm for kernel SVM:\n\n```python\nimport numpy as np\n\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n\ndef rbf_kernel(x1, x2, sigma):\n    return np.exp(-np.linalg.norm(x1 - x2)**2 / (2 * (sigma ** 2)))\n\ndef compute_kernel_matrix(data, kernel, sigma):\n    n_samples = data.shape[0]\n    kernel_matrix = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if kernel == 'linear':\n                kernel_matrix[i, j] = linear_kernel(data[i], data[j])\n            elif kernel == 'rbf':\n                kernel_matrix[i, j] = rbf_kernel(data[i], data[j], sigma)\n    return kernel_matrix\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    \n    kernel_matrix = compute_kernel_matrix(data, kernel, sigma)\n    \n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        \n        for i in range(n_samples):\n            margin = labels[i] * (np.dot(alpha * labels, kernel_matrix[i]) + bias)\n            if margin < 1:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i] + learning_rate * labels[i]\n            else:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i]\n        \n        bias = np.mean(labels - np.dot(alpha * labels, kernel_matrix))\n    \n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n    \n    return alpha.tolist(), bias\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4]])\n# labels = np.array([1, -1, 1])\n# alpha, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.01, iterations=100, sigma=1.0)\n# print(\"Alpha coefficients:\", alpha)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n1. **Kernel Functions**: We define `linear_kernel` and `rbf_kernel` functions to compute the kernel values between two samples.\n2. **Kernel Matrix**: The `compute_kernel_matrix` function calculates the kernel matrix for the entire dataset based on the chosen kernel.\n3. **Pegasos Algorithm**: The main function `pegasos_kernel_svm` iteratively updates the alpha coefficients using the deterministic approach (using all samples in each iteration).\n4. **Learning Rate**: The learning rate decreases over time, which is typical for Pegasos.\n5. **Bias Calculation**: The bias is updated as the mean of the difference between labels and the weighted sum of kernel values.\n6. **Output**: The alpha coefficients and bias are rounded to the nearest 4th decimal and returned as a list.\n\nThis implementation should work for both linear and RBF kernels, and it performs binary classification."}
{"task_id": 22, "completion_id": 0, "completion": "To implement the sigmoid activation function in Python, we can use the mathematical formula for the sigmoid function, which is:\n\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n\nHere's the complete implementation of the function, which rounds the output to four decimal places:\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    # Calculate the sigmoid function\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    # Round the result to four decimal places\n    return round(sigmoid_value, 4)\n```\n\nThis function takes a floating-point number `z` as input, computes the sigmoid of `z`, and returns the result rounded to four decimal places. The `math.exp` function is used to calculate the exponential part of the sigmoid formula."}
{"task_id": 23, "completion_id": 0, "completion": "To implement the softmax activation function, we need to follow these steps:\n\n1. **Exponentiate each score**: This is done to ensure all values are positive and to amplify differences between scores.\n2. **Normalize the scores**: Divide each exponentiated score by the sum of all exponentiated scores. This step ensures that the output values sum to 1, making them interpretable as probabilities.\n3. **Round the results**: Round each value to four decimal places as specified.\n\nHere's the implementation of the `softmax` function:\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Step 1: Exponentiate each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Step 2: Compute the sum of exponentiated scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Step 3: Normalize and round each score\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_values\n```\n\n### Explanation:\n\n- **Exponentiation**: We use `math.exp(score)` to compute the exponential of each score. This is a crucial step in the softmax function.\n- **Normalization**: We divide each exponentiated score by the total sum of all exponentiated scores to ensure the output is a probability distribution.\n- **Rounding**: The `round` function is used to round each value to four decimal places, as required.\n\nThis function will take a list of scores and return a list of softmax values, each rounded to four decimal places."}
{"task_id": 24, "completion_id": 0, "completion": "To implement the function `single_neuron_model`, we need to perform the following steps:\n\n1. Calculate the weighted sum of inputs for each feature vector.\n2. Apply the sigmoid activation function to the weighted sum to get the predicted probabilities.\n3. Compute the mean squared error between the predicted probabilities and the true labels.\n4. Return the predicted probabilities and the mean squared error, both rounded to four decimal places.\n\nHere's the implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Calculate the predicted probabilities using the sigmoid activation function\n    predicted_probabilities = []\n    for feature_vector in features:\n        # Calculate the weighted sum\n        weighted_sum = sum(f * w for f, w in zip(feature_vector, weights)) + bias\n        # Apply sigmoid activation function\n        probability = sigmoid(weighted_sum)\n        # Append the result rounded to 4 decimal places\n        predicted_probabilities.append(round(probability, 4))\n    \n    # Calculate the mean squared error\n    mse = np.mean([(p - l) ** 2 for p, l in zip(predicted_probabilities, labels)])\n    mse = round(mse, 4)\n    \n    # Return the predicted probabilities and the mean squared error\n    return np.array(predicted_probabilities).reshape(-1).tolist(), mse\n\n# Example usage:\nfeatures = [[0.5, 1.5], [1.0, -1.0], [2.0, 0.0]]\nlabels = [1, 0, 1]\nweights = [0.4, -0.6]\nbias = 0.1\n\npredicted_probs, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probs)\nprint(\"Mean Squared Error:\", mse)\n```\n\n### Explanation:\n- **Sigmoid Function**: The `sigmoid` function computes the sigmoid activation, which is used to map the weighted sum to a probability between 0 and 1.\n- **Weighted Sum**: For each feature vector, the weighted sum is calculated using the dot product of the feature vector and weights, plus the bias.\n- **Predicted Probabilities**: The sigmoid function is applied to the weighted sum to get the predicted probability for each example.\n- **Mean Squared Error (MSE)**: The MSE is calculated by averaging the squared differences between the predicted probabilities and the true labels.\n- **Rounding**: Both the predicted probabilities and the MSE are rounded to four decimal places as required."}
{"task_id": 25, "completion_id": 0, "completion": "To implement a single neuron with sigmoid activation and backpropagation using gradient descent, we need to follow these steps:\n\n1. **Forward Pass**: Compute the neuron's output using the sigmoid activation function.\n2. **Loss Calculation**: Calculate the Mean Squared Error (MSE) loss.\n3. **Backward Pass**: Compute the gradients of the loss with respect to the weights and bias.\n4. **Update Parameters**: Adjust the weights and bias using the computed gradients and the learning rate.\n5. **Repeat**: Perform the above steps for a specified number of epochs.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n\n        # Calculate MSE loss\n        errors = predictions - labels\n        mse = np.mean(errors ** 2)\n        mse_list.append(round(mse, 4))\n\n        # Backward pass\n        # Gradient of loss w.r.t. predictions\n        d_loss_d_predictions = 2 * errors / len(labels)\n        \n        # Gradient of predictions w.r.t. linear output\n        d_predictions_d_linear_output = predictions * (1 - predictions)\n        \n        # Gradient of linear output w.r.t. weights and bias\n        d_linear_output_d_weights = features\n        d_linear_output_d_bias = 1\n\n        # Chain rule to get gradients w.r.t. weights and bias\n        d_loss_d_linear_output = d_loss_d_predictions * d_predictions_d_linear_output\n        d_loss_d_weights = np.dot(d_linear_output_d_weights.T, d_loss_d_linear_output)\n        d_loss_d_bias = np.sum(d_loss_d_linear_output * d_linear_output_d_bias)\n\n        # Update weights and bias\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n\n    return weights.round(4).tolist(), round(bias, 4), mse_list\n\n# Example usage:\n# features = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n# labels = np.array([0, 1, 0])\n# initial_weights = np.array([0.1, 0.2])\n# initial_bias = 0.1\n# learning_rate = 0.01\n# epochs = 1000\n# updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n# print(updated_weights, updated_bias, mse_values)\n```\n\n### Explanation:\n- **Sigmoid Function**: Used to squash the linear output to a range between 0 and 1.\n- **MSE Loss**: Measures the average squared difference between predictions and true labels.\n- **Gradient Calculation**: Uses the chain rule to compute gradients for weights and bias.\n- **Parameter Update**: Adjusts weights and bias using the computed gradients and learning rate.\n- **Epoch Loop**: Repeats the process for a specified number of epochs, storing the MSE for each epoch.\n\nThis function will return the updated weights, bias, and a list of MSE values for each epoch, all rounded to four decimal places."}
{"task_id": 26, "completion_id": 0, "completion": "To implement basic autograd operations, we need to define a class that can perform operations like addition, multiplication, and ReLU activation, while also keeping track of the computation graph to enable automatic differentiation. Below is a Python class that accomplishes this:\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    a = Value(2.0)\n    b = Value(-3.0)\n    c = Value(10.0)\n    d = a * b + c\n    e = d.relu()\n    e.backward()\n\n    print(a)  # Value(data=2.0, grad=-3.0)\n    print(b)  # Value(data=-3.0, grad=2.0)\n    print(c)  # Value(data=10.0, grad=1.0)\n    print(d)  # Value(data=4.0, grad=1.0)\n    print(e)  # Value(data=4.0, grad=1.0)\n```\n\n### Explanation:\n- **Addition (`__add__`)**: Computes the sum of two `Value` objects and sets up the backward function to propagate gradients.\n- **Multiplication (`__mul__`)**: Computes the product of two `Value` objects and sets up the backward function to propagate gradients.\n- **ReLU (`relu`)**: Applies the ReLU activation function, which outputs the input if it's positive and zero otherwise. The backward function propagates gradients only if the output is positive.\n- **Backward Propagation (`backward`)**: Computes the gradients by traversing the computation graph in reverse topological order.\n- **Representation (`__repr__`)**: Provides a string representation of the `Value` object, showing its data and gradient.\n\nThis class allows you to perform operations and automatically compute gradients, which is essential for training neural networks using backpropagation."}
{"task_id": 27, "completion_id": 0, "completion": "To compute the transformation matrix from basis B to basis C in \\( \\mathbb{R}^3 \\), we need to find the matrix \\( P \\) such that \\( P \\cdot B = C \\). This involves solving the equation \\( P = C \\cdot B^{-1} \\), where \\( B^{-1} \\) is the inverse of the matrix formed by the basis vectors in B.\n\nHere's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis lists to numpy arrays\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    \n    # Compute the inverse of the matrix B\n    B_inv = np.linalg.inv(B_matrix)\n    \n    # Compute the transformation matrix P\n    P = np.dot(C_matrix, B_inv)\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the numpy array back to a list\n    return P_rounded.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\nprint(transform_basis(B, C))\n```\n\n### Explanation:\n1. **Convert to NumPy Arrays**: We first convert the input lists to NumPy arrays for easier matrix operations.\n2. **Inverse of B**: We compute the inverse of the matrix formed by basis B using `np.linalg.inv()`.\n3. **Compute P**: The transformation matrix \\( P \\) is computed by multiplying matrix C with the inverse of matrix B.\n4. **Rounding**: We round the elements of matrix \\( P \\) to the nearest 4th decimal using `np.round()`.\n5. **Convert to List**: Finally, we convert the rounded NumPy array back to a Python list using the `tolist()` method.\n\nThis function will return the transformation matrix \\( P \\) as a list of lists, with each element rounded to four decimal places."}
{"task_id": 28, "completion_id": 0, "completion": "To compute the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on eigendecomposition of \\(A^T A\\), we can follow the steps outlined in the task description. Here's the implementation:\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Step 1: Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Step 2: Perform eigendecomposition on A^T A\n    eigenvalues, V = np.linalg.eigh(ATA)\n    \n    # Step 3: Construct the singular values as the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Step 4: Form the diagonal matrix S\n    S = np.zeros_like(A, dtype=float)\n    np.fill_diagonal(S, singular_values)\n    \n    # Step 5: Compute U using the formula U = A * V * S^(-1)\n    U = np.zeros_like(A, dtype=float)\n    for i in range(len(singular_values)):\n        if singular_values[i] > 1e-10:  # Avoid division by zero\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    \n    # Step 6: Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Step 7: Return the results as lists\n    return U.tolist(), S.tolist(), V.tolist()\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\n### Explanation:\n1. **Compute \\(A^T A\\)**: This matrix is symmetric and positive semi-definite, which allows us to perform eigendecomposition.\n\n2. **Eigendecomposition**: We use `np.linalg.eigh` to get the eigenvalues and eigenvectors of \\(A^T A\\). The eigenvectors form the columns of matrix \\(V\\).\n\n3. **Singular Values**: The singular values are the square roots of the eigenvalues. We construct the diagonal matrix \\(S\\) using these singular values.\n\n4. **Compute \\(U\\)**: For each non-zero singular value, we compute the corresponding column of \\(U\\) by normalizing \\(A v_i\\).\n\n5. **Rounding**: We round the matrices \\(U\\), \\(S\\), and \\(V\\) to the nearest 4th decimal.\n\n6. **Return**: The matrices are converted to lists using `tolist()` for the output format.\n\nThis implementation should work correctly for any 2x2 matrix, providing the SVD decomposition as specified."}
{"task_id": 29, "completion_id": 0, "completion": "To accomplish the task of shuffling two numpy arrays while maintaining the corresponding order between them, we can use numpy's random permutation functionality. Here's how you can implement the `shuffle_data` function:\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Set the seed for reproducibility if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random permutation of indices based on the length of X\n    indices = np.random.permutation(X.shape[0])\n    \n    # Shuffle X and y using the generated indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert the shuffled arrays to lists and return them\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n# print(X_shuffled)\n# print(y_shuffled)\n```\n\n### Explanation:\n1. **Seed Setting**: If a seed is provided, it is set using `np.random.seed(seed)` to ensure reproducibility of the shuffle.\n2. **Random Permutation**: `np.random.permutation(X.shape[0])` generates a random permutation of indices based on the number of samples in `X`.\n3. **Shuffling**: The arrays `X` and `y` are shuffled using the generated indices, ensuring that the order between them is maintained.\n4. **Conversion to List**: The shuffled numpy arrays are converted to Python lists using the `tolist()` method before returning.\n\nThis function will shuffle the data in a reproducible manner if a seed is provided, and it will return the shuffled data as lists."}
{"task_id": 30, "completion_id": 0, "completion": "To implement a batch iterator function that samples from a numpy array `X` and an optional numpy array `y`, we can use the following approach. The function will yield batches of a specified size, and if `y` is provided, it will yield batches of `(X, y)` pairs; otherwise, it will yield batches of `X` only. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A generator function that yields batches of data from X and optionally y.\n\n    Parameters:\n    X (np.ndarray): The input data array.\n    y (np.ndarray, optional): The target data array. Default is None.\n    batch_size (int): The size of each batch. Default is 64.\n\n    Yields:\n    list: A list of batches of X, and optionally y, converted to Python lists.\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)  # Shuffle indices to ensure random sampling\n\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        X_batch = X[batch_indices]\n        if y is not None:\n            y_batch = y[batch_indices]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\n\n# Example usage:\n# X = np.random.rand(1000, 10)  # Example feature data\n# y = np.random.randint(0, 2, size=(1000,))  # Example target data\n# for X_batch, y_batch in batch_iterator(X, y, batch_size=128):\n#     print(X_batch, y_batch)\n```\n\n### Explanation:\n1. **Parameters**:\n   - `X`: The input data array.\n   - `y`: The optional target data array.\n   - `batch_size`: The size of each batch.\n\n2. **Shuffling**:\n   - We shuffle the indices to ensure that the batches are randomly sampled.\n\n3. **Batching**:\n   - We iterate over the indices in steps of `batch_size`.\n   - For each batch, we select the corresponding indices and slice `X` (and `y` if provided) to get the batch data.\n\n4. **Yielding**:\n   - We convert the numpy arrays to Python lists using the `tolist()` method before yielding them.\n\nThis function is flexible and can be used for both supervised learning (when `y` is provided) and unsupervised learning (when `y` is not provided)."}
{"task_id": 31, "completion_id": 0, "completion": "To accomplish the task of dividing a dataset based on a feature threshold, we can use NumPy for efficient array operations. Below is the implementation of the function `divide_on_feature`:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset into two subsets based on a feature threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset, where each row is a sample and each column is a feature.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value to divide the dataset.\n\n    Returns:\n    tuple: Two lists containing the divided subsets of the dataset.\n    \"\"\"\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Create a boolean mask for samples that meet the condition\n    mask = X[:, feature_i] >= threshold\n    \n    # Divide the dataset into two subsets\n    subset_meet_condition = X[mask]\n    subset_not_meet_condition = X[~mask]\n    \n    # Convert the subsets to lists using tolist()\n    return subset_meet_condition.tolist(), subset_not_meet_condition.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# feature_i = 0\n# threshold = 4\n# subset_meet, subset_not_meet = divide_on_feature(X, feature_i, threshold)\n# print(\"Subset that meets the condition:\", subset_meet)\n# print(\"Subset that does not meet the condition:\", subset_not_meet)\n```\n\n### Explanation:\n- **Parameters**:\n  - `X`: A NumPy array representing the dataset, where each row is a sample and each column is a feature.\n  - `feature_i`: The index of the feature used for division.\n  - `threshold`: The threshold value for dividing the dataset.\n\n- **Process**:\n  - Convert the input dataset `X` to a NumPy array to ensure compatibility with NumPy operations.\n  - Create a boolean mask that identifies samples where the specified feature's value is greater than or equal to the threshold.\n  - Use this mask to divide the dataset into two subsets: one that meets the condition and one that does not.\n  - Convert the resulting subsets to lists using the `tolist()` method to meet the output constraints.\n\nThis function efficiently divides the dataset and returns the subsets as lists, making it suitable for further processing or analysis."}
{"task_id": 32, "completion_id": 0, "completion": "To generate polynomial features for a given dataset, we can use combinations with replacement to create all possible polynomial combinations of the features up to the specified degree. Here's how you can implement the `polynomial_features` function:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Get the number of samples and features\n    n_samples, n_features = X.shape\n    \n    # Generate all combinations of feature indices with replacement\n    combinations = list(combinations_with_replacement(range(n_features), degree))\n    \n    # Initialize the output array with the correct shape\n    poly_features = np.empty((n_samples, len(combinations)))\n    \n    # Compute the polynomial features\n    for i, comb in enumerate(combinations):\n        poly_features[:, i] = np.prod(X[:, comb], axis=1)\n    \n    # Return the polynomial features as a list\n    return poly_features.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# degree = 2\n# print(polynomial_features(X, degree))\n```\n\n### Explanation:\n1. **Input Handling**: The function first ensures that the input `X` is a numpy array, which is necessary for efficient numerical operations.\n\n2. **Combinations Generation**: Using `combinations_with_replacement`, we generate all possible combinations of feature indices up to the specified degree. This allows us to create polynomial terms like \\(x_1^2\\), \\(x_1x_2\\), etc.\n\n3. **Polynomial Feature Calculation**: For each combination, we calculate the product of the corresponding features for each sample. This is done using `np.prod` along the specified axis.\n\n4. **Output Conversion**: Finally, the resulting polynomial features are converted to a Python list using the `tolist()` method, as required by the output constraints.\n\nThis function will generate polynomial features for any given 2D numpy array and degree, providing a flexible tool for feature engineering in machine learning tasks."}
{"task_id": 33, "completion_id": 0, "completion": "To accomplish the task of generating random subsets of a dataset, we can use numpy's random sampling capabilities. Here's a Python function that implements the required functionality:\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)  # Set the random seed for reproducibility\n    subsets = []\n    n_samples = X.shape[0]  # Number of samples in the dataset\n\n    for _ in range(n_subsets):\n        if replacements:\n            # Sample indices with replacement\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            # Sample indices without replacement\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        \n        # Create subsets using the sampled indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Append the subset as a tuple to the list\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y = np.array([0, 1, 0, 1])\n# subsets = get_random_subsets(X, y, n_subsets=3, replacements=True)\n# print(subsets)\n```\n\n### Explanation:\n- **Seed**: We set a seed for the random number generator to ensure reproducibility of the results.\n- **Sampling**: Depending on the `replacements` parameter, we either sample with or without replacement using `np.random.choice`.\n- **Subsets**: For each subset, we sample indices and use them to create subsets of `X` and `y`.\n- **Return**: The subsets are returned as a list of tuples, where each tuple contains the subset of `X` and `y` converted to lists using the `tolist()` method."}
{"task_id": 34, "completion_id": 0, "completion": "To perform one-hot encoding of nominal values in a 1D numpy array, we can create a function that constructs a binary matrix where each row corresponds to an element in the input array, and each column corresponds to a possible category. If `n_col` is not provided, it should be determined from the maximum value in the input array. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Initialize a zero matrix of shape (len(x), n_col)\n    one_hot = np.zeros((len(x), n_col), dtype=int)\n    \n    # Set the appropriate elements to 1\n    one_hot[np.arange(len(x)), x] = 1\n    \n    # Convert the numpy array to a list and return\n    return one_hot.tolist()\n\n# Example usage:\n# x = np.array([0, 1, 2, 1])\n# print(to_categorical(x))\n```\n\n### Explanation:\n1. **Determine `n_col`:** If `n_col` is not provided, it is set to the maximum value in `x` plus one. This ensures that there is a column for each possible category in `x`.\n\n2. **Initialize the One-Hot Matrix:** A zero matrix of shape `(len(x), n_col)` is created. This matrix will hold the one-hot encoded values.\n\n3. **Set the Appropriate Elements to 1:** Using advanced indexing, we set the element at position `(i, x[i])` to 1 for each index `i` in the input array `x`.\n\n4. **Convert to List:** Finally, the numpy array is converted to a Python list using the `tolist()` method and returned.\n\nThis function will correctly one-hot encode the input array and return the result as a list of lists."}
{"task_id": 35, "completion_id": 0, "completion": "To convert a 1D numpy array into a diagonal matrix, you can use the `np.diag` function, which creates a diagonal matrix from a given array. After creating the diagonal matrix, you can convert it to a Python list using the `tolist()` method. Here's how you can implement the `make_diagonal` function:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D numpy array\n    diagonal_matrix = np.diag(x)\n    \n    # Convert the numpy array to a Python list\n    diagonal_list = diagonal_matrix.tolist()\n    \n    return diagonal_list\n```\n\n### Explanation:\n1. **`np.diag(x)`**: This function takes a 1D array `x` and returns a 2D array with the elements of `x` on the diagonal and zeros elsewhere.\n2. **`tolist()`**: This method converts the numpy array into a nested Python list, which is the required output format.\n\nYou can use this function by passing a 1D numpy array to it, and it will return the corresponding diagonal matrix as a list. Here's an example of how to use the function:\n\n```python\n# Example usage\nx = np.array([1, 2, 3])\ndiagonal_list = make_diagonal(x)\nprint(diagonal_list)\n```\n\nThis will output:\n\n```\n[[1, 0, 0], \n [0, 2, 0], \n [0, 0, 3]]\n```\n\nThis output is a list of lists, where each inner list represents a row of the diagonal matrix."}
{"task_id": 36, "completion_id": 0, "completion": "To calculate the accuracy score, you need to determine the proportion of correctly predicted labels out of the total number of labels. Here's how you can implement the function:\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    # Ensure that y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    \n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n    \n    # Calculate the accuracy score\n    accuracy = correct_predictions / total_predictions\n    \n    # Round the result to the nearest 4th decimal\n    return round(accuracy, 4)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0])\n# y_pred = np.array([1, 0, 0, 1, 0])\n# print(accuracy_score(y_true, y_pred))  # Output: 0.8\n```\n\nThis function first ensures that the inputs are numpy arrays, then calculates the number of correct predictions by comparing `y_true` and `y_pred`. It divides the number of correct predictions by the total number of predictions to get the accuracy score, and finally rounds the result to four decimal places before returning it."}
{"task_id": 37, "completion_id": 0, "completion": "To calculate the correlation matrix for a given dataset using numpy, you can follow the steps below. The function will handle both cases: when only one dataset is provided (X) and when two datasets are provided (X and Y). The correlation matrix will be rounded to the nearest fourth decimal and returned as a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # If Y is not provided, calculate the correlation matrix of X with itself\n    if Y is None:\n        Y = X\n    \n    # Ensure Y is a numpy array\n    Y = np.array(Y)\n    \n    # Calculate the covariance matrix\n    covariance_matrix = np.cov(X, Y, rowvar=False)\n    \n    # Extract the standard deviations of X and Y\n    std_X = np.std(X, axis=0, ddof=1)\n    std_Y = np.std(Y, axis=0, ddof=1)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = covariance_matrix / np.outer(std_X, std_Y)\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Return the correlation matrix as a Python list\n    return correlation_matrix.tolist()\n\n# Example usage:\n# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n# print(calculate_correlation_matrix(X))\n# print(calculate_correlation_matrix(X, Y))\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts two parameters, `X` and `Y`. If `Y` is not provided, it defaults to `None`, and the function calculates the correlation matrix of `X` with itself.\n\n2. **Covariance Matrix Calculation**: The covariance matrix is calculated using `np.cov` with `rowvar=False` to treat columns as variables.\n\n3. **Standard Deviations**: The standard deviations of the columns in `X` and `Y` are calculated using `np.std` with `ddof=1` to ensure sample standard deviation.\n\n4. **Correlation Matrix Calculation**: The correlation matrix is derived by normalizing the covariance matrix with the outer product of the standard deviations.\n\n5. **Rounding**: The correlation matrix is rounded to the nearest fourth decimal using `np.round`.\n\n6. **Output**: The correlation matrix is converted to a Python list using the `tolist()` method before returning.\n\nThis function should work efficiently for calculating the correlation matrix for any 2D numpy array input."}
{"task_id": 38, "completion_id": 0, "completion": "To implement the `adaboost_fit` function, we need to follow the AdaBoost algorithm steps. We'll use decision stumps (one-level decision trees) as weak classifiers. Here's how you can implement the function:\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    \n    # Initialize weights\n    w = np.ones(n_samples) / n_samples\n    \n    # List to store classifiers and their parameters\n    classifiers = []\n    \n    for _ in range(n_clf):\n        # Initialize variables to find the best classifier\n        best_clf = None\n        best_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        \n        # Iterate over each feature\n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            possible_thresholds = np.unique(feature_values)\n            \n            # Try each threshold\n            for threshold in possible_thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions\n                    predictions = np.ones(n_samples)\n                    if polarity == 1:\n                        predictions[feature_values < threshold] = -1\n                    else:\n                        predictions[feature_values > threshold] = -1\n                    \n                    # Calculate error\n                    error = np.sum(w[y != predictions])\n                    \n                    # Update best classifier if error is lower\n                    if error < best_error:\n                        best_error = error\n                        best_clf = predictions\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_polarity = polarity\n        \n        # Calculate alpha (classifier weight)\n        alpha = 0.5 * math.log((1.0 - best_error) / (best_error + 1e-10))\n        \n        # Update weights\n        w *= np.exp(-alpha * y * best_clf)\n        w /= np.sum(w)\n        \n        # Store the classifier parameters\n        classifiers.append({\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'alpha': round(alpha, 4)\n        })\n    \n    return classifiers\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 1, -1, -1])\n# n_clf = 3\n# print(adaboost_fit(X, y, n_clf))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the sample weights uniformly.\n2. **Classifier Selection**: For each classifier, we iterate over each feature and possible thresholds to find the best decision stump (weak classifier) that minimizes the weighted error.\n3. **Error Calculation**: The error is calculated as the sum of weights of misclassified samples.\n4. **Weight Update**: We update the sample weights based on the error and the classifier's performance.\n5. **Classifier Storage**: We store the parameters of each classifier, including the feature, threshold, polarity, and alpha (weight of the classifier).\n\nThis implementation rounds the alpha values to the nearest 4th decimal as required."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the log-softmax function, we need to follow these steps:\n\n1. Subtract the maximum score from each score for numerical stability.\n2. Compute the exponential of each adjusted score.\n3. Compute the sum of these exponentials.\n4. Compute the log-softmax by subtracting the log of the sum of exponentials from each adjusted score.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Subtract the maximum score for numerical stability\n    max_score = np.max(scores_array)\n    adjusted_scores = scores_array - max_score\n    \n    # Compute the exponential of each adjusted score\n    exp_scores = np.exp(adjusted_scores)\n    \n    # Compute the sum of the exponentials\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Compute the log-softmax\n    log_softmax_scores = adjusted_scores - np.log(sum_exp_scores)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_log_softmax_scores = np.round(log_softmax_scores, 4)\n    \n    # Convert the numpy array to a list and return\n    return rounded_log_softmax_scores.tolist()\n\n# Example usage:\nscores = [3.0, 1.0, 0.2]\nprint(log_softmax(scores))\n```\n\nThis function will take a list of scores, compute the log-softmax, round the results to the nearest 4th decimal, and return them as a Python list."}
{"task_id": 40, "completion_id": 0, "completion": "To implement the `Dense` class, we need to define the methods for weight initialization, parameter counting, forward pass, backward pass, and output shape. Here's how you can complete the class:\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self, optimizer):\n        # Initialize weights using a uniform distribution\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        \n        # Initialize optimizers for weights and biases\n        self.W_optimizer = copy.deepcopy(optimizer)\n        self.w0_optimizer = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        # Return the total number of trainable parameters\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        # Compute the output of the layer\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        # Calculate the gradient with respect to the input\n        grad_input = np.dot(accum_grad, self.W.T)\n        \n        # If the layer is trainable, update weights and biases\n        if self.trainable:\n            grad_W = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            \n            # Update weights and biases using the optimizer\n            self.W = self.W_optimizer.update(self.W, grad_W)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_w0)\n        \n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        # Return the shape of the output produced by the forward pass\n        return (self.n_units,)\n```\n\n### Explanation:\n\n1. **Weight Initialization (`initialize`)**:\n   - We initialize the weights `W` using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`.\n   - Bias `w0` is initialized to zero.\n   - We also set up optimizers for `W` and `w0` using deep copies of the provided optimizer.\n\n2. **Parameter Count (`parameters`)**:\n   - We calculate the total number of trainable parameters by summing the number of elements in `W` and `w0`.\n\n3. **Forward Pass (`forward_pass`)**:\n   - We compute the output by performing a dot product between the input `X` and the weight matrix `W`, then add the bias `w0`.\n   - The result is rounded to the nearest 4th decimal and converted to a list.\n\n4. **Backward Pass (`backward_pass`)**:\n   - We calculate the gradient with respect to the input using the dot product of the accumulated gradient and the transpose of `W`.\n   - If the layer is trainable, we compute gradients for `W` and `w0` and update them using the optimizer.\n   - The result is rounded to the nearest 4th decimal and converted to a list.\n\n5. **Output Shape (`output_shape`)**:\n   - We return the shape of the output, which is `(self.n_units,)`.\n\nThis implementation ensures that the `Dense` class functions correctly within a neural network framework, adhering to the specified constraints."}
{"task_id": 41, "completion_id": 0, "completion": "To implement a simple 2D convolutional layer, we need to consider the input matrix, the kernel, padding, and stride. The convolution operation involves sliding the kernel over the input matrix, applying the specified padding and stride, and computing the dot product at each position. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Add padding to the input matrix\n    if padding > 0:\n        input_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    \n    # Get dimensions of the input matrix and kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate the dimensions of the output matrix\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    \n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            # Determine the region of the input matrix to apply the kernel\n            start_i = i * stride\n            start_j = j * stride\n            end_i = start_i + kernel_height\n            end_j = start_j + kernel_width\n            \n            # Extract the region and apply the kernel\n            input_region = input_matrix[start_i:end_i, start_j:end_j]\n            conv_value = np.sum(input_region * kernel)\n            \n            # Round the result to the nearest 4th decimal and assign to the output matrix\n            output_matrix[i, j] = round(conv_value, 4)\n    \n    # Convert the output matrix to a list and return\n    return output_matrix.tolist()\n\n# Example usage:\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 1\nstride = 1\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)\n```\n\n### Explanation:\n1. **Padding**: We use `np.pad` to add zero padding around the input matrix. The amount of padding is specified by the `padding` parameter.\n\n2. **Output Dimensions**: The dimensions of the output matrix are calculated based on the input dimensions, kernel size, and stride.\n\n3. **Convolution Operation**: We slide the kernel over the input matrix, compute the dot product for each position, and store the result in the output matrix. The result is rounded to the nearest 4th decimal.\n\n4. **Return**: The output matrix is converted to a list using `tolist()` and returned."}
{"task_id": 42, "completion_id": 0, "completion": "To implement the ReLU (Rectified Linear Unit) activation function in Python, you can follow the given task description. The ReLU function is straightforward: it returns the input value if it is greater than 0, otherwise, it returns 0. Here's how you can implement it:\n\n```python\ndef relu(z: float) -> float:\n    return max(0, z)\n```\n\n### Explanation:\n- The function `relu` takes a single argument `z` of type `float`.\n- It uses the `max` function to return the greater of two values: `0` and `z`.\n- If `z` is greater than 0, `max` will return `z`.\n- If `z` is less than or equal to 0, `max` will return 0.\n\nThis implementation efficiently captures the behavior of the ReLU activation function."}
{"task_id": 43, "completion_id": 0, "completion": "To implement the Ridge Regression loss function, we need to calculate the Mean Squared Error (MSE) between the predicted values and the true labels, and add a regularization term that penalizes large coefficients. The regularization term is scaled by the parameter `alpha`.\n\nHere's the implementation of the `ridge_loss` function:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate predictions\n    y_pred = X.dot(w)\n    \n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate regularization term (L2 norm of the coefficients)\n    regularization_term = alpha * np.sum(w ** 2)\n    \n    # Calculate Ridge loss\n    ridge_loss_value = mse + regularization_term\n    \n    # Return the result rounded to the nearest 4th decimal\n    return round(ridge_loss_value, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# w = np.array([0.5, -0.5])\n# y_true = np.array([1, 2])\n# alpha = 0.1\n# print(ridge_loss(X, w, y_true, alpha))\n```\n\n### Explanation:\n1. **Predictions**: We calculate the predicted values `y_pred` by performing a dot product between the feature matrix `X` and the coefficients `w`.\n2. **Mean Squared Error (MSE)**: We compute the MSE by taking the mean of the squared differences between the true labels `y_true` and the predicted values `y_pred`.\n3. **Regularization Term**: The regularization term is calculated as the L2 norm (sum of squares) of the coefficients `w`, scaled by the regularization parameter `alpha`.\n4. **Ridge Loss**: The Ridge loss is the sum of the MSE and the regularization term.\n5. **Rounding**: The final result is rounded to the nearest 4th decimal before returning.\n\nThis function provides a straightforward implementation of the Ridge Regression loss, combining both the error term and the regularization term to prevent overfitting."}
{"task_id": 44, "completion_id": 0, "completion": "Certainly! The Leaky ReLU activation function is a variant of the ReLU function that allows a small, non-zero gradient when the input is negative. This can help mitigate issues like the \"dying ReLU\" problem. Below is the implementation of the `leaky_relu` function:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Apply the Leaky ReLU activation function to the input.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z\n```\n\n### Explanation:\n- **Parameters**:\n  - `z`: The input value to the activation function.\n  - `alpha`: The slope for negative inputs, with a default value of 0.01.\n  \n- **Return Value**:\n  - The function returns `z` if `z` is greater than 0, otherwise it returns `alpha * z`. This ensures that there is a small gradient when `z` is negative, controlled by the `alpha` parameter.\n\nThis function is straightforward and efficiently handles the Leaky ReLU activation for any given input."}
{"task_id": 45, "completion_id": 0, "completion": "To implement the linear kernel function, we need to compute the dot product of the two input vectors `x1` and `x2`. The dot product is a fundamental operation in linear algebra, and it can be efficiently computed using NumPy's `dot` function. Here's how you can implement the `kernel_function`:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    # Ensure the inputs are numpy arrays\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    \n    # Compute the dot product\n    return np.dot(x1, x2)\n```\n\n### Explanation:\n- **Importing NumPy**: We import NumPy as it provides efficient array operations and the `dot` function for computing the dot product.\n- **Function Definition**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are expected to be array-like objects (lists, tuples, or NumPy arrays).\n- **Convert Inputs to NumPy Arrays**: We convert `x1` and `x2` to NumPy arrays to ensure compatibility with NumPy operations.\n- **Dot Product Calculation**: We use `np.dot(x1, x2)` to compute the dot product, which is the linear kernel between the two vectors.\n\nThis function will return a scalar value representing the dot product of the two input vectors. Make sure that the input vectors are of the same length to avoid errors during the dot product computation."}
{"task_id": 46, "completion_id": 0, "completion": "Certainly! To implement the precision metric, we need to identify the true positives and false positives from the given `y_true` and `y_pred` arrays. Here's how you can implement the `precision` function:\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (TP): cases where both y_true and y_pred are 1\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false positives (FP): cases where y_true is 0 but y_pred is 1\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision: TP / (TP + FP)\n    if true_positives + false_positives == 0:\n        return 0.0  # Avoid division by zero; precision is undefined in this case\n    \n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1, 0])\n# y_pred = np.array([1, 0, 0, 1, 0, 1, 1])\n# print(precision(y_true, y_pred))  # Output: 0.75\n```\n\n### Explanation:\n- **True Positives (TP):** These are the instances where both the true label (`y_true`) and the predicted label (`y_pred`) are 1.\n- **False Positives (FP):** These are the instances where the true label is 0, but the predicted label is 1.\n- **Precision Calculation:** Precision is calculated as the ratio of true positives to the sum of true positives and false positives. If there are no positive predictions (i.e., `TP + FP` is zero), precision is set to 0.0 to avoid division by zero.\n\nThis function will correctly compute the precision metric for binary classification tasks using numpy arrays."}
{"task_id": 47, "completion_id": 0, "completion": "To implement the three variants of gradient descent using Mean Squared Error (MSE) as the loss function, we need to iterate over the data, compute the gradients, and update the weights accordingly. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef compute_mse_loss(X, y, weights):\n    predictions = X.dot(weights)\n    errors = predictions - y\n    mse = np.mean(errors ** 2)\n    return mse\n\ndef compute_gradient(X, y, weights):\n    predictions = X.dot(weights)\n    errors = predictions - y\n    gradient = 2 * X.T.dot(errors) / len(y)\n    return gradient\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    n_samples, n_features = X.shape\n    \n    for iteration in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            gradient = compute_gradient(X, y, weights)\n            weights -= learning_rate * gradient\n        \n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            for i in range(n_samples):\n                xi = X[i:i+1]\n                yi = y[i:i+1]\n                gradient = compute_gradient(xi, yi, weights)\n                weights -= learning_rate * gradient\n        \n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            \n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                X_mini = X_shuffled[start:end]\n                y_mini = y_shuffled[start:end]\n                gradient = compute_gradient(X_mini, y_mini, weights)\n                weights -= learning_rate * gradient\n        \n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n        \n        # Optionally, you can print the loss to monitor convergence\n        # loss = compute_mse_loss(X, y, weights)\n        # print(f\"Iteration {iteration+1}/{n_iterations}, Loss: {loss:.4f}\")\n    \n    # Round the weights to the nearest 4th decimal\n    weights = np.round(weights, 4)\n    \n    # Return the weights as a list\n    return weights.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# weights = np.array([0.1, 0.2])\n# learning_rate = 0.01\n# n_iterations = 1000\n# batch_size = 2\n# method = 'mini-batch'\n# result = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method)\n# print(result)\n```\n\n### Explanation:\n- **Batch Gradient Descent**: Computes the gradient using the entire dataset.\n- **Stochastic Gradient Descent**: Computes the gradient using one sample at a time.\n- **Mini-Batch Gradient Descent**: Computes the gradient using a subset of the dataset (mini-batch).\n\nThe function `compute_gradient` calculates the gradient of the MSE loss with respect to the weights. The weights are updated in each iteration based on the specified method. The final weights are rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 48, "completion_id": 0, "completion": "To implement the Reduced Row Echelon Form (RREF) of a matrix, we need to perform a series of row operations to transform the matrix into a specific form. The RREF of a matrix has the following properties:\n\n1. Each leading entry in a row is 1.\n2. Each leading 1 is the only non-zero entry in its column.\n3. The leading 1 in each row is to the right of the leading 1 in the previous row.\n4. Rows with all zero elements, if any, are at the bottom of the matrix.\n\nHere's a Python function that uses NumPy to convert a given matrix into its RREF:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)  # Convert the input to a NumPy array of floats\n    rows, cols = A.shape\n    lead = 0\n\n    for r in range(rows):\n        if lead >= cols:\n            return A.tolist()\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return A.tolist()\n        # Swap rows i and r\n        A[[i, r]] = A[[r, i]]\n        # Normalize row r\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        # Make all other rows 0 in current column\n        for i in range(rows):\n            if i != r:\n                lv = A[i, lead]\n                A[i] = A[i] - lv * A[r]\n        lead += 1\n\n    return A.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, 1, -1],\n    [3, 6, 2, 0],\n    [2, 4, 1, 1]\n]\n\nrref_matrix = rref(matrix)\nprint(rref_matrix)\n```\n\n### Explanation:\n\n1. **Initialization**: We start by converting the input matrix into a NumPy array of floats to handle division and row operations easily.\n\n2. **Row Operations**:\n   - We iterate over each row and try to find a leading 1.\n   - If the current row's leading element is zero, we search for a row below it with a non-zero element in the same column and swap them.\n   - Once a non-zero leading element is found, we normalize the row by dividing it by the leading element to make it 1.\n   - We then eliminate all other entries in the column by subtracting appropriate multiples of the current row from the other rows.\n\n3. **Return**: Finally, we convert the NumPy array back to a Python list using the `tolist()` method and return it.\n\nThis function should handle matrices of any size and convert them to their RREF form, adhering to the constraints and properties of RREF."}
{"task_id": 49, "completion_id": 0, "completion": "To implement the Adam optimization algorithm, we need to maintain and update the first and second moment estimates of the gradients. Here's how you can implement the `adam_optimizer` function:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)  # First moment vector\n    v = np.zeros_like(x)  # Second moment vector\n    t = 0  # Time step\n\n    for _ in range(num_iterations):\n        t += 1\n        g = np.array(grad(x), dtype=np.float64)  # Compute gradient\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n\n        # Update parameters\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return np.sum(x ** 2)\n\ndef grad(x):\n    return 2 * x\n\n# Initial parameters\nx0 = [1.0, 2.0, 3.0]\n\n# Run the optimizer\noptimized_params = adam_optimizer(f, grad, x0)\nprint(optimized_params)\n```\n\n### Explanation:\n- **Initialization**: We start by initializing the parameters `x`, the first moment vector `m`, and the second moment vector `v`.\n- **Gradient Calculation**: For each iteration, we compute the gradient `g` using the provided gradient function `grad`.\n- **Moment Updates**: We update the first and second moment estimates using the exponential decay rates `beta1` and `beta2`.\n- **Bias Correction**: We apply bias correction to the moment estimates to account for their initialization at zero.\n- **Parameter Update**: We update the parameters using the corrected moment estimates, the learning rate, and the small constant `epsilon` for numerical stability.\n- **Return**: After completing the specified number of iterations, we round the optimized parameters to the nearest 4th decimal and return them as a list.\n\nThis implementation should work for optimizing any differentiable function given its gradient."}
{"task_id": 50, "completion_id": 0, "completion": "To implement Lasso Regression using Gradient Descent, we need to incorporate the L1 regularization term into the gradient descent update rules. The L1 penalty will help in feature selection by driving some of the coefficients to zero. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    \n    for iteration in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        error = y_pred - y\n        \n        # Compute gradients\n        dw = (1/n_samples) * np.dot(X.T, error) + alpha * np.sign(w)\n        db = (1/n_samples) * np.sum(error)\n        \n        # Update weights and bias\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        \n        # Check for convergence\n        if np.linalg.norm(dw, ord=1) < tol and abs(db) < tol:\n            break\n    \n    # Round the results to the nearest 4th decimal\n    w = np.round(w, 4)\n    b = round(b, 4)\n    \n    return w.tolist(), b\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# weights, bias = l1_regularization_gradient_descent(X, y)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the weights `w` to zeros and the bias `b` to zero.\n\n2. **Gradient Calculation**:\n   - The gradient of the loss with respect to the weights `w` includes the L1 penalty term, which is `alpha * np.sign(w)`. This term is responsible for the L1 regularization.\n   - The gradient with respect to the bias `b` is simply the mean of the errors.\n\n3. **Update Rules**:\n   - We update the weights and bias using the calculated gradients and the learning rate.\n\n4. **Convergence Check**:\n   - We check for convergence by examining if the L1 norm of the weight gradient and the absolute value of the bias gradient are below a specified tolerance `tol`.\n\n5. **Rounding**:\n   - Finally, we round the weights and bias to the nearest 4th decimal as required.\n\nThis implementation will iteratively update the weights and bias, applying the L1 penalty to encourage sparsity in the model coefficients."}
{"task_id": 51, "completion_id": 0, "completion": "To solve the problem of calculating the Optimal String Alignment (OSA) distance between two strings, we can use dynamic programming. The OSA distance is similar to the Levenshtein distance but includes an additional operation: transposing two adjacent characters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Get the lengths of the source and target strings\n    len_source = len(source)\n    len_target = len(target)\n    \n    # Create a 2D array to store the distances\n    # (len_source + 1) x (len_target + 1) because we include the empty prefix\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Initialize the base cases\n    for i in range(len_source + 1):\n        dp[i][0] = i  # Cost of deleting all characters from source\n    for j in range(len_target + 1):\n        dp[0][j] = j  # Cost of inserting all characters into source\n    \n    # Fill the dp table\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            \n            # Calculate the minimum cost considering all operations\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,      # Deletion\n                dp[i][j - 1] + 1,      # Insertion\n                dp[i - 1][j - 1] + cost # Substitution\n            )\n            \n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)  # Transposition\n    \n    # The OSA distance is the value in the bottom-right corner of the matrix\n    return dp[len_source][len_target]\n\n# Example usage\nprint(OSA(\"caper\", \"acer\"))  # Output: 2\n```\n\n### Explanation:\n1. **Initialization**: We initialize a 2D array `dp` where `dp[i][j]` represents the OSA distance between the first `i` characters of `source` and the first `j` characters of `target`.\n\n2. **Base Cases**: \n   - `dp[i][0]` is initialized to `i` because transforming the first `i` characters of `source` to an empty string requires `i` deletions.\n   - `dp[0][j]` is initialized to `j` because transforming an empty string to the first `j` characters of `target` requires `j` insertions.\n\n3. **Filling the Table**: For each pair of indices `(i, j)`, we calculate the minimum cost considering:\n   - Deletion: `dp[i-1][j] + 1`\n   - Insertion: `dp[i][j-1] + 1`\n   - Substitution: `dp[i-1][j-1] + cost` where `cost` is `0` if the characters are the same, otherwise `1`.\n\n4. **Transposition**: We check if a transposition is possible (i.e., the last two characters of the current substrings are swapped) and update the cost accordingly.\n\n5. **Result**: The OSA distance is found in `dp[len_source][len_target]`, which represents the minimum number of edits required to transform the entire `source` string into the `target` string."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP) and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    \n    recall_value = TP / denominator\n    \n    # Return recall rounded to three decimal places\n    return round(recall_value, 3)\n```\n\n### Explanation:\n1. **Conversion to Numpy Arrays**: The function starts by converting the input lists `y_true` and `y_pred` into numpy arrays. This allows for efficient element-wise operations.\n\n2. **True Positives (TP)**: This is calculated by counting the number of instances where both `y_true` and `y_pred` are 1.\n\n3. **False Negatives (FN)**: This is calculated by counting the number of instances where `y_true` is 1 but `y_pred` is 0.\n\n4. **Recall Calculation**: Recall is defined as the ratio of true positives to the sum of true positives and false negatives. If the denominator is zero (which means there are no actual positive cases), the function returns 0.0 to avoid division by zero.\n\n5. **Rounding**: The recall value is rounded to three decimal places before being returned."}
{"task_id": 53, "completion_id": 0, "completion": "To implement the self-attention mechanism, we need to compute the query, key, and value matrices, then calculate the attention scores, apply the softmax function, and finally compute the weighted sum of the values. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism.\n\n    Parameters:\n    X (numpy.ndarray): Input sequence matrix of shape (sequence_length, input_dim).\n    W_q (numpy.ndarray): Weight matrix for queries of shape (input_dim, attention_dim).\n    W_k (numpy.ndarray): Weight matrix for keys of shape (input_dim, attention_dim).\n    W_v (numpy.ndarray): Weight matrix for values of shape (input_dim, attention_dim).\n\n    Returns:\n    list: Self-attention output as a list rounded to the nearest 4th decimal.\n    \"\"\"\n    # Compute queries, keys, and values\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T)\n\n    # Scale attention scores\n    d_k = K.shape[-1]\n    attention_scores /= np.sqrt(d_k)\n\n    # Apply softmax to get attention weights\n    attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n\n    # Compute the weighted sum of values\n    attention_output = np.dot(attention_weights, V)\n\n    # Round results to the nearest 4th decimal\n    attention_output = np.round(attention_output, 4)\n\n    # Return as a list\n    return attention_output.tolist()\n\n# Example usage:\n# X = np.array([[1, 0], [0, 1]])\n# W_q = np.array([[1, 0], [0, 1]])\n# W_k = np.array([[1, 0], [0, 1]])\n# W_v = np.array([[1, 0], [0, 1]])\n# print(self_attention(X, W_q, W_k, W_v))\n```\n\n### Explanation:\n1. **Input Matrices**: `X` is the input sequence matrix, and `W_q`, `W_k`, `W_v` are the weight matrices for queries, keys, and values, respectively.\n\n2. **Query, Key, Value Computation**: We compute the query (`Q`), key (`K`), and value (`V`) matrices by multiplying the input matrix `X` with the respective weight matrices.\n\n3. **Attention Scores**: The attention scores are computed as the dot product of the query matrix `Q` and the transpose of the key matrix `K`.\n\n4. **Scaling**: The attention scores are scaled by the square root of the dimension of the keys (`d_k`) to stabilize gradients.\n\n5. **Softmax**: The softmax function is applied to the attention scores to obtain the attention weights, which are used to determine the importance of each part of the input sequence.\n\n6. **Weighted Sum**: The attention output is computed as the weighted sum of the value matrix `V` using the attention weights.\n\n7. **Rounding and Output**: The final attention output is rounded to the nearest 4th decimal and returned as a list.\n\nThis implementation provides a basic self-attention mechanism suitable for educational purposes and small-scale applications."}
{"task_id": 54, "completion_id": 0, "completion": "To implement a simple RNN cell, we need to iterate over the sequence of input vectors, updating the hidden state at each step using the provided weight matrices and bias vector. The tanh activation function will be used to compute the new hidden state. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier matrix operations\n    input_sequence = np.array(input_sequence)\n    h_t = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Iterate over each input vector in the sequence\n    for x_t in input_sequence:\n        # Compute the new hidden state\n        h_t = np.tanh(np.dot(x_t, Wx) + np.dot(h_t, Wh) + b)\n    \n    # Round the final hidden state to 4 decimal places\n    h_t_rounded = np.round(h_t, 4)\n    \n    # Convert the final hidden state to a list and return\n    return h_t_rounded.tolist()\n\n# Example usage:\n# input_sequence = [[0.1, 0.2], [0.3, 0.4]]\n# initial_hidden_state = [0.0, 0.0]\n# Wx = [[0.5, 0.6], [0.7, 0.8]]\n# Wh = [[0.9, 1.0], [1.1, 1.2]]\n# b = [0.1, 0.2]\n# final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n# print(final_hidden_state)\n```\n\n### Explanation:\n- **Input Conversion**: The input sequence, initial hidden state, weight matrices, and bias vector are converted to NumPy arrays to facilitate matrix operations.\n- **Iteration**: We iterate over each input vector in the sequence. For each input vector `x_t`, we compute the new hidden state `h_t` using the formula:\n  \\[\n  h_t = \\tanh(x_t \\cdot Wx + h_{t-1} \\cdot Wh + b)\n  \\]\n  where `x_t` is the current input vector, `h_{t-1}` is the previous hidden state, `Wx` is the input-to-hidden weight matrix, `Wh` is the hidden-to-hidden weight matrix, and `b` is the bias vector.\n- **Rounding**: The final hidden state is rounded to four decimal places.\n- **Output**: The final hidden state is converted back to a Python list using the `tolist()` method and returned."}
{"task_id": 55, "completion_id": 0, "completion": "To implement the `translate_object` function, we will use a 2D translation matrix to move each point in the list by the specified distances `tx` and `ty`. The translation matrix for 2D transformations is defined as:\n\n\\[\n\\begin{bmatrix}\n1 & 0 & tx \\\\\n0 & 1 & ty \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\n\nFor each point \\([x, y]\\), we can represent it in homogeneous coordinates as \\([x, y, 1]\\). The translation is applied by multiplying this point by the translation matrix, resulting in a new point \\([x', y', 1]\\), where:\n\n\\[\nx' = x + tx\n\\]\n\\[\ny' = y + ty\n\\]\n\nHere's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create the translation matrix\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Convert points to homogeneous coordinates\n    homogeneous_points = np.array([[x, y, 1] for x, y in points])\n    \n    # Apply the translation matrix to each point\n    translated_points = homogeneous_points @ translation_matrix.T\n    \n    # Convert back to 2D points and return as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)  # Output: [[3, 5], [5, 7], [7, 9]]\n```\n\n### Explanation:\n1. **Translation Matrix**: We define a 3x3 translation matrix that includes the translation distances `tx` and `ty`.\n2. **Homogeneous Coordinates**: Each point is converted to homogeneous coordinates by appending a `1` to the \\([x, y]\\) coordinates.\n3. **Matrix Multiplication**: We use the `@` operator to perform matrix multiplication between the homogeneous points and the transpose of the translation matrix.\n4. **Convert Back to 2D**: After applying the translation, we extract the first two columns of the result to get the translated 2D points.\n5. **Return as List**: Finally, we convert the NumPy array back to a Python list using the `tolist()` method."}
{"task_id": 56, "completion_id": 0, "completion": "To compute the Kullback-Leibler (KL) divergence between two normal distributions \\( P \\sim N(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\), we can use the formula for KL divergence between two normal distributions:\n\n\\[\nD_{KL}(P \\parallel Q) = \\log\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n\\]\n\nHere's the implementation of the function `kl_divergence_normal`:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    # Calculate the KL divergence using the formula\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n    return kl_div\n\n# Example usage:\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 2.0\n\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence: {kl_div}\")\n```\n\nThis function takes the means and standard deviations of the two normal distributions as input and returns the KL divergence as a floating-point number. The formula is implemented using basic arithmetic operations and the `np.log` function for the logarithm."}
{"task_id": 57, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gauss-Seidel method for solving a system of linear equations:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Ensure A is a numpy array\n    A = np.array(A, dtype=float)\n    # Ensure b is a numpy array\n    b = np.array(b, dtype=float)\n    \n    # Number of variables\n    num_vars = len(b)\n    \n    # Initialize x with zeros if x_ini is not provided\n    if x_ini is None:\n        x = np.zeros(num_vars)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform n iterations\n    for _ in range(n):\n        for i in range(num_vars):\n            # Calculate the sum for the current row\n            sum_ax = sum(A[i][j] * x[j] for j in range(num_vars) if j != i)\n            # Update the current variable using the most recent values\n            x[i] = (b[i] - sum_ax) / A[i][i]\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the result as a Python list\n    return x.tolist()\n\n# Example usage:\nA = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\nb = [4, 7, 3]\nn = 10\nx_ini = [0, 0, 0]\nresult = gauss_seidel(A, b, n, x_ini)\nprint(result)\n```\n\n### Explanation:\n- **Initialization**: If no initial guess is provided, the solution vector `x` is initialized to zeros.\n- **Iteration**: For each iteration, the solution vector `x` is updated using the most recent values. The update formula is derived from rearranging the equation for each variable.\n- **Rounding**: After completing the specified number of iterations, the solution vector is rounded to four decimal places.\n- **Return**: The final solution vector is returned as a Python list using `tolist()`.\n\nThis implementation should work efficiently for systems where the matrix `A` is diagonally dominant, which is a common requirement for the convergence of the Gauss-Seidel method."}
{"task_id": 58, "completion_id": 0, "completion": "To implement Gaussian Elimination with partial pivoting, we need to transform the matrix \\( A \\) into an upper triangular form and then perform backward substitution to find the solution vector \\( x \\). Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = len(b)\n    \n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Partial pivoting: find the row with the largest element in the current column\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        \n        # Swap the current row with the max_row\n        if i != max_row:\n            A[[i, max_row]] = A[[max_row, i]]\n            b[[i, max_row]] = b[[max_row, i]]\n        \n        # Eliminate the entries below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j] -= factor * b[i]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the solution as a Python list\n    return x.tolist()\n\n# Example usage:\nA = [[3, 2, -4], [2, 3, 3], [5, -3, 1]]\nb = [3, 15, 14]\nsolution = gaussian_elimination(A, b)\nprint(solution)\n```\n\n### Explanation:\n1. **Partial Pivoting**: This is done to improve numerical stability by swapping rows to ensure the largest possible pivot element is used.\n2. **Forward Elimination**: This step transforms the matrix into an upper triangular form.\n3. **Backward Substitution**: Once the matrix is in upper triangular form, we solve for the variables starting from the last row upwards.\n4. **Rounding**: The solution vector is rounded to the nearest 4th decimal place.\n5. **Return**: The solution vector is returned as a Python list using `tolist()`.\n\nThis implementation should work for any system of linear equations that can be solved using Gaussian Elimination."}
{"task_id": 59, "completion_id": 0, "completion": "To implement the LSTM network, we need to compute the forget gate, input gate, candidate cell state, and output gate at each time step. These gates will help us update the hidden state and cell state. Let's complete the `forward` method in the `LSTM` class.\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        hidden_states = []\n\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)\n            combined = np.vstack((h_t, x_t))\n\n            # Forget gate\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n\n            # Input gate\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n\n            # Candidate cell state\n            c_hat_t = self.tanh(np.dot(self.Wc, combined) + self.bc)\n\n            # Update cell state\n            c_t = f_t * c_t + i_t * c_hat_t\n\n            # Output gate\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n\n            # Update hidden state\n            h_t = o_t * self.tanh(c_t)\n\n            # Store hidden state\n            hidden_states.append(h_t)\n\n        # Convert hidden states to list and round to 4 decimal places\n        hidden_states = np.array(hidden_states).reshape(x.shape[0], -1).tolist()\n        hidden_states = [[round(val, 4) for val in state] for state in hidden_states]\n\n        # Final hidden state and cell state\n        final_hidden_state = h_t.reshape(-1).tolist()\n        final_hidden_state = [round(val, 4) for val in final_hidden_state]\n\n        final_cell_state = c_t.reshape(-1).tolist()\n        final_cell_state = [round(val, 4) for val in final_cell_state]\n\n        return hidden_states, final_hidden_state, final_cell_state\n```\n\n### Explanation:\n- **Weights and Biases**: We initialize weights and biases for the forget gate (`Wf`, `bf`), input gate (`Wi`, `bi`), candidate cell state (`Wc`, `bc`), and output gate (`Wo`, `bo`).\n- **Gates**: At each time step, we compute the forget gate (`f_t`), input gate (`i_t`), candidate cell state (`c_hat_t`), and output gate (`o_t`).\n- **Cell State Update**: The cell state (`c_t`) is updated using the forget gate and input gate.\n- **Hidden State Update**: The hidden state (`h_t`) is updated using the output gate and the updated cell state.\n- **Output**: We return the hidden states at each time step, the final hidden state, and the final cell state, all rounded to the nearest 4th decimal."}
{"task_id": 60, "completion_id": 0, "completion": "To implement the TF-IDF (Term Frequency-Inverse Document Frequency) computation, we need to follow these steps:\n\n1. **Term Frequency (TF)**: This measures how frequently a term appears in a document. It's calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n\n2. **Inverse Document Frequency (IDF)**: This measures how important a term is. While computing IDF, we need to account for terms that do not appear in any document to avoid division by zero. We can use smoothing by adding 1 to both the numerator and the denominator.\n\n3. **TF-IDF**: This is the product of TF and IDF, which gives us the importance of a term in a document relative to the corpus.\n\nLet's implement the `compute_tf_idf` function:\n\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n\n    # Calculate document frequency for each term in the query\n    doc_count = len(corpus)\n    df = {term: 0 for term in query}\n    \n    for document in corpus:\n        unique_terms = set(document)\n        for term in query:\n            if term in unique_terms:\n                df[term] += 1\n\n    # Calculate IDF with smoothing\n    idf = {term: np.log((doc_count + 1) / (df[term] + 1)) + 1 for term in query}\n\n    # Calculate TF-IDF for each document\n    tf_idf_scores = []\n    for document in corpus:\n        doc_length = len(document)\n        tf_idf_doc = []\n        for term in query:\n            tf = document.count(term) / doc_length if doc_length > 0 else 0\n            tf_idf = tf * idf[term]\n            tf_idf_doc.append(round(tf_idf, 4))\n        tf_idf_scores.append(tf_idf_doc)\n\n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"this\", \"is\", \"a\", \"sample\"],\n    [\"this\", \"is\", \"another\", \"example\", \"example\"],\n    [\"one\", \"more\", \"example\"]\n]\nquery = [\"this\", \"example\", \"sample\", \"not_in_corpus\"]\n\ntf_idf_result = compute_tf_idf(corpus, query)\nprint(tf_idf_result)\n```\n\n### Explanation:\n\n- **Document Frequency (DF)**: We count how many documents contain each term in the query.\n- **Inverse Document Frequency (IDF)**: We use the formula `idf = log((N + 1) / (df + 1)) + 1` to avoid division by zero, where `N` is the total number of documents.\n- **Term Frequency (TF)**: For each document, we calculate the frequency of each query term.\n- **TF-IDF Calculation**: For each document and each query term, we compute the TF-IDF score and round it to four decimal places.\n\nThis implementation handles edge cases such as empty documents, terms not present in the corpus, and an empty corpus."}
{"task_id": 61, "completion_id": 0, "completion": "To implement the F-Score calculation for a binary classification task, we need to compute precision and recall first, and then use these to calculate the F-Score. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        return 0.0\n    \n    f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n    \n    # Return the F-Score rounded to three decimal places\n    return round(f_score, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n# y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n# beta = 1.0\n# print(f_score(y_true, y_pred, beta))  # Output: 0.8\n```\n\n### Explanation:\n- **True Positives (TP):** The number of instances where the model correctly predicted the positive class.\n- **False Positives (FP):** The number of instances where the model incorrectly predicted the positive class.\n- **False Negatives (FN):** The number of instances where the model failed to predict the positive class.\n- **Precision:** The ratio of correctly predicted positive observations to the total predicted positives.\n- **Recall:** The ratio of correctly predicted positive observations to all the actual positives.\n- **F-Score:** A weighted harmonic mean of precision and recall, where `beta` determines the weight of recall in the combined score. When `beta` is 1, it is the F1-Score, which balances precision and recall equally.\n\nThis function will handle cases where there are no positive predictions or no true positives by returning an F-Score of 0.0."}
{"task_id": 62, "completion_id": 0, "completion": "Certainly! Below is the implementation of a simple RNN with backpropagation through time (BPTT) for sequence prediction. The code includes the initialization of weights, the forward pass, and the backward pass.\n\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = [h_prev]\n\n        for t in range(len(x)):\n            x_t = x[t].reshape(-1, 1)\n            h_t = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_t)\n            h_prev = h_t\n\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        loss = 0\n        d_h_next = np.zeros_like(last_hiddens[0])\n\n        for t in reversed(range(len(input_sequence))):\n            y_t = outputs[t]\n            y_true = expected_output[t].reshape(-1, 1)\n            loss += 0.5 * np.sum((y_t - y_true) ** 2)\n\n            dy = y_t - y_true\n            dW_hy += np.dot(dy, last_hiddens[t + 1].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + d_h_next\n            dh_raw = (1 - last_hiddens[t + 1] ** 2) * dh\n\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t].T)\n\n            d_h_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n        return loss\n\n# Example usage:\ninput_size = 3\nhidden_size = 5\noutput_size = 1\nrnn = SimpleRNN(input_size, hidden_size, output_size)\n\n# Dummy data for demonstration\ninput_sequence = [np.random.randn(input_size) for _ in range(10)]\nexpected_output = [np.random.randn(output_size) for _ in range(10)]\n\n# Forward pass\noutputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n\n# Backward pass\nlearning_rate = 0.01\nloss = rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\nprint(f\"Loss: {loss}\")\n```\n\nThis code defines a simple RNN class with methods for forward and backward passes. The forward method processes the input sequence and computes the outputs and hidden states. The backward method performs backpropagation through time to update the weights and biases based on the loss calculated using the mean squared error."}
{"task_id": 63, "completion_id": 0, "completion": "Here's the implementation of the Conjugate Gradient method for solving linear systems:\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Initialize the solution vector\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n\n    # Initialize the residual\n    r = b - np.dot(A, x)\n    # Initialize the direction vector\n    p = r.copy()\n    # Initialize the residual norm\n    rs_old = np.dot(r, r)\n\n    for i in range(n):\n        # Calculate the matrix-vector product Ap\n        Ap = np.dot(A, p)\n        # Calculate the step size alpha\n        alpha = rs_old / np.dot(p, Ap)\n        # Update the solution vector\n        x += alpha * p\n        # Update the residual\n        r -= alpha * Ap\n        # Check for convergence\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        # Update the direction vector\n        p = r + (rs_new / rs_old) * p\n        # Update the residual norm\n        rs_old = rs_new\n\n    # Round the solution to 8 decimal places and convert to list\n    return np.round(x, 8).tolist()\n\n# Example usage:\n# A = np.array([[4, 1], [1, 3]])\n# b = np.array([1, 2])\n# n = 1000\n# x0 = np.array([0, 0])\n# solution = conjugate_gradient(A, b, n, x0)\n# print(solution)\n```\n\n### Explanation:\n- **Initialization**: The function initializes the solution vector `x` to a zero vector if no initial guess `x0` is provided. The residual `r` is initialized as `b - Ax`, and the direction vector `p` is initialized to `r`.\n- **Iteration**: The algorithm iteratively updates the solution vector `x`, the residual `r`, and the direction vector `p` using the Conjugate Gradient method formulas. The step size `alpha` is calculated to minimize the error in the current direction.\n- **Convergence Check**: The loop checks if the residual norm is below the specified tolerance `tol`, indicating convergence.\n- **Return**: The solution vector `x` is rounded to 8 decimal places and converted to a list before being returned.\n\nThis implementation efficiently solves large, sparse, symmetric, positive-definite linear systems using the Conjugate Gradient method."}
{"task_id": 64, "completion_id": 0, "completion": "To implement the Gini Impurity calculation, we need to determine the probability of each class in the list and use these probabilities to compute the impurity. The formula for Gini Impurity is:\n\n\\[ Gini = 1 - \\sum (p_i^2) \\]\n\nwhere \\( p_i \\) is the probability of class \\( i \\). Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each class\n    unique, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the probabilities of each class\n    probabilities = counts / counts.sum()\n    \n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage:\n# y = [1, 1, 2, 2, 3]\n# print(gini_impurity(y))  # Output: 0.64\n```\n\n### Explanation:\n1. **Counting Occurrences**: We use `np.unique` with `return_counts=True` to get the unique classes and their counts in the list `y`.\n2. **Calculating Probabilities**: The probabilities of each class are calculated by dividing the counts by the total number of elements.\n3. **Gini Impurity Calculation**: We compute the Gini Impurity using the formula \\( 1 - \\sum (p_i^2) \\).\n4. **Rounding**: The result is rounded to three decimal places for precision.\n\nThis function will efficiently compute the Gini Impurity for any list of class labels provided."}
{"task_id": 65, "completion_id": 0, "completion": "To implement the conversion of a dense matrix into the Compressed Row Sparse (CSR) format, we need to extract the non-zero elements, their column indices, and the row pointers that indicate the start of each row in the values array. Here's how you can achieve this:\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # Start with 0 for the first row\n\n    for row in dense_matrix:\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        # Append the cumulative count of non-zero elements up to the current row\n        row_pointers.append(len(values))\n\n    return values, column_indices, row_pointers\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [22, 0, 0, 0],\n    [0, 0, 0, 0],\n    [0, 7, 0, 5]\n]\n\nvalues, column_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointers:\", row_pointers)\n```\n\n### Explanation:\n1. **Values Array**: This list stores all non-zero elements from the dense matrix in row-major order.\n2. **Column Indices Array**: This list stores the column index for each non-zero element found in the dense matrix.\n3. **Row Pointer Array**: This list starts with 0 and accumulates the count of non-zero elements up to each row, indicating where each row starts in the values array.\n\nThe example usage demonstrates how the function works with a sample dense matrix, converting it into the CSR format and printing the resulting arrays."}
{"task_id": 66, "completion_id": 0, "completion": "To implement the orthogonal projection of a vector `v` onto another vector `L`, we can use the formula for the projection of `v` onto `L`:\n\n\\[ \\text{proj}_L(v) = \\left( \\frac{v \\cdot L}{L \\cdot L} \\right) L \\]\n\nWhere:\n- \\( v \\cdot L \\) is the dot product of vectors `v` and `L`.\n- \\( L \\cdot L \\) is the dot product of vector `L` with itself.\n- The result is a scalar that scales the vector `L`.\n\nHere's the implementation in Python:\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_vL = sum(v_i * L_i for v_i, L_i in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_LL = sum(L_i * L_i for L_i in L)\n    \n    # Calculate the scalar for the projection\n    scalar = dot_vL / dot_LL\n    \n    # Calculate the projection vector\n    projection = [round(scalar * L_i, 3) for L_i in L]\n    \n    return projection\n\n# Example usage:\nv = [3, 4]\nL = [1, 2]\nprint(orthogonal_projection(v, L))  # Output: [2.2, 4.4]\n```\n\n### Explanation:\n1. **Dot Product Calculation**: We calculate the dot product of `v` and `L` (`dot_vL`) and the dot product of `L` with itself (`dot_LL`).\n2. **Scalar Calculation**: The scalar is computed as the ratio of these dot products.\n3. **Projection Calculation**: Each component of the projection vector is calculated by multiplying the scalar with the corresponding component of `L`, and then rounding to three decimal places.\n4. **Return**: The function returns the projection vector as a list."}
{"task_id": 67, "completion_id": 0, "completion": "To implement the Compressed Column Sparse (CSC) format, we need to traverse the dense matrix column by column, collecting non-zero values and their corresponding row indices, and marking the start of each column in the values list. Here's how you can implement this:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return [], [], []\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    column_pointer = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3],\n    [4, 0, 0],\n    [0, 5, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row indices:\", row_indices)\nprint(\"Column pointer:\", column_pointer)\n```\n\n### Explanation:\n- **Values**: This list stores all non-zero elements of the matrix in column-major order.\n- **Row indices**: This list stores the row index for each non-zero element in the `values` list.\n- **Column pointer**: This list indicates the starting index of each column in the `values` list. The last element in `column_pointer` is the total number of non-zero elements, which helps in determining the range of indices for the last column.\n\nThe function iterates over each column and within each column, it iterates over each row to find non-zero elements. It appends these elements to the `values` list and their row indices to the `row_indices` list. The `column_pointer` list is updated to mark the start of each new column in the `values` list."}
{"task_id": 68, "completion_id": 0, "completion": "To solve this problem, we need to find the column space (or image) of a given matrix \\( A \\). The column space is spanned by the linearly independent columns of the matrix. To identify these independent columns, we can use the row echelon form (REF) of the matrix.\n\nHere's a step-by-step approach to implement the `matrix_image` function:\n\n1. **Convert the Matrix to Row Echelon Form (REF):** Use Gaussian elimination to transform the matrix into REF. This will help us identify the pivot columns, which correspond to the linearly independent columns in the original matrix.\n\n2. **Identify Pivot Columns:** The pivot columns in the REF correspond to the independent columns in the original matrix.\n\n3. **Extract Basis Vectors:** Use the indices of the pivot columns to extract the corresponding columns from the original matrix. These columns form the basis for the column space.\n\n4. **Round and Convert to List:** Round the resulting basis vectors to 8 decimal places and convert them to a list format.\n\nHere's the implementation of the `matrix_image` function:\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert A to a numpy array\n    A = np.array(A, dtype=float)\n    \n    # Get the number of rows and columns\n    rows, cols = A.shape\n    \n    # Perform Gaussian elimination to get the row echelon form\n    row_echelon = A.copy()\n    pivot_columns = []\n    row = 0\n    \n    for col in range(cols):\n        # Find the pivot row for this column\n        pivot_row = None\n        for r in range(row, rows):\n            if row_echelon[r, col] != 0:\n                pivot_row = r\n                break\n        \n        if pivot_row is not None:\n            # Swap the current row with the pivot row\n            row_echelon[[row, pivot_row]] = row_echelon[[pivot_row, row]]\n            \n            # Record the pivot column\n            pivot_columns.append(col)\n            \n            # Normalize the pivot row\n            row_echelon[row] = row_echelon[row] / row_echelon[row, col]\n            \n            # Eliminate all other entries in this column\n            for r in range(rows):\n                if r != row:\n                    row_echelon[r] -= row_echelon[r, col] * row_echelon[row]\n            \n            # Move to the next row\n            row += 1\n    \n    # Extract the basis vectors from the original matrix using pivot columns\n    basis_vectors = A[:, pivot_columns]\n    \n    # Round to 8 decimal places and convert to list\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n    \n    return basis_vectors\n\n# Example usage:\nA = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nprint(matrix_image(A))\n```\n\n### Explanation:\n- **Gaussian Elimination:** We perform Gaussian elimination to transform the matrix into REF. This involves row swapping, scaling, and row addition to create zeros below the pivots.\n- **Pivot Columns:** We track the columns where pivots (leading 1s) appear in the REF. These columns in the original matrix are linearly independent.\n- **Basis Extraction:** We extract these columns from the original matrix to form the basis of the column space.\n- **Rounding and Conversion:** Finally, we round the basis vectors to 8 decimal places and convert them to a list format for the output.\n\nThis function will return the basis vectors that span the column space of the input matrix \\( A \\)."}
{"task_id": 69, "completion_id": 0, "completion": "To calculate the R-squared value, we need to follow these steps:\n\n1. Calculate the total sum of squares (TSS), which measures the total variance in the dependent variable:\n   \\[\n   \\text{TSS} = \\sum (y_{\\text{true}} - \\bar{y})^2\n   \\]\n   where \\(\\bar{y}\\) is the mean of the true values.\n\n2. Calculate the residual sum of squares (RSS), which measures the variance that is not explained by the model:\n   \\[\n   \\text{RSS} = \\sum (y_{\\text{true}} - y_{\\text{pred}})^2\n   \\]\n\n3. Calculate the R-squared value using the formula:\n   \\[\n   R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n   \\]\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Convert inputs to numpy arrays for easier mathematical operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    # Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\n# y_true = [3, -0.5, 2, 7]\n# y_pred = [2.5, 0.0, 2, 8]\n# print(r_squared(y_true, y_pred))  # Output: 0.948\n```\n\nThis function takes two lists or arrays, `y_true` and `y_pred`, and calculates the R-squared value, rounding it to three decimal places. The use of numpy ensures efficient computation, especially for larger datasets."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not isinstance(img, list) or not all(isinstance(row, list) for row in img):\n        return -1\n    \n    # Check for inconsistent row lengths\n    row_length = len(img[0])\n    if any(len(row) != row_length for row in img):\n        return -1\n    \n    total_brightness = 0\n    num_pixels = 0\n    \n    for row in img:\n        for pixel in row:\n            # Check if pixel values are within the valid range\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    \n    # Calculate average brightness\n    if num_pixels == 0:\n        return -1\n    \n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)\n\n# Example usage:\n# img = [\n#     [0, 255, 128],\n#     [64, 32, 16]\n# ]\n# print(calculate_brightness(img))  # Output: 82.5\n```\n\nThis function first checks for edge cases such as an empty matrix, inconsistent row lengths, and invalid pixel values. If any of these conditions are met, it returns `-1`. Otherwise, it calculates the average brightness by summing all pixel values and dividing by the total number of pixels, rounding the result to two decimal places."}
{"task_id": 71, "completion_id": 0, "completion": "To implement the `rmse` function, we need to ensure that it handles various edge cases and calculates the Root Mean Square Error correctly. Here's how you can implement this function:\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are valid numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Inputs must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate the RMSE\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    \n    # Return the RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(rmse(y_true, y_pred))  # Output: 0.612\n```\n\n### Explanation:\n\n1. **Type Checking**: The function first checks if both `y_true` and `y_pred` are instances of `np.ndarray`. If not, it raises a `TypeError`.\n\n2. **Empty Array Check**: It checks if either of the arrays is empty and raises a `ValueError` if so.\n\n3. **Shape Check**: It ensures that both arrays have the same shape. If they don't, it raises a `ValueError`.\n\n4. **RMSE Calculation**: \n   - It calculates the Mean Squared Error (MSE) by taking the mean of the squared differences between `y_true` and `y_pred`.\n   - It then takes the square root of the MSE to get the RMSE.\n\n5. **Rounding**: The RMSE value is rounded to three decimal places before being returned.\n\nThis implementation ensures that the function is robust and handles various edge cases effectively."}
{"task_id": 72, "completion_id": 0, "completion": "To implement the `jaccard_index` function, we need to calculate the intersection and union of the binary arrays `y_true` and `y_pred`. The intersection consists of elements that are 1 in both arrays, while the union consists of elements that are 1 in either array. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Convert inputs to numpy arrays if they aren't already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    \n    # Handle edge case where union is zero\n    if union == 0:\n        return 1.0 if np.all(y_true == 0) and np.all(y_pred == 0) else 0.0\n    \n    # Calculate Jaccard Index\n    jaccard_index_value = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard_index_value, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0]\n# y_pred = [1, 0, 0, 1, 1]\n# print(jaccard_index(y_true, y_pred))  # Output: 0.5\n```\n\n### Explanation:\n1. **Intersection Calculation**: We use a logical AND operation to find elements that are 1 in both `y_true` and `y_pred`, and then sum them up.\n2. **Union Calculation**: We use a logical OR operation to find elements that are 1 in either `y_true` or `y_pred`, and then sum them up.\n3. **Edge Case Handling**: If the union is zero, it means both arrays contain only zeros. In this case, the Jaccard Index is defined as 1.0 if both arrays are identical (all zeros), otherwise 0.0.\n4. **Return Value**: The Jaccard Index is rounded to three decimal places for precision.\n\nThis function efficiently calculates the Jaccard Index for binary classification tasks, handling edge cases appropriately."}
{"task_id": 73, "completion_id": 0, "completion": "To implement the `dice_score` function, we need to calculate the intersection between the true labels (`y_true`) and the predicted labels (`y_pred`). The Dice Score formula is applied to compute the similarity between these two binary arrays. Let's go through the implementation step-by-step:\n\n1. **Intersection Calculation**: The intersection is the number of elements where both `y_true` and `y_pred` are 1.\n\n2. **Sum Calculation**: We need the sum of elements in both `y_true` and `y_pred`.\n\n3. **Dice Score Calculation**: Use the formula provided to compute the Dice Score.\n\n4. **Edge Case Handling**: If both sums are zero (no positives in either array), the Dice Score should be handled to avoid division by zero.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Calculate intersection\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate sums\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Handle edge case where both sums are zero\n    if sum_true + sum_pred == 0:\n        return 1.0  # If there are no positives in both, consider it a perfect match\n    \n    # Calculate Dice Score\n    dice = (2 * intersection) / (sum_true + sum_pred)\n    \n    # Return rounded Dice Score\n    return round(dice, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0])\n# y_pred = np.array([0, 1, 1, 1, 0])\n# print(dice_score(y_true, y_pred))  # Output: 0.8\n```\n\n### Explanation:\n- **Intersection**: `np.sum(y_true * y_pred)` computes the number of elements where both arrays have a value of 1.\n- **Sum Calculation**: `np.sum(y_true)` and `np.sum(y_pred)` give the total number of positive elements in each array.\n- **Edge Case**: If both arrays have no positive elements, the Dice Score is set to 1.0, indicating a perfect match in the absence of positives.\n- **Dice Score**: The formula `(2 * intersection) / (sum_true + sum_pred)` calculates the Dice Score, which is then rounded to three decimal places.\n\nThis function should work correctly for binary classification tasks, including handling edge cases where there are no positive elements in either array."}
{"task_id": 74, "completion_id": 0, "completion": "To implement the function `create_row_hv`, we need to follow the principles of Hyperdimensional Computing (HDC). In HDC, we represent data using high-dimensional vectors, known as hypervectors. The process involves creating hypervectors for each feature and its value, binding them together, and then bundling all these bound hypervectors to form a composite hypervector for the entire row.\n\nHere's how we can implement this:\n\n```python\nimport numpy as np\n\ndef create_random_hv(dim, seed):\n    \"\"\"Generate a random hypervector of given dimensionality using a seed.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\n\ndef bind_hvs(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors using element-wise summation and thresholding.\"\"\"\n    sum_hv = np.sum(hvs, axis=0)\n    # Thresholding: convert positive sums to 1, negative sums to -1\n    return np.where(sum_hv >= 0, 1, -1)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Create a composite hypervector for a dataset row.\"\"\"\n    bound_hvs = []\n    \n    for feature, value in row.items():\n        # Create hypervector for the feature name\n        feature_hv = create_random_hv(dim, random_seeds[feature])\n        \n        # Create hypervector for the feature value using the same seed\n        value_hv = create_random_hv(dim, random_seeds[feature] + hash(value))\n        \n        # Bind feature and value hypervectors\n        bound_hv = bind_hvs(feature_hv, value_hv)\n        \n        # Collect bound hypervectors\n        bound_hvs.append(bound_hv)\n    \n    # Bundle all bound hypervectors to create the composite hypervector\n    composite_hv = bundle_hvs(bound_hvs)\n    \n    # Convert the composite hypervector to a list before returning\n    return composite_hv.tolist()\n\n# Example usage:\nrow = {'feature1': 'value1', 'feature2': 'value2'}\ndim = 10000\nrandom_seeds = {'feature1': 42, 'feature2': 84}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\n\n### Explanation:\n1. **Random Hypervector Generation**: We use a seed to generate reproducible random hypervectors for both feature names and values. The `create_random_hv` function generates a hypervector of dimension `dim` with elements randomly chosen from {-1, 1}.\n\n2. **Binding**: The `bind_hvs` function performs element-wise multiplication to bind two hypervectors, representing the association between a feature name and its value.\n\n3. **Bundling**: The `bundle_hvs` function sums all bound hypervectors and applies a threshold to convert the sum into a binary hypervector. Positive sums are converted to 1, and negative sums to -1.\n\n4. **Composite Hypervector**: The final composite hypervector is returned as a list using `tolist()`.\n\nThis implementation ensures that the hypervectors are reproducible and that the composite hypervector accurately represents the dataset row in high-dimensional space."}
{"task_id": 75, "completion_id": 0, "completion": "To implement the `confusion_matrix` function, we need to calculate the number of true positives, true negatives, false positives, and false negatives from the given data. Here's how you can achieve this:\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize counts for the confusion matrix\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    \n    # Iterate over each pair of true and predicted labels\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    \n    # Create the confusion matrix as a list of lists\n    confusion_matrix = [\n        [true_positive, false_negative],  # First row: [TP, FN]\n        [false_positive, true_negative]   # Second row: [FP, TN]\n    ]\n    \n    return confusion_matrix\n\n# Example usage:\ndata = [\n    [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0]\n]\nprint(confusion_matrix(data))\n```\n\n### Explanation:\n- **True Positive (TP)**: The model correctly predicts the positive class.\n- **True Negative (TN)**: The model correctly predicts the negative class.\n- **False Positive (FP)**: The model incorrectly predicts the positive class (Type I error).\n- **False Negative (FN)**: The model incorrectly predicts the negative class (Type II error).\n\nThe function iterates over each pair of true and predicted labels, updating the counts for TP, TN, FP, and FN accordingly. Finally, it returns the confusion matrix as a 2x2 list of lists."}
{"task_id": 76, "completion_id": 0, "completion": "To calculate the cosine similarity between two vectors, we need to use the formula:\n\n\\[ \\text{cosine\\_similarity} = \\frac{{v1 \\cdot v2}}{{\\|v1\\| \\times \\|v2\\|}} \\]\n\nWhere:\n- \\( v1 \\cdot v2 \\) is the dot product of the vectors.\n- \\( \\|v1\\| \\) and \\( \\|v2\\| \\) are the magnitudes (or Euclidean norms) of the vectors.\n\nHere's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Calculate the dot product of the two vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Ensure neither vector has zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors must not have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the result rounded to three decimal places\n    return round(cosine_sim, 3)\n\n# Example usage:\n# v1 = np.array([1, 2, 3])\n# v2 = np.array([4, 5, 6])\n# print(cosine_similarity(v1, v2))  # Output: 0.975\n```\n\n### Explanation:\n1. **Shape Check**: We first check if the input vectors have the same shape. If not, we raise a `ValueError`.\n2. **Dot Product**: We calculate the dot product of the two vectors using `np.dot`.\n3. **Magnitude Calculation**: We calculate the magnitude of each vector using `np.linalg.norm`.\n4. **Zero Magnitude Check**: We ensure that neither vector has zero magnitude, as this would lead to division by zero.\n5. **Cosine Similarity Calculation**: We compute the cosine similarity using the formula and return the result rounded to three decimal places."}
{"task_id": 77, "completion_id": 0, "completion": "To implement the `performance_metrics` function, we need to calculate the confusion matrix and derive the required performance metrics from it. Here's the complete implementation:\n\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    \n    # Initialize confusion matrix components\n    true_positive = false_positive = true_negative = false_negative = 0\n    \n    # Calculate confusion matrix components\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positive += 1\n        elif a == 0 and p == 1:\n            false_positive += 1\n        elif a == 0 and p == 0:\n            true_negative += 1\n        elif a == 1 and p == 0:\n            false_negative += 1\n    \n    # Confusion matrix\n    confusion_matrix = [[true_positive, false_positive], [false_negative, true_negative]]\n    \n    # Calculate metrics\n    total = len(actual)\n    accuracy = (true_positive + true_negative) / total\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) > 0 else 0\n    negative_predictive_value = true_negative / (true_negative + false_negative) if (true_negative + false_negative) > 0 else 0\n    \n    # Round metrics to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\nactual = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 0, 1, 1, 0, 1]\nresult = performance_metrics(actual, predicted)\nprint(result)\n```\n\n### Explanation:\n1. **Confusion Matrix Calculation**: We iterate over the `actual` and `predicted` lists simultaneously to count true positives, false positives, true negatives, and false negatives.\n\n2. **Metrics Calculation**:\n   - **Accuracy**: The ratio of correctly predicted instances to the total instances.\n   - **Precision**: The ratio of true positives to the sum of true positives and false positives.\n   - **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n   - **F1 Score**: The harmonic mean of precision and recall.\n   - **Specificity**: The ratio of true negatives to the sum of true negatives and false positives.\n   - **Negative Predictive Value**: The ratio of true negatives to the sum of true negatives and false negatives.\n\n3. **Rounding**: Each metric is rounded to three decimal places before returning.\n\nThis function provides a comprehensive set of performance metrics for evaluating binary classification models."}
{"task_id": 78, "completion_id": 0, "completion": "To accomplish the task of calculating various descriptive statistics for a given dataset, we can utilize the NumPy library for efficient numerical operations and the SciPy library for calculating the mode. Below is the implementation of the `descriptive_statistics` function:\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Ensure the data is a NumPy array for consistency\n    data = np.array(data)\n    \n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate median\n    median = np.median(data)\n    \n    # Calculate mode\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 0 else None\n    \n    # Calculate variance\n    variance = np.var(data, ddof=1)  # ddof=1 for sample variance\n    \n    # Calculate standard deviation\n    standard_deviation = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    \n    # Calculate percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    \n    # Calculate interquartile range (IQR)\n    interquartile_range = percentile_75 - percentile_25\n    \n    # Prepare the result dictionary\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,  # Mode is not rounded as it is a value from the data\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return result\n\n# Example usage:\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]\nstatistics = descriptive_statistics(data)\nprint(statistics)\n```\n\n### Explanation:\n- **Mean**: Calculated using `np.mean`.\n- **Median**: Calculated using `np.median`.\n- **Mode**: Calculated using `stats.mode` from the SciPy library. The mode is not rounded because it is a value from the dataset.\n- **Variance**: Calculated using `np.var` with `ddof=1` to get the sample variance.\n- **Standard Deviation**: Calculated using `np.std` with `ddof=1` for sample standard deviation.\n- **Percentiles**: Calculated using `np.percentile` for 25th, 50th, and 75th percentiles.\n- **Interquartile Range (IQR)**: Calculated as the difference between the 75th and 25th percentiles.\n\nAll calculated statistics, except for the mode, are rounded to four decimal places as specified."}
{"task_id": 79, "completion_id": 0, "completion": "To calculate the probability of achieving exactly \\( k \\) successes in \\( n \\) independent Bernoulli trials, each with probability \\( p \\) of success, we can use the Binomial distribution formula:\n\n\\[ P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1-p)^{n-k} \\]\n\nWhere:\n- \\(\\binom{n}{k}\\) is the binomial coefficient, calculated as \\(\\frac{n!}{k!(n-k)!}\\).\n- \\( p^k \\) is the probability of success raised to the power of the number of successes.\n- \\( (1-p)^{n-k} \\) is the probability of failure raised to the power of the number of failures.\n\nHere's the Python function implementing this formula:\n\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient\n    binomial_coefficient = math.comb(n, k)\n    \n    # Calculate the probability using the binomial distribution formula\n    probability = binomial_coefficient * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# print(binomial_probability(10, 3, 0.5))  # Example call to the function\n```\n\n### Explanation:\n- `math.comb(n, k)` is used to calculate the binomial coefficient \\(\\binom{n}{k}\\).\n- The probability is calculated using the formula and then rounded to five decimal places using `round(probability, 5)`.\n- The function returns the rounded probability value."}
{"task_id": 80, "completion_id": 0, "completion": "To calculate the probability density function (PDF) of the normal distribution, we can use the following formula:\n\n\\[ \\text{PDF}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\n\nWhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nHere's the implementation of the function using this formula:\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the PDF using the normal distribution formula\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf_value = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(exponent)\n    \n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf_value, 5)\n\n# Example usage:\n# print(normal_pdf(0, 0, 1))  # Should return the PDF value for x=0, mean=0, std_dev=1\n```\n\nThis function calculates the PDF of the normal distribution for a given value \\( x \\), mean \\( \\mu \\), and standard deviation \\( \\sigma \\), and returns the result rounded to 5 decimal places. You can test the function with different values to verify its correctness."}
{"task_id": 81, "completion_id": 0, "completion": "To calculate the probability of observing exactly \\( k \\) events in a fixed interval using the Poisson distribution formula, you can use the following Python function:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam < 0:\n        raise ValueError(\"Both k and lam must be non-negative.\")\n    \n    # Poisson distribution formula: P(k; \u03bb) = (\u03bb^k * e^(-\u03bb)) / k!\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    \n    return round(probability, 5)\n\n# Example usage:\n# print(poisson_probability(3, 2.5))  # Output: 0.21327\n```\n\n### Explanation:\n- **Parameters**:\n  - `k`: The number of events you want to calculate the probability for. It must be a non-negative integer.\n  - `lam`: The average rate (mean) of occurrences in the interval. It must be a non-negative number.\n  \n- **Formula**:\n  - The Poisson probability mass function is given by:\n    \\[\n    P(k; \\lambda) = \\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!}\n    \\]\n  - Here, \\( \\lambda \\) is the mean rate of occurrences, \\( k \\) is the number of events, \\( e \\) is the base of the natural logarithm, and \\( k! \\) is the factorial of \\( k \\).\n\n- **Implementation**:\n  - The function uses `math.exp` to calculate \\( e^{-\\lambda} \\) and `math.factorial` to compute \\( k! \\).\n  - The result is rounded to 5 decimal places using the `round` function.\n\nThis function will raise a `ValueError` if either `k` or `lam` is negative, ensuring that the inputs are valid for a Poisson distribution calculation."}
{"task_id": 82, "completion_id": 0, "completion": "To calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values, you can use the following Python function:\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray.\")\n    \n    if img.ndim != 2:\n        raise ValueError(\"Input must be a 2D array representing a grayscale image.\")\n    \n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    contrast = max_pixel_value - min_pixel_value\n    \n    return contrast\n\n# Example usage:\n# img = np.array([[0, 50, 100], [150, 200, 255]])\n# print(calculate_contrast(img))  # Output: 255\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the input is a `numpy.ndarray` and if it is a 2D array, which is necessary for a grayscale image.\n- **Contrast Calculation**: The contrast is calculated as the difference between the maximum and minimum pixel values in the image.\n- **Return Value**: The function returns the contrast value, which is an integer representing the difference between the maximum and minimum pixel values.\n\nThis function assumes that the input image is a valid grayscale image with pixel values ranging from 0 to 255."}
{"task_id": 83, "completion_id": 0, "completion": "Certainly! To calculate the dot product of two vectors using NumPy, you can utilize the `numpy.dot()` function, which efficiently computes the dot product of two arrays. Here's how you can implement the `calculate_dot_product` function:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Ensure the input vectors are 1D numpy arrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError(\"Both inputs must be numpy arrays.\")\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must have the same length.\")\n    \n    # Calculate the dot product using numpy's dot function\n    dot_product = np.dot(vec1, vec2)\n    \n    return dot_product\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if both inputs are 1D NumPy arrays and have the same length. If not, it raises a `ValueError`.\n- **Dot Product Calculation**: It uses `np.dot()` to compute the dot product, which is a standard and efficient way to perform this operation in NumPy.\n- **Return Value**: The function returns the dot product as a single number (float).\n\nYou can use this function by passing two 1D NumPy arrays to it, and it will return their dot product."}
{"task_id": 84, "completion_id": 0, "completion": "To perform a Phi Transformation for polynomial features, we can use the `numpy` library to efficiently generate polynomial combinations of the input features. Below is the implementation of the `phi_transform` function:\n\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    # Initialize the result list\n    transformed_data = []\n\n    # Iterate over each data point\n    for x in data:\n        # Generate polynomial features for the current data point\n        features = [round(x**d, 8) for d in range(degree + 1)]\n        transformed_data.append(features)\n\n    return transformed_data\n\n# Example usage:\ndata = [1.0, 2.0, 3.0]\ndegree = 3\nprint(phi_transform(data, degree))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the degree is less than 0. If so, it returns an empty list as specified.\n\n2. **Polynomial Feature Generation**: For each data point `x` in the input list `data`, the function generates polynomial features up to the specified `degree`. This is done using a list comprehension that computes `x**d` for each degree `d` from 0 to `degree`.\n\n3. **Rounding**: Each computed polynomial feature is rounded to 8 decimal places using the `round` function.\n\n4. **Output**: The function returns a nested list where each inner list contains the polynomial features for the corresponding data point.\n\nThis implementation efficiently handles the transformation and ensures the output meets the specified constraints."}
{"task_id": 85, "completion_id": 0, "completion": "To implement the Positional Encoding layer for Transformers, we need to follow the approach described in the original Transformer paper by Vaswani et al. The positional encoding is calculated using sine and cosine functions for each dimension of the model. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    # Return -1 if position is 0 or d_model is less than or equal to 0\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the positional encoding matrix\n    pos_enc = np.zeros((position, d_model), dtype=np.float32)\n    \n    # Calculate the positional encoding using sine and cosine functions\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                pos_enc[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n            else:\n                pos_enc[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n    \n    # Convert the positional encoding matrix to dtype np.float16\n    pos_enc = pos_enc.astype(np.float16)\n    \n    # Convert the numpy array to a list and return\n    return pos_enc.tolist()\n\n# Example usage:\n# pos_encoding(5, 10)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `position` is 0 or `d_model` is less than or equal to 0, returning -1 in such cases as specified.\n\n2. **Matrix Initialization**: A matrix `pos_enc` of shape `(position, d_model)` is initialized with zeros. The dtype is initially set to `np.float32` for precision during calculations.\n\n3. **Positional Encoding Calculation**:\n   - For each position `pos` and each dimension `i`, the encoding is calculated using sine for even indices and cosine for odd indices.\n   - The formula uses a scaling factor of `10000` raised to the power of the dimension index divided by the model dimensionality.\n\n4. **Type Conversion**: After computing the positional encodings, the matrix is converted to `np.float16` to meet the output constraints.\n\n5. **Return**: The numpy array is converted to a list using `tolist()` before returning.\n\nThis implementation follows the standard approach for positional encoding in Transformers, ensuring that the encodings are suitable for use in attention mechanisms."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting: training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n\n    # Check for underfitting: both training and test accuracy are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n\n    # If neither condition is met, the model is a good fit\n    return 0  # Good fit\n```\n\nThis function evaluates the fit quality of a machine learning model by comparing the training and test accuracies. It checks for overfitting by seeing if the training accuracy is significantly higher than the test accuracy, and for underfitting by checking if both accuracies are below a threshold. If neither condition is met, it considers the model to be a good fit."}
{"task_id": 87, "completion_id": 0, "completion": "To implement the Adam optimizer update step function, we need to follow the algorithm's steps, including bias correction for the moving averages. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter_update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter - parameter_update\n    \n    # Round and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n```\n\n### Explanation:\n1. **Moment Updates**: The function updates the first (`m`) and second (`v`) moment estimates using the decay rates `beta1` and `beta2`.\n\n2. **Bias Correction**: The bias correction is applied to both moment estimates (`m_hat` and `v_hat`) to account for initialization bias.\n\n3. **Parameter Update**: The parameter is updated using the corrected moment estimates, the learning rate, and a small constant `epsilon` for numerical stability.\n\n4. **Output Formatting**: The updated parameter, `m`, and `v` are rounded to 5 decimal places and converted to lists using `tolist()` before returning.\n\nThis implementation handles both scalar and array inputs, leveraging NumPy's capabilities for element-wise operations."}
{"task_id": 88, "completion_id": 0, "completion": "To implement a simplified GPT-2-like text generation function, we need to follow the architecture components mentioned in the task description. Here's a step-by-step implementation:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),  # Token embeddings\n        \"wpe\": np.random.rand(1024, 10),  # Positional embeddings\n        \"blocks\": [],  # Placeholder for transformer blocks\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef multi_head_attention(x, hparams, params):\n    # Simplified multi-head attention\n    # For simplicity, we will just return the input as output\n    return x\n\ndef feed_forward_network(x, params):\n    # Simplified feed-forward network\n    # For simplicity, we will just return the input as output\n    return x\n\ndef layer_norm(x, params):\n    # Simplified layer normalization\n    g = params['ln_f']['g']\n    b = params['ln_f']['b']\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + 1e-5) + b\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt\n    token_ids = encoder.encode(prompt)\n    \n    # Initialize the context with token embeddings and positional embeddings\n    context = np.array([params['wte'][token_id] for token_id in token_ids])\n    position_ids = np.arange(len(token_ids))\n    position_embeddings = params['wpe'][position_ids]\n    context += position_embeddings\n    \n    # Generate tokens\n    for _ in range(n_tokens_to_generate):\n        # Apply multi-head attention\n        attention_output = multi_head_attention(context, hparams, params)\n        \n        # Apply feed-forward network\n        ff_output = feed_forward_network(attention_output, params)\n        \n        # Apply layer normalization\n        normalized_output = layer_norm(ff_output, params)\n        \n        # For simplicity, we will just append a random token embedding\n        next_token_embedding = params['wte'][np.random.choice(len(params['wte']))]\n        context = np.vstack([context, next_token_embedding])\n    \n    # Decode the generated token ids\n    generated_token_ids = [np.argmax(embedding) for embedding in context]\n    generated_text = encoder.decode(generated_token_ids)\n    \n    return generated_text\n\n# Example usage\nprompt = \"hello\"\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)\nprint(\"Generated text:\", generated_text)\n```\n\n### Explanation:\n1. **Token Embeddings**: We use random embeddings for simplicity. In a real model, these would be learned parameters.\n2. **Positional Embeddings**: Added to token embeddings to provide positional information.\n3. **Multi-head Attention**: Simplified to return the input directly, as implementing full attention is complex.\n4. **Feed-Forward Network**: Simplified to return the input directly.\n5. **Layer Normalization**: Implemented to stabilize outputs.\n6. **Text Generation**: We generate tokens by appending random embeddings, which is a placeholder for actual token prediction logic.\n\nThis code provides a basic structure to understand the components of GPT-2, but lacks the complexity and functionality of a real model."}
{"task_id": 89, "completion_id": 0, "completion": "To implement the Pattern Weaver's Code using a simplified self-attention mechanism, we need to follow these steps:\n\n1. **Softmax Function**: This function will compute the attention scores for each crystal based on its relationship with others.\n\n2. **Self-Attention Mechanism**: For each crystal, calculate its relationship with every other crystal, compute the attention scores using the softmax function, and derive the final weighted pattern for each crystal.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))  # Subtract max for numerical stability\n        return exp_values / np.sum(exp_values)\n    \n    # Initialize the output list\n    enhanced_patterns = []\n    \n    # Convert crystal values to a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values)\n    \n    # Iterate over each crystal to compute its enhanced pattern\n    for i in range(n):\n        # Calculate the relationship (dot product) of the current crystal with all others\n        relationships = np.dot(crystal_values, crystal_values[i])\n        \n        # Compute attention scores using the softmax function\n        attention_scores = softmax(relationships)\n        \n        # Derive the final weighted pattern for the current crystal\n        weighted_pattern = np.sum(attention_scores[:, np.newaxis] * crystal_values, axis=0)\n        \n        # Append the result to the enhanced_patterns list\n        enhanced_patterns.append(weighted_pattern)\n    \n    # Flatten the list and round each value to 4 decimal places\n    flattened_patterns = [round(val, 4) for pattern in enhanced_patterns for val in pattern]\n    \n    return flattened_patterns\n\n# Example usage:\nn = 3\ncrystal_values = [[1, 2], [3, 4], [5, 6]]\ndimension = 2\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\n### Explanation:\n\n- **Softmax Function**: This function computes the softmax of a given array of values. It ensures numerical stability by subtracting the maximum value from the input array before exponentiation.\n\n- **Self-Attention Calculation**: For each crystal, we compute its relationship with all other crystals using the dot product. The attention scores are then calculated using the softmax function.\n\n- **Weighted Pattern**: The final pattern for each crystal is computed by taking a weighted sum of all crystals, where the weights are the attention scores.\n\n- **Output**: The result is a flattened list of enhanced patterns, with each value rounded to the 4th decimal place.\n\nThis code provides a simplified version of the self-attention mechanism, focusing on the relationships between crystals and their values."}
{"task_id": 90, "completion_id": 0, "completion": "To implement the BM25 ranking function, we need to calculate the BM25 score for each document in the corpus based on the given query. BM25 is a probabilistic retrieval model that considers term frequency, document length, and inverse document frequency. Here's how you can implement this:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Calculate the average document length\n    avg_doc_length = np.mean([len(doc.split()) for doc in corpus])\n    \n    # Calculate the document frequency for each term in the corpus\n    doc_freq = Counter()\n    for doc in corpus:\n        unique_terms = set(doc.split())\n        for term in unique_terms:\n            doc_freq[term] += 1\n    \n    # Calculate the IDF for each term in the query\n    N = len(corpus)\n    idf = {}\n    for term in query.split():\n        # Using the formula: IDF = log((N - df + 0.5) / (df + 0.5) + 1)\n        df = doc_freq.get(term, 0)\n        idf[term] = np.log((N - df + 0.5) / (df + 0.5) + 1)\n    \n    # Calculate BM25 scores for each document\n    scores = []\n    for doc in corpus:\n        doc_length = len(doc.split())\n        term_freq = Counter(doc.split())\n        score = 0.0\n        for term in query.split():\n            if term in term_freq:\n                # Using the formula: BM25 = IDF * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length))))\n                tf = term_freq[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))\n                score += idf[term] * (numerator / denominator)\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage:\ncorpus = [\n    \"the quick brown fox\",\n    \"jumped over the lazy dog\",\n    \"the quick dog\",\n    \"the brown dog jumped\"\n]\nquery = \"quick brown\"\nbm25_scores = calculate_bm25_scores(corpus, query)\nprint(bm25_scores)\n```\n\n### Explanation:\n1. **Average Document Length**: Calculate the average length of documents in the corpus.\n2. **Document Frequency**: Count how many documents contain each term.\n3. **Inverse Document Frequency (IDF)**: Compute the IDF for each term in the query using the BM25 formula.\n4. **BM25 Score Calculation**: For each document, calculate the BM25 score by considering term frequency, document length normalization, and the IDF of query terms.\n5. **Output**: Return a list of scores, each rounded to three decimal places.\n\nThis implementation should give you the BM25 scores for each document in the corpus based on the provided query."}
{"task_id": 91, "completion_id": 0, "completion": "To calculate the F1 score, we need to first compute precision and recall. Precision is the ratio of true positive predictions to the total number of positive predictions, while recall is the ratio of true positive predictions to the total number of actual positive instances. The F1 score is the harmonic mean of precision and recall.\n\nHere's the implementation of the `calculate_f1_score` function:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counts\n    true_positive = 0\n    false_positive = 0\n    false_negative = 0\n\n    # Calculate true positives, false positives, and false negatives\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positive += 1\n        elif true == 0 and pred == 1:\n            false_positive += 1\n        elif true == 1 and pred == 0:\n            false_negative += 1\n\n    # Calculate precision and recall\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n\n    # Calculate F1 score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n\n    # Return the F1 score rounded to three decimal places\n    return round(f1_score, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n# y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n# print(calculate_f1_score(y_true, y_pred))  # Output: 0.667\n```\n\nThis function iterates over the true and predicted labels to count true positives, false positives, and false negatives. It then calculates precision and recall, and finally computes the F1 score, rounding it to three decimal places before returning."}
{"task_id": 92, "completion_id": 0, "completion": "To tackle this task, we'll follow the steps outlined in the function `power_grid_forecast`. We'll use Python's libraries to perform linear regression and handle mathematical operations. Here's the complete implementation:\n\n```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended_data = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption_data[i - 1] - fluctuation)\n    \n    # Step 2: Perform linear regression on the detrended data\n    days = np.arange(1, 11).reshape(-1, 1)  # Days 1 through 10\n    detrended_data = np.array(detrended_data).reshape(-1, 1)\n    \n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # Step 3: Predict day 15's base consumption\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0][0]\n    \n    # Step 4: Add the day 15 fluctuation back\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    \n    # Step 5: Round, then add a 5% safety margin (rounded up)\n    total_consumption_day_15 = math.ceil(total_consumption_day_15)\n    safety_margin = math.ceil(total_consumption_day_15 * 0.05)\n    final_consumption = total_consumption_day_15 + safety_margin\n    \n    # Step 6: Return the final integer\n    return final_consumption\n\n# Example usage:\n# consumption_data = [your daily power usage data for 10 days]\n# print(power_grid_forecast(consumption_data))\n```\n\n### Explanation:\n\n1. **Detrending the Data**: We subtract the known fluctuation from each day's consumption data to isolate the linear trend.\n\n2. **Linear Regression**: We use `sklearn`'s `LinearRegression` to fit a model to the detrended data. This model helps us predict future base consumption without daily fluctuations.\n\n3. **Prediction for Day 15**: Using the fitted model, we predict the base consumption for day 15.\n\n4. **Adding Back Fluctuation**: We calculate the fluctuation for day 15 using the given formula and add it to the predicted base consumption.\n\n5. **Safety Margin**: We round the total consumption and add a 5% safety margin, ensuring the colony's needs are met with a buffer.\n\n6. **Return the Result**: The final consumption value is returned as an integer.\n\nThis code assumes you have the daily power usage data for the first 10 days available as `consumption_data`. You can replace the placeholder with actual data to use the function."}
{"task_id": 93, "completion_id": 0, "completion": "To calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values, you can implement the function as follows:\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute differences between the true and predicted values\n    absolute_errors = np.abs(y_true - y_pred)\n    \n    # Calculate the mean of these absolute differences\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    # Return the result rounded to 3 decimal places\n    return round(mean_absolute_error, 3)\n```\n\n### Explanation:\n1. **Absolute Differences**: The function first calculates the absolute differences between each pair of true and predicted values using `np.abs(y_true - y_pred)`.\n2. **Mean of Absolute Differences**: It then computes the mean of these absolute differences using `np.mean(absolute_errors)`.\n3. **Rounding**: Finally, the result is rounded to three decimal places using Python's built-in `round()` function.\n\nThis function assumes that `y_true` and `y_pred` are NumPy arrays of the same length. If they are not, you should add error handling to manage such cases."}
{"task_id": 94, "completion_id": 0, "completion": "To implement the multi-head attention mechanism, we need to break down the process into three key functions: `compute_qkv`, `self_attention`, and `multi_head_attention`. Here's how you can implement these functions:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the queries, keys, and values matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention for a single head.\n    \"\"\"\n    d_k = K.shape[-1]\n    # Compute scaled dot-product attention\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    # Apply softmax to get attention weights\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    # Compute the output\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention.\n    \"\"\"\n    d_model = X.shape[-1]\n    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n    d_head = d_model // n_heads\n\n    # Split weights for each head\n    W_q_split = np.split(W_q, n_heads, axis=-1)\n    W_k_split = np.split(W_k, n_heads, axis=-1)\n    W_v_split = np.split(W_v, n_heads, axis=-1)\n\n    # Compute attention for each head\n    heads_output = []\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q_split[i], W_k_split[i], W_v_split[i])\n        head_output = self_attention(Q, K, V)\n        heads_output.append(head_output)\n\n    # Concatenate the outputs of all heads\n    concatenated_output = np.concatenate(heads_output, axis=-1)\n\n    # Round the results to the nearest 4th decimal\n    rounded_output = np.round(concatenated_output, 4)\n\n    # Return as a list\n    return rounded_output.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 8)  # Example input\n# W_q = np.random.rand(8, 8)  # Example weights for queries\n# W_k = np.random.rand(8, 8)  # Example weights for keys\n# W_v = np.random.rand(8, 8)  # Example weights for values\n# n_heads = 2  # Number of attention heads\n# output = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n# print(output)\n```\n\n### Explanation:\n1. **compute_qkv**: This function computes the query, key, and value matrices by multiplying the input `X` with the respective weight matrices `W_q`, `W_k`, and `W_v`.\n\n2. **self_attention**: This function calculates the self-attention for a single head. It computes the scaled dot-product attention, applies the softmax function to get attention weights, and then computes the output by multiplying these weights with the value matrix `V`.\n\n3. **multi_head_attention**: This function orchestrates the multi-head attention mechanism. It splits the weight matrices for each head, computes the attention for each head, concatenates the outputs, rounds the results to the nearest 4th decimal, and returns the final output as a list.\n\nThis implementation assumes that the input dimensions and the number of heads are compatible, and it uses numpy for efficient matrix operations."}
{"task_id": 95, "completion_id": 0, "completion": "To calculate the Phi coefficient, we need to determine the correlation between two binary variables. The Phi coefficient is similar to the Pearson correlation coefficient but is specifically designed for binary data. Here's how you can implement the function:\n\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n    \n    # Calculate the contingency table values\n    n00 = n01 = n10 = n11 = 0\n    for xi, yi in zip(x, y):\n        if xi == 0 and yi == 0:\n            n00 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 1 and yi == 1:\n            n11 += 1\n    \n    # Calculate the Phi coefficient\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n01 + n00) * (n10 + n00)) ** 0.5\n    \n    if denominator == 0:\n        raise ValueError(\"Denominator is zero, cannot calculate Phi coefficient.\")\n    \n    phi = numerator / denominator\n    \n    return round(phi, 4)\n\n# Example usage:\nx = [0, 1, 0, 1, 1, 0, 1, 0]\ny = [1, 0, 0, 1, 1, 0, 0, 1]\nprint(phi_corr(x, y))  # Output will be the Phi coefficient rounded to 4 decimal places\n```\n\n### Explanation:\n1. **Contingency Table Calculation**: We calculate the counts for each combination of values (n00, n01, n10, n11) where:\n   - `n00` is the count of pairs where both values are 0.\n   - `n01` is the count of pairs where the first value is 0 and the second is 1.\n   - `n10` is the count of pairs where the first value is 1 and the second is 0.\n   - `n11` is the count of pairs where both values are 1.\n\n2. **Phi Coefficient Formula**: The formula for the Phi coefficient is:\n   \\[\n   \\phi = \\frac{n_{11} \\cdot n_{00} - n_{10} \\cdot n_{01}}{\\sqrt{(n_{11} + n_{10}) \\cdot (n_{11} + n_{01}) \\cdot (n_{01} + n_{00}) \\cdot (n_{10} + n_{00})}}\n   \\]\n   This formula calculates the correlation between the two binary variables.\n\n3. **Error Handling**: We check if the denominator is zero to avoid division by zero errors.\n\n4. **Rounding**: The result is rounded to 4 decimal places for precision."}
{"task_id": 96, "completion_id": 0, "completion": "To implement the Hard Sigmoid activation function, we need to follow its definition, which is a piecewise linear approximation of the standard sigmoid function. The Hard Sigmoid function can be defined as:\n\n\\[ \\text{hard\\_sigmoid}(x) = \\max(0, \\min(1, 0.2x + 0.5)) \\]\n\nThis function linearly scales the input \\( x \\) and then clamps the result between 0 and 1. Here's how you can implement it in Python:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))\n\n# Example usage:\n# print(hard_sigmoid(-5))  # Output: 0.0\n# print(hard_sigmoid(0))   # Output: 0.5\n# print(hard_sigmoid(5))   # Output: 1.0\n```\n\nThis function first scales the input \\( x \\) by multiplying it by 0.2 and then adding 0.5. The result is then clamped between 0 and 1 using the `max` and `min` functions. This approach ensures that the output is always within the desired range, making it computationally efficient and suitable for use in neural networks."}
{"task_id": 97, "completion_id": 0, "completion": "To implement the ELU (Exponential Linear Unit) activation function, we need to handle both positive and negative input values. The ELU function is defined as follows:\n\n- For \\( x > 0 \\), the function returns \\( x \\).\n- For \\( x \\leq 0 \\), the function returns \\( \\alpha \\times (\\exp(x) - 1) \\).\n\nHere's how you can implement this in Python:\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(elu(1.0))  # Should return 1.0\n# print(elu(-1.0)) # Should return approximately -0.6321 if alpha is 1.0\n```\n\n### Explanation:\n- **Positive Input**: If the input \\( x \\) is greater than zero, the function simply returns \\( x \\).\n- **Negative Input**: If the input \\( x \\) is less than or equal to zero, the function calculates the ELU value using the formula \\( \\alpha \\times (\\exp(x) - 1) \\).\n- **Rounding**: The result is rounded to the 4th decimal place using Python's `round()` function to meet the output constraints.\n\nThis implementation provides a simple and efficient way to compute the ELU activation function for any given input."}
{"task_id": 98, "completion_id": 0, "completion": "To implement the PReLU (Parametric ReLU) activation function, we need to define a function that computes the activation value based on the input `x` and a learnable parameter `alpha`. The PReLU function is defined as:\n\n\\[ \\text{PReLU}(x) = \n\\begin{cases} \nx, & \\text{if } x > 0 \\\\\n\\alpha \\cdot x, & \\text{if } x \\leq 0 \n\\end{cases} \\]\n\nHere's the implementation of the PReLU activation function:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage:\n# Compute PReLU activation for a given input\ninput_value = -3.0\nalpha_value = 0.1\nactivation_value = prelu(input_value, alpha_value)\nprint(f\"PReLU activation for input {input_value} with alpha {alpha_value}: {activation_value}\")\n```\n\nThis function checks if the input `x` is greater than zero. If it is, the function returns `x` itself, which is the same behavior as the standard ReLU function. If `x` is less than or equal to zero, it returns `alpha * x`, allowing for a non-zero gradient when the input is negative, which is the key feature of PReLU. The parameter `alpha` can be adjusted to optimize the model during training."}
{"task_id": 99, "completion_id": 0, "completion": "To implement the Softplus activation function, we need to ensure that the computation is numerically stable, especially for large or small values of `x`. The formula for the Softplus function is `log(1 + e^x)`. However, directly computing `e^x` can lead to overflow for large values of `x`. To handle this, we can use a numerically stable approach by leveraging the properties of logarithms and exponentials.\n\nHere's the implementation:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Use a numerically stable approach to compute softplus\n    if x > 709:  # e^709 is close to the maximum float value\n        return round(x, 4)  # log(1 + e^x) \u2248 x for large x\n    elif x < -745:  # e^-745 is close to zero\n        return round(math.exp(x), 4)  # log(1 + e^x) \u2248 e^x for very small x\n    else:\n        return round(math.log1p(math.exp(x)), 4)  # log1p computes log(1 + e^x) accurately\n\n# Example usage:\nprint(softplus(1.0))  # Output: 1.3133\nprint(softplus(-1.0)) # Output: 0.3133\nprint(softplus(1000.0)) # Output: 1000.0\nprint(softplus(-1000.0)) # Output: 0.0\n```\n\n### Explanation:\n- **Numerical Stability**: \n  - For large positive values of `x`, `log(1 + e^x)` is approximately `x` because `e^x` dominates the `1` in the expression.\n  - For large negative values of `x`, `e^x` is very small, so `log(1 + e^x)` is approximately `e^x`.\n- **`math.log1p`**: This function computes `log(1 + e^x)` in a numerically stable way, especially when `x` is small, avoiding precision issues that can arise with direct computation.\n- **Rounding**: The result is rounded to the 4th decimal place as per the output constraints."}
{"task_id": 100, "completion_id": 0, "completion": "To implement the Softsign activation function, we need to compute the value using the formula:\n\n\\[ \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\]\n\nThis function is smooth and bounded between -1 and 1. Below is the implementation of the Softsign function in Python, which rounds the result to the fourth decimal place:\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)\n\n# Example usage:\n# print(softsign(2.0))  # Output: 0.6667\n# print(softsign(-2.0)) # Output: -0.6667\n```\n\nThis function takes a float `x` as input, computes the Softsign value, and returns the result rounded to four decimal places."}
{"task_id": 101, "completion_id": 0, "completion": "To implement the GRPO objective function, we need to compute the clipped surrogate objective and apply a KL divergence penalty. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Compute the clipped surrogate objective\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_objective = np.minimum(rhos * A, clipped_rhos * A)\n\n    # Compute the KL divergence penalty\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n\n    # Compute the final GRPO objective\n    grpo_objective_value = np.sum(surrogate_objective) - beta * kl_divergence\n\n    # Round the result to the nearest 6th decimal\n    return round(grpo_objective_value, 6)\n\n# Example usage\nrhos = [1.1, 0.9, 1.05]\nA = [0.5, 0.2, 0.3]\npi_theta_old = [0.4, 0.3, 0.3]\npi_theta_ref = [0.35, 0.35, 0.3]\n\nobjective_value = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(\"GRPO Objective Value:\", objective_value)\n```\n\n### Explanation:\n1. **Clipping Mechanism**: The likelihood ratios `rhos` are clipped between `1 - epsilon` and `1 + epsilon` to ensure stability during training. This prevents large updates that could destabilize the learning process.\n\n2. **Surrogate Objective**: The surrogate objective is computed using the minimum of the original and clipped likelihood ratios multiplied by the advantage estimates `A`.\n\n3. **KL Divergence Penalty**: The KL divergence between the old policy and the reference policy is computed to penalize deviations from the reference policy, promoting stability.\n\n4. **Final Objective**: The final GRPO objective is the sum of the surrogate objective minus the KL divergence penalty, weighted by `beta`.\n\n5. **Rounding**: The result is rounded to the nearest 6th decimal to meet the output constraints."}
{"task_id": 102, "completion_id": 0, "completion": "To implement the Swish activation function, we need to use the formula:\n\n\\[ \\text{Swish}(x) = x \\cdot \\text{sigmoid}(x) \\]\n\nwhere the sigmoid function is defined as:\n\n\\[ \\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\]\n\nHere's the complete implementation of the Swish activation function in Python, with results rounded to the nearest 4th decimal:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\n\n# Example usage:\n# print(swish(1.0))  # Output: 0.7311\n# print(swish(-1.0)) # Output: -0.2689\n```\n\nThis function calculates the Swish activation value for a given input `x` and rounds the result to the nearest 4th decimal. You can test the function with different input values to see how it behaves."}
{"task_id": 103, "completion_id": 0, "completion": "To implement the SELU activation function, we need to apply the formula for SELU, which is defined as:\n\n\\[ \\text{SELU}(x) = \\text{scale} \\times \\begin{cases} \nx & \\text{if } x > 0 \\\\\n\\alpha \\times (\\exp(x) - 1) & \\text{if } x \\leq 0 \n\\end{cases} \\]\n\nGiven the constants `alpha` and `scale`, we can implement this function in Python. Here's the complete code:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(selu(0.5))  # Output: 0.5254\n# print(selu(-0.5)) # Output: -0.6751\n```\n\n### Explanation:\n- **Positive Input**: For `x > 0`, the SELU function simply scales the input by the `scale` factor.\n- **Negative Input**: For `x <= 0`, the SELU function applies the ELU transformation scaled by `alpha` and then multiplies by `scale`.\n- **Rounding**: The result is rounded to the nearest 4th decimal using Python's `round()` function to meet the output constraints."}
{"task_id": 104, "completion_id": 0, "completion": "To implement the prediction function for binary classification using Logistic Regression, we need to compute the class probabilities using the sigmoid function and then convert these probabilities into binary predictions based on a threshold of 0.5. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to compute probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Convert probabilities to binary predictions using a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return predictions as a Python list\n    return predictions.tolist()\n\n# Example usage:\n# X = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])\n# weights = np.array([0.4, 0.6])\n# bias = -0.2\n# print(predict_logistic(X, weights, bias))\n```\n\n### Explanation:\n1. **Linear Combination**: We calculate the linear combination of the input features and weights, adding the bias term. This is done using `np.dot(X, weights) + bias`.\n\n2. **Sigmoid Function**: The sigmoid function is applied to the linear combination to compute the probabilities. The sigmoid function is defined as `1 / (1 + np.exp(-z))`, where `z` is the linear combination.\n\n3. **Binary Predictions**: We convert the probabilities into binary predictions using a threshold of 0.5. If the probability is greater than or equal to 0.5, the prediction is 1; otherwise, it is 0.\n\n4. **Return as List**: Finally, we convert the predictions to a Python list using the `tolist()` method, as required by the output constraints."}
{"task_id": 105, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for Softmax regression, we need to follow these steps:\n\n1. **Initialize Parameters**: Start with random weights for the model.\n2. **Forward Pass**: Compute the predictions using the softmax function.\n3. **Compute Loss**: Calculate the cross-entropy loss.\n4. **Backward Pass**: Compute the gradient of the loss with respect to the weights.\n5. **Update Parameters**: Adjust the weights using the gradients and the learning rate.\n6. **Iterate**: Repeat the process for a specified number of iterations.\n\nHere's the implementation of the `train_softmaxreg` function:\n\n```python\nimport numpy as np\n\ndef softmax(z):\n    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true, y_pred):\n    m = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(m), y_true])\n    loss = np.sum(log_likelihood) / m\n    return loss\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = np.max(y) + 1\n    \n    # Initialize weights\n    B = np.random.randn(n_features, n_classes)\n    \n    # To store the loss values\n    losses = []\n    \n    for i in range(iterations):\n        # Forward pass: compute predicted probabilities\n        logits = np.dot(X, B)\n        y_pred = softmax(logits)\n        \n        # Compute the loss\n        loss = cross_entropy_loss(y, y_pred)\n        losses.append(round(loss, 4))\n        \n        # Backward pass: compute the gradient\n        y_one_hot = np.zeros((n_samples, n_classes))\n        y_one_hot[np.arange(n_samples), y] = 1\n        gradient = np.dot(X.T, (y_pred - y_one_hot)) / n_samples\n        \n        # Update weights\n        B -= learning_rate * gradient\n    \n    # Round the final weights\n    B_rounded = np.round(B, 4)\n    \n    return B_rounded.tolist(), losses\n\n# Example usage:\n# X = np.array([[...], [...], ...])  # Feature matrix\n# y = np.array([...])  # Labels\n# learning_rate = 0.01\n# iterations = 1000\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n```\n\n### Explanation:\n\n- **Softmax Function**: Converts logits to probabilities.\n- **Cross-Entropy Loss**: Measures the difference between the true labels and predicted probabilities.\n- **Gradient Calculation**: Uses the difference between predicted probabilities and one-hot encoded true labels to compute the gradient.\n- **Parameter Update**: Adjusts the weights using the computed gradient and learning rate.\n- **Rounding**: Both the weights and loss values are rounded to four decimal places as required.\n\nThis implementation assumes that `y` contains integer class labels starting from 0. Adjustments may be needed if the data format differs."}
{"task_id": 106, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for logistic regression, we need to follow these steps:\n\n1. Initialize the model parameters (weights and bias).\n2. Implement the sigmoid function to map predictions to probabilities.\n3. Compute the Binary Cross Entropy loss.\n4. Update the model parameters using gradient descent.\n5. Collect and return the loss values over iterations.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross Entropy loss.\"\"\"\n    # Clip predictions to avoid log(0)\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Initialize parameters\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Collect loss values\n    loss_values = []\n\n    for _ in range(iterations):\n        # Compute linear model\n        linear_model = np.dot(X, weights) + bias\n        # Apply sigmoid function\n        y_pred = sigmoid(linear_model)\n        \n        # Compute loss\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n        \n        # Compute gradients\n        dw = np.dot(X.T, (y_pred - y)) / n_samples\n        db = np.mean(y_pred - y)\n        \n        # Update parameters\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n\n    # Round weights and bias to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n\n    # Return the optimized coefficients and loss values\n    return weights.tolist(), bias, loss_values\n\n# Example usage:\n# X = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]])\n# y = np.array([0, 1, 0])\n# learning_rate = 0.01\n# iterations = 1000\n# weights, bias, loss_values = train_logreg(X, y, learning_rate, iterations)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n# print(\"Loss values:\", loss_values)\n```\n\nThis code initializes the weights and bias, computes predictions using the sigmoid function, calculates the Binary Cross Entropy loss, updates the weights and bias using gradient descent, and collects the loss values over the specified number of iterations. The results are rounded to the nearest 4th decimal place as required."}
{"task_id": 107, "completion_id": 0, "completion": "To implement masked self-attention, we need to compute the attention scores using the query (Q) and key (K) matrices, apply the mask to these scores, and then use the masked scores to compute the weighted sum of the value (V) matrix. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Step 1: Compute attention scores\n    # Attention scores are computed as the dot product of Q and K^T\n    scores = np.dot(Q, K.T)\n    \n    # Step 2: Scale the scores by the square root of the dimension of the key vectors\n    d_k = K.shape[-1]\n    scores /= np.sqrt(d_k)\n    \n    # Step 3: Apply the mask\n    # Masking is done by setting the masked positions to a very large negative number\n    scores = np.where(mask, scores, -np.inf)\n    \n    # Step 4: Compute the attention weights using softmax\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Step 5: Compute the output by weighting the value vectors\n    output = np.dot(attention_weights, V)\n    \n    # Step 6: Return the output as a Python list\n    return output.tolist()\n\n# Example usage:\n# X is the input matrix, W_q, W_k, W_v are the weight matrices for Q, K, V respectively\n# mask is a boolean matrix where True indicates positions to be masked\nX = np.random.rand(5, 4)  # Example input\nW_q = np.random.rand(4, 3)  # Example weight matrix for Q\nW_k = np.random.rand(4, 3)  # Example weight matrix for K\nW_v = np.random.rand(4, 3)  # Example weight matrix for V\nmask = np.array([[True, False, False, False, False],\n                 [True, True, False, False, False],\n                 [True, True, True, False, False],\n                 [True, True, True, True, False],\n                 [True, True, True, True, True]])  # Example mask\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```\n\nThis code snippet implements masked self-attention by following the steps outlined above. The attention scores are computed, scaled, masked, and then used to compute the weighted sum of the value vectors. The final output is returned as a Python list using the `tolist()` method."}
{"task_id": 108, "completion_id": 0, "completion": "To measure the disorder in a basket of apples based on their colors, we can use the concept of entropy from information theory. Entropy is a measure of uncertainty or disorder, and it increases as the variety of items increases. Here's how you can implement this in Python:\n\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    \n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # Round the result to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Example usage:\nprint(disorder([0, 0, 0, 0]))          # Output: 0.0\nprint(disorder([1, 1, 0, 0]))          # Output: 1.0\nprint(disorder([0, 1, 2, 3]))          # Output: 2.0\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Output: 2.0\nprint(disorder([0, 0, 0, 0, 0, 1, 2, 3]))  # Output: 1.5\n```\n\n### Explanation:\n\n1. **Entropy Calculation**: \n   - We use the formula for entropy: \\( H(X) = -\\sum p(x) \\log_2 p(x) \\), where \\( p(x) \\) is the probability of each color.\n   - The entropy is zero when all apples are the same color, as there is no uncertainty.\n   - The entropy increases as the variety of colors increases, reflecting greater disorder.\n\n2. **Counter**: \n   - We use `collections.Counter` to count the occurrences of each color in the list.\n\n3. **Probability Calculation**: \n   - For each color, calculate its probability by dividing its count by the total number of apples.\n\n4. **Rounding**: \n   - The result is rounded to the nearest 4th decimal to meet the output constraints.\n\nThis implementation satisfies the properties described in the task and provides a meaningful measure of disorder based on the variety of apple colors."}
{"task_id": 109, "completion_id": 0, "completion": "To implement Layer Normalization for sequence data, we need to normalize the input tensor across the feature dimension for each sequence. This involves computing the mean and variance for each sequence's features, normalizing the features, and then applying scaling and shifting using the provided `gamma` and `beta` parameters. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Ensure the input dimensions are correct\n    assert X.ndim == 3, \"Input tensor X must be a 3D array\"\n    assert gamma.ndim == 1 and beta.ndim == 1, \"Gamma and Beta must be 1D arrays\"\n    assert X.shape[2] == gamma.shape[0] == beta.shape[0], \"Feature dimension of X must match size of gamma and beta\"\n\n    # Calculate mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n\n    # Normalize the input tensor\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n\n    # Apply scaling (gamma) and shifting (beta)\n    X_scaled_shifted = gamma * X_normalized + beta\n\n    # Round the result to 5 decimal places and convert to list\n    return np.round(X_scaled_shifted, 5).tolist()\n\n# Example usage:\n# X = np.random.rand(2, 3, 4)  # Example input tensor with shape (batch_size, sequence_length, feature_dimension)\n# gamma = np.ones(4)           # Example gamma parameter with shape (feature_dimension,)\n# beta = np.zeros(4)           # Example beta parameter with shape (feature_dimension,)\n# normalized_X = layer_normalization(X, gamma, beta)\n# print(normalized_X)\n```\n\n### Explanation:\n1. **Input Validation**: We first ensure that the input tensor `X` is a 3D array and that `gamma` and `beta` are 1D arrays with the same size as the feature dimension of `X`.\n\n2. **Mean and Variance Calculation**: We compute the mean and variance along the feature dimension (axis=-1) for each sequence in the batch. The `keepdims=True` parameter ensures that the mean and variance maintain the same number of dimensions as the input, which is necessary for broadcasting during normalization.\n\n3. **Normalization**: We normalize the input tensor by subtracting the mean and dividing by the square root of the variance plus a small constant `epsilon` to prevent division by zero.\n\n4. **Scaling and Shifting**: We apply the scaling (`gamma`) and shifting (`beta`) parameters to the normalized data.\n\n5. **Output Formatting**: The result is rounded to 5 decimal places and converted to a list using `tolist()` before returning.\n\nThis implementation should work efficiently for batch processing of sequence data with layer normalization."}
{"task_id": 110, "completion_id": 0, "completion": "To compute the METEOR score, we need to follow the steps outlined in the METEOR metric, which includes calculating unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. Below is the implementation of the `meteor_score` function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the reference and candidate sentences\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Count unigrams in reference and candidate\n    ref_count = Counter(ref_tokens)\n    cand_count = Counter(cand_tokens)\n    \n    # Calculate unigram matches\n    matches = sum(min(ref_count[word], cand_count[word]) for word in cand_count)\n    \n    # Calculate precision and recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    \n    # Calculate F-mean\n    if precision + recall > 0:\n        f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    else:\n        f_mean = 0\n    \n    # Calculate fragmentation penalty\n    # Fragmentation is the number of chunks in the candidate translation\n    chunks = 0\n    last_match_index = -1\n    for i, word in enumerate(cand_tokens):\n        if word in ref_tokens:\n            ref_index = ref_tokens.index(word)\n            if ref_index != last_match_index + 1:\n                chunks += 1\n            last_match_index = ref_index\n    \n    frag_penalty = gamma * (chunks / matches) if matches > 0 else 1\n    \n    # Calculate METEOR score\n    meteor = f_mean * (1 - frag_penalty)\n    \n    # Return the METEOR score rounded to 3 decimal places\n    return round(meteor, 3)\n\n# Example usage:\nreference_translation = \"the cat is on the mat\"\ncandidate_translation = \"the cat is on mat\"\nscore = meteor_score(reference_translation, candidate_translation)\nprint(\"METEOR Score:\", score)\n```\n\n### Explanation:\n1. **Tokenization**: The reference and candidate translations are split into tokens (words).\n2. **Unigram Matches**: We count the number of unigram matches between the reference and candidate translations.\n3. **Precision and Recall**: Precision is the ratio of matched unigrams to the total unigrams in the candidate, and recall is the ratio of matched unigrams to the total unigrams in the reference.\n4. **F-mean**: This is a harmonic mean of precision and recall, weighted by the parameter `alpha`.\n5. **Fragmentation Penalty**: This penalty is based on the number of chunks (continuous sequences of matches) in the candidate translation. The penalty is calculated using the parameter `gamma`.\n6. **METEOR Score**: The final score is computed by combining the F-mean and the fragmentation penalty, and it is rounded to 3 decimal places.\n\nThis implementation provides a basic version of the METEOR score calculation. Note that the actual METEOR metric includes additional features like stemming and synonym matching, which are not covered in this simplified version."}
{"task_id": 111, "completion_id": 0, "completion": "To compute the Pointwise Mutual Information (PMI), we need to use the formula:\n\n\\[ \\text{PMI}(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x) \\cdot P(y)} \\right) \\]\n\nWhere:\n- \\( P(x, y) \\) is the probability of the joint occurrence of events \\( x \\) and \\( y \\).\n- \\( P(x) \\) is the probability of event \\( x \\).\n- \\( P(y) \\) is the probability of event \\( y \\).\n\nGiven:\n- `joint_counts` is the count of joint occurrences of events \\( x \\) and \\( y \\).\n- `total_counts_x` is the count of occurrences of event \\( x \\).\n- `total_counts_y` is the count of occurrences of event \\( y \\).\n- `total_samples` is the total number of samples.\n\nThe probabilities can be calculated as:\n- \\( P(x, y) = \\frac{\\text{joint\\_counts}}{\\text{total\\_samples}} \\)\n- \\( P(x) = \\frac{\\text{total\\_counts\\_x}}{\\text{total\\_samples}} \\)\n- \\( P(y) = \\frac{\\text{total\\_counts\\_y}}{\\text{total\\_samples}} \\)\n\nThe PMI can then be computed using these probabilities. Here's the implementation:\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Calculate probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Avoid division by zero and log of zero by checking probabilities\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return 0.0\n    \n    # Compute PMI\n    pmi = np.log2(p_xy / (p_x * p_y))\n    \n    # Return the PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\n# pmi_value = compute_pmi(10, 50, 60, 1000)\n# print(pmi_value)\n```\n\nThis function calculates the PMI and handles cases where any of the probabilities might be zero, which would otherwise lead to division by zero or taking the logarithm of zero. In such cases, it returns a PMI of 0.0, which is a common convention when dealing with zero probabilities in PMI calculations."}
{"task_id": 112, "completion_id": 0, "completion": "To implement the Min-Max Normalization, we need to scale each value in the list to a range between 0 and 1. This is done using the formula:\n\n\\[ \\text{normalized\\_value} = \\frac{x_i - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} \\]\n\nwhere \\( x_i \\) is an element in the list, and \\(\\text{min}(x)\\) and \\(\\text{max}(x)\\) are the minimum and maximum values in the list, respectively.\n\nHere's how you can implement this in Python:\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Handle the case where all values in x are the same\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    \n    normalized = [(xi - min_val) / (max_val - min_val) for xi in x]\n    return [round(num, 4) for num in normalized]\n\n# Example usage:\n# print(min_max([1, 2, 3, 4, 5]))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```\n\n### Explanation:\n1. **Check for Empty List**: If the input list is empty, return an empty list.\n2. **Find Min and Max**: Calculate the minimum and maximum values of the list.\n3. **Handle Uniform List**: If all elements in the list are the same, the min and max will be equal, and normalization would result in division by zero. In this case, return a list of zeros.\n4. **Normalize**: Use the Min-Max normalization formula to scale each element.\n5. **Round the Results**: Round each normalized value to 4 decimal places as required.\n\nThis function will correctly normalize any list of integers to the range [0, 1], handling edge cases like empty lists and lists with identical elements."}
{"task_id": 113, "completion_id": 0, "completion": "To implement a simple residual block with a shortcut connection using NumPy, we need to perform the following steps:\n\n1. **Matrix Multiplication**: Multiply the input array `x` with the weight matrices `w1` and `w2`.\n2. **ReLU Activation**: Apply the ReLU activation function after each matrix multiplication.\n3. **Shortcut Connection**: Add the original input `x` to the output of the second ReLU activation.\n4. **Final ReLU Activation**: Apply ReLU to the result of the shortcut connection.\n5. **Output Formatting**: Round the final output to 4 decimal places and convert it to a list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First layer: matrix multiplication and ReLU activation\n    z1 = np.dot(x, w1)\n    a1 = relu(z1)\n    \n    # Second layer: matrix multiplication and ReLU activation\n    z2 = np.dot(a1, w2)\n    a2 = relu(z2)\n    \n    # Shortcut connection: add original input to the output of the second layer\n    shortcut = x + a2\n    \n    # Final ReLU activation\n    output = relu(shortcut)\n    \n    # Round the output to 4 decimal places and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n\n# Example usage:\n# x = np.array([1.0, 2.0, 3.0])\n# w1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n# w2 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n# print(residual_block(x, w1, w2))\n```\n\n### Explanation:\n- **ReLU Function**: Defined as `relu(x)`, it returns the element-wise maximum of `x` and `0`.\n- **Matrix Multiplication**: `np.dot(x, w1)` and `np.dot(a1, w2)` perform the matrix multiplication.\n- **Shortcut Connection**: The original input `x` is added to the output of the second layer (`a2`).\n- **Output Formatting**: The final output is rounded to 4 decimal places using `np.round()` and converted to a list using `tolist()`.\n\nThis code provides a simple implementation of a residual block with a shortcut connection using NumPy."}
{"task_id": 114, "completion_id": 0, "completion": "To implement the Global Average Pooling function, you need to compute the average of each feature map across its spatial dimensions (height and width) for each channel. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) containing the average of each feature map.\n    \"\"\"\n    # Check if the input is a 3D array\n    if x.ndim != 3:\n        raise ValueError(\"Input must be a 3D array of shape (height, width, channels)\")\n\n    # Compute the average across the height and width dimensions\n    return np.mean(x, axis=(0, 1))\n\n# Example usage:\n# Assuming x is a 3D NumPy array with shape (height, width, channels)\n# x = np.random.rand(32, 32, 3)  # Example input\n# result = global_avg_pool(x)\n# print(result)  # Output will be a 1D array of shape (channels,)\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the input is a 3D array. If not, it raises a `ValueError`.\n- **Global Average Pooling**: The function uses `np.mean` with `axis=(0, 1)` to compute the mean across the height and width dimensions for each channel. This results in a 1D array where each element corresponds to the average of a feature map.\n- **Output**: The result is a 1D array with a length equal to the number of channels, containing the average values of each feature map."}
{"task_id": 115, "completion_id": 0, "completion": "To implement batch normalization for a 4D NumPy array in the BCHW format, we need to normalize the input across the batch and spatial dimensions for each channel. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Calculate the mean and variance across the batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Apply scale (gamma) and shift (beta) parameters\n    X_scaled_shifted = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    \n    # Round the output to 4 decimal places and convert to list\n    return np.round(X_scaled_shifted, 4).tolist()\n\n# Example usage:\n# X = np.random.rand(2, 3, 4, 4)  # Example input with shape (batch_size=2, channels=3, height=4, width=4)\n# gamma = np.ones((3,))  # Scale parameter for each channel\n# beta = np.zeros((3,))  # Shift parameter for each channel\n# result = batch_normalization(X, gamma, beta)\n# print(result)\n```\n\n### Explanation:\n1. **Mean and Variance Calculation**: We compute the mean and variance across the batch and spatial dimensions (axis 0, 2, and 3) for each channel. The `keepdims=True` ensures that the mean and variance retain the dimensions for broadcasting.\n\n2. **Normalization**: The input is normalized using the formula `(X - mean) / sqrt(variance + epsilon)`, where `epsilon` is used for numerical stability to prevent division by zero.\n\n3. **Scaling and Shifting**: The normalized input is scaled by `gamma` and shifted by `beta`. The `reshape` method is used to ensure that `gamma` and `beta` are broadcast correctly across the batch and spatial dimensions.\n\n4. **Output Formatting**: The final output is rounded to 4 decimal places and converted to a list using `tolist()` to meet the output constraints."}
{"task_id": 116, "completion_id": 0, "completion": "To compute the derivative of a polynomial term of the form `c * x^n`, we can use the power rule from calculus. The power rule states that the derivative of `x^n` is `n * x^(n-1)`. Therefore, the derivative of `c * x^n` is `c * n * x^(n-1)`.\n\nHere's how you can implement the function:\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    # Calculate the derivative using the power rule\n    derivative_value = c * n * x**(n-1)\n    # Return the result rounded to 4 decimal places\n    return round(derivative_value, 4)\n\n# Example usage:\n# The derivative of 3 * x^2 at x = 2 is 3 * 2 * 2^(2-1) = 12\nprint(poly_term_derivative(3, 2, 2))  # Output: 12.0\n```\n\n### Explanation:\n- **Power Rule**: The derivative of `x^n` is `n * x^(n-1)`.\n- **Coefficient**: Multiply the result by the coefficient `c`.\n- **Rounding**: Use Python's `round()` function to ensure the result is rounded to 4 decimal places.\n\nThis function will correctly compute the derivative of a polynomial term at a given point `x` and return the result rounded to four decimal places."}
{"task_id": 117, "completion_id": 0, "completion": "To compute an orthonormal basis for a set of 2D vectors using the Gram-Schmidt process, we need to ensure that the vectors are orthogonal and normalized. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    def normalize(v):\n        norm = np.linalg.norm(v)\n        if norm < tol:\n            return None\n        return v / norm\n\n    def project(u, v):\n        return (np.dot(v, u) / np.dot(u, u)) * u\n\n    orthonormal_vectors = []\n    \n    for v in vectors:\n        v = np.array(v)\n        for u in orthonormal_vectors:\n            v = v - project(u, v)\n        \n        normalized_v = normalize(v)\n        if normalized_v is not None:\n            orthonormal_vectors.append(normalized_v)\n    \n    # Round the vectors to 4 decimal places and convert to list\n    orthonormal_vectors = [np.round(vec, 4).tolist() for vec in orthonormal_vectors]\n    \n    return orthonormal_vectors\n\n# Example usage:\nvectors = [[1, 1], [1, 0]]\northonormal_basis_result = orthonormal_basis(vectors)\nprint(orthonormal_basis_result)\n```\n\n### Explanation:\n1. **Normalization**: The `normalize` function checks if the vector's norm is above the tolerance and normalizes it if so. If the norm is below the tolerance, it returns `None`, indicating that the vector is effectively zero and should not be included in the basis.\n\n2. **Projection**: The `project` function computes the projection of vector `v` onto vector `u`.\n\n3. **Gram-Schmidt Process**: For each vector `v` in the input list:\n   - Subtract the projections of `v` onto each of the already computed orthonormal vectors.\n   - Normalize the resulting vector.\n   - If the normalized vector is not `None`, add it to the list of orthonormal vectors.\n\n4. **Output Formatting**: The orthonormal vectors are rounded to 4 decimal places and converted to lists before being returned.\n\nThis implementation ensures that the returned vectors are orthonormal and span the same subspace as the input vectors, while handling numerical precision issues with the given tolerance."}
{"task_id": 118, "completion_id": 0, "completion": "To compute the cross product of two 3-dimensional vectors in Python, we can utilize the `numpy` library, which provides a convenient function for this purpose. Below is the implementation of the `cross_product` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Ensure the input vectors are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product using numpy's cross function\n    cross_prod = np.cross(a, b)\n    \n    # Round the result to 4 decimal places\n    cross_prod_rounded = np.round(cross_prod, 4)\n    \n    # Convert the result to a list and return\n    return cross_prod_rounded.tolist()\n\n# Example usage:\n# vector_a = [1, 2, 3]\n# vector_b = [4, 5, 6]\n# result = cross_product(vector_a, vector_b)\n# print(result)  # Output: [-3.0, 6.0, -3.0]\n```\n\n### Explanation:\n1. **Input Conversion**: The input vectors `a` and `b` are converted to numpy arrays to facilitate mathematical operations.\n2. **Cross Product Calculation**: The `np.cross` function is used to compute the cross product of the two vectors.\n3. **Rounding**: The resulting vector is rounded to 4 decimal places using `np.round`.\n4. **Output Conversion**: The rounded numpy array is converted to a list using `tolist()` before returning, as specified in the task."}
{"task_id": 119, "completion_id": 0, "completion": "To solve a system of linear equations using Cramer's Rule, we need to calculate the determinant of the coefficient matrix \\( A \\) and the determinants of matrices formed by replacing each column of \\( A \\) with the constant vector \\( b \\). If the determinant of \\( A \\) is zero, the system does not have a unique solution, and we should return -1. Otherwise, we compute the solution vector \\( x \\) by dividing the determinants of these modified matrices by the determinant of \\( A \\).\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, return -1 (no unique solution)\n    if np.isclose(det_A, 0):\n        return -1\n    \n    # Number of variables\n    n = A.shape[0]\n    \n    # Initialize the solution vector\n    x = np.zeros(n)\n    \n    # Calculate the solution using Cramer's Rule\n    for i in range(n):\n        # Create a copy of A to modify\n        A_i = np.copy(A)\n        \n        # Replace the i-th column of A with b\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the modified matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the i-th variable in the solution\n        x[i] = det_A_i / det_A\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the solution as a list\n    return x.tolist()\n\n# Example usage:\nA = [[2, -1, 0], [1, 2, 1], [3, 0, 1]]\nb = [1, 2, 3]\nsolution = cramers_rule(A, b)\nprint(solution)\n```\n\n### Explanation:\n1. **Determinant Calculation**: We use `np.linalg.det()` to calculate the determinant of the matrix \\( A \\) and the modified matrices.\n2. **Check for Unique Solution**: If the determinant of \\( A \\) is zero (checked using `np.isclose()` to handle floating-point precision issues), the system does not have a unique solution, and we return -1.\n3. **Cramer's Rule**: For each variable \\( x_i \\), replace the \\( i \\)-th column of \\( A \\) with \\( b \\), compute the determinant of this new matrix, and divide by the determinant of \\( A \\) to find \\( x_i \\).\n4. **Rounding and Output**: The solution vector is rounded to four decimal places and returned as a list.\n\nThis implementation should work for any square matrix \\( A \\) and corresponding vector \\( b \\) that represent a system of linear equations."}
{"task_id": 120, "completion_id": 0, "completion": "To implement the Bhattacharyya distance between two discrete probability distributions, we need to follow these steps:\n\n1. **Check Input Validity**: Ensure both lists are non-empty and have the same length.\n2. **Calculate the Bhattacharyya Coefficient**: This is done by summing the square roots of the product of corresponding probabilities from the two distributions.\n3. **Compute the Bhattacharyya Distance**: The distance is derived from the coefficient using the formula: `-log(Bhattacharyya Coefficient)`.\n4. **Round the Result**: Round the computed distance to 4 decimal places.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the inputs are valid\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = sum(np.sqrt(p_i * q_i) for p_i, q_i in zip(p, q))\n    \n    # Calculate the Bhattacharyya distance\n    try:\n        distance = -np.log(bc)\n    except ValueError:\n        # In case bc is zero or negative, which should not happen with valid probabilities\n        return 0.0\n    \n    # Round the result to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\n# p = [0.1, 0.4, 0.5]\n# q = [0.2, 0.2, 0.6]\n# print(bhattacharyya_distance(p, q))  # Output should be rounded to 4 decimal places\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the lists `p` and `q` are of the same length and non-empty. If not, it returns `0.0`.\n- **Bhattacharyya Coefficient Calculation**: The coefficient is calculated using the formula `sum(sqrt(p_i * q_i))` for each pair of probabilities `p_i` and `q_i`.\n- **Distance Calculation**: The distance is computed as `-log(bc)`. If the coefficient is zero or negative, which is unlikely with valid probability distributions, the function returns `0.0`.\n- **Rounding**: The final result is rounded to 4 decimal places using Python's `round()` function."}
{"task_id": 121, "completion_id": 0, "completion": "To solve the task of computing the element-wise sum of two vectors, we need to ensure that both vectors have the same length. If they do, we can proceed to sum their corresponding elements. If not, we return -1 to indicate incompatible dimensions. Here's the implementation:\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [a[i] + b[i] for i in range(len(a))]\n    \n    return result\n```\n\n### Explanation:\n1. **Length Check**: We first check if the lengths of the two vectors `a` and `b` are equal. If they are not, we return `-1` to indicate that the operation is invalid due to incompatible dimensions.\n\n2. **Element-wise Sum**: If the vectors have the same length, we use a list comprehension to iterate over the indices of the vectors and compute the sum of corresponding elements. The result is stored in a new list `result`.\n\n3. **Return Result**: Finally, we return the `result` list containing the element-wise sums."}
{"task_id": 122, "completion_id": 0, "completion": "To implement the policy gradient estimator using the REINFORCE algorithm, we need to compute the gradient of the log-policy weighted by the return for each time step in the episodes. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    # Initialize the gradient to zero\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Compute the return for each time step\n        G = 0\n        returns = []\n        for _, _, reward in reversed(episode):\n            G = reward + G\n            returns.insert(0, G)\n        \n        # Compute the gradient for each step in the episode\n        for t, (state, action, _) in enumerate(episode):\n            # Compute the policy using softmax\n            policy = softmax(theta[state, :])\n            \n            # Compute the gradient of the log-policy\n            grad_log_policy = np.zeros_like(theta)\n            grad_log_policy[state, action] = 1\n            grad_log_policy[state, :] -= policy\n            \n            # Update the gradient with the return\n            gradient += grad_log_policy * returns[t]\n    \n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round the results to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    \n    # Return the gradient as a list\n    return gradient.tolist()\n\n# Example usage:\ntheta = np.array([[0.2, 0.5, 0.3], [0.1, 0.7, 0.2]])\nepisodes = [\n    [(0, 1, 1.0), (1, 2, 0.5)],\n    [(0, 0, 0.5), (1, 1, 1.0)]\n]\ngradient = compute_policy_gradient(theta, episodes)\nprint(gradient)\n```\n\n### Explanation:\n1. **Softmax Function**: Computes the softmax probabilities for actions given the state.\n2. **Return Calculation**: For each episode, compute the return `G` for each time step by summing rewards from the end to the current step.\n3. **Gradient Calculation**: For each step in the episode, compute the gradient of the log-policy and multiply it by the return.\n4. **Gradient Averaging**: Average the gradient over all episodes.\n5. **Rounding**: Round the gradient values to the nearest 4th decimal.\n6. **Output**: Convert the gradient to a list using `tolist()` for the final output."}
{"task_id": 123, "completion_id": 0, "completion": "To calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer, we need to compute the floating-point operations (FLOPs) for both types of layers and then determine the percentage savings. Here's how you can implement this in Python:\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    n_experts (int): Number of experts in the MoE layer.\n    k_active (int): Number of active experts in the MoE layer.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n\n    Returns:\n    float: Percentage savings in FLOPs.\n    \"\"\"\n    # FLOPs for a dense layer\n    flops_dense = d_in * d_out\n\n    # FLOPs for an MoE layer\n    # Each active expert contributes to the computation\n    flops_moe = k_active * (d_in * d_out)\n\n    # Calculate savings\n    savings = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    # Round the result to the nearest 1th decimal\n    savings_rounded = round(savings, 1)\n\n    return savings_rounded\n\n# Example usage:\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 1024\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational efficiency savings: {savings_percentage}%\")\n```\n\n### Explanation:\n1. **Dense Layer FLOPs**: The number of FLOPs for a dense layer is calculated as the product of the input dimension (`d_in`) and the output dimension (`d_out`).\n\n2. **MoE Layer FLOPs**: In an MoE layer, only a subset of experts (`k_active`) are active, so the FLOPs are calculated as `k_active * (d_in * d_out)`.\n\n3. **Savings Calculation**: The savings percentage is calculated by comparing the FLOPs of the dense layer and the MoE layer, and then converting this difference into a percentage.\n\n4. **Rounding**: The result is rounded to the nearest 1th decimal to meet the output constraints.\n\nThis function will give you the computational efficiency savings when using an MoE layer compared to a dense layer, based on the specified parameters."}
{"task_id": 124, "completion_id": 0, "completion": "To implement the Noisy Top-K gating function, we need to follow these steps:\n\n1. **Compute the Logits**: Multiply the input matrix `X` with the weight matrix `W_g` to get the logits.\n2. **Add Noise**: Add the pre-sampled noise `N` to the logits. The noise is scaled by the weight matrix `W_noise`.\n3. **Select Top-K**: For each row in the resulting matrix, select the top `k` values.\n4. **Compute Gating Probabilities**: Use a softmax function on the selected top-k values to compute the gating probabilities.\n5. **Return the Result**: Ensure the results are rounded to the nearest 4th decimal and return them as a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Step 1: Compute the logits\n    logits = np.dot(X, W_g)\n    \n    # Step 2: Add noise\n    noisy_logits = logits + np.dot(N, W_noise)\n    \n    # Step 3: Select top-k values\n    topk_indices = np.argsort(noisy_logits, axis=1)[:, -k:]\n    \n    # Create a mask for top-k values\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    np.put_along_axis(mask, topk_indices, True, axis=1)\n    \n    # Step 4: Compute gating probabilities\n    # Mask the logits to only keep top-k values\n    topk_logits = np.where(mask, noisy_logits, -np.inf)\n    \n    # Apply softmax to the top-k logits\n    max_logits = np.max(topk_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(topk_logits - max_logits)\n    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n    gating_probs = exp_logits / sum_exp_logits\n    \n    # Step 5: Round the results to the nearest 4th decimal\n    gating_probs_rounded = np.round(gating_probs, 4)\n    \n    # Return the result as a list\n    return gating_probs_rounded.tolist()\n\n# Example usage:\n# X = np.array([[...], [...]])\n# W_g = np.array([[...], [...]])\n# W_noise = np.array([[...], [...]])\n# N = np.array([[...], [...]])\n# k = 2\n# result = noisy_topk_gating(X, W_g, W_noise, N, k)\n# print(result)\n```\n\n### Explanation:\n- **Logits Calculation**: The logits are calculated by multiplying the input matrix `X` with the weight matrix `W_g`.\n- **Noise Addition**: The noise matrix `N` is scaled by `W_noise` and added to the logits to introduce randomness.\n- **Top-K Selection**: We use `np.argsort` to find the indices of the top `k` values for each row. A boolean mask is created to zero out all but the top-k values.\n- **Softmax Calculation**: The softmax function is applied to the top-k values to convert them into probabilities.\n- **Rounding and Output**: The probabilities are rounded to four decimal places and converted to a list for the final output."}
{"task_id": 125, "completion_id": 0, "completion": "To implement a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing, we need to follow these steps:\n\n1. Compute the gating scores using the gating weight matrix.\n2. Apply softmax to the gating scores to get gating probabilities.\n3. Select the top-k experts based on the gating probabilities for each token.\n4. Compute the output of the selected experts.\n5. Aggregate the results weighted by the normalized gating probabilities.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Step 1: Compute gating scores\n    gating_scores = np.dot(x, Wg)\n    \n    # Step 2: Apply softmax to get gating probabilities\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    \n    # Step 3: Select top-k experts based on gating probabilities\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    \n    # Initialize the output tensor\n    output = np.zeros((x.shape[0], We.shape[1]))\n    \n    # Step 4: Compute the output of the selected experts\n    for i in range(x.shape[0]):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            expert_output = np.dot(x[i], We[expert_idx])\n            output[i] += gating_probs[i, expert_idx] * expert_output\n    \n    # Step 5: Round results to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Return the output as a list\n    return output.tolist()\n\n# Example usage:\n# x = np.random.rand(5, 10)  # Input tensor with 5 tokens and 10 features\n# We = np.random.rand(3, 10, 10)  # 3 experts, each with a 10x10 weight matrix\n# Wg = np.random.rand(10, 3)  # Gating weight matrix for 3 experts\n# n_experts = 3\n# top_k = 2\n# result = moe(x, We, Wg, n_experts, top_k)\n# print(result)\n```\n\n### Explanation:\n- **Gating Scores**: We compute the gating scores by multiplying the input tensor `x` with the gating weight matrix `Wg`.\n- **Softmax**: We apply the softmax function to the gating scores to obtain the gating probabilities.\n- **Top-k Selection**: We select the indices of the top-k experts for each token based on the gating probabilities.\n- **Expert Output**: For each token, we compute the output of the selected experts and aggregate them using the gating probabilities.\n- **Rounding**: We round the final output to the nearest 4th decimal.\n- **Output**: The final output is converted to a list using `tolist()` method."}
{"task_id": 126, "completion_id": 0, "completion": "To implement Group Normalization, we need to divide the channels into groups, compute the mean and variance for each group, normalize the values, and then apply the learned scale (`gamma`) and shift (`beta`). Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n    \n    # Ensure the number of groups divides the number of channels evenly\n    assert C % num_groups == 0, \"Number of groups must divide the number of channels evenly\"\n    \n    # Calculate the number of channels per group\n    channels_per_group = C // num_groups\n    \n    # Reshape the input tensor to (B, num_groups, channels_per_group, H, W)\n    X = X.reshape(B, num_groups, channels_per_group, H, W)\n    \n    # Compute the mean and variance for each group\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    variance = X.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize the input tensor\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Reshape back to the original shape (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply the scale (gamma) and shift (beta)\n    out = gamma * X_normalized + beta\n    \n    # Round the results to the nearest 4th decimal\n    out = np.round(out, 4)\n    \n    # Convert the output to a list\n    return out.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 4, 4, 4)  # Example input tensor\n# gamma = np.ones((4, 1, 1))      # Example gamma\n# beta = np.zeros((4, 1, 1))      # Example beta\n# num_groups = 2                  # Number of groups\n# result = group_normalization(X, gamma, beta, num_groups)\n# print(result)\n```\n\n### Explanation:\n- **Reshape**: The input tensor is reshaped to group the channels. This allows us to compute the mean and variance for each group.\n- **Normalization**: We normalize each group using the computed mean and variance.\n- **Reshape Back**: After normalization, we reshape the tensor back to its original shape.\n- **Scale and Shift**: We apply the learned scale (`gamma`) and shift (`beta`) to the normalized tensor.\n- **Rounding**: The output is rounded to the nearest 4th decimal.\n- **Conversion to List**: Finally, the output is converted to a Python list using `tolist()`.\n\nThis implementation assumes that `gamma` and `beta` are provided with the correct shape to match the number of channels."}
{"task_id": 127, "completion_id": 0, "completion": "To find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\), we can use a gradient descent approach. This method involves iteratively updating the value of \\( x \\) by moving in the direction of the negative gradient of the function, which points towards the minimum.\n\nHere's how you can implement the `find_treasure` function using gradient descent:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Derivative of the function f(x) = x^4 - 3x^3 + 2\n    # f'(x) = 4x^3 - 9x^2\n    def derivative(x):\n        return 4 * x**3 - 9 * x**2\n    \n    x = start_x\n    for _ in range(max_iters):\n        grad = derivative(x)\n        new_x = x - learning_rate * grad\n        \n        # Check for convergence\n        if abs(new_x - x) < tolerance:\n            break\n        \n        x = new_x\n    \n    return round(x, 4)\n\n# Example usage:\n# start_x can be any random initial position\nresult = find_treasure(start_x=0.0)\nprint(\"The treasure is located at x =\", result)\n```\n\n### Explanation:\n1. **Derivative Calculation**: The derivative of the function \\( f(x) = x^4 - 3x^3 + 2 \\) is \\( f'(x) = 4x^3 - 9x^2 \\). This derivative is used to determine the direction and magnitude of the step to take in each iteration.\n\n2. **Gradient Descent Loop**: \n   - Start with an initial guess `start_x`.\n   - Compute the gradient (derivative) at the current point.\n   - Update the current point by moving in the direction of the negative gradient.\n   - Check if the change in \\( x \\) is smaller than the specified tolerance, indicating convergence.\n\n3. **Convergence Check**: The loop stops when the change in \\( x \\) is less than the specified tolerance or when the maximum number of iterations is reached.\n\n4. **Rounding**: The result is rounded to the nearest 4th decimal as required.\n\nThis implementation should efficiently find the minimum of the function, which corresponds to the location of Captain Redbeard's hidden treasure."}
{"task_id": 128, "completion_id": 0, "completion": "To implement the Dynamic Tanh (DyT) function, we need to create a transformation inspired by the Tanh function that can be used in place of layer normalization in Transformer architectures. The function should take an input array `x` and parameters `alpha`, `gamma`, and `beta`, and return a list of transformed values rounded to the nearest 4th decimal.\n\nHere's how you can implement the `dynamic_tanh` function:\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the dynamic tanh transformation\n    # The transformation is inspired by the Tanh function and uses the parameters alpha, gamma, and beta\n    transformed = gamma * np.tanh(alpha * x + beta)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_transformed = np.round(transformed, 4)\n    \n    # Convert the numpy array to a list\n    result_list = rounded_transformed.tolist()\n    \n    return result_list\n\n# Example usage:\nx = np.array([0.1, 0.5, -0.3, 0.7])\nalpha = 1.0\ngamma = 1.0\nbeta = 0.0\nprint(dynamic_tanh(x, alpha, gamma, beta))\n```\n\n### Explanation:\n1. **Transformation**: The function applies a transformation using the Tanh function, scaled by `gamma` and shifted by `beta`. The parameter `alpha` scales the input `x`.\n\n2. **Rounding**: The results are rounded to the nearest 4th decimal using `np.round`.\n\n3. **Conversion to List**: The transformed and rounded numpy array is converted to a Python list using the `tolist()` method.\n\nThis implementation should meet the requirements of the task by providing a normalization-free transformation that can be used in Transformer architectures."}
