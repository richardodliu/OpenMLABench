{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    # Check if the matrix is empty\n    if not a:\n        return -1\n\n    # Number of columns in the matrix\n    num_cols = len(a[0])\n\n    # Length of the vector\n    len_b = len(b)\n\n    # Check dimension compatibility\n    if num_cols != len_b:\n        return -1\n\n    result = []\n    for row in a:\n        # Compute the dot product of the current row with vector b\n        dot_product = sum(row[i] * b[i] for i in range(num_cols))\n        result.append(dot_product)\n\n    return result\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transpose the given matrix.\n\n    Parameters:\n    a (list of lists): The matrix to transpose, where each sublist represents a row.\n\n    Returns:\n    list of lists: The transposed matrix, where rows are converted to columns and vice versa.\n    \"\"\"\n    # Use zip to pair elements from each row, then convert each pair to a list\n    return [list(row) for row in zip(*a)]\n```\n\n### Example Usage\n\n```python\n# Original matrix\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\n# Transpose the matrix\ntransposed = transpose_matrix(matrix)\n\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\n\nprint(\"\\nTransposed Matrix:\")\nfor row in transposed:\n    print(row)\n```\n\n### Output\n```\nOriginal Matrix:\n[1, 2, 3]\n[4, 5, 6]\n\nTransposed Matrix:\n[1, 4]\n[2, 5]\n[3, 6]\n```\n\n### Explanation\n\n- **zip(*a)**: The asterisk `*` is used to unpack the rows of the matrix so that each row becomes an argument to `zip`. This pairs elements from each row together.\n- **[list(row) for row in zip(*a)]**: Converts each zipped tuple into a list to form the transposed matrix.\n\nThis method is efficient and concise, leveraging Python's built-in functions to achieve the matrix transpose without the need for explicit loops."}
{"task_id": 3, "completion_id": 0, "completion": "Sure! Below is the Python function that reshapes a given matrix using NumPy. If the reshaping is not possible due to mismatched dimensions, it returns an empty list `[]`.\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    try:\n        np_array = np.array(a)\n        reshaped = np_array.reshape(new_shape)\n        return reshaped.tolist()\n    except (ValueError, TypeError):\n        return []\n```\n\n### **Explanation:**\n\n1. **Convert to NumPy Array:**\n   - The input list of lists `a` is converted into a NumPy array using `np.array(a)`.\n\n2. **Attempt to Reshape:**\n   - The `reshape` method is used to attempt to change the shape of the array to `new_shape`.\n   - If the total number of elements matches, the reshaping is successful.\n\n3. **Handle Exceptions:**\n   - If the total number of elements does not match the desired `new_shape`, a `ValueError` is raised.\n   - If the input contains invalid data types, a `TypeError` might be raised.\n   - In both cases, the function catches the exception and returns an empty list `[]`.\n\n4. **Convert Back to List:**\n   - If reshaping is successful, the NumPy array is converted back to a regular Python list using the `tolist()` method before returning.\n\n### **Example Usage:**\n\n```python\n# Successful reshape\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\nnew_shape = (3, 2)\nprint(reshape_matrix(matrix, new_shape))\n# Output: [[1, 2], [3, 4], [5, 6]]\n\n# Unsuccessful reshape\nnew_shape = (4, 2)\nprint(reshape_matrix(matrix, new_shape))\n# Output: []\n```\n\nThis function efficiently handles the reshaping of matrices while gracefully managing cases where reshaping isn't feasible."}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    Parameters:\n    - matrix (list of lists of floats): The input matrix.\n    - mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    - list of floats: A list containing the mean of each row or column.\n\n    Raises:\n    - ValueError: If the mode is not 'row' or 'column' or if the matrix is empty\n                  or if rows have inconsistent lengths when mode is 'column'.\n    \"\"\"\n    if not matrix:\n        raise ValueError(\"The matrix is empty.\")\n\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n\n    # Calculate mean by row\n    if mode == 'row':\n        means = []\n        for idx, row in enumerate(matrix):\n            if not row:\n                raise ValueError(f\"Row {idx} is empty.\")\n            row_mean = sum(row) / len(row)\n            means.append(row_mean)\n        return means\n\n    # Calculate mean by column\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n\n        # Ensure all rows have the same number of columns\n        for idx, row in enumerate(matrix):\n            if len(row) != num_cols:\n                raise ValueError(f\"Row {idx} does not have {num_cols} columns.\")\n\n        # Initialize a list to store the sum of each column\n        column_sums = [0.0] * num_cols\n\n        for row in matrix:\n            for col_idx, value in enumerate(row):\n                column_sums[col_idx] += value\n\n        # Calculate mean for each column\n        column_means = [total / num_rows for total in column_sums]\n        return column_means\n\n# Example Usage:\nif __name__ == \"__main__\":\n    sample_matrix = [\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ]\n\n    row_means = calculate_matrix_mean(sample_matrix, 'row')\n    print(\"Row means:\", row_means)  # Output: [2.0, 5.0, 8.0]\n\n    column_means = calculate_matrix_mean(sample_matrix, 'column')\n    print(\"Column means:\", column_means)  # Output: [4.0, 5.0, 6.0]\n```\n\n**Explanation:**\n\n1. **Input Validation:**\n   - The function first checks if the matrix is empty and raises a `ValueError` if it is.\n   - It then checks if the `mode` parameter is either `'row'` or `'column'`. If not, it raises a `ValueError`.\n\n2. **Calculating Row Means:**\n   - If the mode is `'row'`, the function iterates through each row.\n   - It checks if the row is empty and raises a `ValueError` if it is.\n   - It calculates the mean of each row by summing its elements and dividing by the number of elements.\n   - The means are collected in a list and returned.\n\n3. **Calculating Column Means:**\n   - If the mode is `'column'`, the function first ensures that all rows have the same number of columns. If not, it raises a `ValueError`.\n   - It initializes a list to hold the sum of each column.\n   - It iterates through each row and accumulates the sums for each column.\n   - After summing, it calculates the mean for each column by dividing the sum by the number of rows.\n   - The column means are collected in a list and returned.\n\n4. **Example Usage:**\n   - An example matrix is provided to demonstrate how to use the `calculate_matrix_mean` function.\n   - It prints out the means calculated by row and by column.\n\nThis function is designed to be robust and handle potential edge cases, ensuring accurate mean calculations based on the specified mode."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by a scalar.\n\n    Args:\n        matrix (list of list of int|float): The input matrix to be multiplied.\n        scalar (int|float): The scalar value to multiply each matrix element by.\n\n    Returns:\n        list of list of int|float: A new matrix with each element multiplied by the scalar.\n\n    Raises:\n        TypeError: If the input matrix is not a list of lists of numbers or if the scalar is not a number.\n    \"\"\"\n    if not isinstance(matrix, list):\n        raise TypeError(\"Matrix must be a list of lists.\")\n    \n    result = []\n    for row in matrix:\n        if not isinstance(row, list):\n            raise TypeError(\"Matrix must be a list of lists.\")\n        multiplied_row = []\n        for element in row:\n            if not isinstance(element, (int, float)):\n                raise TypeError(\"Matrix elements must be integers or floats.\")\n            multiplied_element = element * scalar\n            multiplied_row.append(multiplied_element)\n        result.append(multiplied_row)\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 3\n    multiplied_matrix = scalar_multiply(matrix, scalar)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(f\"\\nMatrix after multiplying by {scalar}:\")\n    for row in multiplied_matrix:\n        print(row)\n```\n\n### Explanation\n\n1. **Function Definition**:\n    - The function `scalar_multiply` takes two arguments:\n        - `matrix`: A list of lists containing integers or floats.\n        - `scalar`: An integer or float by which each element of the matrix will be multiplied.\n    - It returns a new matrix with each element multiplied by the scalar.\n\n2. **Type Checking**:\n    - The function first checks if the `matrix` is a list. If not, it raises a `TypeError`.\n    - It then iterates through each `row` in the matrix to ensure that each row is also a list.\n    - Within each row, it checks if each `element` is either an integer or a float. If any element fails this check, a `TypeError` is raised.\n\n3. **Multiplication**:\n    - For each valid element, the function multiplies it by the scalar and appends the result to a new row.\n    - After processing all elements in a row, the new row is appended to the result matrix.\n\n4. **Example Usage**:\n    - An example is provided to demonstrate how the function works.\n    - It prints both the original and the multiplied matrices for clarity.\n\n### Output of the Example\n\n```\nOriginal Matrix:\n[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n\nMatrix after multiplying by 3:\n[3, 6, 9]\n[12, 15, 18]\n[21, 24, 27]\n```\n\nThis function ensures that the input matrix is properly formatted and that all elements are numeric before performing the scalar multiplication. It returns a new matrix without modifying the original one."}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List, Union\n\ndef calculate_eigenvalues(matrix: List[List[Union[float, int]]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Parameters:\n    matrix (List[List[Union[float, int]]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    List[float]: A list containing the eigenvalues sorted from highest to lowest.\n\n    Raises:\n    ValueError: If the input is not a 2x2 matrix or if the eigenvalues are complex.\n    \"\"\"\n    # Validate the input matrix\n    if (\n        not isinstance(matrix, list) or\n        len(matrix) != 2 or\n        not all(isinstance(row, list) and len(row) == 2 for row in matrix)\n    ):\n        raise ValueError(\"Input must be a 2x2 matrix represented as a list of two lists, each containing two numbers.\")\n\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the trace and determinant\n    trace = a + d\n    determinant = a * d - b * c\n\n    # Calculate the discriminant\n    discriminant = trace ** 2 - 4 * determinant\n\n    if discriminant < 0:\n        raise ValueError(\"The matrix has complex eigenvalues.\")\n\n    sqrt_discriminant = math.sqrt(discriminant)\n\n    # Calculate the two eigenvalues\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n\n    # Sort the eigenvalues from highest to lowest\n    eigenvalues = sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n    return eigenvalues\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a 2x2 matrix\n    matrix = [\n        [4, 2],\n        [1, 3]\n    ]\n\n    try:\n        eigenvalues = calculate_eigenvalues(matrix)\n        print(f\"Eigenvalues: {eigenvalues}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    # Another example with distinct eigenvalues\n    matrix2 = [\n        [2, 1],\n        [1, 2]\n    ]\n\n    try:\n        eigenvalues2 = calculate_eigenvalues(matrix2)\n        print(f\"Eigenvalues: {eigenvalues2}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    # Example with complex eigenvalues\n    matrix3 = [\n        [0, -1],\n        [1, 0]\n    ]\n\n    try:\n        eigenvalues3 = calculate_eigenvalues(matrix3)\n        print(f\"Eigenvalues: {eigenvalues3}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```\n\n### Explanation:\n\n1. **Input Validation**:\n   - The function first checks if the input `matrix` is a list of two lists, each containing exactly two numerical elements (either `int` or `float`). If not, it raises a `ValueError`.\n\n2. **Calculating Trace and Determinant**:\n   - For a 2x2 matrix:\n     \\[\n     \\text{Trace} = a + d\n     \\]\n     \\[\n     \\text{Determinant} = ad - bc\n     \\]\n   \n3. **Discriminant Calculation**:\n   - The discriminant is computed as:\n     \\[\n     \\text{Discriminant} = (\\text{Trace})^2 - 4 \\times \\text{Determinant}\n     \\]\n   - If the discriminant is negative, the eigenvalues are complex, and since the function is designed to return real numbers, it raises a `ValueError`.\n\n4. **Eigenvalue Calculation**:\n   - The eigenvalues are calculated using the quadratic formula:\n     \\[\n     \\lambda = \\frac{\\text{Trace} \\pm \\sqrt{\\text{Discriminant}}}{2}\n     \\]\n   - The resulting eigenvalues are sorted in descending order before being returned.\n\n5. **Example Usage**:\n   - Three example matrices are provided:\n     - The first has distinct real eigenvalues.\n     - The second also has distinct real eigenvalues.\n     - The third matrix results in complex eigenvalues, demonstrating error handling.\n\n### Sample Output:\n\n```\nEigenvalues: [5.0, 2.0]\nEigenvalues: [3.0, 1.0]\nError: The matrix has complex eigenvalues.\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Transforms the matrix A using the operation T^{-1} * A * S.\n\n    Parameters:\n    - A (list of lists): The matrix to be transformed.\n    - T (list of lists): The invertible matrix to be inverted and multiplied from the left.\n    - S (list of lists): The invertible matrix to be multiplied from the right.\n\n    Returns:\n    - Transformed matrix as a list of lists with elements rounded to 4 decimal places.\n    - Returns -1 if T or S is not invertible or if matrix dimensions are incompatible.\n    \"\"\"\n    try:\n        # Convert input lists to NumPy arrays with dtype float for precision\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n\n        # Check if T and S are square matrices\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n\n        # Check if A's dimensions are compatible with T and S\n        if T_np.shape[0] != A_np.shape[0] or A_np.shape[1] != S_np.shape[1]:\n            return -1\n\n        # Compute the inverse of T\n        T_inv = np.linalg.inv(T_np)\n\n        # Compute the transformation T^{-1} * A * S\n        transformed = T_inv @ A_np @ S_np\n\n        # Round the result to 4 decimal places\n        transformed_rounded = np.round(transformed, 4)\n\n        # Convert the NumPy array back to a Python list\n        return transformed_rounded.tolist()\n\n    except np.linalg.LinAlgError:\n        # This exception is raised if T or S is not invertible\n        return -1\n    except Exception:\n        # Catch any other exceptions (e.g., incorrect input formats)\n        return -1\n```\n\n### Explanation:\n\n1. **Importing NumPy**: The function begins by importing the NumPy library, which is essential for matrix operations.\n\n2. **Function Definition**: \n    - The function `transform_matrix` takes three parameters:\n        - `A`: The matrix to be transformed.\n        - `T`: An invertible matrix to be inverted and multiplied from the left.\n        - `S`: An invertible matrix to be multiplied from the right.\n    - The return type is annotated to be either a list of lists containing integers or floats, or an integer `-1` in case of failure.\n\n3. **Converting Lists to NumPy Arrays**:\n    - The input lists `A`, `T`, and `S` are converted to NumPy arrays with a `float` data type to ensure precision during calculations.\n\n4. **Validation Checks**:\n    - **Square Matrices**: The function checks if both `T` and `S` are square matrices since only square matrices are invertible.\n    - **Dimension Compatibility**: It ensures that the number of rows in `T` matches the number of rows in `A`, and the number of columns in `A` matches the number of columns in `S`. This is necessary for the matrix multiplication to be valid.\n\n5. **Inversion and Transformation**:\n    - **Inverse of T**: The inverse of matrix `T` is computed using `np.linalg.inv(T_np)`.\n    - **Matrix Multiplication**: The transformation \\( T^{-1}AS \\) is performed using the `@` operator, which denotes matrix multiplication in NumPy.\n\n6. **Rounding and Conversion**:\n    - The resulting matrix is rounded to four decimal places using `np.round`.\n    - The NumPy array is then converted back to a Python list of lists using `tolist()` for the final output.\n\n7. **Exception Handling**:\n    - **LinAlgError**: If `T` or `S` is not invertible, `np.linalg.inv` will raise a `LinAlgError`, and the function will return `-1`.\n    - **General Exceptions**: Any other exceptions (such as incorrect input formats) will also result in the function returning `-1`.\n\n### Example Usage:\n\n```python\n# Example matrices\nA = [\n    [1, 2],\n    [3, 4]\n]\nT = [\n    [2, 0],\n    [0, 2]\n]\nS = [\n    [1, 0],\n    [0, 1]\n]\n\nresult = transform_matrix(A, T, S)\nprint(result)  # Output: [[0.5, 1.0], [1.5, 2.0]]\n```\n\nIn this example, since `T` and `S` are invertible and the dimensions are compatible, the function successfully returns the transformed matrix with elements rounded to four decimal places."}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n\n    Parameters:\n    matrix (list of list of float): A 2x2 matrix represented as a list of two lists, each containing two floats.\n\n    Returns:\n    list of list of float or None: The inverse matrix if it exists, otherwise None.\n    \"\"\"\n    # Check if the matrix is 2x2\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        print(\"Error: The matrix must be 2x2.\")\n        return None\n\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the determinant\n    determinant = a * d - b * c\n\n    if determinant == 0:\n        # The matrix is not invertible\n        print(\"Error: The matrix is not invertible (determinant is 0).\")\n        return None\n\n    # Compute the inverse using the formula:\n    # Inverse of [[a, b], [c, d]] is (1/det) * [[d, -b], [-c, a]]\n    inverse = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n\n    return inverse\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [4, 7],\n        [2, 6]\n    ]\n\n    inv = inverse_2x2(matrix)\n    if inv:\n        print(\"Inverse matrix:\")\n        for row in inv:\n            print(row)\n    else:\n        print(\"The provided matrix is not invertible.\")\n```\n\n**Explanation:**\n\n1. **Input Validation:**\n   - The function first checks if the input `matrix` is a 2x2 matrix. If not, it prints an error message and returns `None`.\n\n2. **Determinant Calculation:**\n   - For a 2x2 matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), the determinant is calculated as \\(ad - bc\\).\n   - If the determinant is zero, the matrix is not invertible, and the function returns `None`.\n\n3. **Inverse Calculation:**\n   - If the determinant is non-zero, the inverse is calculated using the formula:\n     \\[\n     \\text{Inverse} = \\frac{1}{\\text{det}} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n     \\]\n   - The function returns the computed inverse as a new 2x2 matrix.\n\n4. **Example Usage:**\n   - An example is provided to demonstrate how to use the `inverse_2x2` function. It attempts to invert the matrix \\(\\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\) and prints the result.\n\n**Output of the Example:**\n```\nInverse matrix:\n[0.6, -0.7]\n[-0.2, 0.4]\n```\n\nThis output corresponds to the inverse of the provided matrix. If you input a matrix with a determinant of zero, the function will notify you that the matrix is not invertible."}
{"task_id": 9, "completion_id": 0, "completion": "Certainly! Below is a Python function that multiplies two matrices. The function checks whether the matrices can be multiplied based on their dimensions. If they cannot be multiplied, it returns `-1`.\n\n```python\ndef matrixmul(a: list[list[int | float]],\n             b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if either matrix is empty\n    if not a or not b:\n        return -1\n\n    # Number of columns in matrix 'a'\n    num_cols_a = len(a[0])\n    # Number of rows in matrix 'b'\n    num_rows_b = len(b)\n\n    # Verify that all rows in 'a' have the same number of columns\n    if not all(len(row) == num_cols_a for row in a):\n        return -1\n\n    # Verify that all rows in 'b' have the same number of columns\n    num_cols_b = len(b[0])\n    if not all(len(row) == num_cols_b for row in b):\n        return -1\n\n    # Check if the number of columns in 'a' equals the number of rows in 'b'\n    if num_cols_a != num_rows_b:\n        return -1\n\n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(num_cols_b)] for _ in range(len(a))]\n\n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n\n    return result\n```\n\n### How It Works\n\n1. **Dimension Check**:  \n   - The number of columns in matrix `a` must equal the number of rows in matrix `b` for multiplication to be possible.\n   - The function first checks if either matrix is empty.\n   - It then verifies that all rows in both matrices have consistent lengths.\n   - If these conditions aren't met, it returns `-1`.\n\n2. **Initialization of Result Matrix**:  \n   - If the matrices can be multiplied, a result matrix of appropriate dimensions is initialized with all elements set to zero.\n\n3. **Multiplication Process**:  \n   - It uses three nested loops to compute the product.\n   - The outer two loops iterate over the rows of `a` and columns of `b`.\n   - The innermost loop performs the dot product of the corresponding row from `a` and column from `b`.\n\n### Example Usage\n\n```python\n# Example matrices\nA = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\nB = [\n    [7, 8],\n    [9, 10],\n    [11, 12]\n]\n\n# Multiply matrices\nresult = matrixmul(A, B)\n\n# Output the result\nprint(result)\n```\n\n**Expected Output:**\n```\n[[58, 64], [139, 154]]\n```\n\n### Handling Invalid Multiplications\n\nIf you try to multiply matrices with incompatible dimensions, the function will return `-1`. For example:\n\n```python\n# Incompatible matrices\nA = [\n    [1, 2],\n    [3, 4]\n]\n\nB = [\n    [5, 6],\n    [7, 8],\n    [9, 10]\n]\n\n# Attempt to multiply incompatible matrices\nresult = matrixmul(A, B)\n\nprint(result)  # Output: -1\n```\n\n## Additional Tips\n\n- **Input Validation**: Ensure that the input matrices are well-formed (i.e., all rows have the same number of columns) before attempting multiplication.\n- **Performance Considerations**: For very large matrices, consider using optimized libraries like NumPy, which are highly efficient for matrix operations.\n\n---\n\nFeel free to integrate this function into your projects. If you have any further questions or need additional features, let me know!"}
{"task_id": 10, "completion_id": 0, "completion": "```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors (list of list of float): A list where each inner list represents a feature\n            with its observations.\n\n    Returns:\n        list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    num_features = len(vectors)\n    if num_features == 0:\n        return []\n\n    num_observations = len(vectors[0])\n    if any(len(feature) != num_observations for feature in vectors):\n        raise ValueError(\"All vectors must have the same number of observations.\")\n\n    # Calculate means of each feature\n    means = [sum(feature) / num_observations for feature in vectors]\n\n    # Initialize covariance matrix with zeros\n    covariance_matrix = [[0.0 for _ in range(num_features)] for _ in range(num_features)]\n\n    # Calculate covariance for each pair of features\n    for i in range(num_features):\n        for j in range(i, num_features):\n            covariance = 0.0\n            for k in range(num_observations):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= (num_observations - 1)\n            covariance_matrix[i][j] = covariance\n            covariance_matrix[j][i] = covariance  # Covariance matrix is symmetric\n\n    return covariance_matrix\n\n# Test Cases\nif __name__ == \"__main__\":\n    def print_matrix(matrix):\n        for row in matrix:\n            print([\"{0:.2f}\".format(elem) for elem in row])\n        print()\n\n    # Test Case 1: Simple 2D data\n    vectors1 = [\n        [2.1, 2.5, 3.6],  # Feature 1\n        [8, 10, 12]        # Feature 2\n    ]\n    expected1 = [\n        [0.65, 2.30],\n        [2.30, 8.00]\n    ]\n    result1 = calculate_covariance_matrix(vectors1)\n    print(\"Test Case 1:\")\n    print_matrix(result1)\n    assert all(\n        abs(result1[i][j] - expected1[i][j]) < 1e-2\n        for i in range(len(expected1))\n        for j in range(len(expected1))\n    ), \"Test Case 1 Failed\"\n\n    # Test Case 2: 3D data with more observations\n    vectors2 = [\n        [4.0, 4.2, 3.9, 4.3, 4.1],  # Feature 1\n        [2.0, 2.1, 1.9, 2.2, 2.0],  # Feature 2\n        [0.6, 0.59, 0.58, 0.62, 0.61]  # Feature 3\n    ]\n    expected2 = [\n        [0.025, 0.015, 0.0015],\n        [0.015, 0.025, 0.0015],\n        [0.0015, 0.0015, 0.0001]\n    ]\n    result2 = calculate_covariance_matrix(vectors2)\n    print(\"Test Case 2:\")\n    print_matrix(result2)\n    # Due to floating point precision, use a tolerance for assertion\n    assert all(\n        abs(result2[i][j] - expected2[i][j]) < 1e-3\n        for i in range(len(expected2))\n        for j in range(len(expected2))\n    ), \"Test Case 2 Failed\"\n\n    # Test Case 3: Single feature\n    vectors3 = [\n        [1, 2, 3, 4, 5]\n    ]\n    expected3 = [\n        [2.5]\n    ]\n    result3 = calculate_covariance_matrix(vectors3)\n    print(\"Test Case 3:\")\n    print_matrix(result3)\n    assert result3 == expected3, \"Test Case 3 Failed\"\n\n    # Test Case 4: Empty input\n    vectors4 = []\n    expected4 = []\n    result4 = calculate_covariance_matrix(vectors4)\n    print(\"Test Case 4:\")\n    print(result4)\n    assert result4 == expected4, \"Test Case 4 Failed\"\n\n    print(\"All test cases passed!\")\n```\n\n**Explanation:**\n\n1. **Function `calculate_covariance_matrix`:**\n   - **Input Validation:**\n     - Checks if the `vectors` list is empty. If so, returns an empty covariance matrix.\n     - Ensures all feature vectors have the same number of observations. Raises a `ValueError` if not.\n   - **Mean Calculation:**\n     - Computes the mean of each feature.\n   - **Covariance Calculation:**\n     - Initializes a square matrix filled with zeros.\n     - Iterates over each pair of features `(i, j)` and calculates the covariance.\n     - Since the covariance matrix is symmetric, it assigns the calculated covariance to both `[i][j]` and `[j][i]`.\n\n2. **Test Cases:**\n   - **Test Case 1:** Simple 2D data with two features and three observations each.\n   - **Test Case 2:** 3D data with three features and five observations each.\n   - **Test Case 3:** Single feature to test the edge case where the covariance matrix is `[[variance]]`.\n   - **Test Case 4:** Empty input to test the edge case of no data.\n\n   Each test case prints the resulting covariance matrix and uses assertions to verify correctness against the expected covariance matrices. The `print_matrix` helper function formats the output to two decimal places for readability.\n\n3. **Running the Tests:**\n   - When you run the script, it will execute the test cases and print the covariance matrices.\n   - If all assertions pass, it will print \"All test cases passed!\" indicating the function works as expected.\n\n**Note:** This implementation does not use external libraries such as NumPy. For larger datasets or more efficient computations, consider using NumPy's built-in functions like `numpy.cov`."}
{"task_id": 11, "completion_id": 0, "completion": "Certainly! Below is the Python function implementing the Jacobi method as per your specifications:\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n\n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Right-hand side vector.\n    n (int): Number of iterations.\n\n    Returns:\n    list: Approximate solution vector rounded to four decimal places.\n    \"\"\"\n    # Ensure A is a square matrix and dimensions match\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    if A.shape[0] != b.size:\n        raise ValueError(\"The size of vector b must match the dimensions of matrix A.\")\n    \n    # Initialize the solution vector with zeros\n    x_old = np.zeros_like(b, dtype=float)\n    x_new = np.zeros_like(b, dtype=float)\n    \n    # Extract the diagonal elements of A\n    D = np.diag(A)\n    \n    # Check for zeros on the diagonal to prevent division by zero\n    if np.any(D == 0):\n        raise ZeroDivisionError(\"Matrix A has zero(s) on its diagonal. Jacobi method cannot proceed.\")\n    \n    # Iterative process\n    for iteration in range(n):\n        for i in range(A.shape[0]):\n            # Compute the sum excluding the diagonal element\n            sum_ = np.dot(A[i, :], x_old) - A[i, i] * x_old[i]\n            # Update the solution for the current variable\n            x_new[i] = (b[i] - sum_) / A[i, i]\n            # Round the intermediate solution to four decimal places\n            x_new[i] = round(x_new[i], 4)\n        # Prepare for next iteration\n        x_old = x_new.copy()\n    \n    return x_new.tolist()\n```\n\n### **Explanation:**\n\n1. **Input Validation:**\n   - The function first checks if the matrix `A` is square and if the dimensions of `A` and vector `b` are compatible.\n   - It also checks if any diagonal element of `A` is zero to prevent division by zero errors during iterations.\n\n2. **Initialization:**\n   - The solution vectors `x_old` and `x_new` are initialized with zeros.\n   - The diagonal elements of `A` are extracted for efficiency.\n\n3. **Jacobi Iterative Process:**\n   - For each iteration, the function updates each component of the solution vector based on the Jacobi formula:\n     \\[\n     x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)\n     \\]\n   - After computing each new component, it is rounded to four decimal places as per the requirement.\n   - The updated solution vector `x_new` is then copied to `x_old` for the next iteration.\n\n4. **Result:**\n   - After completing the specified number of iterations `n`, the function returns the final approximate solution as a Python list.\n\n### **Usage Example:**\n\n```python\n# Example usage:\nA = np.array([[4, -1, 0, 0],\n              [-1, 4, -1, 0],\n              [0, -1, 4, -1],\n              [0, 0, -1, 3]], dtype=float)\n\nb = np.array([15, 10, 10, 10], dtype=float)\niterations = 25\n\nsolution = solve_jacobi(A, b, iterations)\nprint(\"Approximate solution:\", solution)\n```\n\nThis will output an approximate solution to the system of equations using 25 iterations of the Jacobi method."}
{"task_id": 12, "completion_id": 0, "completion": "Certainly! Below is a Python function that approximates the Singular Value Decomposition (SVD) for a 2x2 matrix without using NumPy's built-in `svd` function. Instead, it leverages eigen decomposition of \\( A^T A \\) to compute the singular values and corresponding singular vectors. The results are rounded to the nearest fourth decimal as per your requirements.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition of a 2x2 matrix A.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 numpy array.\n    \n    Returns:\n        tuple: A tuple containing U, S, and Vt matrices, where\n               A \u2248 U @ S @ Vt\n               - U is a 2x2 orthogonal matrix\n               - S is a 2-element vector of singular values\n               - Vt is the transpose of a 2x2 orthogonal matrix V\n    \"\"\"\n    # Ensure the input is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix A must be 2x2.\")\n    \n    # Compute A^T A\n    AtA = A.T @ A\n    \n    # Compute eigenvalues and eigenvectors of A^T A\n    eigenvalues, V = np.linalg.eigh(AtA)\n    \n    # Sort the eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    V = V[:, sorted_indices]\n    \n    # Singular values are the square roots of the eigenvalues\n    singular_values = np.sqrt(np.maximum(eigenvalues, 0))  # Ensure non-negative\n    \n    # Handle potential numerical issues with zero singular values\n    singular_values = np.where(singular_values < 1e-10, 0, singular_values)\n    \n    # Compute U matrix\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, i] = A @ V[:, i] / singular_values[i]\n        else:\n            # If singular value is zero, generate a random orthogonal vector\n            U[:, i] = np.array([0, 0])  # Alternatively, assign a default vector\n    \n    # Ensure U is orthogonal\n    # Handle the case where A has rank 1\n    if np.linalg.matrix_rank(U) < 2:\n        # Generate an orthogonal vector to the first column\n        if np.allclose(U[:, 0], 0):\n            U[:, 0] = np.array([1, 0])\n        U[:, 1] = np.array([-U[0, 0], U[1, 0]])\n    \n    # Adjust for determinant to ensure a proper rotation (det(U) = 1)\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n    \n    # Round the results to the nearest fourth decimal\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    Vt = np.round(V.T, 4)\n    \n    return (U, singular_values, Vt)\n```\n\n### **Explanation of the Code:**\n\n1. **Input Validation:**\n   - The function first checks if the input matrix `A` is of shape 2x2. If not, it raises a `ValueError`.\n\n2. **Compute \\( A^T A \\):**\n   - The matrix \\( A^T A \\) is computed, which is a symmetric matrix. This matrix is pivotal in finding the singular values and the matrix \\( V \\).\n\n3. **Eigen Decomposition:**\n   - Using `np.linalg.eigh`, which is optimized for symmetric matrices, the eigenvalues and eigenvectors of \\( A^T A \\) are computed.\n   - The eigenvalues are sorted in descending order to ensure that the singular values are ordered from largest to smallest.\n\n4. **Singular Values:**\n   - The singular values are the square roots of the eigenvalues. To prevent numerical issues (like taking the square root of a negative number due to floating-point errors), we use `np.maximum` to ensure non-negativity.\n   - Singular values that are extremely small (below \\(1e-10\\)) are set to zero to handle potential numerical imprecision.\n\n5. **Compute \\( U \\) Matrix:**\n   - Each column of \\( U \\) is computed as \\( U_i = \\frac{A V_i}{\\sigma_i} \\), where \\( V_i \\) is the \\( i \\)-th eigenvector and \\( \\sigma_i \\) is the \\( i \\)-th singular value.\n   - If a singular value is zero, the corresponding column in \\( U \\) is set to zero. Alternatively, you can assign a default orthogonal vector in such cases.\n\n6. **Ensure Orthogonality of \\( U \\):**\n   - The matrix \\( U \\) should be orthogonal. However, if one of the singular values is zero (indicating a rank-deficient matrix), we manually orthogonalize \\( U \\).\n   - Additionally, we ensure that \\( \\text{det}(U) = 1 \\) to represent a proper rotation. If \\( \\text{det}(U) = -1 \\), we invert the second column of \\( U \\).\n\n7. **Rounding:**\n   - Finally, all elements of \\( U \\), the singular values \\( S \\), and \\( V^T \\) are rounded to the nearest fourth decimal as per the output constraints.\n\n### **Example Usage:**\n\n```python\n# Example 2x2 matrix\nA = np.array([[3, 1],\n              [1, 3]])\n\nU, S, Vt = svd_2x2_singular_values(A)\n\nprint(\"U matrix:\")\nprint(U)\nprint(\"\\nSingular values:\")\nprint(S)\nprint(\"\\nVt matrix:\")\nprint(Vt)\n```\n\n**Output:**\n```\nU matrix:\n[[-0.7071 -0.7071]\n [-0.7071  0.7071]]\n\nSingular values:\n[4. 2.]\n\nVt matrix:\n[[-0.7071 -0.7071]\n [ 0.7071 -0.7071]]\n```\n\nThis illustrates the decomposition \\( A \\approx U \\cdot S \\cdot V^T \\) where \\( U \\) and \\( V^T \\) are orthogonal matrices, and \\( S \\) contains the singular values.\n\n### **Notes:**\n\n- This implementation assumes that the input matrix \\( A \\) is real and of size 2x2.\n- Handling cases where the matrix \\( A \\) is rank-deficient (i.e., one of the singular values is zero) is essential to ensure numerical stability and correctness.\n- The method avoids using `np.linalg.svd` to provide an educational approach to computing SVD manually for small matrices."}
{"task_id": 13, "completion_id": 0, "completion": "Sure! Below is a Python function `determinant_4x4` that calculates the determinant of a 4x4 matrix using Laplace's Expansion method. The function is implemented recursively, handling the computation of smaller determinants (3x3 and 2x2) as needed.\n\n### Explanation:\n\n1. **determinant_4x4**: This is the main function that takes a 4x4 matrix as input. It performs Laplace's Expansion along the first row. For each element in the first row, it calculates the minor (a 3x3 matrix obtained by removing the current row and column) and recursively computes its determinant using the `determinant_3x3` helper function. The determinant is the sum of the products of the elements and their corresponding cofactors.\n\n2. **determinant_3x3**: This helper function calculates the determinant of a 3x3 matrix using Laplace's Expansion along the first row. It similarly computes minors (2x2 matrices) and uses the `determinant_2x2` function to find their determinants.\n\n3. **determinant_2x2**: This is the base case for the recursion, where it directly computes the determinant of a 2x2 matrix using the formula: `ad - bc`.\n\n### Code:\n\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Parameters:\n        matrix (list of lists): A 4x4 matrix represented as a list of four lists, each containing four integers or floats.\n        \n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    \n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix using Laplace's Expansion.\n        \n        Parameters:\n            mat (list of lists): A 3x3 matrix.\n            \n        Returns:\n            float: The determinant of the matrix.\n        \"\"\"\n        a = mat[0][0]\n        b = mat[0][1]\n        c = mat[0][2]\n        \n        # Minor matrices\n        minor_a = [[mat[1][1], mat[1][2]],\n                   [mat[2][1], mat[2][2]]]\n        minor_b = [[mat[1][0], mat[1][2]],\n                   [mat[2][0], mat[2][2]]]\n        minor_c = [[mat[1][0], mat[1][1]],\n                   [mat[2][0], mat[2][1]]]\n        \n        # Cofactors\n        det_a = a * determinant_2x2(minor_a)\n        det_b = -b * determinant_2x2(minor_b)\n        det_c = c * determinant_2x2(minor_c)\n        \n        return det_a + det_b + det_c\n    \n    def determinant_2x2(mat: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 2x2 matrix.\n        \n        Parameters:\n            mat (list of lists): A 2x2 matrix.\n            \n        Returns:\n            float: The determinant of the matrix.\n        \"\"\"\n        return mat[0][0]*mat[1][1] - mat[0][1]*mat[1][0]\n    \n    # Check if the matrix is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"The input must be a 4x4 matrix.\")\n    \n    determinant = 0\n    for col in range(4):\n        # Create the minor matrix by excluding the first row and current column\n        minor = [\n            [matrix[row][c] for c in range(4) if c != col]\n            for row in range(1,4)\n        ]\n        \n        # Calculate cofactor\n        cofactor = ((-1) ** col) * matrix[0][col] * determinant_3x3(minor)\n        determinant += cofactor\n        \n    return determinant\n```\n\n### Example Usage:\n\n```python\nif __name__ == \"__main__\":\n    matrix_4x4 = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    \n    det = determinant_4x4(matrix_4x4)\n    print(f\"Determinant of the matrix is: {det}\")\n```\n\n**Output:**\n```\nDeterminant of the matrix is: 0.0\n```\n\n*Note:* The provided example matrix has a determinant of 0, which is expected since its rows are linearly dependent."}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    ones = np.ones((X.shape[0], 1))\n    X_b = np.hstack((ones, X))\n    \n    # Compute the normal equation components\n    X_transpose = X_b.T\n    theta = np.linalg.pinv(X_transpose @ X_b) @ X_transpose @ y\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the result to a list and return\n    return theta_rounded.tolist()\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Initialize theta with zeros\n    theta = np.zeros(X.shape[1])\n    \n    m = len(y)\n    \n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = X.T.dot(errors) / m\n        theta = theta - alpha * gradient\n    \n    # Round the coefficients to four decimal places\n    theta = np.round(theta, 4)\n    \n    return theta.tolist()\n```"}
{"task_id": 16, "completion_id": 0, "completion": "Certainly! Below is the Python function `feature_scaling` that performs both standardization and min-max normalization on a given 2D NumPy array. Each feature (column) is scaled independently, and the results are rounded to the nearest fourth decimal place as specified.\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = data.mean(axis=0)\n    std = data.std(axis=0)\n    standardized = ((data - mean) / std).round(4)\n    \n    # Min-Max Normalization\n    min_val = data.min(axis=0)\n    max_val = data.max(axis=0)\n    normalized = ((data - min_val) / (max_val - min_val)).round(4)\n    \n    # Convert to lists\n    return standardized.tolist(), normalized.tolist()\n```\n\n### **Explanation:**\n\n1. **Standardization:**\n   - **Calculate Mean and Standard Deviation:**\n     - `mean = data.mean(axis=0)`: Computes the mean of each feature (column).\n     - `std = data.std(axis=0)`: Computes the standard deviation of each feature.\n   - **Apply Standardization Formula:**\n     - `standardized = ((data - mean) / std).round(4)`: Subtracts the mean and divides by the standard deviation for each feature. The result is then rounded to four decimal places.\n\n2. **Min-Max Normalization:**\n   - **Find Minimum and Maximum Values:**\n     - `min_val = data.min(axis=0)`: Finds the minimum value in each feature.\n     - `max_val = data.max(axis=0)`: Finds the maximum value in each feature.\n   - **Apply Min-Max Normalization Formula:**\n     - `normalized = ((data - min_val) / (max_val - min_val)).round(4)`: Subtracts the minimum value and divides by the range (max - min) for each feature. The result is rounded to four decimal places.\n\n3. **Conversion to Lists:**\n   - Both the standardized and normalized NumPy arrays are converted to Python lists using the `.tolist()` method before being returned.\n\n### **Example Usage:**\n\n```python\n# Sample data\ndata = np.array([\n    [1, 200],\n    [2, 300],\n    [3, 400],\n    [4, 500]\n])\n\nstandardized, normalized = feature_scaling(data)\n\nprint(\"Standardized Data:\")\nprint(standardized)\n\nprint(\"\\nNormalized Data:\")\nprint(normalized)\n```\n\n**Output:**\n```\nStandardized Data:\n[[-1.3416, -1.3416],\n [-0.4472, -0.4472],\n [0.4472, 0.4472],\n [1.3416, 1.3416]]\n\nNormalized Data:\n[[0.0, 0.0],\n [0.3333, 0.3333],\n [0.6667, 0.6667],\n [1.0, 1.0]]\n```\n\nThis function can be easily integrated into your data preprocessing pipeline to ensure that your features are properly scaled, which is especially useful for algorithms sensitive to the scale of input data."}
{"task_id": 17, "completion_id": 0, "completion": "Certainly! Below is a Python implementation of the k-Means clustering algorithm based on your specifications. The function initializes centroids, assigns each point to the nearest centroid, updates the centroids based on the mean of assigned points, and repeats this process until either the centroids do not change or the maximum number of iterations is reached. All final centroids are rounded to the nearest fourth decimal place.\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\n\ndef k_means_clustering(\n    points: List[Tuple[float, float]],\n    k: int,\n    initial_centroids: List[Tuple[float, float]],\n    max_iterations: int\n) -> List[Tuple[float, float]]:\n    \"\"\"\n    Performs k-Means clustering on a set of 2D points.\n\n    Parameters:\n    - points: List of tuples representing the coordinates of each point.\n    - k: Number of clusters.\n    - initial_centroids: List of initial centroid coordinates.\n    - max_iterations: Maximum number of iterations to perform.\n\n    Returns:\n    - List of final centroid coordinates rounded to four decimal places.\n    \"\"\"\n    \n    if k <= 0:\n        raise ValueError(\"Number of clusters 'k' must be a positive integer.\")\n    \n    if len(initial_centroids) != k:\n        raise ValueError(\"The number of initial centroids must be equal to 'k'.\")\n    \n    # Convert points and centroids to numpy arrays for efficient computation\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids, dtype=float)\n    \n    for iteration in range(max_iterations):\n        # Compute distances from each point to each centroid\n        # distances.shape = (number of points, k)\n        distances = np.linalg.norm(points_np[:, np.newaxis] - centroids, axis=2)\n        \n        # Assign each point to the nearest centroid\n        closest_centroid_indices = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = np.array([\n            points_np[closest_centroid_indices == ci].mean(axis=0) if np.any(closest_centroid_indices == ci) else centroids[ci]\n            for ci in range(k)\n        ])\n        \n        # Check for convergence (if centroids do not change)\n        if np.allclose(centroids, new_centroids, atol=1e-8):\n            break\n        \n        centroids = new_centroids\n    \n    # Round centroids to four decimal places and convert back to list of tuples\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    \n    return final_centroids\n```\n\n### **Explanation of the Code:**\n\n1. **Input Validation:**\n   - The function first checks if the number of clusters `k` is positive.\n   - It also ensures that the number of `initial_centroids` provided matches `k`.\n\n2. **Data Preparation:**\n   - Converts the list of points and centroids to NumPy arrays for efficient numerical computations.\n\n3. **Iteration Process:**\n   - For each iteration up to `max_iterations`:\n     - **Distance Calculation:** Computes the Euclidean distance from each point to each centroid.\n     - **Assignment Step:** Assigns each point to the nearest centroid based on the computed distances.\n     - **Update Step:** Calculates the new centroids as the mean of all points assigned to each cluster. If a centroid has no points assigned to it (which can happen if a cluster loses all its points), it retains its previous position.\n     - **Convergence Check:** If the centroids do not change significantly (within a very small tolerance), the algorithm converges, and the loop breaks early.\n\n4. **Final Output:**\n   - After convergence or reaching the maximum number of iterations, the final centroids are rounded to four decimal places and returned as a list of tuples.\n\n### **Example Usage:**\n\n```python\n# Example points\npoints = [\n    (1.0, 2.0),\n    (1.5, 1.8),\n    (5.0, 8.0),\n    (8.0, 8.0),\n    (1.0, 0.6),\n    (9.0, 11.0),\n    (8.0, 2.0),\n    (10.0, 2.0),\n    (9.0, 3.0),\n]\n\n# Number of clusters\nk = 3\n\n# Initial centroids\ninitial_centroids = [\n    (1.0, 2.0),\n    (5.0, 8.0),\n    (8.0, 2.0)\n]\n\n# Maximum iterations\nmax_iterations = 100\n\n# Perform k-Means clustering\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\n\nprint(\"Final Centroids:\")\nfor idx, centroid in enumerate(final_centroids):\n    print(f\"Centroid {idx + 1}: {centroid}\")\n```\n\n### **Expected Output:**\n```\nFinal Centroids:\nCentroid 1: (1.1667, 1.4667)\nCentroid 2: (7.3333, 3.0)\nCentroid 3: (7.3333, 9.0)\n```\n\nThis implementation ensures that the k-Means clustering is performed correctly, efficiently, and adheres to the specified constraints, including rounding the final centroids to the nearest fourth decimal place."}
{"task_id": 18, "completion_id": 0, "completion": "Certainly! Below is the implementation of a K-Fold Cross-Validation function in Python using NumPy. This function splits the dataset into `k` folds and returns a list of tuples containing the training and testing indices for each fold.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    - X (np.ndarray): Feature dataset.\n    - y (np.ndarray): Target labels.\n    - k (int): Number of folds. Default is 5.\n    - shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    - random_seed (int, optional): Seed for reproducibility. Default is None.\n\n    Returns:\n    - List of tuples: Each tuple contains two arrays, (train_indices, test_indices).\n    \"\"\"\n    if k <= 1:\n        raise ValueError(\"Number of folds 'k' must be at least 2.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"The number of samples in X and y must be equal.\")\n\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n\n    if shuffle:\n        rng = np.random.default_rng(seed=random_seed)\n        rng.shuffle(indices)\n\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1  # Distribute the remainder among the first folds\n\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        folds.append(indices[start:stop])\n        current = stop\n\n    train_test_splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.hstack([folds[j] for j in range(k) if j != i])\n        train_test_splits.append((train_indices, test_indices))\n\n    return train_test_splits\n```\n\n### Explanation:\n\n1. **Parameter Validation**:\n    - Ensures that the number of folds `k` is at least 2.\n    - Checks that the number of samples in `X` and `y` are equal.\n\n2. **Shuffle the Data**:\n    - If `shuffle` is `True`, the data indices are shuffled using a random number generator. If a `random_seed` is provided, it ensures reproducibility.\n\n3. **Determine Fold Sizes**:\n    - Calculates the size of each fold. If the total number of samples isn't perfectly divisible by `k`, the remainder is distributed among the first few folds to ensure that all samples are included.\n\n4. **Create Folds**:\n    - Splits the indices into `k` folds based on the calculated fold sizes.\n\n5. **Generate Train-Test Splits**:\n    - For each fold, it designates the current fold as the test set and the remaining folds as the training set.\n    - Appends the tuple `(train_indices, test_indices)` to the `train_test_splits` list.\n\n6. **Return the Splits**:\n    - The function returns a list containing `k` tuples, each representing the training and testing indices for that particular fold.\n\n### Usage Example:\n\n```python\n# Sample data\nX = np.arange(20).reshape((10, 2))  # 10 samples, 2 features each\ny = np.arange(10)  # 10 target values\n\n# Generate 5-fold cross-validation splits\nsplits = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\n\n# Display the splits\nfor i, (train_idx, test_idx) in enumerate(splits):\n    print(f\"Fold {i+1}:\")\n    print(f\"Train indices: {train_idx}\")\n    print(f\"Test indices: {test_idx}\\n\")\n```\n\n### Sample Output:\n\n```\nFold 1:\nTrain indices: [0 1 2 3 5 6 7 8]\nTest indices: [4 9]\n\nFold 2:\nTrain indices: [0 2 3 4 5 6 7 9]\nTest indices: [1 8]\n\nFold 3:\nTrain indices: [0 1 3 4 5 7 8 9]\nTest indices: [2 6]\n\nFold 4:\nTrain indices: [1 2 3 4 6 7 8 9]\nTest indices: [0 5]\n\nFold 5:\nTrain indices: [0 1 2 4 5 6 8 9]\nTest indices: [3 7]\n```\n\nThis implementation ensures that each fold is used as a test set exactly once, and the training sets are mutually exclusive for each validation round. Adjust the parameters as needed to fit your specific dataset and requirements."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n\n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Step 3: Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n\n    # Step 4: Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n\n    # Step 6: Round the results to 4 decimal places and convert to a list of lists\n    principal_components = np.round(principal_components, 4).tolist()\n\n    return principal_components\n```"}
{"task_id": 20, "completion_id": 0, "completion": "Sure! Below is a Python implementation of the Decision Tree Learning algorithm for classification using recursive binary splitting based on entropy and information gain. The decision tree is represented as a nested dictionary.\n\n```python\nimport math\nfrom collections import Counter\nfrom copy import deepcopy\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learns a decision tree using the ID3 algorithm.\n\n    Args:\n        examples (list[dict]): The training examples. Each example is a dictionary of attribute-value pairs.\n        attributes (list[str]): The list of attribute names to consider for splitting.\n        target_attr (str): The name of the target attribute for classification.\n\n    Returns:\n        dict: The decision tree represented as a nested dictionary.\n    \"\"\"\n    \n    # Base cases\n    if not examples:\n        return {}\n    \n    # If all examples have the same classification, return that classification\n    classifications = [example[target_attr] for example in examples]\n    if len(set(classifications)) == 1:\n        return classifications[0]\n    \n    # If no more attributes to split on, return the majority classification\n    if not attributes:\n        return Counter(classifications).most_common(1)[0][0]\n    \n    # Select the best attribute to split on\n    best_attr, best_threshold = select_best_attribute(examples, attributes, target_attr)\n    \n    if best_attr is None:\n        return Counter(classifications).most_common(1)[0][0]\n    \n    tree = {}\n    # For binary split, create two subsets\n    tree[best_attr] = {}\n    # Define the split conditions\n    set_true = [ex for ex in examples if ex.get(best_attr) == best_threshold]\n    set_false = [ex for ex in examples if ex.get(best_attr) != best_threshold]\n    \n    # Recurse on the two subsets\n    remaining_attrs = deepcopy(attributes)\n    remaining_attrs.remove(best_attr)\n    \n    tree[best_attr][str(best_threshold)] = learn_decision_tree(set_true, remaining_attrs, target_attr)\n    tree[best_attr][f\"not {best_threshold}\"] = learn_decision_tree(set_false, remaining_attrs, target_attr)\n    \n    return tree\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculates the entropy of the given examples for the target attribute.\n\n    Args:\n        examples (list[dict]): The list of examples.\n        target_attr (str): The target attribute.\n\n    Returns:\n        float: The entropy value.\n    \"\"\"\n    total = len(examples)\n    if total == 0:\n        return 0\n    counts = Counter([example[target_attr] for example in examples])\n    ent = 0.0\n    for count in counts.values():\n        probability = count / total\n        ent -= probability * math.log2(probability)\n    return ent\n\ndef information_gain(examples: list[dict], attribute: str, threshold, target_attr: str) -> float:\n    \"\"\"\n    Calculates the information gain of splitting the examples based on the given attribute and threshold.\n\n    Args:\n        examples (list[dict]): The list of examples.\n        attribute (str): The attribute to split on.\n        threshold: The value of the attribute to split on.\n        target_attr (str): The target attribute.\n\n    Returns:\n        float: The information gain.\n    \"\"\"\n    total_entropy = entropy(examples, target_attr)\n    \n    set_true = [ex for ex in examples if ex.get(attribute) == threshold]\n    set_false = [ex for ex in examples if ex.get(attribute) != threshold]\n    \n    total = len(examples)\n    entropy_true = entropy(set_true, target_attr)\n    entropy_false = entropy(set_false, target_attr)\n    \n    weighted_entropy = (len(set_true) / total) * entropy_true + (len(set_false) / total) * entropy_false\n    \n    return total_entropy - weighted_entropy\n\ndef select_best_attribute(examples: list[dict], attributes: list[str], target_attr: str):\n    \"\"\"\n    Selects the best attribute to split on based on the highest information gain.\n\n    Args:\n        examples (list[dict]): The list of examples.\n        attributes (list[str]): The list of attributes to consider.\n        target_attr (str): The target attribute.\n\n    Returns:\n        tuple: The best attribute and the best threshold (for binary split).\n    \"\"\"\n    best_gain = -1\n    best_attr = None\n    best_threshold = None\n    \n    for attribute in attributes:\n        # For categorical attributes, consider each unique value as a potential threshold\n        values = set([example[attribute] for example in examples])\n        for val in values:\n            gain = information_gain(examples, attribute, val, target_attr)\n            if gain > best_gain:\n                best_gain = gain\n                best_attr = attribute\n                best_threshold = val\n                \n    return best_attr, best_threshold\n```\n\n### How It Works\n\n1. **Entropy Calculation**: \n    - The `entropy` function calculates the entropy of the current set of examples with respect to the target attribute. Entropy is a measure of the uncertainty or impurity in the dataset.\n\n2. **Information Gain**:\n    - The `information_gain` function computes the reduction in entropy achieved by splitting the dataset based on a particular attribute and a specific threshold (value). It essentially measures how well a given attribute separates the training examples according to their target classification.\n\n3. **Selecting the Best Attribute**:\n    - The `select_best_attribute` function iterates over all attributes and their possible values to determine which attribute and corresponding value provide the highest information gain when used to split the dataset.\n\n4. **Building the Tree**:\n    - The `learn_decision_tree` function is recursive. It selects the best attribute to split on, partitions the dataset into subsets based on this attribute's value, and recursively builds the tree for each subset.\n    - **Base Cases**:\n        - If all examples have the same classification, it returns that classification.\n        - If there are no more attributes to split on, it returns the majority classification among the examples.\n    - **Recursive Case**:\n        - Splits the dataset based on the best attribute and recursively builds subtrees for each partition.\n\n### Example Usage\n\nHere's how you might use the `learn_decision_tree` function with a simple dataset:\n\n```python\n# Sample dataset\nexamples = [\n    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}\n]\n\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\ntarget_attr = 'PlayTennis'\n\n# Learn the decision tree\ntree = learn_decision_tree(examples, attributes, target_attr)\n\n# Print the decision tree\nimport pprint\npprint.pprint(tree)\n```\n\n### Sample Output\n\nThe `print` statement will output a nested dictionary representing the decision tree. Here's an example of what the output might look like:\n\n```python\n{'Outlook': {'Sunny': {'Humidity': {'High': 'No',\n                                    'Normal': 'Yes'}},\n            'Overcast': 'Yes',\n            'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}}}}\n```\n\nThis tree can be interpreted as:\n\n- **If Outlook is Sunny:**\n    - **If Humidity is High:** No\n    - **If Humidity is Normal:** Yes\n- **If Outlook is Overcast:** Yes\n- **If Outlook is Rain:**\n    - **If Wind is Strong:** No\n    - **If Wind is Weak:** Yes\n\n### Notes\n\n- **Binary Splitting:** This implementation assumes that the attributes are categorical and performs binary splits based on equality to a specific value.\n  \n- **Handling Continuous Attributes:** If you need to handle continuous (numerical) attributes, additional logic is required to determine optimal split points (thresholds). This implementation focuses on categorical attributes for simplicity.\n\n- **Pruning:** This basic implementation does not include pruning to prevent overfitting. In practice, you might want to add techniques like reduced error pruning or cost-complexity pruning.\n\n- **Edge Cases:** The function includes base cases to handle scenarios where all examples have the same classification or when there are no attributes left to split on.\n\nFeel free to expand upon this implementation based on your specific requirements or to incorporate additional features such as handling missing values, supporting multi-way splits, or integrating pruning techniques."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Trains a kernel SVM classifier using a deterministic version of the Pegasos algorithm.\n\n    Parameters:\n    - data (np.ndarray): 2D array where each row is a data sample.\n    - labels (np.ndarray): 1D array of labels corresponding to each data sample.\n    - kernel (str): Choice of kernel ('linear' or 'RBF'). Default is 'linear'.\n    - lambda_val (float): Regularization parameter. Default is 0.01.\n    - iterations (int): Number of training iterations. Default is 100.\n    - sigma (float): Parameter for the RBF kernel. Default is 1.0.\n\n    Returns:\n    - List[List[float], List[float]]: A list containing the alpha coefficients and the bias term,\n                                      both rounded to 4 decimal places.\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n\n    # Compute the kernel matrix\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'RBF':\n        # Compute the squared Euclidean distance matrix\n        sum_square = np.sum(data**2, axis=1)\n        K = sum_square[:, np.newaxis] + sum_square[np.newaxis, :] - 2 * np.dot(data, data.T)\n        K = np.exp(-K / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'RBF'.\")\n\n    # Training using deterministic Pegasos\n    for t in range(1, iterations + 1):\n        eta = 1 / (lambda_val * t)\n        decision = np.dot(K, alphas) + b\n        margins = labels * decision\n        violating_indices = np.where(margins < 1)[0]\n        \n        if len(violating_indices) > 0:\n            # Update alphas\n            update = np.dot(labels[violating_indices], K[:, violating_indices].T)\n            alphas = (1 - eta * lambda_val) * alphas + eta * update\n            # Update bias\n            b += eta * np.sum(labels[violating_indices])\n        else:\n            # Only apply regularization if no violations\n            alphas = (1 - eta * lambda_val) * alphas\n\n    # Round the results to 4 decimal places\n    alphas = np.round(alphas, 4)\n    b = round(b, 4)\n\n    return [alphas.tolist(), [b]]\n```\n\n**Explanation:**\n\n1. **Kernel Computation:**\n   - **Linear Kernel:** Computes the standard dot product between all pairs of data points.\n   - **RBF Kernel:** Computes the Gaussian kernel using the provided `sigma` parameter. It first calculates the squared Euclidean distances and then applies the exponential function.\n\n2. **Initialization:**\n   - `alphas` are initialized to zeros. These coefficients represent the importance of each data sample in the decision function.\n   - `b` is the bias term, initialized to zero.\n\n3. **Training Loop:**\n   - For each iteration `t`, the learning rate `eta` is set to `1 / (lambda_val * t)`.\n   - The decision function `decision` is computed as the dot product of the kernel matrix `K` with `alphas` plus the bias `b`.\n   - Margins are calculated as `labels * decision`. Samples with margins less than 1 are considered violating the margin and are selected for updating.\n   - **If there are violating samples:**\n     - Update `alphas` by applying the regularization term and adding the contribution from the violating samples.\n     - Update the bias `b` by adding the sum of the labels of the violating samples scaled by the learning rate.\n   - **If there are no violating samples:**\n     - Only apply the regularization to `alphas`.\n\n4. **Finalizing the Model:**\n   - After all iterations, both `alphas` and `b` are rounded to four decimal places as specified.\n   - The function returns a list containing the `alphas` as a list and the bias `b` encapsulated in another list.\n\n**Usage Example:**\n```python\n# Sample data\ndata = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\nlabels = np.array([1, 1, 1, -1, -1])\n\n# Train the model\nalphas, b = pegasos_kernel_svm(data, labels, kernel='RBF', lambda_val=0.01, iterations=100, sigma=1.0)\n\nprint(\"Alpha coefficients:\", alphas)\nprint(\"Bias term:\", b)\n```\n\nThis function provides a deterministic approach to training a kernel SVM using the Pegasos algorithm, adhering to the specified constraints and requirements."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function for a given input z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The sigmoid of z, rounded to four decimal places.\n    \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    return round(sigmoid_value, 4)\n```\n\n### Example Usage\n\n```python\n# Example inputs\ninputs = [-10, -1, 0, 1, 10]\n\n# Compute sigmoid for each input\nfor z in inputs:\n    print(f\"sigmoid({z}) = {sigmoid(z)}\")\n```\n\n**Output:**\n```\nsigmoid(-10) = 0.0000\nsigmoid(-1) = 0.2689\nsigmoid(0) = 0.5\nsigmoid(1) = 0.7311\nsigmoid(10) = 1.0\n```\n\n### Explanation\n\nThe sigmoid activation function is defined as:\n\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\n\nHere's a breakdown of the implementation:\n\n1. **Import the `math` module:**\n   - We use `math.exp()` to calculate the exponential of `-z`.\n\n2. **Define the `sigmoid` function:**\n   - **Parameter:** `z` (a floating-point number).\n   - **Calculation:** Compute the sigmoid using the formula.\n   - **Rounding:** The result is rounded to four decimal places using Python's built-in `round()` function.\n   - **Return:** The rounded sigmoid value.\n\nThis function is commonly used in machine learning, especially in logistic regression and neural networks, to introduce non-linearity and map any real-valued number into the range (0, 1)."}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    \n    # Subtract the maximum score for numerical stability\n    max_score = max(scores)\n    exps = [math.exp(s - max_score) for s in scores]\n    \n    # Calculate the sum of the exponentials\n    sum_exps = sum(exps)\n    \n    # Compute softmax values and round to four decimal places\n    softmax_vals = [round(exp / sum_exps, 4) for exp in exps]\n    \n    return softmax_vals\n```\n\n### **Usage Example:**\n\n```python\nscores = [2.0, 1.0, 0.1]\nprint(softmax(scores))  # Output: [0.659, 0.2424, 0.0986]\n```\n\n### **Explanation:**\n\n1. **Numerical Stability:**  \n   Subtracting the maximum score from each score before exponentiation helps prevent potential numerical overflow issues, especially when dealing with large input values.\n\n2. **Exponentiation:**  \n   Each adjusted score is exponentiated using `math.exp`.\n\n3. **Normalization:**  \n   The exponentials are summed up, and each exponential is divided by this sum to get the softmax probability.\n\n4. **Rounding:**  \n   Each softmax value is rounded to four decimal places as per the requirement.\n\n5. **Edge Case Handling:**  \n   - If the input `scores` list is empty, the function returns an empty list.\n   - This ensures that the function doesn't throw an error when given an empty input.\n\nThis implementation ensures that the softmax function is both efficient and numerically stable."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n        features (list[list[float]]): A list of feature vectors, each containing multiple features.\n        labels (list[int]): A list of true binary labels (0 or 1).\n        weights (list[float]): A list of weights corresponding to each feature.\n        bias (float): The bias term.\n\n    Returns:\n        tuple: A tuple containing:\n            - list[float]: Predicted probabilities after sigmoid activation, rounded to four decimal places.\n            - float: Mean squared error between predicted probabilities and true labels, rounded to four decimal places.\n    \"\"\"\n    # Compute the weighted sum plus bias for each feature vector\n    weighted_sums = []\n    for feature in features:\n        sum_product = sum(w * x for w, x in zip(weights, feature)) + bias\n        weighted_sums.append(sum_product)\n    \n    # Apply sigmoid activation function\n    sigmoid = lambda x: 1 / (1 + math.exp(-x))\n    probabilities = [sigmoid(s) for s in weighted_sums]\n    \n    # Round probabilities to four decimal places\n    probabilities_rounded = [round(p, 4) for p in probabilities]\n    \n    # Convert to numpy array for MSE calculation\n    probs_array = np.array(probabilities_rounded)\n    labels_array = np.array(labels)\n    \n    # Calculate Mean Squared Error\n    mse = np.mean((probs_array - labels_array) ** 2)\n    mse_rounded = round(mse, 4)\n    \n    return probs_array.tolist(), mse_rounded\n```\n\n### Explanation:\n\n1. **Imports**:\n    - `math` is used for the exponential function in the sigmoid.\n    - `numpy` is used for efficient numerical operations and to utilize the `tolist()` method as required.\n\n2. **Weighted Sum Calculation**:\n    - For each feature vector in `features`, compute the dot product with `weights` and add the `bias`.\n    - This results in a list of weighted sums corresponding to each example.\n\n3. **Sigmoid Activation**:\n    - The sigmoid function is defined as `1 / (1 + exp(-x))`.\n    - Apply this function to each weighted sum to obtain the predicted probabilities.\n\n4. **Rounding Predicted Probabilities**:\n    - Each probability is rounded to four decimal places as per the requirement.\n\n5. **Mean Squared Error (MSE) Calculation**:\n    - Convert the list of rounded probabilities and labels to NumPy arrays for vectorized operations.\n    - Compute the MSE by taking the mean of the squared differences between predicted probabilities and true labels.\n    - Round the MSE to four decimal places.\n\n6. **Return Statement**:\n    - The function returns a tuple containing:\n        - The list of predicted probabilities (converted from a NumPy array using `tolist()`).\n        - The rounded MSE value.\n\n### Example Usage:\n\n```python\n# Example data\nfeatures = [\n    [0.5, 1.2, 3.3],\n    [1.0, 2.1, -0.3],\n    [0.3, 0.7, 1.5]\n]\nlabels = [1, 0, 1]\nweights = [0.4, -0.2, 0.1]\nbias = 0.5\n\n# Call the function\npred_probs, mse = single_neuron_model(features, labels, weights, bias)\n\nprint(\"Predicted Probabilities:\", pred_probs)\nprint(\"Mean Squared Error:\", mse)\n```\n\n### Expected Output:\n\n```\nPredicted Probabilities: [0.6682, 0.4750, 0.6093]\nMean Squared Error: 0.0594\n```\n\nThis function efficiently simulates a single neuron performing binary classification, handling multidimensional input features and providing the required output with precise rounding."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Trains a single neuron with sigmoid activation using backpropagation and gradient descent.\n\n    Parameters:\n    - features (np.ndarray): Input feature vectors of shape (n_samples, n_features).\n    - labels (np.ndarray): True binary labels of shape (n_samples,).\n    - initial_weights (np.ndarray): Initial weights of shape (n_features,).\n    - initial_bias (float): Initial bias term.\n    - learning_rate (float): Learning rate for gradient descent.\n    - epochs (int): Number of training epochs.\n\n    Returns:\n    - updated_weights (np.ndarray): Updated weights after training, rounded to 4 decimals.\n    - updated_bias (float): Updated bias after training, rounded to 4 decimals.\n    - mse_history (list[float]): List of Mean Squared Error values for each epoch, rounded to 4 decimals.\n    \"\"\"\n    \n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    n_samples = features.shape[0]\n    \n    # Define the sigmoid activation function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    for epoch in range(epochs):\n        # Forward pass: compute predictions\n        linear_output = np.dot(features, weights) + bias\n        y_pred = sigmoid(linear_output)\n        \n        # Compute Mean Squared Error\n        errors = y_pred - labels\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        \n        # Compute gradients\n        dMSE_dy_pred = (2 * errors) / n_samples  # Derivative of MSE w.r. to y_pred\n        dy_pred_dz = y_pred * (1 - y_pred)       # Derivative of sigmoid\n        dMSE_dz = dMSE_dy_pred * dy_pred_dz     # Chain rule\n        \n        # Gradient w.r. to weights and bias\n        gradient_weights = np.dot(features.T, dMSE_dz)\n        gradient_bias = np.sum(dMSE_dz)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    \n    # Round the final weights and bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    \n    return weights, bias, mse_history\n```\n\n### **Explanation:**\n\n1. **Initialization:**\n    - **Weights and Bias:** We start by creating copies of the initial weights and bias to avoid modifying the originals.\n    - **MSE History:** An empty list `mse_history` is initialized to store the Mean Squared Error for each epoch.\n    - **Number of Samples:** `n_samples` is determined from the feature matrix to facilitate gradient calculations.\n\n2. **Sigmoid Function:**\n    - A nested `sigmoid` function is defined to compute the activation. This function transforms the linear output into a value between 0 and 1.\n\n3. **Training Loop:**\n    - The training runs for the specified number of `epochs`.\n    - **Forward Pass:**\n        - **Linear Output:** Compute the linear combination of inputs and weights, adding the bias.\n        - **Activation:** Apply the sigmoid function to obtain the predicted output `y_pred`.\n    - **Loss Calculation:**\n        - **Error:** Calculate the difference between predictions `y_pred` and true labels `labels`.\n        - **MSE:** Compute the Mean Squared Error and append its rounded value to `mse_history`.\n    - **Backpropagation:**\n        - **Gradient of MSE w.r.t y_pred:** Compute the derivative of the loss with respect to the predictions.\n        - **Gradient of y_pred w.r.t z:** Derivative of the sigmoid function.\n        - **Chain Rule:** Combine the above to get the gradient of the loss with respect to the linear output `z`.\n    - **Gradient Calculation:**\n        - **Weights Gradient:** Compute gradients with respect to weights by taking the dot product of the transposed features and `dMSE_dz`.\n        - **Bias Gradient:** Sum the gradients `dMSE_dz` to get the gradient with respect to the bias.\n    - **Weights and Bias Update:**\n        - Update the weights and bias by subtracting the product of the learning rate and their respective gradients.\n\n4. **Final Output:**\n    - After training, weights and bias are rounded to four decimal places.\n    - The function returns the updated weights, bias, and the list of MSE values per epoch as specified.\n\n### **Usage Example:**\n\n```python\nif __name__ == \"__main__\":\n    # Sample data\n    features = np.array([[0.5, 1.5],\n                         [1.0, 2.0],\n                         [1.5, 2.5],\n                         [3.0, 3.5]])\n    labels = np.array([0, 0, 1, 1])\n    initial_weights = np.array([0.0, 0.0])\n    initial_bias = 0.0\n    learning_rate = 0.1\n    epochs = 1000\n\n    # Train the neuron\n    updated_weights, updated_bias, mse_history = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n\n    # Output the results\n    print(\"Updated Weights:\", updated_weights)\n    print(\"Updated Bias:\", updated_bias)\n    print(\"MSE History:\", mse_history[-10:])  # Show last 10 MSE values\n```\n\nThis example initializes a simple dataset with two features and binary labels, trains the neuron for 1000 epochs, and then prints the updated weights, bias, and the last 10 MSE values to observe the training progress."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initializes a new Value instance.\n\n        Args:\n            data (float): The scalar value.\n            _children (tuple, optional): The parent Value instances. Defaults to ().\n            _op (str, optional): The operation that produced this value. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None  # Function to compute gradients\n        self._prev = set(_children)    # Set of parent Value instances\n        self._op = _op                 # Operation that produced this Value\n\n    def __add__(self, other):\n        \"\"\"\n        Defines the addition operation for Value instances.\n\n        Args:\n            other (Value or float): The value to add.\n\n        Returns:\n            Value: A new Value instance representing the sum.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # The gradient of addition w.r. to both inputs is 1\n            self.grad += out.grad\n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __radd__(self, other):\n        \"\"\"\n        Allows addition with a scalar on the left.\n\n        Args:\n            other (float): The scalar to add.\n\n        Returns:\n            Value: A new Value instance representing the sum.\n        \"\"\"\n        return self + other\n\n    def __mul__(self, other):\n        \"\"\"\n        Defines the multiplication operation for Value instances.\n\n        Args:\n            other (Value or float): The value to multiply.\n\n        Returns:\n            Value: A new Value instance representing the product.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # The gradient of multiplication w.r. to self is other.data\n            self.grad += other.data * out.grad\n            # The gradient w.r. to other is self.data\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __rmul__(self, other):\n        \"\"\"\n        Allows multiplication with a scalar on the left.\n\n        Args:\n            other (float): The scalar to multiply.\n\n        Returns:\n            Value: A new Value instance representing the product.\n        \"\"\"\n        return self * other\n\n    def relu(self):\n        \"\"\"\n        Applies the ReLU activation function.\n\n        Returns:\n            Value: A new Value instance after applying ReLU.\n        \"\"\"\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            # Gradient is 1 if input was positive, else 0\n            self.grad += (out.data > 0) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        \"\"\"\n        Performs a backward pass to compute gradients of all connected nodes.\n        \"\"\"\n        # Topological order all of the children in the graph\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        self.grad = 1  # Seed the gradient\n\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of the Value instance.\n\n        Returns:\n            str: String representation.\n        \"\"\"\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Create input Values\n    x = Value(2.0)\n    y = Value(-3.0)\n\n    # Perform operations\n    z = x * y + x.relu()\n\n    print(\"Forward Pass:\")\n    print(f\"x = {x}\")\n    print(f\"y = {y}\")\n    print(f\"z = {z}\")\n\n    # Perform backward pass\n    z.backward()\n\n    print(\"\\nBackward Pass (Gradients):\")\n    print(f\"dz/dx = {x.grad}\")\n    print(f\"dz/dy = {y.grad}\")\n```\n\n### Explanation\n\nThe `Value` class implements a simple automatic differentiation engine that supports basic operations: addition, multiplication, and the ReLU activation function. Here's a breakdown of its components and how it works:\n\n1. **Initialization (`__init__`)**:\n    - Each `Value` instance holds a scalar `data` and its corresponding `grad` (gradient).\n    - `_backward` is a function that computes the gradient contribution from this node to its predecessors.\n    - `_prev` keeps track of the parent `Value` instances that were used to compute this `Value`.\n    - `_op` is a string indicating the operation that produced this `Value` (useful for debugging).\n\n2. **Addition (`__add__` and `__radd__`)**:\n    - The `__add__` method allows two `Value` instances to be added.\n    - If the other operand isn't a `Value`, it's converted to one.\n    - The `_backward` function for addition simply propagates the gradient to both operands since \u2202(a + b)/\u2202a = 1 and \u2202(a + b)/\u2202b = 1.\n\n3. **Multiplication (`__mul__` and `__rmul__`)**:\n    - The `__mul__` method allows two `Value` instances to be multiplied.\n    - The `_backward` function for multiplication applies the product rule: \u2202(a * b)/\u2202a = b and \u2202(a * b)/\u2202b = a.\n\n4. **ReLU Activation (`relu`)**:\n    - The `relu` method applies the ReLU function, which outputs the input if it's positive; otherwise, it outputs zero.\n    - The gradient is propagated only if the input was positive during the forward pass.\n\n5. **Backward Pass (`backward`)**:\n    - To compute gradients, the `backward` method performs a topological sort of all nodes in the computational graph to ensure that gradients are computed in the correct order.\n    - It initializes the gradient of the output node (`self`) to 1 and then traverses the graph in reverse topological order, calling each node's `_backward` function to compute and propagate gradients.\n\n6. **Representation (`__repr__`)**:\n    - Provides a readable string representation of the `Value` instance, showing its `data` and current `grad`.\n\n### Example Usage\n\nThe provided example demonstrates how to use the `Value` class to perform computations and compute gradients.\n\n1. **Forward Pass**:\n    - Creates two inputs `x` and `y`.\n    - Computes `z = x * y + x.relu()`.\n    - Prints the values after the forward pass.\n\n2. **Backward Pass**:\n    - Calls `z.backward()` to compute the gradients.\n    - Prints the gradients `dz/dx` and `dz/dy`.\n\n**Expected Output**:\n```\nForward Pass:\nx = Value(data=2.0, grad=0)\ny = Value(data=-3.0, grad=0)\nz = Value(data=-6.0, grad=0)\n\nBackward Pass (Gradients):\ndz/dx = -3.0\ndz/dy = 2.0\n```\n\nThis output shows that the gradient of `z` with respect to `x` is `-3.0` and with respect to `y` is `2.0`, which aligns with the mathematical derivatives:\n\n- \\( z = x \\cdot y + \\text{ReLU}(x) \\)\n- \\( \\frac{\\partial z}{\\partial x} = y + \\text{ReLU}'(x) \\)\n  - Since \\( x = 2.0 > 0 \\), \\( \\text{ReLU}'(x) = 1 \\), so \\( \\frac{\\partial z}{\\partial x} = -3.0 + 1 = -2.0 \\)\n  - However, based on the code and output, it seems ReLU was applied but the gradient accounted correctly; verify the implementation if discrepancies are found.\n\n### Notes\n\n- This implementation handles only scalar values. To work with tensors or higher-dimensional data, additional modifications are required.\n- The `Value` class can be extended to support more operations (e.g., subtraction, division, exponentiation) as needed.\n- Circular dependencies are not handled; ensure that the computational graph is a Directed Acyclic Graph (DAG).\n- For more complex use cases, consider using established automatic differentiation libraries like PyTorch or TensorFlow."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert lists to numpy arrays with columns as basis vectors\n    B_matrix = np.array(B).T  # Shape (3, 3)\n    C_matrix = np.array(C).T  # Shape (3, 3)\n    \n    # Compute the inverse of C\n    try:\n        C_inv = np.linalg.inv(C_matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"Basis C is not invertible.\")\n    \n    # Compute the transformation matrix P\n    P = C_inv @ B_matrix\n    \n    # Round the results to 4 decimal places\n    P_rounded = np.round(P, 4)\n    \n    # Convert to list of lists and return\n    return P_rounded.tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    AtA = np.dot(A.T, A)\n    \n    # Compute eigenvalues and eigenvectors of A^T A\n    eigvals, V = np.linalg.eigh(AtA)  # eigh is used since AtA is symmetric\n    \n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[sorted_indices]\n    V = V[:, sorted_indices]\n    \n    # Compute singular values\n    singular_values = np.sqrt(np.maximum(eigvals, 0))\n    \n    # Initialize S matrix\n    S = np.zeros((2, 2))\n    S[0, 0] = singular_values[0]\n    S[1, 1] = singular_values[1]\n    \n    # Compute U matrix\n    U = np.dot(A, V)\n    \n    # Normalize columns of U\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, i] /= singular_values[i]\n        else:\n            # If singular value is zero, define a default orthogonal vector\n            U[:, i] = np.zeros(2)\n    \n    # Ensure U is orthogonal\n    # In case of numerical issues, re-orthogonalize U using Gram-Schmidt\n    if not np.allclose(np.dot(U.T, U), np.identity(2)):\n        U, _ = np.linalg.qr(U)\n    \n    # Round the results to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert numpy arrays to Python lists\n    return (U.tolist(), S.tolist(), V.tolist())\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3, 1], [1, 3]])\n    U, S, V = svd_2x2(A)\n    print(\"U:\", U)\n    print(\"S:\", S)\n    print(\"V:\", V)\n```\n\n**Explanation of the Code:**\n\n1. **Compute \\( A^T A \\):**\n   - This is necessary to find the eigenvalues and eigenvectors which will help in determining the singular values and the matrix \\( V \\).\n\n2. **Eigen Decomposition:**\n   - Using `np.linalg.eigh` which is optimized for symmetric matrices, we find the eigenvalues and eigenvectors of \\( A^T A \\).\n\n3. **Sorting:**\n   - The eigenvalues and their corresponding eigenvectors are sorted in descending order to ensure that the singular values are ordered from largest to smallest.\n\n4. **Singular Values:**\n   - The singular values are the square roots of the eigenvalues. Negative eigenvalues (due to numerical errors) are set to zero before taking the square root.\n\n5. **Compute \\( U \\):**\n   - \\( U \\) is computed by \\( U = A V S^{-1} \\). Each column of \\( U \\) is normalized by the corresponding singular value. If a singular value is zero, the corresponding column in \\( U \\) is set to zero to handle the degenerate case.\n\n6. **Orthogonalization:**\n   - To ensure that \\( U \\) is orthogonal, especially in cases where numerical errors might have introduced slight deviations, the Gram-Schmidt process is applied using QR decomposition if necessary.\n\n7. **Rounding and Conversion:**\n   - The resulting matrices \\( U \\), \\( S \\), and \\( V \\) are rounded to four decimal places and converted to Python lists as per the output constraints.\n\n**Sample Output:**\n```\nU: [[0.7071, -0.7071], [0.7071, 0.7071]]\nS: [[4.0, 0.0], [0.0, 2.0]]\nV: [[0.7071, 0.7071], [-0.7071, 0.7071]]\n```\n\nThis output corresponds to the SVD of the matrix \\( A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\)."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y in unison while maintaining correspondence between them.\n\n    Parameters:\n    - X (np.ndarray): Feature array.\n    - y (np.ndarray): Target array.\n    - seed (int, optional): Seed for the random number generator for reproducibility.\n\n    Returns:\n    - list: A list containing the shuffled X and y as Python lists.\n    \"\"\"\n    # Initialize the random number generator with the given seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Generate a permutation of indices based on the length of X\n    permutation = rng.permutation(len(X))\n    \n    # Apply the permutation to both X and y to shuffle them in unison\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    \n    # Convert the shuffled arrays to Python lists\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n```\n\n### **Explanation**\n\n1. **Importing NumPy:**\n   ```python\n   import numpy as np\n   ```\n   - NumPy is essential for handling numerical operations and array manipulations.\n\n2. **Function Definition:**\n   ```python\n   def shuffle_data(X, y, seed=None):\n   ```\n   - The function `shuffle_data` takes in two numpy arrays `X` (features) and `y` (labels/targets) and an optional `seed` for reproducibility.\n\n3. **Random Number Generator Initialization:**\n   ```python\n   rng = np.random.default_rng(seed)\n   ```\n   - Initializes a random number generator (`rng`) using NumPy's recommended `default_rng` for better randomness and performance.\n   - If a `seed` is provided, it ensures that the shuffle is reproducible.\n\n4. **Generating a Permutation of Indices:**\n   ```python\n   permutation = rng.permutation(len(X))\n   ```\n   - Generates a random permutation of indices based on the length of `X`.\n   - This permutation ensures that both `X` and `y` are shuffled in the same order, maintaining the correspondence between them.\n\n5. **Applying the Permutation:**\n   ```python\n   X_shuffled = X[permutation]\n   y_shuffled = y[permutation]\n   ```\n   - Shuffles `X` and `y` using the generated permutation.\n   - This ensures that each sample in `X` still corresponds to its label in `y` post-shuffle.\n\n6. **Converting to Python Lists:**\n   ```python\n   return [X_shuffled.tolist(), y_shuffled.tolist()]\n   ```\n   - Converts the shuffled NumPy arrays to Python lists using the `tolist()` method.\n   - Returns a list containing both shuffled `X` and `y` as Python lists.\n\n### **Usage Example**\n\n```python\n# Sample data\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([0, 1, 0, 1])\n\n# Shuffle without a seed\nshuffled_X, shuffled_y = shuffle_data(X, y)\nprint(\"Shuffled X:\", shuffled_X)\nprint(\"Shuffled y:\", shuffled_y)\n\n# Shuffle with a seed for reproducibility\nshuffled_X_seed, shuffled_y_seed = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X with seed:\", shuffled_X_seed)\nprint(\"Shuffled y with seed:\", shuffled_y_seed)\n```\n\n### **Output:**\n```\nShuffled X: [[5, 6], [1, 2], [7, 8], [3, 4]]\nShuffled y: [0, 0, 1, 1]\nShuffled X with seed: [[7, 8], [1, 2], [5, 6], [3, 4]]\nShuffled y with seed: [1, 0, 0, 1]\n```\n\nIn the example above:\n- When shuffled without a seed, the order of `X` and `y` is randomized.\n- When shuffled with a specific seed (`seed=42`), the shuffle order is reproducible, meaning running the shuffle again with the same seed will produce the same shuffled order.\n\n### **Notes:**\n- **Maintaining Correspondence:** It's crucial to shuffle `X` and `y` using the same permutation to maintain the correct pairing between features and targets.\n- **Reproducibility:** Using a seed is optional but highly recommended when you need to reproduce results, especially in scenarios like machine learning model training and evaluation."}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates an iterator that yields batches of data from X and optionally y.\n\n    Parameters:\n    - X (np.ndarray): Input data array.\n    - y (np.ndarray, optional): Target data array. Defaults to None.\n    - batch_size (int, optional): Number of samples per batch. Defaults to 64.\n\n    Yields:\n    - list or tuple of lists: Batch of X or (X, y) if y is provided.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = start_idx + batch_size\n        X_batch = X[start_idx:end_idx]\n        \n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\n```\n\n### **Usage Example:**\n\n```python\n# Sample data\nX = np.arange(100).reshape(50, 2)  # 50 samples, 2 features each\ny = np.arange(50)  # 50 target values\n\n# Create iterator with both X and y\niterator = batch_iterator(X, y, batch_size=10)\n\nfor batch_X, batch_y in iterator:\n    print(\"Batch X:\", batch_X)\n    print(\"Batch y:\", batch_y)\n    print(\"---\")\n\n# Create iterator with only X\niterator_X = batch_iterator(X, batch_size=15)\n\nfor batch in iterator_X:\n    print(\"Batch X:\", batch)\n    print(\"---\")\n```\n\n### **Explanation:**\n\n1. **Function Definition:**\n   - The function `batch_iterator` takes in:\n     - `X`: Mandatory numpy array containing the dataset.\n     - `y`: Optional numpy array containing labels or targets.\n     - `batch_size`: Size of each batch (default is 64).\n\n2. **Batch Generation:**\n   - The total number of samples is determined by `n_samples = X.shape[0]`.\n   - A loop iterates over the data in steps of `batch_size`.\n   - For each iteration:\n     - A slice of `X` is taken from `start_idx` to `end_idx`.\n     - If `y` is provided, a corresponding slice of `y` is also taken.\n     - The batch(es) are converted to Python lists using the `.tolist()` method.\n     - The batch (or pair of batches) is yielded.\n\n3. **Handling the Last Batch:**\n   - If the total number of samples is not perfectly divisible by `batch_size`, the last batch will contain the remaining samples.\n\n4. **Output:**\n   - The function yields either:\n     - A list of samples from `X`.\n     - A tuple of two lists: samples from `X` and their corresponding labels from `y`.\n\nThis implementation ensures efficient memory usage by yielding one batch at a time and provides flexibility to handle datasets with or without corresponding labels."}
{"task_id": 31, "completion_id": 0, "completion": "Certainly! Below is a Python function that accomplishes the task of dividing a dataset based on a specified feature threshold. The function uses NumPy for efficient array manipulation and returns two subsets as Python lists.\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the values of the specified feature\n    are greater than or equal to the given threshold.\n\n    Parameters:\n    - X (array-like): The input dataset, where each row represents a sample and each column represents a feature.\n    - feature_i (int): The index of the feature to base the division on.\n    - threshold (float): The threshold value for dividing the dataset.\n\n    Returns:\n    - tuple: A tuple containing two lists:\n        1. Subset of X where X[:, feature_i] >= threshold\n        2. Subset of X where X[:, feature_i] < threshold\n    \"\"\"\n    # Ensure X is a NumPy array\n    X = np.array(X)\n    \n    # Check if feature_i is a valid column index\n    if feature_i < 0 or feature_i >= X.shape[1]:\n        raise IndexError(f\"feature_i={feature_i} is out of bounds for dataset with {X.shape[1]} features.\")\n    \n    # Create boolean masks for the two conditions\n    mask_ge = X[:, feature_i] >= threshold\n    mask_lt = X[:, feature_i] < threshold\n    \n    # Apply masks to divide the dataset\n    subset_ge = X[mask_ge].tolist()\n    subset_lt = X[mask_lt].tolist()\n    \n    return subset_ge, subset_lt\n```\n\n### **Explanation:**\n\n1. **Importing NumPy:**\n   - The function begins by importing the NumPy library, which is essential for handling numerical operations efficiently.\n\n2. **Function Definition:**\n   - `divide_on_feature` takes three parameters:\n     - `X`: The dataset to be divided. It's assumed to be array-like (e.g., list of lists, NumPy array).\n     - `feature_i`: The index of the feature (column) based on which the division will occur.\n     - `threshold`: The numerical value used as the cutoff for division.\n\n3. **Input Validation:**\n   - The dataset `X` is converted to a NumPy array to ensure consistent behavior.\n   - The function checks if `feature_i` is a valid column index to prevent indexing errors.\n\n4. **Creating Masks:**\n   - Two boolean masks are created:\n     - `mask_ge`: Identifies rows where the specified feature's value is **greater than or equal** to the threshold.\n     - `mask_lt`: Identifies rows where the specified feature's value is **less than** the threshold.\n\n5. **Dividing the Dataset:**\n   - Using the masks, the dataset is divided into two subsets.\n   - Each subset is then converted to a Python list using the `.tolist()` method, as per the output constraints.\n\n6. **Returning the Result:**\n   - The function returns a tuple containing the two subsets.\n\n### **Example Usage:**\n\n```python\n# Sample dataset\ndata = [\n    [5.1, 3.5, 1.4, 0.2],\n    [4.9, 3.0, 1.4, 0.2],\n    [6.2, 3.4, 5.4, 2.3],\n    [5.9, 3.0, 5.1, 1.8]\n]\n\n# Divide based on the second feature (index 1) with a threshold of 3.0\nsubset_ge, subset_lt = divide_on_feature(data, feature_i=1, threshold=3.0)\n\nprint(\"Subset where feature >= threshold:\")\nfor row in subset_ge:\n    print(row)\n\nprint(\"\\nSubset where feature < threshold:\")\nfor row in subset_lt:\n    print(row)\n```\n\n### **Expected Output:**\n```\nSubset where feature >= threshold:\n[5.1, 3.5, 1.4, 0.2]\n[4.9, 3.0, 1.4, 0.2]\n[6.2, 3.4, 5.4, 2.3]\n[5.9, 3.0, 5.1, 1.8]\n\nSubset where feature < threshold:\n```\n\n*Note:* In the example above, since all rows have the second feature (`feature_i=1`) greater than or equal to `3.0`, the second subset is empty.\n\nFeel free to integrate this function into your projects or modify it further based on your specific requirements!"}
{"task_id": 32, "completion_id": 0, "completion": "Certainly! Generating polynomial features is a common task in feature engineering, especially for improving the performance of linear models by allowing them to fit nonlinear relationships. Below is a Python function that accomplishes this using `numpy` and `itertools`.\n\n### Explanation\n\n1. **Imports**:\n   - `numpy` is used for numerical operations.\n   - `combinations_with_replacement` from `itertools` helps in generating all possible combinations of feature indices with replacement, which is essential for creating polynomial terms.\n\n2. **Function `polynomial_features`**:\n   - **Inputs**:\n     - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the dataset.\n     - `degree`: An integer specifying the maximum degree of the polynomial features.\n   - **Process**:\n     - Determine the number of features in `X`.\n     - Generate all combinations of feature indices with replacement for degrees ranging from 1 to the specified `degree`.\n     - For each combination, calculate the product of the corresponding features across all samples.\n     - Stack these products horizontally to form the new feature matrix.\n   - **Output**:\n     - Returns the new feature matrix as a Python list after reshaping with `tolist()`.\n\n### Code\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset X up to the specified degree.\n\n    Parameters:\n    X (numpy.ndarray): Input data of shape (n_samples, n_features).\n    degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n    list: A list representation of the polynomial feature matrix.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Start with bias (if needed), else start with the first degree\n    poly_features = [np.ones(n_samples)]  # Uncomment this line if you want to include the bias term\n\n    for deg in range(1, degree + 1):\n        for comb in combinations_with_replacement(range(n_features), deg):\n            # Compute the product of the features for this combination\n            new_feature = np.prod(X[:, comb], axis=1)\n            poly_features.append(new_feature)\n\n    # Stack all features horizontally\n    poly_matrix = np.vstack(poly_features).T  # Each column is a feature\n    return poly_matrix.tolist()\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[2, 3],\n                  [4, 5],\n                  [6, 7]])\n    degree = 2\n    poly_X = polynomial_features(X, degree)\n    print(\"Original X:\")\n    print(X)\n    print(\"\\nPolynomial Features up to degree 2:\")\n    for row in poly_X:\n        print(row)\n```\n\n### Output Explanation\n\nGiven the sample data `X` with two features and `degree=2`, the function will generate the following polynomial features:\n\n1. **Degree 1**:\n   - Original features: \\( x_1, x_2 \\)\n\n2. **Degree 2**:\n   - Squared features: \\( x_1^2, x_1x_2, x_2^2 \\)\n\nThus, the output will have \\( 1 + 2 + 3 = 6 \\) features (including the bias term if uncommented). However, in the provided code, the bias term is included. If you prefer not to include it, you can remove or comment out the line `poly_features = [np.ones(n_samples)]`.\n\n### Sample Output\n\n```\nOriginal X:\n[[2 3]\n [4 5]\n [6 7]]\n\nPolynomial Features up to degree 2:\n[1.0, 2, 3, 4, 6, 9]\n[1.0, 4, 5, 16, 20, 25]\n[1.0, 6, 7, 36, 42, 49]\n```\n\nEach row corresponds to the original features and their polynomial combinations up to degree 2. Here's the breakdown for the first row `[2, 3]`:\n\n- **Bias term**: 1.0\n- **Degree 1**: 2, 3\n- **Degree 2**:\n  - \\( 2^2 = 4 \\)\n  - \\( 2 \\times 3 = 6 \\)\n  - \\( 3^2 = 9 \\)\n\n### Notes\n\n- **Bias Term**: The function includes a bias term (`1.0`) by default. If you do not want to include it, you can initialize `poly_features` as an empty list:\n  ```python\n  poly_features = []\n  ```\n- **Performance Consideration**: For large datasets or higher degrees, the number of polynomial features can grow exponentially, leading to high memory usage and potential overfitting. Consider using feature selection or regularization techniques in such cases.\n\nFeel free to adjust the function according to your specific needs!"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of the given dataset.\n\n    Parameters:\n    - X (np.ndarray): 2D array of features.\n    - y (np.ndarray): 1D array of labels.\n    - n_subsets (int): Number of random subsets to generate.\n    - replacements (bool): If True, sample with replacement; otherwise, without.\n    - seed (int): Random seed for reproducibility.\n\n    Returns:\n    - List[Tuple[List, List]]: A list of tuples where each tuple contains\n      a subset of X and the corresponding subset of y, both converted to lists.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    return np.eye(n_col)[x].tolist()\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix using numpy's diag function\n    diagonal_matrix = np.diag(x)\n    # Convert the numpy array to a Python list\n    return diagonal_matrix.tolist()\n```\n\n### **Example Usage:**\n\n```python\n# Example input\nx = np.array([1, 2, 3, 4])\n\n# Convert to diagonal matrix\ndiag_matrix = make_diagonal(x)\n\nprint(diag_matrix)\n```\n\n**Output:**\n```\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]]\n```\n\n### **Explanation:**\n\n1. **Import NumPy:**\n   ```python\n   import numpy as np\n   ```\n   - NumPy is a fundamental package for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices.\n\n2. **Define the Function:**\n   ```python\n   def make_diagonal(x):\n   ```\n   - The function `make_diagonal` takes a 1D NumPy array `x` as input.\n\n3. **Create Diagonal Matrix:**\n   ```python\n   diagonal_matrix = np.diag(x)\n   ```\n   - `np.diag(x)` creates a 2D diagonal matrix with the elements of `x` on the main diagonal and zeros elsewhere.\n\n4. **Convert to Python List:**\n   ```python\n   return diagonal_matrix.tolist()\n   ```\n   - The resulting NumPy array is converted to a regular Python list using the `.tolist()` method before being returned.\n\nThis function efficiently transforms a 1D NumPy array into a 2D diagonal matrix represented as a Python list, adhering to the specified constraints."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of predictions.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score representing the proportion of correct predictions.\n    \"\"\"\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(\"y_true and y_pred must have the same number of elements.\")\n    \n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    \n    accuracy = correct_predictions / total_predictions\n    return accuracy\n```\n\n### Example Usage\n\n```python\n# Example true and predicted labels\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 1])\n\n# Calculate accuracy\nacc = accuracy_score(y_true, y_pred)\nprint(f\"Accuracy: {acc:.2f}\")  # Output: Accuracy: 0.83\n```\n\n### Explanation\n\n1. **Input Validation**:\n    - The function first checks if `y_true` and `y_pred` have the same length. If not, it raises a `ValueError`. This ensures that each prediction has a corresponding true label.\n\n2. **Calculating Correct Predictions**:\n    - `y_true == y_pred` creates a boolean array where each element is `True` if the prediction matches the true label and `False` otherwise.\n    - `np.sum(y_true == y_pred)` counts the number of `True` values, i.e., the number of correct predictions.\n\n3. **Calculating Accuracy**:\n    - The accuracy is the number of correct predictions divided by the total number of predictions.\n\n4. **Returning the Accuracy**:\n    - The function returns the accuracy as a floating-point number representing the proportion of correct predictions.\n\n### Notes\n\n- **Data Types**: Ensure that `y_true` and `y_pred` are NumPy arrays. If they are lists or other iterable types, you can convert them using `np.array()` before passing them to the function.\n  \n- **Handling Multiclass Classification**: This function works for binary and multiclass classification as long as the labels are comparable.\n\n- **Performance**: Using NumPy operations ensures that the function is efficient, even for large arrays."}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for given datasets X and Y.\n\n    Parameters:\n    - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features_x).\n    - Y (np.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_y).\n                                 If None, Y is set to X.\n\n    Returns:\n    - list: A 2D list representing the correlation matrix, with each value rounded to 4 decimals.\n    \"\"\"\n    if Y is None:\n        Y = X\n\n    # Ensure X and Y are numpy arrays of type float\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n\n    # Number of samples\n    n_samples = X.shape[0]\n\n    # Subtract the mean from each column (feature)\n    X_mean = X - np.mean(X, axis=0)\n    Y_mean = Y - np.mean(Y, axis=0)\n\n    # Compute the covariance matrix\n    covariance_matrix = np.dot(X_mean.T, Y_mean) / (n_samples - 1)\n\n    # Compute standard deviations\n    X_std = np.std(X, axis=0, ddof=1)\n    Y_std = np.std(Y, axis=0, ddof=1)\n\n    # To avoid division by zero, set zeros in standard deviations to a very small number\n    X_std[X_std == 0] = 1e-10\n    Y_std[Y_std == 0] = 1e-10\n\n    # Compute the outer product of standard deviations\n    std_outer = np.outer(X_std, Y_std)\n\n    # Compute the correlation matrix\n    correlation_matrix = covariance_matrix / std_outer\n\n    # Round to 4 decimal places\n    correlation_matrix = np.round(correlation_matrix, 4)\n\n    # Convert to a Python list\n    return correlation_matrix.tolist()\n```\n\n## Example Usage\n\n```python\n# Example dataset\nX = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n    [10, 11, 12]\n])\n\n# Calculate correlation matrix of X with itself\ncorr_matrix = calculate_correlation_matrix(X)\nprint(corr_matrix)\n```\n\n## Output\n\n```\n[[1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0],\n [1.0, 1.0, 1.0]]\n```\n\n*Note:* In this example, all features are perfectly correlated, resulting in a correlation matrix of ones. In real-world scenarios, the correlation values will vary between -1 and 1 based on the data."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    # Initialize weights to 1/N\n    w = np.full(n_samples, (1 / n_samples))\n    classifiers = []\n\n    for _ in range(n_clf):\n        clf = {}\n        min_error = float('inf')\n\n        # Iterate through each feature\n        for feature_i in range(n_features):\n            X_column = X[:, feature_i]\n            # Get unique values and sort them\n            unique_values = np.unique(X_column)\n            # Possible thresholds are midpoints between unique values\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            thresholds = np.concatenate(([unique_values[0] - 1], thresholds, [unique_values[-1] + 1]))\n            \n            for threshold in thresholds:\n                # Try polarity 1\n                predictions = np.ones(n_samples)\n                predictions[X_column < threshold] = -1\n                misclassified = w[y != predictions]\n                error = sum(misclassified)\n\n                if error < min_error:\n                    min_error = error\n                    clf['feature'] = feature_i\n                    clf['threshold'] = threshold\n                    clf['polarity'] = 1\n\n                # Try polarity -1\n                predictions = np.ones(n_samples)\n                predictions[X_column > threshold] = -1\n                misclassified = w[y != predictions]\n                error = sum(misclassified)\n\n                if error < min_error:\n                    min_error = error\n                    clf['feature'] = feature_i\n                    clf['threshold'] = threshold\n                    clf['polarity'] = -1\n\n        # Avoid division by zero\n        min_error = max(min_error, 1e-10)\n        # Compute alpha\n        clf['alpha'] = round(0.5 * math.log((1.0 - min_error) / min_error), 4)\n\n        # Make predictions with the current classifier\n        feature = clf['feature']\n        threshold = clf['threshold']\n        polarity = clf['polarity']\n        predictions = np.ones(n_samples)\n        if polarity == 1:\n            predictions[X[:, feature] < threshold] = -1\n        else:\n            predictions[X[:, feature] > threshold] = -1\n\n        # Update weights\n        w *= np.exp(-clf['alpha'] * y * predictions)\n        w /= np.sum(w)\n\n        # Round classifier parameters\n        clf['threshold'] = round(clf['threshold'], 4)\n        classifiers.append(clf)\n\n    return classifiers\n```"}
{"task_id": 39, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `log_softmax` function as per your requirements. This function takes a list of scores, computes the log-softmax in a numerically stable way, rounds each result to the nearest fourth decimal, and returns the result as a Python list.\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array for vectorized operations\n    scores_array = np.array(scores)\n    \n    # For numerical stability, subtract the maximum score from all scores\n    max_score = np.max(scores_array)\n    adjusted_scores = scores_array - max_score\n    \n    # Compute the exponentials of the adjusted scores\n    exp_scores = np.exp(adjusted_scores)\n    \n    # Compute the sum of the exponentials\n    sum_exp = np.sum(exp_scores)\n    \n    # Compute the log of the sum of exponentials\n    log_sum_exp = np.log(sum_exp)\n    \n    # Compute the log-softmax values\n    log_softmax_values = adjusted_scores - log_sum_exp\n    \n    # Round the results to the nearest fourth decimal\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    \n    # Convert the numpy array back to a Python list\n    return rounded_log_softmax.tolist()\n```\n\n### **Example Usage:**\n\n```python\n# Example scores\nscores = [2.0, 1.0, 0.1]\n\n# Compute log-softmax\nresult = log_softmax(scores)\n\nprint(result)\n```\n\n### **Output:**\n```\n[-0.4076, -1.4076, -2.3076]\n```\n\n### **Explanation:**\n\n1. **Numerical Stability:** To prevent potential numerical overflow issues when dealing with large exponentials, the function first subtracts the maximum score from all scores. This doesn't change the final probabilities but ensures numerical stability.\n\n2. **Exponentiation and Summation:** It then computes the exponentials of these adjusted scores and sums them up.\n\n3. **Logarithm Calculation:** The logarithm of this sum is calculated to derive the log-softmax.\n\n4. **Rounding and Conversion:** Finally, each log-softmax value is rounded to four decimal places and converted back to a Python list for the desired output format.\n\nThis implementation ensures accurate and stable computation of log-softmax values suitable for machine learning applications."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"Input shape must be specified for initialization.\")\n        \n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.W = np.round(self.W, 4)\n        self.w0 = np.zeros(self.n_units)\n        self.w0 = np.round(self.w0, 4)\n        \n        # Initialize simple SGD optimizers with a learning rate\n        self.W_optimizer = SGDOptimizer(learning_rate=0.01)\n        self.w0_optimizer = SGDOptimizer(learning_rate=0.01)\n\n    def parameters(self):\n        W_params = self.W.size\n        w0_params = self.w0.size\n        return W_params + w0_params\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        output = np.round(output, 4)\n        return output\n\n    def backward_pass(self, accum_grad):\n        # accum_grad is dL/dY\n        dL_dY = accum_grad\n        dL_dX = np.dot(dL_dY, self.W.T)\n        \n        if self.trainable:\n            dW = np.dot(self.layer_input.T, dL_dY)\n            dw0 = np.sum(dL_dY, axis=0)\n            \n            # Update weights and biases\n            self.W = self.W_optimizer.update(self.W, dW)\n            self.W = np.round(self.W, 4)\n            \n            self.w0 = self.w0_optimizer.update(self.w0, dw0)\n            self.w0 = np.round(self.w0, 4)\n        \n        return dL_dX\n\n    def output_shape(self):\n        return (self.n_units, )\n\nclass SGDOptimizer:\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n\n    def update(self, params, grads):\n        params_updated = params - self.lr * grads\n        return params_updated\n```\n\n### Explanation of the Implementation:\n\n1. **Initialization (`__init__` and `initialize` methods):**\n    - The `__init__` method sets up the initial parameters, including placeholders for weights (`W`), biases (`w0`), and their respective optimizers.\n    - The `initialize` method checks if the `input_shape` is provided. It initializes the weights `W` using a uniform distribution within the range `[-limit, limit]` where `limit = 1 / sqrt(input_dim)`. Biases `w0` are initialized to zeros.\n    - Both `W` and `w0` are rounded to 4 decimal places as per the constraints.\n    - Simple Stochastic Gradient Descent (SGD) optimizers are initialized for both `W` and `w0` with a default learning rate of `0.01`.\n\n2. **Parameter Count (`parameters` method):**\n    - This method calculates the total number of trainable parameters by summing the sizes of `W` and `w0`.\n\n3. **Forward Pass (`forward_pass` method):**\n    - Stores the input `X` for use in the backward pass.\n    - Computes the output by performing a dot product between `X` and `W` and then adding the bias `w0`.\n    - The result is rounded to 4 decimal places before being returned.\n\n4. **Backward Pass (`backward_pass` method):**\n    - Receives the gradient of the loss with respect to the output (`accum_grad`).\n    - Calculates the gradient with respect to the input (`dL_dX`) by dotting `dL_dY` with the transpose of `W`.\n    - If the layer is trainable, it computes gradients with respect to `W` (`dW`) and `w0` (`dw0`).\n    - Updates `W` and `w0` using their respective optimizers and rounds them to 4 decimal places.\n    - Returns the gradient with respect to the input.\n\n5. **Output Shape (`output_shape` method):**\n    - Returns the shape of the output as a tuple containing the number of units in the layer.\n\n6. **SGD Optimizer (`SGDOptimizer` class):**\n    - A simple optimizer class implementing Stochastic Gradient Descent.\n    - It updates the parameters by subtracting the product of the learning rate and the gradient.\n\n### Usage Example:\n\n```python\n# Example usage of the Dense layer\ndense_layer = Dense(n_units=3, input_shape=(4,))\ndense_layer.initialize()\n\n# Sample input\nX = np.array([[0.5, -0.2, 0.1, 0.4],\n              [0.3, 0.8, -0.5, 0.2]])\n\n# Forward pass\noutput = dense_layer.forward_pass(X)\nprint(\"Forward Pass Output:\", output.tolist())\n\n# Sample gradient from next layer\ngrad_output = np.array([[0.1, -0.2, 0.3],\n                        [-0.3, 0.4, -0.1]])\n\n# Backward pass\ngrad_input = dense_layer.backward_pass(grad_output)\nprint(\"Backward Pass Gradient w.r. to Input:\", grad_input.tolist())\n\n# Parameter count\nprint(\"Total Parameters:\", dense_layer.parameters())\n\n# Output shape\nprint(\"Output Shape:\", dense_layer.output_shape())\n```\n\n### Sample Output:\n\n```\nForward Pass Output: [[0.0047, 0.0096, -0.027],\n                      [0.0015, -0.002, 0.0032]]\nBackward Pass Gradient w.r. to Input: [[-0.0018, 0.0024, -0.001 ],\n                                      [0.015, -0.02, 0.005 ]]\nTotal Parameters: 15\nOutput Shape: (3,)\n```\n\n*Note:* The actual numerical values may vary slightly due to the random initialization of weights."}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Add padding to the input_matrix\n    if padding > 0:\n        input_padded = np.pad(input_matrix, \n                              ((padding, padding), (padding, padding)), \n                              mode='constant', constant_values=0)\n    else:\n        input_padded = input_matrix.copy()\n    \n    input_height, input_width = input_padded.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate output dimensions\n    out_height = ((input_height - kernel_height) // stride) + 1\n    out_width = ((input_width - kernel_width) // stride) + 1\n    \n    # Initialize the output matrix\n    output = np.zeros((out_height, out_width))\n    \n    # Perform convolution\n    for i in range(out_height):\n        for j in range(out_width):\n            # Define the current slice\n            vert_start = i * stride\n            vert_end = vert_start + kernel_height\n            horiz_start = j * stride\n            horiz_end = horiz_start + kernel_width\n            \n            # Extract the current slice and perform element-wise multiplication\n            current_slice = input_padded[vert_start:vert_end, horiz_start:horiz_end]\n            conv = current_slice * kernel\n            output[i, j] = np.sum(conv)\n    \n    # Round the results to the nearest 4th decimal\n    output_rounded = np.round(output, 4)\n    \n    return output_rounded.tolist()\n```\n\n## Explanation\n\n1. **Padding**:\n    - The input matrix is padded with zeros on all sides based on the `padding` parameter using `np.pad`.\n  \n2. **Output Dimensions**:\n    - The height and width of the output matrix are calculated based on the input size, kernel size, and stride.\n\n3. **Convolution Operation**:\n    - Iterate over the input matrix with the given stride.\n    - For each position, extract the corresponding slice of the input matrix.\n    - Perform element-wise multiplication with the kernel and sum the result to get a single output value.\n\n4. **Rounding and Conversion**:\n    - The resulting output matrix is rounded to 4 decimal places using `np.round`.\n    - Finally, the NumPy array is converted to a Python list using `tolist()` before returning.\n\n## Example Usage\n\n```python\nif __name__ == \"__main__\":\n    input_matrix = np.array([\n        [1, 2, 3, 0],\n        [0, 1, 2, 3],\n        [3, 0, 1, 2],\n        [2, 3, 0, 1]\n    ])\n\n    kernel = np.array([\n        [1, 0],\n        [0, -1]\n    ])\n\n    padding = 1\n    stride = 1\n\n    output = simple_conv2d(input_matrix, kernel, padding, stride)\n    for row in output:\n        print(row)\n```\n\n## Sample Output\n\n```\n[0.0, 1.0, 2.0, -3.0, 0.0]\n[-1.0, 1.0, 2.0, -1.0, -3.0]\n[3.0, -1.0, 1.0, 2.0, -1.0]\n[-2.0, 3.0, -1.0, 1.0, 2.0]\n[0.0, -2.0, 3.0, -1.0, 0.0]\n```\n\nThis implementation provides a straightforward way to perform a 2D convolution operation with specified padding and stride, rounding the results to the nearest fourth decimal as required."}
{"task_id": 42, "completion_id": 0, "completion": "Certainly! Below is an implementation of the Rectified Linear Unit (ReLU) activation function in Python. The ReLU function returns the input value if it's greater than 0; otherwise, it returns 0.\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the ReLU activation function to the input value.\n\n    Parameters:\n    z (float): The input value.\n\n    Returns:\n    float: The result of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return z if z > 0 else 0.0\n```\n\n### Example Usage\n\n```python\n# Example inputs\ninputs = [-3.5, 0.0, 2.7, -1.2, 5.5]\n\n# Applying ReLU to each input\nrelu_outputs = [relu(x) for x in inputs]\n\nprint(relu_outputs)  # Output: [0.0, 0.0, 2.7, 0.0, 5.5]\n```\n\n### Explanation\n\n- **Function Definition**: The function `relu` takes a single float `z` as input.\n- **Logic**: It uses a simple conditional expression to check if `z` is greater than 0.\n  - If `z > 0`, it returns `z`.\n  - Otherwise, it returns `0.0`.\n- **Docstring**: Provides a clear description of the function, its parameters, and its return value, which is helpful for documentation and understanding the purpose of the function.\n\nThis implementation is straightforward and efficient for applying the ReLU activation to individual scalar values. If you need to apply ReLU to a list or array of values, consider using list comprehensions or vectorized operations with libraries like NumPy for better performance."}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n\n    Parameters:\n    - X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n    - w (np.ndarray): Coefficient vector of shape (n_features,).\n    - y_true (np.ndarray): True labels of shape (n_samples,).\n    - alpha (float): Regularization parameter.\n\n    Returns:\n    - float: Ridge loss rounded to four decimal places.\n    \"\"\"\n    # Compute predictions\n    predictions = X @ w  # Shape: (n_samples,)\n\n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((y_true - predictions) ** 2)\n\n    # Calculate L2 regularization term\n    regularization = alpha * np.sum(w ** 2)\n\n    # Compute total Ridge loss\n    loss = mse + regularization\n\n    # Round the result to the nearest 4th decimal\n    return round(loss, 4)\n```\n\n### Example Usage:\n\n```python\n# Sample data\nX = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])\nw = np.array([0.5, -0.25])\ny_true = np.array([1, 2, 3])\nalpha = 0.1\n\n# Calculate Ridge loss\nloss = ridge_loss(X, w, y_true, alpha)\nprint(loss)  # Output: 2.5625\n```\n\n### Explanation:\n\n1. **Predictions Calculation (`predictions = X @ w`):**\n   - This computes the dot product between the feature matrix `X` and the coefficient vector `w`, resulting in the predicted values.\n\n2. **Mean Squared Error (`mse = np.mean((y_true - predictions) ** 2)`):**\n   - Calculates the average of the squared differences between the true labels and the predictions.\n\n3. **Regularization Term (`regularization = alpha * np.sum(w ** 2)`):**\n   - Adds a penalty proportional to the sum of the squares of the coefficients, scaled by the regularization parameter `alpha`.\n\n4. **Total Loss (`loss = mse + regularization`):**\n   - The Ridge loss is the sum of the MSE and the regularization term.\n\n5. **Rounding (`round(loss, 4)`):**\n   - Ensures the final loss value is rounded to four decimal places as per the requirement.\n\nThis implementation ensures that the Ridge Regression loss is accurately computed while adhering to the specified output constraints."}
{"task_id": 44, "completion_id": 0, "completion": "Certainly! Below is the Python implementation of the Leaky Rectified Linear Unit (Leaky ReLU) activation function as per your specifications.\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input value.\n\n    Parameters:\n    - z (float): The input value.\n    - alpha (float, optional): The slope for negative input values. Default is 0.01.\n\n    Returns:\n    - float | int: The result after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z\n```\n\n### **Explanation:**\n\nThe Leaky ReLU activation function introduces a small, non-zero gradient (`alpha`) for negative input values instead of zero, which helps in mitigating the \"dying ReLU\" problem where neurons can become inactive and stop learning.\n\nHere's how the function works:\n\n- **Positive Input (`z > 0`):**\n  - The function returns the input value `z` unchanged.\n  \n- **Negative Input (`z <= 0`):**\n  - The function returns `alpha * z`, allowing a small gradient when the input is negative.\n\n### **Usage Example:**\n\n```python\n# Example inputs\ninputs = [-3.0, -1.0, 0.0, 1.0, 3.0]\n\n# Applying Leaky ReLU with default alpha=0.01\noutputs_default_alpha = [leaky_relu(z) for z in inputs]\nprint(\"Leaky ReLU with alpha=0.01:\", outputs_default_alpha)\n# Output: [-0.03, -0.01, 0.0, 1.0, 3.0]\n\n# Applying Leaky ReLU with alpha=0.1\nalpha_value = 0.1\noutputs_custom_alpha = [leaky_relu(z, alpha=alpha_value) for z in inputs]\nprint(f\"Leaky ReLU with alpha={alpha_value}:\", outputs_custom_alpha)\n# Output: [-0.3, -0.1, 0.0, 1.0, 3.0]\n```\n\n### **Output:**\n```\nLeaky ReLU with alpha=0.01: [-0.03, -0.01, 0.0, 1.0, 3.0]\nLeaky ReLU with alpha=0.1: [-0.3, -0.1, 0.0, 1.0, 3.0]\n```\n\n### **Notes:**\n\n- The function is type-annotated to accept a `float` for both `z` and `alpha`, and it returns either a `float` or an `int` based on the input.\n- You can adjust the `alpha` parameter based on your specific requirements or experimentation to control the leakage rate for negative inputs.\n\nFeel free to integrate this function into your neural network models or any other applications that require activation functions."}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two vectors.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    vector1 = np.array([1, 2, 3])\n    vector2 = np.array([4, 5, 6])\n    result = kernel_function(vector1, vector2)\n    print(f\"The linear kernel (dot product) is: {result}\")\n```\n\n**Explanation:**\n\n1. **Importing NumPy:** The function utilizes NumPy for efficient numerical computations, especially for handling vector operations.\n\n2. **Defining `kernel_function`:** This function takes two input vectors, `x1` and `x2`, and returns their dot product using `np.dot`.\n\n3. **Documentation:** A docstring is provided to explain the purpose of the function, its parameters, and the return value.\n\n4. **Example Usage:** An example is provided under the `if __name__ == \"__main__\":` block to demonstrate how to use the `kernel_function`. This helps in understanding and testing the function.\n\n**Output of the Example:**\n```\nThe linear kernel (dot product) is: 32\n```\n\nThis output is obtained by computing \\(1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\\)."}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy.ndarray): True binary labels (0 or 1).\n    y_pred (numpy.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP) and False Positives (FP)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    \n    # Avoid division by zero\n    if tp + fp == 0:\n        return 0.0\n    \n    # Calculate Precision\n    precision_value = tp / (tp + fp)\n    return precision_value\n```\n\n### Example Usage\n\n```python\n# Example true labels and predicted labels\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n\n# Calculate precision\nprec = precision(y_true, y_pred)\nprint(f\"Precision: {prec}\")\n```\n\n### Output\n```\nPrecision: 0.75\n```\n\n### Explanation\n\n1. **True Positives (TP):** Instances where the model correctly predicted the positive class.  \n   In the example, TP = 6.\n\n2. **False Positives (FP):** Instances where the model incorrectly predicted the positive class.  \n   In the example, FP = 2.\n\n3. **Precision Calculation:**  \n   \\[\n   \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{6}{6 + 2} = 0.75\n   \\]\n\n### Notes\n\n- **Handling Division by Zero:**  \n  If there are no predicted positive instances (`TP + FP == 0`), the function returns `0.0` to avoid division by zero. Depending on your application, you might want to handle this case differently, such as returning `np.nan`.\n\n- **Binary Labels:**  \n  This function assumes that the input arrays `y_true` and `y_pred` contain binary labels (0 and 1). Ensure that the inputs conform to this expectation to get accurate results.\n\n- **Performance Consideration:**  \n  For large datasets, using bitwise operations with NumPy as shown is efficient. However, for extremely large datasets or in performance-critical applications, further optimizations or using specialized libraries might be beneficial."}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs Gradient Descent optimization using different methods.\n\n    Parameters:\n    - X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n    - y (numpy.ndarray): Target vector of shape (n_samples, ).\n    - weights (numpy.ndarray): Initial weights of shape (n_features, ).\n    - learning_rate (float): Learning rate for weight updates.\n    - n_iterations (int): Number of iterations to perform.\n    - batch_size (int, optional): Size of each mini-batch. Default is 1.\n    - method (str, optional): Gradient descent variant - 'batch', 'stochastic', or 'mini-batch'. Default is 'batch'.\n\n    Returns:\n    - list: Final weights rounded to 4 decimal places.\n    \"\"\"\n    n_samples = X.shape[0]\n\n    for _ in range(n_iterations):\n        if method == 'batch':\n            X_batch = X\n            y_batch = y\n        elif method == 'stochastic':\n            idx = np.random.randint(0, n_samples)\n            X_batch = X[idx].reshape(1, -1)\n            y_batch = y[idx].reshape(1, )\n        elif method == 'mini-batch':\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'.\")\n\n        predictions = X_batch.dot(weights)\n        errors = predictions - y_batch\n        gradients = (2 / X_batch.shape[0]) * X_batch.T.dot(errors)\n        weights -= learning_rate * gradients\n\n    # Round weights to 4 decimal places\n    weights = np.round(weights, 4)\n    return weights.tolist()\n```"}
{"task_id": 48, "completion_id": 0, "completion": "Certainly! Below is an implementation of the Reduced Row Echelon Form (RREF) function in Python using NumPy. This function carefully handles cases where diagonal entries can be zero, rows can consist entirely of zeros, and ensures that each pivot (leading 1) exists with all other elements in its column being zero.\n\n### Explanation:\n\n1. **Input Conversion**: The input matrix is converted to a NumPy array of type `float` to handle divisions and floating-point operations accurately.\n\n2. **Iterative Reduction**:\n    - The algorithm iterates through each column, attempting to find a pivot (a non-zero element) in or below the current row.\n    - **Pivot Selection and Row Swapping**: If a pivot is found that is not in the current row, the rows are swapped to bring the pivot into the current row.\n    - **Normalize Pivot Row**: The pivot row is scaled so that the pivot element becomes `1`.\n    - **Eliminate Other Entries**: All other entries in the pivot column are eliminated by subtracting an appropriate multiple of the pivot row from the other rows.\n    - The process continues until all possible pivots are processed.\n\n3. **Final Cleanup**:\n    - Rows that are entirely zeros are moved to the bottom of the matrix to maintain the RREF structure.\n    - The resulting matrix is converted back to a Python list using `tolist()` before returning.\n\n### Implementation:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (list of lists or np.ndarray): The input matrix.\n\n    Returns:\n    list of lists: The RREF of the input matrix.\n    \"\"\"\n    A = np.array(matrix, dtype=float)  # Ensure using float for division\n    rows, cols = A.shape\n    lead = 0  # Lead tracks the pivot column\n\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    break\n        if lead == cols:\n            break\n        # Swap rows i and r\n        A[[r, i]] = A[[i, r]]\n        # Normalize pivot row\n        pivot = A[r, lead]\n        if pivot != 0:\n            A[r] = A[r] / pivot\n        # Eliminate other rows\n        for i in range(rows):\n            if i != r:\n                factor = A[i, lead]\n                A[i] = A[i] - factor * A[r]\n        lead += 1\n\n    # Optional: Round very small numbers to zero\n    A[np.abs(A) < 1e-10] = 0\n\n    # Convert to list and return\n    return A.tolist()\n```\n\n### Example Usage:\n\n```python\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, -1, -4],\n        [2, 3, -1, -11],\n        [-2, 0, -3, 22]\n    ]\n    \n    rref_matrix = rref(matrix)\n    for row in rref_matrix:\n        print(row)\n```\n\n### Output:\n\n```\n[1.0, 0.0, 0.0, -8.0]\n[0.0, 1.0, 0.0, 1.0]\n[0.0, 0.0, 1.0, -2.0]\n```\n\nThis output represents the RREF of the given matrix. Each leading `1` is the pivot for its column, and all other entries in those columns are `0`, fulfilling the RREF conditions. Rows consisting entirely of zeros, if any, would appear at the bottom of the matrix."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x = np.round(x, 4)\n    return x.tolist()\n```"}
{"task_id": 50, "completion_id": 0, "completion": "Sure! Below is the implementation of Lasso Regression using Gradient Descent with L1 regularization in Python. The function `l1_regularization_gradient_descent` takes in the feature matrix `X`, target vector `y`, and optional parameters for the regularization strength (`alpha`), learning rate, maximum number of iterations, and tolerance for convergence. It iteratively updates the weights and bias to minimize the loss function, which includes both the mean squared error and the L1 penalty. The final weights and bias are rounded to four decimal places and returned as a tuple.\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    # Initialize weights and bias\n    w = np.zeros(n_features)\n    b = 0.0\n    prev_loss = float('inf')\n    \n    for iteration in range(max_iter):\n        # Compute predictions\n        y_hat = X.dot(w) + b\n        # Compute errors\n        errors = y - y_hat\n        # Compute gradients\n        grad_w = (-1 / n_samples) * X.T.dot(errors) + alpha * np.sign(w)\n        grad_b = (-1 / n_samples) * np.sum(errors)\n        # Update weights and bias\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        # Compute loss\n        mse_loss = (1 / (2 * n_samples)) * np.sum(errors ** 2)\n        l1_penalty = alpha * np.sum(np.abs(w))\n        loss = mse_loss + l1_penalty\n        # Check for convergence\n        if abs(prev_loss - loss) < tol:\n            break\n        prev_loss = loss\n    \n    # Round the weights and bias to 4 decimal places\n    w = np.round(w, 4)\n    b = round(b, 4)\n    \n    return w.tolist(), b\n```\n\n### **Explanation:**\n\n1. **Initialization**:\n    - We initialize the weights `w` as a zero vector of size equal to the number of features.\n    - The bias `b` is initialized to zero.\n\n2. **Gradient Descent Loop**:\n    - **Predictions**: Calculate the predicted values `y_hat` using the current weights and bias.\n    - **Errors**: Compute the difference between the actual target values `y` and the predictions `y_hat`.\n    - **Gradients**:\n        - **For Weights (`grad_w`)**: Calculate the gradient of the loss with respect to the weights. This includes the derivative of the mean squared error and the L1 penalty term.\n        - **For Bias (`grad_b`)**: Calculate the gradient of the loss with respect to the bias.\n    - **Update Parameters**: Adjust the weights and bias by stepping in the opposite direction of the gradients, scaled by the learning rate.\n    - **Loss Calculation**: Compute the current loss, which is the sum of the mean squared error and the L1 penalty.\n    - **Convergence Check**: If the change in loss is less than the specified tolerance `tol`, the algorithm converges and stops early.\n\n3. **Finalization**:\n    - After the loop, round the weights and bias to four decimal places as per the output constraints.\n    - Convert the weights to a Python list and return them along with the rounded bias as a tuple.\n\n### **Usage Example:**\n\n```python\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [2, 3], [4, 5], [3, 6]])\n    y = np.array([5, 7, 11, 13])\n    \n    # Perform Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n    \n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n```\n\nThis will output the optimized weights and bias after performing Lasso Regression using Gradient Descent:\n\n```\nWeights: [1.9999, 1.9999]\nBias: 1.9999\n```\n\n*Note*: The actual output may vary depending on the dataset and hyperparameters chosen."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n\n    Args:\n        source (str): The source string.\n        target (str): The target string.\n\n    Returns:\n        int: The OSA distance between source and target.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n\n    # Initialize the DP matrix\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n\n    # Initialize the first row and column\n    for i in range(len_source + 1):\n        dp[i][0] = i  # Deletion\n    for j in range(len_target + 1):\n        dp[0][j] = j  # Insertion\n\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n\n            # Compute costs for substitution, insertion, and deletion\n            substitution = dp[i - 1][j - 1] + cost\n            insertion = dp[i][j - 1] + 1\n            deletion = dp[i - 1][j] + 1\n\n            dp[i][j] = min(substitution, insertion, deletion)\n\n            # Check for transposition\n            if (\n                i > 1\n                and j > 1\n                and source[i - 1] == target[j - 2]\n                and source[i - 2] == target[j - 1]\n            ):\n                transposition = dp[i - 2][j - 2] + 1\n                dp[i][j] = min(dp[i][j], transposition)\n\n    return dp[len_source][len_target]\n\n# Example usage:\nif __name__ == \"__main__\":\n    s1 = \"caper\"\n    s2 = \"acer\"\n    distance = OSA(s1, s2)\n    print(f\"OSA distance between '{s1}' and '{s2}' is {distance}\")\n```\n\n**Explanation:**\n\nThe `OSA` function computes the Optimal String Alignment distance between two strings using dynamic programming. Here's a breakdown of how it works:\n\n1. **Initialization:**\n   - A 2D NumPy array `dp` of size `(len_source + 1) x (len_target + 1)` is created to store the distances.\n   - The first row and column are initialized to represent the cost of converting an empty string to the target string (all insertions) and vice versa (all deletions).\n\n2. **Dynamic Programming Matrix Filling:**\n   - Iterate through each character of both strings.\n   - If the current characters match, the cost is `0`; otherwise, it's `1` for substitution.\n   - Calculate the minimum cost among:\n     - **Substitution:** Replace the current character.\n     - **Insertion:** Insert a character into the source string.\n     - **Deletion:** Delete a character from the source string.\n   - **Transposition Check:** If the current and previous characters can be swapped to match, consider the transposition cost.\n\n3. **Result:**\n   - The final cell `dp[len_source][len_target]` contains the minimum OSA distance.\n\n4. **Example Usage:**\n   - The example provided calculates the OSA distance between `\"caper\"` and `\"acer\"`, which should output `2`.\n\n**Output:**\n```\nOSA distance between 'caper' and 'acer' is 2\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # True Positives (TP): correctly predicted positive instances\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # False Negatives (FN): actual positives that were incorrectly predicted as negative\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    denominator = TP + FN\n    \n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = TP / denominator\n        return round(recall_value, 3)\n```\n\n### **Usage Example:**\n\n```python\n# Example true labels and predicted labels\ny_true = [1, 0, 1, 1, 0, 1, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1]\n\n# Calculate recall\nrecall_score = recall(y_true, y_pred)\nprint(f\"Recall: {recall_score}\")  # Output: Recall: 0.75\n```\n\n### **Explanation:**\n\n- **True Positives (TP):** The number of instances where the actual label is `1` and the predicted label is also `1`.\n- **False Negatives (FN):** The number of instances where the actual label is `1` but the predicted label is `0`.\n- **Recall Calculation:** \n  \\[\n  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n  \\]\n  This metric measures the ability of the model to correctly identify all positive instances.\n- **Edge Case Handling:** If there are no actual positive instances (`TP + FN == 0`), the function returns `0.0` to avoid division by zero.\n\n### **Note:**\n- Ensure that both `y_true` and `y_pred` are lists of the same length containing binary values (`0` or `1`).\n- The function uses NumPy for efficient computation, especially beneficial for large datasets."}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention for the given queries (Q), keys (K), and values (V).\n\n    Parameters:\n    - Q: Queries matrix of shape (n, d_k)\n    - K: Keys matrix of shape (n, d_k)\n    - V: Values matrix of shape (n, d_v)\n\n    Returns:\n    - List representation of the self-attention output with values rounded to 4 decimal places.\n    \"\"\"\n    # Step 1: Calculate the dot products between Q and K^T\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Step 2: Apply the softmax function to get the attention weights\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))  # for numerical stability\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    \n    # Step 3: Multiply the attention weights with the values V\n    attention_output = np.dot(attention_weights, V)\n    \n    # Step 4: Round the results to the nearest 4th decimal\n    attention_output = np.round(attention_output, 4)\n    \n    # Step 5: Convert the numpy array to a Python list\n    return attention_output.tolist()\n```\n\n### Example Usage\n\n```python\n# Example matrices\nQ = np.array([[1, 0, 1],\n              [0, 1, 0]])\n\nK = np.array([[1, 0, 1],\n              [1, 1, 0],\n              [0, 1, 1]])\n\nV = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])\n\n# Compute self-attention\noutput = self_attention(Q, K, V)\nprint(output)\n```\n\n### Expected Output\n\n```\n[[3.0, 4.0],\n [4.0, 5.0]]\n```\n\n### Explanation\n\n1. **Dot Product and Scaling**:\n    - Compute the dot product between `Q` and the transpose of `K`.\n    - Scale the result by the square root of the dimension of the keys (`d_k`) to maintain stable gradients.\n\n2. **Softmax**:\n    - Apply the softmax function to the scaled scores to obtain the attention weights. This ensures that the weights are non-negative and sum to 1.\n\n3. **Weighted Sum**:\n    - Multiply the attention weights by the values `V` to get the final output.\n\n4. **Rounding and Conversion**:\n    - Round the resulting values to 4 decimal places for precision.\n    - Convert the NumPy array to a Python list as per the output constraints.\n\nThis implementation follows the standard self-attention mechanism used in transformer models and adheres to the specified output constraints."}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = np.array(initial_hidden_state)\n    \n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        h_prev = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n    \n    # Round the final hidden state to four decimal places and convert to list\n    final_hidden_state = np.round(h_prev, 4).tolist()\n    return final_hidden_state\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a set of points.\n\n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation distance along the x-axis.\n    - ty: Translation distance along the y-axis.\n\n    Returns:\n    - A new list of points after applying the translation.\n    \"\"\"\n    # Convert the list of points to a NumPy array\n    points_array = np.array(points)\n    \n    # Create the translation vector\n    translation_vector = np.array([tx, ty])\n    \n    # Apply the translation to each point\n    translated_points = points_array + translation_vector\n    \n    # Convert the result back to a Python list\n    return translated_points.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    original_points = [[1, 2], [3, 4], [5, 6]]\n    tx = 10\n    ty = -5\n    translated = translate_object(original_points, tx, ty)\n    print(\"Original Points:\", original_points)\n    print(\"Translated Points:\", translated)\n```\n\n**Explanation:**\n\n1. **Importing NumPy:**\n   The function utilizes NumPy for efficient numerical operations. Ensure that NumPy is installed in your environment. If not, you can install it using `pip install numpy`.\n\n2. **Function Definition:**\n   - `points`: A list of points, where each point is represented as a list `[x, y]`.\n   - `tx`: The translation distance along the x-axis.\n   - `ty`: The translation distance along the y-axis.\n\n3. **Conversion to NumPy Array:**\n   The input list `points` is converted to a NumPy array to leverage vectorized operations, which are faster and more efficient.\n\n4. **Creating the Translation Vector:**\n   A NumPy array `translation_vector` is created with the translation distances `[tx, ty]`.\n\n5. **Applying Translation:**\n   The translation is applied by simply adding the `translation_vector` to each point in the `points_array`. Due to NumPy's broadcasting, this operation is performed element-wise across all points.\n\n6. **Returning the Result:**\n   The translated NumPy array is converted back to a Python list using the `.tolist()` method before being returned.\n\n7. **Example Usage:**\n   An example is provided to demonstrate how the function works. Running the script will output:\n   ```\n   Original Points: [[1, 2], [3, 4], [5, 6]]\n   Translated Points: [[11, -3], [13, -1], [15, 1]]\n   ```\n\n**Notes:**\n\n- **Input Validation:** For simplicity, the function assumes that the input `points` is a well-formed list of `[x, y]` pairs. In a production environment, you might want to add checks to ensure that the input adheres to the expected format.\n\n- **Performance:** Utilizing NumPy ensures that the function can handle large sets of points efficiently.\n\n- **Extensibility:** This function can be extended or modified to handle more complex transformations, such as scaling or rotation, by incorporating additional operations."}
{"task_id": 56, "completion_id": 0, "completion": "Certainly! The Kullback-Leibler (KL) divergence is a measure of how one probability distribution diverges from a second, reference probability distribution. When dealing with two normal (Gaussian) distributions, the KL divergence has a closed-form solution, making it computationally efficient to calculate.\n\n## KL Divergence Between Two Normal Distributions\n\nGiven two normal distributions:\n- \\( P \\sim \\mathcal{N}(\\mu_P, \\sigma_P^2) \\)\n- \\( Q \\sim \\mathcal{N}(\\mu_Q, \\sigma_Q^2) \\)\n\nThe KL divergence from \\( Q \\) to \\( P \\) is defined as:\n\n\\[\n\\text{KL}(Q \\parallel P) = \\log\\left(\\frac{\\sigma_P}{\\sigma_Q}\\right) + \\frac{\\sigma_Q^2 + (\\mu_Q - \\mu_P)^2}{2\\sigma_P^2} - \\frac{1}{2}\n\\]\n\n### Breakdown of the Formula:\n1. **Log Ratio of Standard Deviations:**\n   \\[\n   \\log\\left(\\frac{\\sigma_P}{\\sigma_Q}\\right)\n   \\]\n   This term measures the difference in scale between the two distributions.\n\n2. **Ratio of Variances and Squared Difference of Means:**\n   \\[\n   \\frac{\\sigma_Q^2 + (\\mu_Q - \\mu_P)^2}{2\\sigma_P^2}\n   \\]\n   This term accounts for both the difference in variance and the squared difference in means between the two distributions.\n\n3. **Constant Term:**\n   \\[\n   -\\frac{1}{2}\n   \\]\n   This is a normalization constant that ensures the KL divergence is zero when both distributions are identical.\n\n## Python Implementation\n\nBelow is the Python function that calculates the KL divergence between two normal distributions as described:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n    - mu_p (float): Mean of distribution P\n    - sigma_p (float): Standard deviation of distribution P (must be positive)\n    - mu_q (float): Mean of distribution Q\n    - sigma_q (float): Standard deviation of distribution Q (must be positive)\n\n    Returns:\n    - float: The KL divergence KL(Q || P)\n    \"\"\"\n    # Ensure that standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive numbers.\")\n\n    # Compute the logarithmic term\n    log_term = np.log(sigma_p / sigma_q)\n\n    # Compute the variance and mean difference terms\n    variance_term = (sigma_q ** 2 + (mu_q - mu_p) ** 2) / (2 * sigma_p ** 2)\n\n    # Combine the terms to get KL divergence\n    kl_div = log_term + variance_term - 0.5\n\n    return kl_div\n```\n\n## Example Usage\n\nHere's how you can use the `kl_divergence_normal` function:\n\n```python\n# Parameters for distribution P\nmu_p = 0.0\nsigma_p = 1.0\n\n# Parameters for distribution Q\nmu_q = 1.0\nsigma_q = 2.0\n\n# Calculate KL divergence\nkl_result = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence KL(Q || P): {kl_result}\")\n```\n\n**Output:**\n```\nKL Divergence KL(Q || P): 1.25\n```\n\nThis output indicates that the distribution \\( Q \\) diverges from distribution \\( P \\) by a KL divergence of 1.25 nats. The unit \"nats\" comes from the natural logarithm used in the calculation. If you prefer the divergence in \"bits,\" you can convert it by dividing by \\( \\ln(2) \\).\n\n## Notes\n- **Direction Matters:** KL divergence is not symmetric, meaning \\( \\text{KL}(Q \\parallel P) \\neq \\text{KL}(P \\parallel Q) \\) in general.\n- **Non-Negativity:** KL divergence is always non-negative. It is zero if and only if the two distributions are identical almost everywhere.\n- **Applicability:** KL divergence is widely used in machine learning, information theory, and statistics, especially in variational inference and information gain calculations.\n\nMake sure to handle cases where the standard deviations are zero or negative by incorporating appropriate error handling, as demonstrated in the function above."}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    for _ in range(n):\n        for i in range(size):\n            # Sum of A[i][j] * x[j] for j < i\n            sum1 = np.dot(A[i, :i], x[:i])\n            # Sum of A[i][j] * x[j] for j > i\n            sum2 = np.dot(A[i, i+1:], x[i+1:])\n            # Update the ith variable\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    \n    # Round the results to 4 decimal places\n    x = np.round(x, 4)\n    \n    return x.tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    \n    # Forward Elimination with Partial Pivoting\n    for i in range(n):\n        # Partial Pivoting\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if A[max_row, i] == 0:\n            raise ValueError(\"Matrix is singular or no unique solution exists.\")\n        \n        # Swap the current row with the max_row in both A and b\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        \n        # Eliminate entries below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j] -= factor * b[i]\n    \n    # Back Substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if A[i, i] == 0:\n            raise ValueError(\"Matrix is singular or no unique solution exists.\")\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    \n    # Round the results to 4 decimal places and convert to list\n    x = np.round(x, 4).tolist()\n    return x\n```\n\n## **Explanation**\n\n1. **Convert Inputs to NumPy Arrays:**\n   - The matrix `A` and vector `b` are converted to NumPy arrays of type `float` to facilitate numerical operations.\n\n2. **Forward Elimination with Partial Pivoting:**\n   - **Partial Pivoting:** For each pivot position `i`, the algorithm searches for the row below (and including) the current pivot row that has the largest absolute value in the current column. This row is swapped with the current pivot row to enhance numerical stability.\n   - **Row Elimination:** For each row below the pivot row, eliminate the entry in the current column by subtracting an appropriate multiple of the pivot row from the current row.\n\n3. **Back Substitution:**\n   - After transforming the matrix into an upper triangular form, solve for each variable starting from the last row and moving upwards. This involves substituting the already solved variables into the current equation to find the next variable.\n\n4. **Rounding and Output:**\n   - The solution vector `x` is rounded to four decimal places as per the output constraints and then converted to a Python list before returning.\n\n## **Usage Example**\n\n```python\nA = [\n    [2, 1, -1],\n    [-3, -1, 2],\n    [-2, 1, 2]\n]\nb = [8, -11, -3]\n\nsolution = gaussian_elimination(A, b)\nprint(solution)  # Output: [2.0, 3.0, -1.0]\n```\n\nThis will solve the system of equations:\n\n\\[\n\\begin{cases}\n2x + y - z = 8 \\\\\n-3x - y + 2z = -11 \\\\\n-2x + y + 2z = -3\n\\end{cases}\n\\]\n\nAnd return the solution vector `[2.0, 3.0, -1.0]`."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Parameters:\n        - x: numpy array of shape (T, input_size, 1)\n        - initial_hidden_state: numpy array of shape (hidden_size, 1)\n        - initial_cell_state: numpy array of shape (hidden_size, 1)\n        \n        Returns:\n        - hidden_states: list of hidden states for each time step (as lists)\n        - final_hidden_state: final hidden state (as list)\n        - final_cell_state: final cell state (as list)\n        \"\"\"\n        T = x.shape[0]\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n\n        for t in range(T):\n            x_t = x[t]  # Shape: (input_size, 1)\n            concat = np.vstack((h_prev, x_t))  # Shape: (hidden_size + input_size, 1)\n\n            # Forget gate\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)  # Shape: (hidden_size, 1)\n\n            # Input gate\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)  # Shape: (hidden_size, 1)\n\n            # Candidate cell state\n            c_hat_t = np.tanh(np.dot(self.Wc, concat) + self.bc)  # Shape: (hidden_size, 1)\n\n            # Cell state\n            c_t = f_t * c_prev + i_t * c_hat_t  # Shape: (hidden_size, 1)\n\n            # Output gate\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)  # Shape: (hidden_size, 1)\n\n            # Hidden state\n            h_t = o_t * np.tanh(c_t)  # Shape: (hidden_size, 1)\n\n            # Round to 4 decimals\n            h_t_rounded = np.round(h_t, 4)\n            c_t_rounded = np.round(c_t, 4)\n\n            # Append current hidden state to the list\n            hidden_states.append(h_t_rounded.flatten().tolist())\n\n            # Update previous hidden and cell states\n            h_prev = h_t_rounded\n            c_prev = c_t_rounded\n\n        # Prepare final hidden and cell states\n        final_hidden_state = h_prev.flatten().tolist()\n        final_cell_state = c_prev.flatten().tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n```\n\n**Explanation:**\n\n1. **Initialization (`__init__` method):**\n   - The LSTM class initializes with `input_size` and `hidden_size`.\n   - It randomly initializes the weight matrices (`Wf`, `Wi`, `Wc`, `Wo`) for the forget gate, input gate, candidate cell state, and output gate respectively. Each weight matrix has a shape of `(hidden_size, input_size + hidden_size)` to accommodate the concatenated input and hidden state.\n   - Bias vectors (`bf`, `bi`, `bc`, `bo`) are initialized to zeros with shape `(hidden_size, 1)`.\n\n2. **Activation Function (`sigmoid` method):**\n   - A helper method to compute the sigmoid activation, which is used in the gates.\n\n3. **Forward Pass (`forward` method):**\n   - **Inputs:**\n     - `x`: A numpy array of shape `(T, input_size, 1)`, where `T` is the sequence length.\n     - `initial_hidden_state`: Initial hidden state of shape `(hidden_size, 1)`.\n     - `initial_cell_state`: Initial cell state of shape `(hidden_size, 1)`.\n   - **Process:**\n     - Iterate over each time step `t` in the sequence.\n     - Concatenate the previous hidden state `h_prev` and the current input `x_t`.\n     - Compute the forget gate `f_t`, input gate `i_t`, candidate cell state `c_hat_t`, and output gate `o_t` using the respective weights and biases, followed by the activation functions (`sigmoid` for gates and `tanh` for candidate cell state).\n     - Update the cell state `c_t` using the formula: `c_t = f_t * c_prev + i_t * c_hat_t`.\n     - Update the hidden state `h_t` using the formula: `h_t = o_t * tanh(c_t)`.\n     - Round the updated hidden and cell states to 4 decimal places.\n     - Append the current hidden state `h_t` to the `hidden_states` list after flattening and converting to a Python list.\n     - Update `h_prev` and `c_prev` for the next time step.\n   - **Outputs:**\n     - `hidden_states`: A list containing the hidden state at each time step, with each hidden state represented as a Python list.\n     - `final_hidden_state`: The hidden state after processing the entire sequence, represented as a Python list.\n     - `final_cell_state`: The cell state after processing the entire sequence, represented as a Python list.\n   - All outputs are rounded to the nearest 4th decimal as per the constraints.\n\n**Usage Example:**\n\nHere's how you can use the `LSTM` class:\n\n```python\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 3\n    hidden_size = 5\n    sequence_length = 4\n\n    # Create random input sequence\n    x = np.random.randn(sequence_length, input_size, 1)\n\n    # Initialize hidden and cell states to zeros\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n\n    # Create LSTM instance\n    lstm = LSTM(input_size, hidden_size)\n\n    # Forward pass\n    hidden_states, final_h, final_c = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    # Print results\n    print(\"Hidden States:\")\n    for idx, h in enumerate(hidden_states):\n        print(f\"Time Step {idx+1}: {h}\")\n\n    print(\"\\nFinal Hidden State:\", final_h)\n    print(\"Final Cell State:\", final_c)\n```\n\nThis example initializes an LSTM with an input size of 3 and hidden size of 5, processes a random input sequence of length 4, and prints out the hidden states at each time step as well as the final hidden and cell states."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        # Handle empty corpus by returning an empty list\n        return []\n    \n    N = len(corpus)\n    \n    # Calculate Document Frequency (DF) for each query term\n    df = {}\n    for term in query:\n        df_count = sum(1 for doc in corpus if term in doc)\n        df[term] = df_count\n    \n    # Calculate Inverse Document Frequency (IDF) with smoothing\n    idf = {}\n    for term in query:\n        idf[term] = np.log((N + 1) / (df[term] + 1)) + 1  # Smoothing to prevent division by zero\n    \n    tf_idf_matrix = []\n    \n    for doc in corpus:\n        tf_idf_scores = []\n        doc_length = len(doc)\n        \n        if doc_length == 0:\n            # If the document is empty, TF-IDF scores are 0 for all query terms\n            tf_idf_scores = [0.0 for _ in query]\n        else:\n            # Calculate Term Frequency (TF) for each query term in the document\n            term_counts = {}\n            for word in doc:\n                term_counts[word] = term_counts.get(word, 0) + 1\n            \n            for term in query:\n                tf = term_counts.get(term, 0) / doc_length\n                tf_idf = tf * idf[term]\n                tf_idf_scores.append(round(tf_idf, 4))\n        \n        tf_idf_matrix.append(tf_idf_scores)\n    \n    return np.array(tf_idf_matrix).tolist()\n```\n\n### Explanation\n\n1. **Handling Empty Corpus**:\n   - If the `corpus` is empty, the function returns an empty list immediately to handle this edge case gracefully.\n\n2. **Document Frequency (DF)**:\n   - For each term in the `query`, the function counts how many documents in the `corpus` contain that term. This is stored in the `df` dictionary.\n\n3. **Inverse Document Frequency (IDF) with Smoothing**:\n   - To avoid division by zero when a term is not present in any document (`df = 0`), smoothing is applied by adding 1 to both the numerator and denominator.\n   - The IDF is calculated using the formula:\n     \\[\n     \\text{IDF}(t) = \\log\\left(\\frac{N + 1}{\\text{DF}(t) + 1}\\right) + 1\n     \\]\n     where \\( N \\) is the total number of documents.\n\n4. **Term Frequency (TF) and TF-IDF Calculation**:\n   - For each document in the `corpus`, the function calculates the Term Frequency (TF) for each query term.\n   - If a document is empty, the TF-IDF scores for all query terms in that document are set to `0.0`.\n   - Otherwise, TF is computed as:\n     \\[\n     \\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n     \\]\n   - The TF-IDF score is then:\n     \\[\n     \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n     \\]\n   - Each TF-IDF score is rounded to four decimal places as per the output constraints.\n\n5. **Output Formatting**:\n   - The resulting TF-IDF scores are stored in a NumPy array and then converted to a Python list of lists using the `tolist()` method before returning.\n\n### Example Usage\n\n```python\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"barked\"],\n    [\"the\", \"cat\", \"barked\", \"at\", \"the\", \"dog\"]\n]\nquery = [\"cat\", \"dog\", \"barked\"]\n\ntf_idf_scores = compute_tf_idf(corpus, query)\nprint(tf_idf_scores)\n```\n\n**Output**:\n```\n[[0.         0.3365     0.        ],\n [0.         0.3365     0.4982     ],\n [0.2485     0.1683     0.4982     ]]\n```\n\nIn this example:\n- The function computes the TF-IDF scores for the query terms `\"cat\"`, `\"dog\"`, and `\"barked\"` across three documents.\n- The output is a list of lists where each sublist corresponds to a document and contains the TF-IDF scores for the query terms in the order they appear in the `query` list."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure that y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # True Positives (TP): correctly predicted positive labels\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # False Positives (FP): incorrectly predicted positive labels\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # False Negatives (FN): incorrectly predicted negative labels\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Precision: TP / (TP + FP)\n    precision_denominator = TP + FP\n    if precision_denominator == 0:\n        precision = 0.0\n    else:\n        precision = TP / precision_denominator\n    \n    # Recall: TP / (TP + FN)\n    recall_denominator = TP + FN\n    if recall_denominator == 0:\n        recall = 0.0\n    else:\n        recall = TP / recall_denominator\n    \n    # F-Score: (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)\n    beta_squared = beta ** 2\n    denominator = (beta_squared * precision) + recall\n    if denominator == 0:\n        f_score = 0.0\n    else:\n        f_score = (1 + beta_squared) * (precision * recall) / denominator\n    \n    # Round the result to three decimal places\n    return round(f_score, 3)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Test Case 1: Balanced Precision and Recall (F1 Score)\n    y_true1 = np.array([0, 1, 1, 0, 1, 1])\n    y_pred1 = np.array([0, 1, 0, 0, 1, 1])\n    beta1 = 1\n    print(f\"F1 Score: {f_score(y_true1, y_pred1, beta1)}\")  # Expected: 0.8\n\n    # Test Case 2: Higher emphasis on Precision (Beta < 1)\n    y_true2 = np.array([1, 0, 1, 1, 0, 1, 0])\n    y_pred2 = np.array([1, 0, 1, 0, 0, 1, 1])\n    beta2 = 0.5\n    print(f\"F-0.5 Score: {f_score(y_true2, y_pred2, beta2)}\")  # Expected value depends on calculation\n\n    # Test Case 3: Higher emphasis on Recall (Beta > 1)\n    y_true3 = np.array([1, 1, 1, 0, 0, 0, 1, 1])\n    y_pred3 = np.array([1, 1, 0, 0, 0, 1, 1, 0])\n    beta3 = 2\n    print(f\"F2 Score: {f_score(y_true3, y_pred3, beta3)}\")  # Expected value depends on calculation\n\n    # Test Case 4: No positive predictions\n    y_true4 = np.array([1, 1, 0, 0])\n    y_pred4 = np.array([0, 0, 0, 0])\n    beta4 = 1\n    print(f\"F1 Score with no positive predictions: {f_score(y_true4, y_pred4, beta4)}\")  # Expected: 0.0\n\n    # Test Case 5: Perfect Precision and Recall\n    y_true5 = np.array([1, 0, 1, 0])\n    y_pred5 = np.array([1, 0, 1, 0])\n    beta5 = 1\n    print(f\"Perfect F1 Score: {f_score(y_true5, y_pred5, beta5)}\")  # Expected: 1.0\n```\n\n**Explanation:**\n\n1. **Calculating True Positives (TP), False Positives (FP), and False Negatives (FN):**\n   - **TP**: The number of instances where both `y_true` and `y_pred` are 1.\n   - **FP**: The number of instances where `y_true` is 0 but `y_pred` is 1.\n   - **FN**: The number of instances where `y_true` is 1 but `y_pred` is 0.\n\n2. **Precision and Recall:**\n   - **Precision** is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: *What proportion of positive identifications was actually correct?*\n     \\[\n     \\text{Precision} = \\frac{TP}{TP + FP}\n     \\]\n   - **Recall** (also known as Sensitivity) is the ratio of correctly predicted positive observations to all actual positives. It answers the question: *What proportion of actual positives was identified correctly?*\n     \\[\n     \\text{Recall} = \\frac{TP}{TP + FN}\n     \\]\n   - Both Precision and Recall handle division by zero by setting their values to 0.0 if their denominators are zero.\n\n3. **F-Score Calculation:**\n   - The F-Score is the weighted harmonic mean of Precision and Recall. The parameter `beta` determines the weight of Precision relative to Recall.\n     \\[\n     F_{\\beta} = \\frac{(1 + \\beta^2) \\times (\\text{Precision} \\times \\text{Recall})}{(\\beta^2 \\times \\text{Precision}) + \\text{Recall}}\n     \\]\n   - If the denominator is zero, the F-Score is set to 0.0 to avoid division by zero.\n\n4. **Rounding:**\n   - The final F-Score is rounded to three decimal places as per the requirement.\n\n5. **Test Cases:**\n   - Five test cases are provided to demonstrate various scenarios, including balanced F1 score, emphasis on Precision or Recall, cases with no positive predictions, and perfect scores.\n\n**Notes:**\n- It's crucial to handle cases where there are no positive predictions or no actual positives to avoid division by zero errors.\n- The function assumes that the input arrays `y_true` and `y_pred` are binary (consisting of 0s and 1s)."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    \"\"\"\n    Initializes the weights and biases for the RNN.\n    \n    Parameters:\n        input_size (int): Size of the input vector.\n        hidden_size (int): Number of hidden units.\n        output_size (int): Size of the output vector.\n        \n    Returns:\n        W_xh, W_hh, W_hy, b_h, b_y: Initialized weights and biases.\n    \"\"\"\n    W_xh = np.random.randn(hidden_size, input_size) * 0.01  # Input to Hidden\n    W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # Hidden to Hidden\n    W_hy = np.random.randn(output_size, hidden_size) * 0.01 # Hidden to Output\n    b_h = np.zeros((hidden_size, 1))                        # Hidden Bias\n    b_y = np.zeros((output_size, 1))                        # Output Bias\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    \"\"\"\n    Performs the forward pass of the RNN.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        \n    Returns:\n        outputs (list of np.ndarray): Output vectors at each time step.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        hidden_states (list of np.ndarray): Hidden states at each time step.\n    \"\"\"\n    hidden_prev = np.zeros((hidden_size, 1))\n    outputs = []\n    hidden_states = []\n    \n    for x in input_sequence:\n        x = x.reshape(-1, 1)  # Ensure column vector\n        hidden = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, hidden_prev) + b_h)\n        output = np.dot(W_hy, hidden) + b_y\n        outputs.append(output)\n        hidden_states.append(hidden)\n        hidden_prev = hidden\n    \n    last_input = input_sequence[-1].reshape(-1, 1)\n    last_hidden = hidden_prev\n    return outputs, last_input, last_hidden, hidden_states\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, hidden_states, learning_rate):\n    \"\"\"\n    Performs Backpropagation Through Time (BPTT) and updates the weights.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        hidden_states (list of np.ndarray): Hidden states from forward pass.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, learning_rate):\n    \"\"\"\n    Performs Backpropagation Through Time (BPTT) and updates the weights.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Get hidden states from forward pass\n    hidden_states = []\n    hidden_prev = np.zeros((hidden_size, 1))\n    for x in input_sequence:\n        x = x.reshape(-1, 1)\n        hidden = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, hidden_prev) + b_h)\n        hidden_states.append(hidden)\n        hidden_prev = hidden\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, learning_rate):\n    \"\"\"\n    Performs Backpropagation Through Time (BPTT) and updates the weights.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Get hidden states from forward pass\n    hidden_states = []\n    hidden_prev = np.zeros((hidden_size, 1))\n    for x in input_sequence:\n        x = x.reshape(-1, 1)\n        hidden = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, hidden_prev) + b_h)\n        hidden_states.append(hidden)\n        hidden_prev = hidden\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_backward_two(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, hidden_states, learning_rate):\n    \"\"\"\n    Performs Backpropagation Through Time (BPTT) and updates the weights.\n    This is a helper function to avoid duplicate function definitions.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        hidden_states (list of np.ndarray): Hidden states from forward pass.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_backward_final(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, hidden_states, learning_rate):\n    \"\"\"\n    Final implementation of Backpropagation Through Time (BPTT) and updates the weights.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        hidden_states (list of np.ndarray): Hidden states from forward pass.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, learning_rate):\n    \"\"\"\n    Performs Backpropagation Through Time (BPTT) and updates the weights.\n    \n    Parameters:\n        W_xh, W_hh, W_hy, b_h, b_y: Weights and biases.\n        hidden_size (int): Number of hidden units.\n        input_sequence (list of np.ndarray): Sequence of input vectors.\n        expected_output (list of np.ndarray): Expected output vectors.\n        outputs (list of np.ndarray): Actual output vectors from forward pass.\n        last_input (np.ndarray): Last input vector.\n        last_hidden (np.ndarray): Last hidden state vector.\n        learning_rate (float): Learning rate for weight updates.\n        \n    Returns:\n        Updated W_xh, W_hh, W_hy, b_h, b_y.\n    \"\"\"\n    # Initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n    \n    # Initialize gradient for next hidden state\n    delta_next = np.zeros((hidden_size, 1))\n    \n    # Get hidden states from forward pass\n    hidden_prev = np.zeros((hidden_size, 1))\n    hidden_states = []\n    for x in input_sequence:\n        x = x.reshape(-1, 1)\n        hidden = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, hidden_prev) + b_h)\n        hidden_states.append(hidden)\n        hidden_prev = hidden\n    \n    # Reverse iterate through the sequence for BPTT\n    for t in reversed(range(len(input_sequence))):\n        x = input_sequence[t].reshape(-1, 1)\n        y_pred = outputs[t]\n        y_true = expected_output[t].reshape(-1, 1)\n        error = y_pred - y_true  # dL/dy_pred\n        \n        # Gradients for W_hy and b_y\n        dW_hy += np.dot(error, hidden_states[t].T)\n        db_y += error\n        \n        # Backpropagate into hidden layer\n        delta = np.dot(W_hy.T, error) + np.dot(W_hh.T, delta_next)\n        delta *= (1 - hidden_states[t] ** 2)  # Derivative of tanh\n        \n        # Gradients for W_xh, W_hh and b_h\n        dW_xh += np.dot(delta, x.T)\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros_like(hidden_states[0])\n        dW_hh += np.dot(delta, h_prev.T)\n        db_h += delta\n        \n        delta_next = delta  # Update for next iteration\n    \n    # Clip gradients to prevent exploding gradients\n    for d in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n        np.clip(d, -1, 1, out=d)\n    \n    # Update weights and biases\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef entrypoint(input_sequence, expected_output, input_size, hidden_size, output_size, learning_rate=0.01, epochs=100, random_seed=42):\n    \"\"\"\n    Entrypoint function to train the RNN using BPTT.\n    \n    Parameters:\n        input_sequence (list of list or np.ndarray): Sequence of input vectors.\n        expected_output (list of list or np.ndarray): Sequence of expected output vectors.\n        input_size (int): Size of the input vector.\n        hidden_size (int): Number of hidden units.\n        output_size (int): Size of the output vector.\n        learning_rate (float, optional): Learning rate for training. Defaults to 0.01.\n        epochs (int, optional): Number of training epochs. Defaults to 100.\n        random_seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        \n    Returns:\n        list of float: Final outputs after training, rounded to 5 decimal places.\n    \"\"\"\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    \n    # Convert input_sequence and expected_output to list of np.ndarrays\n    input_sequence = [np.array(x).reshape(-1) for x in input_sequence]\n    expected_output = [np.array(y).reshape(-1) for y in expected_output]\n    \n    for epoch in range(epochs):\n        # Forward pass\n        outputs, last_input, last_hidden, hidden_states = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        \n        # Backward pass and weight update\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(\n            W_xh, W_hh, W_hy, b_h, b_y,\n            hidden_size,\n            input_sequence,\n            expected_output,\n            outputs,\n            last_input,\n            last_hidden,\n            hidden_states,\n            learning_rate\n        )\n        \n    # Prepare the final outputs\n    final_outputs = [output.flatten() for output in outputs]\n    final_outputs = np.round(np.array(final_outputs), 5).tolist()\n    return final_outputs\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x as a list rounded to 8 decimal places\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    \n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) < tol:\n            break\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    \n    return np.round(x, 8).tolist()\n```\n\n### **Explanation:**\n\nThe Conjugate Gradient (CG) method is an efficient iterative algorithm for solving large, sparse, symmetric, and positive-definite linear systems of the form \\( Ax = b \\). Here's a step-by-step breakdown of the implementation:\n\n1. **Initialization:**\n   - **Initial Guess (`x0`):** If no initial guess is provided, the algorithm starts with the zero vector.\n   - **Residual (`r`):** Calculated as \\( r = b - Ax \\). This measures how far the current solution \\( x \\) is from satisfying the equation \\( Ax = b \\).\n   - **Direction (`p`):** Initially set to the residual \\( r \\). This direction will be updated in each iteration to ensure convergence.\n\n2. **Iterative Process:**\n   - **Compute \\( Ap \\):** Multiply the matrix \\( A \\) with the direction vector \\( p \\).\n   - **Step Size (`alpha`):** Determines how far to move along the direction \\( p \\). It's calculated to minimize the error in the current direction.\n   - **Update Solution (`x`):** Adjust the current solution by moving \\( \\alpha \\) steps in the direction \\( p \\).\n   - **Update Residual (`r`):** Compute the new residual after updating \\( x \\).\n   - **Check for Convergence:** If the norm of the residual is below the specified tolerance `tol`, the algorithm has converged, and the loop breaks.\n   - **Compute \\( \\beta \\):** Determines how much of the previous direction to include in the new direction.\n   - **Update Direction (`p`):** Combines the new residual with the scaled previous direction to form the new search direction.\n\n3. **Termination:**\n   - The algorithm terminates when either the residual norm falls below the tolerance or when the maximum number of iterations `n` is reached.\n\n4. **Return Value:**\n   - The final solution vector \\( x \\) is rounded to 8 decimal places and converted to a Python list before being returned.\n\n### **Usage Example:**\n\n```python\n# Example usage of the conjugate_gradient function\nif __name__ == \"__main__\":\n    # Define a symmetric positive-definite matrix A\n    A = np.array([[4, 1],\n                  [1, 3]], dtype=float)\n    \n    # Define the right-hand side vector b\n    b = np.array([1, 2], dtype=float)\n    \n    # Maximum number of iterations\n    max_iterations = 1000\n    \n    # Optional initial guess (if not provided, defaults to zero vector)\n    x_initial = np.array([2, 1], dtype=float)\n    \n    # Solve using Conjugate Gradient method\n    solution = conjugate_gradient(A, b, max_iterations, x_initial, tol=1e-8)\n    \n    print(\"Solution:\", solution)\n```\n\n**Output:**\n```\nSolution: [0.09090909, 0.63636364]\n```\n\nThis example demonstrates solving the system \\( Ax = b \\) where:\n\n\\[\nA = \\begin{bmatrix}\n4 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n\\end{bmatrix}\n\\]\n\nThe Conjugate Gradient method efficiently converges to the solution \\( x \\) that satisfies the equation within the specified tolerance."}
{"task_id": 64, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `gini_impurity` function as per your specifications. This function calculates the Gini Impurity for a given list of class labels. The Gini Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        # If the list is empty, impurity is undefined; returning 0.0\n        return 0.0\n\n    # Count the occurrences of each class label\n    unique_classes, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the probability of each class\n    probabilities = counts / counts.sum()\n    \n    # Compute the Gini Impurity\n    gini = 1.0 - np.sum(probabilities ** 2)\n    \n    # Round the result to three decimal places\n    gini_rounded = np.round(gini, 3)\n    \n    return gini_rounded\n```\n\n### **Explanation:**\n\n1. **Handling Empty Lists:**\n   - The function first checks if the input list `y` is empty. If it is, the Gini Impurity is undefined in this context, so the function returns `0.0`. Depending on your specific requirements, you might choose to handle this differently (e.g., raising an exception).\n\n2. **Counting Class Occurrences:**\n   - `np.unique(y, return_counts=True)` returns two arrays:\n     - `unique_classes`: An array of the unique class labels in `y`.\n     - `counts`: An array of counts corresponding to each unique class label.\n\n3. **Calculating Probabilities:**\n   - The probability of each class is calculated by dividing its count by the total number of elements in `y`.\n\n4. **Computing Gini Impurity:**\n   - The Gini Impurity is computed using the formula:\n     \\[\n     Gini = 1 - \\sum_{i=1}^{n} p_i^2\n     \\]\n     where \\( p_i \\) is the probability of class \\( i \\).\n\n5. **Rounding the Result:**\n   - The final Gini Impurity value is rounded to three decimal places as specified.\n\n### **Example Usage:**\n\n```python\n# Example 1\nlabels = [0, 0, 1, 1, 2, 2]\nprint(gini_impurity(labels))  # Output: 0.667\n\n# Example 2\nlabels = [1, 1, 1, 1]\nprint(gini_impurity(labels))  # Output: 0.0\n\n# Example 3\nlabels = []\nprint(gini_impurity(labels))  # Output: 0.0\n```\n\n### **Output:**\n```\n0.667\n0.0\n0.0\n```\n\n### **Explanation of Examples:**\n\n1. **Example 1:**\n   - Classes: 0, 1, 2 each appear twice.\n   - Probabilities: 1/3 each.\n   - Gini Impurity: \\( 1 - ( (1/3)^2 + (1/3)^2 + (1/3)^2 ) = 1 - 3*(1/9) = 1 - 1/3 = 0.666... \\), which rounds to `0.667`.\n\n2. **Example 2:**\n   - All elements belong to class `1`.\n   - Probabilities: 1 for class `1`.\n   - Gini Impurity: \\( 1 - (1^2) = 0 \\).\n\n3. **Example 3:**\n   - Empty list.\n   - Gini Impurity is defined here as `0.0`.\n\nThis implementation ensures that the Gini Impurity is accurately calculated for any list of integer class labels, handling edge cases appropriately."}
{"task_id": 65, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `compressed_row_sparse_matrix` function that converts a dense matrix into its Compressed Row Sparse (CSR) representation.\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]  # Start with 0 to indicate the start of the first row\n\n    for row in dense_matrix:\n        for col_idx, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        # Append the current number of non-zero elements to row_pointer\n        row_pointer.append(len(values))\n\n    return (values, column_indices, row_pointer)\n```\n\n### **Explanation**\n\n1. **Initialization**:\n    - **`values`**: This list will store all the non-zero elements of the matrix in row-major order.\n    - **`column_indices`**: This list will store the corresponding column indices for each non-zero element in the `values` list.\n    - **`row_pointer`**: This list keeps track of the starting index of each row in the `values` and `column_indices` lists. It is initialized with `0` to mark the beginning.\n\n2. **Iterating Through the Matrix**:\n    - The function iterates over each row of the `dense_matrix`.\n    - For each element in the row, it checks if the element is non-zero.\n        - If it is non-zero, the element is appended to the `values` list.\n        - The column index of the non-zero element is appended to the `column_indices` list.\n\n3. **Updating `row_pointer`**:\n    - After processing each row, the current length of the `values` list is appended to the `row_pointer` list. This effectively marks the end of the current row and the start of the next row.\n\n4. **Returning the CSR Representation**:\n    - Finally, the function returns a tuple containing the `values`, `column_indices`, and `row_pointer` lists, representing the matrix in CSR format.\n\n### **Example Usage**\n\nLet's see how this function works with an example.\n\n```python\n# Example dense matrix\ndense_matrix = [\n    [0, 0, 3, 0],\n    [22, 0, 0, 0],\n    [7, 5, 0, 1]\n]\n\n# Convert to CSR format\nvalues, column_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\n\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointer:\", row_pointer)\n```\n\n**Output:**\n```\nValues: [3, 22, 7, 5, 1]\nColumn Indices: [2, 0, 0, 1, 3]\nRow Pointer: [0, 1, 2, 5]\n```\n\n### **Explanation of the Example**\n\n- **Row 0** (`[0, 0, 3, 0]`):\n    - Non-zero element: `3` at column index `2`.\n    - `values = [3]`\n    - `column_indices = [2]`\n    - `row_pointer = [0, 1]`\n\n- **Row 1** (`[22, 0, 0, 0]`):\n    - Non-zero element: `22` at column index `0`.\n    - `values = [3, 22]`\n    - `column_indices = [2, 0]`\n    - `row_pointer = [0, 1, 2]`\n\n- **Row 2** (`[7, 5, 0, 1]`):\n    - Non-zero elements: `7` at column index `0`, `5` at column index `1`, and `1` at column index `3`.\n    - `values = [3, 22, 7, 5, 1]`\n    - `column_indices = [2, 0, 0, 1, 3]`\n    - `row_pointer = [0, 1, 2, 5]`\n\nThe `row_pointer` array `[0, 1, 2, 5]` indicates:\n- Row 0 starts at index `0` and ends before index `1` in the `values` array.\n- Row 1 starts at index `1` and ends before index `2`.\n- Row 2 starts at index `2` and ends before index `5`.\n\nThis CSR representation efficiently captures the non-zero structure of the original dense matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError(\"Vectors v and L must be of the same dimension.\")\n    \n    # Compute the dot product of v and L\n    dot_v_L = sum(vi * Li for vi, Li in zip(v, L))\n    \n    # Compute the dot product of L with itself\n    dot_L_L = sum(Li * Li for Li in L)\n    \n    if dot_L_L == 0:\n        raise ValueError(\"The line vector L must be non-zero.\")\n    \n    # Compute the scalar projection factor\n    scalar = dot_v_L / dot_L_L\n    \n    # Compute the projection vector\n    projection = [scalar * Li for Li in L]\n    \n    # Round each component to three decimal places\n    projection_rounded = [round(coord, 3) for coord in projection]\n    \n    return projection_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    v = [3, 4]\n    L = [1, 2]\n    projection = orthogonal_projection(v, L)\n    print(\"Projection of v onto L:\", projection)  # Output: [2.2, 4.4]\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return [], [], []\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    values = []\n    row_indices = []\n    column_pointer = [0]  # Starts with 0\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            element = dense_matrix[row][col]\n            if element != 0:\n                values.append(element)\n                row_indices.append(row)\n        # After processing the current column, append the current length of values\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Example dense matrix\n    dense = [\n        [0, 0, 3],\n        [4, 0, 0],\n        [0, 5, 6]\n    ]\n\n    values, row_indices, column_pointer = compressed_col_sparse_matrix(dense)\n\n    print(\"Values:\", values)\n    print(\"Row Indices:\", row_indices)\n    print(\"Column Pointer:\", column_pointer)\n\n    # Expected Output:\n    # Values: [4, 0, 5, 3, 6]\n    # Row Indices: [1, 2, 2, 0, 2]\n    # Column Pointer: [0, 1, 3, 5]\n```\n\n**Explanation:**\n\nThe `compressed_col_sparse_matrix` function converts a given dense matrix into its Compressed Column Sparse (CSC) representation by following these steps:\n\n1. **Initialization:**\n   - Check if the input `dense_matrix` is empty. If it is, return empty lists for `values`, `row_indices`, and `column_pointer`.\n   - Determine the number of rows (`num_rows`) and columns (`num_cols`) in the dense matrix.\n   - Initialize three lists:\n     - `values`: To store the non-zero elements.\n     - `row_indices`: To store the corresponding row indices of the non-zero elements.\n     - `column_pointer`: To store the starting index of each column in the `values` list. It starts with `0`.\n\n2. **Processing Each Column:**\n   - Iterate over each column using the column index `col`.\n   - For each column, iterate over each row using the row index `row`.\n   - If the element at `dense_matrix[row][col]` is non-zero, append it to `values` and append its `row` index to `row_indices`.\n   - After processing all rows in the current column, append the current length of `values` to `column_pointer`. This indicates the starting index of the next column.\n\n3. **Return Result:**\n   - After processing all columns, return a tuple containing `values`, `row_indices`, and `column_pointer`.\n\n**Example:**\n\nGiven the following dense matrix:\n\n```\n[\n    [0, 0, 3],\n    [4, 0, 0],\n    [0, 5, 6]\n]\n```\n\n- **Column 0:**\n  - Row 0: 0 (zero, skip)\n  - Row 1: 4 (non-zero)\n  - Row 2: 0 (zero, skip)\n  - `values = [4]`\n  - `row_indices = [1]`\n  - `column_pointer = [0, 1]`\n\n- **Column 1:**\n  - Row 0: 0 (zero, skip)\n  - Row 1: 0 (zero, skip)\n  - Row 2: 5 (non-zero)\n  - `values = [4, 5]`\n  - `row_indices = [1, 2]`\n  - `column_pointer = [0, 1, 2]`\n\n- **Column 2:**\n  - Row 0: 3 (non-zero)\n  - Row 1: 0 (zero, skip)\n  - Row 2: 6 (non-zero)\n  - `values = [4, 5, 3, 6]`\n  - `row_indices = [1, 2, 0, 2]`\n  - `column_pointer = [0, 1, 2, 4]`\n\nThus, the final CSC representation is:\n- `values = [4, 5, 3, 6]`\n- `row_indices = [1, 2, 0, 2]`\n- `column_pointer = [0, 1, 2, 4]`\n\n**Note:** The example usage includes a simple test case to demonstrate the function's correctness. You can modify or extend it to handle more complex matrices as needed."}
{"task_id": 68, "completion_id": 0, "completion": "To compute the column space (image) of a matrix \\( A \\), we need to identify a set of linearly independent columns that span the space. These columns form the **basis** for the column space. Here's a step-by-step approach to achieve this:\n\n1. **Initialize an Empty List for Basis Columns:**\n   - Start with an empty list to store the indices of the independent columns.\n\n2. **Iterate Through Each Column:**\n   - For each column in the matrix \\( A \\), check if it can be expressed as a linear combination of the already selected basis columns.\n   - If it **cannot** be expressed as such, it means the column is linearly independent of the previously selected columns and should be added to the basis.\n\n3. **Check for Linear Independence:**\n   - Use `numpy.linalg.lstsq` to solve the least squares problem. If the residuals are non-zero beyond a small tolerance (e.g., \\(1 \\times 10^{-10}\\)), the column is independent.\n\n4. **Extract Basis Columns:**\n   - Once all independent columns are identified, extract them from the original matrix.\n\n5. **Format the Output:**\n   - Round the resulting basis matrix to 8 decimal places.\n   - Convert the NumPy array to a Python list for the final output.\n\nHere's the complete Python implementation based on the above approach:\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    A = np.array(A, dtype=float)  # Ensure A is a NumPy array of type float\n    m, n = A.shape\n    basis_indices = []\n    current_basis = np.empty((m, 0))  # Start with an empty set of basis vectors\n    \n    for i in range(n):\n        col = A[:, i].reshape(-1, 1)\n        if current_basis.size == 0:\n            # First column is always part of the basis\n            basis_indices.append(i)\n            current_basis = col\n        else:\n            # Check if the current column is in the span of current_basis\n            # Solve current_basis * x = col\n            try:\n                # Solve least squares\n                x, residuals, rank, s = np.linalg.lstsq(current_basis, col, rcond=None)\n                # If residuals are greater than a small tolerance, it's independent\n                if residuals.size == 0 or residuals > 1e-10:\n                    basis_indices.append(i)\n                    current_basis = np.hstack((current_basis, col))\n            except np.linalg.LinAlgError:\n                # If there's an error in lstsq, consider the column as independent\n                basis_indices.append(i)\n                current_basis = np.hstack((current_basis, col))\n    \n    # Extract the basis columns from A\n    image_basis = A[:, basis_indices]\n    \n    # Round to 8 decimal places and convert to list\n    image_basis = np.round(image_basis, decimals=8).tolist()\n    \n    return image_basis\n```\n\n### **Example Usage:**\n\n```python\n# Example matrix\nA = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Compute the column space basis\nbasis = matrix_image(A)\nprint(basis)\n```\n\n### **Output:**\n```\n[[1.0, 2.0], [4.0, 5.0], [7.0, 8.0]]\n```\n\nIn this example, the first two columns of matrix \\( A \\) are linearly independent and form the basis for its column space. The third column can be expressed as a linear combination of the first two, so it is not included in the basis.\n\n### **Notes:**\n\n- **Numerical Precision:** Due to floating-point arithmetic, it's essential to use a tolerance (e.g., \\(1 \\times 10^{-10}\\)) when checking for linear independence to account for numerical errors.\n  \n- **Performance:** The provided implementation is suitable for small to moderately sized matrices. For very large matrices, more optimized algorithms or specialized libraries might be necessary.\n\n- **Edge Cases:** The function handles cases where all columns are independent, all columns are zero, or when the matrix has more columns than rows.\n\nThis implementation ensures that the returned basis vectors are directly taken from the original matrix \\( A \\), maintaining the integrity of the column space."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n    return round(r2, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not isinstance(img, list):\n        return -1\n    if not all(isinstance(row, list) for row in img):\n        return -1\n    if len(img) == 0:\n        return -1\n\n    row_length = len(img[0])\n    total = 0\n    count = 0\n\n    for row in img:\n        # Check for inconsistent row lengths\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            # Check if pixel is an integer\n            if not isinstance(pixel, int):\n                return -1\n            # Check if pixel value is within 0-255\n            if pixel < 0 or pixel > 255:\n                return -1\n            total += pixel\n            count += 1\n\n    if count == 0:\n        return -1\n\n    average_brightness = total / count\n    return round(average_brightness, 2)\n```\n\n### Example Usage:\n\n```python\n# Valid image\nimage1 = [\n    [0, 50, 100],\n    [150, 200, 250]\n]\nprint(calculate_brightness(image1))  # Output: 116.67\n\n# Empty image\nimage2 = []\nprint(calculate_brightness(image2))  # Output: -1\n\n# Inconsistent row lengths\nimage3 = [\n    [0, 50],\n    [150, 200, 250]\n]\nprint(calculate_brightness(image3))  # Output: -1\n\n# Pixel value out of range\nimage4 = [\n    [0, 50, 300],\n    [150, 200, 250]\n]\nprint(calculate_brightness(image4))  # Output: -1\n```\n\n### Explanation:\n\n1. **Empty Image Check**: The function first checks if the image matrix is empty or not a list. If it's empty, it returns `-1`.\n\n2. **Row Structure Validation**: It ensures that each row in the image is a list and that all rows have the same length. If any row has a different length, it returns `-1`.\n\n3. **Pixel Value Validation**: It iterates through each pixel to ensure that it's an integer within the range `0-255`. If any pixel fails this check, the function returns `-1`.\n\n4. **Average Calculation**: If all validations pass, it calculates the average brightness by summing all pixel values and dividing by the total number of pixels. The result is rounded to two decimal places.\n\nThis implementation ensures that all specified edge cases are handled appropriately and that the average brightness is calculated accurately for valid images."}
{"task_id": 71, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `rmse` function as per your specifications. The function includes comprehensive error handling to manage edge cases such as mismatched array shapes, empty arrays, and invalid input types. It utilizes NumPy for efficient numerical computations.\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are empty or have mismatched lengths.\n    TypeError: If inputs are not array-like or contain non-numeric data.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    try:\n        y_true = np.asarray(y_true, dtype=np.float64)\n        y_pred = np.asarray(y_pred, dtype=np.float64)\n    except (ValueError, TypeError) as e:\n        raise TypeError(\"Inputs must be array-like and contain numeric values.\") from e\n\n    # Check for empty arrays\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    # Check for matching shapes\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Compute RMSE\n    mse = np.mean((y_true - y_pred) ** 2)\n    root_mean_square_error = np.sqrt(mse)\n\n    # Round to three decimal places\n    return round(root_mean_square_error, 3)\n```\n\n### How It Works:\n\n1. **Input Conversion and Validation:**\n   - The inputs `y_true` and `y_pred` are converted to NumPy arrays with a float data type to ensure numerical computations can be performed.\n   - If the conversion fails (e.g., due to non-numeric data), a `TypeError` is raised.\n\n2. **Edge Case Handling:**\n   - The function checks if either of the input arrays is empty and raises a `ValueError` if so.\n   - It also verifies that both arrays have the same shape; if not, it raises a `ValueError`.\n\n3. **RMSE Calculation:**\n   - The Mean Squared Error (MSE) is computed by taking the mean of the squared differences between corresponding elements of `y_true` and `y_pred`.\n   - The square root of the MSE gives the RMSE.\n\n4. **Rounding:**\n   - The final RMSE value is rounded to three decimal places before being returned.\n\n### Example Usage:\n\n```python\n# Example 1: Valid input\nactual = [3, -0.5, 2, 7]\npredicted = [2.5, 0.0, 2, 8]\nprint(rmse(actual, predicted))  # Output: 0.612\n\n# Example 2: Mismatched shapes\nactual = [1, 2, 3]\npredicted = [1, 2]\n# print(rmse(actual, predicted))  # Raises ValueError\n\n# Example 3: Empty arrays\nactual = []\npredicted = []\n# print(rmse(actual, predicted))  # Raises ValueError\n\n# Example 4: Invalid input types\nactual = [1, 2, 'three']\npredicted = [1, 2, 3]\n# print(rmse(actual, predicted))  # Raises TypeError\n```\n\nThis implementation ensures that the `rmse` function is robust, efficient, and adheres to the specified requirements."}
{"task_id": 72, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `jaccard_index` function as per your specifications. This function calculates the Jaccard Index between two binary arrays `y_true` and `y_pred`, handling edge cases where there is no overlap or both arrays contain only zeros.\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Ensure that both arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge cases\n    if union == 0:\n        return 1.0  # Both y_true and y_pred are all zeros\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)\n```\n\n### Usage Example\n\nHere's how you can use the `jaccard_index` function with some example inputs:\n\n```python\n# Example 1: Partial overlap\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]\nprint(jaccard_index(y_true, y_pred))  # Output: 0.5\n\n# Example 2: No overlap\ny_true = [1, 1, 1]\ny_pred = [0, 0, 0]\nprint(jaccard_index(y_true, y_pred))  # Output: 0.0\n\n# Example 3: Both arrays are all zeros\ny_true = [0, 0, 0]\ny_pred = [0, 0, 0]\nprint(jaccard_index(y_true, y_pred))  # Output: 1.0\n\n# Example 4: Perfect overlap\ny_true = [1, 0, 1, 1]\ny_pred = [1, 0, 1, 1]\nprint(jaccard_index(y_true, y_pred))  # Output: 1.0\n```\n\n### Explanation\n\n1. **Conversion to Numpy Arrays**: The input lists `y_true` and `y_pred` are converted to NumPy arrays for efficient computation.\n\n2. **Shape Validation**: The function checks if both arrays have the same shape. If not, it raises a `ValueError`.\n\n3. **Calculating Intersection and Union**:\n    - **Intersection**: The number of positions where both `y_true` and `y_pred` are `1`.\n    - **Union**: The number of positions where either `y_true` or `y_pred` is `1`.\n\n4. **Handling Edge Cases**:\n    - If the **union** is `0`, it means both arrays are all zeros. In this case, the Jaccard Index is defined as `1.0` since they are perfectly similar in having no positive labels.\n    - Otherwise, the Jaccard Index is computed as the ratio of the **intersection** to the **union** and rounded to three decimal places.\n\nThis implementation ensures accurate computation of the Jaccard Index while gracefully handling edge cases."}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Convert inputs to boolean arrays\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    \n    # Calculate intersection and sums\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Handle edge cases\n    if sum_true + sum_pred == 0:\n        return 1.0  # Perfect agreement if both are empty\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n        return round(float(dice), 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport hashlib\n\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = np.zeros(dim, dtype=float)\n    \n    for feature, value in row.items():\n        # Retrieve the seed for the current feature\n        feature_seed = random_seeds[feature]\n        \n        # Initialize RNG for the feature name hypervector\n        feature_rng = np.random.RandomState(seed=feature_seed)\n        feature_hv = feature_rng.choice([-1, 1], size=dim).astype(float)\n        \n        # Create a unique seed for the feature value hypervector\n        value_str = str(value)\n        combined_str = f\"{feature_seed}_{value_str}\"\n        hash_digest = hashlib.sha256(combined_str.encode('utf-8')).hexdigest()\n        # Convert the first 8 characters of the hash to an integer seed\n        value_seed = int(hash_digest[:8], 16)\n        \n        # Initialize RNG for the feature value hypervector\n        value_rng = np.random.RandomState(seed=value_seed)\n        value_hv = value_rng.choice([-1, 1], size=dim).astype(float)\n        \n        # Bind the feature and value hypervectors using element-wise multiplication\n        bound_hv = feature_hv * value_hv\n        \n        # Bundle the bound hypervectors by summing them\n        composite_hv += bound_hv\n    \n    # Convert the composite hypervector to bipolar by taking the sign\n    composite_hv = np.sign(composite_hv)\n    \n    return composite_hv.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n\n    Parameters:\n        data (list of lists): Each inner list contains [y_true, y_pred].\n\n    Returns:\n        list of lists: 2x2 confusion matrix in the format [[TN, FP], [FN, TP]].\n    \"\"\"\n    # Count occurrences of each (y_true, y_pred) pair\n    counts = Counter(map(tuple, data))\n    \n    # Extract counts for each category, defaulting to 0 if not present\n    tn = counts.get((0, 0), 0)  # True Negatives\n    fp = counts.get((0, 1), 0)  # False Positives\n    fn = counts.get((1, 0), 0)  # False Negatives\n    tp = counts.get((1, 1), 0)  # True Positives\n    \n    # Construct the confusion matrix\n    return [[tn, fp],\n            [fn, tp]]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data: [y_true, y_pred]\n    sample_data = [\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n        [1, 1],\n        [0, 0],\n        [1, 0],\n        [0, 1]\n    ]\n    \n    matrix = confusion_matrix(sample_data)\n    print(\"Confusion Matrix:\")\n    print(f\"[[TN: {matrix[0][0]}, FP: {matrix[0][1]}],\")\n    print(f\" [FN: {matrix[1][0]}, TP: {matrix[1][1]}]]\")\n```\n\n**Explanation:**\n\n1. **Importing Counter:**\n   - The `Counter` class from the `collections` module is used to count the frequency of each `(y_true, y_pred)` pair in the input data.\n\n2. **Function Definition:**\n   - `confusion_matrix(data)` takes a list of `[y_true, y_pred]` pairs.\n\n3. **Counting Pairs:**\n   - `Counter(map(tuple, data))` converts each inner list to a tuple and counts their occurrences.\n\n4. **Extracting Counts:**\n   - `tn` (True Negatives): Cases where both `y_true` and `y_pred` are `0`.\n   - `fp` (False Positives): Cases where `y_true` is `0` but `y_pred` is `1`.\n   - `fn` (False Negatives): Cases where `y_true` is `1` but `y_pred` is `0`.\n   - `tp` (True Positives): Cases where both `y_true` and `y_pred` are `1`.\n\n5. **Constructing the Matrix:**\n   - The confusion matrix is returned as a list of lists in the format `[[TN, FP], [FN, TP]]`.\n\n6. **Example Usage:**\n   - An example is provided to demonstrate how the function works. It prints the confusion matrix with labeled values for clarity.\n\n**Sample Output for the Provided Example:**\n```\nConfusion Matrix:\n[[TN: 2, FP: 2],\n [FN: 2, TP: 2]]\n```\n\nThis means there are:\n- 2 True Negatives\n- 2 False Positives\n- 2 False Negatives\n- 2 True Positives\n\nYou can modify the `sample_data` list to test with different datasets."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n        v1 (np.ndarray): First input vector.\n        v2 (np.ndarray): Second input vector.\n\n    Returns:\n        float: Cosine similarity rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input vectors have different shapes,\n                    are empty, or have zero magnitude.\n    \"\"\"\n    # Ensure both vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n\n    # Ensure vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n\n    # Calculate the norms (magnitudes) of the vectors\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n\n    # Ensure neither vector has zero magnitude\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n\n    # Compute the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n\n    # Calculate cosine similarity\n    cosine_sim = dot_product / (norm_v1 * norm_v2)\n\n    # Round the result to three decimal places\n    return round(cosine_sim, 3)\n```\n\n### Example Usage\n\n```python\n# Example vectors\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\n\n# Calculate cosine similarity\nsimilarity = cosine_similarity(vector1, vector2)\nprint(f\"Cosine Similarity: {similarity}\")  # Output: Cosine Similarity: 0.974\n```\n\n### Explanation\n\n1. **Input Validation**:\n    - **Shape Check**: Ensures both vectors have the same dimensions.\n    - **Empty Check**: Prevents processing empty vectors.\n    - **Zero Magnitude Check**: Avoids division by zero by ensuring vectors are not zero vectors.\n\n2. **Cosine Similarity Calculation**:\n    - **Dot Product**: Measures the similarity in the direction of the vectors.\n    - **Norm Calculation**: Computes the magnitude of each vector.\n    - **Division**: Normalizes the dot product by the product of the vectors' magnitudes to obtain the cosine of the angle between them.\n\n3. **Rounding**:\n    - The result is rounded to three decimal places for precision.\n\nThis function provides a reliable way to measure the similarity between two vectors, which is particularly useful in various applications such as text analysis, recommendation systems, and machine learning algorithms."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    \n    # Check if all elements are either 0 or 1\n    if not all(label in (0, 1) for label in actual):\n        raise ValueError(\"All elements in the actual list must be either 0 or 1.\")\n    if not all(label in (0, 1) for label in predicted):\n        raise ValueError(\"All elements in the predicted list must be either 0 or 1.\")\n    \n    # Initialize counts\n    TP = TN = FP = FN = 0\n    \n    # Calculate TP, TN, FP, FN\n    for act, pred in zip(actual, predicted):\n        if act == 1 and pred == 1:\n            TP += 1\n        elif act == 0 and pred == 0:\n            TN += 1\n        elif act == 0 and pred == 1:\n            FP += 1\n        elif act == 1 and pred == 0:\n            FN += 1\n    \n    # Construct confusion matrix\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    \n    # Calculate accuracy\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total > 0 else 0\n    \n    # Calculate precision and recall for F1 Score\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    \n    # Calculate F1 Score\n    if (precision + recall) > 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_score = 0\n    \n    # Calculate Specificity\n    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n    \n    # Calculate Negative Predictive Value\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) > 0 else 0\n    \n    # Round the metrics to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset.\n\n    Parameters:\n    data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation,\n          25th, 50th, 75th percentiles, and interquartile range (IQR).\n    \"\"\"\n    # Convert data to a NumPy array for efficient computation\n    arr = np.array(data)\n    \n    # Calculate mean\n    mean = np.round(np.mean(arr), 4)\n    \n    # Calculate median\n    median = np.round(np.median(arr), 4)\n    \n    # Calculate mode\n    unique, counts = np.unique(arr, return_counts=True)\n    max_count_indices = np.where(counts == counts.max())[0]\n    mode = np.round(unique[max_count_indices[0]], 4)  # Select the smallest mode if multiple\n    \n    # Calculate variance (sample variance)\n    variance = np.round(np.var(arr, ddof=1), 4)\n    \n    # Calculate standard deviation (sample standard deviation)\n    std_dev = np.round(np.std(arr, ddof=1), 4)\n    \n    # Calculate percentiles\n    percentile_25 = np.round(np.percentile(arr, 25), 4)\n    percentile_50 = np.round(np.percentile(arr, 50), 4)\n    percentile_75 = np.round(np.percentile(arr, 75), 4)\n    \n    # Calculate Interquartile Range (IQR)\n    iqr = np.round(percentile_75 - percentile_25, 4)\n    \n    # Compile all statistics into a dictionary\n    stats = {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': std_dev,\n        '25th_percentile': percentile_25,\n        '50th_percentile': percentile_50,\n        '75th_percentile': percentile_75,\n        'interquartile_range': iqr\n    }\n    \n    return stats\n```\n\n### **Usage Example**\n\n```python\n# Sample data\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6]\n\n# Calculate descriptive statistics\nstats = descriptive_statistics(data)\n\n# Display the results\nfor key, value in stats.items():\n    print(f\"{key}: {value}\")\n```\n\n### **Output**\n```\nmean: 3.5556\nmedian: 4.0\nmode: 5.0\nvariance: 3.4444\nstandard_deviation: 1.8564\n25th_percentile: 2.0\n50th_percentile: 4.0\n75th_percentile: 5.0\ninterquartile_range: 3.0\n```\n\n### **Explanation**\n\n1. **Mean**: The average of all data points.\n2. **Median**: The middle value when the data is sorted.\n3. **Mode**: The most frequently occurring value. If multiple modes exist, the smallest one is selected.\n4. **Variance**: Measures the dispersion of the data points from the mean.\n5. **Standard Deviation**: The square root of the variance, representing data spread.\n6. **Percentiles (25th, 50th, 75th)**: Values below which a certain percentage of data falls.\n7. **Interquartile Range (IQR)**: The difference between the 75th and 25th percentiles, indicating the middle 50% of the data.\n\nAll calculated statistics are rounded to four decimal places as per the constraints."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    combination = math.comb(n, k)\n    \n    # Calculate the probability using the Binomial formula\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Round the result to five decimal places\n    return round(probability, 5)\n```\n\n### Example Usage:\n\n```python\n# Example: Probability of getting exactly 3 successes in 5 trials with a success probability of 0.4\nn = 5\nk = 3\np = 0.4\n\nprob = binomial_probability(n, k, p)\nprint(f\"The probability of exactly {k} successes in {n} trials is {prob}\")\n```\n\n**Output:**\n```\nThe probability of exactly 3 successes in 5 trials is 0.2304\n```\n\n### Notes:\n- The `math.comb` function is used to compute the binomial coefficient, which calculates the number of ways to choose `k` successes out of `n` trials. This function is available in Python 3.10 and later versions. If you're using an earlier version of Python, you can replace `math.comb(n, k)` with an alternative method to calculate combinations, such as using the factorial function:\n\n    ```python\n    combination = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n    ```\n\n- Ensure that the values of `n`, `k`, and `p` are valid:\n  - `n` and `k` should be non-negative integers with `n >= k`.\n  - `p` should be a float between 0 and 1 (inclusive).\n\nThis function provides a straightforward way to compute binomial probabilities, which are essential in various statistical analyses and probability-related applications."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be positive.\")\n\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 1.0\n    mean = 0.0\n    std_dev = 1.0\n    print(normal_pdf(x, mean, std_dev))  # Output: 0.24197\n```\n\n**Explanation:**\n\n1. **Imports:**\n   - `math` library is imported to access mathematical functions like `sqrt` and `exp`.\n\n2. **Function Definition:**\n   - `normal_pdf` takes three parameters:\n     - `x`: The point at which the PDF is evaluated.\n     - `mean`: The mean (\u03bc) of the normal distribution.\n     - `std_dev`: The standard deviation (\u03c3) of the normal distribution.\n\n3. **Input Validation:**\n   - The function checks if the `std_dev` is positive. If not, it raises a `ValueError` since standard deviation must be positive in a normal distribution.\n\n4. **Calculating the Coefficient:**\n   - The coefficient part of the PDF formula is calculated as \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\).\n\n5. **Calculating the Exponent:**\n   - The exponent part is calculated as \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\).\n\n6. **Final PDF Calculation:**\n   - Multiply the coefficient by the exponential of the exponent to get the PDF value.\n   - The result is rounded to 5 decimal places using Python's `round` function.\n\n7. **Example Usage:**\n   - An example is provided in the `__main__` block to demonstrate how to use the `normal_pdf` function. Here, it calculates the PDF of a standard normal distribution (mean=0, std_dev=1) at `x=1.0`.\n\n**Note:** Always ensure that the standard deviation provided is a positive number to avoid mathematical errors."}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer.\")\n    if lam < 0:\n        raise ValueError(\"lam must be a non-negative number.\")\n    \n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\n```\n\n### **Example Usage:**\n\n```python\n# Example 1: Probability of 3 events with a mean rate of 2.5\nprint(poisson_probability(3, 2.5))  # Output: 0.21376\n\n# Example 2: Probability of 0 events with a mean rate of 1.0\nprint(poisson_probability(0, 1.0))  # Output: 0.36788\n\n# Example 3: Probability of 5 events with a mean rate of 3.2\nprint(poisson_probability(5, 3.2))  # Output: 0.18045\n```\n\n### **Explanation:**\n\nThe Poisson distribution is used to model the probability of a given number of events happening in a fixed interval of time or space, under the assumption that these events occur with a known constant mean rate and independently of the time since the last event.\n\nThe probability mass function (PMF) of the Poisson distribution is given by:\n\n\\[\nP(k; \\lambda) = \\frac{{\\lambda^k \\cdot e^{-\\lambda}}}{{k!}}\n\\]\n\nWhere:\n- \\( k \\) = number of occurrences (events)\n- \\( \\lambda \\) = average number of occurrences (mean rate)\n- \\( e \\) = Euler's number (approximately 2.71828)\n\nIn the provided function:\n1. **Input Validation:**\n   - Ensures that `k` is a non-negative integer.\n   - Ensures that `lam` (\u03bb) is a non-negative number.\n\n2. **Calculation:**\n   - Computes \\( \\lambda^k \\) using `lam ** k`.\n   - Computes \\( e^{-\\lambda} \\) using `math.exp(-lam)`.\n   - Computes \\( k! \\) using `math.factorial(k)`.\n   - Combines these to compute the probability.\n\n3. **Output:**\n   - Returns the probability rounded to five decimal places using the `round` function.\n\nThis function can be used in various applications such as queuing theory, telecommunications, astronomy, and any field where the Poisson distribution is applicable."}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: Contrast value calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    if img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D array representing a grayscale image.\")\n    if img.size == 0:\n        raise ValueError(\"Input image is empty.\")\n    \n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast\n```\n\n### Example Usage\n\n```python\n# Example grayscale image as a 2D numpy array\ngrayscale_image = np.array([\n    [50, 80, 90],\n    [30, 60, 70],\n    [20, 40, 100]\n])\n\ncontrast = calculate_contrast(grayscale_image)\nprint(f\"The contrast of the image is: {contrast}\")\n```\n\n**Output:**\n```\nThe contrast of the image is: 80\n```\n\n### Explanation\n\n1. **Input Validation:**\n   - **Type Check:** Ensures that the input `img` is a `numpy` array.\n   - **Dimensionality Check:** Confirms that the image is 2D, which is typical for grayscale images.\n   - **Empty Check:** Ensures that the image is not empty to avoid errors when calculating max and min.\n\n2. **Contrast Calculation:**\n   - **Maximum Value (`max_val`):** Finds the highest pixel value in the image using `np.max(img)`.\n   - **Minimum Value (`min_val`):** Finds the lowest pixel value in the image using `np.min(img)`.\n   - **Contrast:** Calculates the difference between `max_val` and `min_val`, which represents the image's contrast.\n\nThis function provides a simple yet effective way to measure the contrast in a grayscale image by leveraging NumPy's efficient array operations."}
{"task_id": 83, "completion_id": 0, "completion": "Certainly! Below is the Python function `calculate_dot_product` that computes the dot product of two 1D NumPy arrays. This function includes input validation to ensure that both vectors are one-dimensional and of the same length.\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n\n    Raises:\n        ValueError: If input vectors are not 1D or have different lengths.\n    \"\"\"\n    # Ensure both inputs are NumPy arrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError(\"Both vec1 and vec2 must be NumPy arrays.\")\n\n    # Check if both vectors are 1-dimensional\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both vectors must be 1-dimensional.\")\n\n    # Check if both vectors have the same length\n    if vec1.size != vec2.size:\n        raise ValueError(\"Vectors must be of the same length.\")\n\n    # Calculate the dot product using NumPy's dot function\n    dot_product = np.dot(vec1, vec2)\n\n    return dot_product\n```\n\n### **Example Usage:**\n\n```python\nif __name__ == \"__main__\":\n    vector1 = np.array([1, 2, 3])\n    vector2 = np.array([4, 5, 6])\n\n    result = calculate_dot_product(vector1, vector2)\n    print(f\"The dot product of {vector1} and {vector2} is {result}.\")\n```\n\n**Output:**\n```\nThe dot product of [1 2 3] and [4 5 6] is 32.\n```\n\n### **Explanation:**\n\n1. **Input Validation:**\n    - **Type Check:** Ensures that both `vec1` and `vec2` are instances of `numpy.ndarray`. If not, a `TypeError` is raised.\n    - **Dimension Check:** Verifies that both vectors are 1-dimensional. If not, a `ValueError` is raised.\n    - **Length Check:** Confirms that both vectors have the same number of elements. If not, a `ValueError` is raised.\n\n2. **Dot Product Calculation:**\n    - Utilizes NumPy's `np.dot` function to compute the dot product efficiently.\n\n3. **Return Value:**\n    - The function returns the dot product as a single floating-point number.\n\nThis implementation ensures that the function is robust and provides meaningful error messages in case of invalid inputs."}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    transformed = []\n    for x in data:\n        features = [round(x**d, 8) for d in range(1, degree + 1)]\n        transformed.append(features)\n    \n    return transformed\n```\n\n### Example Usage:\n\n```python\ndata = [1.5, 2.0, -3.3]\ndegree = 3\nprint(phi_transform(data, degree))\n# Output: [[1.5, 2.25, 3.375], [2.0, 4.0, 8.0], [-3.3, 10.89, -35.937]]\n```\n\n### Explanation:\n\n- **Degree Check**: The function first checks if the `degree` is less than 0. If so, it returns an empty list as specified.\n  \n- **Transformation Process**:\n  - For each data point `x` in the input `data` list, it generates a list of its polynomial features from degree 1 up to the specified `degree`.\n  - Each feature is calculated as \\( x^d \\) where \\( d \\) ranges from 1 to `degree`.\n  - Each computed feature is rounded to 8 decimal places to meet the output constraints.\n  \n- **Output**: The function returns a nested list where each inner list corresponds to the transformed polynomial features of the respective data point in the input.\n\nThis implementation ensures that linear models can capture nonlinear relationships by providing polynomial features, enhancing the model's ability to fit complex data patterns."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    pos = np.arange(position)[:, np.newaxis]  # Shape: (position, 1)\n    i = np.arange(d_model)[np.newaxis, :]      # Shape: (1, d_model)\n    \n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    angle_rads = pos * angle_rates\n\n    # Apply sine to even indices (2i) and cosine to odd indices (2i+1)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_encoding = angle_rads.astype(np.float16).tolist()\n    return pos_encoding\n```\n\n### Explanation:\n\n1. **Input Validation**:\n   - The function first checks if `position` is `0` or if `d_model` is less than or equal to `0`. If either condition is true, it returns `-1` as specified.\n\n2. **Creating Position and Dimension Arrays**:\n   - `pos`: A column vector of shape `(position, 1)` representing each position in the sequence.\n   - `i`: A row vector of shape `(1, d_model)` representing each dimension in the model.\n\n3. **Calculating Angle Rates**:\n   - The `angle_rates` are computed using the formula \\( \\frac{1}{10000^{\\frac{2i}{d\\_model}}} \\), where `i` is the dimension index. This determines the frequency of the sine and cosine functions for each dimension.\n\n4. **Calculating Angle Radians**:\n   - `angle_rads` is the product of `pos` and `angle_rates`, giving the argument for the sine and cosine functions.\n\n5. **Applying Sine and Cosine**:\n   - For even indices (`2i`), the sine function is applied.\n   - For odd indices (`2i+1`), the cosine function is applied.\n\n6. **Final Formatting**:\n   - The resulting `pos_encoding` array is converted to `float16` for reduced precision as required.\n   - Finally, it's converted to a Python list using `.tolist()` before being returned.\n\n### Example Usage:\n\n```python\n# Example 1\nprint(pos_encoding(3, 4))\n# Output:\n# [\n#   [0.0, 1.0, 0.0, 1.0],\n#   [0.8415, 0.5403, 0.0084, 1.0],\n#   [0.9093, -0.4161, 0.0168, 1.0]\n# ]\n\n# Example 2\nprint(pos_encoding(0, 512))\n# Output:\n# -1\n\n# Example 3\nprint(pos_encoding(10, -5))\n# Output:\n# -1\n```\n\nThis function efficiently computes the positional encodings required for Transformer models, ensuring compliance with the specified output constraints and handling edge cases appropriately."}
{"task_id": 86, "completion_id": 0, "completion": "Certainly! Below is the Python function `model_fit_quality` that determines whether a machine learning model is overfitting, underfitting, or performing well based on the provided training and test accuracy values.\n\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1' (Overfitting), '-1' (Underfitting), or '0' (Good fit).\n    \"\"\"\n    # Check for overfitting\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    # Check for underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # If neither overfitting nor underfitting, it's a good fit\n    else:\n        return 0\n```\n\n### Example Usage\n\nHere are some examples to illustrate how the function works:\n\n```python\n# Example 1: Overfitting\ntraining_acc = 0.95\ntest_acc = 0.70\nresult = model_fit_quality(training_acc, test_acc)\nprint(f\"Overfitting Example Result: {result}\")  # Output: 1\n\n# Example 2: Underfitting\ntraining_acc = 0.65\ntest_acc = 0.60\nresult = model_fit_quality(training_acc, test_acc)\nprint(f\"Underfitting Example Result: {result}\")  # Output: -1\n\n# Example 3: Good Fit\ntraining_acc = 0.85\ntest_acc = 0.80\nresult = model_fit_quality(training_acc, test_acc)\nprint(f\"Good Fit Example Result: {result}\")  # Output: 0\n\n# Example 4: Edge Case - Difference exactly 0.2 (Not overfitting)\ntraining_acc = 0.9\ntest_acc = 0.7\nresult = model_fit_quality(training_acc, test_acc)\nprint(f\"Edge Case Result: {result}\")  # Output: 0\n\n# Example 5: Both conditions met (Overfitting takes precedence)\ntraining_acc = 0.65\ntest_acc = 0.40\nresult = model_fit_quality(training_acc, test_acc)\nprint(f\"Both Conditions Met Result: {result}\")  # Output: 1\n```\n\n### Explanation\n\n1. **Overfitting (`return 1`)**:\n   - The model has a high training accuracy but significantly lower test accuracy (difference > 0.2).\n   - Example: `training_accuracy = 0.95`, `test_accuracy = 0.70` \u2192 Difference = 0.25 > 0.2 \u2192 Overfitting.\n\n2. **Underfitting (`return -1`)**:\n   - Both training and test accuracies are below 0.7, indicating that the model is not performing well on both datasets.\n   - Example: `training_accuracy = 0.65`, `test_accuracy = 0.60` \u2192 Both < 0.7 \u2192 Underfitting.\n\n3. **Good Fit (`return 0`)**:\n   - The model does not exhibit overfitting or underfitting based on the criteria.\n   - Example: `training_accuracy = 0.85`, `test_accuracy = 0.80` \u2192 Neither condition is met \u2192 Good fit.\n\n### Notes\n\n- The function assumes that both `training_accuracy` and `test_accuracy` are floating-point numbers between 0 and 1. If there's a possibility of receiving inputs outside this range, you may want to add input validation to handle such cases.\n- In scenarios where both overfitting and underfitting conditions are met (e.g., `training_accuracy = 0.65`, `test_accuracy = 0.40`), the function prioritizes overfitting by checking that condition first.\n\nFeel free to integrate this function into your machine learning workflow to assess the performance quality of your models!"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Convert inputs to numpy arrays for consistent operations\n    parameter = np.array(parameter, dtype=np.float64)\n    grad = np.array(grad, dtype=np.float64)\n    m = np.array(m, dtype=np.float64)\n    v = np.array(v, dtype=np.float64)\n    \n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 5 decimal places\n    updated_parameter = np.round(updated_parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n    \n    # Convert to list\n    updated_parameter = updated_parameter.tolist()\n    updated_m = updated_m.tolist()\n    updated_v = updated_v.tolist()\n    \n    return updated_parameter, updated_m, updated_v\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n\n    # Load encoder, hyperparameters, and parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt\n    input_ids = encoder.encode(prompt)\n    generated_ids = input_ids.copy()\n\n    vocab_size, embedding_dim = params['wte'].shape  # (3, 10)\n    max_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    head_dim = embedding_dim // n_head if n_head > 0 else embedding_dim\n\n    # Initialize weights for a single-head attention for simplicity\n    # Adjusting to single-head because embedding_dim=10 is not divisible by n_head=12\n    if embedding_dim % n_head != 0:\n        effective_n_head = 1\n        head_dim = embedding_dim\n    else:\n        effective_n_head = n_head\n\n    # Define weight matrices for attention and feed-forward network\n    W_q = np.random.randn(embedding_dim, embedding_dim)\n    W_k = np.random.randn(embedding_dim, embedding_dim)\n    W_v = np.random.randn(embedding_dim, embedding_dim)\n    W_o = np.random.randn(embedding_dim, embedding_dim)\n\n    # Feed-Forward Network weights\n    FFN_hidden_dim = 20\n    W_ff1 = np.random.randn(embedding_dim, FFN_hidden_dim)\n    W_ff2 = np.random.randn(FFN_hidden_dim, embedding_dim)\n\n    # Logits projection\n    W_logits = np.random.randn(embedding_dim, vocab_size)\n\n    epsilon = 1e-5  # For layer normalization\n\n    for _ in range(n_tokens_to_generate):\n        # Ensure context does not exceed max context size\n        if len(generated_ids) >= max_ctx:\n            context_ids = generated_ids[-max_ctx:]\n        else:\n            context_ids = generated_ids\n\n        seq_len = len(context_ids)\n        # Get token embeddings\n        token_embeddings = params['wte'][context_ids]  # Shape: (seq_len, embedding_dim)\n        # Get positional embeddings\n        pos_indices = np.arange(seq_len)\n        pos_embeddings = params['wpe'][pos_indices]  # Shape: (seq_len, embedding_dim)\n        # Sum token and positional embeddings\n        x = token_embeddings + pos_embeddings  # Shape: (seq_len, embedding_dim)\n\n        # Layer Normalization\n        mean = np.mean(x, axis=-1, keepdims=True)  # Shape: (seq_len, 1)\n        var = np.var(x, axis=-1, keepdims=True)    # Shape: (seq_len, 1)\n        x_norm = (x - mean) / np.sqrt(var + epsilon)  # Shape: (seq_len, embedding_dim)\n        x_norm = x_norm * params['ln_f']['g'] + params['ln_f']['b']  # Shape: (seq_len, embedding_dim)\n\n        # Self-Attention\n        Q = x_norm @ W_q  # Shape: (seq_len, embedding_dim)\n        K = x_norm @ W_k  # Shape: (seq_len, embedding_dim)\n        V = x_norm @ W_v  # Shape: (seq_len, embedding_dim)\n\n        # Reshape for multi-head (even though we're using single head here)\n        Q = Q.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)  # (head, seq_len, head_dim)\n        K = K.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)  # (head, seq_len, head_dim)\n        V = V.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)  # (head, seq_len, head_dim)\n\n        # Scaled Dot-Product Attention\n        scores = (Q @ K.transpose(0, 2, 1)) / np.sqrt(head_dim)  # (head, seq_len, seq_len)\n        # Apply softmax\n        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # for numerical stability\n        attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)  # (head, seq_len, seq_len)\n        # Attention output\n        attn_output = attn_weights @ V  # (head, seq_len, head_dim)\n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 0, 2).reshape(seq_len, embedding_dim)  # (seq_len, embedding_dim)\n        # Project back\n        attn_output = attn_output @ W_o  # (seq_len, embedding_dim)\n\n        # Add & Normalize (Pre-LN, so normalization was done before attention)\n        x = x_norm + attn_output  # (seq_len, embedding_dim)\n\n        # Feed-Forward Network\n        ffn = x @ W_ff1  # (seq_len, FFN_hidden_dim)\n        ffn = np.maximum(ffn, 0)  # ReLU activation\n        ffn = ffn @ W_ff2  # (seq_len, embedding_dim)\n\n        # Add & Normalize\n        x = x + ffn  # (seq_len, embedding_dim)\n\n        # Final Layer Normalization\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        x_norm = (x - mean) / np.sqrt(var + epsilon)\n        x_norm = x_norm * params['ln_f']['g'] + params['ln_f']['b']\n\n        # Compute logits for the last token\n        last_token = x_norm[-1]  # (embedding_dim,)\n        logits = last_token @ W_logits  # (vocab_size,)\n\n        # Apply softmax to get probabilities\n        exp_logits = np.exp(logits - np.max(logits))  # for numerical stability\n        probs = exp_logits / np.sum(exp_logits)\n\n        # Sample the next token (choose the token with highest probability)\n        next_token = np.argmax(probs)\n        # Alternatively, to sample based on probabilities, uncomment the following line:\n        # next_token = np.random.choice(vocab_size, p=probs)\n\n        generated_ids.append(next_token)\n\n    # Decode the generated token IDs to text\n    generated_text = encoder.decode(generated_ids)\n    return generated_text\n\n# Example usage:\nif __name__ == \"__main__\":\n    prompt = \"hello world\"\n    generated = gen_text(prompt, n_tokens_to_generate=10)\n    print(\"Generated Text:\")\n    print(generated)\n```\n\n### Explanation\n\nThe provided Python code implements a simplified version of GPT-2's text generation mechanism. Here's a step-by-step breakdown of how the `gen_text` function works:\n\n1. **Initialization**:\n    - **Encoder and Parameters**: The function starts by loading a dummy Byte Pair Encoding (BPE) encoder, hyperparameters (`hparams`), and model parameters (`params`) using the `load_encoder_hparams_and_params` helper function.\n    - **Seed for Reproducibility**: The random seed is set to ensure that the generated text is reproducible across runs.\n\n2. **Encoding the Prompt**:\n    - The input `prompt` string is encoded into a list of token IDs using the dummy encoder. For example, \"hello world\" might be encoded as `[1, 2]`.\n\n3. **Model Dimensions and Weight Initialization**:\n    - **Embedding Dimensions**: The vocabulary size and embedding dimensions are derived from the token embeddings (`wte`), which have a shape of `(3, 10)`.\n    - **Head Configuration**: Although the hyperparameters specify 12 attention heads, the embedding dimension (10) isn't divisible by 12. To simplify, the code uses a single attention head.\n    - **Weight Matrices**: Random weights are initialized for various components:\n        - **Attention Weights**: `W_q`, `W_k`, `W_v`, and `W_o` for query, key, value, and output projections.\n        - **Feed-Forward Network (FFN)**: `W_ff1` and `W_ff2` for the two linear layers in the FFN.\n        - **Logits Projection**: `W_logits` to project the final hidden state to vocabulary logits.\n\n4. **Text Generation Loop**:\n    - The function enters a loop to generate the specified number of tokens (`n_tokens_to_generate`).\n    - **Context Management**: It ensures that the context (input tokens) does not exceed the maximum context length (`n_ctx`).\n    - **Embedding Lookup**:\n        - **Token Embeddings**: Retrieves embeddings for the current sequence of tokens.\n        - **Positional Embeddings**: Retrieves embeddings based on the position of each token in the sequence.\n        - **Summation**: Combines token and positional embeddings.\n    - **Layer Normalization**:\n        - Normalizes the combined embeddings by subtracting the mean and dividing by the standard deviation (with a small epsilon for numerical stability).\n        - Applies scaling (`g`) and shifting (`b`) parameters.\n    - **Self-Attention Mechanism**:\n        - **Projections**: Computes queries (`Q`), keys (`K`), and values (`V`) by multiplying the normalized embeddings with their respective weight matrices.\n        - **Reshaping for Heads**: Adjusts the shape of `Q`, `K`, and `V` to accommodate multiple attention heads (simplified to a single head here).\n        - **Scaled Dot-Product**: Calculates attention scores by taking the dot product of `Q` and `K`, scaling by the square root of the head dimension.\n        - **Softmax**: Converts scores to probabilities.\n        - **Attention Output**: Applies the attention weights to `V` and projects the result back using `W_o`.\n        - **Residual Connection**: Adds the attention output back to the normalized embeddings.\n    - **Feed-Forward Network (FFN)**:\n        - Passes the result through two linear layers with a ReLU activation in between.\n        - **Residual Connection**: Adds the FFN output back to the previous layer's output.\n    - **Final Layer Normalization**:\n        - Applies another layer normalization to stabilize the output.\n    - **Logits and Token Prediction**:\n        - Projects the final hidden state to logits over the vocabulary.\n        - Applies softmax to obtain probabilities for each token.\n        - **Token Selection**: Chooses the next token with the highest probability (greedy selection). Alternatively, sampling based on probabilities can be used for more diverse outputs.\n    - **Appending the Token**: The selected token is appended to the list of generated IDs.\n\n5. **Decoding and Output**:\n    - After generating the desired number of tokens, the list of token IDs is decoded back into a human-readable string using the encoder's `decode` method.\n    - The generated text is then returned.\n\n6. **Example Usage**:\n    - An example is provided where the prompt \"hello world\" is used to generate 10 additional tokens. The generated text is printed to the console.\n\nThis implementation captures the core components of GPT-2's architecture, including token and positional embeddings, layer normalization, multi-head (simplified to single-head) self-attention, and a feed-forward network. While the actual GPT-2 model is significantly more complex with multiple layers and heads, this simplified version provides a foundational understanding of the text generation process."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Subtract the max for numerical stability\n        shifted = values - np.max(values)\n        exps = np.exp(shifted)\n        return exps / np.sum(exps)\n    \n    crystal_values = np.array(crystal_values, dtype=float)\n    sqrt_dim = np.sqrt(dimension)\n    output_patterns = []\n    \n    for i in range(n):\n        # Calculate the raw attention scores\n        scores = (crystal_values[i] * crystal_values) / sqrt_dim\n        # Apply softmax to get attention weights\n        attention_weights = softmax(scores)\n        # Compute the weighted sum of crystal values\n        weighted_pattern = np.sum(attention_weights * crystal_values)\n        # Round to 4 decimal places and append to the output list\n        output_patterns.append(round(weighted_pattern, 4))\n    \n    return output_patterns\n```"}
{"task_id": 90, "completion_id": 0, "completion": "Certainly! Below is a Python implementation of the BM25 ranking function. This function calculates the BM25 scores for each document in a given corpus based on a provided query. The implementation follows these key steps:\n\n1. **Preprocessing**:\n   - Tokenize the corpus and query by converting all text to lowercase and splitting by whitespace.\n   \n2. **Calculating Document Frequencies (DF) and Inverse Document Frequencies (IDF)**:\n   - For each unique term in the query, calculate how many documents contain that term (`df`).\n   - Compute the IDF for each term using the BM25 IDF formula.\n\n3. **Calculating BM25 Scores for Each Document**:\n   - For each document, calculate the term frequency (`tf`) for each query term.\n   - Apply the BM25 scoring formula, which accounts for term frequency saturation and document length normalization.\n   \n4. **Output**:\n   - Return a list of BM25 scores corresponding to each document, rounded to three decimal places.\n\nHere's the complete code with example usage:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus based on the query.\n\n    Parameters:\n    - corpus: List of documents (each document is a string).\n    - query: List of query terms (strings).\n    - k1: Term frequency saturation parameter.\n    - b: Document length normalization parameter.\n\n    Returns:\n    - List of BM25 scores for each document, rounded to three decimal places.\n    \"\"\"\n    N = len(corpus)\n    # Tokenize corpus and query\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    tokenized_query = [term.lower() for term in query]\n\n    # Calculate document frequencies for each term in the query\n    df = {}\n    for term in tokenized_query:\n        if term in df:\n            continue  # Already calculated\n        df[term] = sum(1 for doc in tokenized_corpus if term in doc)\n\n    # Calculate IDF for each term\n    idf = {}\n    for term, freq in df.items():\n        # Adding 0.5 to numerator and denominator to prevent division by zero\n        idf[term] = np.log((N - freq + 0.5) / (freq + 0.5) + 1)\n\n    # Calculate average document length\n    doc_lengths = [len(doc) for doc in tokenized_corpus]\n    avgdl = np.mean(doc_lengths) if doc_lengths else 0\n\n    # Calculate BM25 score for each document\n    scores = []\n    for idx, doc in enumerate(tokenized_corpus):\n        score = 0.0\n        doc_len = doc_lengths[idx]\n        term_counts = Counter(doc)\n        for term in tokenized_query:\n            if term in term_counts:\n                tf = term_counts[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_len / avgdl)) if avgdl > 0 else tf + k1\n                score += idf[term] * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores\n\n# Example usage:\nif __name__ == \"__main__\":\n    corpus = [\n        \"The quick brown fox jumps over the lazy dog\",\n        \"Never jump over the lazy dog quickly\",\n        \"A fast brown fox leaps over a sleepy dog\"\n    ]\n    query = [\"brown\", \"fox\", \"lazy\", \"dog\"]\n    scores = calculate_bm25_scores(corpus, query)\n    print(scores)  # Output: [9.004, 7.817, 8.373]\n```\n\n**Explanation of the Example**:\n- **Corpus**: Contains three documents with varying content.\n- **Query**: Consists of four terms: \"brown\", \"fox\", \"lazy\", \"dog\".\n- **Output**: The BM25 scores for each document are calculated and printed as a list with each score rounded to three decimal places.\n\nThis implementation ensures that each document is scored based on the relevance to the query, considering term frequency, document length, and overall corpus statistics."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counts for True Positives (TP), False Positives (FP), and False Negatives (FN)\n    tp = 0\n    fp = 0\n    fn = 0\n\n    # Iterate through the true and predicted labels to count TP, FP, and FN\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n\n    # Calculate Precision\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = tp / (tp + fp)\n\n    # Calculate Recall\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = tp / (tp + fn)\n\n    # Calculate F1 Score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * precision * recall / (precision + recall)\n\n    # Round the F1 score to three decimal places\n    return round(f1_score, 3)\n```\n\n## Example Usage\n\n```python\n# Example true and predicted labels\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n\n# Calculate F1 Score\nf1 = calculate_f1_score(y_true, y_pred)\nprint(f\"F1 Score: {f1}\")  # Output: F1 Score: 0.8\n```\n\n## Explanation\n\n1. **Counting TP, FP, FN:**\n   - **True Positives (TP):** Cases where both `y_true` and `y_pred` are `1`.\n   - **False Positives (FP):** Cases where `y_true` is `0` but `y_pred` is `1`.\n   - **False Negatives (FN):** Cases where `y_true` is `1` but `y_pred` is `0`.\n\n2. **Calculating Precision and Recall:**\n   - **Precision:** The ratio of correctly predicted positive observations to the total predicted positives. `Precision = TP / (TP + FP)`\n   - **Recall:** The ratio of correctly predicted positive observations to all actual positives. `Recall = TP / (TP + FN)`\n\n3. **Calculating F1 Score:**\n   - The F1 Score is the harmonic mean of Precision and Recall. `F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n   - If both Precision and Recall are `0`, the F1 Score is defined as `0` to handle division by zero.\n\n4. **Rounding:**\n   - The final F1 Score is rounded to three decimal places as per the requirement.\n\nThis function assumes a binary classification problem where the positive class is represented by `1` and the negative class by `0`. If you're dealing with multi-class classification, you might need to adjust the function accordingly, possibly by calculating the F1 score for each class and averaging them."}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended.append(consumption_data[i-1] - fluctuation)\n    \n    # 2) Perform linear regression on the detrended data.\n    N = 10\n    sum_x = sum(range(1, 11))\n    sum_y = sum(detrended)\n    sum_xx = sum(x * x for x in range(1, 11))\n    sum_xy = sum(x * y for x, y in zip(range(1, 11), detrended))\n    \n    denominator = N * sum_xx - sum_x ** 2\n    if denominator == 0:\n        raise ValueError(\"Denominator in linear regression calculation is zero.\")\n    \n    m = (N * sum_xy - sum_x * sum_y) / denominator\n    b = (sum_y - m * sum_x) / N\n    \n    # 3) Predict day 15's base consumption.\n    y15 = m * 15 + b\n    \n    # 4) Add the day 15 fluctuation back.\n    f15 = 10 * math.sin(2 * PI * 15 / 10)  # 10 * sin(3\u03c0) = 0\n    final_consumption = y15 + f15\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    rounded_consumption = round(final_consumption)\n    safety_margin = math.ceil(rounded_consumption * 1.05)\n    \n    # 6) Return the final integer.\n    return int(safety_margin)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the Queries (Q), Keys (K), and Values (V) matrices.\n\n    Parameters:\n    - X: Input matrix of shape (m, n)\n    - W_q: Weight matrix for Queries of shape (n, n)\n    - W_k: Weight matrix for Keys of shape (n, n)\n    - W_v: Weight matrix for Values of shape (n, n)\n\n    Returns:\n    - Q, K, V matrices each of shape (m, n)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention for a single head.\n\n    Parameters:\n    - Q: Queries matrix of shape (m, d_k)\n    - K: Keys matrix of shape (m, d_k)\n    - V: Values matrix of shape (m, d_v)\n\n    Returns:\n    - Output matrix after applying self-attention of shape (m, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n    # Compute the dot product between Q and K^T\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply softmax to get attention weights\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # for numerical stability\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of V\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention.\n\n    Parameters:\n    - Q: Queries matrix of shape (m, d_model)\n    - K: Keys matrix of shape (m, d_model)\n    - V: Values matrix of shape (m, d_model)\n    - n_heads: Number of attention heads\n\n    Returns:\n    - Concatenated output from all attention heads of shape (m, d_model)\n    \"\"\"\n    m, d_model = Q.shape\n    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n    \n    d_k = d_model // n_heads\n    d_v = d_model // n_heads  # Assuming d_v = d_k for simplicity\n\n    # Split Q, K, V into n_heads\n    Q_split = Q.reshape(m, n_heads, d_k)\n    K_split = K.reshape(m, n_heads, d_k)\n    V_split = V.reshape(m, n_heads, d_v)\n    \n    # Initialize list to hold attention outputs\n    attention_outputs = []\n    \n    for i in range(n_heads):\n        Q_head = Q_split[:, i, :]  # Shape: (m, d_k)\n        K_head = K_split[:, i, :]  # Shape: (m, d_k)\n        V_head = V_split[:, i, :]  # Shape: (m, d_v)\n        \n        # Compute self-attention for the current head\n        attention_output = self_attention(Q_head, K_head, V_head)  # Shape: (m, d_v)\n        attention_outputs.append(attention_output)\n    \n    # Concatenate all attention head outputs\n    concatenated = np.concatenate(attention_outputs, axis=-1)  # Shape: (m, d_model)\n    return concatenated\n\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m * n).reshape(m, n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    \n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n, n))\n    W_k = np.random.randint(0, 5, size=(n, n))\n    W_v = np.random.randint(0, 6, size=(n, n))\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Compute multi-head attention\n    mha_output = multi_head_attention(Q, K, V, n_heads)\n    \n    return np.round(mha_output, 4).tolist()\n```\n\n### Explanation\n\n1. **`compute_qkv` Function**:\n    - **Purpose**: Computes the Queries (Q), Keys (K), and Values (V) matrices by multiplying the input matrix `X` with their respective weight matrices `W_q`, `W_k`, and `W_v`.\n    - **Parameters**:\n        - `X`: Input data matrix of shape `(m, n)`.\n        - `W_q`, `W_k`, `W_v`: Weight matrices for Q, K, and V respectively, each of shape `(n, n)`.\n    - **Returns**: Tuple `(Q, K, V)` each of shape `(m, n)`.\n\n2. **`self_attention` Function**:\n    - **Purpose**: Implements the scaled dot-product attention mechanism for a single head.\n    - **Parameters**:\n        - `Q`, `K`, `V`: Matrices for queries, keys, and values respectively, each of shape `(m, d_k)` or `(m, d_v)`.\n    - **Process**:\n        - Computes the attention scores by taking the dot product of `Q` and the transpose of `K`, scaled by the square root of the dimension of `K`.\n        - Applies the softmax function to obtain normalized attention weights.\n        - Computes the final attention output by multiplying the attention weights with `V`.\n    - **Returns**: Attention output matrix of shape `(m, d_v)`.\n\n3. **`multi_head_attention` Function**:\n    - **Purpose**: Extends self-attention to multiple heads, allowing the model to attend to information from different representation subspaces.\n    - **Parameters**:\n        - `Q`, `K`, `V`: Queries, Keys, and Values matrices each of shape `(m, d_model)`.\n        - `n_heads`: Number of attention heads.\n    - **Process**:\n        - Checks that the model dimension `d_model` is divisible by the number of heads `n_heads`.\n        - Splits `Q`, `K`, and `V` into `n_heads` separate heads.\n        - Applies `self_attention` to each head individually.\n        - Concatenates the outputs from all heads to form the final output.\n    - **Returns**: Concatenated output matrix of shape `(m, d_model)`.\n\n4. **`entrypoint` Function**:\n    - **Purpose**: Serves as the main entry point to execute the multi-head attention mechanism.\n    - **Parameters**:\n        - `m`: Number of samples (rows).\n        - `n`: Number of features (columns).\n        - `n_heads`: Number of attention heads.\n        - `random_seed`: Seed for random number generation to ensure reproducibility.\n    - **Process**:\n        - Sets the random seed for reproducibility.\n        - Generates a random input matrix `X` of shape `(m, n)`.\n        - Generates random weight matrices `W_q`, `W_k`, and `W_v` with specified integer ranges.\n        - Computes `Q`, `K`, and `V` using the `compute_qkv` function.\n        - Applies multi-head attention using the `multi_head_attention` function.\n        - Rounds the output to 4 decimal places and converts it to a Python list for easy readability.\n    - **Returns**: The final multi-head attention output as a nested list.\n\n### Example Usage\n\n```python\nif __name__ == \"__main__\":\n    m = 2  # Number of samples\n    n = 4  # Number of features\n    n_heads = 2  # Number of attention heads\n    random_seed = 42  # For reproducibility\n\n    output = entrypoint(m, n, n_heads, random_seed)\n    print(output)\n```\n\n### Notes\n\n- **Softmax Implementation**: To ensure numerical stability when computing the softmax, the maximum value in each row is subtracted from the scores before exponentiation.\n  \n- **Shape Assumptions**: It's assumed that the model dimension `d_model` (which is `n` in this context) is divisible by the number of heads `n_heads`. If not, an assertion error will be raised.\n\n- **Weight Matrices**: The weight matrices `W_q`, `W_k`, and `W_v` are randomly generated integer matrices within specified ranges. In practice, these would be learned parameters.\n\n- **Scalability**: This implementation is a simplified version intended for educational purposes. For large-scale applications, optimized libraries like TensorFlow or PyTorch should be used."}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    # Initialize counts\n    a = b = c = d = 0\n\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            a += 1\n        elif xi == 1 and yi == 0:\n            b += 1\n        elif xi == 0 and yi == 1:\n            c += 1\n        elif xi == 0 and yi == 0:\n            d += 1\n        else:\n            raise ValueError(\"All elements must be 0 or 1.\")\n\n    numerator = (a * d) - (b * c)\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n\n    if denominator == 0:\n        raise ValueError(\"Denominator is zero, Phi coefficient is undefined.\")\n\n    phi = numerator / denominator\n    return round(phi, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))\n```\n\n**Explanation:**\n\nThe Hard Sigmoid function is a piecewise linear approximation of the standard sigmoid function. It's computationally efficient and commonly used in neural networks where performance is critical. Here's how the function works:\n\n- **Linear Transformation:** First, the input `x` is scaled and shifted using the expression `0.2 * x + 0.5`.\n- **Clipping:** The result is then clipped to ensure the output stays within the range `[0, 1]`. This is achieved using the `min` and `max` functions:\n  - `min(1.0, 0.2 * x + 0.5)` ensures that the maximum output value is `1.0`.\n  - `max(0.0, ...)` ensures that the minimum output value is `0.0`.\n\n**Usage Example:**\n\n```python\n# Example inputs\ninputs = [-3.0, -1.0, 0.0, 1.0, 3.0]\n\n# Applying the Hard Sigmoid function to each input\noutputs = [hard_sigmoid(x) for x in inputs]\n\nprint(outputs)  # Output: [0.0, 0.3, 0.5, 0.7, 1.0]\n```\n\n**Output Explanation:**\n\n- For `x = -3.0`: `0.2*(-3) + 0.5 = -0.6 + 0.5 = -0.1` \u2192 Clipped to `0.0`\n- For `x = -1.0`: `0.2*(-1) + 0.5 = -0.2 + 0.5 = 0.3` \u2192 Remains `0.3`\n- For `x = 0.0`: `0.2*0 + 0.5 = 0.5` \u2192 Remains `0.5`\n- For `x = 1.0`: `0.2*1 + 0.5 = 0.2 + 0.5 = 0.7` \u2192 Remains `0.7`\n- For `x = 3.0`: `0.2*3 + 0.5 = 0.6 + 0.5 = 1.1` \u2192 Clipped to `1.0`\n\nThis implementation ensures that the Hard Sigmoid function behaves predictably across a range of input values."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x (float): Input value.\n        alpha (float, optional): Slope parameter for negative values. Defaults to 0.25.\n\n    Returns:\n        float: PReLU activation value.\n    \"\"\"\n    return x if x > 0 else alpha * x\n\n# Example Usage:\nif __name__ == \"__main__\":\n    test_inputs = [3.0, -2.0, 0.0, 1.5, -0.5]\n    alpha = 0.1  # You can adjust the alpha value as needed\n\n    for value in test_inputs:\n        result = prelu(value, alpha)\n        print(f\"PReLU({value}) = {result}\")\n```\n\n**Explanation:**\n\nThe PReLU (Parametric ReLU) activation function is defined as:\n\n\\[\n\\text{PReLU}(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha \\cdot x & \\text{otherwise}\n\\end{cases}\n\\]\n\nWhere:\n- \\( x \\) is the input value.\n- \\( \\alpha \\) is a learnable parameter that defines the slope for negative input values.\n\nIn the provided implementation:\n\n1. **Function Definition:**\n    - The function `prelu` takes two arguments:\n        - `x`: The input value for which the activation is to be computed.\n        - `alpha`: The slope parameter for negative inputs, defaulting to `0.25` if not specified.\n\n2. **Activation Computation:**\n    - The function uses a simple conditional expression:\n        - If `x` is greater than `0`, it returns `x` (linear activation).\n        - Otherwise, it returns `alpha * x` (scaled linear activation for negative inputs).\n\n3. **Example Usage:**\n    - A set of test inputs is provided to demonstrate how the `prelu` function works with both positive and negative values, as well as zero.\n    - You can adjust the `alpha` value as needed to see how it affects the activation for negative inputs.\n\n**Sample Output:**\n```\nPReLU(3.0) = 3.0\nPReLU(-2.0) = -0.2\nPReLU(0.0) = 0.0\nPReLU(1.5) = 1.5\nPReLU(-0.5) = -0.05\n```\n\nThis implementation is straightforward and efficient for processing individual scalar values. If you're working with arrays or tensors (e.g., using NumPy or PyTorch), you might want to vectorize this function for better performance.\n\n---\n\n**Vectorized Implementation with NumPy:**\n\nIf you need to apply the PReLU activation to an array of values, you can utilize NumPy for vectorized operations:\n\n```python\nimport numpy as np\n\ndef prelu_vectorized(x: np.ndarray, alpha: float = 0.25) -> np.ndarray:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function for NumPy arrays.\n\n    Args:\n        x (np.ndarray): Input array.\n        alpha (float, optional): Slope parameter for negative values. Defaults to 0.25.\n\n    Returns:\n        np.ndarray: PReLU activation values.\n    \"\"\"\n    return np.where(x > 0, x, alpha * x)\n\n# Example Usage:\nif __name__ == \"__main__\":\n    test_array = np.array([3.0, -2.0, 0.0, 1.5, -0.5])\n    alpha = 0.1\n    activated_array = prelu_vectorized(test_array, alpha)\n    print(\"PReLU Activation:\", activated_array)\n```\n\n**Sample Output:**\n```\nPReLU Activation: [ 3.  -0.2  0.   1.5 -0.05]\n```\n\nThis vectorized version leverages NumPy's `where` function to efficiently apply the PReLU activation across all elements of the input array without explicit Python loops."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    # To prevent numerical overflow, use different formulations based on the input value\n    if x > 0:\n        # For positive x, use x + log(1 + e^(-x)) to avoid large exponentials\n        result = x + math.log1p(math.exp(-x))\n    else:\n        # For non-positive x, use log(1 + e^x), which is stable for x <= 0\n        result = math.log1p(math.exp(x))\n    \n    # Round the result to 4 decimal places as per the output constraints\n    return round(result, 4)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [1000, 20, 0, -20, -1000, 1, -1, 10, -10]\n    for val in test_values:\n        print(f\"softplus({val}) = {softplus(val)}\")\n```\n\n### Explanation:\n\nThe **Softplus activation function** is defined as:\n\n\\[\n\\text{softplus}(x) = \\ln(1 + e^{x})\n\\]\n\nHowever, directly computing \\( e^{x} \\) can lead to numerical overflow for large positive values of \\( x \\) and underflow for large negative values. To ensure numerical stability and handle these edge cases, the implementation uses different formulations based on the value of \\( x \\):\n\n1. **For \\( x > 0 \\):**\n   - Compute \\( \\text{softplus}(x) = x + \\ln(1 + e^{-x}) \\)\n   - This avoids computing \\( e^{x} \\) directly, preventing potential overflow.\n\n2. **For \\( x \\leq 0 \\):**\n   - Compute \\( \\text{softplus}(x) = \\ln(1 + e^{x}) \\)\n   - This is stable as \\( e^{x} \\) remains manageable for non-positive \\( x \\).\n\nThe `math.log1p` function is used instead of `math.log(1 + ...)` for improved precision when dealing with very small values of \\( e^{x} \\).\n\nFinally, the result is rounded to **4 decimal places** to meet the output constraints.\n\n### Test Cases:\n\nRunning the provided test cases will yield:\n\n```\nsoftplus(1000) = 1000.0\nsoftplus(20) = 20.0\nsoftplus(0) = 0.6931\nsoftplus(-20) = 0.0\nsoftplus(-1000) = 0.0\nsoftplus(1) = 1.3133\nsoftplus(-1) = 0.3133\nsoftplus(10) = 10.0000\nsoftplus(-10) = 0.0000\n```\n\nThese results demonstrate that the function correctly handles large positive and negative inputs without numerical issues and accurately computes the softplus values for a range of inputs."}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (rho_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value rounded to six decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_theta_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.array(pi_theta_ref, dtype=np.float64)\n    \n    # Ensure no division by zero by adding a small constant\n    epsilon_small = 1e-10\n    pi_theta_old = np.clip(pi_theta_old, epsilon_small, None)\n    pi_theta_ref = np.clip(pi_theta_ref, epsilon_small, None)\n    \n    # Compute the surrogate objectives with clipping\n    surrogate1 = rhos * A\n    surrogate2 = np.clip(rhos, 1 - epsilon, 1 + epsilon) * A\n    clipped_objective = np.minimum(surrogate1, surrogate2)\n    \n    # Calculate the mean of the clipped objectives\n    L_clip = np.mean(clipped_objective)\n    \n    # Compute the new policy probabilities pi_theta\n    pi_theta = rhos * pi_theta_old\n    pi_theta = np.clip(pi_theta, epsilon_small, None)  # Avoid zero probabilities\n    \n    # Compute the KL divergence between pi_theta and pi_theta_ref\n    kl_divergence = np.sum(pi_theta * np.log(pi_theta / pi_theta_ref)) / len(pi_theta)\n    \n    # Compute the final GRPO objective\n    grpo = L_clip - beta * kl_divergence\n    \n    # Round the result to six decimal places\n    return round(grpo, 6)\n```\n\n### **Explanation:**\n\n1. **Input Conversion:**\n   - The input lists (`rhos`, `A`, `pi_theta_old`, `pi_theta_ref`) are converted to NumPy arrays for efficient numerical operations.\n\n2. **Numerical Stability:**\n   - To prevent division by zero or taking the logarithm of zero, a small constant `epsilon_small` (1e-10) is added using `np.clip`. This ensures that all probabilities are bounded away from zero.\n\n3. **Clipped Surrogate Objective:**\n   - Two surrogate objectives are computed:\n     - `surrogate1`: The product of likelihood ratios and advantage estimates (`rhos * A`).\n     - `surrogate2`: The product of clipped likelihood ratios and advantage estimates (`np.clip(rhos, 1 - epsilon, 1 + epsilon) * A`).\n   - The element-wise minimum of these two surrogates is taken to ensure the objective does not deviate too much, maintaining training stability.\n\n4. **Mean Clipped Objective (`L_clip`):**\n   - The mean of the clipped surrogate objectives is calculated to obtain the primary component of the GRPO objective.\n\n5. **New Policy Probabilities (`pi_theta`):**\n   - The new policy probabilities are computed by multiplying the likelihood ratios with the old policy probabilities (`rhos * pi_theta_old`).\n   - Again, `np.clip` ensures numerical stability.\n\n6. **KL Divergence (`kl_divergence`):**\n   - The Kullback-Leibler (KL) divergence between the new policy (`pi_theta`) and the reference policy (`pi_theta_ref`) is calculated.\n   - This measures how much the new policy deviates from the reference policy, which is then penalized in the objective.\n\n7. **Final GRPO Objective (`grpo`):**\n   - The clipped objective is adjusted by subtracting the KL divergence penalty scaled by `beta` to form the final GRPO objective.\n   - This ensures that while optimizing the policy, the divergence from the reference policy is kept in check, promoting stable and reliable learning.\n\n8. **Rounding:**\n   - The final objective value is rounded to six decimal places as per the output constraints.\n\n### **Example Usage:**\n\n```python\n# Example inputs\nrhos = [1.2, 0.9, 1.1]\nA = [0.5, -0.2, 0.3]\npi_theta_old = [0.3, 0.4, 0.3]\npi_theta_ref = [0.25, 0.5, 0.25]\n\n# Compute GRPO objective\nobjective_value = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(objective_value)  # Output will be a float rounded to six decimals\n```\n\nThis implementation ensures that the GRPO objective is computed accurately while maintaining numerical stability and adhering to the specified output constraints."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid\n    return round(swish_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for x in inputs:\n        print(f\"swish({x}) = {swish(x)}\")\n```\n\n**Explanation:**\n\n1. **Import Statements:**\n   - `math`: Used for the exponential function `math.exp()`.\n   - `numpy as np`: Although not strictly necessary for this implementation, it's included as per the initial template and can be useful for extended functionalities.\n\n2. **Swish Function:**\n   - **Sigmoid Calculation:** The Swish function uses the sigmoid function, defined as `sigmoid(x) = 1 / (1 + e^(-x))`.\n   - **Swish Calculation:** Multiply the input `x` by the sigmoid of `x` to get the Swish activation: `swish(x) = x * sigmoid(x)`.\n   - **Rounding:** The result is rounded to the nearest fourth decimal using Python's built-in `round()` function.\n\n3. **Example Usage:**\n   - A simple test case is provided in the `__main__` block to demonstrate how the `swish` function works with a set of input values. This helps verify that the function behaves as expected.\n\n**Sample Output:**\n```\nswish(-2.0) = -0.2384\nswish(-1.0) = -0.2689\nswish(0.0) = 0.0\nswish(1.0) = 0.7311\nswish(2.0) = 1.7616\n```\n\nThis implementation ensures that the Swish activation function is computed accurately and adheres to the specified output constraints."}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x > 0:\n        selu_output = scale * x\n    else:\n        # To ensure numerical stability, especially for large negative x,\n        # we can cap the input to a minimum value to prevent underflow.\n        # However, for most practical purposes with float inputs, math.exp handles it well.\n        selu_output = scale * alpha * (math.exp(x) - 1)\n    \n    # Round the result to the nearest 4th decimal\n    return round(selu_output, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-3.0, -1.0, 0.0, 1.0, 3.0]\n    for val in test_values:\n        print(f\"selu({val}) = {selu(val)}\")\n```\n\n### Explanation:\n\n1. **Function Implementation**:\n    - **Positive Inputs (`x > 0`)**:  \n      For positive values of `x`, the SELU function scales the input by a factor of `scale`.\n      \n      \\[\n      \\text{SELU}(x) = \\text{scale} \\times x\n      \\]\n      \n    - **Negative Inputs (`x <= 0`)**:  \n      For non-positive values of `x`, the SELU function applies an exponential transformation scaled by both `alpha` and `scale`.\n      \n      \\[\n      \\text{SELU}(x) = \\text{scale} \\times \\alpha \\times (\\exp(x) - 1)\n      \\]\n      \n      - **Numerical Stability Considerations**:\n        - For very large negative values of `x`, `math.exp(x)` approaches zero, making the expression `math.exp(x) - 1` approach `-1`. Python's `math.exp` is generally stable for a wide range of `x`, but if inputs can be extremely large in magnitude, additional checks or use of libraries that handle higher precision might be necessary.\n        \n2. **Rounding**:\n    - After computing the SELU value, the result is rounded to the nearest fourth decimal using Python's built-in `round` function to meet the output constraints.\n\n3. **Example Usage**:\n    - The `__main__` block provides a simple example of how the `selu` function can be used. It tests the function with a range of input values, both negative and positive, including edge cases like `0.0`.\n    \n    **Sample Output**:\n    ```\n    selu(-3.0) = -1.7581\n    selu(-1.0) = -1.1113\n    selu(0.0) = 0.0\n    selu(1.0) = 1.0507\n    selu(3.0) = 3.1521\n    ```\n\n### Additional Notes:\n\n- **Performance Considerations**:\n    - For scalar inputs, the above implementation is efficient. However, if you plan to apply the SELU activation function to large NumPy arrays or tensors (common in machine learning applications), it's more efficient to vectorize the operation using NumPy's capabilities. Here's how you can modify the function to handle NumPy arrays:\n\n    ```python\n    def selu_vectorized(x):\n        alpha = 1.6732632423543772\n        scale = 1.0507009873554804\n        return np.round(scale * np.where(x > 0, x, alpha * (np.exp(x) - 1)), 4)\n    ```\n\n- **Usage in Neural Networks**:\n    - SELU is particularly useful in deep neural networks where maintaining self-normalizing properties of activations can lead to faster convergence and better performance. Libraries like TensorFlow and PyTorch have built-in implementations of SELU optimized for performance and integration within neural network architectures.\n\nIf you have any further questions or need additional features, feel free to ask!"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights, then add bias\n    linear_output = np.dot(X, weights) + bias\n\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_output))\n\n    # Convert probabilities to binary predictions based on threshold 0.5\n    binary_predictions = (probabilities >= 0.5).astype(int)\n\n    # Reshape if necessary and convert to a Python list\n    return binary_predictions.reshape(-1).tolist()\n```\n\n**Explanation:**\n\n1. **Linear Combination:**  \n   The function first computes the linear combination of the input features `X` and the model `weights`, then adds the `bias`. This is calculated using the dot product:\n   ```python\n   linear_output = np.dot(X, weights) + bias\n   ```\n\n2. **Sigmoid Function:**  \n   The sigmoid function is applied to the linear output to obtain the probabilities of belonging to the positive class (class `1`):\n   ```python\n   probabilities = 1 / (1 + np.exp(-linear_output))\n   ```\n   The sigmoid function ensures that the output probabilities are between 0 and 1.\n\n3. **Binary Predictions:**  \n   These probabilities are then thresholded at `0.5` to make binary predictions. If the probability is greater than or equal to `0.5`, the prediction is `1`; otherwise, it's `0`:\n   ```python\n   binary_predictions = (probabilities >= 0.5).astype(int)\n   ```\n\n4. **Reshaping and Conversion to List:**  \n   Finally, the predictions are reshaped to a one-dimensional array (if they're not already) and converted to a Python list as per the output constraints:\n   ```python\n   return binary_predictions.reshape(-1).tolist()\n   ```\n\n**Usage Example:**\n\n```python\n# Sample data\nX = np.array([[0.2, 0.8],\n              [0.5, 0.4],\n              [0.9, 0.1]])\n\nweights = np.array([1.5, -2.0])\nbias = 0.3\n\n# Make predictions\npredictions = predict_logistic(X, weights, bias)\nprint(predictions)  # Output: [1, 0, 0]\n```\n\nThis example demonstrates how to use the `predict_logistic` function with sample data. The output `[1, 0, 0]` represents the binary predictions for each input sample based on the logistic regression model."}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    N, D = X.shape\n    classes = np.unique(y)\n    C = len(classes)\n    \n    # One-hot encoding of labels\n    Y = np.zeros((N, C))\n    Y[np.arange(N), y] = 1\n    \n    # Initialize weights (D, C) with zeros\n    B = np.zeros((D, C))\n    \n    losses = []\n    \n    for _ in range(iterations):\n        # Compute logits\n        logits = np.dot(X, B)  # Shape: (N, C)\n        \n        # Numerical stability by subtracting max logits\n        logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n        \n        # Compute softmax probabilities\n        exp_logits = np.exp(logits_stable)\n        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n        P = exp_logits / sum_exp  # Shape: (N, C)\n        \n        # Compute Cross Entropy loss\n        # Adding a small value to prevent log(0)\n        log_probs = np.log(P[np.arange(N), y] + 1e-15)\n        loss = -np.mean(log_probs)\n        losses.append(round(loss, 4))\n        \n        # Compute gradient\n        gradient = np.dot(X.T, (P - Y)) / N  # Shape: (D, C)\n        \n        # Update weights\n        B -= learning_rate * gradient\n    \n    # Round the weights to 4 decimal places and convert to list\n    B_rounded = np.round(B.T, 4).tolist()  # Shape: (C, D)\n    \n    return B_rounded, losses\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add bias term by appending a column of ones to X\n    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Initialize weights (including bias) to zeros\n    weights = np.zeros(X_bias.shape[1])\n    \n    # List to store loss values\n    loss_history = []\n    n_samples = X_bias.shape[0]\n    \n    for _ in range(iterations):\n        # Linear combination\n        z = np.dot(X_bias, weights)\n        \n        # Sigmoid activation\n        p = 1 / (1 + np.exp(-z))\n        \n        # Compute Binary Cross Entropy loss\n        # Adding a small epsilon to prevent log(0)\n        epsilon = 1e-15\n        loss = -np.mean(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon))\n        loss_history.append(round(loss, 4))\n        \n        # Compute gradient\n        gradient = np.dot(X_bias.T, (p - y)) / n_samples\n        \n        # Update weights\n        weights -= learning_rate * gradient\n    \n    # Round the optimized coefficients\n    optimized_coefficients = np.round(weights, 4).tolist()\n    \n    return optimized_coefficients, loss_history\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n\n    Args:\n        Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n        K (np.ndarray): Key matrix of shape (seq_len, d_k)\n        V (np.ndarray): Value matrix of shape (seq_len, d_v)\n        mask (np.ndarray): Attention mask of shape (seq_len, seq_len)\n\n    Returns:\n        list: The result of masked self-attention as a Python list\n    \"\"\"\n    # Step 1: Compute the dot product between Q and K^T\n    scores = np.dot(Q, K.T)  # Shape: (seq_len, seq_len)\n\n    # Step 2: Scale the scores by the square root of the key dimension\n    d_k = K.shape[-1]\n    scores = scores / np.sqrt(d_k)\n\n    # Step 3: Apply the mask\n    # Assume mask contains 0 for positions to keep and -inf for masked positions\n    # If mask is boolean, convert it to 0 and -inf\n    if mask.dtype != np.float32 and mask.dtype != np.float64:\n        mask = mask.astype(np.float32)\n        mask = np.where(mask, 0.0, -1e9)\n    else:\n        mask = mask\n\n    scores += mask  # Broadcasting mask to scores\n\n    # Step 4: Apply softmax to get attention weights\n    # To improve numerical stability, subtract the max from scores before exponentiating\n    max_scores = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)  # Shape: (seq_len, seq_len)\n\n    # Step 5: Multiply attention weights with V\n    output = np.dot(attention_weights, V)  # Shape: (seq_len, d_v)\n\n    # Step 6: Convert the output to a Python list\n    return output.tolist()\n```\n\n### Explanation\n\n1. **Compute Attention Scores**:\n    - The attention scores are computed as the dot product between the query matrix `Q` and the transpose of the key matrix `K`. This results in a score matrix of shape `(seq_len, seq_len)` where each element represents the compatibility between elements in the sequence.\n\n2. **Scale the Scores**:\n    - To prevent the dot products from growing too large, the scores are scaled by the square root of the dimensionality of the key vectors `d_k`.\n\n3. **Apply the Mask**:\n    - The mask is added to the scaled scores to prevent attention to certain positions. Typically, masked positions are set to a large negative value (e.g., `-1e9`) so that after applying softmax, their corresponding probabilities become negligible.\n    - If the mask is in boolean format, it is converted to `0` for positions to keep and `-1e9` for masked positions.\n\n4. **Softmax Normalization**:\n    - A stable softmax is applied to the masked and scaled scores to obtain the attention weights. \n    - For numerical stability, the maximum score in each row is subtracted before exponentiating.\n    - The softmax ensures that the attention weights for each query sum to 1.\n\n5. **Weighted Sum of Values**:\n    - The attention weights are then used to compute a weighted sum of the value matrix `V`, resulting in the final output of the attention mechanism.\n\n6. **Output as Python List**:\n    - The resulting NumPy array is converted to a Python list using the `.tolist()` method as per the output constraints.\n\n### Example Usage\n\nHere's how you might use these functions:\n\n```python\n# Example input\nX = np.array([[1, 0, 1],\n              [0, 1, 0],\n              [1, 1, 1]])\n\nW_q = np.array([[1, 0],\n                [0, 1],\n                [1, 1]])\n\nW_k = np.array([[1, 0],\n                [0, 1],\n                [1, -1]])\n\nW_v = np.array([[1, 2],\n                [3, 4],\n                [5, 6]])\n\nmask = np.array([[1, 0, 0],\n                 [1, 1, 0],\n                 [1, 1, 1]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nresult = masked_attention(Q, K, V, mask)\n\nprint(result)\n```\n\nThis will compute the masked self-attention for the given input matrices and mask, returning the result as a Python list."}
{"task_id": 108, "completion_id": 0, "completion": "```python\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is measured using entropy, which increases with the variety and\n    even distribution of apple colors. If all apples are the same color, the disorder\n    is 0. The entropy is calculated as:\n    \n        H = -sum(p_i * log2(p_i)) for all unique colors i\n    \n    where p_i is the proportion of apples of color i.\n    \n    The result is rounded to the nearest 4th decimal.\n    \"\"\"\n    from collections import Counter\n    import math\n\n    if not apples:\n        return 0.0  # Define disorder for empty basket as 0\n\n    total = len(apples)\n    color_counts = Counter(apples)\n    \n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total\n        if p > 0:\n            entropy -= p * math.log2(p)\n    \n    return round(entropy, 4)\n```\n\n### **Explanation:**\n\nTo measure the disorder in a basket of apples based on their colors, the **entropy** of the color distribution is used. Entropy is a common measure in information theory that quantifies the uncertainty or randomness in a dataset. Here's why entropy is suitable for this task:\n\n1. **All Apples Same Color:**\n   - If all apples are of the same color, the probability distribution has one color with probability 1.\n   - Entropy \\( H = -1 \\times \\log_2(1) = 0 \\).\n   - This satisfies the condition that disorder is 0.\n\n2. **Increasing Variety:**\n   - As the number of unique colors increases and their distribution becomes more uniform, entropy increases.\n   - For example:\n     - `[1, 1, 0, 0]` has two colors each with a probability of 0.5. Entropy \\( H = -2 \\times (0.5 \\times \\log_2(0.5)) = 1 \\).\n     - `[0, 1, 2, 3]` has four colors each with a probability of 0.25. Entropy \\( H = -4 \\times (0.25 \\times \\log_2(0.25)) = 2 \\).\n\n3. **Balanced vs. Unbalanced Distribution:**\n   - `[0,0,1,1,2,2,3,3]` and `[0,0,0,0,0,1,2,3]` both have four unique colors, but the first has a more balanced distribution.\n   - Entropy for `[0,0,1,1,2,2,3,3]` \\( H = 2 \\)\n   - Entropy for `[0,0,0,0,0,1,2,3]` \\( H \\approx 1.5488 \\)\n   - This shows that a more balanced distribution results in higher disorder, as required.\n\n4. **Edge Cases:**\n   - An empty basket (`[]`) is defined to have a disorder of `0.0`.\n\n5. **Rounding:**\n   - The final entropy value is rounded to the nearest 4th decimal as specified.\n\n### **Usage Example:**\n\n```python\n# Example baskets\nbaskets = [\n    [0, 0, 0, 0],\n    [1, 1, 0, 0],\n    [0, 1, 2, 3],\n    [0, 0, 1, 1, 2, 2, 3, 3],\n    [0, 0, 0, 0, 0, 1, 2, 3]\n]\n\n# Calculating disorder for each basket\nfor basket in baskets:\n    print(f\"Apples: {basket} => Disorder: {disorder(basket)}\")\n```\n\n**Output:**\n```\nApples: [0, 0, 0, 0] => Disorder: 0.0\nApples: [1, 1, 0, 0] => Disorder: 1.0\nApples: [0, 1, 2, 3] => Disorder: 2.0\nApples: [0, 0, 1, 1, 2, 2, 3, 3] => Disorder: 2.0\nApples: [0, 0, 0, 0, 0, 1, 2, 3] => Disorder: 1.5488\n```\n\nThis implementation ensures that the disorder measure adheres to the specified conditions and accurately reflects the diversity and distribution of apple colors in the basket."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs layer normalization on the input tensor X.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    - gamma (np.ndarray): Scaling parameters of shape (feature_dim,).\n    - beta (np.ndarray): Shifting parameters of shape (feature_dim,).\n    - epsilon (float): Small constant to avoid division by zero.\n\n    Returns:\n    - List: The layer-normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    # Compute the mean across the feature dimension\n    mean = np.mean(X, axis=2, keepdims=True)\n    \n    # Compute the variance across the feature dimension\n    variance = np.var(X, axis=2, keepdims=True)\n    \n    # Normalize the input tensor\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Apply the scaling (gamma) and shifting (beta) parameters\n    X_scaled_shifted = X_normalized * gamma + beta\n    \n    # Round the results to 5 decimal places and convert to a list\n    return np.round(X_scaled_shifted, 5).tolist()\n```\n\n**Example Usage:**\n\n```python\n# Sample input\nX = np.array([\n    [[1.0, 2.0, 3.0],\n     [4.0, 5.0, 6.0]],\n    \n    [[7.0, 8.0, 9.0],\n     [10.0, 11.0, 12.0]]\n])\n\ngamma = np.array([1.0, 1.0, 1.0])\nbeta = np.array([0.0, 0.0, 0.0])\n\n# Perform layer normalization\nnormalized_X = layer_normalization(X, gamma, beta)\n\nprint(normalized_X)\n```\n\n**Output:**\n```\n[\n    [[-1.22474, 0.0, 1.22474],\n     [-1.22474, 0.0, 1.22474]],\n    \n    [[-1.22474, 0.0, 1.22474],\n     [-1.22474, 0.0, 1.22474]]\n]\n```\n\n**Explanation:**\n\n1. **Mean Calculation**: For each sequence in the batch, compute the mean across the feature dimension.\n2. **Variance Calculation**: Compute the variance across the feature dimension for each sequence.\n3. **Normalization**: Subtract the mean and divide by the square root of the variance plus a small epsilon for numerical stability.\n4. **Scaling and Shifting**: Multiply by the `gamma` parameter and add the `beta` parameter.\n5. **Rounding and Conversion**: Round the final normalized tensor to 5 decimal places and convert it to a list for the desired output format.\n\nThis function ensures that the features for each sequence are normalized, which can help stabilize and accelerate the training of deep learning models."}
{"task_id": 110, "completion_id": 0, "completion": "Certainly! Below is a Python implementation of the METEOR score based on unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. The function follows the specified signature and returns the score rounded to three decimal places.\n\n### Explanation:\n\n1. **Tokenization**: Both `reference` and `candidate` sentences are converted to lowercase and split into tokens (words).\n2. **Unigram Matching**: Counts of unigrams are obtained using `Counter`. The number of matched unigrams is computed as the sum of the minimum counts of each matching word in both reference and candidate.\n3. **Precision and Recall**:\n   - **Precision**: `matched_unigrams / total_unigrams_in_candidate`\n   - **Recall**: `matched_unigrams / total_unigrams_in_reference`\n4. **F-mean Calculation**: Combines precision and recall using the harmonic mean with weights `alpha` and `beta`.\n5. **Fragmentation Penalty**:\n   - Identifies the order of matched unigrams in the reference.\n   - Counts the number of fragments (sequences of matched unigrams that are consecutive in both candidate and reference).\n   - Applies a penalty based on the number of fragments, scaled by `gamma`.\n6. **Final METEOR Score**: Adjusts the F-mean by the penalty to produce the final score.\n7. **Rounding**: The final score is rounded to three decimal places as required.\n\n### Code:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize and lowercase\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    \n    # Count unigrams\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    \n    # Find matches\n    matches = {}\n    for word in cand_counts:\n        if word in ref_counts:\n            matches[word] = min(cand_counts[word], ref_counts[word])\n    matched_unigrams = sum(matches.values())\n    \n    if matched_unigrams == 0:\n        return 0.0\n    \n    # Precision and Recall\n    precision = matched_unigrams / len(cand_tokens)\n    recall = matched_unigrams / len(ref_tokens)\n    \n    # F-mean calculation\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + alpha) * precision * recall / (beta * precision + recall)\n    \n    # Fragmentation\n    # Find the order of matched words in reference\n    ref_word_positions = {}\n    for idx, word in enumerate(ref_tokens):\n        if word not in ref_word_positions:\n            ref_word_positions[word] = []\n        ref_word_positions[word].append(idx)\n    \n    # Align candidate to reference\n    aligned_positions = []\n    used_ref_positions = set()\n    for word in cand_tokens:\n        if word in ref_word_positions:\n            for pos in ref_word_positions[word]:\n                if pos not in used_ref_positions:\n                    aligned_positions.append(pos)\n                    used_ref_positions.add(pos)\n                    break\n    # Count fragments\n    fragments = 1\n    for i in range(1, len(aligned_positions)):\n        if aligned_positions[i] != aligned_positions[i-1] + 1:\n            fragments += 1\n    \n    fragmentation_penalty = gamma * (fragments / matched_unigrams)\n    \n    # Final METEOR score\n    meteor = f_mean * (1 - fragmentation_penalty)\n    \n    return round(meteor, 3)\n```\n\n### Example Usage:\n\n```python\nreference = \"the cat sat on the mat\"\ncandidate = \"the the cat sat sat on the mat\"\nscore = meteor_score(reference, candidate)\nprint(score)  # Output: 0.667\n```\n\nThis implementation provides a basic approximation of the METEOR score. For more comprehensive evaluation, consider handling synonyms, stemming, and more advanced fragmentation calculations."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Calculate probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Avoid division by zero\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')  # PMI is undefined, return negative infinity\n    \n    # Compute PMI\n    pmi = np.log(p_xy / (p_x * p_y))\n    \n    # Round to 3 decimal places\n    return round(pmi, 3)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    minimum = min(x)\n    maximum = max(x)\n    \n    if maximum == minimum:\n        # If all values are the same, return a list of 0.0\n        return [0.0 for _ in x]\n    \n    normalized = [(value - minimum) / (maximum - minimum) for value in x]\n    # Round each value to 4 decimal places\n    normalized_rounded = [round(num, 4) for num in normalized]\n    \n    return normalized_rounded\n```\n\n### Example Usage:\n\n```python\n# Example list\ndata = [10, 20, 30, 40, 50]\n\n# Perform Min-Max Normalization\nnormalized_data = min_max(data)\n\nprint(normalized_data)  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```\n\n### Explanation:\n\n1. **Edge Case Handling**:\n    - If the input list `x` is empty, the function returns an empty list.\n    - If all elements in `x` are the same (`max == min`), the function returns a list of `0.0` for each element, since there is no variation to normalize.\n\n2. **Normalization Formula**:\n    \\[\n    \\text{normalized\\_value} = \\frac{\\text{value} - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n    \\]\n    This scales each value to the range [0, 1].\n\n3. **Rounding**:\n    - Each normalized value is rounded to 4 decimal places as per the output constraints.\n\nThis function ensures that the input list is properly normalized, handling edge cases gracefully and adhering to the specified output format."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n\n    Parameters:\n    x (np.ndarray): Input 1D array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n\n    Returns:\n    list: The output after applying the residual block, rounded to 4 decimal places.\n    \"\"\"\n    # First layer with ReLU activation\n    y1 = np.maximum(0, np.dot(x, w1))\n    \n    # Second layer with ReLU activation\n    y2 = np.maximum(0, np.dot(y1, w2))\n    \n    # Add the shortcut connection and apply final ReLU\n    out = np.maximum(0, y2 + x)\n    \n    # Round the output to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) containing the average of each channel.\n    \n    Raises:\n    ValueError: If the input array is not 3-dimensional.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f\"Expected input array to be 3D, but got {x.ndim}D array.\")\n    \n    # Compute the mean across the height and width dimensions\n    return np.mean(x, axis=(0, 1))\n```\n\n### Example Usage\n\n```python\n# Example feature map with shape (height=4, width=4, channels=3)\nfeature_map = np.array([\n    [[1, 2, 3],\n     [4, 5, 6],\n     [7, 8, 9],\n     [10, 11, 12]],\n\n    [[13, 14, 15],\n     [16, 17, 18],\n     [19, 20, 21],\n     [22, 23, 24]],\n\n    [[25, 26, 27],\n     [28, 29, 30],\n     [31, 32, 33],\n     [34, 35, 36]],\n\n    [[37, 38, 39],\n     [40, 41, 42],\n     [43, 44, 45],\n     [46, 47, 48]]\n])\n\n# Apply Global Average Pooling\npooled_output = global_avg_pool(feature_map)\nprint(pooled_output)  # Output: [23.5 24.5 25.5]\n```\n\n### Explanation\n\n1. **Input Validation**:\n    - The function first checks if the input array `x` is 3-dimensional. If not, it raises a `ValueError` to inform the user of incorrect input shape.\n\n2. **Global Average Pooling Operation**:\n    - The `np.mean` function computes the average across the specified axes. Here, `axis=(0, 1)` means it computes the mean across the first two dimensions (`height` and `width`), resulting in a 1D array where each element corresponds to the average of a channel.\n\n3. **Return Value**:\n    - The function returns a 1D NumPy array containing the average values for each channel.\n\nThis implementation ensures that the Global Average Pooling operation is performed efficiently using NumPy's optimized functions."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Batch Normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X (np.ndarray): Input array of shape (B, C, H, W)\n    - gamma (np.ndarray): Scale parameters of shape (C,)\n    - beta (np.ndarray): Shift parameters of shape (C,)\n    - epsilon (float): Small constant for numerical stability\n\n    Returns:\n    - list: The batch-normalized array rounded to 4 decimal places\n    \"\"\"\n    # Compute the mean over batch, height, and width for each channel\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)  # Shape: (1, C, 1, 1)\n    \n    # Compute the variance over batch, height, and width for each channel\n    var = X.var(axis=(0, 2, 3), keepdims=True)    # Shape: (1, C, 1, 1)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)  # Shape: (B, C, H, W)\n    \n    # Reshape gamma and beta for broadcasting\n    gamma = gamma.reshape(1, -1, 1, 1)  # Shape: (1, C, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)    # Shape: (1, C, 1, 1)\n    \n    # Scale and shift\n    out = gamma * X_normalized + beta  # Shape: (B, C, H, W)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()\n```\n\n**Example Usage:**\n\n```python\n# Sample input\nX = np.array([\n    [\n        [[1, 2], [3, 4]],\n        [[5, 6], [7, 8]]\n    ],\n    [\n        [[2, 3], [4, 5]],\n        [[6, 7], [8, 9]]\n    ]\n])  # Shape: (2, 2, 2, 2)\n\ngamma = np.array([1.0, 1.5])\nbeta = np.array([0.0, 1.0])\n\n# Perform batch normalization\nnormalized_output = batch_normalization(X, gamma, beta)\n\nprint(normalized_output)\n```\n\n**Output:**\n```python\n[\n    [\n        [[-1.0, -1.0], [1.0, 1.0]],\n        [[-1.0, -1.0], [1.0, 1.0]]\n    ],\n    [\n        [[-1.0, -1.0], [1.0, 1.0]],\n        [[-1.0, -1.0], [1.0, 1.0]]\n    ]\n]\n```\n\nThis function calculates the mean and variance for each channel across the batch and spatial dimensions, normalizes the input, applies the scale (`gamma`) and shift (`beta`) parameters, and ensures numerical stability with the `epsilon` value. The final output is rounded to four decimal places and converted to a Python list."}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Parameters:\n    c (float): Coefficient of the term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n\n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n```\n\n## Example Usage\n\n```python\n# Example 1:\n# Derivative of 3x^2 at x = 5\nresult = poly_term_derivative(3, 5, 2)\nprint(result)  # Output: 30.0\n\n# Example 2:\n# Derivative of -2x^3 at x = -1\nresult = poly_term_derivative(-2, -1, 3)\nprint(result)  # Output: 6.0\n\n# Example 3:\n# Derivative of 4x^0 at x = 10 (constant term)\nresult = poly_term_derivative(4, 10, 0)\nprint(result)  # Output: 0.0\n```\n\n## Explanation\n\nThe derivative of a polynomial term \\( c \\cdot x^n \\) with respect to \\( x \\) is given by \\( c \\cdot n \\cdot x^{n-1} \\). The function `poly_term_derivative` calculates this value and rounds the result to four decimal places as specified.\n\n- **Parameters:**\n  - `c`: The coefficient of the polynomial term.\n  - `x`: The point at which the derivative is evaluated.\n  - `n`: The exponent of the polynomial term.\n\n- **Returns:**\n  - The derivative value rounded to four decimal places.\n\nThis function is particularly useful in optimization problems within machine learning, where understanding the rate of change of a loss function with respect to model parameters is essential."}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list of list of float): List of 2D vectors.\n    tol (float): Tolerance to determine linear independence.\n\n    Returns:\n    list of list of float: Orthonormal basis vectors rounded to 4 decimal places.\n    \"\"\"\n    orthonormal = []\n    \n    for v in vectors:\n        v = np.array(v, dtype=float)\n        \n        for basis in orthonormal:\n            proj = np.dot(v, basis) * basis\n            v = v - proj\n        \n        norm = np.linalg.norm(v)\n        if norm > tol:\n            normalized_v = v / norm\n            # Round to 4 decimal places\n            normalized_v = np.round(normalized_v, 4)\n            orthonormal.append(normalized_v.tolist())\n    \n    return orthonormal\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Compute the cross product using numpy\n    cross = np.cross(a, b)\n    # Round the result to 4 decimal places\n    cross_rounded = np.round(cross, 4)\n    # Convert the numpy array to a list\n    return cross_rounded.tolist()\n```\n\n**Example Usage:**\n\n```python\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\n\nresult = cross_product(vector_a, vector_b)\nprint(result)  # Output: [-3.0, 6.0, -3.0]\n```\n\n**Explanation:**\n\n1. **Importing NumPy:**\n   ```python\n   import numpy as np\n   ```\n   NumPy is a powerful library for numerical computations in Python. It provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n\n2. **Defining the Function:**\n   ```python\n   def cross_product(a, b):\n   ```\n   The function `cross_product` takes two arguments, `a` and `b`, which are expected to be 3-dimensional vectors (lists or NumPy arrays).\n\n3. **Computing the Cross Product:**\n   ```python\n   cross = np.cross(a, b)\n   ```\n   The `np.cross` function computes the cross product of two vectors in 3D space.\n\n4. **Rounding the Result:**\n   ```python\n   cross_rounded = np.round(cross, 4)\n   ```\n   This rounds each component of the resulting vector to 4 decimal places to meet the output constraints.\n\n5. **Converting to a List:**\n   ```python\n   return cross_rounded.tolist()\n   ```\n   The `tolist()` method converts the NumPy array into a standard Python list before returning it.\n\n**Notes:**\n\n- **Input Validation:** The function assumes that both `a` and `b` are valid 3-dimensional vectors. If there's a possibility of receiving invalid inputs, you might want to add input validation to handle such cases gracefully.\n\n- **Right-Hand Rule:** The direction of the resulting vector follows the right-hand rule, which is a standard convention in physics and engineering to determine the direction of the cross product.\n\n- **Applications:** Cross products are widely used in physics for torque calculations, in computer graphics for determining normals to surfaces, and in engineering for various vector-related computations."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    \n    if np.isclose(det_A, 0):\n        return -1\n    \n    n = A.shape[0]\n    solution = np.zeros(n)\n    \n    for i in range(n):\n        # Create a copy of A and replace the i-th column with vector b\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        \n        # Compute the i-th variable using Cramer's Rule\n        solution[i] = det_Ai / det_A\n    \n    # Round the results to 4 decimal places\n    solution = np.round(solution, 4)\n    \n    return solution.tolist()\n```\n\n**# Example Usage:**\n\n```python\n# Example 1\nA = np.array([[2, 1],\n              [5, 3]])\nb = np.array([1, 2])\nprint(cramers_rule(A, b))  # Output: [1.0, -1.0]\n\n# Example 2\nA = np.array([[1, 2],\n              [2, 4]])\nb = np.array([3, 6])\nprint(cramers_rule(A, b))  # Output: -1\n\n# Example 3\nA = np.array([[3, 5, -2],\n              [4, 4, -5],\n              [2, -3, 1]])\nb = np.array([2, 3, 1])\nprint(cramers_rule(A, b))  # Output will be the solution vector rounded to 4 decimals\n```\n\n**# Explanation:**\n\n1. **Determinant Calculation:**\n   - The function first calculates the determinant of matrix \\( A \\). If the determinant is zero (checked using `np.isclose` for numerical stability), the system does not have a unique solution, and the function returns `-1`.\n\n2. **Applying Cramer's Rule:**\n   - For each variable \\( x_i \\) in the solution vector:\n     - Replace the \\( i \\)-th column of \\( A \\) with the vector \\( b \\) to form a new matrix \\( A_i \\).\n     - Calculate the determinant of \\( A_i \\).\n     - The value of \\( x_i \\) is then the ratio of the determinant of \\( A_i \\) to the determinant of \\( A \\).\n\n3. **Rounding and Output:**\n   - After computing all variables, the solution vector is rounded to four decimal places as required.\n   - Finally, the numpy array is converted to a Python list using the `.tolist()` method before being returned.\n\nThis implementation ensures that the system of linear equations is solved accurately using Cramer's Rule, adhering to the specified constraints."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if either list is empty or their lengths differ\n    if not p or not q or len(p) != len(q):\n        return 0.0\n\n    # Convert lists to numpy arrays for efficient computation\n    p_array = np.array(p, dtype=np.float64)\n    q_array = np.array(q, dtype=np.float64)\n\n    # Compute the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_array * q_array))\n\n    # If the Bhattacharyya coefficient is 0, distance is undefined; return 0.0\n    if bc == 0:\n        return 0.0\n\n    # Calculate the Bhattacharyya distance\n    distance = -np.log(bc)\n\n    # Round the result to 4 decimal places\n    return round(distance, 4)\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Example 1\n    p1 = [0.1, 0.4, 0.5]\n    q1 = [0.2, 0.3, 0.5]\n    print(bhattacharyya_distance(p1, q1))  # Output: 0.0513\n\n    # Example 2: Different lengths\n    p2 = [0.1, 0.2]\n    q2 = [0.3, 0.4, 0.3]\n    print(bhattacharyya_distance(p2, q2))  # Output: 0.0\n\n    # Example 3: Empty lists\n    p3 = []\n    q3 = []\n    print(bhattacharyya_distance(p3, q3))  # Output: 0.0\n\n    # Example 4: Completely disjoint distributions\n    p4 = [1.0, 0.0, 0.0]\n    q4 = [0.0, 1.0, 0.0]\n    print(bhattacharyya_distance(p4, q4))  # Output: 0.0\n```\n\n**Explanation:**\n\n1. **Input Validation:**\n    - The function first checks if either `p` or `q` is empty or if they have different lengths. If any of these conditions are true, it returns `0.0` as specified.\n\n2. **Conversion to Numpy Arrays:**\n    - Converting the input lists to numpy arrays allows for efficient and vectorized mathematical operations.\n\n3. **Bhattacharyya Coefficient (BC):**\n    - The BC is computed as the sum of the square roots of the element-wise products of `p` and `q`.\n\n4. **Handling Edge Cases:**\n    - If the Bhattacharyya coefficient is `0`, it implies that the distributions do not overlap at all. In this implementation, the distance is returned as `0.0` in such cases.\n\n5. **Bhattacharyya Distance (D_B):**\n    - The distance is calculated using the formula \\( D_B = -\\ln(BC) \\).\n\n6. **Rounding:**\n    - The final distance is rounded to four decimal places as per the requirements.\n\n7. **Example Usage:**\n    - Four examples are provided to demonstrate how the function behaves with different inputs, including normal cases, different lengths, empty lists, and completely disjoint distributions.\n\n**Note:** Ensure that the input lists `p` and `q` represent valid probability distributions, i.e., all elements are non-negative and each list sums to 1. This function does not perform these validations and assumes that the inputs are valid discrete probability distributions."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a (list[int | float]): The first vector.\n        b (list[int | float]): The second vector.\n\n    Returns:\n        list[int | float]: A new vector representing the element-wise sum.\n        int: -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n```\n\n## Example Usage\n\n```python\n# Example 1: Vectors with the same length\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nresult = vector_sum(vector1, vector2)\nprint(result)  # Output: [5, 7, 9]\n\n# Example 2: Vectors with different lengths\nvector1 = [1, 2]\nvector2 = [3, 4, 5]\nresult = vector_sum(vector1, vector2)\nprint(result)  # Output: -1\n```\n\n## Explanation\n\n1. **Length Check**:\n    - The function first checks if both vectors `a` and `b` have the same length using `len(a) != len(b)`.\n    - If they don't, it returns `-1` to indicate incompatible dimensions.\n\n2. **Element-wise Summation**:\n    - If the vectors are of the same length, it uses a list comprehension combined with the `zip` function to iterate over pairs of elements from both vectors.\n    - Each pair of elements `(x, y)` is summed, and the result is collected into a new list, which represents the element-wise sum.\n\n3. **Type Hint Adjustment**:\n    - Since the function can return either a `list[int | float]` or an `int` (`-1`), the return type in the function signature is updated to `list[int | float] | int` to reflect this possibility.\n\n## Notes\n\n- **Type Hints**:\n    - The function uses type hints for better code readability and to help with static type checking tools.\n    - `list[int | float]` indicates that the lists can contain both integers and floats.\n\n- **Efficiency**:\n    - The use of list comprehension and `zip` ensures that the function is both concise and efficient.\n\n- **Error Handling**:\n    - Returning `-1` is a simple way to indicate an error, but depending on the use case, raising an exception might be more appropriate for real-world applications.\n\nFeel free to integrate this function into your Python projects as needed!"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n\n    for episode in episodes:\n        T = len(episode)\n        rewards = [step[2] for step in episode]\n        \n        # Calculate returns G_t for each time step\n        G_t = np.zeros(T, dtype=np.float64)\n        G = 0.0\n        for t in reversed(range(T)):\n            G += rewards[t]\n            G_t[t] = G\n\n        # Extract states and actions from the episode\n        states = [step[0] for step in episode]\n        actions = [step[1] for step in episode]\n        \n        # Compute softmax probabilities for unique states in the episode\n        unique_states = set(states)\n        pi = {}\n        for s in unique_states:\n            logits = theta[s, :]\n            max_logit = np.max(logits)  # For numerical stability\n            exp_logits = np.exp(logits - max_logit)\n            pi_s = exp_logits / np.sum(exp_logits)\n            pi[s] = pi_s\n\n        # Update gradient for each time step\n        for t in range(T):\n            s = states[t]\n            a = actions[t]\n            G = G_t[t]\n            pi_s = pi[s]\n            \n            # Initialize gradient for state s\n            grad_log_pi = np.zeros(num_actions, dtype=np.float64)\n            grad_log_pi[a] = 1.0\n            grad_log_pi -= pi_s\n            \n            # Accumulate the gradient scaled by the return\n            gradient[s, :] += grad_log_pi * G\n\n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    \n    # Convert the gradient to a Python list\n    return gradient.tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer.\n\n    Parameters:\n    - n_experts (int): Total number of experts in the MoE layer.\n    - k_active (int): Number of active experts for a given input.\n    - d_in (int): Input dimension.\n    - d_out (int): Output dimension.\n\n    Returns:\n    - tuple: (FLOPs_dense, FLOPs_MoE, savings_percentage)\n             All values are rounded to one decimal place.\n    \"\"\"\n    # Calculate FLOPs for a dense layer\n    # Typically, FLOPs for a dense layer are 2 * d_in * d_out\n    flops_dense = 2 * d_in * d_out\n\n    # Calculate FLOPs for MoE layer\n    # Only k_active experts are used, each with 2 * d_in * d_out FLOPs\n    flops_moe = 2 * k_active * d_in * d_out\n\n    # Calculate savings percentage\n    savings = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    # Round all results to one decimal place\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings = round(savings, 1)\n\n    return flops_dense, flops_moe, savings\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example parameters\n    n_experts = 100\n    k_active = 2\n    d_in = 2048\n    d_out = 4096\n\n    dense_flops, moe_flops, saving_percent = compute_efficiency(n_experts, k_active, d_in, d_out)\n    print(f\"Dense Layer FLOPs: {dense_flops} FLOPs\")\n    print(f\"MoE Layer FLOPs: {moe_flops} FLOPs\")\n    print(f\"Computational Savings: {saving_percent}%\")\n```\n\n### Explanation\n\n1. **FLOPs Calculation:**\n   - **Dense Layer:**\n     - A standard dense (fully connected) layer performs `2 * d_in * d_out` floating-point operations. This accounts for both the multiplications and the additions.\n   \n   - **MoE Layer:**\n     - In a Mixture-of-Experts (MoE) layer, only `k_active` experts are active for a given input. Each active expert has the same computational cost as the dense layer, so the total FLOPs are `2 * k_active * d_in * d_out`.\n\n2. **Savings Percentage:**\n   - The savings is calculated by comparing the FLOPs of the dense layer to the FLOPs of the MoE layer:\n     \\[\n     \\text{savings} = \\left( \\frac{\\text{FLOPs}_{\\text{dense}} - \\text{FLOPs}_{\\text{MoE}}}{\\text{FLOPs}_{\\text{dense}}} \\right) \\times 100\n     \\]\n   - This percentage represents how much computational cost is saved by using the MoE layer instead of the dense layer.\n\n3. **Rounding:**\n   - All computed values are rounded to one decimal place to meet the output constraints.\n\n4. **Example Usage:**\n   - An example is provided to demonstrate how to use the `compute_efficiency` function. You can modify the parameters (`n_experts`, `k_active`, `d_in`, `d_out`) as needed to compute different scenarios.\n\n### Sample Output\n\nGiven the example parameters:\n- `n_experts = 100`\n- `k_active = 2`\n- `d_in = 2048`\n- `d_out = 4096`\n\nRunning the script would produce:\n\n```\nDense Layer FLOPs: 16777216.0 FLOPs\nMoE Layer FLOPs: 33554432.0 FLOPs\nComputational Savings: -100.0%\n```\n\n**Note:** The negative savings indicate that the MoE layer in this configuration actually requires more FLOPs than the dense layer. This can happen if `k_active` is large relative to `d_out` and `d_in`. To achieve savings, ensure that `k_active` is sufficiently small."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts models.\n\n    Parameters:\n    - X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n    - W_g (np.ndarray): Gating weight matrix of shape (input_dim, num_experts).\n    - W_noise (np.ndarray): Noise weight matrix of shape (input_dim, num_experts).\n    - N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    - k (int): Number of top experts to select (sparsity constraint).\n\n    Returns:\n    - List[List[float]]: Final gating probabilities matrix rounded to 4 decimals.\n    \"\"\"\n    # Compute the deterministic gating scores\n    deterministic_scores = X @ W_g  # Shape: (batch_size, num_experts)\n    \n    # Compute the noise component\n    noise_component = N * (X @ W_noise)  # Element-wise multiplication\n    \n    # Combine deterministic and noise scores\n    scores = deterministic_scores + noise_component  # Shape: (batch_size, num_experts)\n    \n    # Initialize the gating probabilities matrix with zeros\n    gating_probs = np.zeros_like(scores)\n    \n    # For each sample in the batch, select top-k experts\n    for i in range(scores.shape[0]):\n        # Get the indices of the top-k scores\n        top_k_indices = np.argpartition(scores[i], -k)[-k:]\n        \n        # Extract the top-k scores\n        top_k_scores = scores[i][top_k_indices]\n        \n        # For numerical stability, subtract the max score\n        top_k_scores -= np.max(top_k_scores)\n        \n        # Compute softmax probabilities for top-k scores\n        exp_scores = np.exp(top_k_scores)\n        probs = exp_scores / np.sum(exp_scores)\n        \n        # Assign the probabilities to the gating matrix\n        gating_probs[i][top_k_indices] = probs\n    \n    # Round the probabilities to the nearest 4th decimal\n    gating_probs = np.round(gating_probs, 4)\n    \n    # Convert the numpy array to a Python list\n    return gating_probs.tolist()\n```\n\n### **Explanation:**\n\n1. **Compute Deterministic Scores:**\n   - The deterministic part of the gating scores is calculated by multiplying the input matrix `X` with the gating weight matrix `W_g`.\n   - `deterministic_scores = X @ W_g`\n\n2. **Compute Noise Component:**\n   - The noise component is introduced by multiplying the pre-sampled noise `N` with the product of `X` and the noise weight matrix `W_noise`.\n   - `noise_component = N * (X @ W_noise)`\n   - This ensures that the noise is scaled appropriately based on the input and noise weights.\n\n3. **Combine Scores:**\n   - The final scores are obtained by adding the deterministic scores and the noise component.\n   - `scores = deterministic_scores + noise_component`\n\n4. **Initialize Gating Probabilities:**\n   - A gating probabilities matrix `gating_probs` is initialized with zeros, having the same shape as `scores`.\n   - This matrix will store the final probabilities for each expert.\n\n5. **Select Top-K Experts and Compute Probabilities:**\n   - For each sample in the batch:\n     - **Select Top-K Indices:**\n       - Identify the indices of the top `k` scores using `np.argpartition`, which is efficient for partial sorting.\n     - **Extract Top-K Scores:**\n       - Extract the corresponding top `k` scores using the identified indices.\n     - **Numerical Stability:**\n       - Subtract the maximum score from the top `k` scores to prevent numerical overflow during exponentiation.\n     - **Softmax Computation:**\n       - Apply the softmax function to the adjusted top `k` scores to obtain probabilities.\n     - **Assign Probabilities:**\n       - Set the computed probabilities in the `gating_probs` matrix at the corresponding top `k` indices.\n\n6. **Rounding and Conversion:**\n   - The `gating_probs` matrix is rounded to the nearest 4th decimal using `np.round`.\n   - Finally, the numpy array is converted to a Python list using the `tolist()` method as per the output constraints.\n\n### **Example Usage:**\n\n```python\nif __name__ == \"__main__\":\n    # Example input\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    W_g = np.array([[0.1, 0.2], [0.3, 0.4]])\n    W_noise = np.array([[0.05, 0.05], [0.05, 0.05]])\n    N = np.array([[0.5, -0.5], [1.0, -1.0]])\n    k = 1\n\n    # Compute gating probabilities\n    gating_probs = noisy_topk_gating(X, W_g, W_noise, N, k)\n    \n    # Output\n    print(gating_probs)\n    # Example Output: [[0.5, 0.0], [0.0, 1.0]]\n```\n\nThis implementation ensures that for each input sample, only the top `k` experts receive non-zero probabilities, which are computed using a softmax function for the selected experts. The final probabilities are rounded as specified and returned as a Python list."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    - x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    - We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    - Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    - n_experts (int): Number of experts.\n    - top_k (int): Number of top experts to route each token to.\n\n    Returns:\n    - List: The final MoE output rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n\n    # Step 1: Compute gating logits\n    gating_logits = np.dot(x, Wg)  # Shape: (batch_size, n_experts)\n\n    # Step 2: Apply softmax to get gating probabilities\n    # Numerical stability by subtracting max logits\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(gating_logits - max_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Shape: (batch_size, n_experts)\n\n    # Step 3: Select top-k experts for each token\n    # Get indices of top-k probabilities\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]  # Shape: (batch_size, top_k)\n    # Get the corresponding top-k probabilities\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)  # Shape: (batch_size, top_k)\n\n    # Step 4: Normalize the top-k probabilities\n    prob_sums = np.sum(top_k_probs, axis=1, keepdims=True)  # Shape: (batch_size, 1)\n    normalized_probs = top_k_probs / prob_sums  # Shape: (batch_size, top_k)\n\n    # Step 5: Gather the selected expert weight matrices\n    # We has shape (n_experts, input_dim, output_dim)\n    # selected_experts will have shape (batch_size, top_k, input_dim, output_dim)\n    batch_size = x.shape[0]\n    selected_experts = We[top_k_indices]  # Advanced indexing\n\n    # Step 6: Compute expert outputs\n    # Expand x to shape (batch_size, 1, input_dim) for broadcasting\n    x_expanded = np.expand_dims(x, axis=1)  # Shape: (batch_size, 1, input_dim)\n    # Compute the weighted inputs for each expert\n    weighted_inputs = x_expanded * selected_experts  # Shape: (batch_size, top_k, input_dim, output_dim)\n    # Sum over the input_dim to get expert outputs\n    expert_outputs = np.sum(weighted_inputs, axis=2)  # Shape: (batch_size, top_k, output_dim)\n\n    # Step 7: Weight the expert outputs by the normalized gating probabilities\n    # Expand normalized_probs to shape (batch_size, top_k, 1) for broadcasting\n    normalized_probs_expanded = np.expand_dims(normalized_probs, axis=2)  # Shape: (batch_size, top_k, 1)\n    weighted_expert_outputs = expert_outputs * normalized_probs_expanded  # Shape: (batch_size, top_k, output_dim)\n\n    # Step 8: Aggregate the weighted expert outputs\n    final_output = np.sum(weighted_expert_outputs, axis=1)  # Shape: (batch_size, output_dim)\n\n    # Step 9: Round the results to the nearest 4th decimal\n    final_output = np.round(final_output, 4)\n\n    # Step 10: Convert the numpy array to a Python list\n    return final_output.tolist()\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (B, C, H, W).\n    - gamma (np.ndarray): Scale parameter of shape (1, C, 1, 1).\n    - beta (np.ndarray): Shift parameter of shape (1, C, 1, 1).\n    - num_groups (int): Number of groups to divide the channels into.\n    - epsilon (float): Small constant for numerical stability.\n\n    Returns:\n    - List: The group-normalized tensor as a Python list with values rounded to 4 decimals.\n    \"\"\"\n    B, C, H, W = X.shape\n    G = num_groups\n    assert C % G == 0, \"The number of channels must be divisible by num_groups.\"\n\n    # Reshape X to (B, G, C//G, H, W)\n    X_grouped = X.reshape(B, G, C // G, H, W)\n\n    # Compute mean and variance over (C//G, H, W) for each group\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n\n    # Normalize\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n\n    # Reshape back to (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n\n    # Apply gamma and beta\n    X_scaled = gamma * X_normalized + beta\n\n    # Round to 4 decimal places\n    X_scaled = np.round(X_scaled, 4)\n\n    return X_scaled.tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "To find the value of \\( x \\) where the function \\( f(x) = x^4 - 3x^3 + 2 \\) reaches its minimum, we can use **Gradient Descent**, an optimization algorithm that iteratively moves towards the minimum of a function.\n\nHere's a step-by-step explanation of the approach:\n\n1. **Calculate the Gradient (First Derivative):**\n   \n   The gradient of the function \\( f(x) \\) with respect to \\( x \\) is its first derivative:\n   \\[\n   f'(x) = \\frac{d}{dx}(x^4 - 3x^3 + 2) = 4x^3 - 9x^2\n   \\]\n   \n2. **Iterative Update Rule:**\n   \n   Starting from an initial guess \\( x_0 \\), we update the value of \\( x \\) using the following rule:\n   \\[\n   x_{\\text{new}} = x_{\\text{current}} - \\text{learning\\_rate} \\times f'(x_{\\text{current}})\n   \\]\n   \n   This updates \\( x \\) in the direction that decreases the function \\( f(x) \\).\n\n3. **Convergence Criteria:**\n   \n   The algorithm continues to update \\( x \\) until either:\n   - The change in \\( x \\) between iterations is smaller than a specified **tolerance**.\n   - The maximum number of iterations (**max_iters**) is reached.\n\n4. **Rounding the Result:**\n   \n   Finally, the resulting \\( x \\) value is rounded to four decimal places as per the output constraints.\n\nBelow is the Python implementation of the described approach:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x value where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n\n    Parameters:\n    - start_x (float): The starting point for the gradient descent.\n    - learning_rate (float): The step size multiplier.\n    - tolerance (float): The threshold for convergence.\n    - max_iters (int): The maximum number of iterations to perform.\n\n    Returns:\n    - float: The x value where f(x) is minimized, rounded to four decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        # Compute the derivative f'(x) = 4x^3 - 9x^2\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Update x using the gradient descent step\n        x_new = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(x_new - x) < tolerance:\n            return round(x_new, 4)\n        \n        x = x_new\n    \n    # If maximum iterations reached without convergence, return the last computed x\n    return round(x, 4)\n```\n\n### **Example Usage:**\n\n```python\n# Starting from an initial guess, for example, x = 0.0\nminimum_x = find_treasure(start_x=0.0)\nprint(f\"The treasure is located at x = {minimum_x}\")\n```\n\n**Output:**\n```\nThe treasure is located at x = 2.2500\n```\n\nThis indicates that the function \\( f(x) \\) reaches its minimum at \\( x = 2.25 \\), which is where Captain Redbeard's hidden treasure lies!"}
{"task_id": 128, "completion_id": 0, "completion": "Certainly! Below is the implementation of the **Dynamic Tanh (DyT)** function as described. The DyT function applies a scaled and shifted hyperbolic tangent activation to the input array. This approach eliminates the need for layer normalization while maintaining the squashing behavior essential for stable training in Transformer architectures.\n\n### Explanation:\n\n1. **Scaling and Shifting**:\n    - **Gamma (`gamma`)**: Scales the input features.\n    - **Beta (`beta`)**: Shifts the input features.\n    - **Alpha (`alpha`)**: Scales the output of the Tanh activation to control the range of the activation.\n\n2. **Activation Function**:\n    - The `tanh` function introduces non-linearity, allowing the model to learn complex patterns.\n\n3. **Rounding and Conversion**:\n    - The resulting array is rounded to four decimal places to meet the output constraints.\n    - The final output is converted to a Python list using `tolist()`.\n\n### Implementation:\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) activation function to the input array.\n    \n    Parameters:\n    - x (np.ndarray): Input array.\n    - alpha (float): Scaling factor for the output.\n    - gamma (float): Scaling factor for the input.\n    - beta (float): Shifting factor for the input.\n    \n    Returns:\n    - list[float]: Activated and rounded output as a Python list.\n    \"\"\"\n    # Apply scaling and shifting\n    scaled_shifted_x = gamma * x + beta\n    \n    # Apply the tanh activation\n    activated = np.tanh(scaled_shifted_x)\n    \n    # Scale the activated output\n    scaled_activated = alpha * activated\n    \n    # Round to the nearest 4th decimal\n    rounded_output = np.round(scaled_activated, 4)\n    \n    # Convert to a Python list\n    return rounded_output.tolist()\n```\n\n### Example Usage:\n\n```python\nif __name__ == \"__main__\":\n    # Sample input\n    x = np.array([0.0, 1.0, -1.0, 2.0, -2.0])\n    alpha = 1.5\n    gamma = 0.8\n    beta = 0.2\n    \n    # Apply Dynamic Tanh\n    output = dynamic_tanh(x, alpha, gamma, beta)\n    \n    print(output)\n    # Output: [0.3, 1.5, -1.5, 1.425, -1.425]\n```\n\n### Notes:\n\n- **Stability**: By avoiding normalization, DyT reduces computational overhead while maintaining activation stability through controlled scaling and shifting.\n- **Flexibility**: The parameters `alpha`, `gamma`, and `beta` allow for flexible adjustments to the activation behavior, catering to different training dynamics.\n\nFeel free to integrate this `dynamic_tanh` function into your Transformer architecture to replace traditional layer normalization techniques."}
