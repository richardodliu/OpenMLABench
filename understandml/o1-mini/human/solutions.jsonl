{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a:\n        return -1\n    num_cols = len(a[0])\n    len_b = len(b)\n    if num_cols != len_b:\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((row[i] * b[i] for i in range(num_cols)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    Parameters:\n    - matrix (list of lists of floats): The input matrix.\n    - mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    - list of floats: A list containing the mean of each row or column.\n\n    Raises:\n    - ValueError: If the mode is not 'row' or 'column' or if the matrix is empty\n                  or if rows have inconsistent lengths when mode is 'column'.\n    \"\"\"\n    if not matrix:\n        raise ValueError('The matrix is empty.')\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    if mode == 'row':\n        means = []\n        for (idx, row) in enumerate(matrix):\n            if not row:\n                raise ValueError(f'Row {idx} is empty.')\n            row_mean = sum(row) / len(row)\n            means.append(row_mean)\n        return means\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        for (idx, row) in enumerate(matrix):\n            if len(row) != num_cols:\n                raise ValueError(f'Row {idx} does not have {num_cols} columns.')\n        column_sums = [0.0] * num_cols\n        for row in matrix:\n            for (col_idx, value) in enumerate(row):\n                column_sums[col_idx] += value\n        column_means = [total / num_rows for total in column_sums]\n        return column_means"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by a scalar.\n\n    Args:\n        matrix (list of list of int|float): The input matrix to be multiplied.\n        scalar (int|float): The scalar value to multiply each matrix element by.\n\n    Returns:\n        list of list of int|float: A new matrix with each element multiplied by the scalar.\n\n    Raises:\n        TypeError: If the input matrix is not a list of lists of numbers or if the scalar is not a number.\n    \"\"\"\n    if not isinstance(matrix, list):\n        raise TypeError('Matrix must be a list of lists.')\n    result = []\n    for row in matrix:\n        if not isinstance(row, list):\n            raise TypeError('Matrix must be a list of lists.')\n        multiplied_row = []\n        for element in row:\n            if not isinstance(element, (int, float)):\n                raise TypeError('Matrix elements must be integers or floats.')\n            multiplied_element = element * scalar\n            multiplied_row.append(multiplied_element)\n        result.append(multiplied_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import math\nfrom typing import List, Union\ndef calculate_eigenvalues(matrix: List[List[Union[float, int]]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Parameters:\n    matrix (List[List[Union[float, int]]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    List[float]: A list containing the eigenvalues sorted from highest to lowest.\n\n    Raises:\n    ValueError: If the input is not a 2x2 matrix or if the eigenvalues are complex.\n    \"\"\"\n    if not isinstance(matrix, list) or len(matrix) != 2 or (not all((isinstance(row, list) and len(row) == 2 for row in matrix))):\n        raise ValueError('Input must be a 2x2 matrix represented as a list of two lists, each containing two numbers.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    if discriminant < 0:\n        raise ValueError('The matrix has complex eigenvalues.')\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    eigenvalues = sorted([eigenvalue1, eigenvalue2], reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Transforms the matrix A using the operation T^{-1} * A * S.\n\n    Parameters:\n    - A (list of lists): The matrix to be transformed.\n    - T (list of lists): The invertible matrix to be inverted and multiplied from the left.\n    - S (list of lists): The invertible matrix to be multiplied from the right.\n\n    Returns:\n    - Transformed matrix as a list of lists with elements rounded to 4 decimal places.\n    - Returns -1 if T or S is not invertible or if matrix dimensions are incompatible.\n    \"\"\"\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        if T_np.shape[0] != A_np.shape[0] or A_np.shape[1] != S_np.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        transformed = T_inv @ A_np @ S_np\n        transformed_rounded = np.round(transformed, 4)\n        return transformed_rounded.tolist()\n    except np.linalg.LinAlgError:\n        return -1\n    except Exception:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n\n    Parameters:\n    matrix (list of list of float): A 2x2 matrix represented as a list of two lists, each containing two floats.\n\n    Returns:\n    list of list of float or None: The inverse matrix if it exists, otherwise None.\n    \"\"\"\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        print('Error: The matrix must be 2x2.')\n        return None\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        print('Error: The matrix is not invertible (determinant is 0).')\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not b:\n        return -1\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    if not all((len(row) == num_cols_a for row in a)):\n        return -1\n    num_cols_b = len(b[0])\n    if not all((len(row) == num_cols_b for row in b)):\n        return -1\n    if num_cols_a != num_rows_b:\n        return -1\n    result = [[0 for _ in range(num_cols_b)] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors (list of list of float): A list where each inner list represents a feature\n            with its observations.\n\n    Returns:\n        list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    num_features = len(vectors)\n    if num_features == 0:\n        return []\n    num_observations = len(vectors[0])\n    if any((len(feature) != num_observations for feature in vectors)):\n        raise ValueError('All vectors must have the same number of observations.')\n    means = [sum(feature) / num_observations for feature in vectors]\n    covariance_matrix = [[0.0 for _ in range(num_features)] for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(i, num_features):\n            covariance = 0.0\n            for k in range(num_observations):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= num_observations - 1\n            covariance_matrix[i][j] = covariance\n            covariance_matrix[j][i] = covariance\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n\n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Right-hand side vector.\n    n (int): Number of iterations.\n\n    Returns:\n    list: Approximate solution vector rounded to four decimal places.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.size:\n        raise ValueError('The size of vector b must match the dimensions of matrix A.')\n    x_old = np.zeros_like(b, dtype=float)\n    x_new = np.zeros_like(b, dtype=float)\n    D = np.diag(A)\n    if np.any(D == 0):\n        raise ZeroDivisionError('Matrix A has zero(s) on its diagonal. Jacobi method cannot proceed.')\n    for iteration in range(n):\n        for i in range(A.shape[0]):\n            sum_ = np.dot(A[i, :], x_old) - A[i, i] * x_old[i]\n            x_new[i] = (b[i] - sum_) / A[i, i]\n            x_new[i] = round(x_new[i], 4)\n        x_old = x_new.copy()\n    return x_new.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition of a 2x2 matrix A.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 numpy array.\n    \n    Returns:\n        tuple: A tuple containing U, S, and Vt matrices, where\n               A \u2248 U @ S @ Vt\n               - U is a 2x2 orthogonal matrix\n               - S is a 2-element vector of singular values\n               - Vt is the transpose of a 2x2 orthogonal matrix V\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix A must be 2x2.')\n    AtA = A.T @ A\n    (eigenvalues, V) = np.linalg.eigh(AtA)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    V = V[:, sorted_indices]\n    singular_values = np.sqrt(np.maximum(eigenvalues, 0))\n    singular_values = np.where(singular_values < 1e-10, 0, singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, i] = A @ V[:, i] / singular_values[i]\n        else:\n            U[:, i] = np.array([0, 0])\n    if np.linalg.matrix_rank(U) < 2:\n        if np.allclose(U[:, 0], 0):\n            U[:, 0] = np.array([1, 0])\n        U[:, 1] = np.array([-U[0, 0], U[1, 0]])\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    Vt = np.round(V.T, 4)\n    return (U, singular_values, Vt)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Parameters:\n        matrix (list of lists): A 4x4 matrix represented as a list of four lists, each containing four integers or floats.\n        \n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix using Laplace's Expansion.\n        \n        Parameters:\n            mat (list of lists): A 3x3 matrix.\n            \n        Returns:\n            float: The determinant of the matrix.\n        \"\"\"\n        a = mat[0][0]\n        b = mat[0][1]\n        c = mat[0][2]\n        minor_a = [[mat[1][1], mat[1][2]], [mat[2][1], mat[2][2]]]\n        minor_b = [[mat[1][0], mat[1][2]], [mat[2][0], mat[2][2]]]\n        minor_c = [[mat[1][0], mat[1][1]], [mat[2][0], mat[2][1]]]\n        det_a = a * determinant_2x2(minor_a)\n        det_b = -b * determinant_2x2(minor_b)\n        det_c = c * determinant_2x2(minor_c)\n        return det_a + det_b + det_c\n\n    def determinant_2x2(mat: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 2x2 matrix.\n        \n        Parameters:\n            mat (list of lists): A 2x2 matrix.\n            \n        Returns:\n            float: The determinant of the matrix.\n        \"\"\"\n        return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input must be a 4x4 matrix.')\n    determinant = 0\n    for col in range(4):\n        minor = [[matrix[row][c] for c in range(4) if c != col] for row in range(1, 4)]\n        cofactor = (-1) ** col * matrix[0][col] * determinant_3x3(minor)\n        determinant += cofactor\n    return determinant"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    ones = np.ones((X.shape[0], 1))\n    X_b = np.hstack((ones, X))\n    X_transpose = X_b.T\n    theta = np.linalg.pinv(X_transpose @ X_b) @ X_transpose @ y\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = X.T.dot(errors) / m\n        theta = theta - alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = data.mean(axis=0)\n    std = data.std(axis=0)\n    standardized = ((data - mean) / std).round(4)\n    min_val = data.min(axis=0)\n    max_val = data.max(axis=0)\n    normalized = ((data - min_val) / (max_val - min_val)).round(4)\n    return (standardized.tolist(), normalized.tolist())"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Performs k-Means clustering on a set of 2D points.\n\n    Parameters:\n    - points: List of tuples representing the coordinates of each point.\n    - k: Number of clusters.\n    - initial_centroids: List of initial centroid coordinates.\n    - max_iterations: Maximum number of iterations to perform.\n\n    Returns:\n    - List of final centroid coordinates rounded to four decimal places.\n    \"\"\"\n    if k <= 0:\n        raise ValueError(\"Number of clusters 'k' must be a positive integer.\")\n    if len(initial_centroids) != k:\n        raise ValueError(\"The number of initial centroids must be equal to 'k'.\")\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids, dtype=float)\n    for iteration in range(max_iterations):\n        distances = np.linalg.norm(points_np[:, np.newaxis] - centroids, axis=2)\n        closest_centroid_indices = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_np[closest_centroid_indices == ci].mean(axis=0) if np.any(closest_centroid_indices == ci) else centroids[ci] for ci in range(k)])\n        if np.allclose(centroids, new_centroids, atol=1e-08):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    - X (np.ndarray): Feature dataset.\n    - y (np.ndarray): Target labels.\n    - k (int): Number of folds. Default is 5.\n    - shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    - random_seed (int, optional): Seed for reproducibility. Default is None.\n\n    Returns:\n    - List of tuples: Each tuple contains two arrays, (train_indices, test_indices).\n    \"\"\"\n    if k <= 1:\n        raise ValueError(\"Number of folds 'k' must be at least 2.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('The number of samples in X and y must be equal.')\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.default_rng(seed=random_seed)\n        rng.shuffle(indices)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        folds.append(indices[start:stop])\n        current = stop\n    train_test_splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.hstack([folds[j] for j in range(k) if j != i])\n        train_test_splits.append((train_indices, test_indices))\n    return train_test_splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4).tolist()\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Trains a kernel SVM classifier using a deterministic version of the Pegasos algorithm.\n\n    Parameters:\n    - data (np.ndarray): 2D array where each row is a data sample.\n    - labels (np.ndarray): 1D array of labels corresponding to each data sample.\n    - kernel (str): Choice of kernel ('linear' or 'RBF'). Default is 'linear'.\n    - lambda_val (float): Regularization parameter. Default is 0.01.\n    - iterations (int): Number of training iterations. Default is 100.\n    - sigma (float): Parameter for the RBF kernel. Default is 1.0.\n\n    Returns:\n    - List[List[float], List[float]]: A list containing the alpha coefficients and the bias term,\n                                      both rounded to 4 decimal places.\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'RBF':\n        sum_square = np.sum(data ** 2, axis=1)\n        K = sum_square[:, np.newaxis] + sum_square[np.newaxis, :] - 2 * np.dot(data, data.T)\n        K = np.exp(-K / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'RBF'.\")\n    for t in range(1, iterations + 1):\n        eta = 1 / (lambda_val * t)\n        decision = np.dot(K, alphas) + b\n        margins = labels * decision\n        violating_indices = np.where(margins < 1)[0]\n        if len(violating_indices) > 0:\n            update = np.dot(labels[violating_indices], K[:, violating_indices].T)\n            alphas = (1 - eta * lambda_val) * alphas + eta * update\n            b += eta * np.sum(labels[violating_indices])\n        else:\n            alphas = (1 - eta * lambda_val) * alphas\n    alphas = np.round(alphas, 4)\n    b = round(b, 4)\n    return [alphas.tolist(), [b]]"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function for a given input z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The sigmoid of z, rounded to four decimal places.\n    \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    return round(sigmoid_value, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    max_score = max(scores)\n    exps = [math.exp(s - max_score) for s in scores]\n    sum_exps = sum(exps)\n    softmax_vals = [round(exp / sum_exps, 4) for exp in exps]\n    return softmax_vals"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n        features (list[list[float]]): A list of feature vectors, each containing multiple features.\n        labels (list[int]): A list of true binary labels (0 or 1).\n        weights (list[float]): A list of weights corresponding to each feature.\n        bias (float): The bias term.\n\n    Returns:\n        tuple: A tuple containing:\n            - list[float]: Predicted probabilities after sigmoid activation, rounded to four decimal places.\n            - float: Mean squared error between predicted probabilities and true labels, rounded to four decimal places.\n    \"\"\"\n    weighted_sums = []\n    for feature in features:\n        sum_product = sum((w * x for (w, x) in zip(weights, feature))) + bias\n        weighted_sums.append(sum_product)\n    sigmoid = lambda x: 1 / (1 + math.exp(-x))\n    probabilities = [sigmoid(s) for s in weighted_sums]\n    probabilities_rounded = [round(p, 4) for p in probabilities]\n    probs_array = np.array(probabilities_rounded)\n    labels_array = np.array(labels)\n    mse = np.mean((probs_array - labels_array) ** 2)\n    mse_rounded = round(mse, 4)\n    return (probs_array.tolist(), mse_rounded)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Trains a single neuron with sigmoid activation using backpropagation and gradient descent.\n\n    Parameters:\n    - features (np.ndarray): Input feature vectors of shape (n_samples, n_features).\n    - labels (np.ndarray): True binary labels of shape (n_samples,).\n    - initial_weights (np.ndarray): Initial weights of shape (n_features,).\n    - initial_bias (float): Initial bias term.\n    - learning_rate (float): Learning rate for gradient descent.\n    - epochs (int): Number of training epochs.\n\n    Returns:\n    - updated_weights (np.ndarray): Updated weights after training, rounded to 4 decimals.\n    - updated_bias (float): Updated bias after training, rounded to 4 decimals.\n    - mse_history (list[float]): List of Mean Squared Error values for each epoch, rounded to 4 decimals.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    n_samples = features.shape[0]\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    for epoch in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        y_pred = sigmoid(linear_output)\n        errors = y_pred - labels\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        dMSE_dy_pred = 2 * errors / n_samples\n        dy_pred_dz = y_pred * (1 - y_pred)\n        dMSE_dz = dMSE_dy_pred * dy_pred_dz\n        gradient_weights = np.dot(features.T, dMSE_dz)\n        gradient_bias = np.sum(dMSE_dz)\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    return (weights, bias, mse_history)"}
{"task_id": 26, "completion_id": 0, "solution": ""}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    try:\n        C_inv = np.linalg.inv(C_matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError('Basis C is not invertible.')\n    P = C_inv @ B_matrix\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    AtA = np.dot(A.T, A)\n    (eigvals, V) = np.linalg.eigh(AtA)\n    sorted_indices = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[sorted_indices]\n    V = V[:, sorted_indices]\n    singular_values = np.sqrt(np.maximum(eigvals, 0))\n    S = np.zeros((2, 2))\n    S[0, 0] = singular_values[0]\n    S[1, 1] = singular_values[1]\n    U = np.dot(A, V)\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, i] /= singular_values[i]\n        else:\n            U[:, i] = np.zeros(2)\n    if not np.allclose(np.dot(U.T, U), np.identity(2)):\n        (U, _) = np.linalg.qr(U)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y in unison while maintaining correspondence between them.\n\n    Parameters:\n    - X (np.ndarray): Feature array.\n    - y (np.ndarray): Target array.\n    - seed (int, optional): Seed for the random number generator for reproducibility.\n\n    Returns:\n    - list: A list containing the shuffled X and y as Python lists.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    permutation = rng.permutation(len(X))\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates an iterator that yields batches of data from X and optionally y.\n\n    Parameters:\n    - X (np.ndarray): Input data array.\n    - y (np.ndarray, optional): Target data array. Defaults to None.\n    - batch_size (int, optional): Number of samples per batch. Defaults to 64.\n\n    Yields:\n    - list or tuple of lists: Batch of X or (X, y) if y is provided.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = start_idx + batch_size\n        X_batch = X[start_idx:end_idx]\n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the values of the specified feature\n    are greater than or equal to the given threshold.\n\n    Parameters:\n    - X (array-like): The input dataset, where each row represents a sample and each column represents a feature.\n    - feature_i (int): The index of the feature to base the division on.\n    - threshold (float): The threshold value for dividing the dataset.\n\n    Returns:\n    - tuple: A tuple containing two lists:\n        1. Subset of X where X[:, feature_i] >= threshold\n        2. Subset of X where X[:, feature_i] < threshold\n    \"\"\"\n    X = np.array(X)\n    if feature_i < 0 or feature_i >= X.shape[1]:\n        raise IndexError(f'feature_i={feature_i} is out of bounds for dataset with {X.shape[1]} features.')\n    mask_ge = X[:, feature_i] >= threshold\n    mask_lt = X[:, feature_i] < threshold\n    subset_ge = X[mask_ge].tolist()\n    subset_lt = X[mask_lt].tolist()\n    return (subset_ge, subset_lt)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset X up to the specified degree.\n\n    Parameters:\n    X (numpy.ndarray): Input data of shape (n_samples, n_features).\n    degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n    list: A list representation of the polynomial feature matrix.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    poly_features = [np.ones(n_samples)]\n    for deg in range(1, degree + 1):\n        for comb in combinations_with_replacement(range(n_features), deg):\n            new_feature = np.prod(X[:, comb], axis=1)\n            poly_features.append(new_feature)\n    poly_matrix = np.vstack(poly_features).T\n    return poly_matrix.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of the given dataset.\n\n    Parameters:\n    - X (np.ndarray): 2D array of features.\n    - y (np.ndarray): 1D array of labels.\n    - n_subsets (int): Number of random subsets to generate.\n    - replacements (bool): If True, sample with replacement; otherwise, without.\n    - seed (int): Random seed for reproducibility.\n\n    Returns:\n    - List[Tuple[List, List]]: A list of tuples where each tuple contains\n      a subset of X and the corresponding subset of y, both converted to lists.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    return np.eye(n_col)[x].tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of predictions.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score representing the proportion of correct predictions.\n    \"\"\"\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('y_true and y_pred must have the same number of elements.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return accuracy"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for given datasets X and Y.\n\n    Parameters:\n    - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features_x).\n    - Y (np.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_y).\n                                 If None, Y is set to X.\n\n    Returns:\n    - list: A 2D list representing the correlation matrix, with each value rounded to 4 decimals.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    n_samples = X.shape[0]\n    X_mean = X - np.mean(X, axis=0)\n    Y_mean = Y - np.mean(Y, axis=0)\n    covariance_matrix = np.dot(X_mean.T, Y_mean) / (n_samples - 1)\n    X_std = np.std(X, axis=0, ddof=1)\n    Y_std = np.std(Y, axis=0, ddof=1)\n    X_std[X_std == 0] = 1e-10\n    Y_std[Y_std == 0] = 1e-10\n    std_outer = np.outer(X_std, Y_std)\n    correlation_matrix = covariance_matrix / std_outer\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        clf = {}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            X_column = X[:, feature_i]\n            unique_values = np.unique(X_column)\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            thresholds = np.concatenate(([unique_values[0] - 1], thresholds, [unique_values[-1] + 1]))\n            for threshold in thresholds:\n                predictions = np.ones(n_samples)\n                predictions[X_column < threshold] = -1\n                misclassified = w[y != predictions]\n                error = sum(misclassified)\n                if error < min_error:\n                    min_error = error\n                    clf['feature'] = feature_i\n                    clf['threshold'] = threshold\n                    clf['polarity'] = 1\n                predictions = np.ones(n_samples)\n                predictions[X_column > threshold] = -1\n                misclassified = w[y != predictions]\n                error = sum(misclassified)\n                if error < min_error:\n                    min_error = error\n                    clf['feature'] = feature_i\n                    clf['threshold'] = threshold\n                    clf['polarity'] = -1\n        min_error = max(min_error, 1e-10)\n        clf['alpha'] = round(0.5 * math.log((1.0 - min_error) / min_error), 4)\n        feature = clf['feature']\n        threshold = clf['threshold']\n        polarity = clf['polarity']\n        predictions = np.ones(n_samples)\n        if polarity == 1:\n            predictions[X[:, feature] < threshold] = -1\n        else:\n            predictions[X[:, feature] > threshold] = -1\n        w *= np.exp(-clf['alpha'] * y * predictions)\n        w /= np.sum(w)\n        clf['threshold'] = round(clf['threshold'], 4)\n        classifiers.append(clf)\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    adjusted_scores = scores_array - max_score\n    exp_scores = np.exp(adjusted_scores)\n    sum_exp = np.sum(exp_scores)\n    log_sum_exp = np.log(sum_exp)\n    log_softmax_values = adjusted_scores - log_sum_exp\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    return rounded_log_softmax.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be specified for initialization.')\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.W = np.round(self.W, 4)\n        self.w0 = np.zeros(self.n_units)\n        self.w0 = np.round(self.w0, 4)\n        self.W_optimizer = SGDOptimizer(learning_rate=0.01)\n        self.w0_optimizer = SGDOptimizer(learning_rate=0.01)\n\n    def parameters(self):\n        W_params = self.W.size\n        w0_params = self.w0.size\n        return W_params + w0_params\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        output = np.round(output, 4)\n        return output\n\n    def backward_pass(self, accum_grad):\n        dL_dY = accum_grad\n        dL_dX = np.dot(dL_dY, self.W.T)\n        if self.trainable:\n            dW = np.dot(self.layer_input.T, dL_dY)\n            dw0 = np.sum(dL_dY, axis=0)\n            self.W = self.W_optimizer.update(self.W, dW)\n            self.W = np.round(self.W, 4)\n            self.w0 = self.w0_optimizer.update(self.w0, dw0)\n            self.w0 = np.round(self.w0, 4)\n        return dL_dX\n\n    def output_shape(self):\n        return (self.n_units,)\nclass SGDOptimizer:\n\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n\n    def update(self, params, grads):\n        params_updated = params - self.lr * grads\n        return params_updated"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    if padding > 0:\n        input_padded = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    else:\n        input_padded = input_matrix.copy()\n    (input_height, input_width) = input_padded.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (input_height - kernel_height) // stride + 1\n    out_width = (input_width - kernel_width) // stride + 1\n    output = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            vert_start = i * stride\n            vert_end = vert_start + kernel_height\n            horiz_start = j * stride\n            horiz_end = horiz_start + kernel_width\n            current_slice = input_padded[vert_start:vert_end, horiz_start:horiz_end]\n            conv = current_slice * kernel\n            output[i, j] = np.sum(conv)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the ReLU activation function to the input value.\n\n    Parameters:\n    z (float): The input value.\n\n    Returns:\n    float: The result of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return z if z > 0 else 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n\n    Parameters:\n    - X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n    - w (np.ndarray): Coefficient vector of shape (n_features,).\n    - y_true (np.ndarray): True labels of shape (n_samples,).\n    - alpha (float): Regularization parameter.\n\n    Returns:\n    - float: Ridge loss rounded to four decimal places.\n    \"\"\"\n    predictions = X @ w\n    mse = np.mean((y_true - predictions) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two vectors.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy.ndarray): True binary labels (0 or 1).\n    y_pred (numpy.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    if tp + fp == 0:\n        return 0.0\n    precision_value = tp / (tp + fp)\n    return precision_value"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs Gradient Descent optimization using different methods.\n\n    Parameters:\n    - X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n    - y (numpy.ndarray): Target vector of shape (n_samples, ).\n    - weights (numpy.ndarray): Initial weights of shape (n_features, ).\n    - learning_rate (float): Learning rate for weight updates.\n    - n_iterations (int): Number of iterations to perform.\n    - batch_size (int, optional): Size of each mini-batch. Default is 1.\n    - method (str, optional): Gradient descent variant - 'batch', 'stochastic', or 'mini-batch'. Default is 'batch'.\n\n    Returns:\n    - list: Final weights rounded to 4 decimal places.\n    \"\"\"\n    n_samples = X.shape[0]\n    for _ in range(n_iterations):\n        if method == 'batch':\n            X_batch = X\n            y_batch = y\n        elif method == 'stochastic':\n            idx = np.random.randint(0, n_samples)\n            X_batch = X[idx].reshape(1, -1)\n            y_batch = y[idx].reshape(1)\n        elif method == 'mini-batch':\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'.\")\n        predictions = X_batch.dot(weights)\n        errors = predictions - y_batch\n        gradients = 2 / X_batch.shape[0] * X_batch.T.dot(errors)\n        weights -= learning_rate * gradients\n    weights = np.round(weights, 4)\n    return weights.tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (list of lists or np.ndarray): The input matrix.\n\n    Returns:\n    list of lists: The RREF of the input matrix.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    break\n        if lead == cols:\n            break\n        A[[r, i]] = A[[i, r]]\n        pivot = A[r, lead]\n        if pivot != 0:\n            A[r] = A[r] / pivot\n        for i in range(rows):\n            if i != r:\n                factor = A[i, lead]\n                A[i] = A[i] - factor * A[r]\n        lead += 1\n    A[np.abs(A) < 1e-10] = 0\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    prev_loss = float('inf')\n    for iteration in range(max_iter):\n        y_hat = X.dot(w) + b\n        errors = y - y_hat\n        grad_w = -1 / n_samples * X.T.dot(errors) + alpha * np.sign(w)\n        grad_b = -1 / n_samples * np.sum(errors)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        mse_loss = 1 / (2 * n_samples) * np.sum(errors ** 2)\n        l1_penalty = alpha * np.sum(np.abs(w))\n        loss = mse_loss + l1_penalty\n        if abs(prev_loss - loss) < tol:\n            break\n        prev_loss = loss\n    w = np.round(w, 4)\n    b = round(b, 4)\n    return (w.tolist(), b)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n\n    Args:\n        source (str): The source string.\n        target (str): The target string.\n\n    Returns:\n        int: The OSA distance between source and target.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            substitution = dp[i - 1][j - 1] + cost\n            insertion = dp[i][j - 1] + 1\n            deletion = dp[i - 1][j] + 1\n            dp[i][j] = min(substitution, insertion, deletion)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                transposition = dp[i - 2][j - 2] + 1\n                dp[i][j] = min(dp[i][j], transposition)\n    return dp[len_source][len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = TP / denominator\n        return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention for the given queries (Q), keys (K), and values (V).\n\n    Parameters:\n    - Q: Queries matrix of shape (n, d_k)\n    - K: Keys matrix of shape (n, d_k)\n    - V: Values matrix of shape (n, d_v)\n\n    Returns:\n    - List representation of the self-attention output with values rounded to 4 decimal places.\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = np.dot(attention_weights, V)\n    attention_output = np.round(attention_output, 4)\n    return attention_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = np.array(initial_hidden_state)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        h_prev = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n    final_hidden_state = np.round(h_prev, 4).tolist()\n    return final_hidden_state"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a set of points.\n\n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation distance along the x-axis.\n    - ty: Translation distance along the y-axis.\n\n    Returns:\n    - A new list of points after applying the translation.\n    \"\"\"\n    points_array = np.array(points)\n    translation_vector = np.array([tx, ty])\n    translated_points = points_array + translation_vector\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n    - mu_p (float): Mean of distribution P\n    - sigma_p (float): Standard deviation of distribution P (must be positive)\n    - mu_q (float): Mean of distribution Q\n    - sigma_q (float): Standard deviation of distribution Q (must be positive)\n\n    Returns:\n    - float: The KL divergence KL(Q || P)\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive numbers.')\n    log_term = np.log(sigma_p / sigma_q)\n    variance_term = (sigma_q ** 2 + (mu_q - mu_p) ** 2) / (2 * sigma_p ** 2)\n    kl_div = log_term + variance_term - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(size):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if A[max_row, i] == 0:\n            raise ValueError('Matrix is singular or no unique solution exists.')\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j] -= factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if A[i, i] == 0:\n            raise ValueError('Matrix is singular or no unique solution exists.')\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Parameters:\n        - x: numpy array of shape (T, input_size, 1)\n        - initial_hidden_state: numpy array of shape (hidden_size, 1)\n        - initial_cell_state: numpy array of shape (hidden_size, 1)\n        \n        Returns:\n        - hidden_states: list of hidden states for each time step (as lists)\n        - final_hidden_state: final hidden state (as list)\n        - final_cell_state: final cell state (as list)\n        \"\"\"\n        T = x.shape[0]\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n        for t in range(T):\n            x_t = x[t]\n            concat = np.vstack((h_prev, x_t))\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_hat_t = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c_prev + i_t * c_hat_t\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            h_t_rounded = np.round(h_t, 4)\n            c_t_rounded = np.round(c_t, 4)\n            hidden_states.append(h_t_rounded.flatten().tolist())\n            h_prev = h_t_rounded\n            c_prev = c_t_rounded\n        final_hidden_state = h_prev.flatten().tolist()\n        final_cell_state = c_prev.flatten().tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    N = len(corpus)\n    df = {}\n    for term in query:\n        df_count = sum((1 for doc in corpus if term in doc))\n        df[term] = df_count\n    idf = {}\n    for term in query:\n        idf[term] = np.log((N + 1) / (df[term] + 1)) + 1\n    tf_idf_matrix = []\n    for doc in corpus:\n        tf_idf_scores = []\n        doc_length = len(doc)\n        if doc_length == 0:\n            tf_idf_scores = [0.0 for _ in query]\n        else:\n            term_counts = {}\n            for word in doc:\n                term_counts[word] = term_counts.get(word, 0) + 1\n            for term in query:\n                tf = term_counts.get(term, 0) / doc_length\n                tf_idf = tf * idf[term]\n                tf_idf_scores.append(round(tf_idf, 4))\n        tf_idf_matrix.append(tf_idf_scores)\n    return np.array(tf_idf_matrix).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision_denominator = TP + FP\n    if precision_denominator == 0:\n        precision = 0.0\n    else:\n        precision = TP / precision_denominator\n    recall_denominator = TP + FN\n    if recall_denominator == 0:\n        recall = 0.0\n    else:\n        recall = TP / recall_denominator\n    beta_squared = beta ** 2\n    denominator = beta_squared * precision + recall\n    if denominator == 0:\n        f_score = 0.0\n    else:\n        f_score = (1 + beta_squared) * (precision * recall) / denominator\n    return round(f_score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x as a list rounded to 8 decimal places\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / counts.sum()\n    gini = 1.0 - np.sum(probabilities ** 2)\n    gini_rounded = np.round(gini, 3)\n    return gini_rounded"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError('Vectors v and L must be of the same dimension.')\n    dot_v_L = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_L_L = sum((Li * Li for Li in L))\n    if dot_L_L == 0:\n        raise ValueError('The line vector L must be non-zero.')\n    scalar = dot_v_L / dot_L_L\n    projection = [scalar * Li for Li in L]\n    projection_rounded = [round(coord, 3) for coord in projection]\n    return projection_rounded"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            element = dense_matrix[row][col]\n            if element != 0:\n                values.append(element)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    basis_indices = []\n    current_basis = np.empty((m, 0))\n    for i in range(n):\n        col = A[:, i].reshape(-1, 1)\n        if current_basis.size == 0:\n            basis_indices.append(i)\n            current_basis = col\n        else:\n            try:\n                (x, residuals, rank, s) = np.linalg.lstsq(current_basis, col, rcond=None)\n                if residuals.size == 0 or residuals > 1e-10:\n                    basis_indices.append(i)\n                    current_basis = np.hstack((current_basis, col))\n            except np.linalg.LinAlgError:\n                basis_indices.append(i)\n                current_basis = np.hstack((current_basis, col))\n    image_basis = A[:, basis_indices]\n    image_basis = np.round(image_basis, decimals=8).tolist()\n    return image_basis"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list):\n        return -1\n    if not all((isinstance(row, list) for row in img)):\n        return -1\n    if len(img) == 0:\n        return -1\n    row_length = len(img[0])\n    total = 0\n    count = 0\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int):\n                return -1\n            if pixel < 0 or pixel > 255:\n                return -1\n            total += pixel\n            count += 1\n    if count == 0:\n        return -1\n    average_brightness = total / count\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are empty or have mismatched lengths.\n    TypeError: If inputs are not array-like or contain non-numeric data.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true, dtype=np.float64)\n        y_pred = np.asarray(y_pred, dtype=np.float64)\n    except (ValueError, TypeError) as e:\n        raise TypeError('Inputs must be array-like and contain numeric values.') from e\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    root_mean_square_error = np.sqrt(mse)\n    return round(root_mean_square_error, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n        return round(float(dice), 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\nimport hashlib\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = np.zeros(dim, dtype=float)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_rng = np.random.RandomState(seed=feature_seed)\n        feature_hv = feature_rng.choice([-1, 1], size=dim).astype(float)\n        value_str = str(value)\n        combined_str = f'{feature_seed}_{value_str}'\n        hash_digest = hashlib.sha256(combined_str.encode('utf-8')).hexdigest()\n        value_seed = int(hash_digest[:8], 16)\n        value_rng = np.random.RandomState(seed=value_seed)\n        value_hv = value_rng.choice([-1, 1], size=dim).astype(float)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n\n    Parameters:\n        data (list of lists): Each inner list contains [y_true, y_pred].\n\n    Returns:\n        list of lists: 2x2 confusion matrix in the format [[TN, FP], [FN, TP]].\n    \"\"\"\n    counts = Counter(map(tuple, data))\n    tn = counts.get((0, 0), 0)\n    fp = counts.get((0, 1), 0)\n    fn = counts.get((1, 0), 0)\n    tp = counts.get((1, 1), 0)\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n        v1 (np.ndarray): First input vector.\n        v2 (np.ndarray): Second input vector.\n\n    Returns:\n        float: Cosine similarity rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input vectors have different shapes,\n                    are empty, or have zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    cosine_sim = dot_product / (norm_v1 * norm_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((label in (0, 1) for label in actual)):\n        raise ValueError('All elements in the actual list must be either 0 or 1.')\n    if not all((label in (0, 1) for label in predicted)):\n        raise ValueError('All elements in the predicted list must be either 0 or 1.')\n    TP = TN = FP = FN = 0\n    for (act, pred) in zip(actual, predicted):\n        if act == 1 and pred == 1:\n            TP += 1\n        elif act == 0 and pred == 0:\n            TN += 1\n        elif act == 0 and pred == 1:\n            FP += 1\n        elif act == 1 and pred == 0:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total > 0 else 0\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    if precision + recall > 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_score = 0\n    specificity = TN / (TN + FP) if TN + FP > 0 else 0\n    negative_predictive_value = TN / (TN + FN) if TN + FN > 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset.\n\n    Parameters:\n    data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation,\n          25th, 50th, 75th percentiles, and interquartile range (IQR).\n    \"\"\"\n    arr = np.array(data)\n    mean = np.round(np.mean(arr), 4)\n    median = np.round(np.median(arr), 4)\n    (unique, counts) = np.unique(arr, return_counts=True)\n    max_count_indices = np.where(counts == counts.max())[0]\n    mode = np.round(unique[max_count_indices[0]], 4)\n    variance = np.round(np.var(arr, ddof=1), 4)\n    std_dev = np.round(np.std(arr, ddof=1), 4)\n    percentile_25 = np.round(np.percentile(arr, 25), 4)\n    percentile_50 = np.round(np.percentile(arr, 50), 4)\n    percentile_75 = np.round(np.percentile(arr, 75), 4)\n    iqr = np.round(percentile_75 - percentile_25, 4)\n    stats = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': std_dev, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\n    return stats"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: Contrast value calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('Input must be a numpy array.')\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array representing a grayscale image.')\n    if img.size == 0:\n        raise ValueError('Input image is empty.')\n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n\n    Raises:\n        ValueError: If input vectors are not 1D or have different lengths.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError('Both vec1 and vec2 must be NumPy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vectors must be 1-dimensional.')\n    if vec1.size != vec2.size:\n        raise ValueError('Vectors must be of the same length.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed = []\n    for x in data:\n        features = [round(x ** d, 8) for d in range(1, degree + 1)]\n        transformed.append(features)\n    return transformed"}
{"task_id": 85, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter, dtype=np.float64)\n    grad = np.array(grad, dtype=np.float64)\n    m = np.array(m, dtype=np.float64)\n    v = np.array(v, dtype=np.float64)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n    updated_parameter = updated_parameter.tolist()\n    updated_m = updated_m.tolist()\n    updated_v = updated_v.tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "def gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    generated_ids = input_ids.copy()\n    (vocab_size, embedding_dim) = params['wte'].shape\n    max_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    head_dim = embedding_dim // n_head if n_head > 0 else embedding_dim\n    if embedding_dim % n_head != 0:\n        effective_n_head = 1\n        head_dim = embedding_dim\n    else:\n        effective_n_head = n_head\n    W_q = np.random.randn(embedding_dim, embedding_dim)\n    W_k = np.random.randn(embedding_dim, embedding_dim)\n    W_v = np.random.randn(embedding_dim, embedding_dim)\n    W_o = np.random.randn(embedding_dim, embedding_dim)\n    FFN_hidden_dim = 20\n    W_ff1 = np.random.randn(embedding_dim, FFN_hidden_dim)\n    W_ff2 = np.random.randn(FFN_hidden_dim, embedding_dim)\n    W_logits = np.random.randn(embedding_dim, vocab_size)\n    epsilon = 1e-05\n    for _ in range(n_tokens_to_generate):\n        if len(generated_ids) >= max_ctx:\n            context_ids = generated_ids[-max_ctx:]\n        else:\n            context_ids = generated_ids\n        seq_len = len(context_ids)\n        token_embeddings = params['wte'][context_ids]\n        pos_indices = np.arange(seq_len)\n        pos_embeddings = params['wpe'][pos_indices]\n        x = token_embeddings + pos_embeddings\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        x_norm = (x - mean) / np.sqrt(var + epsilon)\n        x_norm = x_norm * params['ln_f']['g'] + params['ln_f']['b']\n        Q = x_norm @ W_q\n        K = x_norm @ W_k\n        V = x_norm @ W_v\n        Q = Q.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)\n        K = K.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)\n        V = V.reshape(seq_len, effective_n_head, head_dim).transpose(1, 0, 2)\n        scores = Q @ K.transpose(0, 2, 1) / np.sqrt(head_dim)\n        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n        attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n        attn_output = attn_weights @ V\n        attn_output = attn_output.transpose(1, 0, 2).reshape(seq_len, embedding_dim)\n        attn_output = attn_output @ W_o\n        x = x_norm + attn_output\n        ffn = x @ W_ff1\n        ffn = np.maximum(ffn, 0)\n        ffn = ffn @ W_ff2\n        x = x + ffn\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        x_norm = (x - mean) / np.sqrt(var + epsilon)\n        x_norm = x_norm * params['ln_f']['g'] + params['ln_f']['b']\n        last_token = x_norm[-1]\n        logits = last_token @ W_logits\n        exp_logits = np.exp(logits - np.max(logits))\n        probs = exp_logits / np.sum(exp_logits)\n        next_token = np.argmax(probs)\n        generated_ids.append(next_token)\n    generated_text = encoder.decode(generated_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        shifted = values - np.max(values)\n        exps = np.exp(shifted)\n        return exps / np.sum(exps)\n    crystal_values = np.array(crystal_values, dtype=float)\n    sqrt_dim = np.sqrt(dimension)\n    output_patterns = []\n    for i in range(n):\n        scores = crystal_values[i] * crystal_values / sqrt_dim\n        attention_weights = softmax(scores)\n        weighted_pattern = np.sum(attention_weights * crystal_values)\n        output_patterns.append(round(weighted_pattern, 4))\n    return output_patterns"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus based on the query.\n\n    Parameters:\n    - corpus: List of documents (each document is a string).\n    - query: List of query terms (strings).\n    - k1: Term frequency saturation parameter.\n    - b: Document length normalization parameter.\n\n    Returns:\n    - List of BM25 scores for each document, rounded to three decimal places.\n    \"\"\"\n    N = len(corpus)\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    tokenized_query = [term.lower() for term in query]\n    df = {}\n    for term in tokenized_query:\n        if term in df:\n            continue\n        df[term] = sum((1 for doc in tokenized_corpus if term in doc))\n    idf = {}\n    for (term, freq) in df.items():\n        idf[term] = np.log((N - freq + 0.5) / (freq + 0.5) + 1)\n    doc_lengths = [len(doc) for doc in tokenized_corpus]\n    avgdl = np.mean(doc_lengths) if doc_lengths else 0\n    scores = []\n    for (idx, doc) in enumerate(tokenized_corpus):\n        score = 0.0\n        doc_len = doc_lengths[idx]\n        term_counts = Counter(doc)\n        for term in tokenized_query:\n            if term in term_counts:\n                tf = term_counts[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_len / avgdl)) if avgdl > 0 else tf + k1\n                score += idf[term] * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = tp / (tp + fp)\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = tp / (tp + fn)\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * precision * recall / (precision + recall)\n    return round(f1_score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended.append(consumption_data[i - 1] - fluctuation)\n    N = 10\n    sum_x = sum(range(1, 11))\n    sum_y = sum(detrended)\n    sum_xx = sum((x * x for x in range(1, 11)))\n    sum_xy = sum((x * y for (x, y) in zip(range(1, 11), detrended)))\n    denominator = N * sum_xx - sum_x ** 2\n    if denominator == 0:\n        raise ValueError('Denominator in linear regression calculation is zero.')\n    m = (N * sum_xy - sum_x * sum_y) / denominator\n    b = (sum_y - m * sum_x) / N\n    y15 = m * 15 + b\n    f15 = 10 * math.sin(2 * PI * 15 / 10)\n    final_consumption = y15 + f15\n    rounded_consumption = round(final_consumption)\n    safety_margin = math.ceil(rounded_consumption * 1.05)\n    return int(safety_margin)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention for a single head.\n\n    Parameters:\n    - Q: Queries matrix of shape (m, d_k)\n    - K: Keys matrix of shape (m, d_k)\n    - V: Values matrix of shape (m, d_v)\n\n    Returns:\n    - Output matrix after applying self-attention of shape (m, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention.\n\n    Parameters:\n    - Q: Queries matrix of shape (m, d_model)\n    - K: Keys matrix of shape (m, d_model)\n    - V: Values matrix of shape (m, d_model)\n    - n_heads: Number of attention heads\n\n    Returns:\n    - Concatenated output from all attention heads of shape (m, d_model)\n    \"\"\"\n    (m, d_model) = Q.shape\n    assert d_model % n_heads == 0, 'd_model must be divisible by n_heads'\n    d_k = d_model // n_heads\n    d_v = d_model // n_heads\n    Q_split = Q.reshape(m, n_heads, d_k)\n    K_split = K.reshape(m, n_heads, d_k)\n    V_split = V.reshape(m, n_heads, d_v)\n    attention_outputs = []\n    for i in range(n_heads):\n        Q_head = Q_split[:, i, :]\n        K_head = K_split[:, i, :]\n        V_head = V_split[:, i, :]\n        attention_output = self_attention(Q_head, K_head, V_head)\n        attention_outputs.append(attention_output)\n    concatenated = np.concatenate(attention_outputs, axis=-1)\n    return concatenated"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    a = b = c = d = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            a += 1\n        elif xi == 1 and yi == 0:\n            b += 1\n        elif xi == 0 and yi == 1:\n            c += 1\n        elif xi == 0 and yi == 0:\n            d += 1\n        else:\n            raise ValueError('All elements must be 0 or 1.')\n    numerator = a * d - b * c\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    if denominator == 0:\n        raise ValueError('Denominator is zero, Phi coefficient is undefined.')\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x (float): Input value.\n        alpha (float, optional): Slope parameter for negative values. Defaults to 0.25.\n\n    Returns:\n        float: PReLU activation value.\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 0:\n        result = x + math.log1p(math.exp(-x))\n    else:\n        result = math.log1p(math.exp(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (rho_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value rounded to six decimal places.\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_theta_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.array(pi_theta_ref, dtype=np.float64)\n    epsilon_small = 1e-10\n    pi_theta_old = np.clip(pi_theta_old, epsilon_small, None)\n    pi_theta_ref = np.clip(pi_theta_ref, epsilon_small, None)\n    surrogate1 = rhos * A\n    surrogate2 = np.clip(rhos, 1 - epsilon, 1 + epsilon) * A\n    clipped_objective = np.minimum(surrogate1, surrogate2)\n    L_clip = np.mean(clipped_objective)\n    pi_theta = rhos * pi_theta_old\n    pi_theta = np.clip(pi_theta, epsilon_small, None)\n    kl_divergence = np.sum(pi_theta * np.log(pi_theta / pi_theta_ref)) / len(pi_theta)\n    grpo = L_clip - beta * kl_divergence\n    return round(grpo, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid\n    return round(swish_value, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_output = scale * x\n    else:\n        selu_output = scale * alpha * (math.exp(x) - 1)\n    return round(selu_output, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    binary_predictions = (probabilities >= 0.5).astype(int)\n    return binary_predictions.reshape(-1).tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, D) = X.shape\n    classes = np.unique(y)\n    C = len(classes)\n    Y = np.zeros((N, C))\n    Y[np.arange(N), y] = 1\n    B = np.zeros((D, C))\n    losses = []\n    for _ in range(iterations):\n        logits = np.dot(X, B)\n        logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits_stable)\n        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n        P = exp_logits / sum_exp\n        log_probs = np.log(P[np.arange(N), y] + 1e-15)\n        loss = -np.mean(log_probs)\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, P - Y) / N\n        B -= learning_rate * gradient\n    B_rounded = np.round(B.T, 4).tolist()\n    return (B_rounded, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n    weights = np.zeros(X_bias.shape[1])\n    loss_history = []\n    n_samples = X_bias.shape[0]\n    for _ in range(iterations):\n        z = np.dot(X_bias, weights)\n        p = 1 / (1 + np.exp(-z))\n        epsilon = 1e-15\n        loss = -np.mean(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon))\n        loss_history.append(round(loss, 4))\n        gradient = np.dot(X_bias.T, p - y) / n_samples\n        weights -= learning_rate * gradient\n    optimized_coefficients = np.round(weights, 4).tolist()\n    return (optimized_coefficients, loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n\n    Args:\n        Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n        K (np.ndarray): Key matrix of shape (seq_len, d_k)\n        V (np.ndarray): Value matrix of shape (seq_len, d_v)\n        mask (np.ndarray): Attention mask of shape (seq_len, seq_len)\n\n    Returns:\n        list: The result of masked self-attention as a Python list\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    d_k = K.shape[-1]\n    scores = scores / np.sqrt(d_k)\n    if mask.dtype != np.float32 and mask.dtype != np.float64:\n        mask = mask.astype(np.float32)\n        mask = np.where(mask, 0.0, -1000000000.0)\n    else:\n        mask = mask\n    scores += mask\n    max_scores = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is measured using entropy, which increases with the variety and\n    even distribution of apple colors. If all apples are the same color, the disorder\n    is 0. The entropy is calculated as:\n    \n        H = -sum(p_i * log2(p_i)) for all unique colors i\n    \n    where p_i is the proportion of apples of color i.\n    \n    The result is rounded to the nearest 4th decimal.\n    \"\"\"\n    from collections import Counter\n    import math\n    if not apples:\n        return 0.0\n    total = len(apples)\n    color_counts = Counter(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total\n        if p > 0:\n            entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs layer normalization on the input tensor X.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    - gamma (np.ndarray): Scaling parameters of shape (feature_dim,).\n    - beta (np.ndarray): Shifting parameters of shape (feature_dim,).\n    - epsilon (float): Small constant to avoid division by zero.\n\n    Returns:\n    - List: The layer-normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled_shifted = X_normalized * gamma + beta\n    return np.round(X_scaled_shifted, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    matches = {}\n    for word in cand_counts:\n        if word in ref_counts:\n            matches[word] = min(cand_counts[word], ref_counts[word])\n    matched_unigrams = sum(matches.values())\n    if matched_unigrams == 0:\n        return 0.0\n    precision = matched_unigrams / len(cand_tokens)\n    recall = matched_unigrams / len(ref_tokens)\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + alpha) * precision * recall / (beta * precision + recall)\n    ref_word_positions = {}\n    for (idx, word) in enumerate(ref_tokens):\n        if word not in ref_word_positions:\n            ref_word_positions[word] = []\n        ref_word_positions[word].append(idx)\n    aligned_positions = []\n    used_ref_positions = set()\n    for word in cand_tokens:\n        if word in ref_word_positions:\n            for pos in ref_word_positions[word]:\n                if pos not in used_ref_positions:\n                    aligned_positions.append(pos)\n                    used_ref_positions.add(pos)\n                    break\n    fragments = 1\n    for i in range(1, len(aligned_positions)):\n        if aligned_positions[i] != aligned_positions[i - 1] + 1:\n            fragments += 1\n    fragmentation_penalty = gamma * (fragments / matched_unigrams)\n    meteor = f_mean * (1 - fragmentation_penalty)\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    minimum = min(x)\n    maximum = max(x)\n    if maximum == minimum:\n        return [0.0 for _ in x]\n    normalized = [(value - minimum) / (maximum - minimum) for value in x]\n    normalized_rounded = [round(num, 4) for num in normalized]\n    return normalized_rounded"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n\n    Parameters:\n    x (np.ndarray): Input 1D array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n\n    Returns:\n    list: The output after applying the residual block, rounded to 4 decimal places.\n    \"\"\"\n    y1 = np.maximum(0, np.dot(x, w1))\n    y2 = np.maximum(0, np.dot(y1, w2))\n    out = np.maximum(0, y2 + x)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X (np.ndarray): Input array of shape (B, C, H, W)\n    - gamma (np.ndarray): Scale parameters of shape (C,)\n    - beta (np.ndarray): Shift parameters of shape (C,)\n    - epsilon (float): Small constant for numerical stability\n\n    Returns:\n    - list: The batch-normalized array rounded to 4 decimal places\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    gamma = gamma.reshape(1, -1, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)\n    out = gamma * X_normalized + beta\n    return np.round(out, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Parameters:\n    c (float): Coefficient of the term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n\n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list of list of float): List of 2D vectors.\n    tol (float): Tolerance to determine linear independence.\n\n    Returns:\n    list of list of float: Orthonormal basis vectors rounded to 4 decimal places.\n    \"\"\"\n    orthonormal = []\n    for v in vectors:\n        v = np.array(v, dtype=float)\n        for basis in orthonormal:\n            proj = np.dot(v, basis) * basis\n            v = v - proj\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            normalized_v = v / norm\n            normalized_v = np.round(normalized_v, 4)\n            orthonormal.append(normalized_v.tolist())\n    return orthonormal"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    cross_rounded = np.round(cross, 4)\n    return cross_rounded.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    n = A.shape[0]\n    solution = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        solution[i] = det_Ai / det_A\n    solution = np.round(solution, 4)\n    return solution.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p, dtype=np.float64)\n    q_array = np.array(q, dtype=np.float64)\n    bc = np.sum(np.sqrt(p_array * q_array))\n    if bc == 0:\n        return 0.0\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a (list[int | float]): The first vector.\n        b (list[int | float]): The second vector.\n\n    Returns:\n        list[int | float]: A new vector representing the element-wise sum.\n        int: -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        T = len(episode)\n        rewards = [step[2] for step in episode]\n        G_t = np.zeros(T, dtype=np.float64)\n        G = 0.0\n        for t in reversed(range(T)):\n            G += rewards[t]\n            G_t[t] = G\n        states = [step[0] for step in episode]\n        actions = [step[1] for step in episode]\n        unique_states = set(states)\n        pi = {}\n        for s in unique_states:\n            logits = theta[s, :]\n            max_logit = np.max(logits)\n            exp_logits = np.exp(logits - max_logit)\n            pi_s = exp_logits / np.sum(exp_logits)\n            pi[s] = pi_s\n        for t in range(T):\n            s = states[t]\n            a = actions[t]\n            G = G_t[t]\n            pi_s = pi[s]\n            grad_log_pi = np.zeros(num_actions, dtype=np.float64)\n            grad_log_pi[a] = 1.0\n            grad_log_pi -= pi_s\n            gradient[s, :] += grad_log_pi * G\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer.\n\n    Parameters:\n    - n_experts (int): Total number of experts in the MoE layer.\n    - k_active (int): Number of active experts for a given input.\n    - d_in (int): Input dimension.\n    - d_out (int): Output dimension.\n\n    Returns:\n    - tuple: (FLOPs_dense, FLOPs_MoE, savings_percentage)\n             All values are rounded to one decimal place.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings = round(savings, 1)\n    return (flops_dense, flops_moe, savings)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts models.\n\n    Parameters:\n    - X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n    - W_g (np.ndarray): Gating weight matrix of shape (input_dim, num_experts).\n    - W_noise (np.ndarray): Noise weight matrix of shape (input_dim, num_experts).\n    - N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    - k (int): Number of top experts to select (sparsity constraint).\n\n    Returns:\n    - List[List[float]]: Final gating probabilities matrix rounded to 4 decimals.\n    \"\"\"\n    deterministic_scores = X @ W_g\n    noise_component = N * (X @ W_noise)\n    scores = deterministic_scores + noise_component\n    gating_probs = np.zeros_like(scores)\n    for i in range(scores.shape[0]):\n        top_k_indices = np.argpartition(scores[i], -k)[-k:]\n        top_k_scores = scores[i][top_k_indices]\n        top_k_scores -= np.max(top_k_scores)\n        exp_scores = np.exp(top_k_scores)\n        probs = exp_scores / np.sum(exp_scores)\n        gating_probs[i][top_k_indices] = probs\n    gating_probs = np.round(gating_probs, 4)\n    return gating_probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    - x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    - We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    - Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    - n_experts (int): Number of experts.\n    - top_k (int): Number of top experts to route each token to.\n\n    Returns:\n    - List: The final MoE output rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    gating_logits = np.dot(x, Wg)\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(gating_logits - max_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    prob_sums = np.sum(top_k_probs, axis=1, keepdims=True)\n    normalized_probs = top_k_probs / prob_sums\n    batch_size = x.shape[0]\n    selected_experts = We[top_k_indices]\n    x_expanded = np.expand_dims(x, axis=1)\n    weighted_inputs = x_expanded * selected_experts\n    expert_outputs = np.sum(weighted_inputs, axis=2)\n    normalized_probs_expanded = np.expand_dims(normalized_probs, axis=2)\n    weighted_expert_outputs = expert_outputs * normalized_probs_expanded\n    final_output = np.sum(weighted_expert_outputs, axis=1)\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (B, C, H, W).\n    - gamma (np.ndarray): Scale parameter of shape (1, C, 1, 1).\n    - beta (np.ndarray): Shift parameter of shape (1, C, 1, 1).\n    - num_groups (int): Number of groups to divide the channels into.\n    - epsilon (float): Small constant for numerical stability.\n\n    Returns:\n    - List: The group-normalized tensor as a Python list with values rounded to 4 decimals.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'The number of channels must be divisible by num_groups.'\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_scaled = gamma * X_normalized + beta\n    X_scaled = np.round(X_scaled, 4)\n    return X_scaled.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x value where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n\n    Parameters:\n    - start_x (float): The starting point for the gradient descent.\n    - learning_rate (float): The step size multiplier.\n    - tolerance (float): The threshold for convergence.\n    - max_iters (int): The maximum number of iterations to perform.\n\n    Returns:\n    - float: The x value where f(x) is minimized, rounded to four decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * gradient\n        if abs(x_new - x) < tolerance:\n            return round(x_new, 4)\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) activation function to the input array.\n    \n    Parameters:\n    - x (np.ndarray): Input array.\n    - alpha (float): Scaling factor for the output.\n    - gamma (float): Scaling factor for the input.\n    - beta (float): Shifting factor for the input.\n    \n    Returns:\n    - list[float]: Activated and rounded output as a Python list.\n    \"\"\"\n    scaled_shifted_x = gamma * x + beta\n    activated = np.tanh(scaled_shifted_x)\n    scaled_activated = alpha * activated\n    rounded_output = np.round(scaled_activated, 4)\n    return rounded_output.tolist()"}
