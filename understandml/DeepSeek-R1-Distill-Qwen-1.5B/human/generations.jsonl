{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the dot product of a matrix and a vector. The dot product is a fundamental operation in linear algebra where each element of the resulting vector is the sum of the products of the corresponding elements from a row of the matrix and the vector.\n\n### Approach\n1. **Check for Empty Matrix or Vector**: If the matrix is empty or the vector is empty, return -1 as the operation is not possible.\n2. **Check Column Compatibility**: Determine the number of columns in the matrix and the length of the vector. If they do not match, return -1.\n3. **Compute Dot Product**: For each row in the matrix, compute the dot product with the vector. This involves iterating through each element of the row and the vector, multiplying corresponding elements, and summing the results.\n\n### Solution Code\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    if not a:\n        return -1\n    if not b:\n        return -1\n    cols = len(a[0])\n    len_b = len(b)\n    if cols != len_b:\n        return -1\n    result = []\n    for row in a:\n        dot = 0\n        for i in range(len_b):\n            dot += row[i] * b[i]\n        result.append(dot)\n    return result\n```\n\n### Explanation\n1. **Empty Check**: The function first checks if the matrix or the vector is empty. If either is empty, it returns -1.\n2. **Column Compatibility**: The number of columns in the matrix is determined by the length of the first row (assuming the matrix is non-empty). The length of the vector is checked against this number. If they do not match, the function returns -1.\n3. **Dot Product Calculation**: For each row in the matrix, the function computes the dot product with the vector. This is done by iterating through each element of the row and the vector, multiplying corresponding elements, and summing these products. The result is collected in a list which is returned as the final output.\n\nThis approach ensures that the function handles all edge cases, including empty matrices and vectors, and correctly computes the dot product when the dimensions are compatible."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that computes the transpose of a given matrix. The transpose of a matrix is obtained by converting its rows into columns and its columns into rows. This operation is fundamental in linear algebra and is widely used in various applications such as solving systems of linear equations and data manipulation.\n\n### Approach\n1. **Understand the Problem**: The transpose of a matrix is a new matrix whose rows are the columns of the original matrix. For example, if the original matrix is a 3x2 matrix, the transposed matrix will be a 2x3 matrix.\n2. **Determine Dimensions**: Calculate the number of rows and columns in the input matrix. The number of rows in the transposed matrix will be equal to the number of columns in the original matrix, and vice versa.\n3. **Construct the Transposed Matrix**: For each column index in the original matrix, create a new row in the transposed matrix by collecting the elements from each row at that column index.\n4. **Handle Edge Cases**: If the input matrix is empty or contains rows with varying lengths, the function should handle these cases gracefully, though the problem guarantees a proper matrix.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    if not a:\n        return []\n    rows = len(a)\n    cols = len(a[0])\n    transposed = []\n    for j in range(cols):\n        new_row = [a[i][j] for i in range(rows)]\n        transposed.append(new_row)\n    return transposed\n```\n\n### Explanation\n1. **Check for Empty Matrix**: If the input matrix `a` is empty, the function immediately returns an empty list.\n2. **Calculate Dimensions**: The number of rows is determined by `len(a)`, and the number of columns is determined by `len(a[0])` (assuming `a` is not empty).\n3. **Construct Transposed Matrix**: For each column index `j`, a new row is created by iterating over each row `i` in the original matrix and collecting the element at position `j`. This new row is then added to the transposed matrix.\n4. **Return Result**: The transposed matrix is returned as the result.\n\nThis approach ensures that the function efficiently computes the transpose by leveraging list comprehensions and iterating through each element, resulting in a clear and concise solution."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that reshapes a given matrix into a specified shape using numpy. If the reshaping is not possible, the function should return an empty list. The function should handle both integers and floating-point numbers in the matrix.\n\n### Approach\n1. **Convert Matrix to numpy Array**: Convert the input matrix into a numpy array to utilize numpy's reshaping capabilities.\n2. **Validate New Shape**: Check if the new shape is a tuple of two integers. If not, return an empty list.\n3. **Check Compatibility**: Verify if the product of the dimensions of the original matrix matches the product of the dimensions of the new shape. If not, return an empty list.\n4. **Reshape and Convert**: If the new shape is valid, reshape the numpy array and convert it back to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    a_np = np.array(a)\n    if not isinstance(new_shape, tuple) or len(new_shape) != 2 or not all(isinstance(x, int) for x in new_shape):\n        return []\n    current_shape = a_np.shape\n    if current_shape[0] * current_shape[1] != new_shape[0] * new_shape[1]:\n        return []\n    reshaped = a_np.reshape(new_shape)\n    return reshaped.tolist()\n```\n\n### Explanation\n1. **Convert Matrix to numpy Array**: The input matrix `a` is converted into a numpy array `a_np` to facilitate the reshaping process.\n2. **Validate New Shape**: The function checks if `new_shape` is a tuple of exactly two integers. If not, it returns an empty list.\n3. **Check Compatibility**: The function calculates the total number of elements in the original matrix and the new shape. If they do not match, it returns an empty list.\n4. **Reshape and Convert**: If the new shape is valid, the numpy array is reshaped using `reshape(new_shape)`, and the result is converted back to a list of lists using `tolist()`.\n\nThis approach ensures that the function handles all edge cases, including invalid new shapes and empty matrices, by returning an empty list when reshaping is not possible."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to write a function that calculates the mean of a matrix either by row or by column, based on a specified mode. The function should handle both scenarios efficiently and return the appropriate means in a list.\n\n### Approach\n1. **Understanding the Problem**: The task is to compute the mean of each row or each column of a given matrix based on the specified mode ('row' or 'column'). The matrix is a list of lists of floats, and the mode determines whether we compute the mean for each row or each column.\n\n2. **Handling Rows**: For each row in the matrix, the mean is computed by summing all the elements in the row and dividing by the number of elements in that row. This straightforward approach works for each row individually.\n\n3. **Handling Columns**: For each column, since the number of columns might not be known beforehand, we can transpose the matrix. Transposing converts columns into rows, allowing us to compute the mean of each transposed row (which was originally a column) using the same method as for rows.\n\n4. **Efficiency and Edge Cases**: The solution efficiently handles both row and column calculations using list comprehensions and built-in functions. It assumes the matrix is valid and non-empty, though edge cases like empty rows or columns are handled gracefully.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode"}
{"task_id": 5, "completion_id": 0, "completion": "To solve this problem, we need to write a function that multiplies each element of a given matrix by a scalar value and returns the resulting matrix. The function should handle various cases, including empty matrices and different types of elements (int or float).\n\n### Approach\n1. **Problem Analysis**: The task involves multiplying each element of a matrix by a scalar. This operation is straightforward and can be efficiently performed using nested list comprehensions.\n2. **Intuition**: For each row in the matrix, create a new row where each element is the product of the corresponding element in the original row and the scalar value. This can be achieved using list comprehensions within list comprehensions.\n3. **Algorithm Selection**: Using list comprehensions is efficient and concise for this problem. It allows us to iterate over each element of the matrix and apply the scalar multiplication in a clean manner.\n4. **Edge Cases**: The function should handle empty matrices gracefully, returning an empty matrix if the input is empty. It should also handle matrices with rows of varying lengths, though this is beyond the scope of the problem as each row is expected to have the same number of elements.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    return [[element * scalar for element in row] for row in matrix]\n```\n\n### Explanation\n- **Matrix Multiplication**: The function uses list comprehensions to iterate over each row of the matrix and then over each element in the row. Each element is multiplied by the scalar value.\n- **Efficiency**: The approach is efficient with a time complexity of O(n*m), where n is the number of rows and m is the number of columns in the matrix. This is optimal for this problem as it processes each element exactly once.\n- **Type Handling**: The function handles both integers and floats, ensuring that the result maintains the appropriate numeric type for each element.\n\nThis solution is both concise and efficient, leveraging Python's list comprehensions for clarity and performance."}
{"task_id": 6, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the eigenvalues of a given 2x2 matrix and return them sorted from highest to lowest. Eigenvalues are scalar values associated with a linear system of equations, and they are crucial in various fields of science and engineering.\n\n### Approach\n1. **Extract Matrix Elements**: The input matrix is a 2x2 matrix, so we extract the elements a, b, c, and d from the matrix.\n2. **Calculate Trace**: The trace of the matrix is the sum of the diagonal elements, i.e., a + d.\n3. **Calculate Determinant**: The determinant of the matrix is calculated as ad - bc.\n4. **Compute Discriminant**: The discriminant of the characteristic equation is given by the square of the trace minus four times the determinant.\n5. **Find Eigenvalues**: Using the quadratic formula, the eigenvalues are computed as (trace \u00b1 sqrt(discriminant)) / 2.\n6. **Sort Eigenvalues**: The eigenvalues are sorted in ascending order and then reversed to get them in descending order.\n\n### Solution Code\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    trace = a + d\n    det = a * d - b * c\n    discriminant = trace ** 2 - 4 * det\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalues = [(trace + sqrt_discriminant) / 2, (trace - sqrt_discriminant) / 2]\n    eigenvalues.sort()\n    eigenvalues = eigenvalues[::-1]\n    return eigenvalues\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The elements a, b, c, and d are extracted from the input matrix.\n2. **Trace Calculation**: The trace is the sum of the diagonal elements (a and d).\n3. **Determinant Calculation**: The determinant is calculated using the formula ad - bc.\n4. **Discriminant Calculation**: The discriminant is computed to determine the nature of the roots (real or complex).\n5. **Eigenvalue Calculation**: Using the quadratic formula, the eigenvalues are computed. This involves solving the characteristic equation derived from the matrix.\n6. **Sorting**: The eigenvalues are sorted in ascending order and then reversed to ensure they are returned from highest to lowest.\n\nThis approach efficiently computes the eigenvalues using basic linear algebra principles and ensures the results are returned in the required order."}
{"task_id": 7, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to transform a given matrix \\( A \\) using the operation \\( T^{-1}AS \\), where \\( T \\) and \\( S \\) are invertible matrices. The function should first validate if \\( T \\) and \\( S \\) are invertible and then perform the transformation. If either \\( T \\) or \\( S \\) is not invertible, the function should return -1.\n\n### Approach\n1. **Check Invertibility**: Verify if matrices \\( T \\) and \\( S \\) are square matrices. If either is not square, they cannot be invertible, so return -1.\n2. **Compute Inverses**: Use numpy's `linalg.inv` function to compute the inverses of \\( T \\) and \\( S \\). If either inverse does not exist (due to non-invertible matrices), return -1.\n3. **Matrix Multiplication**: Perform the matrix multiplications in the order \\( T^{-1} \\times A \\) and then the result with \\( S \\).\n4. **Round Results**: Round each element of the resulting matrix to four decimal places.\n5. **Return Result**: Convert the resulting matrix to a list of lists using `tolist()` and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if T is square\n    if len(T) != len(T[0]) or len(T) == 0:\n        return -1\n    # Check if S is square\n    if len(S) != len(S[0]) or len(S) == 0:\n        return -1\n    # Check if T is invertible\n    try:\n        T_inv = np.linalg.inv(T)\n    except np.linalg.LinAlgError:\n        return -1\n    # Check if S is invertible\n    try:\n        S_inv = np.linalg.inv(S)\n    except np.linalg.LinAlgError:\n        return -1\n    # Compute T inverse multiplied by A\n    AT = np.dot(T_inv, A)\n    # Compute AT multiplied by S\n    result = np.dot(AT, S)\n    # Round each element to 4 decimal places\n    rounded = np.round(result, 4)\n    # Convert to a list of lists\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Invertibility Check**: The function first checks if \\( T \\) and \\( S \\) are square matrices. If either is not square, it returns -1.\n2. **Matrix Inversion**: Using numpy's `linalg.inv` function, the inverses of \\( T \\) and \\( S \\) are computed. If any inverse does not exist, the function returns -1.\n3. **Matrix Multiplication**: The function performs the matrix multiplications in the order \\( T^{-1} \\times A \\) and then the result with \\( S \\).\n4. **Rounding**: Each element of the resulting matrix is rounded to four decimal places to ensure precision.\n5. **Return Result**: The resulting matrix is converted to a list of lists using `tolist()` and returned.\n\nThis approach ensures that the function handles all edge cases, including non-invertible matrices and incorrect dimensions, and returns the correct transformed matrix or -1 as required."}
{"task_id": 8, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the inverse of a 2x2 matrix if it exists. If the matrix is not invertible (i.e., its determinant is zero), the function should return `None`.\n\n### Approach\n1. **Extract Matrix Elements**: Extract the elements `a`, `b`, `c`, and `d` from the input matrix.\n2. **Calculate Determinant**: Compute the determinant of the matrix using the formula `det = a*d - b*c`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is not invertible, so return `None`.\n4. **Compute Inverse Matrix**: If the determinant is non-zero, compute each element of the inverse matrix using the formula `1/det` multiplied by the submatrix [[d, -b], [-c, a]].\n5. **Return Inverse Matrix**: Return the computed inverse matrix.\n\n### Solution Code\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_a = d / determinant\n    inv_b = -b / determinant\n    inv_c = -c / determinant\n    inv_d = a / determinant\n    return [[inv_a, inv_b], [inv_c, inv_d]]\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The elements `a`, `b`, `c`, and `d` are extracted from the input matrix. These represent the elements of the matrix [[a, b], [c, d]].\n2. **Calculate Determinant**: The determinant is calculated using the formula for a 2x2 matrix, which is `ad - bc`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is singular and not invertible, so the function returns `None`.\n4. **Compute Inverse Matrix**: If the determinant is non-zero, the inverse matrix elements are calculated using the formula for the inverse of a 2x2 matrix, which involves dividing each element of the submatrix [[d, -b], [-c, a]] by the determinant.\n5. **Return Inverse Matrix**: The computed inverse matrix is returned as the result.\n\nThis approach ensures that the function efficiently and correctly computes the inverse of a 2x2 matrix or identifies that the matrix is not invertible."}
{"task_id": 9, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies two matrices and returns the resulting matrix. If the matrices cannot be multiplied due to incompatible dimensions, the function should return -1.\n\n### Approach\n1. **Check for Valid Matrices**: First, we need to ensure that the matrices are valid. A valid matrix must have at least one row and one column. If either matrix is empty, the function should return -1.\n2. **Dimension Compatibility Check**: Matrix multiplication is only possible if the number of columns in the first matrix is equal to the number of rows in the second matrix. If this condition is not met, the function returns -1.\n3. **Initialize Result Matrix**: The result of multiplying two matrices will have dimensions equal to the number of rows of the first matrix and the number of columns of the second matrix. We initialize this result matrix with zeros.\n4. **Matrix Multiplication**: For each element in the result matrix, compute the dot product of the corresponding row from the first matrix and column from the second matrix.\n\n### Solution Code\n```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if either matrix is empty\n    if len(a) == 0 or len(b) == 0:\n        return -1\n    \n    # Check if the number of columns in the first matrix matches the number of rows in the second\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result matrix with zeros\n    m = len(a)\n    n = len(a[0])\n    p = len(b)\n    result = [[0 for _ in range(p)] for _ in range(m)]\n    \n    # Compute each element of the result matrix\n    for i in range(m):\n        for j in range(p):\n            total = 0\n            for k in range(n):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    \n    return result\n```\n\n### Explanation\n1. **Check for Empty Matrices**: The function first checks if either matrix is empty. If so, it returns -1.\n2. **Dimension Check**: It then checks if the number of columns in the first matrix matches the number of rows in the second matrix. If not, it returns -1.\n3. **Result Matrix Initialization**: The result matrix is initialized with zeros, having dimensions equal to the number of rows of the first matrix and the number of columns of the second matrix.\n4. **Matrix Multiplication**: For each element in the result matrix, the function computes the dot product of the corresponding row from the first matrix and column from the second matrix, summing the products of corresponding elements.\n\nThis approach ensures that the function efficiently handles matrix multiplication and returns the correct result or -1 if the matrices cannot be multiplied."}
{"task_id": 10, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the covariance matrix for a given set of vectors. The covariance matrix is a square matrix where each element (i, j) represents the covariance between the i-th and j-th features of the input vectors. \n\n### Approach\n1. **Compute the Mean for Each Feature**: The first step is to calculate the mean value for each feature (column) in the input vectors. This mean value will be used to center the data, which is necessary for computing the covariance.\n2. **Initialize the Covariance Matrix**: Create a matrix of the same size as the input vectors, initialized with zeros. This matrix will store the covariance values.\n3. **Calculate the Covariance for Each Pair of Features**: For each pair of features (i, j), compute the sum of the products of the deviations of each feature from their respective means. Divide this sum by the number of observations to get the covariance.\n\n### Solution Code\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n = len(vectors)\n    if n == 0:\n        return []\n    m = len(vectors[0])\n    if m == 0:\n        return []\n    \n    means = [sum(row) / n for row in vectors]\n    cov_matrix = [[0.0 for _ in range(m)] for _ in range(m)]\n    \n    for i in range(m):\n        for j in range(m):\n            sum_ij = 0.0\n            for k in range(n):\n                x_i = vectors[k][i]\n                x_j = vectors[k][j]\n                sum_ij += (x_i - means[i]) * (x_j - means[j])\n            cov_matrix[i][j] = sum_ij / n\n    \n    return cov_matrix\n```\n\n### Explanation\n1. **Compute the Mean**: For each feature, the mean is calculated by summing all the values in the feature and dividing by the number of observations. This mean is stored in a list where each element corresponds to a feature.\n2. **Initialize Covariance Matrix**: The covariance matrix is initialized as a list of lists, where each sublist corresponds to a feature, and each element in the sublist corresponds to another feature.\n3. **Calculate Covariance**: For each pair of features (i, j), the covariance is computed by iterating through each observation, calculating the product of the deviations from the mean for each feature, summing these products, and then dividing by the number of observations. This value is stored in the covariance matrix at position (i, j).\n\nThis approach ensures that we correctly compute the covariance matrix, taking into account the mean values of each feature to center the data, which is essential for accurate covariance calculations."}
{"task_id": 12, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the jacobian method. The SVD decomposes a matrix into three matrices: U, \u03a3, and V^T, where \u03a3 contains the singular values. The singular values are the square roots of the eigenvalues of the matrix A^T A.\n\n### Approach\n1. **Matrix Multiplication**: Compute the matrix product A^T A, which results in a 2x2 matrix.\n2. **Eigenvalues Calculation**: Calculate the eigenvalues of the matrix A^T A. The eigenvalues are found using the characteristic equation, which involves the trace and determinant of the matrix.\n3. **Singular Values Calculation**: The singular values are the square roots of the eigenvalues of A^T A.\n4. **Rounding**: Round the singular values to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    a, b = A[0, 0], A[0, 1]\n    c, d = A[1, 0], A[1, 1]\n    p = a**2 + b**2\n    r = c**2 + d**2\n    q = a * c + b * d\n    D = (p - r)**2 + 4 * q**2\n    sqrt_D = math.sqrt(D)\n    lambda1 = (p + r + sqrt_D) / 2\n    lambda2 = (p + r - sqrt_D) / 2\n    sv1 = math.sqrt(lambda1)\n    sv2 = math.sqrt(lambda2)\n    return (round(sv1, 4), round(sv2, 4))\n```\n\n### Explanation\n1. **Matrix Multiplication**: The matrix A^T A is computed by multiplying the transpose of A with A. This results in a 2x2 matrix.\n2. **Eigenvalues Calculation**: The eigenvalues of a 2x2 matrix can be found using the formula involving the trace and determinant. The trace is the sum of the diagonal elements, and the determinant is the product of the diagonal elements minus the product of the off-diagonal elements.\n3. **Singular Values Calculation**: The eigenvalues of A^T A are used to find the singular values by taking their square roots.\n4. **Rounding**: The singular values are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the SVD for a 2x2 matrix using basic linear algebra operations, ensuring accuracy and simplicity."}
{"task_id": 13, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. The determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the matrix. Laplace's Expansion allows us to compute the determinant by expanding along a row or column, which simplifies the problem into smaller subproblems.\n\n### Approach\n1. **Laplace's Expansion**: The determinant of a 4x4 matrix can be computed by expanding along the first row. Each element in the first row is multiplied by its corresponding cofactor, which involves computing the determinant of a 3x3 minor matrix.\n2. **Minor Matrix**: For each element in the first row, we form a minor matrix by removing the first row and the column containing the element. This minor matrix is then used to compute its determinant.\n3. **Cofactor Calculation**: The cofactor for each element is calculated as \\((-1)^{i+j}\\) where \\(i\\) is the row index and \\(j\\) is the column index. For the first row, \\(i = 0\\), so the sign alternates based on the column index.\n4. **Recursive Calculation**: The determinant of a 3x3 matrix is computed using the rule of Sarrus or the cofactor expansion, which is implemented recursively.\n\n### Solution Code\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def determinant_3x3(submatrix):\n        a, b, c = submatrix[0]\n        d, e, f = submatrix[1]\n        g, h, i = submatrix[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    \n    total = 0\n    for j in range(4):\n        minor = [row[:j] + row[j+1:] for row in matrix[1:]]\n        det_minor = determinant_3x3(minor)\n        sign = (-1) ** j\n        term = matrix[0][j] * sign * det_minor\n        total += term\n    return total\n```\n\n### Explanation\n1. **Helper Function**: The `determinant_3x3` function computes the determinant of a 3x3 matrix using the formula for the determinant of a 3x3 matrix.\n2. **Main Function**: The `determinant_4x4` function initializes the total determinant to 0. It then iterates over each element in the first row of the 4x4 matrix.\n3. **Minor Matrix Construction**: For each element in the first row, a minor matrix is constructed by removing the first row and the corresponding column.\n4. **Cofactor Calculation**: The sign for each element is calculated as \\((-1)^j\\), where \\(j\\) is the column index. The determinant of the minor matrix is then computed.\n5. **Term Calculation**: Each term is calculated by multiplying the element, its sign, and the determinant of the minor matrix. These terms are summed to get the total determinant.\n\nThis approach efficiently breaks down the problem into smaller subproblems, leveraging recursion to handle the computation of determinants for smaller matrices. The solution is both efficient and easy to understand, making it suitable for matrices of various sizes."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using the normal equation approach. The normal equation is a method to find the optimal coefficients for a linear regression model without explicitly computing the inverse of the matrix, which can be computationally expensive for large datasets.\n\n### Approach\n1. **Problem Analysis**: The task is to find the coefficients of a linear regression model using the normal equation. The normal equation is given by \\(\\theta = (X^T X)^{-1} X^T y\\), where \\(X\\) is the feature matrix and \\(y\\) is the target vector. This approach avoids the need for iterative optimization methods like gradient descent.\n\n2. **Intuition**: The normal equation provides a direct solution to the linear regression problem. By computing the transpose of \\(X\\), multiplying it with \\(X\\), inverting the resulting matrix, and then multiplying by the transpose of \\(X\\) and \\(y\\), we can find the coefficients \\(\\theta\\).\n\n3. **Algorithm Selection**: Using numpy for matrix operations is efficient and avoids issues with floating point precision. The steps involve converting the input lists to numpy arrays, computing the necessary matrices, and then applying the normal equation formula.\n\n4. **Edge Cases**: The function should handle cases where \\(X\\) is a single row or a matrix with multiple rows and columns. The normal equation should work as long as \\(X\\) is not singular, which is assumed in this problem.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X_transpose = X.T\n    A = X_transpose @ X\n    A_inv = np.linalg.inv(A)\n    b = X_transpose @ y\n    theta = A_inv @ b\n    theta = np.round(theta, 4)\n    return theta\n```\n\n### Explanation\n1. **Input Conversion**: The input lists are converted to numpy arrays to facilitate efficient matrix operations.\n2. **Matrix Transpose**: The feature matrix \\(X\\) is transposed to facilitate the multiplication \\(X^T X\\).\n3. **Matrix Multiplication**: The product \\(X^T X\\) is computed, resulting in matrix \\(A\\).\n4. **Matrix Inversion**: The inverse of matrix \\(A\\) is computed using numpy's `linalg.inv` function.\n5. **Vector Multiplication**: The product \\(X^T y\\) is computed, resulting in vector \\(b\\).\n6. **Theta Calculation**: The coefficients \\(\\theta\\) are obtained by multiplying the inverse of \\(A\\) with \\(b\\).\n7. **Rounding**: The resulting coefficients are rounded to four decimal places for the final output.\n\nThis approach ensures that we efficiently and accurately compute the coefficients for the linear regression model using the normal equation method."}
{"task_id": 15, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a linear regression model using gradient descent. The goal is to find the coefficients of the linear regression model that minimize the cost function. The function will take in the feature matrix, target values, learning rate, and number of iterations as inputs and return the coefficients rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start by initializing the coefficients (theta) to zero. The number of coefficients is equal to the number of features in the feature matrix, which includes an intercept term.\n2. **Gradient Descent Update**: For each iteration, compute the predictions using the current coefficients. Calculate the error between the predictions and the actual target values. Update the coefficients using the gradient of the cost function, scaled by the learning rate.\n3. **Cost Function**: The cost function is the mean squared error between the predicted values and the actual target values. The gradient of this function is used to update the coefficients.\n4. **Rounding**: After completing all iterations, round the coefficients to four decimal places and return them as a list.\n\n### Solution Code\n```python}python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    n = X.shape[1]\n    m = X.shape[0]\n    theta = np.zeros(n)\n    \n    for _ in range(iterations):\n        predictions = X @ theta\n        error = predictions - y\n        gradient = (X.T @ error) / m\n        theta -= alpha * gradient\n    \n    theta = np.round(theta, 4)\n    return theta.tolist()\n```\n\n### Explanation\n1. **Initialization**: The coefficients `theta` are initialized to zero. The number of coefficients is determined by the number of features in the feature matrix `X`, which includes an intercept term.\n2. **Gradient Descent Update**: For each iteration, the predictions are computed as the dot product of the feature matrix `X` and the coefficients `theta`. The error is the difference between these predictions and the actual target values `y`. The gradient is computed as the average of the outer product of the feature matrix and the error vector. The coefficients are updated by subtracting the product of the learning rate and the gradient.\n3. **Cost Function**: The cost function is the mean squared error, which is minimized by the gradient descent algorithm. The gradient is the average of the partial derivatives of the cost function with respect to each coefficient.\n4. **Rounding**: After completing the gradient descent process for the specified number of iterations, the coefficients are rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that we find the optimal coefficients for the linear regression model efficiently using gradient descent."}
{"task_id": 19, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Principal Component Analysis (PCA) function from scratch in Python. PCA is a dimensionality reduction technique that transforms the data into a new coordinate system where the first few dimensions capture the most variance in the data. This is achieved by computing the eigenvalues and eigenvectors of the covariance matrix of the data.\n\n### Approach\nThe approach to implement PCA involves the following steps:\n\n1. **Standardize the Data**: Compute the mean of each feature and subtract it from each feature to center the data around zero. This step is crucial because PCA is sensitive to the scale of the data.\n\n2. **Compute the Covariance Matrix**: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between the features.\n\n3. **Compute Eigenvalues and Eigenvectors**: Use the `numpy.linalg.eigh` function to compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues represent the variance explained by each principal component, and the eigenvectors are the directions of these principal components.\n\n4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order to determine the principal components in the order of the explained variance.\n\n5. **Select Top k Principal Components**: Retain only the top k eigenvectors corresponding to the largest eigenvalues. These eigenvectors form the new coordinate system.\n\n6. **Project Data onto Principal Components**: Transform the original data into the new coordinate system using the selected principal components.\n\n7. **Round the Results**: Round each element of the projected data to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Compute mean and subtract it\n    n_samples = data.shape[0]\n    n_features = data.shape[1]\n    mean = np.mean(data, axis=0)\n    data_centered = data - mean\n\n    # Step 2: Compute covariance matrix\n    try:\n        cov_matrix = np.cov(data_centered, rowvar=False)\n    except:\n        # Handle cases where data_centered is rank-deficient\n        pass\n\n    # Step 3: Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Step 4: Sort eigenvalues and eigenvectors\n    eigen_pairs = list(zip(eigenvalues, eigenvectors))\n    eigen_pairs.sort(key=lambda x: -x[0])\n\n    # Step 5: Take top k eigenvectors\n    principal_components = [vec for val, vec in eigen_pairs[:k]]\n\n    # Step 6: Project data onto principal components\n    projection = data_centered @ principal_components\n\n    # Step 7: Round each element to 4 decimal places\n    rounded_projection = []\n    for row in projection:\n        rounded_row = [round(x, 4) for x in row]\n        rounded_projection.append(rounded_row)\n\n    # Step 8: Return as a list of lists\n    return rounded_projection\n```\n\n### Explanation\n1. **Standardization**: The data is centered by subtracting the mean of each feature. This ensures that the mean of each feature in the transformed data is zero, which is a necessary condition for PCA.\n\n2. **Covariance Matrix**: The covariance matrix is computed to understand how each feature varies with respect to others. This matrix is crucial for determining the principal components.\n\n3. **Eigenvalues and Eigenvectors**: By computing the eigenvalues and eigenvectors of the covariance matrix, we determine the directions (principal components) and their corresponding variances (eigenvalues). The eigenvectors with the largest eigenvalues are the principal components that capture the most variance.\n\n4. **Sorting and Selection**: Eigenvalues are sorted in descending order, and the top k eigenvectors are selected. These eigenvectors form the new axes of the reduced-dimensional space.\n\n5. **Projection**: The original data is projected onto the selected principal components, transforming it into a lower-dimensional space while retaining the most variance.\n\n6. **Rounding**: The results are rounded to four decimal places to meet the problem's precision requirements.\n\nThis approach ensures that the data is transformed into a new coordinate system where the first few dimensions capture the most variance, making it easier to visualize and analyze."}
{"task_id": 22, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of the sigmoid activation function given an input value \\( z \\). The sigmoid function is a fundamental function in machine learning and neural networks, often used in binary classification problems. The function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). \n\n### Approach\nThe sigmoid function can be computed using the formula \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). The steps to compute this function are as follows:\n1. Compute the exponential of \\(-z\\), which is \\( e^{-z} \\).\n2. Add 1 to the result from step 1.\n3. Take the reciprocal of the result from step 2.\n4. Round the result to four decimal places.\n\nThis approach ensures that we accurately compute the sigmoid value and that the result is precise to four decimal places.\n\n### Solution Code\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    exp_z = math.exp(-z)\n    denominator = 1 + exp_z\n    result = 1 / denominator\n    return round(result, 4)\n```\n\n### Explanation\n1. **Import the Math Module**: The `math` module is imported to use the `exp` function, which computes the exponential of a number.\n2. **Compute Exponential of -z**: Using `math.exp(-z)`, we compute \\( e^{-z} \\) where \\( z \\) is the input.\n3. **Calculate Denominator**: Add 1 to the result of step 2 to get the denominator for the sigmoid function.\n4. **Compute Result**: Take the reciprocal of the denominator to get the sigmoid value.\n5. **Round Result**: The result is rounded to four decimal places using the `round` function to meet the problem's requirements.\n\nThis method efficiently computes the sigmoid function and ensures the result is precise and formatted correctly."}
{"task_id": 23, "completion_id": 0, "completion": "To solve this problem, we need to compute the softmax activation function for a given list of scores. The softmax function converts each score into a probability value between 0 and 1, which is useful in various machine learning applications for normalizing raw scores into probabilities.\n\n### Approach\nThe softmax function is defined as follows:\nFor each score \\( x_i \\) in the input list, the softmax value is computed as:\n\\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\]\nwhere \\( e \\) is the base of the natural logarithm, and the denominator is the sum of the exponentials of all scores.\n\nThe steps to compute the softmax function are as follows:\n1. Compute the exponential of each score in the input list.\n2. Sum all these exponential values to get the normalization denominator.\n3. Divide each exponential value by the sum to get the probability values.\n4. Round each probability value to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    exponentials = [math.exp(score) for score in scores]\n    sum_exp = sum(exponentials)\n    probabilities = [exp / sum_exp for exp in exponentials]\n    rounded = [round(prob, 4) for prob in probabilities]\n    return rounded\n```\n\n### Explanation\n1. **Input Handling**: The function takes a list of float values as input. If the list is empty, the function returns an empty list.\n2. **Exponential Calculation**: Using the `math.exp` function, each score in the list is exponentiated.\n3. **Sum of Exponentials**: The sum of all exponentiated values is computed to serve as the normalization denominator.\n4. **Probability Calculation**: Each exponentiated value is divided by the sum to get the probability value for that score.\n5. **Rounding**: Each probability value is rounded to four decimal places using Python's `round` function.\n6. **Return**: The function returns the list of rounded probability values.\n\nThis approach ensures that the scores are converted to probabilities in a normalized manner, which is essential for many machine learning applications."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single neuron model that can perform binary classification using a sigmoid activation function. The model will take a list of feature vectors, associated true binary labels, and the neuron's weights and bias as inputs. The goal is to compute the predicted probabilities after applying the sigmoid activation function and the mean squared error (MSE) between the predicted probabilities and the true labels, both rounded to four decimal places.\n\n### Approach\n1. **Compute Predictions**: For each feature vector, compute the dot product with the weights, add the bias, and apply the sigmoid activation function to get the predicted probability.\n2. **Compute MSE**: Calculate the MSE between the predicted probabilities and the true labels. The MSE is the average of the squared differences between each prediction and its corresponding label.\n3. **Round Results**: Round both the predicted probabilities and the MSE to four decimal places.\n4. **Return Results**: Return the rounded predicted probabilities as a list of floats and the rounded MSE as a float.\n\n### Solution Code\n```python\nimport math\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Compute the predictions\n    predictions = []\n    for feature in features:\n        z = sum(w * f for w, f in zip(weights, feature)) + bias\n        p = 1 / (1 + math.exp(-z))\n        predictions.append(p)\n    \n    # Compute the mean squared error\n    mae = 0.0\n    for p, y in zip(predictions, labels):\n        mae += (p - y) ** 2\n    mae /= len(labels)\n    mae_rounded = round(mae, 4)\n    \n    # Round the predictions to four decimal places\n    rounded_p = [round(p, 4) for p in predictions]\n    \n    return (rounded_p, mae_rounded)\n```\n\n### Explanation\n1. **Compute Predictions**: For each feature vector, the dot product of the weights and the feature vector is computed, plus the bias. This value is then passed through the sigmoid function to get the predicted probability.\n2. **Compute MSE**: The MSE is calculated by taking the average of the squared differences between each predicted probability and its corresponding label. This measures how well the predictions match the actual labels.\n3. **Round Results**: Both the predicted probabilities and the MSE are rounded to four decimal places to meet the problem's requirements.\n4. **Return Results**: The function returns a tuple where the first element is a list of rounded predicted probabilities and the second element is the rounded MSE.\n\nThis approach ensures that the model is efficiently and correctly implemented to handle binary classification tasks using a single neuron with sigmoid activation."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a basic autograd operations class that supports addition, multiplication, and ReLU (Rectified Linear Unit) operations. This class will handle scalar values and compute gradients for these operations through automatic differentiation.\n\n### Approach\nThe approach involves creating a `Value` class that can handle scalar values and perform basic arithmetic operations. Each operation will create a new `Value` instance, compute the new value and its gradient, and set up the necessary attributes for tracking dependencies and gradients. The class will use a function-based approach to compute gradients, which is essential for autograd operations.\n\n1. **Initialization**: The `Value` class is initialized with a data value, a gradient, a function for backpropagation (which is called when the value is used in a computation), a set of previous children, and an operation string.\n2. **Addition**: When two `Value` instances are added, a new `Value` is created with the sum of their data values. The gradient of the new value is the sum of the gradients of the operands.\n3. **Multiplication**: When two `Value` instances are multiplied, a new `Value` is created with the product of their data values. The gradient of the new value is the sum of the product of each operand's gradient with the other operand's data value.\n4. **ReLU Activation**: When a `Value` instance is passed through the ReLU activation function, a new `Value` is created with the data value thresholded at zero. The gradient is 1 if the data value is positive, otherwise 0.\n\n### Solution Code\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda self: None  # This is a placeholder for the actual computation\n        self._prev = set(_children)\n        self._op = _op\n\n    def add(self, other):\n        new_data = self.data + other.data\n        new_grad = self.grad + other.grad\n        new_value = Value(new_data, grad=new_grad)\n        new_value._backward = lambda a, b: a.grad + b.grad\n        new_value._prev = set(self._prev, other._prev)\n        new_value._op = 'add'\n        return new_value\n\n    def multiply(self, other):\n        new_data = self.data * other.data\n        new_grad = self.grad * other.data + other.grad * self.data\n        new_value = Value(new_data, grad=new_grad)\n        new_value._backward = lambda a, b: a.grad * b.data + b.grad * a.data\n        new_value._prev = set(self._prev, other._prev)\n        new_value._op = 'multiply'\n        return new_value\n\n    def relu(self):\n        new_data = self.data * (self.data > 0)\n        new_grad = 1 if self.data > 0 else 0\n        new_value = Value(new_data, grad=new_grad)\n        new_value._backward = lambda a: 1 if a.data > 0 else 0\n        new_value._prev = set(self._prev)\n        new_value._op = 'relu'\n        return new_value\n```\n\n### Explanation\n- **Initialization**: The `Value` class is initialized with a data value, a gradient, and a set of previous children. The operation string is used to track the type of operation performed.\n- **Addition**: The `add` method creates a new `Value` instance with the sum of the data values of the operands. The gradient of the new value is the sum of the gradients of the operands.\n- **Multiplication**: The `multiply` method creates a new `Value` instance with the product of the data values of the operands. The gradient is computed as the sum of the product of each operand's gradient with the other operand's data value.\n- **ReLU Activation**: The `relu` method creates a new `Value` instance with the data value thresholded at zero. The gradient is 1 if the data value is positive, otherwise 0.\n\nThis approach ensures that each operation correctly computes the gradient, enabling automatic differentiation and efficient computation of gradients for complex operations."}
{"task_id": 27, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transformation matrix from basis B to basis C for R^3. This matrix, P, allows us to convert vectors from the B basis to the C basis by multiplying P with the vector in B basis.\n\n### Approach\n1. **Understanding the Problem**: The transformation matrix P from basis B to C is such that if a vector v is represented in B, then in C it is given by v_C = P * v_B. To find P, we need to express each vector in C as a linear combination of the vectors in B.\n\n2. **Matrix Inversion**: For each vector in C, we can express it as a linear combination of the vectors in B. This can be done by solving a system of linear equations for each vector in C. The coefficients of these linear combinations form the columns of the transformation matrix P.\n\n3. **Matrix Operations**: Using numpy, we can efficiently compute the inverse of matrix B and then use this inverse to transform each vector in C into the corresponding columns of P.\n\n4. **Rounding**: Each element of the resulting matrix P is rounded to the nearest 4th decimal place to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_np = np.array(B)\n    C_np = np.array(C)\n    inv_B = np.linalg.inv(B_np)\n    P = np.zeros((3, 3), dtype=float)\n    for i in range(3):\n        c = C_np[i]\n        P[:, i] = np.dot(inv_B, c)\n    P = np.round(P, 4)\n    return P.tolist()\n```\n\n### Explanation\n1. **Convert Bases to NumPy Arrays**: The input bases B and C are converted into numpy arrays for efficient matrix operations.\n\n2. **Compute the Inverse of B**: The inverse of matrix B is computed using numpy's `linalg.inv` function. This inverse matrix is crucial for transforming vectors from B to C.\n\n3. **Construct the Transformation Matrix P**: For each vector in C, we compute the corresponding column in P by multiplying the inverse of B with the vector. This is done using numpy's matrix multiplication functionality.\n\n4. **Round the Result**: Each element of the resulting matrix P is rounded to four decimal places to ensure precision.\n\n5. **Return the Result**: The resulting matrix P is converted back to a list of lists of floats and returned.\n\nThis approach efficiently computes the transformation matrix using linear algebra operations, ensuring accuracy and performance."}
{"task_id": 28, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on eigendecomposition of \\( A^T A \\). The SVD will be returned as a tuple of three matrices: \\( U \\), \\( S \\), and \\( V \\) such that \\( A = U \\cdot S \\cdot V^T \\).\n\n### Approach\n1. **Compute \\( A^T A \\)**: This is a 2x2 matrix obtained by transposing \\( A \\) and then multiplying it with \\( A \\).\n2. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of \\( A^T A \\). Since \\( A^T A \\) is symmetric, its eigenvectors are orthogonal.\n3. **Construct \\( S \\)**: The singular values are the square roots of the eigenvalues. Form the diagonal matrix \\( S \\) using these singular values.\n4. **Construct \\( V \\)**: The matrix \\( V \\) is formed by the eigenvectors of \\( A^T A \\).\n5. **Construct \\( U \\)**: For each non-zero singular value, compute the corresponding left singular vector by normalizing \\( A \\) multiplied by the right singular vector. Form the matrix \\( U \\) using these normalized vectors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    A_T = A.T\n    A_T_A = A_T @ A\n\n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(A_T_A)\n    eigenvalues = np.sort(eigenvalues, reverse=True)\n\n    # Construct S\n    S = np.array([[np.sqrt(eigenvalues[0]), 0], [0, np.sqrt(eigenvalues[1])]])\n\n    # Construct V\n    V = eigenvectors\n\n    # Compute U\n    U = []\n    for i in range(2):\n        v_i = eigenvectors[:, i]\n        a_v_i = A @ v_i\n        norm = np.linalg.norm(a_v_i)\n        if norm == 0:\n            u_i = np.array([0.0, 0.0])\n        else:\n            u_i = a_v_i / norm\n        U.append(u_i)\n\n    # Convert U, S, V to numpy arrays and round to 4 decimal places\n    U = np.array(U).reshape(2, 2)\n    S = np.array(S).reshape(2, 2)\n    V = np.array(V).reshape(2, 2)\n    U = [round(x, 4) for x in U.flatten()]\n    S = [round(x, 4) for x in S.flatten()]\n    V = [round(x, 4) for x in V.flatten()]\n\n    return (U, S, V)\n```\n\n### Explanation\n1. **Matrix Transpose and Multiplication**: The matrix \\( A \\) is transposed and multiplied by itself to form \\( A^T A \\), which is a 2x2 matrix.\n2. **Eigenvalue and Eigenvector Calculation**: The eigenvalues and eigenvectors of \\( A^T A \\) are computed. These eigenvectors form the columns of matrix \\( V \\).\n3. **Singular Values**: The singular values are derived from the eigenvalues, which are the square roots of the eigenvalues of \\( A^T A \\).\n4. **Matrix \\( U \\)**: For each non-zero singular value, the corresponding left singular vector is computed by normalizing the product of \\( A \\) and the right singular vector. These normalized vectors form the columns of matrix \\( U \\).\n\nThis approach ensures that the SVD is computed efficiently and accurately for a 2x2 matrix, providing the required decomposition in the form of \\( U \\), \\( S \\), and \\( V \\)."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to write a function that takes two numpy arrays, X and y, and returns them in a random shuffled order while maintaining the corresponding order between them. The function should have an optional seed parameter for reproducibility.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Create a List of Tuples**: We start by creating a list of tuples where each tuple contains the index, X value, and corresponding Y value. This list helps us keep track of the original indices of the elements.\n\n2. **Shuffle the List**: Using numpy's `shuffle` function, we shuffle the list of tuples. The `shuffle` function modifies the list in place, so after shuffling, the order of the tuples is randomized.\n\n3. **Extract New Values**: After shuffling, we extract the new X and Y values from the shuffled list. This ensures that the elements are in a random order while maintaining their original pairing.\n\n4. **Convert to Lists**: Finally, we convert the shuffled X and Y arrays into lists using `tolist()` and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Create a list of tuples containing indices, X, and Y values\n    data = list(zip(np.arange(len(X)), X, y))\n    \n    # Set the seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Shuffle the list to ensure randomness\n    np.random.shuffle(data)\n    \n    # Extract the new X and Y values\n    new_X = [item[1] for item in data]\n    new_y = [item[2] for item in data]\n    \n    # Convert to numpy arrays and then to lists\n    new_X = np.array(new_X).tolist()\n    new_y = np.array(new_y).tolist()\n    \n    # Return as a list of arrays\n    return [new_X, new_y]\n```\n\n### Explanation\n- **List of Tuples**: The list `data` is created by zipping together an array of indices and the corresponding X and Y values. This allows us to"}
{"task_id": 31, "completion_id": 0, "completion": "To solve this problem, we need to write a function that divides a given dataset into two subsets based on a specified feature and a threshold value. The function will return these subsets as two separate numpy arrays.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Iterate Through Each Sample**: Loop through each row of the dataset to check the value of the specified feature against the given threshold.\n2. **Condition Check**: For each row, determine if the feature's value is greater than or equal to the threshold. If it is, the row is added to the condition subset; otherwise, it goes to the other subset.\n3. **Convert to numpy Arrays**: After collecting all the rows that meet the condition, convert these rows into a numpy array. The same process is repeated for the rows that do not meet the condition.\n4. **Return the Result**: Return the two subsets as a list of two numpy arrays.\n\nThis approach ensures that we efficiently split the dataset using vectorized operations, which are both concise and efficient.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    condition_subset = X[np.where(X[:, feature_i] >= threshold)[0]]\n    other_subset = X[np.where(X[:, feature_i] < threshold)[0]]\n    return [condition_subset, other_subset]\n```\n\n### Explanation\n- **Condition Check**: The code uses boolean indexing to efficiently check each feature value against the threshold. The `np.where` function is used to find indices where the condition is true, and these indices are used to extract the corresponding rows from the dataset.\n- **Efficiency**: By leveraging numpy's vectorized operations, the solution efficiently handles the dataset, ensuring that the operations are performed in a concise and computationally efficient manner.\n- **Return Value**: The result is returned as a list of two numpy arrays, where each array represents one of the subsets. The first element of the list is the subset of rows where the feature meets or exceeds the threshold, and the second element is the subset where the feature is below the threshold.\n\nThis approach ensures clarity, efficiency, and conciseness in handling the dataset and performing the required operations."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform one-hot encoding on a 1D numpy array to convert each unique value into a new column of binary values. The function should automatically determine the number of columns based on the input array's unique values.\n\n### Approach\n1. **Determine the Number of Columns**: If the number of columns (`n_col`) is not provided, it is automatically determined by the number of unique values in the input array. This is done using `np.unique` with `return_counts=True` to get the unique values and their counts.\n2. **Create One-Hot Encoded Rows**: For each value in the input array, create a row in the result where the corresponding column is set to 1 and all other columns are set to 0.\n3. **Return the Result**: Convert the result into a numpy array and then into a Python list using `tolist()` to meet the problem's requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        unique_values = np.unique(x, return_counts=True)[0]\n        n_col = len(unique_values)\n    # Create one-hot encoded rows\n    result = []\n    for value in x:\n        row = [0] * n_col\n        row[value] = 1\n        result.append(row)\n    # Convert to numpy array and then to a Python list\n    return np.array(result).tolist()\n```\n\n### Explanation\n- **Determine Columns**: The function first checks if `n_col` is None. If it is, it uses `np.unique` to find the unique values in the input array and sets `n_col` to the count of these unique values.\n- **One-Hot Encoding**: For each value in the input array, a new row is created in the result. This row has `n_col` elements, all initialized to 0, except for the position corresponding to the value, which is set to 1.\n- **Result Conversion**: The result is converted into a numpy array and then into a Python list using `tolist()` to ensure the output format is as required.\n\nThis approach efficiently handles both cases where the number of columns is provided or not, ensuring the correct one-hot encoding is applied."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this problem, we need to convert a 1D numpy array into a diagonal matrix. A diagonal matrix is a square matrix where all the elements except those on the main diagonal are zero. The elements on the main diagonal are the elements of the input array, and the rest of the elements are zeros.\n\n### Approach\n1. **Understanding the Problem**: Given a 1D numpy array, we need to create a 2D numpy array where the diagonal elements are the elements of the input array, and all other elements are zero.\n2. **Using numpy's diag Function**: The numpy function `np.diag` can be used to extract the diagonal elements from the input array and create a 2D array where these elements form the diagonal. For example, if the input array is [1, 2, 3], `np.diag([1, 2, 3])` will return a 2D array with 1, 2, and 3 on the main diagonal.\n3. **Converting to List of Lists**: The result from `np.diag` is a 2D numpy array. To convert this into a list of lists (a Python list), we use the `tolist()` method.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    return np.array(np.diag(x)).tolist()\n```\n\n### Explanation\n- **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n- **Extracting Diagonal Elements**: Using `np.diag(x)`, we get a 2D numpy array where the diagonal elements are the elements of `x`.\n- **Conversion to List**: The result from `np.diag(x)` is converted into a list of lists using `.tolist()`, which is then returned as the output.\n\nThis approach efficiently constructs the desired diagonal matrix using numpy functions, ensuring both correctness and performance."}
{"task_id": 36, "completion_id": 0, "completion": "To solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is a measure of how often the model's predictions match the true labels. This score is computed as the proportion of correct predictions out of the total number of predictions.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the accuracy score given two 1D numpy arrays, `y_true` (true labels) and `y_pred` (predicted labels). The accuracy is calculated by comparing each element of `y_true` with the corresponding element in `y_pred` and determining the proportion of correct predictions.\n\n2. **Intuition**: Accuracy is the ratio of correct predictions to the total number of predictions. This can be efficiently computed using numpy operations, which are optimized for performance.\n\n3. **Algorithm Selection**: We use numpy to compare the two arrays for equality. The sum of these comparisons gives the count of correct predictions. Dividing this count by the total number of predictions (length of the arrays) gives the accuracy. The result is then rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Comparison of Arrays**: The line `y_true == y_pred` creates a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n\n2. **Count of Correct Predictions**: The sum of this boolean array gives the number of correct predictions.\n\n3. **Total Predictions**: The length of the `y_true` array (or `y_pred`, since they are of the same length) gives the total number of predictions.\n\n4. **Accuracy Calculation**: The accuracy is the ratio of correct predictions to the total predictions. This ratio is then rounded to four decimal places using the `round` function"}
{"task_id": 37, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the correlation matrix for a given dataset. The correlation matrix is a 2D numpy array where each element represents the correlation coefficient between two variables. If the optional parameter `Y` is not provided, the function should calculate the correlation matrix of `X` with itself.\n\n### Approach\n1. **Check for Optional Parameter**: If the optional parameter `Y` is not provided, set `Y` to `X`.\n2. **Compute Covariance Matrix**: Use numpy's `np.cov` function to compute the covariance matrix between `X` and `Y`. If `Y` is provided, this will be the covariance matrix of `X` with itself if `Y` is `X`.\n3. **Compute Standard Deviations**: Calculate the standard deviations of each column in `X` and `Y` using numpy's `np.std` function.\n4. **Calculate Correlation Matrix**: For each pair of columns, compute the correlation coefficient using the formula (covariance / (product of standard deviations)). Handle cases where standard deviations are zero to avoid division by zero errors.\n5. **Round Results**: Round each element of the correlation matrix to four decimal places.\n6. **Convert to List**: Convert the numpy array result to a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    try:\n        cov_matrix = np.cov(X, Y)\n    except:\n        # Handle cases where X and Y have different shapes, though problem assumes same columns\n        pass\n    std_x = np.std(X, axis=0)\n    std_y = np.std(Y, axis=0)\n    correlation = np.zeros((X.shape[1], Y.shape[1]))\n    for i in range(X.shape[1]):\n        for j in range(Y.shape[1]):\n            if std_x[i] == 0 or std_y[j] == 0:\n                correlation[i][j] = 0.0\n            else:\n                correlation[i][j] = cov_matrix[i][j] / (std_x[i] * std_y[j])\n    rounded = np.round(correlation, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n- **Check for Optional Parameter**: The function first checks if `Y` is provided. If not, it sets `Y` to `X`.\n- **Compute Covariance Matrix**: Using `np.cov`, the covariance matrix is computed. This matrix contains the covariance between each pair of columns.\n- **Compute Standard Deviations**: The standard deviations of each column in `X` and `Y` are calculated to be used in the correlation formula.\n- **Calculate Correlation Matrix**: For each pair of columns, the correlation coefficient is computed. If the standard deviation of either column is zero, the correlation is set to zero to avoid division by zero.\n- **Round Results**: Each element of the correlation matrix is rounded to four decimal places.\n- **Convert to List**: The result is converted to a Python list for the final output.\n\nThis approach ensures that the correlation matrix is computed efficiently and correctly handles edge cases such as zero standard deviations."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the log-softmax function for a given 1D numpy array of scores. The log-softmax function is a numerically stable way to compute the softmax function, which is often used in machine learning for normalization of probabilities. The log-softmax function is particularly useful in neural networks for output layer activation functions.\n\n### Approach\nThe log-softmax function is computed as the logarithm of the softmax function. The softmax function converts a vector of scores into probabilities by exponentiating each score and normalizing the result. The log-softmax function then takes the logarithm of this normalized result, which is more numerically stable for large score values.\n\nThe steps to compute the log-softmax function are as follows:\n1. Compute the exponential of each score in the input array.\n2. Sum these exponentials to get the denominator of the softmax function.\n3. Subtract the logarithm of this sum from each of the exponentials to get the log-softmax values.\n4. Round each log-softmax value to four decimal places.\n5. Return the rounded values as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    exp_scores = np.exp(scores)\n    sum_exp = np.sum(exp_scores)\n    log_sof = scores - np.log(sum_exp)\n    rounded = [round(x, 4) for x in log_sof]\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Convert Input to Array**: The input list is converted to a numpy array to facilitate element-wise operations.\n2. **Exponentiate Scores**: Each score is exponentiated to compute the numerator of the softmax function.\n3. **Sum Exponentials**: The sum of these exponentiated values is computed to get the denominator of the softmax function.\n4. **Compute Log-Sum**: The logarithm of the sum of exponentials is subtracted from each exponentiated score to get the log-softmax values.\n5. **Round Values**: Each log-softmax value is rounded to four decimal places to ensure numerical stability and precision.\n6. **Return Result**: The rounded values are returned as a list.\n\nThis approach ensures that the computation is both efficient and numerically stable, making it suitable for handling large score values and avoiding overflow issues."}
{"task_id": 42, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function. The ReLU function is a commonly used activation function in neural networks due to its simplicity and effectiveness in introducing non-linearity to the network.\n\n### Approach\nThe ReLU function is defined such that it returns the input value if it is greater than zero, and returns zero otherwise. This function helps in introducing non-linearity in the network by allowing the output to be zero for negative inputs, which helps in preventing the vanishing gradient problem in neural networks.\n\nThe approach to implement the ReLU function is straightforward:\n1. Check if the input value is greater than zero.\n2. If the input value is greater than zero, return the input value as it is.\n3. If the input value is zero or negative, return zero.\n\nThis approach ensures that the function efficiently computes the output in constant time, O(1).\n\n### Solution Code\n```python\ndef relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0\n```\n\n### Explanation\nThe provided solution defines a function `relu` that takes a single float input `z` and returns the output of the ReLU activation function. The function checks if `z` is greater than zero. If it is, the function returns `z` as it is. If `z` is zero or negative, the function returns zero. This implementation efficiently handles all possible cases of the input value, ensuring correct computation of the ReLU function."}
{"task_id": 43, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates the Ridge Regression loss function. The function takes a feature matrix, coefficient vector, true labels, and a regularization parameter as inputs and returns the loss value rounded to four decimal places.\n\n### Approach\n\n1. **Problem Analysis**: \n   - The goal is to compute the Ridge Regression loss, which combines the Mean Squared Error (MSE) and a regularization term to prevent overfitting.\n   - The Ridge Regression loss function is given by:\n     \\[\n     \\text{Loss} = \\sum{(y_{\\text{pred}} - y_{\\text{true}})^2} + \\alpha \\sum{w_i^2}\n     \\]\n     where \\( y_{\\text{pred}} \\) is the predicted value, \\( y_{\\text{true}} \\) is the true label, \\( w \\) is the coefficient vector, and \\( \\alpha \\) is the regularization parameter.\n\n2. **Intuition**:\n   - The first term, \\( \\sum{(y_{\\text{pred}} - y_{\\text{true}})^2} \\), represents the MSE, which measures the average squared difference between predicted and true values.\n   - The second term, \\( \\alpha \\sum{w_i^2} \\), is the regularization term that penalizes large values in the coefficients vector \\( w \\).\n\n3. **Algorithm**:\n   - Compute the predicted values using matrix multiplication of the feature matrix \\( X \\) and the coefficient vector \\( w \\).\n   - Calculate the squared errors between the predicted values and the true labels.\n   - Sum these squared errors to get the MSE contribution.\n   - Compute the regularization term by summing the squares of the coefficients and multiplying by the regularization parameter \\( \\alpha"}
{"task_id": 44, "completion_id": 0, "completion": "To solve this problem, we need to implement a function called `leaky_relu` that computes the Leaky Rectified Linear Unit (Leaky ReLU) activation function. This function is a variation of the standard ReLU (Rectified Linear Unit) function, which is commonly used in neural networks.\n\n### Approach\nThe Leaky ReLU function is defined as:\n- `max(alpha * z, z)` where `alpha` is a small positive slope (default value is 0.01).\n- This means that for negative values of `z`, the function returns `z` (i.e., it does not reset negative values to zero), and for positive values, it returns `alpha * z`.\n\nThe approach to solve this problem involves:\n1. Checking if the input `z` is less than or equal to zero. If it is, the function returns `z` as is.\n2. If `z` is positive, the function returns `alpha * z`.\n\nThis approach ensures that the function behaves correctly for both positive and negative inputs, with the slope applied only to positive values.\n\n### Solution Code\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    if z <= 0:\n        return z\n    else:\n        return alpha * z\n```\n\n### Explanation\n- The function `leaky_relu` takes two parameters: `z` (a float) and `alpha` (an optional float with a default value of 0.01).\n- The function first checks if `z` is less than or equal to zero. If true, it returns `z` as it is, since the Leaky ReLU function does not reset negative values to zero.\n- If `z` is positive, the function returns `alpha * z`, which applies the Leaky ReLU slope to the input value.\n\nThis implementation efficiently handles both positive and negative values of `z`, ensuring that the output is correct according to the Leaky ReLU activation function's definition."}
{"task_id": 45, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function called `kernel_function` that computes the linear kernel between two input vectors `x1` and `x2`. The linear kernel is defined as the dot product of these two vectors.\n\n### Approach\nThe linear kernel is a fundamental concept in machine learning, particularly in algorithms like Support Vector Machines (SVM). The dot product of two vectors is calculated by summing the products of their corresponding elements. This can be efficiently computed using numpy's `dot` function, which handles both 1D and 2D arrays.\n\nThe steps to solve this problem are as follows:\n1. **Import numpy**: This library is used to compute the dot product efficiently.\n2. **Define the function**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are the input vectors.\n3. **Compute the dot product**: Use numpy's `dot` function to compute the dot product of `x1` and `x2` and return the result.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\nThe function `kernel_function` starts by importing the numpy library, which is necessary for efficient computation of the dot product. The function then takes two arguments, `x1` and `x2`, representing the input vectors. The dot product of these vectors is computed using numpy's `dot` function, which is both efficient and concise. The result of this computation is returned as the output of the function.\n\nThis approach ensures that the function works efficiently even for larger vectors, leveraging numpy's optimized internal functions for performance. The `dot` function handles both 1D and 2D arrays, making it versatile for various applications in machine learning and data analysis."}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the precision metric for a binary classification problem. Precision is a measure of the accuracy of positive predictions. It is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n\n### Approach\n1. **Identify True Positives (TP)**: These are the cases where both the true label and the predicted label are 1.\n2. **Identify False Positives (FP)**: These are the cases where the true label is 0 but the predicted label is 1.\n3. **Calculate Precision**: Precision is calculated as the ratio of true positives to the sum of true positives and false positives. If there are no false positives, precision is 1.0 to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = tp + fp\n    if denominator == 0:\n        return 1.0\n    else:\n        return tp / denominator\n```\n\n### Explanation\n1. **True Positives (TP)**: We use a boolean mask `(y_true == 1) & (y_pred == 1)` to find where both the true label and the predicted label are 1. Summing this boolean array gives the count of true positives.\n2. **False Positives (FP)**: Similarly, we use the mask `(y_true == 0) & (y_pred == 1)` to find where the true label is 0 but the predicted label is 1. Summing this gives the count of false positives.\n3. **Precision Calculation**: The precision is the ratio of true positives to the sum of true positives and false positives. If the sum of true positives and false positives is zero, it means there are no positive predictions, so we return 1.0 to indicate perfect precision in this case.\n\nThis approach efficiently computes the precision using vectorized operations provided by NumPy, ensuring optimal performance even for large datasets."}
{"task_id": 50, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Lasso Regression using Gradient Descent. Lasso Regression adds an L1 penalty to the loss function, which helps in shrinking some of the feature coefficients to zero, effectively performing feature selection. This is achieved through gradient descent, which iteratively updates the weights and bias to minimize the loss function.\n\n### Approach\n1. **Problem Analysis**: The objective function for Lasso Regression is given by:\n   \\[\n   J(w, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - (\\sum_{j=1}^p X_{ij} w_j + b))^2\n   \\]\n   where \\( y_i \\) is the actual value for the \\( i \\)-th sample, \\( \\hat{y_i} = \\sum_{j=1}^p X_{ij} w_j + b \\) is the predicted value, \\( w_j \\) are the weights, and \\( b \\) is the bias. The L1 penalty term is added to this loss function.\n\n2. **Gradient Calculation**: The gradient for each weight \\( w_j \\) and bias \\( b \\) is computed as:\n   - For weights: The gradient is the sum of the derivative of the squared error term plus the derivative of the L1 penalty term.\n   - For bias: The gradient is the sum of the derivative of the squared error term plus the derivative of the L1 penalty term.\n\n3. **Update Rule**: Using the computed gradients, the weights and bias are updated iteratively using the learning rate.\n\n4. **Efficient Computation**: Using vectorized operations, the gradients for all weights and bias are computed efficiently without explicit loops over each feature.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n = len(y)\n    m = X.shape[1]\n    weights = np.zeros(m)\n    bias = 0.0\n\n    for _ in range(max_iter):\n        error = y - (X @ weights + bias)\n        # Compute gradient for weights\n        grad_weights = (1.0 / n) * X.T @ error + alpha * np.sign(weights)\n        # Compute gradient for bias\n        grad_bias = (-1.0 / n) * np.sum(error) + alpha * np.sign(bias)\n        # Update weights and bias\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n\n    # Round to four decimal places\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights, bias)\n```\n\n### Explanation\n- **Initialization**: Weights and bias are initialized to zero.\n- **Error Calculation**: The error between the predicted values and the actual values is computed.\n- **Gradient Calculation**: The gradients for each weight and bias are computed using vectorized operations to ensure efficiency.\n- **Weight Update**: The weights and bias are updated using the computed gradients and the learning rate.\n- **Rounding**: The final weights and bias are rounded to four decimal places before returning.\n\nThis approach ensures that we efficiently compute the gradients and update the weights and bias iteratively, leading to the optimal solution for Lasso Regression using Gradient Descent."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Optimal String Alignment (OSA) distance between two given strings, `source` and `target`. The OSA distance is the minimum number of edits (insertions, deletions, substitutions, and transposes) required to transform one string into the other.\n\n### Approach\nThe approach to solve this problem involves using dynamic programming (DP) to efficiently compute the minimal edit distance. The key idea is to build a DP table where `dp[i][j]` represents the minimal cost to align the first `i` characters of `source` with the first `j` characters of `target`.\n\n1. **Initialization**: Create a DP table with dimensions `(len(source) + 1) x (len(target) + 1)`, initialized to zero.\n\n2. **Filling the DP Table**:\n   - For each character in `source` and `target`, compute the cost of aligning them by considering three options:\n     - **Substitution**: If the characters at the current positions are different, the cost is the sum of the previous diagonal value (aligning the previous characters) and 1.\n     - **Insertion**: If the character in `source` is different, the cost is the value from the previous row (aligning the previous characters of `source`).\n     - **Delete**: If the character in `target` is different, the cost is the value from the previous column (aligning the previous characters of `target`).\n   - Additionally, check if a transpose operation is possible, which is only valid when the current characters are the same as the next characters in the respective strings. If a transpose is possible, add 1 to the cost from the previous diagonal value.\n\n3. **Result Extraction**: The value at `dp[len(source)][len(target)]` will give the minimal edit distance.\n\n### Solution Code\n```python\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i-1] == target[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                option1 = dp[i-1][j-1] + 1\n                option2 = dp[i-1][j] + 1\n                option3 = dp[i][j-1] + 1\n                dp[i][j] = min(option1, option2, option3)\n            \n            # Check for transpose possibility\n            if source[i-1] == target[j-1] and source[i] == target[j]:\n                dp[i][j] = min(dp[i][j], dp[i-1][j-1] + 1)\n    \n    return dp[m][n]\n```\n\n### Explanation\n- **Initialization**: The DP table is initialized with zeros to represent the base case where no characters are processed.\n- **Filling the Table**: For each character in `source` and `target`, the code calculates the cost of aligning them by considering the three possible operations (substitution, insertion, delete). If a transpose is possible, it also checks if this operation would yield a lower cost.\n- **Result Extraction**: The value at the bottom-right corner of the DP table gives the minimal edit distance, which is the OSA distance between the two strings.\n\nThis approach ensures that we efficiently compute the minimal edit distance using dynamic programming, considering all possible operations and their costs."}
{"task_id": 52, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the recall metric for a binary classification model. Recall is a performance measure that evaluates how well a model identifies positive instances from all the actual positive cases in a dataset.\n\n### Approach\nThe recall metric is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). True positives are cases where the model correctly predicts a positive label, while false negatives are cases where the model incorrectly predicts a negative label when the actual label is positive.\n\nThe steps to compute recall are as follows:\n1. **Identify True Positives (TP):** This is the count of cases where both the true label and the predicted label are 1.\n2. **Identify False Negatives (FN):** This is the count of cases where the true label is 1 but the predicted label is 0.\n3. **Calculate Recall:** Divide the number of true positives by the sum of true positives and false negatives. If the sum of true positives and false negatives is zero, return 0.0 to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    tp = np.sum(y_true * y_pred)\n    fn = np.sum(y_true * (1 - y_pred))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        return tp / denominator\n```\n\n### Explanation\n1. **True Positives (TP):** This is calculated by taking the dot product of the true labels and the predicted labels. This gives a count of how many times both labels are 1.\n2. **False Negatives (FN):** This is calculated by taking the dot product of the true labels and the inverse of the predicted labels (i.e., 1 - predicted labels). This gives a count of how many times the true label is 1 but the predicted label is 0.\n3. **Recall Calculation:** The recall is the ratio of true positives to the sum of true positives and false negatives. If the sum of true positives and false negatives is zero, it means there are no positive cases, so the recall is set to 0.0.\n\nThis approach efficiently computes the recall using vectorized operations provided by NumPy, making the solution both concise and efficient."}
{"task_id": 54, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple RNN cell that processes a sequence of input vectors and produces the final hidden state. The RNN cell will maintain a hidden state that is updated at each time step based on the current input vector and the previous hidden state.\n\n### Approach\n1. **Initialization**: Start with the initial hidden state, which is provided as input.\n2. **Iterate through each input vector**: For each input vector in the sequence, compute the pre-activation input to the hidden layer.\n3. **Apply tanh activation**: Use the tanh activation function to transform the pre-activation input into the new hidden state.\n4. **Update hidden state**: The new hidden state becomes the previous hidden state for the next iteration.\n5. **Return the final hidden state**: After processing all input vectors, return the final hidden state, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h = np.array(initial_hidden_state)\n    for x in input_sequence:\n        pre_activation = np.dot(Wx, h) + b\n        h = np.tanh(pre_activation)\n    return [round(num, 4) for num in h]\n```\n\n### Explanation\n- **Initialization**: The initial hidden state is converted from a list to a numpy array for efficient computation.\n- **Processing each input vector**: For each input vector, the pre-activation input is computed by taking the dot product of the input vector and the weight matrix `Wx`, then adding the bias vector `b`.\n- **Activation function**: The tanh activation function is applied to the pre-activation input to get the new hidden state.\n- **Update hidden state**: The new hidden state is used as the input for the next iteration.\n- **Final output**: After processing all input vectors, the final hidden state is returned, rounded to four decimal places.\n\nThis approach ensures that each step of the RNN processing is efficiently computed using numpy for vector operations, making the implementation both concise and efficient."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to apply a 2D translation matrix to a list of points. The translation matrix will shift each point by a specified distance in the x and y directions. The solution involves converting the points into a numpy array, applying the translation using matrix multiplication, and converting the result back into a list of lists.\n\n### Approach\n1. **Convert Points to numpy Array**: Convert the list of points into a numpy array to facilitate matrix operations.\n2. **Create Translation Matrix**: Construct a 2x3 numpy array representing the translation. The first two rows are identity elements, and the third row contains the translation distances in the x and y directions.\n3. **Matrix Multiplication**: Multiply the points array by the translation matrix. This operation will shift each point by the specified tx and ty distances.\n4. **Convert Back to List**: Convert the resulting numpy array back into a list of lists to match the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    translation = np.array([[1, 0, tx], [0, 1, ty]])\n    points_np = np.array(points)\n    translated = np.dot(translation, points_np)\n    return translated.tolist()\n```\n\n### Explanation\n1. **Matrix Construction**: The translation matrix is constructed as a 2x3 array where the first two rows are identity matrices (1s on the diagonal and 0s elsewhere) and the third row contains the translation values tx and ty.\n2. **Point Conversion**: The input list of points is converted into a numpy array to allow for efficient matrix operations.\n3. **Matrix Multiplication**: The points array is multiplied by the translation matrix. This operation shifts each point by the specified tx and ty values.\n4. **Result Conversion**: The resulting numpy array is converted back into a list of lists, which is the required output format.\n\nThis approach efficiently handles the translation using numpy's matrix operations, ensuring that the solution is both concise and performant."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Gaussian Elimination with partial pivoting to transform a system of linear equations into an upper triangular matrix. Once the matrix is in upper triangular form, we can use back substitution to find the solution vector.\n\n### Approach\n1. **Form the Augmented Matrix**: Combine the coefficient matrix `A` and the constants vector `b` into an augmented matrix.\n2. **Partial Pivoting**: For each column, find the row with the maximum absolute value in the current column from the current row down to the last row. Swap the current row with this row to ensure the pivot element is the largest, which helps in reducing numerical errors.\n3. **Elimination**: Eliminate the elements below the pivot by subtracting a multiple of the pivot row from each of the rows below it. This step transforms the matrix into an upper triangular form.\n4. **Back Substitution**: Starting from the last row, solve for each variable using the values from the augmented matrix. This step uses the fact that the matrix is now upper triangular, allowing us to express each variable in terms of the next variable.\n\n### Solution Code\n```python\ndef gaussian_elimination(A, b):\n    # Create the augmented matrix\n    n = len(A[0])\n    augmented = []\n    for i in range(n):\n        row = A[i].copy()\n        row.append(b[i])\n        augmented.append(row)\n    \n    # Perform partial pivoting and elimination\n    for i in range(n):\n        # Find the pivot row\n        max_row = i\n        for j in range(i, n):\n            if abs(augmented[j][i]) > abs(augmented[max_row][i]):\n                max_row = j\n        # Swap rows i and max_row\n        augmented[i], augmented[max_row] = augmented[max_row], augmented[i]\n        \n        # Eliminate elements below the pivot\n        pivot = augmented[i][i]\n        for j in range(i + 1, n):\n            factor = augmented[j][i] / pivot\n            for k in range(i, n):\n                augmented[j][k] -= factor * augmented[i][k]\n    \n    # Back substitution\n    x = [0.0] * n\n    for i in range(n - 1, -1, -1):\n        x[i] = augmented[i][n] / augmented[i][i]\n    \n    # Round to four decimal places\n    x = [round(num, 4) for num in x]\n    \n    # Convert to a list and return\n    return x\n```\n\n### Explanation\n1. **Augmented Matrix Formation**: The augmented matrix is created by appending the constants vector `b` to the coefficient matrix `A`.\n2. **Partial Pivoting**: For each column, the pivot element is identified as the row with the maximum absolute value in that column. This helps in reducing numerical errors during elimination.\n3. **Elimination**: For each row below the pivot, the elements below the pivot are reduced to zero by subtracting a multiple of the pivot row from the current row. This transforms the matrix into an upper triangular form.\n4. **Back Substitution**: Starting from the last row, each variable is solved for using the values from the augmented matrix. This step leverages the upper triangular form to express each variable in terms of the next variable, allowing us to find the solution vector.\n\nThis approach ensures that we efficiently solve the system of linear equations using Gaussian Elimination with partial pivoting, resulting in an accurate and efficient solution."}
{"task_id": 60, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the TF-IDF (Term Frequency-Inverse Document Frequency) scores for a given query against a corpus of documents. The TF-IDF score for a term in a document is calculated using the TF and IDF formulas, which measure the frequency of the term in the document and its importance across all documents, respectively.\n\n### Approach\n1. **Handle Empty Corpus**: If the input corpus is empty, return an empty list immediately.\n2. **Compute Total Count**: Calculate the total number of words across all documents in the corpus.\n3. **Precompute Word Information**: For each word in the query, compute the count of occurrences in each document and determine how many documents contain the word.\n4. **Calculate Averages**: Compute the average TF across all documents and the average number of documents containing each word in the query.\n5. **Compute TF-IDF Scores**: For each document, compute the TF and TF-IDF scores for each word in the query using the precomputed values.\n\n### Solution Code\n```python\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    total_count = sum(len(doc) for doc in corpus)\n    word_list = []\n    for word in query:\n        count_in_doc = [doc.count(word) for doc in corpus]\n        df = len([doc for doc in corpus if word in doc])\n        word_list.append((count_in_doc, df))\n    average_TF = total_count / len(corpus)\n    sum_df = sum(df for count_in_doc, df in word_list)\n    average_df = sum_df / len(query)\n    N = len(corpus)\n    result = []\n    for doc in corpus:\n        scores = []\n        for i, (count_in_doc, df) in enumerate(word_list):\n            TF = count_in_doc / average_TF\n            TF_IDF = TF * math.log((N + 1) / (count_in_doc + 1))\n            scores.append(round(TF_IDF, 5))\n        result.append(scores)\n    return result\n```\n\n### Explanation\n1. **Handling Empty Corpus**: The function first checks if the corpus is empty and returns an empty list if true.\n2. **Total Word Count**: The total number of words across all documents is computed to determine the TF for each word.\n3. **Word Information**: For each word in the query, the function calculates how many times the word appears in each document (count_in_doc) and how many documents contain the word (df).\n4. **Averages Calculation**: The average TF across all documents and the average number of documents containing each word in the query are computed to avoid division by zero issues.\n5. **TF-IDF Calculation**: For each document, the TF for each word is calculated, and the TF-IDF score is computed using the logarithmic formula. The scores are then rounded to five decimal places and stored in the result.\n\nThis approach ensures that the TF-IDF scores are computed efficiently and correctly, handling edge cases such as empty documents and words not appearing in any documents."}
{"task_id": 61, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the F-Score for a binary classification task. The F-Score is a harmonic mean of Precision and Recall, and it provides a single metric that balances both. The score is influenced by a parameter `beta` which adjusts the weight between Precision and Recall.\n\n### Approach\n1. **Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)**:\n   - True Positives (TP) are the number of correct predictions where both the true label and predicted label are 1.\n   - False Positives (FP) are the number of incorrect predictions where the true label is 1 but the predicted label is 0.\n   - False Negatives (FN) are the number of incorrect predictions where the true label is 0 but the predicted label is 1.\n\n2. **Compute Precision and Recall**:\n   - Precision is the ratio of True Positives to the sum of True Positives and False Positives.\n   - Recall is the ratio of True Positives to the sum of True Positives and False Negatives.\n\n3. **Calculate F-Score**:\n   - The F-Score is computed using the formula: \\( F = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\).\n   - The parameter `beta` is used to adjust the weight between Precision and Recall. When `beta` is 1, the F-Score is the standard harmonic mean. When `beta` is not 1, it gives more weight to either Precision or Recall.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n    \n    :param y_true: numpy array of true labels (binary)\n    :param y_pred: numpy array of predicted labels (binary)\n    :param beta: float value that adjusts the importance of Precision and Recall\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum(y_true * y_pred)\n    FP = np.sum((1 - y_true) * y_pred)\n    FN = np.sum(y_true * (1 - y_pred))\n    \n    # Compute Precision and Recall\n    p = TP / (TP + FP) if (TP + FP) != 0 else 1.0\n    r = TP / (TP + FN) if (TP + FN) != 0 else 1.0\n    \n    # Compute F-Score\n    f_score_value = (2 * p * r) / (p + r)\n    \n    # Round to three decimal places\n    return round(f_score_value, 3)\n```\n\n### Explanation\n- **True Positives (TP)**: Counted where both the true label and predicted label are 1.\n- **False Positives (FP)**: Counted where the true label is 1 but the predicted label is 0.\n- **False Negatives (FN)**: Counted where the true label is 0 but the predicted label is 1.\n- **Precision**: Measures the proportion of correctly predicted positive values.\n- **Recall**: Measures the proportion of correctly predicted negative values.\n- **F-Score**: Balances Precision and Recall using the parameter `beta` to give more weight to either metric, providing a single score that represents the model's performance.\n\nThis approach ensures that we accurately measure the model's performance using the F-Score, which is a robust metric for binary classification tasks."}
{"task_id": 64, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini Impurity for a set of class labels. Gini Impurity is a measure used in decision tree algorithms to assess the quality of a split. It helps in determining how well a given split can separate the data into classes.\n\n### Approach\nThe Gini Impurity is calculated using the formula:\n\\[ Gini = 1 - \\sum \\left( \\frac{p_i^2}{n} \\right) \\]\nwhere \\( p_i \\) is the proportion of samples in class \\( i \\) and \\( n \\) is the total number of samples.\n\n1. **Count Frequencies**: Use a dictionary to count the occurrences of each class label in the input list.\n2. **Calculate Proport"}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given dense matrix into its Compressed Row Sparse (CSR) format. The CSR format is an efficient way to store sparse matrices, which are matrices with a large number of zero elements. The CSR format uses three lists: values, column indices, and row pointers, to store the non-zero elements and their positions efficiently.\n\n### Approach\n1. **Initialization**: Start by initializing three lists: `values` to store all non-zero elements, `column_indices` to store the column indices of these elements, and `row_pointers` to store the starting index of each row in the `values` list. The `row_pointers` list starts with a single zero to indicate the starting index of the first row.\n\n2. **Iterate through each row**: For each row in the dense matrix, iterate through each element to check if it is non-zero.\n\n3. **Collect non-zero elements**: For each non-zero element, append its value to the `values` list and record its column index in the `column_indices` list.\n\n4. **Update row pointers**: After processing each row, update the `row_pointers` list to reflect the starting index of the next row. This is done by appending the current length of the `values` list to `row_pointers`.\n\n### Solution Code\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    \n    for row in dense_matrix:\n        for col, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col)\n        row_pointers.append(len(values))\n    \n    return (values, column_indices, row_pointers)\n```\n\n### Explanation\n- **Initialization**: The `row_pointers` list starts with `[0]` to indicate that the first row starts at index 0.\n- **Row Processing**: For each row, iterate through each element. If the element is non-zero, add it to `values` and record its column index in `column_indices`.\n- **Row Pointer Update**: After processing each row, update `row_pointers` to reflect the current length of `values`, which indicates the starting index of the next row.\n\nThis approach efficiently stores the non-zero elements of the matrix in a compressed format, making it suitable for large sparse matrices where most elements are zero."}
{"task_id": 66, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the orthogonal projection of a vector `v` onto another vector `L`, which defines the direction of the line onto which we are projecting. The result should be a vector on the line `L` that is closest to `v`.\n\n### Approach\nThe orthogonal projection of a vector `v` onto a line defined by vector `L` can be calculated using the formula for the projection of a vector onto another vector. The formula involves the dot product of `v` and `L`, and the dot product of `L` with itself. Here are the steps:\n\n1. **Compute the Dot Product**: Calculate the dot product of vectors `v` and `L`.\n2. **Compute the Squared Norm of L**: Calculate the dot product of `L` with itself, which gives the squared norm of `L`.\n3. **Compute the Scalar Projection**: Divide the dot product of `v` and `L` by the squared norm of `L` to get the scalar projection.\n4. **Compute the Projection Vector**: Multiply the scalar projection by each element of `L` to get the projection vector.\n5. **Round the Result**: Round the resulting projection vector to three decimal places.\n\n### Solution Code\n```python\ndef orthogonal_projection(v, L):\n    # Compute the dot product of v and L\n    dot_vL = sum(v[i] * L[i] for i in range(len(v)))\n    # Compute the squared norm of L\n    dot_LL = sum(L[i] * L[i] for i in range(len(L)))\n    # Compute the scalar projection\n    scalar = dot_vL / dot_LL\n    # Compute the projection vector\n    projection = [scalar * L[i] for i in range(len(L))]\n    # Round to three decimal places\n    projection_rounded = [round(x, 3) for x in projection]\n    return projection_rounded\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of `v` and `L` is computed by summing the products of corresponding elements of `v` and `L`.\n2. **Squared Norm of L**: The squared norm of `L` is computed by summing the squares of its elements.\n3. **Scalar Projection**: The scalar projection is obtained by dividing the dot product of `v` and `L` by the squared norm of `L`.\n4. **Projection Vector**: The projection vector is obtained by multiplying the scalar projection with each element of `L`.\n5. **Rounding**: The resulting projection vector is rounded to three decimal places to meet the problem's requirements.\n\nThis approach ensures that we accurately compute the orthogonal projection using vector operations, which are both efficient and straightforward."}
{"task_id": 67, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a dense matrix into its Compressed Column Sparse (CSC) format. The CSC format efficiently stores sparse matrices by only storing non-zero elements, which is particularly useful for matrices with a high number of zero elements.\n\n### Approach\nThe approach to convert a dense matrix into its CSC format involves the following steps:\n\n1. **Initialization**: Determine the number of rows and columns in the dense matrix. Handle edge cases where the matrix might be empty.\n2. **Data Collection**: For each column, collect all non-zero elements along with their row indices.\n3. **Index Management**: Maintain lists to store the values, row indices, and column pointers. The column pointers indicate the starting index of each column's non-zero elements in the values list.\n4. **Efficient Storage**: Use the collected values and row indices to construct the CSC format efficiently.\n\n### Solution Code\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n    Returns a tuple of (values, row_indices, column_ptr)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ( [], [], [] )\n    \n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    column_ptr = [0] * cols\n    \n    for c in range(cols):\n        col_values = []\n        for r in range(rows):\n            if dense_matrix[r][c] != 0:\n                col_values.append((dense_matrix[r][c], r))\n        for val, row in col_values:\n            values.append(val)\n            row_indices.append(row)\n        current_ptr = 0\n        for i in range(c):\n            current_ptr += len(row_indices[i])\n        column_ptr[c] = current_ptr\n    \n    return (values, row_indices, column_ptr)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if the input matrix is empty. If it is, it returns empty lists for values, row indices, and column pointers.\n2. **Data Collection**: For each column, the function iterates through each row to collect non-zero elements. Each non-zero element is stored along with its row index.\n3. **Index Management**: After collecting non-zero elements for a column, the function calculates the starting index for the column's non-zero elements in the values list. This is done by summing the lengths of the row indices lists of all previous columns.\n4. **Efficient Storage**: The values, row indices, and column pointers are collected into their respective lists, which are then returned as a tuple.\n\nThis approach ensures that the CSC format efficiently stores the non-zero elements of the matrix, making it suitable for large sparse matrices."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the R-squared (R\u00b2) value for a regression model given true values `y_true` and predicted values `y_pred`. The R-squared value measures how well the independent variables explain the variability of the dependent variable.\n\n### Approach\nThe R-squared value is calculated using the formula:\n\\[ R^2 = 1 - \\frac{\\text{Sum of Squared Residuals}}{\\text{Total Sum of Squares}} \\]\n\nWhere:\n- **Sum of Squared Residuals** is the sum of the squared differences between the true values and the predicted values.\n- **Total Sum of Squares** is the sum of the squared differences between each true value and the mean of the true values.\n\nSteps to compute R-squared:\n1. Calculate the mean of the true values `y_true`.\n2. Compute the total sum of squares (TSS) as the sum of the squared differences between each true value and the mean.\n3. Compute the sum of squared residuals (SSR) as the sum of the squared differences between each true value and the predicted values.\n4. If the total sum of squares (TSS) is zero, return 1.0 (since all values are the same, the model explains all variance).\n5. Otherwise, compute R-squared as \\( 1 - \\frac{SSR}{TSS} \\) and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    mean_y_true = np.mean(y_true)\n    sum_total_sq = np.sum((y_true - mean_y_true) ** 2)\n    sum_sq_resid = np.sum((y_true - y_pred) ** 2)\n    \n    if sum_total_sq == 0:\n        return 1.0\n    else:\n        r_squared_value = 1 - (sum_sq_resid / sum_total_sq)\n        return round(r_squared_value, 3)\n```\n\n### Explanation\n- **Mean Calculation**: The mean of the true values is computed to determine the baseline prediction.\n- **Total Sum of Squares (TSS)**: This measures the total variance in the true values around their mean.\n- **Sum of Squared Residuals (SSR)**: This measures the variance in the true values that is not explained by the model.\n- **R-squared Calculation**: The formula \\( R^2 = 1 - \\frac{SSR}{TSS} \\) adjusts the SSR relative to the total variance, giving a value between 0 and 1. If all values are the same, TSS is zero, and R-squared is 1.0.\n\nThis approach efficiently computes the R-squared value using vectorized operations provided by NumPy, ensuring both accuracy and performance."}
{"task_id": 70, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the average brightness of a grayscale image represented as a 2D matrix. The function should handle edge cases such as an empty image, inconsistent row lengths, and invalid pixel values.\n\n### Approach\n1. **Check for Empty Image**: If the image matrix is empty, return -1.\n2. **Check Row Consistency**: Ensure all rows have the same length. If any row has a different length, return -1.\n3. **Validate Pixel Values**: Each pixel value must be within the valid range (0-255). If any pixel is outside this range, return -1.\n4. **Calculate Brightness**: If all checks pass, compute the sum of all pixel values, determine the total number of pixels, and then calculate the average brightness. Round the result to two decimal places.\n\n### Solution Code\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img:\n        return -1\n    \n    # Check if all rows have the same length\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    # Check each pixel is within the valid range\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    \n    # Calculate the total number of pixels\n    total_pixels = len(img) * len(img[0])\n    sum_pixels = 0\n    \n    # Sum all pixel values\n    for row in img:\n        sum_pixels += sum(row)\n    \n    # Calculate the average brightness\n    average = sum_pixels / total_pixels\n    \n    # Return the average rounded to two decimal places\n    return round(average, 2)\n```\n\n### Explanation\n1. **Empty Image Check**: The function first checks if the image matrix is empty. If it is, the function returns -1.\n2. **Row Consistency Check**: The function then checks if all rows have the same length. If any row has a different length, the function returns -1.\n3. **Pixel Value Validation**: Each pixel value is checked to ensure it falls within the valid range (0-255). If any pixel is outside this range, the function returns -1.\n4. **Brightness Calculation**: If all checks pass, the function calculates the total number of pixels, sums all the pixel values, computes the average, and returns the result rounded to two decimal places.\n\nThis approach ensures that all edge cases are handled efficiently and the function returns the correct average brightness or -1 if any invalid conditions are encountered."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Root Mean Square Error (RMSE) between two arrays of actual and predicted values. The RMSE is a common metric used to evaluate the accuracy of regression models, providing a measure of the standard deviation of the residuals. \n\n### Approach\nThe approach to calculate the RMSE involves the following steps:\n\n1. **Check Input Validity**: Ensure that both inputs are numpy arrays. If not, return `None`.\n2. **Check Array Compatibility**: Verify that the two arrays have the same length. If not, return `None`.\n3. **Compute Squared Differences**: Calculate the squared differences between each corresponding element of the two arrays.\n4. **Compute Mean of Squared Differences**: Find the mean of these squared differences.\n5. **Compute Square Root**: Take the square root of the mean to get the RMSE.\n6. **Round the Result**: Round the final RMSE value to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if both inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        return None\n    # Check if the shapes are compatible\n    if y_true.shape != y_pred.shape:\n        return None\n    # Calculate squared differences\n    squared_diff = (y_true - y_pred) ** 2\n    # Compute the mean of squared differences\n    mean = np.mean(squared_diff)\n    # Compute the square root to get RMSE\n    rmse_value = np.sqrt(mean)\n    # Round to three decimal places\n    return round(rmse_value, 3)\n```\n\n### Explanation\n1. **Input Validity Check**: The function first checks if both `y_true` and `y_pred` are numpy arrays. If either is not, the function returns `None`.\n2. **Array Compatibility Check**: The function then checks if the two arrays have the same length. If they do not, the function returns `None`.\n3. **Squared Differences Calculation**: The squared differences between each element of `y_true` and `y_pred` are computed using element-wise subtraction and squaring.\n4. **Mean Calculation**: The mean of these squared differences is computed using `np.mean()`.\n5. **Square Root Calculation**: The square root of the mean is taken to compute the RMSE.\n6. **Rounding**: The result is rounded to three decimal places using `round()` and returned.\n\nThis approach ensures that the function handles various edge cases, including mismatched array shapes, invalid input types, and empty arrays, providing a robust solution for calculating RMSE."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Jaccard Index for binary classification tasks. The Jaccard Index is a measure of similarity between two sets and is widely used in binary classification to evaluate the overlap between predicted and true labels.\n\n### Approach\nThe Jaccard Index is defined as the ratio of the number of elements in the intersection of two sets to the number of elements in their union. For binary classification, this translates to the number of positions where both the true and predicted labels are 1 (intersection) divided by the total number of positions where either the true or predicted label is 1 (union).\n\nThe steps to compute the Jaccard Index are as follows:\n1. **Calculate the Intersection**: This is the count of positions where both the true and predicted labels are 1.\n2. **Calculate the Union**: This is the count of positions where either the true or predicted label is 1. It is computed as the sum of the counts of 1s in both arrays minus the count of positions where both are 1 (to avoid double-counting).\n3. **Handle Edge Cases**: If the union is zero (i.e., both arrays are entirely zeros), the Jaccard Index is 1.0.\n4. **Compute the Index**: If the union is non-zero, compute the ratio of the intersection to the union and round it to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    union = sum_true + sum_pred - intersection\n    if union == 0:\n        return 1.0\n    else:\n        return intersection / union\n```\n\n### Explanation\n- **Intersection Calculation**: Using `np.sum(y_true * y_pred)`, we count the number of positions where both `y_true` and `y_pred` are 1.\n- **Union Calculation**: Using `sum_true + sum_pred - intersection`, we compute the total number of positions where either `y_true` or `y_pred` is 1, ensuring we do not double-count the positions where both are 1.\n- **Edge Case Handling**: If the union is zero, it means both arrays are entirely zeros, so the Jaccard Index is 1.0.\n- **Index Calculation**: The Jaccard Index is the ratio of the intersection to the union, rounded to three decimal places.\n\nThis approach efficiently handles the computation using numpy for vectorized operations, ensuring optimal performance even for larger arrays."}
{"task_id": 73, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Dice Score between two binary classification arrays. The Dice Score is a measure of similarity between two sets and is particularly useful in tasks like image segmentation and binary classification. It ranges from 0 (no overlap) to 1 (perfect overlap).\n\n### Approach\n1. **Calculate Intersection**: The intersection of two arrays is the count of elements that are 1 in both arrays. This can be efficiently computed using the logical AND operation followed by summation.\n2. **Compute Sums**: Calculate the sum of elements in `y_true` and `y_pred`. These sums will be used to compute the denominator of the Dice Score formula.\n3. **Handle Edge Cases**: If the sum of `y_true` or `y_pred` is zero, the Dice Score is zero because there are no elements to compare. If both sums are zero,"}
{"task_id": 74, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). Each feature in the dataset is represented by a hypervector, and these hypervectors are combined into a single composite hypervector.\n\n### Approach\n1. **Extract Features and Hypervectors**: For each feature in the dataset row, retrieve the corresponding hypervector using the provided seed from `random_seeds`.\n2. **Generate Hypervectors**: For each feature, generate a hypervector of dimension `dim` using the corresponding seed. This is done using a random number generator with the seed to ensure reproducibility.\n3. **Concatenate Hypervectors**: Combine all the generated hypervectors into a single composite hypervector by concatenating them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    feature_hv_list = []\n    for feature, value in row.items():\n        seed = random_seeds[feature]\n        # Generate a hypervector of dimension 'dim' using the seed\n        hv = np.random.default_rng(seed=seed).random(dim)\n        feature_hv_list.append hv\n    # Concatenate all feature hypervectors to form the composite hypervector\n    composite_hv = np.concatenate(feature_hv_list)\n    return composite_hv.tolist()\n```\n\n### Explanation\n- **Extract Features and Hypervectors**: The function iterates over each feature in the dataset row, using the corresponding seed from `random_seeds` to generate the hypervector for that feature.\n- **Generate Hypervectors**: For each feature, a hypervector of dimension `dim` is generated using a random number generator initialized with the feature's seed. This ensures that each hypervector is reproducible.\n- **Concatenate Hypervectors**: The hypervectors of all features are concatenated into a single list, which forms the composite hypervector. This composite hypervector represents the entire dataset row in a high-dimensional space.\n\nThis approach ensures that the composite hypervector accurately reflects the dataset row by combining hypervectors of each feature, leveraging the properties of HDC for efficient and reproducible data representation."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to generate a confusion matrix for a binary classification problem. The confusion matrix provides a visual summary of the performance of a classification model, showing the number of correct and incorrect predictions made by the model.\n\n### Approach\nThe approach to generate the confusion matrix involves the following steps:\n\n1. **Initialization**: Create a 2x2 matrix initialized with zeros. This matrix will represent the counts of correct and incorrect predictions.\n2. **Iteration**: Loop through each pair of actual and predicted labels provided in the input data.\n3. **Update Matrix**: For each pair, determine if the prediction is correct or incorrect. If the prediction matches the actual label, increment the corresponding cell in the matrix. If not, increment the cell where the actual label is on the row and the predicted label is on the"}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the cosine similarity between two vectors. Cosine similarity is a measure that indicates the degree of similarity between two vectors by examining the angle between them. The cosine similarity is computed using the dot product of the vectors divided by the product of their magnitudes.\n\n### Approach\n1. **Dot Product Calculation**: The dot product of two vectors is computed using `np.dot(v1, v2)`, which gives the sum of the products of the corresponding entries of the two sequences of vectors.\n2. **Magnitude Calculation**: The magnitude (or norm) of each vector is computed using `np.linalg.norm()`, which gives the Euclidean norm of the vector.\n3. **Cosine Similarity Formula**: The cosine similarity is calculated by dividing the dot product by the product of the magnitudes of the two vectors.\n4. **Rounding**: The result is rounded to three decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)\n```\n\n### Explanation\n- **Dot Product**: The dot product of two vectors is calculated using `np.dot(v1, v2)`, which efficiently computes the sum of the products of the corresponding entries of the vectors.\n- **Magnitudes**: The magnitude of each vector is found using `np.linalg.norm()`, which computes the Euclidean norm (square root of the sum of the squares of the vector components).\n- **Cosine Similarity**: The cosine similarity is derived by dividing the dot product by the product of the magnitudes of the vectors. This gives a value between -1 and 1, where 1 indicates identical vectors, -1 indicates exactly opposite vectors, and 0 indicates orthogonality.\n- **Rounding**: The result is rounded to three decimal places using `round()`, ensuring the output meets the specified precision.\n\nThis approach efficiently computes the cosine similarity using vector operations provided by NumPy, ensuring both accuracy and performance."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the performance of a binary classification model using two lists: the actual class labels and the predicted class labels. The goal is to compute various performance metrics such as accuracy, F1 score, Specificity, and Negative Predictive Value.\n\n### Approach\n1. **Confusion Matrix Calculation**: We first construct a 2x2 confusion matrix to count the number of correct and incorrect predictions. The matrix is defined as:\n   - True Positives (TP): Correctly predicted positive instances.\n   - False Positives (FP): Incorrectly predicted positive instances.\n   - True Negatives (TN): Correctly predicted negative instances.\n   - False Negatives (FN): Incorrectly predicted negative instances.\n\n2. **Metrics Calculation**:\n   - **Accuracy**: The proportion of correct predictions out of total predictions.\n   - **F1 Score**: A measure that combines precision and recall, providing a balanced measure of a model's performance.\n   - **Specificity**: The proportion of actual negative instances that are correctly identified.\n   - **Negative Predictive Value (NPV)**: The proportion of actual negative instances that are correctly identified.\n\n3. **Rounding**: Each metric is rounded to three decimal places for precision.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    tp = 0\n    fp = 0\n    fn = 0\n    tn = 0\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        else:\n            tn += 1\n    \n    # Compute the confusion matrix\n    confusion_matrix = [\n        [tn, fp],\n        [fn, tp]\n    ]\n    \n    total = len(actual)\n    accuracy = (tp + tn) / total\n    f1 = 2 * (tp / (tp + fp)) * (tp / (tp + fn)) / (1 / (tp + fp) + 1 / (tp + fn))\n    specificity = tn / (tn + fp)\n    npv = tn / (tn + fp)\n    \n    # Round each metric to three decimal places\n    accuracy_rounded = round(accuracy, 3)\n    f1_rounded = round(f1, 3)\n    specificity_rounded = round(specificity, 3)\n    npv_rounded = round(npv, 3)\n    \n    return (confusion_matrix, accuracy_rounded, f1_rounded, specificity_rounded, npv_rounded)\n```\n\n### Explanation\n1. **Confusion Matrix Construction**: We iterate through each pair of actual and predicted values. Depending on whether the actual and predicted values are 0 or 1, we update the counts for True Positives, False Positives, True Negatives, and False Negatives.\n\n2. **Metrics Calculation**:\n   - **Accuracy**: Calculated as the ratio of correct predictions (TP + TN) to the total number of predictions.\n   - **F1 Score**: Computed using the harmonic mean of precision and recall, where precision is TP / (TP + FP) and recall is TP / (TP + FN).\n   - **Specificity**: The ratio of correctly identified negatives to the total negatives.\n   - **Negative Predictive Value**: The ratio of correctly identified negatives to the total negatives.\n\n3. **Rounding**: Each metric is rounded to three decimal places to ensure precision in the output.\n\nThis approach ensures that we accurately measure the performance of the classification model using standard metrics and provide a clear, precise output as required."}
{"task_id": 79, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with a probability p of success. This can be efficiently computed using the Binomial distribution formula.\n\n### Approach\nThe Binomial distribution formula is given by:\n\\[ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\]\nwhere:\n- \\( \\binom{n}{k} \\) is the binomial coefficient, representing the number of ways to choose k successes out of n trials.\n- \\( p \\) is the probability of success on a single trial.\n- \\( (1-p) \\) is the probability of failure on a single trial.\n\nThe steps to compute the probability are as follows:\n1. Check if k is outside the valid range (0 to n). If so, return 0.\n2. If k is 0, the probability is \\( (1-p)^n \\).\n3. If k is n, the probability is \\( p^n \\).\n4. Otherwise, compute the binomial coefficient and the probabilities using the formula and return the result rounded to five decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    if k < 0 or k > n:\n        return 0.0\n    if k == 0:\n        prob = (1 - p) ** n\n    elif k == n:\n        prob = p ** n\n    else:\n        prob = math.comb(n, k) * (p ** k) * ((1 - p) ** (n - k))\n    return round(prob, 5)\n```\n\n### Explanation\n- **Edge Cases Handling**: The function first checks if k is outside the valid range (0 to n). If so, it returns 0.0 as the probability is impossible.\n- **Single Success Probability**: If k is 0, the probability is the chance of all trials failing, which is \\( (1-p)^n \\).\n- **All Successes Probability**: If k is n, the probability is the chance of all trials succeeding, which is \\( p^n \\).\n- **General Case**: For other values of k, the function calculates the binomial coefficient, which is the number of ways to choose k successes from n trials. It then multiplies this coefficient by the probability of k successes and (n-k) failures, resulting in the final probability.\n- **Rounding**: The result is rounded to five decimal places to meet the problem's requirements.\n\nThis approach ensures that the function efficiently computes the probability using mathematical operations and handles edge cases appropriately."}
{"task_id": 80, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the probability density function (PDF) of a normal distribution for a given value, mean, and standard deviation. The function should return the PDF value rounded to five decimal places.\n\n### Approach\nThe probability density function (PDF) of a normal distribution is given by the formula:\n\\[ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{ -\\frac{(x - \\mu)^2}{2\\sigma^2} } \\]\nwhere:\n- \\( x \\) is the value at which the PDF is evaluated,\n- \\( \\mu \\) is the mean of the distribution,\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nThe steps to calculate the PDF are:\n1. Compute the exponent term, which is \\(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\).\n2. Calculate the exponential of the exponent term.\n3. Multiply the result by \\(\\frac{1}{\\sqrt{2\\pi}}\\).\n4. Round the result to five decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    exponent = (x - mean) ** 2 / (2 * (std_dev ** 2))\n    value = (1 / math.sqrt(2 * math.pi)) * math.exp(-exponent)\n    return round(value, 5)\n```\n\n### Explanation\nThe function `normal_pdf` calculates the PDF of a normal distribution using the given formula. Here's a breakdown of the steps:\n1. **Exponent Calculation**: The term \\((x - \\mu)^2 / (2 \\sigma^2)\\) is computed to determine the exponent part of the PDF formula.\n2. **Exponential Calculation**: The exponential of the computed exponent is taken using the `math.exp` function.\n3. **PDF Value**: The exponential value is multiplied by \\(1 / \\sqrt{2\\pi}\\) to get the PDF value.\n4. **Rounding**: The result is rounded to five decimal places using the `round` function to meet the problem requirements.\n\nThis approach ensures that the function accurately computes the PDF value for any given \\(x\\), mean, and standard deviation, handling both normal and degenerate distributions appropriately."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution. The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space. It is characterized by the fact that the probability of an event occurring is constant and independent of the time since the last event.\n\n### Approach\nThe approach to solve this problem involves using the Poisson probability formula, which is given by:\n\n\\[ P(k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\]\n\nWhere:\n- \\( \\lambda \\) is the average rate (mean) of occurrences in the interval.\n- \\( k \\) is the number of occurrences we are interested in.\n- \\( e \\) is the base of the natural logarithm (approximately equal to 2.71828).\n- \\( k! \\) is the factorial of \\( k \\).\n\nThe steps to compute the probability are as follows:\n1. Compute \\( e^{-\\lambda} \\) using the exponential function.\n2. Compute \\( \\lambda^k \\) by raising \\( \\lambda \\) to the power of \\( k \\).\n3. Compute the factorial of \\( k \\) using the `math.factorial` function.\n4. Combine these values into the Poisson probability formula.\n5. Round the result to 5 decimal places to get the final probability.\n\n### Solution Code\n\n```python\nimport math"}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values. This measure of contrast helps in understanding the intensity differences in an image, with higher contrast indicating more vibrant images and lower contrast indicating more uniform images.\n\n### Approach\n1. **Understand the Problem**: The task is to compute the contrast of a grayscale image, which is defined as the difference between the maximum and minimum pixel values in the image.\n2. **Input Handling**: The input is a 2D numpy array where each element represents a pixel value ranging from 0 (black) to 255 (white).\n3. **Compute Max and Min Values**: Using numpy functions, we can efficiently find the maximum and minimum pixel values in the image.\n4. **Calculate Contrast**: Subtract the minimum value from the maximum value to get the contrast.\n5. **Return the Result**: The computed contrast is returned as the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast value of the image.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val\n```\n\n### Explanation\n- **Importing numpy**: The numpy library is used for efficient array operations, including finding the maximum and minimum values.\n- **Function Definition**: The function `calculate_contrast` takes a 2D numpy array `img` as input.\n- **Finding Max and Min**: Using `np.max(img)` and `np.min(img)`, we determine the maximum and minimum pixel values in the image.\n- **Contrast Calculation**: The contrast is computed by subtracting the minimum value from the maximum value.\n- **Return Value**: The function returns the computed contrast value, which represents the intensity difference in the image.\n\nThis approach efficiently computes the contrast using numpy functions, making the solution both concise and performant."}
{"task_id": 83, "completion_id": 0, "completion": "To solve this problem, we need to write a function that calculates the dot product of two 1D NumPy arrays. The dot product is a fundamental operation in linear algebra, where each element of one array is multiplied by the corresponding element of another array, and the results are summed up.\n\n### Approach\nThe approach to solve this problem involves using NumPy's built-in function `np.dot()`, which efficiently computes the dot product of two arrays. The steps are as follows:\n\n1. **Import NumPy**: This is necessary to use the `np.dot()` function.\n2. **Define the Function**: Create a function that takes two 1D NumPy arrays as input parameters.\n3. **Compute the Dot Product**: Utilize `np.dot()` to compute the dot product of the two input arrays.\n4. **Return the Result**: The result of the dot product is returned as a single number.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\nThe function `calculate_dot_product` takes two arguments, `vec1` and `vec2`, which are 1D NumPy arrays. It uses the `np.dot()` function to compute the dot product, which is efficient and handles the computation internally. The result is then returned as a single number, which is the dot product of the two input vectors.\n\nThis approach ensures that the function is both concise and efficient, leveraging NumPy's optimized operations for numerical computations."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features. This transformation allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features.\n\n### Approach\n1. **Problem Analysis**: The task is to transform each feature in the input data into a polynomial feature vector. For a given degree `d`, each feature `x` is transformed into a vector of `[x^0, x^1, ..., x^d]`. If the degree is less than 0, the function should return an empty list.\n2. **Intuition**: Polynomial features are created by raising each feature to successive powers starting from 0 up to the given degree. This increases the dimensionality of the feature space, allowing linear models to capture nonlinear relationships.\n3. **Algorithm**:\n   - Check if the degree is less than 0. If so, return an empty list.\n   - For each data point in the input list, generate a polynomial feature vector by raising each feature to the power of 0, 1, ..., up to the given degree.\n   - Round each element of the resulting feature vector to 8 decimal places.\n   - Collect all these feature vectors into a nested list where each inner list represents the transformed features of a data point.\n4. **Complexity**: The time complexity is O(n * d * m), where `n` is the number of data points, `d` is the degree, and `m` is the number of features per data point. This is because for each data point, we perform `d+1` exponentiations and `d+1` rounding operations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n    \n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n        \n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for point in data:\n        transformed = []\n        for feature in point:\n            for k in range(degree + 1):\n                transformed.append(feature ** k)\n        transformed = [round(num, 8) for num in transformed]\n        result.append(transformed)\n    return result\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the degree is less than 0 and returns an empty list if true.\n- **Feature Transformation**: For each data point, each feature is raised to the power of 0 up to the given degree, resulting in a polynomial feature vector.\n- **Rounding**: Each element of the resulting feature vector is rounded to 8 decimal places to ensure precision.\n- **Nested List Construction**: The transformed feature vectors are collected into a nested list, where each inner list corresponds to a data point's polynomial features.\n\nThis approach efficiently transforms each feature into a higher-dimensional space, enabling the use of linear models to capture complex relationships in the data."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function should"}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the F1 score given predicted and true labels. The F1 score is a harmonic mean of precision and recall, which makes it suitable for imbalanced datasets. The approach involves counting true positives (TP), false positives (FP), and false negatives (FN) to compute these metrics, and then using them to determine the F1 score.\n\n### Approach\n1. **Count True Positives (TP), False Positives (FP), and False Negatives (FN)**:\n   - Iterate through each pair of true labels (y_true) and predicted labels (y_pred).\n   - Update TP, FP, and FN based on the values of y_true and y_pred.\n\n2. **Calculate Precision and Recall**:\n   - Precision is the ratio of true positives to the sum of true positives and false positives.\n   - Recall is the ratio of true positives to the sum of true positives and false negatives.\n\n3. **Compute F1 Score**:\n   - The F1 score is calculated using the harmonic mean of precision and recall. If either precision or recall is zero, the F1 score is zero.\n   - Use the formula: \\( F1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\).\n\n4. **Round the Result**:\n   - The result is rounded to three decimal places for the final output.\n\n### Solution Code\n```python\ndef calculate_f1_score(y_true, y_pred):\n    TP = 0\n    FP = 0\n    FN = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            TP += 1\n        elif yt == 1 and yp == 0:\n            FP += 1\n        elif yt == 0 and yp == 1:\n            FN += 1\n    # Calculate precision\n    if TP + FP == 0:\n        precision = 0.0\n    else:\n        precision = TP / (TP + FP)\n    # Calculate recall\n    if TP + FN == 0:\n        recall = 0.0\n    else:\n        recall = FN / (TP + FN)\n    # Calculate F1 score\n    if (precision + recall) == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    # Round to three decimal places\n    return round(f1, 3)\n```\n\n### Explanation\n- **Counting TP, FP, FN**: The code iterates through each pair of true and predicted labels, updating the counts based on whether the label is a true positive, false positive, or false negative.\n- **Precision and Recall Calculation**: These are computed as the ratio of true positives to the sum of true positives and respective errors.\n- **F1 Score Calculation**: The harmonic mean of precision and recall is computed, ensuring that if either metric is zero, the F1 score defaults to zero.\n- **Rounding**: The result is rounded to three decimal places to meet the problem's requirements.\n\nThis approach ensures that the F1 score is accurately computed and appropriately rounded, providing a robust measure of a model's performance."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a linear regression on the power consumption data, remove the known fluctuation, predict the base consumption for day 15, add back the fluctuation for day 15, and include a 5% safety margin. The final answer must be an integer.\n\n### Approach\n1. **Calculate Fluctuation**: Compute the fluctuation for each day using the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\).\n2. **Subtract Fluctuation**: Subtract the fluctuation from each day's power consumption to get the detrended data.\n3. **Linear Regression**: Perform linear regression on the detrended data to find the best fit line, which gives us the slope (a) and intercept (b).\n4. **Predict Base Consumption**: Use the linear regression model to predict the base consumption for day 15.\n5. **Add Back Fluctuation**: Add back the fluctuation for day 15 to the predicted base consumption.\n6. **Safety Margin**: Round the result to the nearest integer and add a 5% safety margin (rounded up).\n\n### Solution Code\n```python\nimport math\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Compute the fluctuation for each day\n    f = [10 * math.sin(2 * math.pi * i / 10) for i in range(1, 11)]\n    \n    # Subtract the fluctuation from each data point to get detrended data\n    detrended = [consumption_data[i] - f[i] for i in range(10)]\n    \n    # Perform linear regression on detrended data\n    n = len(detrended)\n    sum_x = sum(i for i in range(1, 11))  # Sum of x values (1 to 10)\n    sum_y = sum(detrended)\n    sum_xy = sum(i * y for i, y in zip(range(1, 11), detrended))\n    sum_x2 = sum(i ** 2 for i in range(1, 11))\n    \n    a = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - a * sum_x) / n\n    \n    # Predict base consumption for day 15\n    base_day15 = a * 15 + b\n    \n    # Compute the fluctuation for day 15\n    f_day15 = 10 * math.sin(2 * math.pi * 15 / 10)\n    \n    # Total prediction\n    total = base_day15 + f_day15\n    \n    # Round to nearest integer and add 5% safety margin\n    rounded_total = round(total)\n    safety = math.ceil(rounded_total * 0.05)\n    final_answer = rounded_total + safety\n    \n    return final_answer\n```\n\n### Explanation\n1. **Fluctuation Calculation**: The fluctuation for each day is calculated using the given formula, which is then stored in a list.\n2. **Detrending**: The detrended data is obtained by subtracting the fluctuation from each day's power consumption.\n3. **Linear Regression**: The coefficients (slope and intercept) of the best fit line are calculated using the least squares method.\n4. **Prediction**: Using the best fit line, the base consumption for day 15 is predicted.\n5. **Fluctuation Adjustment**: The known fluctuation for day 15 is added back to the predicted base consumption.\n6. **Safety Margin**: The result is rounded to the nearest integer and a 5% safety margin is added to ensure robustness.\n\nThis approach ensures that we accurately predict the power consumption, account for known fluctuations, and include a safety margin for reliability."}
{"task_id": 93, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. MAE is a measure of the average magnitude of errors in a set of predictions, without considering their direction. This problem requires us to implement a function that computes this metric efficiently using numpy.\n\n### Approach\n1. **Calculate Absolute Differences**: Compute the absolute difference between each corresponding element in the actual values (y_true) and predicted values (y_pred). This gives us an array of absolute errors.\n2. **Compute Mean of Absolute Errors**: Calculate the mean of the absolute errors array. This provides the average magnitude of the errors.\n3. **Round the Result**: Round the computed mean to three decimal places to get the final MAE value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    absolute_diffs = np.abs(y_true - y_pred)\n    mae = np.mean(absolute_diffs)\n    return round(mae, 3)\n```\n\n### Explanation\n1. **Absolute Differences Calculation**: Using numpy's absolute value function `np.abs()`, we compute the absolute differences between each element of y_true and y_pred. This step ensures that each error is non-negative.\n2. **Mean Calculation**: The mean of these absolute differences is computed using numpy's `mean()` function. This gives us the average magnitude of the errors.\n3. **Rounding**: The result is rounded to three decimal places using Python's `round()` function to meet the problem's requirement for precision.\n\nThis approach efficiently computes the MAE using vectorized operations provided by numpy, ensuring both accuracy and performance."}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the multi-head attention mechanism, a key component in the Transformer model. This mechanism allows the model to focus on different parts of the input simultaneously, which is crucial for handling long-range dependencies in sequences.\n\n### Approach\nThe multi-head attention mechanism involves three main steps: computing query, key, and value vectors, and then combining these vectors using multiple attention heads.\n\n1. **Compute Query, Key, and Value Vectors**: For each head, we multiply the input matrix by the corresponding weight matrix to get the query, key, and value vectors. These vectors are then split into multiple parts based on the number of heads.\n\n2. **Compute Attention Scores**: For each head, we compute the dot product of the query and key vectors. This score is then normalized by dividing by the square root of the key vector dimension and applying the softmax function to get attention weights.\n\n3. **Compute Output Vectors**: Using the attention weights, we multiply the value vectors by these weights to get the output vectors for each head.\n\n4. **Concatenate Outputs**: The output vectors from all heads are concatenated to form the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    batch_size = X.shape[0]\n    input_dim = X.shape[1]\n    output_dim = input_dim // n_heads\n\n    # Split the input X into n_heads parts\n    X_split = [X[i*output_dim:(i+1)*output_dim] for i in range(n_heads)]\n\n    # Compute query, key, value vectors for each head\n    q = [X_split[i] @ W_q for i in range(n_heads)]\n    k = [X_split[i] @ W_k for i in range(n_heads)]\n    v = [X_split[i] @ W_v for i in range(n_heads)]\n\n    attention_outputs = []\n    for q_head, k_head, v_head in zip(q, k, v):\n        # Compute attention scores\n        q_k = q_head @ k_head.T\n        attention_scores = q_k / np.sqrt(input_dim)\n        attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n        # Compute output vector\n        output = attention_weights @ v_head\n        attention_outputs.append(output)\n\n    # Concatenate all attention outputs\n    final_output = np.concatenate(attention_outputs, axis=1)\n\n    # Round to 4 decimal places\n    final_output = np.round(final_output, 4)\n\n    return final_output.tolist()\n```\n\n### Explanation\n1. **Splitting the Input**: The input matrix `X` is split into `n_heads` parts, each of size `(batch_size, input_dim / n_heads)`.\n\n2. **Computing Vectors**: For each head, the query, key, and value vectors are computed by multiplying the input part with the corresponding weight matrix.\n\n3. **Attention Scores**: The dot product of the query and key vectors is computed, normalized, and used to get attention weights.\n\n4. **Output Computation**: The value vectors are multiplied by the attention weights to get the output vectors for each head.\n\n5. **Concatenation**: The output vectors from all heads are concatenated to form the final output, which is then rounded to 4 decimal places.\n\nThis approach ensures that the model can efficiently process sequences by focusing on different parts of the input through multiple attention heads, making it suitable for tasks like text translation and summarization."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement the Hard Sigmoid activation function, which is an efficient approximation of the standard sigmoid function. The Hard Sigmoid function is designed to be computationally efficient by using clamping to ensure the output values are within the range [0, 1].\n\n### Approach\nThe Hard Sigmoid function is defined as follows:\n- For any input value \\( x \\), compute \\( 0.5 \\times (1 + x) \\).\n- If the result is less than 0, clamp it to 0.\n- If the result is greater than 1, clamp it to 1.\n- Otherwise, return the computed value.\n\nThis approach ensures that the output is always within the range [0, 1], which is the same as the standard sigmoid function but with a faster computation time.\n\n### Solution Code\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    return max(0.0, min(1.0, 0.5 * (1 + x)))\n```\n\n### Explanation\n1. **Compute the Linear Transformation**: The input \\( x \\) is first transformed using the formula \\( 0.5 \\times (1 + x) \\). This step linearly scales the input into a range that is suitable for the standard sigmoid function.\n\n2. **Clamp the Result**: The result of the linear transformation is then clamped to ensure it lies within the range [0, 1]. This is done using Python's built-in `max` and `min` functions. If the result is less than 0, it is set to 0. If the result is more than 1, it is set to 1.\n\n3. **Return the Result**: The clamped value is returned as the output of the Hard Sigmoid function.\n\nThis implementation efficiently computes the output in constant time \\( O(1) \\) by avoiding any complex mathematical operations, making it very suitable for applications where computational efficiency is important."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function, which is designed to mitigate the limitations of the Rectified Linear Unit (ReLU) by providing negative outputs for negative inputs. This function is particularly useful in neural networks as it helps in introducing non-linearity while allowing negative outputs.\n\n### Approach\nThe ELU activation function is defined as follows:\n- For positive inputs (x > 0), the function returns the input value x.\n- For negative inputs (x \u2264 0), the function returns alpha multiplied by (e^x - 1), where alpha is a parameter that defaults to 1.0.\n\nThe approach to implement this function involves:\n1. Checking if the input value x is positive. If it is, the function returns x.\n2. If x is negative, the function computes the ELU value using the formula alpha * (e^x - 1).\n3. The result is then rounded to four decimal places to meet the problem's output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```\n\n### Explanation\n- The function `elu` takes two parameters: `x` (the input value) and `alpha` (a parameter that defaults to 1.0).\n- It checks if `x` is positive. If true, it returns `x` rounded to four decimal places.\n- If `x` is negative, it computes the ELU value using the formula `alpha * (math.exp(x) - 1)` and returns this value rounded to four decimal places.\n- The use of `math.exp` ensures that the exponential function is computed accurately, even for very small negative values of `x`.\n\nThis implementation efficiently handles both positive and negative inputs, ensuring that the output conforms to the specified rounding constraints."}
{"task_id": 98, "completion_id": 0, "completion": "To solve this problem, we need to implement the PReLU (Parametric ReLU) activation function, which is a variant of the ReLU activation function. The PReLU function introduces a learnable parameter for negative inputs, allowing it to scale these inputs differently than the standard ReLU function.\n\n### Approach\nThe PReLU function works as follows:\n1. For positive input values, the function returns the input value as is.\n2. For negative input values, the function scales the input by a parameter called `alpha`, which is a learnable slope.\n3. The function returns 0 for zero input values.\n\nThe approach involves checking each input value individually and applying the respective transformation based on whether the input is positive, negative, or zero. This can be efficiently done using a simple conditional check for each value.\n\n### Solution Code\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    if x > 0:\n        return x\n    else:\n        return x * alpha\n```\n\n### Explanation\n- The function `prelu` takes two arguments: `x` (the input value) and `alpha` (the slope parameter for negative inputs, with a default value of 0.25).\n- If `x` is positive, the function returns `x` directly.\n- If `x` is negative, the function returns `x` multiplied by `alpha`.\n- If `x` is zero, the function returns 0, which is the same as the standard ReLU function.\n\nThis implementation ensures that the function is efficient and handles all possible cases, including positive, negative, and zero inputs. The conditional check ensures that each input is processed correctly according to the PReLU activation rule."}
{"task_id": 99, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softplus activation function, which is a smooth approximation of the ReLU (Rectified Linear Unit) function. The Softplus function is defined as the natural logarithm of (1 + e^x), where x is the input value. This function is designed to handle edge cases to prevent numerical overflow or underflow, ensuring accurate computation for all input values.\n\n### Approach\n1. **Understanding the Softplus Function**: The Softplus function computes the natural logarithm of (1 + e^x). This function is smooth and differentiable, making it suitable for use in neural networks.\n2. **Handling Edge Cases**: \n   - For very large positive values of x, e^x can be extremely large, leading to overflow. However, since 1 + e^x will still be a very large number, the logarithm will approach infinity, which is correctly handled by the function.\n   - For very large negative values of x, e^x approaches zero, making 1 + e^x approach 1. The logarithm of 1 is zero, which is correctly returned.\n3. **Implementation Steps**:\n   - Import the `math` module to use the exponential and logarithm functions.\n   - Compute the value of the Softplus function using the formula `math.log(1 + math.exp(x))`.\n   - Round the result to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    return round(math.log(1 + math.exp(x)), 4)\n```\n\n### Explanation\n- **Importing Math Module**: The `math` module is imported to access the exponential (`math.exp`) and logarithm (`math.log`) functions.\n- **Softplus Calculation**: The function computes the value of the Softplus function using the formula `math.log(1 + math.exp(x))`.\n- **Rounding**: The result is rounded to four decimal places using `round(value, 4)` to ensure precision as required.\n\nThis approach ensures that the Softplus function handles all possible input values correctly, including edge cases, and returns the result with the required precision."}
{"task_id": 100, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softsign activation function, which is a smooth function that maps any real number to the interval [-1, 1]. The Softsign function is defined as \\( \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\). The output of this function is always bounded between -1 and 1.\n\n### Approach\n1. **Understanding the Softsign Function**: The Softsign function takes an input \\( x \\) and returns \\( \\frac{x}{1 + |x|} \\). This ensures that the output is always within the range [-1, 1]. For positive values of \\( x \\), the function decreases towards 0 as \\( x \\) increases, and for negative values, it increases towards -1 as \\( x \\) becomes more negative.\n2. **Implementation Steps**:\n   - Compute the absolute value of \\( x \\).\n   - Add 1 to this absolute value.\n   - Divide \\( x \\) by this sum to get the Softsign value.\n   - Round the result to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```\n\n### Explanation\n- **Step 1**: The function takes a float input \\( x \\).\n- **Step 2**: It calculates the absolute value of \\( x \\) to handle both positive and negative inputs uniformly.\n- **Step 3**: The absolute value is added to 1 to avoid division by zero and to ensure the denominator is always positive.\n- **Step 4**: The result of the division is computed, which gives the Softsign value.\n- **Step 5**: The result is rounded to four decimal places using Python's `round` function to meet the problem's output requirements.\n\nThis approach ensures that the function is efficient and correctly handles both positive and negative inputs, providing the desired bounded output."}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the GRPO (Group Relative Policy Optimization) objective function, which is used in reinforcement learning to optimize policy parameters. The function combines the benefits of policy gradient methods with the structure of policy optimization, and it is known for its stability and efficiency.\n\n### Approach\nThe GRPO objective function involves several key components:\n1. **Logarithmic Ratios (Likelihood Ratios):** These are the ratios of the new policy probabilities to the old policy probabilities.\n2. **Advantage Estimates:** These are the estimates of the advantage functions, which measure how much better the new policy is compared to the old policy.\n3. **Reference Policy Probabilities:** These are the probabilities defined by the reference policy.\n4. **Clipping Parameter (epsilon):** This parameter ensures that the likelihood ratios do not exceed a certain threshold.\n5. **KL Divergence Penalty (beta):** This parameter penalizes the KL divergence between the reference and old policies to maintain stability.\n\nThe objective function is computed as the sum of the advantage estimates multiplied by the logarithm of the likelihood ratios, adjusted by the reference policy probabilities, minus a penalty term involving the KL divergence between the reference and old policies.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    sum_A_log_ratio = 0.0\n    kl_div = 0.0\n    for i in range(len(rho)):\n        log_rho = np.log(rho[i])\n        log_pi_old = np.log(pi_old[i])\n        term1 = A[i] * log_rho\n        term2 = A[i] * log_pi_old\n        sum_A_log_ratio += term1 - term2\n        pi_ref_i = pi_ref[i]\n        kl_term = pi_ref_i * np.log(pi_ref_i / pi_old[i])\n        kl_div += kl_term\n    objective = sum_A_log_ratio - beta * kl_div\n    return round(objective, 6)\n```\n\n### Explanation\n1. **Logarithmic Ratios:** For each state, compute the logarithm of the likelihood ratio (rho) divided by the old policy probability (pi_old).\n2. **Sum of Advantage Terms:** Compute the sum of the advantage estimates (A) multiplied by the logarithm of the likelihood ratios.\n3. **KL Divergence Calculation:** Compute the KL divergence between the reference policy probabilities and the old policy probabilities.\n4. **Objective Calculation:** The objective function is the sum of the advantage terms minus the product of the KL divergence penalty (beta) and the KL divergence.\n\nThis approach ensures that the policy optimization is stable and efficient by combining the benefits of both policy gradient methods and the structure of policy optimization. The function returns the computed objective value rounded to six decimal places for stability and consistency."}
{"task_id": 102, "completion_id": 0, "completion": "To solve this problem, we need to implement the Swish activation function, which is a self-gated activation function that has shown superior performance compared to other activation functions like ReLU in deep neural networks. The Swish function is defined as \\( x \\times \\text{sigmoid}(x) \\), where \\(\\text{sigmoid}(x)\\) is the sigmoid function.\n\n### Approach\n1. **Understand the Swish Function**: The Swish function computes the product of the input value \\( x \\) and the sigmoid of \\( x \\). The sigmoid function is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Mathematical Computation**: For a given input \\( x \\), compute the sigmoid value using the formula and then multiply it by \\( x \\).\n3. **Edge Cases Handling**: Consider cases where \\( x \\) is very large or very small. However, the current implementation handles these cases correctly using Python's math library.\n4. **Rounding**: The result is rounded to the nearest 4th decimal place to meet the problem's output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid\n    return round(swish_val, 4)\n```\n\n### Explanation\n- **Importing Math Module**: The `math` module is imported to use the exponential function `exp` for computing the sigmoid value.\n- **Sigmoid Calculation**: For a given input \\( x \\), the sigmoid value is calculated as \\( 1 / (1 + e^{-x}) \\).\n- **Swish Value Calculation**: The Swish value is then computed by multiplying \\( x \\) with the sigmoid value.\n- **Rounding**: The result is rounded to 4 decimal places using Python's `round` function to meet the specified precision.\n\nThis approach ensures that the Swish function is implemented efficiently and correctly handles all edge cases, providing the desired output constraints."}
{"task_id": 104, "completion_id": 0, "completion": "To solve this problem, we need to implement a function for binary classification using logistic regression. The function will compute the class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Approach\n1. **Compute Linear Combination**: First, we compute the linear combination of the input features and the model weights, then add the bias term.\n2. **Apply Sigmoid Function**: The linear combination is passed through the sigmoid function to convert it into probabilities between 0 and 1.\n3. **Threshold for Binary Prediction**: Convert these probabilities into binary predictions by checking if each probability is greater than or equal to 0.5. This gives us 1 if the probability is above 0.5 and 0 otherwise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using logistic regression.\n    \n    Args:\n        X: Input feature matrix (shape: NxD)\n        weights: Model weights (shape: D)\n        bias: Model bias term\n    \n    Returns:\n        Binary predictions (0 or 1) as a list after converting the probabilities.\n    \"\"\"\n    z = X @ weights + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0"}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to perform Layer Normalization on a 3D tensor. Layer Normalization is a technique used in neural networks to normalize the activations of each layer, which helps in improving the training of deep neural networks by reducing internal covariate shift.\n\n### Approach\nThe approach to implement Layer Normalization involves the following steps:\n\n1. **Compute the Mean (mu)**: For each feature across all elements in the batch and sequence, compute the mean. This mean will be used to center the data.\n2. **Compute the Standard Deviation (sigma)**: For each feature, compute the population standard deviation, which is the square root of the variance. This standard deviation will be used to scale the data.\n3. **Normalize the Data**: Subtract the mean from each element and divide by the standard deviation. This step centers the data around zero and scales it by the standard deviation.\n4. **Scale and Shift**: Multiply the normalized data by gamma (a learnable parameter) and add beta (another learnable parameter) to shift and scale the data respectively.\n5. **Round the Result**: Round each element of the normalized data to 5 decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute the mean for each feature across all elements in the batch and sequence\n    mu = np.mean(X, axis=(0, 1), keepdims=True)\n    \n    # Compute the standard deviation for each feature\n    sigma = np.std(X, axis=(0, 1), ddof=0, keepdims=True)\n    \n    # Subtract the mean and divide by the standard deviation\n    X_centered = X - mu\n    X_normalized = X_centered / sigma\n    \n    # Multiply by gamma and add beta\n    X_scaled = X_normalized * gamma\n    X_final = X_scaled + beta\n    \n    # Round to 5 decimal places and convert to a list\n    X_final_rounded = np.round(X_final, 5).tolist()\n    \n    return X_final_rounded\n```\n\n### Explanation\n- **Mean Calculation**: The mean for each feature is computed across all elements in the batch and sequence. This is done using `np.mean` with `axis=(0, 1)` to handle the batch and sequence dimensions.\n- **Standard Deviation Calculation**: The standard deviation is computed using `np.std` with `ddof=0` to get the population standard deviation.\n- **Normalization**: Each element is centered by subtracting the mean and scaled by dividing by the standard deviation.\n- **Scaling and Shifting**: The normalized data is scaled by gamma and shifted by beta.\n- **Rounding and Conversion**: The result is rounded to 5 decimal places and converted to a list for the final output.\n\nThis approach ensures that the data is normalized correctly across all features, making it suitable for various machine learning tasks."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Pointwise Mutual Information (PMI) given the joint occurrence counts of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events deviates from what would be expected if the events were independent.\n\n### Approach\n1. **Understanding PMI**: PMI is calculated as the sum of the pointwise mutual information for each pair of events. The pointwise mutual information for a pair of events (x, y) is given by the logarithm of the ratio of the observed joint count to the expected joint count under independence. The expected joint count is calculated as (total count of x * total count of y) / total number of samples.\n\n2. **Formula**: The PMI for each pair (x, y) is computed as:\n   \\[\n   \\text{PMI}(x, y) = \\text{joint\\_counts}[x][y] \\times \\log_2\\left(\\frac{\\text{joint\\_counts}[x][y]}{\\text{total\\_x} \\times \\text{total\\_y} / \\text{total\\_samples}}\\right)\n   \\]\n   where:\n   - `joint_counts` is a 2D array where `joint_counts[x][y]` is the count of occurrences of events x and y.\n   - `total_counts_x` is the total count of event x.\n   - `total_counts_y` is the total count of event y.\n   - `total_samples` is the total number of samples.\n\n3. **Summing PMI Values**: The overall PMI is the sum of the PMI values for all pairs of events (x, y).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    sum_pmi = 0.0\n    for x in range(len(joint_counts)):\n        for y in range(len(joint_counts[0])):\n            expected = (total_counts_x * total_counts_y) / total_samples\n            current_count = joint_counts[x][y]\n            log_term = np.log2(current_count / expected)\n            sum_pmi += current_count * log_term\n    return round(sum_pmi, 3)\n```\n\n### Explanation\n- **Initialization**: The function initializes a variable `sum_pmi` to accumulate the total PMI.\n- **Loop Through Events**: The nested loops iterate through each possible pair of events (x, y).\n- **Expected Count Calculation**: For each pair (x, y), the expected count under independence is calculated as `(total_counts_x * total_counts_y) / total_samples`.\n- **Logarithmic Calculation**: The logarithm of the ratio of the observed count to the expected count is computed using base 2.\n- **PMI Contribution**: The contribution of each pair to the overall PMI is added to `sum_pmi`.\n- **Return Result**: The accumulated PMI is rounded to three decimal places and returned.\n\nThis approach efficiently computes the PMI by leveraging the properties of logarithms and expected counts, ensuring accurate and efficient calculation."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Min-Max Normalization on a list of integers. This normalization technique scales the data such that all values fall within the range [0, 1]. The function should return a list of floats, each rounded to four decimal places.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return an empty list immediately.\n2. **Compute Min and Max**: Determine the minimum and maximum values of the input list. If the minimum and maximum values are the same, all elements are identical, and the output will be a list of zeros.\n3. **Handle Identical Values**: If the minimum and maximum values are identical, return a list containing a single zero.\n4. **Normalize Values**: For each value in the list, compute the normalized value using the formula \\((\\text{value} - \\text{min}) / (\\text{max} - \\text{min})\\).\n5. **Round Values**: Each normalized value is rounded to four decimal places before being added to the result list.\n\n### Solution Code\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_x = min(x)\n    max_x = max(x)\n    if min_x == max_x:\n        return [0.0]\n    scaled = [(num - min_x) / (max_x - min_x) for num in x]\n    return [round(num, 4) for num in scaled]\n```\n\n### Explanation\n- **Empty List Check**: The function first checks if the input list is empty and returns an empty list if true.\n- **Min and Max Calculation**: The minimum and maximum values of the list are computed. If these values are the same, the function returns a list with a single zero.\n- **Normalization**: Each value in the list is transformed using the Min-Max formula to scale it to the range [0, 1].\n- **Rounding**: Each normalized value is rounded to four decimal places to ensure precision and meet the problem requirements.\n\nThis approach ensures that the function handles all edge cases, including empty lists and lists with identical values, efficiently and correctly."}
{"task_id": 114, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer. The function should take an input of shape (height, width, channels) and return a 1D array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n\n### Approach\nThe approach involves computing the average of each feature map across all height and width dimensions. This is efficiently achieved using NumPy's built-in functions. Specifically, we can use the `mean` function with the `axis` parameter set to `(0, 1)`, which computes the mean across the first two dimensions (height and width) for each channel, resulting in a 1D array of length equal to the number of channels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return x.mean(axis=(0, 1))\n```\n\n### Explanation\n1. **Input Shape**: The input is a 3D NumPy array with shape (height, width, channels). Here, `height` and `width` are the dimensions of each feature map, and `channels` represent the number of feature maps (or channels).\n2. **Compute Averages**: Using `x.mean(axis=(0, 1))`, we compute the average of all elements across the height and width dimensions for each channel. This results in a 1D array where each element is the average of a feature map.\n3. **Efficiency**: The use of NumPy's `mean` function with the specified axis ensures efficient computation, leveraging optimized C-based operations for performance.\n\nThis approach efficiently computes the Global Average Pooling by reducing the dimensionality of the feature maps while maintaining the number of channels, which is essential for further processing in convolutional neural networks."}
{"task_id": 116, "completion_id": 0, "completion": "To solve this problem, we need to compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`. The derivative of this term is given by the power rule in calculus, which states that the derivative of `c * x^n` with respect to `x` is `c * n * x^(n-1)`. \n\n### Approach\nThe approach to solve this problem involves the following steps:\n1"}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product of two vectors results in a third vector that is perpendicular to both of the original vectors, following the right-hand rule. This concept is fundamental in physics, engineering, and 3D graphics.\n\n### Approach\nThe cross product of two vectors \\( a = [a_1, a_2, a_3] \\)"}
{"task_id": 121, "completion_id": 0, "completion": "To solve this problem, we need to write a function that computes the element-wise sum of two vectors. If the vectors have incompatible dimensions, the function should return -1.\n\n### Approach\n1. **Check Length Compatibility**: The first step is to check if the lengths of the two vectors are the same. If they are not, the function should return -1.\n2. **Element-wise Sum**: If the vectors have the same length, we create a new vector where each element is the sum of the corresponding elements from the input vectors.\n3. **Return Result**: The result is returned as a new list. If the vectors are empty, the result will also be an empty list.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]\n```\n\n### Explanation\n- **Check Length Compatibility**: The function first checks if the lengths of the two input vectors `a` and `b` are different. If they are, it immediately returns -1.\n- **Element-wise Sum**: If the lengths are the same, the function uses a list comprehension to iterate over each index. For each index, it adds the corresponding elements from both vectors and collects these sums into a new list.\n- **Return Result**: The resulting list of sums is returned. This ensures that the function handles both compatible and incompatible vector dimensions correctly.\n\nThis approach efficiently handles the problem requirements and ensures that the solution is both correct and optimal."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the policy gradient estimator using the REINFORCE algorithm. The goal is to compute the average gradient of the log-policy multiplied by the return at each time step for a given list of episodes. The policy is parameterized by a 2D NumPy array `theta` of shape `(num_states, num_actions)`.\n\n### Approach\n1. **Compute the Return**: For each episode, the return is the sum of all rewards in that episode. This gives us the total reward the agent receives when following the policy for that episode.\n2. **Compute Log Probability**: For each state-action pair in the episode, compute the log probability of the action under the current policy. This is done using the softmax function applied to the policy parameters `theta[s, :]` for each state `s`.\n3. **Gradient Contribution**: For each state-action pair, compute the product of the log probability and the return. Sum these products across all state-action pairs in the episode to get the gradient contribution for that episode.\n4. **Average Gradient**: Average the gradient contributions across all episodes to get the average gradient.\n5. **Rounding**: Round each element of the average gradient to four decimal places.\n6. **Return Result**: Return the result as a list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    total_gradient = 0.0\n    num_episodes = len(episodes)\n    \n    for episode in episodes:\n        return_ = sum(r for s, a, r in episode)\n        for s, a, r in episode:\n            sum_exp = np.sum(np.exp(theta[s, :]))\n            log_p = np.log(sum_exp)\n            total_gradient += log_p * r\n    \n    average_gradient = total_gradient / num_episodes\n    average_gradient_rounded = [round(x, 4) for x in average_gradient]\n    \n    return list(average_gradient_rounded)\n```\n\n### Explanation\n- **Return Calculation**: For each episode, the return is computed as the sum of all rewards. This represents the total reward the agent receives when following the policy for that episode.\n- **Log Probability Calculation**: For each state `s` and action `a` in the episode, the log probability of action `a` given state `s` is computed using the softmax function. This is done by exponentiating each element of `theta[s, :]`, summing them, and taking the natural logarithm of the sum.\n- **Gradient Contribution**: For each state-action pair, the product of the log probability and the return is computed and accumulated to get the total gradient contribution for the episode.\n- **Average Gradient**: The total gradient contributions from all episodes are averaged to get the average gradient, which is then rounded to four decimal places.\n- **Result**: The result is returned as a list using `tolist()` to ensure the output format is correct.\n\nThis approach efficiently computes the policy gradient using REINFORCE, providing the average gradient which can be used to update the policy parameters."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the computational efficiency of a MoE (Mixture-of-Experts) layer compared to a dense layer. The efficiency is determined by the ratio of the number of floating-point operations (FLOPs) required by the MoE layer to those required by the dense layer, expressed as a percentage.\n\n### Approach\n1. **Understanding the Problem**: The task is to compute the efficiency of a MoE layer over a dense layer. The efficiency is determined by the ratio of the number of FLOPs required by the MoE layer to those required by the dense layer, expressed as a percentage.\n\n2. **Formulas**:\n   - **Dense Layer FLOPs**: The dense layer computes each output element as a dot product of the input and weight matrix. For an input dimension `d_in` and output dimension `d_out`, the number of FLOPs is `d_in * d_out^2`.\n   - **MoE Layer FLOPs**: The MoE layer uses a gating mechanism where each active expert contributes a part of the output. For `k_active` active experts, the number of FLOPs is `k_active * d_in * d_out`.\n\n3. **Efficiency Calculation**: The efficiency is calculated as the ratio of the MoE layer's FLOPs to the dense layer's FLOPs, multiplied by 100 to get a percentage.\n\n### Solution Code\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    moe_flops = k_active * d_in * d_out\n    dense_flops = d_in * (d_out ** 2)\n    efficiency = (moe_flops / dense_flops) * 100\n    return round(efficiency, 1)\n```\n\n### Explanation\n- **Moe FLOPs Calculation**: The formula `k_active * d_in * d_out` computes the total FLOPs for the MoE layer, considering each active expert contributes a dot product operation.\n- **Dense FLOPs Calculation**: The formula `d_in * (d_out ** 2)` computes the total FLOPs for the dense layer, which is a straightforward dot product for each output element.\n- **Efficiency Calculation**: The efficiency is computed by dividing the MoE layer's FLOPs by the dense layer's FLOPs and converting it to a percentage. The result is rounded to the nearest tenth for precision.\n\nThis approach ensures that we accurately measure the computational efficiency of the MoE layer relative to a dense layer, providing a clear understanding of the performance improvement."}
{"task_id": 125, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Sparse Mixture of Experts (MoE) layer that combines multiple expert networks to produce a final output. The MoE layer uses a gating network to determine which expert's output to use for each input token, and it aggregates these outputs weighted by the gate probabilities.\n\n### Approach\n1. **Compute Gate Probabilities**: The gate probabilities are computed using the product of the gating weights matrix (Wg) and the input tensor (x). These probabilities are then normalized using the softmax function to ensure they sum to 1.\n2. **Select Top-K Experts**: For each input token, the top-k experts are selected based on the computed gate probabilities. This is done by finding the indices of the top-k probabilities.\n3. **Aggregate Expert Outputs**: The outputs of the top-k experts are aggregated by summing their outputs, each weighted by their respective gate probabilities. This sum is then rounded to four decimal places.\n4. **Return Result**: The final outputs are returned as a list after reshaping.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    # Compute the gate probabilities\n    p = np.exp(Wg @ x) / np.sum(np.exp(Wg @ x), axis=1, keepdims=True)\n    \n    final_output = []\n    for i in range(x.shape[1]):\n        # Get the p_row for this token\n        p_row = p[i, :]\n        # Find top-k indices\n        sorted_indices = np.argsort(p_row, axis=0)\n        top_k_indices = sorted_indices[-top_k:]  # Get the last top_k elements\n        # Compute the sum of the top-k experts' outputs\n        sum_expert = 0\n        for j in top_k_indices:\n            sum_expert += We[j] @ x[i, :]\n        # Round and append\n        final_output.append(round(sum_expert, 4))\n    \n    return final_output\n```\n\n### Explanation\n1. **Gate Probabilities Calculation**: The gate probabilities are computed using the softmax function on the product of the gating weights matrix (Wg) and the input tensor (x). This ensures that each input token is weighted by the probability of being selected by each expert.\n2. **Top-K Selection**: For each input token, the indices of the top-k probabilities are identified. This is done by sorting the gate probabilities and selecting the highest values.\n3. **Expert Output Aggregation**: The outputs of the selected top-k experts are summed, each weighted by its corresponding gate probability. This sum is then rounded to four decimal places to ensure precision.\n4. **Result Compilation**: The results for each input token are compiled into a list, which is returned as the final output.\n\nThis approach efficiently combines the outputs of multiple experts, ensuring that the final result is a compact and meaningful aggregation of the input data."}
