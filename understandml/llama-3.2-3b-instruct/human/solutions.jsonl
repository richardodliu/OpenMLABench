{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the dot product of a matrix and a vector.\n\n    Args:\n        a (list[list[int|float]]): The matrix.\n        b (list[int|float]): The vector.\n\n    Returns:\n        list[int|float]: The resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n\n    Returns:\n        list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    if not a:\n        return []\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transposed_matrix = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    return transposed_matrix"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n        new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n        list[list[int|float]]: The reshaped matrix, or an empty list if it cannot be reshaped.\n    \"\"\"\n    arr = np.array(a)\n    if arr.shape != new_shape:\n        return []\n    reshaped_arr = arr.reshape(new_shape).tolist()\n    return reshaped_arr"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n        matrix (list[list[float]]): A 2D list of floating point numbers.\n        mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n        list[float]: A list of means according to the specified mode.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Invalid mode. Mode should be either 'row' or 'column'.\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed_matrix = list(zip(*matrix))\n        return [sum(col) / len(col) for col in transposed_matrix]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n\n    Args:\n        matrix (list[list[int|float]]): The input matrix.\n        scalar (int|float): The scalar to multiply the matrix by.\n\n    Returns:\n        list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = []\n        for element in row:\n            new_row.append(element * scalar)\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[float]: A list of eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    matrix = np.array(matrix)\n    (eigenvalues, _) = np.linalg.eig(matrix)\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    return eigenvalues.tolist()"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^-1 * A * S, \n    where T and S are invertible matrices.\n\n    Args:\n        A (list[list[int|float]]): The input matrix to be transformed.\n        T (list[list[int|float]]): The first invertible matrix.\n        S (list[list[int|float]]): The second invertible matrix.\n\n    Returns:\n        list[list[int|float]]: The transformed matrix, or -1 if the transformation is not possible.\n    \"\"\"\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if not np.allclose(np.linalg.det(T), 1) or not np.allclose(np.linalg.det(S), 1):\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    transformed_A = np.dot(T_inv, np.dot(A, S))\n    transformed_A = np.round(transformed_A, 4)\n    return transformed_A.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n        matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        list[list[float]]: The inverse of the input matrix, or None if the matrix is not invertible.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_a = d / det\n    inv_b = -b / det\n    inv_c = -c / det\n    inv_d = a / det\n    return [[inv_a, inv_b], [inv_c, inv_d]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices.\n\n    Args:\n        a (list[list[int | float]]): The first matrix.\n        b (list[list[int | float]]): The second matrix.\n\n    Returns:\n        list[list[int | float]]: The product of the two matrices, or -1 if they cannot be multiplied.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\nimport unittest\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    vectors_array = np.array(vectors)\n    means = np.mean(vectors_array, axis=0)\n    centered_data = vectors_array - means\n    covariance_matrix = np.cov(centered_data, rowvar=False)\n    covariance_matrix_list = covariance_matrix.tolist()\n    return covariance_matrix_list"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n\n    Args:\n    A (np.ndarray): The coefficient matrix.\n    b (np.ndarray): The constant vector.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x, rounded to four decimal places.\n    \"\"\"\n    (num_equations, num_variables) = A.shape\n    x = np.zeros(num_variables)\n    for _ in range(n):\n        new_x = np.copy(x)\n        for i in range(num_equations):\n            sum = 0\n            for j in range(num_variables):\n                if i != j:\n                    sum += A[i, j] * new_x[j]\n            new_x[i] = (b[i] - sum) / A[i, i]\n        x = new_x\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the singular values and the right singular vectors.\n    \"\"\"\n    assert A.shape == (2, 2), 'Input matrix must be 2x2.'\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    singular_vectors = np.dot(A, eigenvectors)\n    singular_values = np.round(singular_values, 4)\n    singular_vectors = np.round(singular_vectors, 4)\n    return (singular_values, singular_vectors)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    This function calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) == 2:\n        (a, b) = matrix[0]\n        (c, d) = matrix[1]\n        return a * d - b * c\n    det = 0\n    for i in range(4):\n        sub_matrix = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        sign = 1 if i % 2 == 0 else -1\n        sub_det = determinant_4x4(sub_matrix)\n        det += sign * matrix[0][i] * sub_det\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n\n    Args:\n    X (list[list[float]]): Matrix of features.\n    y (list[float]): Vector of target values.\n\n    Returns:\n    list[float]: Coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    num_features = X.shape[1]\n    X_T = X.T\n    theta = np.linalg.inv(X_T @ X) @ X_T @ y\n    theta = [round(coef, 4) for coef in theta]\n    return theta"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target values.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    np.ndarray: Coefficients of the linear regression model as a list, rounded to four decimal places.\n    \"\"\"\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, coefficients)\n        errors = predictions - y\n        gradients = 2 * np.dot(X.T, errors) / X.shape[0]\n        coefficients -= alpha * gradients\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n        tuple[list[list[float]], list[list[float]]]: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_max_data = np.round(min_max_data, 4).tolist()\n    return (standardized_data, min_max_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function implements the k-Means clustering algorithm.\n\n    Args:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    k (int): An integer representing the number of clusters to form.\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates.\n    max_iterations (int): An integer representing the maximum number of iterations to perform.\n\n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Divide the dataset into k folds and return a list of train-test indices for each fold.\n\n    Parameters:\n    X (np.ndarray): Feature matrix\n    y (np.ndarray): Target vector\n    k (int, optional): Number of folds (default: 5)\n    shuffle (bool, optional): Whether to shuffle the data before splitting (default: True)\n    random_seed (int, optional): Random seed for shuffling (default: None)\n\n    Returns:\n    list: List of train-test indices for each fold\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if shuffle:\n        indices = np.arange(X.shape[0])\n        np.random.shuffle(indices)\n        (X, y) = (X[indices], y[indices])\n    n_samples = X.shape[0]\n    n_folds = X.shape[1]\n    train_test_indices = []\n    for i in range(k):\n        start = i * n_samples // k\n        end = (i + 1) * n_samples // k\n        fold_indices = indices[start:end]\n        (X_train, X_test) = (X[fold_indices], X[~fold_indices])\n        (y_train, y_test) = (y[fold_indices], y[~fold_indices])\n        train_test_indices.append((X_train, y_train, X_test, y_test))\n    return train_test_indices"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a given dataset.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - list[list[float]]: A list of lists, where each sublist contains the coefficients of a principal component.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(-eigenvalues)\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree for classification using recursive binary splitting based on entropy and information gain.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attributes (list[str]): A list of attribute names.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - dict: A nested dictionary representing the decision tree.\n    \"\"\"\n    if len(examples) == 1:\n        return {attr: examples[0][target_attr] for attr in attributes}\n    initial_entropy = calculate_entropy(examples, target_attr)\n    best_attr = None\n    best_gain = 0\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        split_examples = split_examples(examples, attr)\n        gain = calculate_gain(examples, split_examples, attr, target_attr)\n        if gain > best_gain:\n            best_attr = attr\n            best_gain = gain\n    if best_attr is None:\n        return {attr: Counter((example[target_attr] for example in examples)).most_common(1)[0][0] for attr in attributes}\n    left_tree = learn_decision_tree(split_examples[0], attributes, target_attr)\n    right_tree = learn_decision_tree(split_examples[1], attributes, target_attr)\n    return {best_attr: {left_tree, right_tree}}\ndef calculate_entropy(examples, target_attr):\n    \"\"\"\n    Calculate the entropy of the target attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The entropy of the target attribute.\n    \"\"\"\n    class_freq = Counter((example[target_attr] for example in examples))\n    entropy = -sum((freq / len(examples) * math.log2(freq / len(examples)) for freq in class_freq.values()))\n    return entropy\ndef split_examples(examples, attr):\n    \"\"\"\n    Split the examples based on the given attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples.\n    - attr (str): The name of the attribute.\n\n    Returns:\n    - list: A list of lists of examples, where each inner list represents a child node.\n    \"\"\"\n    child_nodes = []\n    values = [example[attr] for example in examples]\n    child_nodes_dict = {}\n    for value in set(values):\n        child_node = [example for example in examples if example[attr] == value]\n        child_nodes_dict[value] = child_node\n    return list(child_nodes_dict.values())\ndef calculate_gain(examples, child_nodes, attr, target_attr):\n    \"\"\"\n    Calculate the information gain for the given attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples.\n    - child_nodes (list): A list of child nodes.\n    - attr (str): The name of the attribute.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The information gain for the given attribute.\n    \"\"\"\n    child_entropy = sum((calculate_entropy(child_node, target_attr) for child_node in child_nodes))\n    parent_entropy = calculate_entropy(examples, target_attr)\n    gain = parent_entropy - child_entropy\n    return gain"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a deterministic Pegasos kernel SVM classifier from scratch.\n\n    Parameters:\n    - data (np.ndarray): 2D array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): 1D array where each entry corresponds to the label of the sample.\n    - kernel (str): 'linear' or 'rbf' kernel to use.\n    - lambda_val (float): regularization parameter.\n    - iterations (int): number of iterations.\n    - sigma (float): standard deviation for RBF kernel.\n\n    Returns:\n    - alpha (list): list of alpha coefficients.\n    - bias (float): bias term.\n    \"\"\"\n    alpha = np.zeros((data.shape[0],))\n    bias = 0.0\n    if kernel == 'linear':\n\n        def kernel_func(x, y):\n            return np.dot(x, y)\n    elif kernel == 'rbf':\n\n        def kernel_func(x, y):\n            return np.exp(-(x - y) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Invalid kernel choice')\n    for _ in range(iterations):\n        K = np.dot(data, data.T)\n        if kernel == 'rbf':\n            K = np.exp(-(data - data.T) ** 2 / (2 * sigma ** 2))\n        grad = np.zeros((data.shape[0],))\n        for i in range(data.shape[0]):\n            for j in range(data.shape[0]):\n                if labels[i] != labels[j]:\n                    grad[i] += lambda_val * (labels[i] - np.dot(alpha, data[i]) - bias) * (labels[j] - np.dot(alpha, data[j]) - bias) * kernel_func(data[i], data[j])\n        alpha = np.maximum(alpha - lambda_val * grad, 0)\n        bias = np.mean(labels - np.dot(alpha, data) - bias)\n    return ([round(alpha[i], 4) for i in range(alpha.shape[0])], round(bias, 4))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n        z (float): The input value to the sigmoid function.\n\n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    exp_z = math.exp(-z)\n    sigmoid_z = 1 / (1 + exp_z)\n    return round(sigmoid_z, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n        scores (list[float]): A list of scores.\n\n    Returns:\n        list[float]: The softmax values as a list, each rounded to four decimal places.\n    \"\"\"\n    total = sum(scores)\n    exp_scores = [math.exp(score) for score in scores]\n    softmax_values = [exp_score / total for exp_score in exp_scores]\n    softmax_values = [round(value, 4) for value in softmax_values]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    - features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    - labels (list[int]): A list of true binary labels.\n    - weights (list[float]): A list of weights, one for each feature.\n    - bias (float): The neuron's bias.\n\n    Returns:\n    - A tuple containing:\n        - A list of predicted probabilities after sigmoid activation, rounded to four decimal places.\n        - The mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    dot_products = [sum((feature * weight for (feature, weight) in zip(feature_vector, weights))) + bias for feature_vector in features]\n    predicted_probabilities = [1 / (1 + math.exp(-dot_product)) for dot_product in dot_products]\n    mse = sum(((predicted_probability - label) ** 2 for (predicted_probability, label) in zip(predicted_probabilities, labels))) / len(labels)\n    predicted_probabilities = [round(probability, 4) for probability in predicted_probabilities]\n    mse = round(mse, 4)\n    return (np.array(predicted_probabilities).tolist(), mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    - features (np.ndarray): A 2D array of feature vectors.\n    - labels (np.ndarray): A 1D array of true binary labels.\n    - initial_weights (np.ndarray): A 1D array of initial weights.\n    - initial_bias (float): The initial bias value.\n    - learning_rate (float): The learning rate for gradient descent.\n    - epochs (int): The number of epochs to train the neuron.\n\n    Returns:\n    - updated_weights (np.ndarray): The updated weights after training.\n    - updated_bias (float): The updated bias after training.\n    - mse_values (list[float]): A list of MSE values for each epoch, rounded to four decimal places.\n    \"\"\"\n    updated_weights = initial_weights.tolist()\n    updated_bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        predicted_output = sigmoid(np.dot(features, updated_weights) + updated_bias)\n        error = labels - predicted_output\n        weight_gradients = np.dot(features.T, error)\n        bias_gradient = np.sum(error)\n        updated_weights = [w - learning_rate * g for (w, g) in zip(updated_weights, weight_gradients)]\n        updated_bias -= learning_rate * bias_gradient\n        mse = np.mean(error ** 2)\n        mse_values.append(round(mse, 4))\n    return (np.array(updated_weights), round(updated_bias, 4), mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        if isinstance(other, Value):\n            return Add(self, other)\n        else:\n            return Add(self, Scalar(other))\n\n    def __mul__(self, other):\n        if isinstance(other, Value):\n            return Mul(self, other)\n        else:\n            return Mul(self, Scalar(other))\n\n    def relu(self):\n        return ReLU(self)\nclass Add(Value):\n\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n        self._backward = self._backward_add\n\n    def _backward(self):\n        self.left._backward()\n        self.right._backward()\n        self.grad = self.left.grad + self.right.grad\nclass Mul(Value):\n\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n        self._backward = self._backward_mul\n\n    def _backward(self):\n        self.left._backward()\n        self.right._backward()\n        self.grad = self.left.grad * self.right.grad + self.left.grad * self.right.grad\nclass ReLU(Value):\n\n    def __init__(self, value):\n        self.value = value\n        self._backward = self._backward_relu\n\n    def _backward(self):\n        self.value._backward()\n        if self.value.grad > 0:\n            self.grad = self.value.grad\n        else:\n            self.grad = 0\nclass Scalar:\n\n    def __init__(self, value):\n        self.value = value\n\n    def __add__(self, other):\n        return Add(Scalar(self.value), other)\n\n    def __mul__(self, other):\n        return Mul(Scalar(self.value), other)\n\n    def __radd__(self, other):\n        return Add(other, self)\n\n    def __rmul__(self, other):\n        return Mul(other, self)\n\n    def __neg__(self):\n        return Scalar(-self.value)\n\n    def __abs__(self):\n        return Scalar(abs(self.value))\n\n    def __truediv__(self, other):\n        return Div(Scalar(self.value), other)\n\n    def __rtruediv__(self, other):\n        return Div(other, Scalar(self.value))\n\n    def __pow__(self, other):\n        return Pow(Scalar(self.value), other)\n\n    def __rpow__(self, other):\n        return Pow(other, Scalar(self.value))\nclass Div:\n\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n        self._backward = self._backward_div\n\n    def _backward(self):\n        self.left._backward()\n        self.right._backward()\n        self.grad = (self.left.grad * self.right.value - self.left.grad * self.right.value) / self.right.value ** 2\nclass Pow:\n\n    def __init__(self, base, exponent):\n        self.base = base\n        self.exponent = exponent\n        self._backward = self._backward_pow\n\n    def _backward(self):\n        self.base._backward()\n        self.grad = self.exponent * self.base.grad * self.base.value ** (self.exponent - 1)"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n\n    Args:\n    B (list[list[int]]): Basis vectors in basis B.\n    C (list[list[int]]): Basis vectors in basis C.\n\n    Returns:\n    list[list[float]]: Transformation matrix P from basis B to C.\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.inv(C).dot(B)\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix.\n\n    Parameters:\n    A (np.ndarray): 2x2 matrix.\n\n    Returns:\n    tuple: U, S, V such that A = U * S * V.\n    \"\"\"\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    V = eigenvectors\n    U = np.dot(A, V) / singular_values\n    U = np.round(U, 4).tolist()\n    V = np.round(V, 4).tolist()\n    singular_values = np.round(singular_values, 4).tolist()\n    return (U, singular_values, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Parameters:\n    X (numpy array): The feature array.\n    y (numpy array): The target array.\n    seed (int, optional): The seed for reproducibility. Defaults to None.\n\n    Returns:\n    tuple: A tuple containing the shuffled X and y arrays.\n    \"\"\"\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterator for dataset.\n\n    Args:\n        X (numpy array): Input data.\n        y (numpy array, optional): Target data. Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 64.\n\n    Yields:\n        list: Batch of (X, y) pairs or X only.\n    \"\"\"\n    if y is not None:\n        if X.shape[0] != y.shape[0]:\n            raise ValueError('X and y must have the same number of samples')\n    num_batches = (X.shape[0] + batch_size - 1) // batch_size\n    for i in range(num_batches):\n        start = i * batch_size\n        end = min((i + 1) * batch_size, X.shape[0])\n        if y is not None:\n            yield (X[start:end].tolist(), y[start:end].tolist())\n        else:\n            yield X[start:end].tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset into two subsets based on a specified feature and threshold.\n\n    Parameters:\n    X (numpy array): The input dataset.\n    feature_i (int): The index of the feature to divide on.\n    threshold (float): The threshold value.\n\n    Returns:\n    tuple: Two numpy arrays, the first with samples that meet the condition and the second with samples that do not.\n    \"\"\"\n    feature_values = X[:, feature_i]\n    mask = feature_values >= threshold\n    subset1 = X[mask]\n    subset2 = X[~mask]\n    return (subset1, subset2)\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\nfeature_i = 1\nthreshold = 7"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): Degree of polynomial features to generate.\n\n    Returns:\n    list: Polynomial features as a list of numpy arrays.\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_terms = (degree + 1) * (degree + 2) // 2\n    output = np.zeros((num_samples, num_terms))\n    for (i, feature) in enumerate(X.T):\n        for k in range(1, degree + 1):\n            for combination in combinations_with_replacement(range(k), k):\n                term = np.prod([feature[j] ** combination[j] for j in range(k)])\n                output[:, num_terms - k - 1] = term\n    output = output.tolist()\n    return output"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy array): 2D numpy array\n    y (numpy array): 1D numpy array\n    n_subsets (int): Number of subsets to generate\n    replacements (bool, optional): Whether to generate subsets with replacements. Defaults to True.\n    seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    list: List of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset)\n    \"\"\"\n    np.random.seed(seed)\n    num_samples = X.shape[0]\n    subset_size = X.shape[1]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(num_samples, size=subset_size, replace=True)\n        else:\n            indices = np.random.choice(num_samples, size=subset_size, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    x (numpy array): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it is automatically determined from the input array.\n\n    Returns:\n    list: One-hot encoded array as a python list.\n    \"\"\"\n    unique_values = np.unique(x)\n    n_col = len(unique_values) if n_col is None else n_col\n    encoded_array = np.eye(n_col)[x]\n    encoded_list = encoded_array.tolist()\n    return encoded_list"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (numpy.ndarray): 1D numpy array\n\n    Returns:\n    numpy.ndarray: 2D diagonal matrix\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (1D numpy array): The true labels.\n    y_pred (1D numpy array): The predicted labels.\n\n    Returns:\n    float: The accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    accuracy = round(accuracy, 4)\n    return accuracy"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    Y (2D numpy array, optional): Additional dataset. Defaults to None.\n\n    Returns:\n    2D numpy array: Correlation matrix.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    n_clf (int): Number of classifiers\n\n    Returns:\n    list: List of classifiers with their parameters\n    \"\"\"\n    w = np.ones_like(y) / len(y)\n    classifiers = []\n    errors = []\n    for i in range(n_clf):\n        best_threshold = None\n        best_error = 1\n        for j in range(X.shape[1]):\n            feature = X[:, j]\n            idx = np.argsort(feature)\n            mid = len(idx) // 2\n            left_idx = idx[:mid]\n            right_idx = idx[mid:]\n            left_error = np.mean(y[left_idx])\n            right_error = np.mean(y[right_idx])\n            error = (left_error + right_error) / 2\n            if error < best_error:\n                best_error = error\n                best_threshold = (left_error + right_error) / 2\n        weight = np.exp(-best_error * np.log(0.5))\n        w = w * weight\n        classifiers.append((best_threshold, weight))\n        errors.append(best_error)\n    return [round(threshold, 4) for (threshold, weight) in classifiers]"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list) -> list:\n    \"\"\"\n    Compute the log-softmax of a 1D numpy array of scores.\n\n    Args:\n    scores (list): A 1D numpy array of scores.\n\n    Returns:\n    list: The log-softmax of the input scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    shifted_scores = scores - max_score\n    exp_scores = np.exp(shifted_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_softmax = np.log(exp_scores / sum_exp_scores)\n    log_softmax = np.round(log_softmax, 4)\n    return log_softmax.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        \"\"\"\n        Initialize the weights and biases of the layer.\n        \"\"\"\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided for initialization.')\n        self.W = np.random.uniform(-1 / math.sqrt(self.input_shape[0]), 1 / math.sqrt(self.input_shape[0]), size=(self.input_shape[1], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def parameters(self):\n        \"\"\"\n        Return the total number of trainable parameters in the layer.\n        \"\"\"\n        return self.n_units * self.input_shape[1] + self.n_units\n\n    def forward_pass(self, X, training):\n        \"\"\"\n        Compute the output of the layer by performing a dot product between the input X and the weight matrix W, \n        and then adding the bias w0.\n        \"\"\"\n        if self.layer_input is None:\n            self.layer_input = X\n        output = np.dot(self.layer_input, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        Calculate and return the gradient with respect to the input.\n        If the layer is trainable, update the weights and biases using the optimizer's update rule.\n        \"\"\"\n        if self.trainable:\n            grad_X = np.dot(accum_grad, self.W.T)\n            if self.optimizer_W is not None:\n                self.optimizer_W.update(self.W, accum_grad)\n            if self.optimizer_w0 is not None:\n                self.optimizer_w0.update(self.w0, accum_grad)\n        return grad_X\n\n    def output_shape(self):\n        \"\"\"\n        Return the shape of the output produced by the forward pass.\n        \"\"\"\n        return (self.n_units,)\n\n    def set_input(self, X):\n        \"\"\"\n        Set the input of the layer.\n        \"\"\"\n        self.layer_input = X\noutput = dense_layer.forward_pass(np.random.rand(1, 784), training=True)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    This function implements a simple 2D convolutional layer.\n\n    Args:\n    input_matrix (np.ndarray): The input matrix to be processed.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The amount of padding to be applied.\n    stride (int): The stride of the convolution.\n\n    Returns:\n    list: The output of the convolutional layer as a list of lists.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            start_i = i * stride\n            start_j = j * stride\n            end_i = start_i + kernel_height\n            end_j = start_j + kernel_width\n            output_matrix[i, j] = np.sum(input_matrix[start_i:end_i, start_j:end_j] * kernel)\n    output_list = [round(num, 4) for row in output_matrix for num in row]\n    return output_list"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n        z (float): The input value to be passed through the ReLU function.\n\n    Returns:\n        float: The value after applying the ReLU function.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function.\n\n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n\n    Returns:\n    float: The Ridge loss value.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    ridge_loss = mse + reg_term\n    return round(ridge_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): Input value to be activated.\n        alpha (float, optional): Slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        float: Activated value after applying the Leaky ReLU function.\n    \"\"\"\n    if z < 0:\n        return alpha * z\n    else:\n        return z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Args:\n        x1 (numpy.ndarray): The first input vector.\n        x2 (numpy.ndarray): The second input vector.\n\n    Returns:\n        float: The linear kernel value between x1 and x2.\n    \"\"\"\n    assert x1.shape == x2.shape, 'Input vectors must have the same shape'\n    kernel_value = np.dot(x1, x2)\n    return kernel_value"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given two numpy arrays: y_true and y_pred.\n\n    Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric value.\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    precision_value = tp / (tp + fp) if tp + fp != 0 else 0\n    return precision_value"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent with MSE loss on a given dataset.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target vector\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations for gradient descent\n    batch_size (int, optional): Batch size for mini-batch gradient descent. Defaults to 1.\n    method (str, optional): Gradient descent variant. Can be 'sgd', 'batch', or 'mbgd'. Defaults to 'batch'.\n\n    Returns:\n    numpy array: Reshaped and rounded weights after gradient descent.\n    \"\"\"\n    weights = weights.tolist()\n\n    def mse_loss(weights):\n        predictions = np.dot(X, weights)\n        return np.mean((predictions - y) ** 2)\n    for _ in range(n_iterations):\n        if method == 'sgd':\n            weights = [w - learning_rate * np.dot(X[i], y - np.dot(X[i], weights)) for (i, w) in enumerate(weights)]\n        elif method == 'batch':\n            weights = [w - learning_rate * np.dot(X, y - np.dot(X, weights)) / len(X) for w in weights]\n        elif method == 'mbgd':\n            for i in range(0, len(X), batch_size):\n                batch_X = X[i:i + batch_size]\n                batch_y = y[i:i + batch_size]\n                weights = [w - learning_rate * np.dot(batch_X, batch_y - np.dot(batch_X, w)) / batch_size for w in weights]\n        else:\n            raise ValueError(\"Invalid method. Must be 'sgd', 'batch', or 'mbgd'.\")\n    return [round(w, 4) for w in weights]"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    This function converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Args:\n        matrix (numpy array): The input matrix to be converted into RREF.\n    \n    Returns:\n        list: The RREF of the input matrix as a list of lists.\n    \"\"\"\n    (num_rows, num_cols) = matrix.shape\n    matrix = matrix.copy()\n    for col in range(num_cols):\n        max_row = np.argmax(np.abs(matrix[:, col]))\n        if np.abs(matrix[max_row, col]) == 0:\n            continue\n        matrix[[max_row, col]] = matrix[[col, max_row]]\n        pivot = matrix[max_row, col]\n        matrix[max_row, :] /= pivot\n        for row in range(num_rows):\n            if row != max_row:\n                factor = matrix[row, col]\n                matrix[row, :] -= factor * matrix[max_row, :]\n    matrix = [row for row in matrix if any(row)]\n    return [list(row) for row in matrix]\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size (default: 0.001).\n    beta1 (float, optional): Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 (float, optional): Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon (float, optional): A small constant for numerical stability (default: 1e-8).\n    num_iterations (int, optional): Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n    numpy array: The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = x0.copy()\n    for _ in range(num_iterations):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** _)\n        v_hat = v / (1 - beta2 ** _)\n        lr = learning_rate * np.sqrt(v_hat) / (1 - beta1 ** _)\n        x = x - lr * m_hat\n    return np.round(x, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> list:\n    \"\"\"\n    Lasso Regression using Gradient Descent with L1 Regularization\n\n    Parameters:\n    X (np.array): Design matrix\n    y (np.array): Target vector\n    alpha (float, optional): Regularization parameter. Defaults to 0.1.\n    learning_rate (float, optional): Learning rate. Defaults to 0.01.\n    max_iter (int, optional): Maximum number of iterations. Defaults to 1000.\n    tol (float, optional): Tolerance for convergence. Defaults to 1e-4.\n\n    Returns:\n    list: Weights and bias after training\n    \"\"\"\n    w = np.zeros(X.shape[1])\n    b = 0\n    cost_history = []\n    gradient = np.zeros(X.shape[1])\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        loss = 1 / (2 * len(y)) * np.sum((y - y_pred) ** 2)\n        gradient = np.dot(X.T, y - y_pred) + alpha * np.sign(w)\n        w = w - learning_rate * gradient\n        b = b - learning_rate * np.sum(y - y_pred)\n        if len(cost_history) > 0 and np.abs(gradient).max() < tol:\n            break\n        cost_history.append(loss)\n    return [round(w, 4), round(b, 4)]"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n\n    The OSA distance represents the minimum number of edits required to transform one string into another.\n    The allowed edit operations are:\n    - Insert a character\n    - Delete a character\n    - Substitute a character\n    - Transpose two adjacent characters\n\n    Args:\n        source (str): The source string.\n        target (str): The target string.\n\n    Returns:\n        int: The minimum number of edits needed to convert the source string into the target string.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1))\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            substitution_cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + substitution_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return int(dp[m, n])"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in binary classification.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    recall_value = tp / (tp + fn) if tp + fn != 0 else 0.0\n    recall_value = round(recall_value, 3)\n    return recall_value"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention output.\n\n    Args:\n    X (numpy array): Input sequence.\n    W_q (numpy array): Query weights.\n    W_k (numpy array): Key weights.\n    W_v (numpy array): Value weights.\n\n    Returns:\n    numpy array: Self-attention output.\n    \"\"\"\n    attention_scores = np.matmul(X, W_q.T) / np.linalg.norm(W_q, axis=1, keepdims=True)\n    attention_weights = np.softmax(attention_scores, axis=1)\n    weighted_sum = np.matmul(attention_weights, W_v)\n    self_attention_output = weighted_sum\n    self_attention_output = np.round(self_attention_output, 4)\n    return self_attention_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Parameters:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    hidden_state = np.array(initial_hidden_state).reshape(1, -1)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector).reshape(1, -1)\n        weighted_sum = np.dot(Wx, input_vector) + np.dot(Wh, hidden_state)\n        weighted_sum += np.array(b).reshape(1, -1)\n        hidden_state = np.tanh(weighted_sum)\n    return np.round(hidden_state.tolist(), 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): Translation distance in the x direction.\n        ty (float): Translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n    translated_points_array = np.dot(translation_matrix, points_array.T).T\n    return translated_points_array.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    This function calculates the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): The mean of the first normal distribution.\n    sigma_p (float): The standard deviation of the first normal distribution.\n    mu_q (float): The mean of the second normal distribution.\n    sigma_q (float): The standard deviation of the second normal distribution.\n    \n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    sigma_p_squared = sigma_p ** 2\n    sigma_q_squared = sigma_q ** 2\n    log_ratio = np.log(sigma_p_squared / sigma_q_squared)\n    mean_diff = mu_p - mu_q\n    mean_diff_squared = mean_diff ** 2\n    kl_divergence = 0.5 * log_ratio + 0.5 * mean_diff_squared / sigma_q_squared\n    return kl_divergence"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implement the Gauss-Seidel method for solving a system of linear equations (Ax = b).\n\n    Parameters:\n    A (numpy array): Square matrix of coefficients\n    b (numpy array): Right-hand side vector\n    n (int): Number of iterations\n    x_ini (numpy array, optional): Initial guess for the solution vector (default: None)\n\n    Returns:\n    numpy array: Approximated solution vector (x) after n iterations\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square')\n    if x_ini is None:\n        x = np.zeros(A.shape[0])\n    else:\n        x = np.array(x_ini)\n    for _ in range(n):\n        x_temp = np.copy(x)\n        for i in range(A.shape[0]):\n            row_sum = sum((A[i, j] * x_temp[j] for j in range(i)))\n            x_temp[i] = (b[i] - row_sum) / A[i, i]\n        x = x_temp\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations using Gaussian Elimination with partial pivoting.\n\n    Parameters:\n    A (numpy array): Coefficient matrix\n    b (numpy array): Right-hand side vector\n\n    Returns:\n    x (list): Solution vector\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        max_row = np.argmax(np.abs(A[i:, i])) + i\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        pivot = A[i, i]\n        A[i] = A[i] / pivot\n        b[i] = b[i] / pivot\n        for j in range(i + 1, n):\n            factor = A[j, i]\n            A[j] = A[j] - factor * A[i]\n            b[j] = b[j] - factor * b[i]\n    for i in range(n - 1, -1, -1):\n        x[i] = b[i]\n        for j in range(i - 1, -1, -1):\n            x[j] = x[j] - A[j, i] * x[i]\n    return [round(xj, 4) for xj in x]\nA = np.array([[3, 2, -1], [6, -4, 8], [12, -10, 18]])\nb = np.array([9, 12, 15])\nx = gaussian_elimination(A, b)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        hidden_states = []\n        for t in range(len(x)):\n            f = np.tanh(np.dot(self.Wf, np.hstack((x[t], hidden_state))) + self.bf)\n            f = f * (1 - np.exp(-self.bi))\n            f = f * np.exp(-self.bc)\n            i = np.tanh(np.dot(self.Wi, np.hstack((x[t], hidden_state))) + self.bi)\n            i = i * (1 - np.exp(-self.bo))\n            c = np.tanh(np.dot(self.Wc, np.hstack((x[t], hidden_state))) + self.bc)\n            c = c * f\n            o = np.tanh(np.dot(self.Wo, np.hstack((x[t], hidden_state))) + self.bo)\n            o = o * i\n            hidden_state = o\n            cell_state = c\n            hidden_states.append(hidden_state)\n        return (np.array(hidden_states).tolist(), hidden_state, cell_state)\nx = np.array([[1, 2, 3], [4, 5, 6]])\ninitial_hidden_state = np.array([0.5, 0.5])\ninitial_cell_state = np.array([0.5, 0.5])"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty')\n    num_docs = len(corpus)\n    total_words = sum((len(doc) for doc in corpus))\n    df = {}\n    for doc in corpus:\n        for term in doc:\n            if term not in df:\n                df[term] = 1\n            else:\n                df[term] += 1\n    idf = {}\n    for term in df:\n        idf[term] = np.log((num_docs + 1) / (df[term] + 1))\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            if term in doc:\n                tf = doc.count(term) / len(doc)\n                tf_idf = tf * idf[term]\n                tf_idf = round(tf_idf, 4)\n                doc_scores.append(tf_idf)\n            else:\n                doc_scores.append(0)\n        tf_idf_scores.append(doc_scores)\n    return np.array(tf_idf_scores).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    if beta == 1:\n        f_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    f_score = round(f_score, 3)\n    return f_score"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        for i in range(len(x)):\n            h = np.tanh(np.dot(self.W_xh, x[i]) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x[i])\n            last_hiddens.append(h)\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected output.\n        \"\"\"\n        dW_xh = np.zeros((hidden_size, input_size))\n        dW_hh = np.zeros((hidden_size, hidden_size))\n        dW_hy = np.zeros((output_size, hidden_size))\n        db_h = np.zeros((hidden_size, 1))\n        db_y = np.zeros((output_size, 1))\n        for i in range(len(input_sequence)):\n            error = expected_output[i] - outputs[i]\n            dW_xh += np.dot(error * np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h), np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h))\n            dW_hh += np.dot(error * np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h), np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h))\n            dW_hy += np.dot(error * np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h), np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h))\n            db_h += error * np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h)\n            db_y += error * np.tanh(np.dot(W_xh, input_sequence[i]) + np.dot(W_hh, last_hiddens[i]) + b_h)\n        dW_xh /= len(input_sequence)\n        dW_hh /= len(input_sequence)\n        dW_hy /= len(input_sequence)\n        db_h /= len(input_sequence)\n        db_y /= len(input_sequence)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\ninput_size = 10\nhidden_size = 20\noutput_size = 10\nlearning_rate = 0.01\ninput_sequence = np.random.rand(100, input_size)\nexpected_output = np.random.rand(100, output_size)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    n = len(y)\n    class_counts = np.bincount(y)\n    class_probabilities = class_counts / n\n    gini = 1.0\n    for p in class_probabilities:\n        gini -= p ** 2\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_pointers = []\n    for i in range(num_rows):\n        row_pointer = 0\n        for j in range(num_cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                col_indices.append(j)\n                row_pointer += 1\n        row_pointers.append(row_pointer)\n    return (values, col_indices, row_pointers)\ndense_matrix = [[1, 0, 3, 0], [0, 5, 0, 7], [0, 0, 9, 0], [0, 0, 0, 11]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    magnitude_squared_L = np.dot(L, L)\n    scalar_factor = dot_product / magnitude_squared_L\n    projection = scalar_factor * L\n    projection = np.round(projection, 3)\n    return projection.tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for i in range(num_rows):\n        for j in range(num_cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n                column_pointer[j + 1] = column_pointer[j] + 1\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a given matrix A.\n\n    Args:\n    A (numpy.ndarray): Input matrix.\n\n    Returns:\n    list: Basis vectors that span the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    if not isinstance(A, np.ndarray):\n        raise ValueError('Input matrix must be a numpy array.')\n    if len(A.shape) != 2:\n        raise ValueError('Input matrix must be a 2D array.')\n    num_cols = A.shape[1]\n    basis_vectors = []\n    for i in range(num_cols):\n        is_independent = True\n        for j in range(len(basis_vectors)):\n            if np.allclose(A[:, i], basis_vectors[j] * A[:, i] / basis_vectors[j]):\n                is_independent = False\n                break\n        if is_independent:\n            basis_vectors.append(A[:, i])\n    basis_vectors = [list(map(round, vec, 8)) for vec in basis_vectors]\n    return basis_vectors"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): The true values.\n    y_pred (array-like): The predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    mean_y_true = np.mean(y_true)\n    ss_tot = np.sum((y_true - mean_y_true) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r_squared = 1 - ss_res / ss_tot\n    return round(r_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n        img (list of lists): A 2D matrix representing the image, where each element is a pixel value between 0 (black) and 255 (white).\n\n    Returns:\n        float: The average brightness of the image, rounded to two decimal places. Returns -1 for edge cases.\n    \"\"\"\n    if not img:\n        return -1\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n    total_brightness = 0\n    for row in img:\n        if not all((0 <= pixel <= 255 for pixel in row)):\n            return -1\n        total_brightness += sum(row)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Args:\n        y_true (numpy.ndarray): The actual values.\n        y_pred (numpy.ndarray): The predicted values.\n\n    Returns:\n        float: The RMSE value rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input arrays have mismatched shapes or if the inputs are not numpy arrays.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both inputs must be numpy arrays')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape')\n    if len(y_true) == 0:\n        raise ValueError('Input arrays cannot be empty')\n    diff = y_true - y_pred\n    squared_diff = diff ** 2\n    mean_squared_diff = np.mean(squared_diff)\n    rmse = np.sqrt(mean_squared_diff)\n    return round(rmse, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (numpy array): True labels.\n    y_pred (numpy array): Predicted labels.\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    elif intersection == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (numpy array): True labels.\n    y_pred (numpy array): Predicted labels.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FP + FN == 0:\n        return 0.0\n    else:\n        dice = 2 * TP / (2 * TP + FP + FN)\n        return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    hv = []\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, None)\n        if seed is not None:\n            hv_value = np.random.rand(dim) * seed\n        else:\n            hv_value = np.random.rand(dim)\n        hv_name = np.zeros(dim)\n        hv_feature = np.concatenate((hv_name, hv_value))\n        hv.append(hv_feature)\n    hv = np.array(hv).tolist()\n    return hv"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n        data (list of lists): A list of pairs, where each pair contains the actual label (`y_true`) and the predicted label (`y_pred`).\n\n    Returns:\n        list of lists: A 2x2 confusion matrix, where each element represents the number of observations with a specific combination of actual and predicted labels.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    tn = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1:\n            tp += 1\n        else:\n            fn += 1\n        if y_pred == 1:\n            fp += 1\n        else:\n            tn += 1\n    matrix = [[tp, fp], [fn, tn]]\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n        v1 (numpy array): The first input vector.\n        v2 (numpy array): The second input vector.\n\n    Returns:\n        float: The cosine similarity between the two vectors, rounded to three decimal places.\n    \"\"\"\n    assert v1.shape == v2.shape, 'Input vectors must have the same shape'\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nimport math\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Args:\n    - actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    - predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    - tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    if not all((x in [0, 1] for x in actual)) or not all((x in [0, 1] for x in predicted)):\n        raise ValueError('All elements in the actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = [[0, 0], [0, 0]]\n    for (a, p) in zip(actual, predicted):\n        if a == 1:\n            confusion_matrix[0][1] += 1\n        else:\n            confusion_matrix[1][0] += 1\n    accuracy = (confusion_matrix[0][0] + confusion_matrix[1][1]) / len(actual)\n    tp = confusion_matrix[0][0]\n    fp = confusion_matrix[0][1]\n    fn = confusion_matrix[1][0]\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    f1_score = round(f1_score, 3)\n    specificity = confusion_matrix[1][1] / (confusion_matrix[1][0] + confusion_matrix[1][1]) if confusion_matrix[1][1] + confusion_matrix[1][0] > 0 else 0\n    specificity = round(specificity, 3)\n    npv = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[1][0]) if confusion_matrix[1][1] + confusion_matrix[1][0] > 0 else 0\n    npv = round(npv, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, npv)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Parameters:\n    data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    if not isinstance(data, (list, np.ndarray)):\n        raise ValueError('Input data must be a list or NumPy array')\n    if not all((isinstance(x, (int, float)) for x in data)):\n        raise ValueError('All elements in the data must be numerical')\n    mean = round(np.mean(data), 4)\n    median = round(np.median(data), 4)\n    mode = round(np.bincount(data).argmax(), 4)\n    variance = round(np.var(data), 4)\n    std_dev = round(np.std(data), 4)\n    percentiles = {'25th_percentile': round(np.percentile(data, 25), 4), '50th_percentile': round(np.percentile(data, 50), 4), '75th_percentile': round(np.percentile(data, 75), 4)}\n    iqr = round(np.subtract(*np.percentile(data, [75, 25])), 4)\n    output = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': std_dev, '25th_percentile': percentiles['25th_percentile'], '50th_percentile': percentiles['50th_percentile'], '75th_percentile': percentiles['75th_percentile'], 'interquartile_range': iqr}\n    return output"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not isinstance(n, int) or not isinstance(k, int) or (not isinstance(p, (int, float))):\n        raise ValueError('n, k, and p must be integers or floats')\n    if n < 0 or k < 0 or p < 0 or (p > 1):\n        raise ValueError('n, k, and p must be non-negative and p must be less than or equal to 1')\n    probability = math.comb(n, k) * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 3\np = 0.5\nprobability = binomial_probability(n, k, p)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(exponent)\n    return round(pdf, 5)\nx = 1.5\nmean = 0\nstd_dev = 1"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    probability = round(probability, 5)\n    return probability"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    img = np.array(img)\n    if len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D array')\n    if not np.all((img >= 0) & (img <= 255)):\n        raise ValueError('All pixel values must be between 0 and 255')\n    contrast = np.max(img) - np.min(img)\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two input vectors.\n\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Input vectors must be of the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_features = []\n    for point in data_array:\n        point_features = []\n        for i in range(degree + 1):\n            for j in range(degree + 1 - i):\n                coefficient = np.math.comb(degree, i) * np.math.comb(degree, j)\n                value = coefficient * point ** i * point ** j\n                point_features.append(value)\n        transformed_features.append([round(feature, 8) for feature in point_features])\n    return transformed_features"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int) -> list:\n    \"\"\"\n    Calculate positional encodings for a sequence length (position) and model dimensionality (d_model)\n    using sine and cosine functions as specified in the Transformer architecture.\n\n    Args:\n        position (int): The sequence length.\n        d_model (int): The model dimensionality.\n\n    Returns:\n        list: The positional encoding array of dtype np.float16 converted to a list.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model))\n    for i in range(position):\n        for j in range(d_model):\n            pe[i, j] = 1 / np.power(10000, 2 * i / d_model + j / d_model)\n            pe[i, j] += np.sin(pe[i, j] * np.pi)\n    return pe.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_corrected = beta1 * m + (1 - beta1) * grad\n    v_corrected = beta2 * v + (1 - beta2) * grad ** 2\n    adjusted_learning_rate = learning_rate * np.sqrt(v_corrected / (1 - beta2 ** t))\n    updated_parameter = parameter - adjusted_learning_rate * m_corrected / (1 - beta1 ** t)\n    updated_m = beta1 * m + (1 - beta1) * grad\n    updated_v = beta2 * v + (1 - beta2) * grad ** 2\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(updated_m, 5).tolist()\n    updated_v = np.round(updated_v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n\n    class DummyModel(nn.Module):\n\n        def __init__(self):\n            super(DummyModel, self).__init__()\n            self.token_embeddings = nn.Embedding(hparams['n_ctx'], hparams['n_ctx'])\n            self.positional_embeddings = nn.Embedding(hparams['n_ctx'], hparams['n_ctx'])\n            self.self_attn = MultiHeadAttention(hparams['n_head'], hparams['n_ctx'])\n            self.feed_forward = nn.Linear(hparams['n_ctx'], hparams['n_ctx'])\n\n        def forward(self, x):\n            x = self.token_embeddings(x)\n            x = x + self.positional_embeddings(x)\n            x = self.self_attn(x)\n            x = self.feed_forward(x)\n            return x\n    model = DummyModel()\n    for param in params.values():\n        for (key, value) in param.items():\n            if isinstance(value, np.ndarray):\n                param[key] = torch.tensor(value, dtype=torch.float32)\n    input_ids = torch.tensor([encoder.encode(prompt)], dtype=torch.long)\n    output_ids = torch.zeros((1, n_tokens_to_generate), dtype=torch.long)\n    hidden_state = torch.zeros((1, hparams['n_ctx'], hparams['n_ctx']))\n    for i in range(n_tokens_to_generate):\n        output = model(input_ids)\n        hidden_state = output\n        last_token_id = output[:, -1, :].argmax(dim=1)\n        output_ids = torch.cat((output_ids, last_token_id.unsqueeze(-1)), dim=1)\n    output_text = encoder.decode(output_ids)\n    return output_text\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, n_head, n_ctx):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.n_ctx = n_ctx\n        self.query_linear = nn.Linear(n_ctx, n_head * n_ctx)\n        self.key_linear = nn.Linear(n_ctx, n_head * n_ctx)\n        self.value_linear = nn.Linear(n_ctx, n_head * n_ctx)\n        self.out_linear = nn.Linear(n_head * n_ctx, n_ctx)\n\n    def forward(self, query, key, value):\n        query = self.query_linear(query)\n        key = self.key_linear(key)\n        value = self.value_linear(value)\n        attention_scores = torch.matmul(query, key.T) / math.sqrt(self.n_ctx)\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, value)\n        output = self.out_linear(attention_output)\n        return output\nprompt = 'Hello, world!'\nn_tokens_to_generate = 10\noutput_text = gen_text(prompt, n_tokens_to_generate)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef softmax(values):\n    \"\"\"\n    Compute the softmax of a vector of values.\n\n    Args:\n    values (numpy.ndarray): The input values.\n\n    Returns:\n    numpy.ndarray: The softmax of the input values.\n    \"\"\"\n    e_values = np.exp(values - np.max(values))\n    return e_values / e_values.sum()\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    Compute the self-attention mechanism for a sequence of crystals.\n\n    Args:\n    n (int): The number of crystals.\n    crystal_values (numpy.ndarray): The values of the crystals.\n    dimension (int): The dimension of the crystals.\n\n    Returns:\n    list: A list of floats, each representing the weighted pattern for a crystal.\n    \"\"\"\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            dot_product = np.dot(crystal_values[i], crystal_values[j])\n            attention_score = softmax(dot_product)\n            attention_scores[i, j] = attention_score\n    weighted_patterns = np.zeros((n, dimension))\n    for i in range(n):\n        weighted_pattern = np.sum(attention_scores[i], axis=0)\n        weighted_pattern /= np.sum(weighted_pattern)\n        weighted_patterns[i] = weighted_pattern\n    return [round(pattern[0], 4) for pattern in weighted_patterns]\nn = 3\ncrystal_values = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query in a corpus.\n\n    Parameters:\n    corpus (list): A list of documents in the corpus.\n    query (str): The query to be searched in the corpus.\n    k1 (float, optional): The parameter for term frequency saturation. Defaults to 1.5.\n    b (float, optional): The parameter for document length normalization. Defaults to 0.75.\n\n    Returns:\n    list: A list of BM25 scores for each document in the corpus.\n    \"\"\"\n    query_terms = query.split()\n    scores = {}\n    for (i, doc) in enumerate(corpus):\n        doc_terms = doc.split()\n        tf = [doc_terms.count(term) for term in query_terms]\n        idf = np.log(len(corpus) / (1 + np.sum([1 for term in set(doc_terms) if term in query_terms])))\n        bm25_score = k1 * tf[0] * (1 + b * (len(doc_terms) / len(corpus))) * idf\n        scores[i] = bm25_score\n    return [round(score, 3) for score in scores.values()]"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must have the same length')\n    tp = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 1))\n    fp = sum((1 for (t, p) in zip(y_true, y_pred) if t == 0 and p == 1))\n    fn = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    f1 = round(f1, 3)\n    return f1"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Forecasts the power grid's base consumption for day 15, taking into account the daily fluctuation and a 5% safety margin.\n\n    Parameters:\n    consumption_data (list): A list of daily power usage measurements (10 days)\n\n    Returns:\n    int: The predicted base consumption for day 15, rounded up with a 5% safety margin\n    \"\"\"\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = [x - y for (x, y) in zip(consumption_data, fluctuation)]\n    A = np.array(detrended_data).reshape(-1, 1)\n    coefficients = np.linalg.lstsq(A, np.ones(len(detrended_data)), rcond=None)[0]\n    predicted_consumption = coefficients[0] * 15 + coefficients[1]\n    predicted_consumption += 10 * math.sin(2 * PI * 15 / 10)\n    safety_margin = math.ceil(predicted_consumption * 1.05)\n    return safety_margin"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'Input arrays must have the same length'\n    abs_diff = np.abs(y_true - y_pred)\n    mean_abs_diff = np.mean(abs_diff)\n    mae = round(mean_abs_diff, 3)\n    return mae\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.0])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n    X (np.ndarray): Input tensor.\n    W_q (np.ndarray): Query weights.\n    W_k (np.ndarray): Key weights.\n    W_v (np.ndarray): Value weights.\n\n    Returns:\n    tuple: (Q, K, V)\n    \"\"\"\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the attention scores and weights.\n\n    Args:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n\n    Returns:\n    np.ndarray: Attention weights.\n    \"\"\"\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    return attention_weights\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement the multi-head attention mechanism.\n\n    Args:\n    X (np.ndarray): Input tensor.\n    W_q (np.ndarray): Query weights.\n    W_k (np.ndarray): Key weights.\n    W_v (np.ndarray): Value weights.\n    n_heads (int): Number of attention heads.\n\n    Returns:\n    list: List of attention outputs for each head.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    attention_weights = [self_attention(Q[:, i * n_heads:(i + 1) * n_heads], K[:, i * n_heads:(i + 1) * n_heads], V[:, i * n_heads:(i + 1) * n_heads]) for i in range(n_heads)]\n    attention_outputs = [np.matmul(attention_weights[i], V[:, i * n_heads:(i + 1) * n_heads]) for i in range(n_heads)]\n    attention_output = np.concatenate(attention_outputs, axis=-1)\n    return np.round(attention_output, 4).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length')\n    num_x = sum(x)\n    num_y = sum(y)\n    intersection = sum((1 for (a, b) in zip(x, y) if a == b))\n    phi = intersection / len(x) * (intersection / len(y))\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -0.5:\n        return 0\n    elif x > 0.5:\n        return 1\n    else:\n        sigmoid_x = 1 / (1 + 2.5 ** (-x - 0.5))\n        return 0.2 * sigmoid_x"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * (math.exp(x / alpha) - 1)\n    else:\n        return x"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -1000:\n        return -1000.0\n    if x > 1000:\n        return math.log(1 + math.exp(x))\n    softplus_value = math.log(1 + math.exp(x))\n    return round(softplus_value, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    x = float(x)\n    softsign_value = x / (1 + abs(x))\n    softsign_value = max(-1, min(softsign_value, 1))\n    softsign_value = round(softsign_value, 4)\n    return softsign_value"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_theta_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.sum(A * np.log(clipped_rhos)) - kl_divergence * beta\n    grpo_obj = round(grpo_obj, 6)\n    return grpo_obj"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    swish_value = x * np.exp(math.log(1 - x + x ** 2) / 2)\n    swish_value = round(swish_value, 4)\n    return swish_value"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    abs_x = abs(x)\n    selu_value = scale * np.maximum(abs_x - alpha, 0)\n    return round(selu_value, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    dot_product = np.dot(X, weights)\n    linear_combination = dot_product + bias\n    sigmoid = 1 / (1 + np.exp(-linear_combination))\n    predictions = np.where(sigmoid > 0.5, 1, 0)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_classes = len(np.unique(y))\n    B = np.zeros((num_classes, X.shape[1]))\n    losses = []\n    for _ in range(iterations):\n        predictions = np.exp(X @ B) / np.sum(np.exp(X @ B), axis=0, keepdims=True)\n        gradient = X.T @ (predictions - y) / predictions\n        B -= learning_rate * gradient\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        losses.append(round(loss, 4))\n    return ([round(param, 4) for param in B.flatten().tolist()], losses)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])\nlearning_rate = 0.01\niterations = 1000"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations for training.\n\n    Returns:\n    tuple[list[float], list[float]]: Optimized coefficients and loss values over iterations.\n    \"\"\"\n    theta = np.zeros(X.shape[1])\n    losses = []\n    coefficients = [theta.tolist()]\n    for _ in range(iterations):\n        predictions = 1 / (1 + np.exp(-np.dot(X, theta)))\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, predictions - y) / len(y)\n        theta -= learning_rate * gradient\n        coefficients.append(theta.tolist())\n    return (coefficients, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_scores = attention_scores * mask\n    weighted_sum = np.dot(attention_scores, V)\n    return weighted_sum"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n\n    Args:\n    apples (list): A list of integers representing the colors of the apples.\n\n    Returns:\n    float: The disorder of the apples, rounded to the nearest 4th decimal.\n    \"\"\"\n    color_counts = Counter(apples)\n    num_colors = len(color_counts)\n    disorder = -sum((count / num_colors * math.log2(count / num_colors) for count in color_counts.values()))\n    return round(disorder, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n    gamma (np.ndarray): Scaling parameters with shape (feature_dim,)\n    beta (np.ndarray): Shifting parameters with shape (feature_dim,)\n    epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: Normalized X values rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=1, keepdims=True)\n    std = np.std(X, axis=1, keepdims=True)\n    X_scaled = (X - mean) * gamma + beta\n    X_normalized = X_scaled / (std + epsilon)\n    X_normalized = [round(x, 5) for x in X_normalized.flatten().tolist()]\n    return X_normalized"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Parameters:\n    - reference (list): The reference translation.\n    - candidate (list): The candidate translation.\n    - alpha (float, optional): Weight for unigram matches. Defaults to 0.9.\n    - beta (float, optional): Weight for precision. Defaults to 3.\n    - gamma (float, optional): Weight for recall. Defaults to 0.5.\n\n    Returns:\n    - float: The METEOR score, rounded to 3 decimal places.\n    \"\"\"\n    reference = [word.lower() for word in reference]\n    candidate = [word.lower() for word in candidate]\n    unigrams = np.sum([1 for i in range(len(reference)) if reference[i] == candidate[i]])\n    precision = unigrams / len(set(candidate))\n    recall = unigrams / len(reference)\n    f_mean = 2 * (precision * recall) / (precision + recall)\n    fragmentation_penalty = len(set(candidate)) / len(set(reference))\n    meteor = alpha * unigrams / len(reference) + beta * f_mean + gamma * (1 - fragmentation_penalty)\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    joint_counts (numpy array): Joint occurrence count of two events.\n    total_counts_x (numpy array): Individual count of event X.\n    total_counts_y (numpy array): Individual count of event Y.\n    total_samples (int): Total number of samples.\n\n    Returns:\n    float: PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_count = total_counts_x * total_counts_y / total_samples\n    pmi_values = np.log2(joint_counts / expected_joint_count)\n    avg_pmi = np.mean(pmi_values)\n    avg_pmi = round(avg_pmi, 3)\n    return avg_pmi"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of floats representing the normalized values, \n        rounded to 4 decimal places.\n    \"\"\"\n    min_value = min(x)\n    max_value = max(x)\n    if min_value == max_value:\n        return []\n    range_value = max_value - min_value\n    normalized_values = []\n    for value in x:\n        shifted_value = value - min_value\n        normalized_value = shifted_value / range_value\n        normalized_values.append(round(normalized_value, 4))\n    return normalized_values"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): The input 1D array.\n    w1 (np.ndarray): The first weight matrix.\n    w2 (np.ndarray): The second weight matrix.\n    \n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    x1 = np.matmul(x, w1)\n    x2 = np.matmul(x1, w2)\n    x2 = np.maximum(x2, 0)\n    x2 = x2 + x\n    x2 = np.maximum(x2, 0)\n    x2 = np.round(x2, 4)\n    return x2.tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Args:\n        x (np.ndarray): Input feature maps with shape (height, width, channels)\n\n    Returns:\n        np.ndarray: A 1D array of shape (channels,) where each element is the average of all values in the corresponding feature map\n    \"\"\"\n    num_channels = x.shape[2]\n    avg_pool = np.mean(x, axis=(0, 1))\n    return avg_pool"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n\n    Args:\n        X (np.ndarray): Input feature maps in BCHW format (batch, channels, height, width)\n        gamma (np.ndarray): Scale (gamma) parameters for each channel\n        beta (np.ndarray): Shift (beta) parameters for each channel\n        epsilon (float, optional): Epsilon value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n        list: Normalized feature maps in BCHW format, rounded to 4 decimal places and converted to a list\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    std = np.std(X, axis=(0, 2, 3), keepdims=True)\n    normalized_X = (X - mean) / std + beta\n    normalized_X = gamma * normalized_X\n    normalized_X = [round(x, 4) for x in normalized_X.tolist()]\n    return normalized_X"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form c * x^n at a given point x.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative, rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('Exponent must be a non-negative integer.')\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    basis = [np.array(v) for v in vectors]\n    for i in range(1, len(vectors)):\n        proj = np.dot(basis[i - 1], basis[i - 1]) / np.dot(basis[i - 1], basis[i - 1]) * basis[i - 1]\n        basis[i] -= proj\n        norm = np.linalg.norm(basis[i])\n        if norm > tol:\n            basis[i] /= norm\n    return [list(map(round, v, 4)) for v in basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Parameters:\n    a (list or numpy array): The first 3D vector.\n    b (list or numpy array): The second 3D vector.\n\n    Returns:\n    list: The cross product of the two input vectors, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Input vectors must be 3D')\n    cross = np.cross(a, b)\n    result = [round(x, 4) for x in cross.tolist()]\n    return result"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n\n    Parameters:\n    A (numpy array): Square coefficient matrix.\n    b (numpy array): Constant vector.\n\n    Returns:\n    x (numpy array): Solution vector. If the system has no unique solution, returns -1.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = np.zeros_like(b)\n    det_A_x = np.linalg.det(np.copy(A))\n    det_A_y = np.linalg.det(np.copy(A[:, 1:]))\n    det_A_z = np.linalg.det(np.copy(A[:, 1:]))\n    x[0] = det_A_x / det_A\n    x[1] = det_A_y / det_A\n    x[2] = det_A_z / det_A\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Args:\n    p (list[float]): The first probability distribution.\n    q (list[float]): The second probability distribution.\n\n    Returns:\n    float: The Bhattacharyya distance between the two distributions, rounded to 4 decimal places.\n    \"\"\"\n    if len(p) != len(q):\n        return 0.0\n    if len(p) == 0:\n        return 0.0\n    min_len = min(len(p), len(q))\n    distance = 0.0\n    for i in range(min_len):\n        product = p[i] * q[i]\n        distance += product\n    distance = np.sqrt(distance)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n\n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    n_experts (int): Number of experts in the MoE layer.\n    k_active (int): Number of active experts.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n\n    Returns:\n    float: Computational cost savings percentage.\n    \"\"\"\n    dense_weights = d_in * d_out\n    moe_weights = k_active * d_in + (n_experts - k_active) * d_out\n    dense_flops = dense_weights * 2\n    moe_flops = moe_weights * 2\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    savings = round(savings, 1)\n    return savings"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the final gating probabilities matrix using the Noisy Top-K Gating mechanism.\n\n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrices.\n    W_noise (np.ndarray): Pre-sampled noise.\n    N (np.ndarray): Number of experts.\n    k (int): Sparsity constraint.\n\n    Returns:\n    list: Final gating probabilities matrix.\n    \"\"\"\n    noisy_gating = np.dot(X, W_g) + np.dot(W_noise, N)\n    top_k_gating = np.argsort(noisy_gating, axis=1)[:, -k:]\n    final_gating = np.zeros_like(noisy_gating)\n    for i in range(noisy_gating.shape[0]):\n        final_gating[i, top_k_gating[i]] = 1\n    final_gating = np.round(final_gating, 4)\n    final_gating_list = [list(x) for x in final_gating.tolist()]\n    return final_gating_list"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Compute the final MoE output by selecting the top-k experts per token, \n    applying their transformations, and aggregating the results weighted by the normalized gating probabilities.\n\n    Args:\n    x (np.ndarray): Input tensor.\n    We (np.ndarray): Set of expert weight matrices.\n    Wg (np.ndarray): Gating weight matrix.\n    n_experts (int): Number of experts.\n    top_k (int): Top-k experts to select.\n\n    Returns:\n    list: Final MoE output.\n    \"\"\"\n    gating_probabilities = np.exp(Wg * x) / np.sum(np.exp(Wg * x), axis=1, keepdims=True)\n    normalized_gating_probabilities = gating_probabilities / np.sum(gating_probabilities, axis=1, keepdims=True)\n    top_k_experts = np.argsort(-normalized_gating_probabilities, axis=1)[:, :top_k]\n    moe_outputs = np.zeros_like(x)\n    for i in range(n_experts):\n        moe_outputs += We[i, top_k_experts] * x\n    final_output = np.sum(moe_outputs * normalized_gating_probabilities, axis=1)\n    final_output = np.round(final_output, 4)\n    final_output = final_output.tolist()\n    return final_output"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (B, C, H, W)\n    gamma (np.ndarray): Learned scale for each group\n    beta (np.ndarray): Learned shift for each group\n    num_groups (int): Number of groups to normalize over\n    epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: Normalized tensor with shape (B, C, H, W)\n    \"\"\"\n    channels_per_group = X.shape[1] // num_groups\n    normalized_X = np.zeros_like(X)\n    for i in range(num_groups):\n        start_idx = i * channels_per_group\n        end_idx = (i + 1) * channels_per_group\n        group_channels = X[:, start_idx:end_idx]\n        mean = np.mean(group_channels, axis=(0, 2, 3), keepdims=True)\n        std = np.std(group_channels, axis=(0, 2, 3), keepdims=True)\n        normalized_group = (group_channels - mean) / (std + epsilon)\n        normalized_group = gamma[i] * normalized_group + beta[i]\n        normalized_X[:, start_idx:end_idx] = normalized_group\n    normalized_X = np.round(normalized_X, 4)\n    return normalized_X.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the minimum of the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n\n    Args:\n    start_x (float): The initial position.\n    learning_rate (float, optional): The step size for each iteration. Defaults to 0.1.\n    tolerance (float, optional): The maximum difference between the current estimate and the previous estimate. Defaults to 1e-6.\n    max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    float: The x value where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def f_prime(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    prev_x = x\n    iter_count = 0\n    while iter_count < max_iters:\n        x = x - learning_rate * f_prime(x)\n        if abs(x - prev_x) < tolerance:\n            break\n        prev_x = x\n        iter_count += 1\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n\n    Args:\n    x (np.ndarray): Input array\n    alpha (float): Alpha value for DyT\n    gamma (float): Gamma value for DyT\n    beta (float): Beta value for DyT\n\n    Returns:\n    list[float]: List of DyT values\n    \"\"\"\n    dyt_values = alpha * np.tanh(x) + (1 - alpha) * gamma * np.tanh(beta * x)\n    dyt_values = np.round(dyt_values, 4)\n    return dyt_values.tolist()"}
