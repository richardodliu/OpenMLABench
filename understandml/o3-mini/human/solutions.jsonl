{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b:\n        return -1\n    expected_cols = len(a[0])\n    for row in a:\n        if len(row) != expected_cols:\n            return -1\n    if expected_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    np_a = np.array(a)\n    if np_a.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped = np_a.reshape(new_shape)\n    return reshaped.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        return [sum(row) / len(row) if row else 0.0 for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        means = []\n        for col in range(num_cols):\n            col_sum = 0.0\n            for row in matrix:\n                col_sum += row[col]\n            means.append(col_sum / num_rows)\n        return means\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    import math\n    if discriminant < 0:\n        raise ValueError('The given matrix has complex eigenvalues.')\n    sqrt_disc = math.sqrt(discriminant)\n    eigen1 = (trace + sqrt_disc) / 2\n    eigen2 = (trace - sqrt_disc) / 2\n    return sorted([eigen1, eigen2], reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms the matrix A using the operation T^{-1} * A * S.\n    \n    Parameters:\n      - A: Matrix to be transformed. Its dimensions should be (m x n).\n      - T: Invertible matrix of size (m x m).\n      - S: Invertible matrix of size (n x n).\n      \n    Returns:\n      - The transformed matrix with each element rounded to 4 decimal places as a Python list (via numpy's tolist()).\n      - If T or S is not invertible (i.e., their determinants are zero or close to zero), returns -1.\n    \"\"\"\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    (m, n) = A_np.shape\n    if T_np.shape != (m, m):\n        return -1\n    if S_np.shape != (n, n):\n        return -1\n    try:\n        det_T = np.linalg.det(T_np)\n        det_S = np.linalg.det(S_np)\n    except np.linalg.LinAlgError:\n        return -1\n    tol = 1e-10\n    if abs(det_T) < tol or abs(det_S) < tol:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    transformed = T_inv @ A_np @ S_np\n    transformed_rounded = np.around(transformed, decimals=4)\n    return transformed_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    if not (len(matrix) == 2 and all((len(row) == 2 for row in matrix))):\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_det = 1 / determinant\n    inverse_matrix = [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not b or (not a[0]) or (not b[0]):\n        return -1\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    for row in a:\n        if len(row) != num_cols_a:\n            return -1\n    num_cols_b = len(b[0])\n    for row in b:\n        if len(row) != num_cols_b:\n            return -1\n    if num_cols_a != num_rows_b:\n        return -1\n    result = [[0 for _ in range(num_cols_b)] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors:\n        raise ValueError(\"The input list 'vectors' must contain at least one feature list.\")\n    num_features = len(vectors)\n    n = len(vectors[0])\n    for feature in vectors:\n        if len(feature) != n:\n            raise ValueError('All feature lists must have the same number of observations.')\n    if n < 2:\n        raise ValueError('At least two observations are needed to compute covariance.')\n    means = []\n    for feature in vectors:\n        means.append(sum(feature) / n)\n    cov_matrix = [[0.0 for _ in range(num_features)] for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(i, num_features):\n            cov_sum = 0.0\n            for k in range(n):\n                cov_sum += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_value = cov_sum / (n - 1)\n            cov_matrix[i][j] = cov_value\n            cov_matrix[j][i] = cov_value\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = A.shape[0]\n    x = np.zeros(N)\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(N):\n            s = 0.0\n            for j in range(N):\n                if i != j:\n                    s += A[i, j] * x[j]\n            new_x[i] = (b[i] - s) / A[i, i]\n        x = np.round(new_x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    B = A.T @ A\n    a = B[0, 0]\n    b = B[0, 1]\n    d = B[1, 1]\n    tol = 1e-10\n    if abs(b) < tol:\n        eig1 = a\n        eig2 = d\n    else:\n        tau = (d - a) / (2 * b)\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1 + tau ** 2))\n        eig1 = a - t * b\n        eig2 = d + t * b\n    eig1 = max(eig1, 0)\n    eig2 = max(eig2, 0)\n    sigma1 = np.sqrt(eig1)\n    sigma2 = np.sqrt(eig2)\n    s_vals = sorted([sigma1, sigma2], reverse=True)\n    s_vals_rounded = (round(s_vals[0], 4), round(s_vals[1], 4))\n    return s_vals_rounded"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        det = 0.0\n        for col in range(n):\n            minor = [row[:col] + row[col + 1:] for row in mat[1:]]\n            sign = (-1) ** col\n            det += sign * mat[0][col] * determinant(minor)\n        return det\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input must be a 4x4 matrix.')\n    return determinant(matrix)"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_mat = np.array(X)\n    y_vec = np.array(y)\n    XT_X = X_mat.T.dot(X_mat)\n    XT_y = X_mat.T.dot(y_vec)\n    theta = np.linalg.inv(XT_X).dot(XT_y)\n    theta_rounded = [round(coef, 4) for coef in theta]\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    theta = np.zeros((X.shape[1], 1))\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    safe_stds = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / safe_stds\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    ranges = np.where(maxs - mins == 0, 1, maxs - mins)\n    normalized = (data - mins) / ranges\n    standardized = np.around(standardized, 4)\n    normalized = np.around(normalized, 4)\n    return (standardized.tolist(), normalized.tolist())"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids, dtype=float)\n    for iteration in range(max_iterations):\n        distances = np.sqrt(((points_np[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n        assignments = np.argmin(distances, axis=1)\n        new_centroids = np.copy(centroids)\n        for i in range(k):\n            assigned_points = points_np[assignments == i]\n            if assigned_points.shape[0] > 0:\n                new_centroids[i] = assigned_points.mean(axis=0)\n        if np.allclose(new_centroids, centroids, atol=1e-08):\n            break\n        centroids = new_centroids\n    centroids_rounded = np.round(centroids, 4)\n    final_centroids = [tuple(coord) for coord in centroids_rounded.tolist()]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation.\n    \n    Parameters:\n    X (np.ndarray): Features matrix with shape (n_samples, n_features).\n    y (np.ndarray): Labels array with shape (n_samples,).\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    random_seed (int or None): Seed for random shuffling. Default is None.\n    \n    Returns:\n    list of tuples: Each tuple contains (train_indices, test_indices) for the respective fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples != len(y):\n        raise ValueError('X and y must have the same number of samples')\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    current = 0\n    splits = []\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate([indices[:start], indices[stop:]])\n        splits.append((train_idx, test_idx))\n        current = stop\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=0)\n    stds[stds == 0] = 1\n    standardized_data = (data - means) / stds\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4).tolist()\n    return principal_components_rounded"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(examples, target_attr):\n        freq = Counter((example[target_attr] for example in examples))\n        total = len(examples)\n        ent = 0.0\n        for count in freq.values():\n            p = count / total\n            ent -= p * math.log(p, 2)\n        return ent\n\n    def information_gain(examples, attr, target_attr):\n        remainder = 0.0\n        total = len(examples)\n        attr_values = {}\n        for example in examples:\n            value = example[attr]\n            attr_values.setdefault(value, []).append(example)\n        for subset in attr_values.values():\n            remainder += len(subset) / total * entropy(subset, target_attr)\n        return entropy(examples, target_attr) - remainder\n\n    def plural_vote(examples, target_attr):\n        freq = Counter((example[target_attr] for example in examples))\n        return freq.most_common(1)[0][0]\n    if not examples:\n        return None\n    target_values = [example[target_attr] for example in examples]\n    if all((val == target_values[0] for val in target_values)):\n        return target_values[0]\n    if not attributes:\n        return plural_vote(examples, target_attr)\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {'attribute': best_attr, 'branches': {}}\n    values = set((example[best_attr] for example in examples))\n    for value in values:\n        subset = [example for example in examples if example[best_attr] == value]\n        new_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n        tree['branches'][value] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        alpha = (1 - eta * lambda_val) * alpha\n        for i in range(n_samples):\n            if kernel == 'linear':\n                k_vals = np.dot(data, data[i])\n            elif kernel == 'RBF':\n                diff = data - data[i]\n                sq_dists = np.sum(diff * diff, axis=1)\n                k_vals = np.exp(-sq_dists / (2 * sigma * sigma))\n            else:\n                raise ValueError(\"Unsupported kernel type: use either 'linear' or 'RBF'.\")\n            f_val = np.sum(alpha * labels * k_vals) + bias\n            if labels[i] * f_val < 1:\n                alpha[i] += eta\n                bias += eta * labels[i]\n    alpha = np.around(alpha, decimals=4)\n    bias = round(bias, 4)\n    return (alpha.tolist(), bias)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    return [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    pred_probs = []\n    for x in features:\n        z = sum((x_i * w for (x_i, w) in zip(x, weights))) + bias\n        prob = 1 / (1 + math.exp(-z))\n        pred_probs.append(round(prob, 4))\n    squared_errors = [(p - label) ** 2 for (p, label) in zip(pred_probs, labels)]\n    mse = round(sum(squared_errors) / len(squared_errors), 4)\n    pred_probs_list = np.array(pred_probs).tolist()\n    return (pred_probs_list, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    n_samples = features.shape[0]\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        errors = predictions - labels\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        delta = 2 * errors / n_samples * predictions * (1 - predictions)\n        grad_weights = np.dot(features.T, delta)\n        grad_bias = np.sum(delta)\n        weights = weights - learning_rate * grad_weights\n        bias = bias - learning_rate * grad_bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    weights_list = weights.tolist()\n    return (np.array(weights_list), bias, mse_history)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (1.0 if self.data > 0 else 0.0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_mat = np.array(B).T\n    C_mat = np.array(C).T\n    C_inv = np.linalg.inv(C_mat)\n    P = C_inv @ B_mat\n    return np.round(P, 4).tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    AtA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(AtA)\n    order = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[order]\n    V = eigenvectors[:, order]\n    sigma = np.sqrt(eigenvalues)\n    U = np.zeros((2, 2))\n    tol = 1e-10\n    for i in range(2):\n        if sigma[i] > tol:\n            U[:, i] = A @ V[:, i] / sigma[i]\n        elif i == 0:\n            U[:, i] = np.array([1, 0])\n        else:\n            U[:, i] = np.array([-U[1, 0], U[0, 0]])\n    S = np.diag(sigma)\n    U = np.around(U, 4)\n    S = np.around(S, 4)\n    V = np.around(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in numpy arrays X and y while preserving correspondence,\n    then return the shuffled arrays as Python lists using tolist().\n    \n    Parameters:\n    X (np.array): Array containing the input features.\n    y (np.array): Array containing the target values.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: (shuffled_X_list, shuffled_y_list) where both are Python lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if len(X) != len(y):\n        raise ValueError('X and y must have the same number of samples.')\n    indices = np.random.permutation(len(X))\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[start_idx:end_idx].tolist()\n        if y is not None:\n            y_batch = y[start_idx:end_idx].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    mask = X[:, feature_i] >= threshold\n    subset_meeting = X[mask]\n    subset_not_meeting = X[~mask]\n    return (subset_meeting.tolist(), subset_not_meeting.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for array X up to the specified degree.\n    \n    Parameters:\n      X      : 2D numpy array of shape (n_samples, n_features)\n      degree : integer, maximum degree of polynomial features to generate.\n      \n    Returns:\n      A python list (generated by np.array(...).tolist()) containing the new features.\n      The features include the bias term (i.e. constant 1) corresponding to degree 0.\n      \n    Example:\n      If X = [[x1, x2]] and degree=2, the generated features will be:\n      [1, x1, x2, x1*x1, x1*x2, x2*x2]\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    comb_indices = []\n    for d in range(degree + 1):\n        comb_indices.extend(combinations_with_replacement(range(n_features), d))\n    output = np.empty((n_samples, len(comb_indices)))\n    for i in range(n_samples):\n        feature_list = []\n        for indices in comb_indices:\n            if len(indices) == 0:\n                product = 1\n            else:\n                product = 1\n                for idx in indices:\n                    product *= X[i, idx]\n            feature_list.append(product)\n        output[i, :] = feature_list\n    return output.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subset_tuple = (X_subset.tolist(), y_subset.tolist())\n        subsets.append(subset_tuple)\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    x = np.array(x)\n    if n_col is None:\n        n_col = int(np.max(x)) + 1\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        corr = np.corrcoef(X, rowvar=False)\n        return np.around(corr, 4).tolist()\n    else:\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError('X and Y must have the same number of rows (observations)')\n        mean_X = np.mean(X, axis=0)\n        mean_Y = np.mean(Y, axis=0)\n        X_centered = X - mean_X\n        Y_centered = Y - mean_Y\n        std_X = np.std(X_centered, axis=0, ddof=0)\n        std_Y = np.std(Y_centered, axis=0, ddof=0)\n        n = X.shape[0]\n        cov = X_centered.T @ Y_centered / n\n        denominator = np.outer(std_X, std_Y)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            corr = np.where(denominator == 0, 0, cov / denominator)\n        return np.around(corr, 4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    D = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_clf = {}\n        best_pred = None\n        for j in range(n_features):\n            feature_values = X[:, j]\n            unique_vals = np.unique(feature_values)\n            candidates = []\n            if unique_vals.size == 1:\n                candidates.append(unique_vals[0])\n            else:\n                for k in range(len(unique_vals) - 1):\n                    candidates.append((unique_vals[k] + unique_vals[k + 1]) / 2)\n                candidates.append(unique_vals[0] - 1)\n                candidates.append(unique_vals[-1] + 1)\n            for threshold in candidates:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples)\n                    predictions[polarity * (feature_values - threshold) < 0] = -1\n                    misclassified = predictions != y\n                    error = np.sum(D[misclassified])\n                    error_rounded = round(error, 4)\n                    if error_rounded < best_error:\n                        best_error = error_rounded\n                        best_clf = {'feature_index': j, 'threshold': round(threshold, 4), 'polarity': polarity}\n                        best_pred = predictions.copy()\n        if best_error == 0:\n            best_error = 1e-10\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        alpha = round(alpha, 4)\n        best_clf['alpha'] = alpha\n        classifiers.append(best_clf)\n        D = D * np.exp(-alpha * y * best_pred)\n        D = D / np.sum(D)\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_arr = np.array(scores)\n    stabilized_scores = scores_arr - np.max(scores_arr)\n    exp_scores = np.exp(stabilized_scores)\n    sum_exp = np.sum(exp_scores)\n    log_softmax_values = stabilized_scores - np.log(sum_exp)\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    return rounded_log_softmax.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass SGD:\n\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n\n    def update(self, param, grad):\n        return param - self.lr * grad\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError('input_shape must be provided to initialize the Dense layer.')\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.W_optimizer = SGD(learning_rate=0.01)\n        self.w0_optimizer = SGD(learning_rate=0.01)\n\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        grad_input = np.dot(accum_grad, self.W.T)\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0)\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, grad_W)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_w0)\n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (k_h, k_w) = kernel.shape\n    padded_matrix = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    out_height = (padded_matrix.shape[0] - k_h) // stride + 1\n    out_width = (padded_matrix.shape[1] - k_w) // stride + 1\n    output = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            vert_start = i * stride\n            vert_end = vert_start + k_h\n            horiz_start = j * stride\n            horiz_end = horiz_start + k_w\n            current_region = padded_matrix[vert_start:vert_end, horiz_start:horiz_end]\n            output[i, j] = np.sum(current_region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    total_loss = mse + reg_term\n    return round(total_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float|int: The result of the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    \n    Precision is defined as:\n        precision = TP / (TP + FP)\n    \n    where:\n        TP (true positives) are the cases where y_true == 1 and y_pred == 1.\n        FP (false positives) are the cases where y_true == 0 and y_pred == 1.\n        \n    Parameters:\n        y_true (np.array): numpy array of true binary labels (0s and 1s).\n        y_pred (np.array): numpy array of predicted binary labels (0s and 1s).\n        \n    Returns:\n        float: The precision metric. If there are no positive predictions,\n               returns 0.0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    n_samples = len(y)\n    for i in range(n_iterations):\n        if method == 'batch':\n            predictions = X.dot(weights)\n            errors = predictions - y\n            gradient = 2 * X.T.dot(errors) / n_samples\n            weights = weights - learning_rate * gradient\n        elif method == 'stochastic':\n            idx = np.random.randint(n_samples)\n            X_i = X[idx:idx + 1]\n            y_i = y[idx:idx + 1]\n            prediction = X_i.dot(weights)\n            error = prediction - y_i\n            gradient = 2 * X_i.T.dot(error)\n            weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            if batch_size > n_samples:\n                raise ValueError('batch_size should not exceed number of samples.')\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n            predictions = X_batch.dot(weights)\n            errors = predictions - y_batch\n            gradient = 2 * X_batch.T.dot(errors) / batch_size\n            weights = weights - learning_rate * gradient\n        else:\n            raise ValueError(\"Method should be 'batch', 'stochastic', or 'mini-batch'\")\n    final_predictions = X.dot(weights)\n    final_predictions = np.round(final_predictions, 4)\n    return final_predictions.tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Compute the Reduced Row Echelon Form of the given matrix.\n    The function converts the matrix (either as a list of lists or a numpy array)\n    into its reduced row echelon form (RREF) and returns the result as a Python list.\n    \n    Parameters:\n        matrix (list or numpy.ndarray): The input matrix.\n    \n    Returns:\n        list: The RREF of the matrix as a list of lists.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while np.abs(A[i, lead]) < 1e-12:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead >= cols:\n                    break\n        if lead >= cols:\n            break\n        A[[r, i]] = A[[i, r]]\n        pivot = A[r, lead]\n        if np.abs(pivot) > 1e-12:\n            A[r] = A[r] / pivot\n        for i in range(rows):\n            if i != r:\n                factor = A[i, lead]\n                A[i] = A[i] - factor * A[r]\n        lead += 1\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Adam Optimizer to minimize the function f.\n    \n    Parameters:\n        f (function): Objective function.\n        grad (function): Function to compute the gradient of f.\n        x0 (array-like): Initial parameter values.\n        learning_rate (float): Step size.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): Small constant for numerical stability.\n        num_iterations (int): Number of iterations.\n        \n    Returns:\n        list: Optimized parameters rounded to four decimals.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.around(x, decimals=4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0.0\n    for i in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        error = y_pred - y\n        grad_w = 1 / n * np.dot(X.T, error) + alpha * np.sign(w)\n        grad_b = 1 / n * np.sum(error)\n        new_w = w - learning_rate * grad_w\n        new_b = b - learning_rate * grad_b\n        if np.linalg.norm(new_w - w, ord=1) < tol and abs(new_b - b) < tol:\n            (w, b) = (new_w, new_b)\n            break\n        (w, b) = (new_w, new_b)\n    w = np.round(w, 4)\n    b = round(b, 4)\n    return (w.tolist(), b)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    D = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        D[i, 0] = i\n    for j in range(n + 1):\n        D[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            deletion = D[i - 1, j] + 1\n            insertion = D[i, j - 1] + 1\n            substitution = D[i - 1, j - 1] + cost\n            D[i, j] = min(deletion, insertion, substitution)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                D[i, j] = min(D[i, j], D[i - 2, j - 2] + 1)\n    return D[m, n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = Q.shape[-1]\n    scaling_factor = np.sqrt(d_k)\n    scores = np.dot(Q, K.T) / scaling_factor\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = np.dot(softmax_scores, V)\n    rounded_output = np.round(attention_output, 4)\n    return rounded_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Processes a sequence of input vectors through a simple RNN cell and returns the final hidden state.\n    Each hidden state update is computed using the tanh activation function:\n    \n        h_t = tanh(Wx * x_t + Wh * h_{t-1} + b)\n    \n    All results are rounded to 4 decimal places.\n    \n    Parameters:\n    - input_sequence: A list of input vectors (each input vector is a list of floats).\n    - initial_hidden_state: The initial hidden state vector (a list of floats).\n    - Wx: The weight matrix for input-to-hidden connections.\n    - Wh: The weight matrix for hidden-to-hidden connections.\n    - b: The bias vector.\n    \n    Returns:\n    - A list of floats representing the final hidden state (rounded to 4 decimal places) after processing the entire sequence.\n    \"\"\"\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    final_hidden_state = np.round(h, 4).tolist()\n    return final_hidden_state"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_np = np.array(points)\n    ones = np.ones((points_np.shape[0], 1))\n    points_homogeneous = np.hstack([points_np, ones])\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed_homogeneous = (translation_matrix @ points_homogeneous.T).T\n    transformed_points = transformed_homogeneous[:, :2]\n    return transformed_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence between two normal distributions:\n      P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2)\n\n    Parameters:\n      mu_p   : Mean of distribution P.\n      sigma_p: Standard deviation of distribution P.\n      mu_q   : Mean of distribution Q.\n      sigma_q: Standard deviation of distribution Q.\n\n    Returns:\n      The KL divergence D_KL(P || Q) as a float.\n      \n    The formula used is:\n      D_KL(P || Q) = log(sigma_q/sigma_p) + (sigma_p^2 + (mu_p - mu_q)**2) / (2*sigma_q**2) - 0.5\n    \"\"\"\n    log_term = np.log(sigma_q / sigma_p)\n    var_ratio_term = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = log_term + var_ratio_term - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    if x_ini is not None:\n        x = np.array(x_ini, dtype=float)\n    else:\n        x = np.zeros(N, dtype=float)\n    for _ in range(n):\n        for i in range(N):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if max_row != i:\n            A[[i, max_row]] = A[[max_row, i]]\n            b[[i, max_row]] = b[[max_row, i]]\n        if np.abs(A[i, i]) < 1e-12:\n            raise ValueError('Matrix is singular or nearly singular!')\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if np.abs(A[i, i]) < 1e-12:\n            raise ValueError('Matrix is singular or nearly singular during back substitution!')\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns:\n          - A list of hidden states for each time step (each hidden state is a list after rounding).\n          - The final hidden state (rounded to 4 decimals and converted to a list).\n          - The final cell state (rounded to 4 decimals and converted to a list).\n\n        Parameters:\n          x: numpy array of shape (T, input_size) where T is number of time steps.\n          initial_hidden_state: numpy array of shape (hidden_size, 1)\n          initial_cell_state: numpy array of shape (hidden_size, 1)\n        \"\"\"\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)\n            concat = np.concatenate((h_prev, x_t), axis=0)\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_bar = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c_prev + i_t * c_bar\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(np.round(h_t, 4).tolist())\n            h_prev = h_t\n            c_prev = c_t\n        final_hidden_state = np.round(h_prev, 4).tolist()\n        final_cell_state = np.round(c_prev, 4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    N = len(corpus)\n    df = {}\n    for term in query:\n        count = 0\n        for doc in corpus:\n            if term in doc:\n                count += 1\n        df[term] = count\n    idf = {}\n    for term in query:\n        idf[term] = log((N + 1) / (df[term] + 1)) + 1\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_result = []\n        total_words = len(doc)\n        for term in query:\n            tf = 0.0\n            if total_words > 0:\n                tf = doc.count(term) / total_words\n            tf_idf = tf * idf[term]\n            doc_result.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_result)\n    result_array = np.array(tf_idf_scores)\n    return result_array.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision == 0 and recall == 0:\n        fscore = 0.0\n    else:\n        fscore = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(fscore, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights (scaled by 0.01) and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        input_sequence: list of np.ndarray, each of shape (input_size, 1)\n\n        Returns:\n          outputs: list of np.ndarray, each of shape (output_size, 1) for each time step.\n          last_inputs: dictionary mapping time step to input (needed for BPTT).\n          last_hiddens: dictionary mapping time step to hidden state (needed for BPTT).\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        last_inputs = {}\n        last_hiddens = {}\n        outputs = []\n        for (t, x) in enumerate(input_sequence):\n            last_inputs[t] = x\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            last_hiddens[t] = h\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.001):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the RNN weights.\n\n        input_sequence: list of np.ndarray, each with shape (input_size, 1)\n        expected_output: list of np.ndarray, each with shape (output_size, 1). Each entry is the expected output at that time step.\n        outputs: list of np.ndarray, output produced during forward pass.\n        last_inputs: dictionary from forward pass (maps t -> x).\n        last_hiddens: dictionary from forward pass (maps t -> h at time t).\n        learning_rate: learning rate for the update.\n\n        The loss function is defined as: Loss = (1/2)*sum_t[ || expected[t] - output[t] ||^2 ].\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        T = len(input_sequence)\n        for t in reversed(range(T)):\n            dy = outputs[t] - expected_output[t]\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            h_prev = np.zeros((self.hidden_size, 1)) if t == 0 else last_hiddens[t - 1]\n            dW_hh += np.dot(dh_raw, h_prev.T)\n            db_h += dh_raw\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        loss = 0.0\n        for t in range(T):\n            loss += 0.5 * np.sum((expected_output[t] - outputs[t]) ** 2)\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n    \n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to a list)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x0.astype(float)\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        if np.linalg.norm(r) < tol:\n            break\n        rs_new = np.dot(r, r)\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.around(x, decimals=8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for row in dense_matrix:\n        for (j, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(j)\n        row_ptr.append(len(values))\n    return (values, col_indices, row_ptr)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (a list of numbers)\n    :param L: The line vector defining the direction of projection (a list of numbers)\n    :return: List representing the projection of v onto L, with each component rounded to three decimals\n    \"\"\"\n    dot_vL = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_LL = sum((Li * Li for Li in L))\n    if dot_LL == 0:\n        raise ValueError('The line vector L must be non-zero.')\n    coeff = dot_vL / dot_LL\n    proj = [round(coeff * Li, 3) for Li in L]\n    return proj"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            element = dense_matrix[row][col]\n            if element != 0:\n                values.append(element)\n                row_indices.append(row)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    pivot_row = 0\n    pivot_columns = []\n    A_copy = A.copy()\n    for col in range(n):\n        pivot_found = None\n        for r in range(pivot_row, m):\n            if abs(A_copy[r, col]) > 1e-12:\n                pivot_found = r\n                break\n        if pivot_found is None:\n            continue\n        if pivot_found != pivot_row:\n            A_copy[[pivot_row, pivot_found]] = A_copy[[pivot_found, pivot_row]]\n        pivot_columns.append(col)\n        pivot_val = A_copy[pivot_row, col]\n        A_copy[pivot_row] = A_copy[pivot_row] / pivot_val\n        for r in range(pivot_row + 1, m):\n            factor = A_copy[r, col]\n            A_copy[r] = A_copy[r] - factor * A_copy[pivot_row]\n        pivot_row += 1\n        if pivot_row == m:\n            break\n    basis = A[:, pivot_columns]\n    return np.round(basis, 8).tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_res / ss_total\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    expected_row_length = len(img[0])\n    for row in img:\n        if len(row) != expected_row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        num_pixels += len(row)\n        total_brightness += sum(row)\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual values (y_true) and predicted values (y_pred).\n    \n    Parameters:\n      y_true (array-like): Actual values.\n      y_pred (array-like): Predicted values.\n      \n    Returns:\n      float: The RMSE value rounded to three decimal places.\n      \n    Raises:\n      ValueError: If the input arrays have mismatched shapes, are empty, or cannot be converted to numpy arrays.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n    except Exception as e:\n        raise ValueError('Input values must be array-like and convertible to a numpy array.') from e\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.logical_and(y_true == 1, y_pred == 1).sum()\n    union = np.logical_or(y_true == 1, y_pred == 1).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true binary labels.\n    y_pred (np.ndarray): Array of predicted binary labels.\n    \n    Returns:\n    float: Dice score rounded to 3 decimal places.\n    \n    Note:\n    - If both y_true and y_pred have no positives (i.e., sums are zero), the function returns 1.0\n      since there is a perfect match (nothing predicted, nothing true).\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    sum_true = np.sum(y_true == 1)\n    sum_pred = np.sum(y_pred == 1)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a dataset row using Hyperdimensional Computing.\n    \n    For each feature in the row:\n      1. Generate a bipolar hypervector for the feature name using the provided random seed.\n      2. Generate a bipolar hypervector for the feature value using a modified seed based on the feature seed \n         and the value (converted to string and hashed), so that the same value always leads to the same hypervector.\n      3. Bind them using element-wise multiplication.\n    \n    Then, bundle (i.e., sum) all bound hypervectors across features.\n    \n    Optionally, you may binarize the bundled vector by taking the sign (i.e. majority vote) so that each dimension is +1 or -1.\n    \n    Args:\n      row (dict): Dictionary representing a row with feature key and value.\n      dim (int): Dimensionality of the hypervectors.\n      random_seeds (dict): Dictionary with feature names as keys and integer seeds as values.\n    \n    Returns: \n      list: The composite hypervector (as a list) representing the row.\n    \"\"\"\n    bound_vectors = []\n    for (feature, value) in row.items():\n        base_seed = random_seeds[feature]\n        rs_name = np.random.RandomState(base_seed)\n        name_hv = rs_name.choice([-1, 1], size=dim)\n        value_str = str(value)\n        offset = abs(hash(value_str)) % 10000\n        value_seed = base_seed + offset\n        rs_value = np.random.RandomState(value_seed)\n        value_hv = rs_value.choice([-1, 1], size=dim)\n        bound = name_hv * value_hv\n        bound_vectors.append(bound)\n    composite = np.sum(bound_vectors, axis=0)\n    composite_bipolar = np.where(composite >= 0, 1, -1)\n    return composite_bipolar.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    counts = Counter()\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            counts['TN'] += 1\n        elif y_true == 0 and y_pred == 1:\n            counts['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            counts['FN'] += 1\n        elif y_true == 1 and y_pred == 1:\n            counts['TP'] += 1\n    return [[counts.get('TN', 0), counts.get('FP', 0)], [counts.get('FN', 0), counts.get('TP', 0)]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    similarity = np.dot(v1, v2) / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    TP = FP = TN = FN = 0\n    for (act, pred) in zip(actual, predicted):\n        if act not in (0, 1) or pred not in (0, 1):\n            raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n        if act == 1 and pred == 1:\n            TP += 1\n        elif act == 0 and pred == 1:\n            FP += 1\n        elif act == 1 and pred == 0:\n            FN += 1\n        elif act == 0 and pred == 0:\n            TN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total != 0 else 0\n    if 2 * TP + FP + FN == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * TP / (2 * TP + FP + FN)\n    if TN + FP == 0:\n        specificity = 0\n    else:\n        specificity = TN / (TN + FP)\n    if TN + FN == 0:\n        negative_predictive_value = 0\n    else:\n        negative_predictive_value = TN / (TN + FN)\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    (unique_vals, counts) = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    modes = unique_vals[counts == max_count]\n    mode_val = np.min(modes)\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(float(mode_val), 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials (rounded to five decimal places)\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    success_prob = p ** k\n    failure_prob = (1 - p) ** (n - k)\n    probability = binom_coeff * success_prob * failure_prob\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image, computed as the difference between the \n               maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('The input image must be a numpy.ndarray.')\n    if img.ndim != 2:\n        raise ValueError('The input image must be a 2D array representing a grayscale image.')\n    contrast = np.max(img) - np.min(img)\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vec1 and vec2 must be 1D arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features \n                           of the corresponding data point, each rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed = []\n    for x in data:\n        features = []\n        for power in range(degree + 1):\n            features.append(round(x ** power, 8))\n        transformed.append(features)\n    return transformed"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    positions = np.arange(position).reshape(-1, 1)\n    dims = np.arange(d_model).reshape(1, -1)\n    angle_rates = 1 / np.power(10000, 2 * (dims // 2) / np.float32(d_model))\n    angle_rads = positions * angle_rates\n    pos_encoding = np.empty_like(angle_rads)\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    return pos_encoding.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, 1 if overfitting, -1 if underfitting, or 0 if a good fit.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value (scalar or numpy array)\n    :param grad: Current gradient (scalar or numpy array)\n    :param m: First moment estimate (scalar or numpy array)\n    :param v: Second moment estimate (scalar or numpy array)\n    :param t: Current timestep (int)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v),\n             where each element is rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.around(updated_parameter, decimals=5).tolist()\n    m = np.around(m, decimals=5).tolist()\n    v = np.around(v, decimals=5).tolist()\n    return (updated_parameter, m, v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    tokens = encoder.encode(prompt)\n    for i in range(n_tokens_to_generate):\n        if len(tokens) >= hparams['n_ctx']:\n            break\n        context_embeddings = []\n        for (pos, token_id) in enumerate(tokens):\n            token_emb = params['wte'][token_id]\n            pos_emb = params['wpe'][pos]\n            context_embeddings.append(token_emb + pos_emb)\n        context = np.stack(context_embeddings, axis=0)\n        attention_output = context.mean(axis=0)\n        ff_output = np.tanh(attention_output)\n        epsilon = 1e-05\n        mean_val = ff_output.mean()\n        std_val = ff_output.std() + epsilon\n        norm_output = (ff_output - mean_val) / std_val\n        norm_output = norm_output * params['ln_f']['g'] + params['ln_f']['b']\n        logits = norm_output.dot(params['wte'].T)\n        exp_logits = np.exp(logits - np.max(logits))\n        probs = exp_logits / exp_logits.sum()\n        next_token = np.random.choice(len(probs), p=probs)\n        tokens.append(int(next_token))\n    generated_text = encoder.decode(tokens)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        shifted = values - np.max(values)\n        exps = np.exp(shifted)\n        return exps / np.sum(exps)\n    crystals = np.array(crystal_values, dtype=float)\n    scores = np.outer(crystals, crystals) / np.sqrt(dimension)\n    result = []\n    for i in range(n):\n        attention_weights = softmax(scores[i])\n        weighted_pattern = np.dot(attention_weights, crystals)\n        result.append(round(weighted_pattern, 4))\n    return result"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus given the query.\n    \n    Parameters:\n      corpus: list of documents (each document is a string).\n      query: string with query words.\n      k1: tuning parameter for term frequency scaling.\n      b: tuning parameter for document length normalization.\n      \n    Returns:\n      A list of BM25 scores (rounded to three decimals) for each document in the corpus.\n    \"\"\"\n    tokenized_corpus = [doc.split() for doc in corpus]\n    N = len(tokenized_corpus)\n    doc_lengths = [len(doc) for doc in tokenized_corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    query_terms = query.split()\n    doc_freq = {}\n    for term in query_terms:\n        count = sum((1 for doc in tokenized_corpus if term in set(doc)))\n        doc_freq[term] = count\n\n    def bm25_term_score(term, freq, doc_len, df):\n        idf = np.log((N - df + 0.5) / (df + 0.5) + 1)\n        score = idf * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * (doc_len / avg_doc_length)))\n        return score\n    scores = []\n    for doc in tokenized_corpus:\n        score = 0.0\n        doc_len = len(doc)\n        term_freqs = Counter(doc)\n        for term in query_terms:\n            freq = term_freqs.get(term, 0)\n            if freq > 0:\n                score += bm25_term_score(term, freq, doc_len, doc_freq[term])\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    predicted_positives = sum((1 for pred in y_pred if pred == 1))\n    actual_positives = sum((1 for true in y_true if true == 1))\n    if predicted_positives == 0:\n        precision = 0.0\n    else:\n        precision = tp / predicted_positives\n    if actual_positives == 0:\n        recall = 0.0\n    else:\n        recall = tp / actual_positives\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    n = len(consumption_data)\n    detrended = []\n    for i in range(1, n + 1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        base_usage = consumption_data[i - 1] - fluctuation\n        detrended.append(base_usage)\n    sum_x = 0\n    sum_y = 0\n    sum_xy = 0\n    sum_x2 = 0\n    for i in range(1, n + 1):\n        x = i\n        y = detrended[i - 1]\n        sum_x += x\n        sum_y += y\n        sum_xy += x * y\n        sum_x2 += x * x\n    a = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - a * sum_x) / n\n    day_predict = 15\n    predicted_base = a * day_predict + b\n    fluctuation_day15 = 10 * math.sin(2 * PI * day_predict / 10)\n    predicted_total = predicted_base + fluctuation_day15\n    safe_prediction = math.ceil(predicted_total * 1.05)\n    return safe_prediction"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    abs_errors = np.abs(y_true - y_pred)\n    mean_abs_error = np.mean(abs_errors)\n    return round(mean_abs_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute and return the Query (Q), Key (K) and Value (V) matrices.\n    \n    Arguments:\n      X     : (seq_len, d_model) input matrix.\n      W_q   : (d_model, d_model) query projection matrix.\n      W_k   : (d_model, d_model) key projection matrix.\n      W_v   : (d_model, d_model) value projection matrix.\n    \n    Returns:\n      A tuple (Q, K, V) each of shape (seq_len, d_model).  \n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    Q = np.around(Q, decimals=4)\n    K = np.around(K, decimals=4)\n    V = np.around(V, decimals=4)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray):\n    \"\"\"\n    Compute self-attention using scaled dot-product attention.\n    \n    Arguments:\n      Q: (seq_len, head_dim) Query matrix.\n      K: (seq_len, head_dim) Key matrix.\n      V: (seq_len, head_dim) Value matrix.\n    \n    Returns:\n      Attention output: (seq_len, head_dim) matrix.\n    \"\"\"\n    d = Q.shape[-1]\n    logits = Q @ K.T / np.sqrt(d)\n    logits = logits - np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits)\n    softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    attention = softmax @ V\n    attention = np.around(attention, decimals=4)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention for the input X.\n    \n    Arguments:\n      X      : (seq_len, d_model) input matrix.\n      W_q    : (d_model, d_model) query projection matrix.\n      W_k    : (d_model, d_model) key projection matrix.\n      W_v    : (d_model, d_model) value projection matrix.\n      n_heads: number of attention heads.\n    \n    Returns:\n      Multi-head attention output as a python list (after rounding to 4 decimals).\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    (seq_len, d_model) = Q.shape\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads')\n    head_dim = d_model // n_heads\n\n    def split_heads(X_mat):\n        X_reshaped = X_mat.reshape(seq_len, n_heads, head_dim)\n        return X_reshaped.transpose(1, 0, 2)\n    Q_split = split_heads(Q)\n    K_split = split_heads(K)\n    V_split = split_heads(V)\n    heads_output = []\n    for i in range(n_heads):\n        q_head = Q_split[i]\n        k_head = K_split[i]\n        v_head = V_split[i]\n        head_out = self_attention(q_head, k_head, v_head)\n        heads_output.append(head_out)\n    concat = np.concatenate(heads_output, axis=-1)\n    concat = np.around(concat, decimals=4)\n    return concat.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n11 = n10 = n01 = n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n        else:\n            raise ValueError('Lists must contain only binary values (0 or 1).')\n    numerator = n11 * n00 - n10 * n01\n    denom_part1 = n11 + n10\n    denom_part2 = n01 + n00\n    denom_part3 = n11 + n01\n    denom_part4 = n10 + n00\n    denominator = (denom_part1 * denom_part2 * denom_part3 * denom_part4) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    if x > 0:\n        result = x + math.log1p(math.exp(-x))\n    else:\n        result = math.log1p(math.exp(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n        \n    Returns:\n        The computed GRPO objective value (rounded to the nearest 6th decimal).\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_theta_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.array(pi_theta_ref, dtype=np.float64)\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate = np.minimum(rhos * A, rhos_clipped * A)\n    surrogate_mean = np.mean(surrogate)\n    kl_div = np.sum(pi_theta_old * np.log(pi_theta_old / (pi_theta_ref + 1e-08)))\n    objective = surrogate_mean - beta * kl_div\n    return round(objective, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid\n    return round(swish_value, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n    \n    Args:\n        x: Input value\n        \n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        result = scale * x\n    else:\n        result = scale * (alpha * math.expm1(x))\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a python list.\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, n_features) = X.shape\n    num_classes = int(np.max(y)) + 1\n    B = np.zeros((n_features, num_classes))\n    losses = []\n    for i in range(iterations):\n        scores = np.dot(X, B)\n        scores -= np.max(scores, axis=1, keepdims=True)\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        correct_logprobs = -np.log(probs[np.arange(N), y])\n        loss = np.mean(correct_logprobs)\n        losses.append(round(loss, 4))\n        dscores = probs.copy()\n        dscores[np.arange(N), y] -= 1\n        dscores /= N\n        dB = np.dot(X.T, dscores)\n        B = B - learning_rate * dB\n    B = np.round(B, 4)\n    return (B.tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression,\n    optimizing parameters with Binary Cross Entropy loss.\n    Returns a tuple containing the list of loss values over iterations\n    and the final coefficient values (rounded to the 4th decimal).\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros((n, 1))\n    y = y.reshape(-1, 1)\n    loss_history = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    epsilon = 1e-15\n    for i in range(iterations):\n        z = np.dot(X, theta)\n        y_hat = sigmoid(z)\n        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n        loss = -1 / m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n        loss_history.append(round(loss, 4))\n        gradient = 1 / m * np.dot(X.T, y_hat - y)\n        theta = theta - learning_rate * gradient\n    final_coefficients = np.round(theta, 4).flatten().tolist()\n    return (loss_history, final_coefficients)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Steps:\n    1. Compute the scaled dot-product between Q and K\u1d40.\n    2. Apply the mask: the positions where mask == 0 get a large negative value.\n    3. Apply softmax to get the attention weights.\n    4. Multiply the attention weights with V.\n    5. Return the output as a Python list.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    masked_scores = np.where(mask == 0, -1000000000.0, scores)\n    max_per_row = np.max(masked_scores, axis=1, keepdims=True)\n    exp_scores = np.exp(masked_scores - max_per_row)\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    output = np.dot(softmax_scores, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy (base 2) as the metric.\n    The more balanced and varied the colors, the higher the disorder.\n    \"\"\"\n    if not apples:\n        return 0.0\n    freq = {}\n    for color in apples:\n        freq[color] = freq.get(color, 0) + 1\n    total = len(apples)\n    entropy = 0.0\n    import math\n    for count in freq.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    out = X_normalized * gamma + beta\n    out_rounded = np.round(out, 5)\n    return out_rounded.tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    ref_positions = defaultdict(list)\n    for (idx, token) in enumerate(ref_tokens):\n        ref_positions[token].append(idx)\n    alignment = []\n    last_assigned = -1\n    ref_available = {token: list(indices) for (token, indices) in ref_positions.items()}\n    for tok in cand_tokens:\n        if tok in ref_available and ref_available[tok]:\n            chosen_index = None\n            for pos in ref_available[tok]:\n                if pos > last_assigned:\n                    chosen_index = pos\n                    break\n            if chosen_index is not None:\n                alignment.append(chosen_index)\n                last_assigned = chosen_index\n                ref_available[tok].remove(chosen_index)\n    m = len(alignment)\n    if m == 0:\n        return 0.0\n    chunks = 1\n    for i in range(1, len(alignment)):\n        if alignment[i] != alignment[i - 1] + 1:\n            chunks += 1\n    precision = m / len(cand_tokens)\n    recall = m / len(ref_tokens)\n    denominator = (1 - alpha) * recall + alpha * precision\n    if denominator == 0:\n        f_mean = 0\n    else:\n        f_mean = precision * recall / denominator\n    penalty = gamma * (chunks / m) ** beta\n    score = f_mean * (1 - penalty)\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n    \n    PMI = log [P(x,y) / (P(x)*P(y))]\n        = log [ (joint_counts / total_samples) / ((total_counts_x/total_samples) * (total_counts_y/total_samples)) ]\n        = log [ (joint_counts * total_samples) / (total_counts_x * total_counts_y) ]\n    \n    Args:\n        joint_counts (int or float): The count of simultaneous occurrences of events x and y.\n        total_counts_x (int or float): The count of event x occurrences.\n        total_counts_y (int or float): The count of event y occurrences.\n        total_samples (int or float): The total number of samples.\n    \n    Returns:\n        float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    pmi = np.log(joint_counts * total_samples / (total_counts_x * total_counts_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    range_val = max_val - min_val\n    if range_val == 0:\n        return [0.0 for _ in x]\n    normalized = [round((value - min_val) / range_val, 4) for value in x]\n    return normalized"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n\n    def relu(z):\n        return np.maximum(0, z)\n    z1 = np.dot(w1, x)\n    a1 = relu(z1)\n    z2 = np.dot(w2, a1)\n    output = relu(z2 + x)\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform global average pooling on a 3D numpy array representing feature maps.\n    \n    Parameters:\n    x (np.ndarray): Input array with shape (height, width, channels)\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    gamma = gamma.reshape(1, -1, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)\n    X_out = gamma * X_norm + beta\n    return np.round(X_out, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term c * x^n using the power rule.\n    \n    Parameters:\n      c (float): the coefficient of the term.\n      x (float): the point at which to evaluate the derivative.\n      n (float): the exponent in the term.\n      \n    Returns:\n      float: the derivative evaluated at x, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by given 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: List of 2D vectors (each vector itself is a list of floats).\n    - tol: Tolerance used to decide if a vector is linearly independent.\n    \n    Returns:\n    - A list of orthonormal vectors (each as a list of 4-decimal rounded floats) that span the same subspace.\n    \"\"\"\n    ortho_basis = []\n    for v in vectors:\n        v_vec = np.array(v, dtype=float)\n        for b in ortho_basis:\n            proj = np.dot(v_vec, b) * b\n            v_vec = v_vec - proj\n        norm_v = np.linalg.norm(v_vec)\n        if norm_v > tol:\n            v_vec = v_vec / norm_v\n            ortho_basis.append(v_vec)\n    ortho_basis_rounded = [np.round(vec, 4).tolist() for vec in ortho_basis]\n    return ortho_basis_rounded"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    result = np.around(result, decimals=4)\n    return result.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    n = A.shape[0]\n    solution = np.zeros(n)\n    for i in range(n):\n        A_copy = A.copy()\n        A_copy[:, i] = b\n        det_Ai = np.linalg.det(A_copy)\n        solution[i] = det_Ai / det_A\n    solution = np.round(solution, 4)\n    return solution.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_arr = np.array(p)\n    q_arr = np.array(q)\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n    if bc <= 0:\n        return float('inf')\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    grad_total = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        T = len(episode)\n        returns = np.zeros(T)\n        G = 0.0\n        for t in reversed(range(T)):\n            (_, _, reward) = episode[t]\n            G += reward\n            returns[t] = G\n        for (t, (state, action, _)) in enumerate(episode):\n            exps = np.exp(theta[state, :])\n            probs = exps / np.sum(exps)\n            grad_log = np.zeros(num_actions, dtype=np.float64)\n            grad_log[action] = 1.0\n            grad_log -= probs\n            grad_total[state, :] += grad_log * returns[t]\n    grad_avg = grad_total / len(episodes)\n    grad_avg = np.around(grad_avg, decimals=4)\n    return grad_avg.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100 if flops_dense > 0 else 0\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return (flops_dense, flops_moe, savings_percentage)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g)\n    noise_std = np.dot(X, W_noise)\n    noisy_logits = logits + noise_std * N\n    (batch_size, num_experts) = noisy_logits.shape\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    for i in range(batch_size):\n        topk_indices = np.argsort(noisy_logits[i])[-k:]\n        mask[i, topk_indices] = True\n    masked_logits = np.where(mask, noisy_logits, -np.inf)\n    row_max = np.max(masked_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(masked_logits - row_max)\n    sum_exps = np.sum(exp_logits, axis=1, keepdims=True)\n    probs = exp_logits / sum_exps\n    probs_rounded = np.round(probs, 4)\n    return probs_rounded.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture of Experts (MoE) layer.\n    \n    Parameters:\n      x        : Input tensor of shape (batch_size, input_dim)\n      We       : Expert weight matrices with shape (n_experts, input_dim, output_dim)\n      Wg       : Gating weight matrix with shape (input_dim, n_experts)\n      n_experts: Number of experts (should match the first dimension of We)\n      top_k    : Number of experts to select per token\n      \n    Returns:\n      A python list (via numpy's tolist()) representing the MoE output, where the result is rounded \n      to the nearest 4th decimal.\n    \"\"\"\n    logits = np.dot(x, Wg)\n    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    gate_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    expert_outputs = []\n    for i in range(n_experts):\n        expert_output = np.dot(x, We[i])\n        expert_outputs.append(expert_output)\n    expert_outputs = np.stack(expert_outputs, axis=1)\n    final_output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        topk_indices = np.argsort(gate_probs[i])[::-1][:top_k]\n        topk_gate_probs = gate_probs[i][topk_indices]\n        norm_factor = np.sum(topk_gate_probs)\n        if norm_factor == 0:\n            normalized_probs = np.zeros_like(topk_gate_probs)\n        else:\n            normalized_probs = topk_gate_probs / norm_factor\n        outputs = expert_outputs[i, topk_indices, :]\n        combined = np.dot(normalized_probs, outputs)\n        final_output[i] = combined\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'The number of channels must be divisible by num_groups'\n    channels_per_group = C // num_groups\n    X_group = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = X_group.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_group.var(axis=(2, 3, 4), keepdims=True)\n    X_group_normalized = (X_group - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_group_normalized.reshape(B, C, H, W)\n    out = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n    rounded_out = np.around(out, decimals=4)\n    return rounded_out.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def grad(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = grad(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            x = new_x\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    y = gamma * np.tanh(alpha * x + beta)\n    y_rounded = np.around(y, decimals=4)\n    return y_rounded.flatten().tolist()"}
