{"task_id": 1, "completion_id": 0, "completion": "def matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # First, check if the matrix is empty or b is empty\n    if not a or not b:\n        return -1\n    \n    # Determine the expected number of columns from the first row\n    expected_cols = len(a[0])\n    \n    # Verify that all rows in the matrix have the same number of columns and this matches the length of b\n    for row in a:\n        if len(row) != expected_cols:\n            return -1  # Inconsistent row lengths in matrix\n    if expected_cols != len(b):\n        return -1  # Incompatible dimensions\n    \n    # Compute the dot product for each row\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    vector = [1, 0, -1]\n    output = matrix_dot_vector(matrix, vector)\n    print(output)  # Should print [-2, -2, -2]"}
{"task_id": 2, "completion_id": 0, "completion": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    # Using Python's built-in zip function with unpacking (*) to group elements by column.\n    return [list(row) for row in zip(*a)]\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    print(\"Original matrix:\")\n    for row in matrix:\n        print(row)\n    \n    transposed = transpose_matrix(matrix)\n    print(\"\\nTransposed matrix:\")\n    for row in transposed:\n        print(row)"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input list to a numpy array\n    np_a = np.array(a)\n    \n    # Check if reshape is possible by verifying total number of elements\n    if np_a.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the array to the new shape\n    reshaped = np_a.reshape(new_shape)\n    \n    # Return the reshaped array as a Python list\n    return reshaped.tolist()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "Below is the complete solution:\n\n------------------------------------------------\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    # Check if matrix is empty\n    if not matrix or not matrix[0]:\n        return []\n    \n    if mode == 'row':\n        # Calculate mean for each row.\n        return [sum(row) / len(row) if row else 0.0 for row in matrix]\n    \n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        means = []\n        for col in range(num_cols):\n            col_sum = 0.0\n            for row in matrix:\n                col_sum += row[col]\n            means.append(col_sum / num_rows)\n        return means\n    \n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_matrix = [\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ]\n    \n    print(\"Row means:\", calculate_matrix_mean(sample_matrix, 'row'))      # Output: [2.0, 5.0, 8.0]\n    print(\"Column means:\", calculate_matrix_mean(sample_matrix, 'column'))  # Output: [4.0, 5.0, 6.0]\n\n------------------------------------------------\n\nExplanation:\n1. The function first checks if the matrix is empty.\n2. For the 'row' mode, it iterates over each row to compute the mean.\n3. For the 'column' mode, it iterates over the column indices and then sums the corresponding elements from each row.\n4. Finally, if an invalid mode is provided, it raises a ValueError."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    # Multiply each element of each row in the matrix by the scalar\n    return [[element * scalar for element in row] for row in matrix]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 3\n    result = scalar_multiply(sample_matrix, scalar)\n    print(\"Original matrix:\")\n    for row in sample_matrix:\n        print(row)\n    print(\"\\nMatrix after scalar multiplication by\", scalar, \":\")\n    for row in result:\n        print(row)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    # Ensure the matrix is 2x2:\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2x2 matrix.\")\n    \n    # Unpack matrix elements\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate trace and determinant\n    trace = a + d\n    determinant = a * d - b * c\n    \n    # Compute the discriminant: (a+d)^2 - 4*(a*d - b*c)\n    discriminant = trace**2 - 4 * determinant\n    \n    # In this solution, we assume eigenvalues are real.\n    # If the discriminant is negative, math.sqrt will raise a ValueError.\n    # For completeness, you might use cmath.sqrt to handle complex eigenvalues.\n    import math\n    if discriminant < 0:\n        raise ValueError(\"The given matrix has complex eigenvalues.\")\n    \n    sqrt_disc = math.sqrt(discriminant)\n    \n    # Calculate the two eigenvalues\n    eigen1 = (trace + sqrt_disc) / 2\n    eigen2 = (trace - sqrt_disc) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigen1, eigen2], reverse=True)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test with a sample 2x2 matrix\n    matrix = [[4, 2],\n              [1, 3]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(\"Eigenvalues (highest to lowest):\", eigenvalues)"}
{"task_id": 7, "completion_id": 0, "completion": "import numpy as np\n\ndef transform_matrix(A: list[list[int|float]], \n                     T: list[list[int|float]], \n                     S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms the matrix A using the operation T^{-1} * A * S.\n    \n    Parameters:\n      - A: Matrix to be transformed. Its dimensions should be (m x n).\n      - T: Invertible matrix of size (m x m).\n      - S: Invertible matrix of size (n x n).\n      \n    Returns:\n      - The transformed matrix with each element rounded to 4 decimal places as a Python list (via numpy's tolist()).\n      - If T or S is not invertible (i.e., their determinants are zero or close to zero), returns -1.\n    \"\"\"\n    # Convert input lists to numpy arrays of type float for precision in computation.\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    \n    # Validate dimensions: T must be square with shape matching number of rows of A.\n    m, n = A_np.shape\n    if T_np.shape != (m, m):\n        # T's dimensions must match the number of rows of A.\n        return -1\n    if S_np.shape != (n, n):\n        # S's dimensions must match the number of columns of A.\n        return -1\n\n    # Check invertibility of T and S using determinant.\n    try:\n        det_T = np.linalg.det(T_np)\n        det_S = np.linalg.det(S_np)\n    except np.linalg.LinAlgError:\n        return -1\n\n    # A tolerance to account for floating point errors.\n    tol = 1e-10\n    if abs(det_T) < tol or abs(det_S) < tol:\n        return -1\n\n    # Compute the inverses of T (and possibly S if needed later).\n    T_inv = np.linalg.inv(T_np)\n    \n    # Perform the transformation: T^{-1} * A * S.\n    transformed = T_inv @ A_np @ S_np  # Using the @ operator for matrix multiplication.\n    \n    # Round to the nearest 4th decimal.\n    transformed_rounded = np.around(transformed, decimals=4)\n    \n    # Convert the result to python list of lists.\n    return transformed_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    # Assuming matrix is of size 2x2: [[a, b], [c, d]]\n    if not (len(matrix) == 2 and all(len(row) == 2 for row in matrix)):\n        raise ValueError(\"Input must be a 2x2 matrix.\")\n    \n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    determinant = a * d - b * c\n\n    if determinant == 0:\n        return None\n\n    # Compute inverse using formula: (1/det) * [[d, -b], [-c, a]]\n    inv_det = 1 / determinant\n    inverse_matrix = [\n        [ d * inv_det, -b * inv_det],\n        [-c * inv_det,  a * inv_det]\n    ]\n    \n    return inverse_matrix\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int|float]],\n              b: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if matrices can be multiplied: columns in a must equal rows in b.\n    if not a or not b or not a[0] or not b[0]:\n        return -1  # one of the matrices is empty or not well-formed\n\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    \n    # Validate that all rows have consistent lengths for matrix a\n    for row in a:\n        if len(row) != num_cols_a:\n            return -1  # irregular matrix a\n    \n    # Validate b matrix row lengths and determine number of columns in b\n    num_cols_b = len(b[0])\n    for row in b:\n        if len(row) != num_cols_b:\n            return -1  # irregular matrix b\n\n    if num_cols_a != num_rows_b:\n        return -1\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(num_cols_b)] for _ in range(len(a))]\n\n    # Perform matrix multiplication\n    for i in range(len(a)):  # iterate over rows of a\n        for j in range(num_cols_b):  # iterate over columns of b\n            for k in range(num_cols_a):  # or range(num_rows_b)\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "Below is one complete solution. In this example the function expects a list of lists where each inner list contains observations for one feature. The covariance between two features (lists) is computed using the sample formula:\n\n\u2003\u2003cov(i, j) = \u03a3 (x\u1d62 - mean(x\u1d62)) * (x\u2c7c - mean(x\u2c7c)) / (n - 1)\n\nwhere n is the number of observations. Test cases are provided in the main block.\n\n------------------------------------------------------------\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    # Check if there is at least one feature\n    if not vectors:\n        raise ValueError(\"The input list 'vectors' must contain at least one feature list.\")\n    \n    num_features = len(vectors)\n    n = len(vectors[0])\n    \n    # All feature lists should have the same number of observations.\n    for feature in vectors:\n        if len(feature) != n:\n            raise ValueError(\"All feature lists must have the same number of observations.\")\n    \n    # If there is only one observation, covariance is not defined (division by zero).\n    if n < 2:\n        raise ValueError(\"At least two observations are needed to compute covariance.\")\n\n    # Compute means for each feature.\n    means = []\n    for feature in vectors:\n        means.append(sum(feature) / n)\n\n    # Initialize covariance matrix with zeros.\n    cov_matrix = [[0.0 for _ in range(num_features)] for _ in range(num_features)]\n    \n    # Calculate covariance for each pair of features.\n    for i in range(num_features):\n        for j in range(i, num_features):\n            cov_sum = 0.0\n            for k in range(n):\n                cov_sum += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_value = cov_sum / (n - 1)\n            cov_matrix[i][j] = cov_value\n            # Since covariance matrix is symmetric.\n            cov_matrix[j][i] = cov_value\n\n    return cov_matrix\n\n# --- Test Cases ---\nif __name__ == '__main__':\n    # Test Case 1: Two features with three observations each.\n    # Features:\n    # Feature 1: [1, 2, 3]\n    # Feature 2: [4, 5, 6]\n    # Expected: mean1=2, mean2=5, covariance = [(x-2)*(y-5) sum] / 2 = 1 for both variance and covariance.\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    cov_matrix1 = calculate_covariance_matrix(vectors1)\n    print(\"Test Case 1 Covariance Matrix:\")\n    for row in cov_matrix1:\n        print(row)\n    # Expected output:\n    # [1.0, 1.0]\n    # [1.0, 1.0]\n\n    # Test Case 2: Two features with four observations.\n    # Using data:\n    # Feature 1: [2.1, 2.5, 4.0, 3.6]\n    # Feature 2: [8, 12, 14, 10]\n    # Manually computed (approximately):\n    # Variance feature1: 0.80333, covariance: \u2248 1.53333, variance feature2: \u2248 6.66667\n    vectors2 = [\n        [2.1, 2.5, 4.0, 3.6],\n        [8, 12, 14, 10]\n    ]\n    cov_matrix2 = calculate_covariance_matrix(vectors2)\n    print(\"\\nTest Case 2 Covariance Matrix:\")\n    for row in cov_matrix2:\n        print(row)\n\n    # Test Case 3: Three features with five observations.\n    vectors3 = [\n        [1, 2, 3, 4, 5],\n        [5, 4, 3, 2, 1],\n        [2, 3, 2, 3, 2]\n    ]\n    cov_matrix3 = calculate_covariance_matrix(vectors3)\n    print(\"\\nTest Case 3 Covariance Matrix:\")\n    for row in cov_matrix3:\n        print(row)\n\n    # Test with a single feature (its variance should be returned as a 1x1 matrix)\n    vectors4 = [\n        [10, 12, 14, 16, 18]\n    ]\n    cov_matrix4 = calculate_covariance_matrix(vectors4)\n    print(\"\\nTest Case 4 Covariance Matrix (Single Feature):\")\n    for row in cov_matrix4:\n        print(row)\n\n    # Note: You can add more tests or use assert statements as needed.\n    \n------------------------------------------------------------\n\nExplanation:\n1. The function verifies that the input is non-empty and that all feature lists have the same number of observations.\n2. It then computes the mean for each feature.\n3. It calculates the covariance for every pair of features (taking advantage of the symmetry: covariance[i][j] == covariance[j][i]).\n4. A series of test cases are provided:\n   \u2022 Test Case 1 and 2 check our computed values against known results.\n   \u2022 Test Case 3 shows a scenario with three features.\n   \u2022 Test Case 4 demonstrates the covariance matrix for a single feature.\n   \nThis solution should work correctly for calculating a covariance matrix using only the standard Python library."}
{"task_id": 11, "completion_id": 0, "completion": "Below is the Python function that implements the Jacobi method for solving Ax = b. In each iteration, the intermediate solution is rounded to 4 decimal places before being used in the subsequent iteration. Finally, the resulting NumPy array is reshaped to a Python list using the tolist() method.\n\n--------------------------------------------------\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Number of variables\n    N = A.shape[0]\n    \n    # Initial guess: zeros\n    x = np.zeros(N)\n    \n    # Jacobi iterations\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(N):\n            # Sum all a_ij * x[j] for j != i\n            s = 0.0\n            for j in range(N):\n                if i != j:\n                    s += A[i, j] * x[j]\n            # Update x[i] computed from (b[i] - sum)/A[i, i]\n            new_x[i] = (b[i] - s) / A[i, i]\n        # Round each element of the new solution to 4 decimal places\n        x = np.round(new_x, 4)\n        \n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the coefficient matrix A and right-hand side vector b\n    A = np.array([[10.0, -1.0, 2.0, 0.0],\n                  [-1.0, 11.0, -1.0, 3.0],\n                  [2.0, -1.0, 10.0, -1.0],\n                  [0.0, 3.0, -1.0, 8.0]])\n    b = np.array([6.0, 25.0, -11.0, 15.0])\n    \n    # Number of iterations\n    iterations = 25\n    \n    solution = solve_jacobi(A, b, iterations)\n    print(\"Approximate solution:\", solution)\n--------------------------------------------------\n\nExplanation:\n\n1. We start with an initial guess of a zero vector.\n2. For each iteration, we compute a new solution vector using the Jacobi update rule:\n   \u2022 For each variable i, we subtract the sum over j \u2260 i (i.e. A[i,j]*x[j]) from b[i] and then divide by A[i,i].\n3. The solution vector is rounded to 4 decimal places using np.round.\n4. Finally, after completing n iterations, the NumPy array is converted to a list using tolist() and returned.\n\nYou can run the provided example to see how the function converges to an approximate solution."}
{"task_id": 12, "completion_id": 0, "completion": "Below is one way to solve the problem. For a 2\u00d72 matrix A its singular values are the square roots of the eigen\u2010values of A\u1d40A. In our \u201cJacobian method\u201d we note that A\u1d40A is symmetric and for a 2\u00d72 symmetric matrix only one Jacobi rotation is needed to zero out the off\u2011diagonal element. The following code computes the rotation angle (or equivalently the tangent of the angle) so that the rotation diagonalizes A\u1d40A; then the square roots of the diagonal elements (after sorting in descending order) yield the singular values. All results are rounded to 4 decimals.\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Compute the symmetric matrix A\u1d40A.\n    B = A.T @ A\n    a = B[0, 0]\n    b = B[0, 1]  # B[1,0] is the same since B is symmetric.\n    d = B[1, 1]\n    \n    # Set a tolerance level for \u201czero\u201d\n    tol = 1e-10\n    \n    # If b is (near) zero, B is already diagonal.\n    if abs(b) < tol:\n        # Diagonals are the eigenvalues.\n        eig1 = a\n        eig2 = d\n    else:\n        # Compute the Jacobi rotation parameter.\n        # Here, tau = (d - a) / (2b)\n        tau = (d - a) / (2 * b)\n        # We then compute t such that tan(2\u03b8) is eliminated.\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1 + tau**2))\n        # Alternatively, one could compute theta = 0.5 * np.arctan2(2 * b, d - a)\n        # and then set t = np.tan(theta), but the formula above\n        # avoids potential numerical issues.\n        \n        # Using the standard formulas (which can be derived by applying the rotation)\n        eig1 = a - t * b\n        eig2 = d + t * b\n\n    # For a positive semi-definite matrix (which A\u1d40A is), the eigenvalues\n    # are non-negative but might have small negative parts due to floating-point issues.\n    eig1 = max(eig1, 0)\n    eig2 = max(eig2, 0)\n    \n    # The singular values are the square roots of these eigenvalues.\n    sigma1 = np.sqrt(eig1)\n    sigma2 = np.sqrt(eig2)\n    \n    # Sort the singular values in descending order.\n    s_vals = sorted([sigma1, sigma2], reverse=True)\n    \n    # Round the result to the nearest 4th decimal.\n    s_vals_rounded = (round(s_vals[0], 4), round(s_vals[1], 4))\n    \n    return s_vals_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3, 1],\n                  [1, 3]])\n    singular_values = svd_2x2_singular_values(A)\n    print(\"Singular values:\", singular_values)\n\n------------------------------------------------------------\n\nExplanation:\n\n1. We compute B = A\u1d40A, a symmetric matrix.\n2. If the off\u2010diagonal element b is nearly zero, then B is already diagonal and the eigen\u2011values are its diagonal entries.\n3. Otherwise we compute tau = (d \u2013 a)\u2044(2b), then the rotation parameter t which is used to \u201czero\u201d the off\u2011diagonal. The eigen\u2011values are obtained as\n   \u2022 eig1 = a \u2013 t*b\n   \u2022 eig2 = d + t*b\n4. The singular values are the square roots of these eigen\u2011values.\n5. Finally, we sort the singular values (largest first) and round them to 4 decimals before returning them as a tuple.\n\nThis code meets the specifications and uses a simple Jacobi rotation to approximate the SVD for a 2\u00d72 matrix without calling numpy\u2019s built\u2011in svd function."}
{"task_id": 13, "completion_id": 0, "completion": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    # Helper function: recursively compute the determinant of any square matrix.\n    def determinant(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n        # Base case for 1x1 matrix\n        if n == 1:\n            return mat[0][0]\n        # Base case for 2x2 matrix\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        det = 0.0\n        # Laplace expansion along the first row\n        for col in range(n):\n            # Create the minor by excluding the first row and current column\n            minor = [row[:col] + row[col+1:] for row in mat[1:]]\n            sign = (-1) ** col  # sign for current element\n            det += sign * mat[0][col] * determinant(minor)\n        return det\n\n    # Since we know our matrix should be 4x4, we can call our generic determinant function.\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"The input must be a 4x4 matrix.\")\n        \n    return determinant(matrix)\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat_4x4 = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    print(\"Determinant:\", determinant_4x4(mat_4x4))  # This particular matrix has a determinant of 0.\n    \n    # Another test matrix\n    mat_4x4_b = [\n        [3, 2, 0, 1],\n        [4, 0, 1, 2],\n        [3, 0, 2, 1],\n        [9, 2, 3, 1]\n    ]\n    print(\"Determinant:\", determinant_4x4(mat_4x4_b))"}
{"task_id": 14, "completion_id": 0, "completion": "Below is one correct implementation using NumPy:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert inputs to NumPy arrays for matrix operations\n    X_mat = np.array(X)\n    y_vec = np.array(y)\n    \n    # Compute the coefficients using the normal equation:\n    # \u03b8 = (X^T * X)^(-1) * X^T * y\n    XT_X = X_mat.T.dot(X_mat)\n    XT_y = X_mat.T.dot(y_vec)\n    \n    # Use np.linalg.inv to compute the inverse of (X^T * X)\n    theta = np.linalg.inv(XT_X).dot(XT_y)\n    \n    # Round each coefficient to 4 decimal places.\n    theta_rounded = [round(coef, 4) for coef in theta]\n    return theta_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # In this example, X already includes an intercept term as the first column.\n    X = [\n        [1, 1.0],\n        [1, 2.0],\n        [1, 3.0],\n        [1, 4.0]\n    ]\n    y = [2.1, 2.9, 3.8, 5.1]\n    \n    coefficients = linear_regression_normal_equation(X, y)\n    print(\"Coefficients:\", coefficients)\n------------------------------------------------------------\n\nExplanation:\n1. The function first converts the input list-of-lists X and list y into NumPy arrays.\n2. Using the normal equation formula \u03b8 = (X\u1d40X)^(-1) X\u1d40y, it computes the coefficients.\n3. Each coefficient is then rounded to four decimal places before being returned.\n4. The included example shows how to call the function. Note that X includes the intercept term as its first column."}
{"task_id": 15, "completion_id": 0, "completion": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    # Ensure y is a column vector\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    \n    # Initialize coefficients (theta) with zeros\n    theta = np.zeros((X.shape[1], 1))\n    \n    for _ in range(iterations):\n        # Compute predictions\n        predictions = np.dot(X, theta)\n        # Compute error\n        error = predictions - y\n        # Calculate gradient\n        gradient = (1 / m) * np.dot(X.T, error)\n        # Update theta using gradient descent rule\n        theta = theta - alpha * gradient\n\n    # Rounding each coefficient to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert to Python list using tolist() method and return\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "completion": "Below is the complete Python function that performs both standardization and min-max normalization on a given 2D NumPy array. Each scaled value is rounded to the nearest 4th decimal, and the final output is returned as Python lists using the tolist() method.\n\n------------------------------------------------------------\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Calculate mean and standard deviation for each feature (column)\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    \n    # Standardize the data: (x - mean) / std.\n    # Avoid division by zero by setting std to 1 when it is 0.\n    safe_stds = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / safe_stds\n    \n    # Calculate min and max for each feature (column)\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    \n    # Normalize the data: (x - min) / (max - min)\n    # Handle constant features where (max - min) is 0 by setting denominator to 1.\n    ranges = np.where((maxs - mins) == 0, 1, maxs - mins)\n    normalized = (data - mins) / ranges\n    \n    # Round the results to the nearest 4th decimal\n    standardized = np.around(standardized, 4)\n    normalized = np.around(normalized, 4)\n    \n    # Convert the NumPy arrays back to Python lists\n    return standardized.tolist(), normalized.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    sample_data = np.array([[1, 100], [2, 200], [3, 300]])\n    \n    standard_scaled, minmax_normalized = feature_scaling(sample_data)\n    \n    print(\"Standardized Data:\")\n    print(standard_scaled)\n    print(\"\\nMin-Max Normalized Data:\")\n    print(minmax_normalized)\n------------------------------------------------------------\n\nExplanation:\n1. The function computes the mean and standard deviation of each feature and uses them to standardize the data.\n2. It then calculates the minimum and maximum values of each feature and uses these to apply min-max normalization.\n3. Safety measures are included: if a feature has a constant value (std or range is zero), the transformation denominator is set to 1 to avoid division by zero.\n4. Finally, both resulting arrays are rounded to 4 decimal places and converted to Python lists before being returned."}
{"task_id": 17, "completion_id": 0, "completion": "import numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert the list of points and centroids to numpy arrays for ease of computation.\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids, dtype=float)\n    \n    for iteration in range(max_iterations):\n        # Step 1: Assign points to the closest centroid.\n        # Calculate the distance from each point to each centroid.\n        # We use broadcasting for efficient computation.\n        # distances shape: (number_of_points, k)\n        distances = np.sqrt(((points_np[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n        # For each point, find the index of the centroid with the minimum distance.\n        assignments = np.argmin(distances, axis=1)\n        \n        # Step 2: Update centroids as the mean of points assigned to each centroid.\n        new_centroids = np.copy(centroids)\n        for i in range(k):\n            assigned_points = points_np[assignments == i]\n            # If no points are assigned to the centroid, leave it unchanged.\n            if assigned_points.shape[0] > 0:\n                new_centroids[i] = assigned_points.mean(axis=0)\n        \n        # Check for convergence: if centroids do not change, then break out.\n        if np.allclose(new_centroids, centroids, atol=1e-8):\n            break\n        \n        centroids = new_centroids\n    \n    # Round final centroids to the nearest fourth decimal and convert to list of tuples.\n    centroids_rounded = np.round(centroids, 4)\n    final_centroids = [tuple(coord) for coord in centroids_rounded.tolist()]\n    return final_centroids\n\n# Example usage (uncomment to test):\n# points = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\n# initial_centroids = [(1.0, 2.0), (5.0, 8.0)]\n# k = 2\n# max_iterations = 100\n# print(k_means_clustering(points, k, initial_centroids, max_iterations))"}
{"task_id": 18, "completion_id": 0, "completion": "Below is one way to implement K-Fold Cross-Validation in Python:\n\n--------------------------------------------------\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation.\n    \n    Parameters:\n    X (np.ndarray): Features matrix with shape (n_samples, n_features).\n    y (np.ndarray): Labels array with shape (n_samples,).\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    random_seed (int or None): Seed for random shuffling. Default is None.\n    \n    Returns:\n    list of tuples: Each tuple contains (train_indices, test_indices) for the respective fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Check that X and y have same number of samples\n    if n_samples != len(y):\n        raise ValueError(\"X and y must have the same number of samples\")\n    \n    # Create indices\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    # Determine the size of each fold and initialize splits list\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1  # Distribute the remainder among the first folds\n    \n    current = 0\n    splits = []\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate([indices[:start], indices[stop:]])\n        splits.append((train_idx, test_idx))\n        current = stop\n    \n    return splits\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generating dummy data\n    X = np.arange(20).reshape(10, 2)\n    y = np.arange(10)\n    \n    folds = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\n    \n    for fold, (train_idx, test_idx) in enumerate(folds):\n        print(f\"Fold {fold + 1}:\")\n        print(\"Train indices:\", train_idx)\n        print(\"Test indices:\", test_idx)\n        print()\n--------------------------------------------------\n\nExplanation:\n1. We first verify that the number of samples in X and y is the same.\n2. If shuffling is enabled, we create an array of indices, optionally seeded with random_seed, and shuffle them.\n3. The dataset is then split into k folds by determining fold sizes that evenly partition n_samples.\n4. For each fold, test indices correspond to the current slice of indices and the train set is comprised of the remaining indices.\n5. Finally, the function returns a list of tuples, where each tuple includes the train and test indices for one fold.\n\nThis implementation adheres to the requirements and can be easily integrated or modified further as needed."}
{"task_id": 19, "completion_id": 0, "completion": "import numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the dataset (zero mean and unit variance)\n    # Compute mean and std for each feature (column)\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=0)\n    # Avoid division by zero by replacing zeros in std with 1\n    stds[stds == 0] = 1\n    standardized_data = (data - means) / stds\n\n    # Step 2: Compute the covariance matrix of the standardized data\n    # Using row samples, so covariance matrix is computed on columns:\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Step 3: Compute eigenvalues and eigenvectors of the covariance matrix\n    # Using np.linalg.eigh because covariance matrix is symmetric\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Step 4: Sort the eigenvalues and eigenvectors in descending order of eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the first k eigenvectors as principal components\n    principal_components = sorted_eigenvectors[:, :k]\n\n    # Round all results to the nearest 4th decimal and convert to list of lists.\n    principal_components_rounded = np.round(principal_components, 4).tolist()\n    return principal_components_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset: 10 samples with 3 features\n    sample_data = np.array([\n        [2.5, 2.4, 1.2],\n        [0.5, 0.7, 0.3],\n        [2.2, 2.9, 1.5],\n        [1.9, 2.2, 1.1],\n        [3.1, 3.0, 1.8],\n        [2.3, 2.7, 1.4],\n        [2.0, 1.6, 0.9],\n        [1.0, 1.1, 0.8],\n        [1.5, 1.6, 1.0],\n        [1.1, 0.9, 0.5]\n    ])\n    \n    # Perform PCA to extract 2 principal components\n    pcs = pca(sample_data, 2)\n    print(\"Principal Components (each column is a component):\")\n    for vec in pcs:\n        print(vec)"}
{"task_id": 20, "completion_id": 0, "completion": "Below is one complete solution. The function uses recursion to build a decision tree based on entropy and information gain. In this implementation, when a node is pure (all examples have the same target value) or no attributes remain for further splitting, the function returns the most common target value. The decision tree is represented as a nested dictionary where internal nodes store the attribute on which the split occurs and each branch is stored as a sub\u2010tree. For example, a decision node might look like:\n\n\u2003\u2003{\"attribute\": \"color\", \"branches\": { \"red\": \"yes\", \"blue\": {\"attribute\": \"size\", \"branches\": {...}} } }\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Helper function to calculate the entropy of a set of examples\n    def entropy(examples, target_attr):\n        # Count the frequency of each target value in examples\n        freq = Counter(example[target_attr] for example in examples)\n        total = len(examples)\n        ent = 0.0\n        for count in freq.values():\n            p = count / total\n            ent -= p * math.log(p, 2)  # log base 2\n        return ent\n\n    # Helper function to compute information gain of splitting on attr\n    def information_gain(examples, attr, target_attr):\n        remainder = 0.0\n        total = len(examples)\n        # Group examples by attribute value\n        attr_values = {}\n        for example in examples:\n            value = example[attr]\n            attr_values.setdefault(value, []).append(example)\n        # Compute the weighted entropy for the split\n        for subset in attr_values.values():\n            remainder += (len(subset) / total) * entropy(subset, target_attr)\n        return entropy(examples, target_attr) - remainder\n\n    # Helper function to find the most common target value in examples\n    def plural_vote(examples, target_attr):\n        freq = Counter(example[target_attr] for example in examples)\n        return freq.most_common(1)[0][0]\n\n    # If no examples remain, return None or a default value (could be modified)\n    if not examples:\n        return None\n\n    # Base case: if all examples have the same target value, return that value\n    target_values = [example[target_attr] for example in examples]\n    if all(val == target_values[0] for val in target_values):\n        return target_values[0]\n\n    # Base case: If there are no more attributes to split on, return the majority value\n    if not attributes:\n        return plural_vote(examples, target_attr)\n\n    # Select the attribute with the highest information gain\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {\"attribute\": best_attr, \"branches\": {}}\n\n    # Get the set of all values for best_attr in examples\n    values = set(example[best_attr] for example in examples)\n\n    # For each possible value of the best attribute, build a branch\n    for value in values:\n        # Build a subset of examples where best_attr == value\n        subset = [example for example in examples if example[best_attr] == value]\n        # Remove best_attr from the list of attributes for the subtree\n        new_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n        tree[\"branches\"][value] = subtree\n\n    return tree\n\n# Example usage:\nif __name__ == '__main__':\n    # Create a simple dataset\n    examples = [\n        {\"color\": \"red\", \"size\": \"small\", \"shape\": \"circle\", \"label\": \"A\"},\n        {\"color\": \"red\", \"size\": \"large\", \"shape\": \"square\", \"label\": \"B\"},\n        {\"color\": \"blue\", \"size\": \"small\", \"shape\": \"circle\", \"label\": \"A\"},\n        {\"color\": \"blue\", \"size\": \"large\", \"shape\": \"square\", \"label\": \"B\"},\n        {\"color\": \"red\", \"size\": \"small\", \"shape\": \"square\", \"label\": \"A\"},\n    ]\n    # List attributes (do not include the target attribute \"label\")\n    attributes = [\"color\", \"size\", \"shape\"]\n    decision_tree = learn_decision_tree(examples, attributes, target_attr=\"label\")\n    print(decision_tree)\n\n------------------------------------------------------------\n\nExplanation:\n\n1. The function first defines helper functions:\n   \u2022 entropy() computes the entropy of the target values in a list of examples.\n   \u2022 information_gain() computes the gain for a given attribute by subtracting the weighted entropy of partitions from the original entropy.\n   \u2022 plural_vote() returns the most frequent target label (used when no further splits are possible).\n\n2. The base cases check whether all examples share the same target label or whether there are no more attributes available to split.\n\n3. The best attribute (with highest information gain) is selected and the tree is built recursively for each value of that attribute.\n\nThis solution should help you build a decision tree classifier that recursively splits the dataset using binary (or multi-way) splits based on information gain."}
{"task_id": 21, "completion_id": 0, "completion": "Below is one acceptable implementation. In this version we \u201csimulate\u201d the Pegasos update in the kernel space by keeping track of an \u201calpha\u2010vector\u201d (one coefficient per training sample) so that the decision function is\n\n\u2003\u2003f(x) = \u03a3\u2c7c alpha[j] * labels[j] * K(data[j], x) + bias .\n\nAt each iteration we use all samples (i.e. a \u201cdeterministic\u201d pass) and apply the following two\u2010step update:\n\u2003\u20031. Scale all alpha values by (1 \u2013 \u03b7\u00b7\u03bb) (note that only the weight vector is regularized, not the bias).\n\u2003\u20032. For each training sample i, compute f(x[i]). If the \u201cmargin\u201d (labels[i] * f(x[i])) is less than 1, then add \u03b7 to alpha[i] and update the bias by \u03b7 * labels[i].\n\nIn what follows \u03b7 = 1/(\u03bb\u00b7t) at iteration t. (For the linear kernel the kernel function is just a dot product; for the RBF, we use the Gaussian kernel with parameter sigma.) Finally, we round the solution to 4 decimals and convert the alpha vector to a Python list using NumPy\u2019s tolist() method.\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    # Initialize alpha coefficients (one per training sample) and bias.\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n\n    # Loop over iterations (t = 1,2,...)\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        # First, scale all previous alpha values.\n        alpha = (1 - eta * lambda_val) * alpha\n\n        # Then loop over all training examples\n        for i in range(n_samples):\n            # Compute kernel evaluations between sample i and all training samples.\n            if kernel == 'linear':\n                # Linear kernel: dot product between data[j] and data[i]\n                k_vals = np.dot(data, data[i])\n            elif kernel == 'RBF':\n                # RBF (Gaussian) kernel: exp(-||x - y||^2/(2*sigma^2))\n                diff = data - data[i]\n                sq_dists = np.sum(diff * diff, axis=1)\n                k_vals = np.exp(-sq_dists / (2 * sigma * sigma))\n            else:\n                raise ValueError(\"Unsupported kernel type: use either 'linear' or 'RBF'.\")\n\n            # Compute the decision function f(x_i)\n            f_val = np.sum(alpha * labels * k_vals) + bias\n\n            # Check the margin constraint for the hinge loss.\n            if labels[i] * f_val < 1:\n                # If the sample is \"misclassified\" (or close to the margin),\n                # update the dual coefficient for this sample and the bias.\n                alpha[i] += eta\n                bias += eta * labels[i]\n\n    # Round the results to the nearest 4th decimal.\n    alpha = np.around(alpha, decimals=4)\n    bias = round(bias, 4)\n    return alpha.tolist(), bias\n\n# Example usage:\nif __name__ == '__main__':\n    # Generating a simple toy dataset with two features\n    X = np.array([[2, 3],\n                  [1, 1],\n                  [2, 1],\n                  [3, 2],\n                  [6, 6],\n                  [7, 8],\n                  [8, 8],\n                  [9, 10]])\n    # Labels: first four are -1 and last four are +1 (binary classification)\n    y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n    \n    # Train our kernel SVM with a linear kernel.\n    alphas, bias_val = pegasos_kernel_svm(X, y, kernel='linear', lambda_val=0.01, iterations=50)\n    print(\"Alpha coefficients:\", alphas)\n    print(\"Bias:\", bias_val)\n\n------------------------------------------------------------\n\nExplanation of key parts:\n1. We compute the learning rate (\u03b7 = 1/(\u03bbt)) at each iteration.\n2. Before scanning through the samples, we \u201cdecay\u201d all the previous alpha coefficients by (1 \u2013 \u03b7\u03bb).\n3. For each training sample, we compute f(x[i]) using the kernel function. If the condition y[i]*f(x[i]) < 1 holds then we update alpha[i] by adding \u03b7, and update the bias using the label.\n4. Finally, the resulting alpha vector is rounded to 4 decimals and converted to a standard Python list, and similarly the bias is rounded.\n\nThis code implements a deterministic Pegasos-style kernel SVM training algorithm as requested."}
{"task_id": 22, "completion_id": 0, "completion": "import math\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    # For better numerical stability, subtract the max score from each score\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    # Compute and round the softmax values to 4 decimal places\n    return [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_scores = [2.0, 1.0, 0.1]\n    print(softmax(sample_scores))\n```"}
{"task_id": 24, "completion_id": 0, "completion": "import math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # List to store predicted probabilities\n    pred_probs = []\n    \n    # Compute prediction for each feature vector\n    for x in features:\n        # Calculate the linear combination (weighted sum + bias)\n        z = sum(x_i * w for x_i, w in zip(x, weights)) + bias\n        # Sigmoid activation function\n        prob = 1 / (1 + math.exp(-z))\n        # Round the probability to 4 decimal places\n        pred_probs.append(round(prob, 4))\n    \n    # Compute Mean Squared Error (MSE)\n    squared_errors = [(p - label) ** 2 for p, label in zip(pred_probs, labels)]\n    mse = round(sum(squared_errors) / len(squared_errors), 4)\n    \n    # Reshape the predicted probabilities into a numpy array and convert back to list\n    pred_probs_list = np.array(pred_probs).tolist()\n    \n    return (pred_probs_list, mse)"}
{"task_id": 25, "completion_id": 0, "completion": "Below is one acceptable solution:\n\n-------------------------------------------------\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, \n                 labels: np.ndarray, \n                 initial_weights: np.ndarray, \n                 initial_bias: float, \n                 learning_rate: float, \n                 epochs: int) -> (np.ndarray, float, list[float]):\n    # Sigmoid activation function\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    # Initialize weights and bias.\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    n_samples = features.shape[0]  # assuming features rows are individual samples\n    \n    # Training loop over epochs\n    for epoch in range(epochs):\n        # Forward pass: compute linear combination and activation\n        z = np.dot(features, weights) + bias  # shape (n_samples,)\n        predictions = sigmoid(z)\n        \n        # Compute Mean Squared Error for this epoch\n        errors = predictions - labels   # error vector\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        \n        # Backpropagation:\n        # MSE = 1/n * sum((prediction-label)^2)\n        # d(MSE)/d(prediction) = 2*(prediction-label)/n\n        # sigmoid derivative = prediction * (1-prediction)\n        # So d(MSE)/dz = [2*(prediction-label)/n] * prediction*(1-prediction)\n        delta = (2 * errors / n_samples) * predictions * (1 - predictions)  # shape (n_samples,)\n        \n        # Compute gradients for weights and bias\n        grad_weights = np.dot(features.T, delta)   # shape same as weights\n        grad_bias = np.sum(delta)\n        \n        # Update weights and bias using gradient descent\n        weights = weights - learning_rate * grad_weights\n        bias = bias - learning_rate * grad_bias\n        \n    # Round final weights and bias to four decimals\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    # Convert weights to a python list before returning\n    weights_list = weights.tolist()\n    \n    return np.array(weights_list), bias, mse_history\n\n# Example test run (uncomment to run):\n# if __name__ == \"__main__\":\n#     # Sample data: 4 samples, 2 features\n#     X = np.array([[0.5, 1.2],\n#                   [1.0, -0.7],\n#                   [-0.3, 0.8],\n#                   [0.2, 0.4]])\n#     y = np.array([1, 0, 1, 0])\n#     initial_weights = np.array([0.1, -0.2])\n#     initial_bias = 0.0\n#     learning_rate = 0.1\n#     epochs = 1000\n# \n#     updated_weights, updated_bias, mse_history = train_neuron(X, y, initial_weights, initial_bias, learning_rate, epochs)\n#     print(\"Updated weights:\", updated_weights)\n#     print(\"Updated bias:\", updated_bias)\n#     print(\"Last epoch MSE:\", mse_history[-1])\n-------------------------------------------------\n\nExplanation:\n\n1. We define a sigmoid function.\n2. In each epoch, we perform a forward pass (computing z and then using the sigmoid function) and calculate the mean squared error (MSE) which is then rounded to four decimals and stored.\n3. For backpropagation, we compute the gradient of the MSE loss with respect to the neuron's output (after applying the sigmoid derivative) and use it to compute gradients for the weights and bias.\n4. We update the weights and bias using the learning rate.\n5. Finally, we round the final weights and bias to four decimals. The weights are converted to a Python list (via numpy\u2019s tolist() method) and then packed into a numpy array before returning, as required.\n6. An example test run is provided in a commented block. \n\nThis solution meets the task requirements."}
{"task_id": 26, "completion_id": 0, "completion": "Below is one solution that implements a minimal autograd engine supporting addition, multiplication, and the ReLU activation function:\n\n------------------------------------------------------------\n# Python code:\n------------------------------------------------------------\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data                # stored scalar value\n        self.grad = 0                   # gradient (initialized to zero)\n        self._backward = lambda: None   # function to backpropagate gradients\n        self._prev = set(_children)     # set of child nodes that contributed to this Value\n        self._op = _op                  # the operation that produced this Value (for visualization/debugging)\n\n    # Overload the addition operator\n    def __add__(self, other):\n        # Allow other to be a scalar, wrap it if needed.\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # the derivative of addition wrt each input is 1\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    # Right-hand add to handle cases like (scalar + Value)\n    def __radd__(self, other):\n        return self + other\n\n    # Overload the multiplication operator\n    def __mul__(self, other):\n        # Allow other to be a scalar, wrap it in a Value if needed.\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # For the product, the gradients are swapped\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    # Right-hand multiplication to handle cases like (scalar * Value)\n    def __rmul__(self, other):\n        return self * other\n\n    # ReLU activation function\n    def relu(self):\n        # ReLU returns the input if positive, otherwise zero\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            # Gradient is 1 if input > 0, else 0.\n            self.grad += (1.0 if self.data > 0 else 0.0) * out.grad\n        out._backward = _backward\n        return out\n\n    # Backpropagation: perform a reverse-mode automatic differentiation.\n    def backward(self):\n        # Build a topologically sorted list of nodes\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        # Set the gradient of the final node\n        self.grad = 1.0\n\n        # Traverse the nodes in reverse topological order and run the backward functions.\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage:\nif __name__ == '__main__':\n    # Create some Value instances\n    a = Value(2.0)\n    b = Value(-3.0)\n    c = Value(10.0)\n\n    # Compute a simple computation: d = a * b + c, then apply ReLU on the result.\n    d = a * b\n    e = d + c\n    f = e.relu()\n\n    # Run backpropagation from f\n    f.backward()\n\n    # Print the forward values and gradients\n    print(\"f =\", f.data)\n    print(\"Gradient at a:\", a.grad)\n    print(\"Gradient at b:\", b.grad)\n    print(\"Gradient at c:\", c.grad)\n\n------------------------------------------------------------\n\nExplanation:\n\n1. The Value class stores a scalar (data) and its gradient (grad). The _prev attribute tracks which Value nodes were used to compute this node so that the backward pass knows how to propagate gradients.\n\n2. Overloaded __add__ and __mul__ create new Value objects with appropriate _op tags and children information. Their corresponding _backward functions update the gradients of the child nodes.\n\n3. The relu() method creates a new Value that is the result of applying the ReLU function.\n\n4. The backward() method performs a reverse topological sort of the computation graph (starting from the final output node) and applies all the _backward functions in reverse order so that gradients are correctly propagated.\n\n5. The example at the bottom shows how to set up a small computation graph, run backpropagation, and then print out the gradients.\n\nThis minimal autograd example mirrors the approach explained in Andrej Karpathy's micrograd project."}
{"task_id": 27, "completion_id": 0, "completion": "Below is the complete Python function. In this solution we first interpret each list inside B and C as giving the coordinates (in \u211d\u00b3) of one of the basis vectors. To compute the transformation matrix from basis B to basis C (i.e. so that for any vector v you have [v]_C = P \u00b7 [v]_B), we first note that if v has coordinates [v]_B then in the standard coordinate system it is given by\n\n\u2003\u2003v = [B] \u00b7 [v]_B\n\nand then since the coordinates in C satisfy\n\n\u2003\u2003[v]_C = [C]\u207b\u00b9 \u00b7 v,\n\nit follows that\n\n\u2003\u2003[v]_C = [C]\u207b\u00b9 \u00b7 [B] \u00b7 [v]_B.\n\nThus the transformation (change\u2010of\u2010basis) matrix from B to C is\n\n\u2003\u2003P = [C]\u207b\u00b9 \u00b7 [B].\n\nIn our code we want to treat the given basis vectors as column vectors, so we take the transpose of the input list when constructing our matrices. Finally, we round each entry in P to the nearest 4th decimal and return a Python list.\n\nBelow is the complete code:\n\n-------------------------------------------------------\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert input lists to numpy arrays and form matrices whose columns are the basis vectors.\n    B_mat = np.array(B).T\n    C_mat = np.array(C).T\n\n    # Compute the inverse of the C matrix\n    C_inv = np.linalg.inv(C_mat)\n    \n    # Compute the transformation matrix P from basis B to C\n    P = C_inv @ B_mat\n    \n    # Round the results to 4 decimals and return the result as a Python list.\n    return np.round(P, 4).tolist()\n\n# Example usage with sample bases:\nif __name__ == \"__main__\":\n    # Standard basis B (for instance) and an alternate basis C provided in standard coordinates.\n    B = [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n    \n    C = [\n        [2, 0, 0],\n        [0, 3, 0],\n        [0, 0, 4]\n    ]\n    \n    P = transform_basis(B, C)\n    print(\"Transformation matrix P from basis B to C:\")\n    for row in P:\n        print(row)\n-------------------------------------------------------\n\nExplanation of the code:\n1. We convert B and C into numpy arrays and then transpose them so that each column corresponds to one basis vector.\n2. We compute the inverse of the matrix for basis C.\n3. The transformation matrix is then given by multiplying C\u207b\u00b9 with B.\n4. We round to 4 decimal places (using np.round) and then use the tolist() method to return a Python list with the desired format.\n\nThis function complies with the task requirements."}
{"task_id": 28, "completion_id": 0, "completion": "Below is one acceptable solution. In the code the eigen\u2010decomposition of A\u1d40A is used to get the singular values (the square roots of the eigen\u2013values) and the right\u2013singular vectors (columns of V). Then U is computed by U = A\u00b7V/\u03c3 and the final S matrix is built as a diagonal matrix of the singular values. (Note that in the standard SVD we have A = U \u00b7 S \u00b7 V\u1d40; here we \u201creturn\u201d V so that one may check A = U\u00b7S\u00b7(V\u1d40) if needed.) All matrix entries are rounded to 4 decimal places and then converted to Python lists by calling tolist(). \n\nBelow is the complete code:\n\n--------------------------------------------------\n#!/usr/bin/env python3\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Check input shape (must be 2x2)\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n        \n    # Step 1. Compute A\u1d40 A\n    AtA = A.T @ A\n\n    # Step 2. Compute the eigen\u2013values and eigen\u2013vectors of A\u1d40A.\n    # Using eigh (symmetric matrix) which returns eigenvalues in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(AtA)\n    \n    # Reverse the order (largest first) so that the singular values are in descending order.\n    order = eigenvalues.argsort()[::-1]  # gets indices in descending order\n    eigenvalues = eigenvalues[order]\n    V = eigenvectors[:, order]  # columns are the right\u2013singular vectors\n\n    # Step 3. The singular values (nonnegative) are the square roots of the eigen-values.\n    sigma = np.sqrt(eigenvalues)\n\n    # Step 4. Compute U = A \u00b7 V / sigma for each singular value.\n    # (Take care of the case sigma == 0)\n    U = np.zeros((2, 2))\n    tol = 1e-10  # tolerance for singular value zero\n    for i in range(2):\n        if sigma[i] > tol:\n            U[:, i] = (A @ V[:, i]) / sigma[i]\n        else:\n            # If the singular value is (close to) zero, pick any unit vector orthogonal\n            # to the ones computed so far.\n            # For 2x2 this is easy: if i == 0, use [1,0]. For i==1, choose perpendicular to U[:,0].\n            if i == 0:\n                U[:, i] = np.array([1, 0])\n            else:\n                # In 2D, an easy orthogonal vector: if U[:,0] = [a,b], then [-b,a] is orthogonal.\n                U[:, i] = np.array([-U[1, 0], U[0, 0]])\n                \n    # Step 5. Create the diagonal matrix S (2x2) from the singular values.\n    S = np.diag(sigma)\n    \n    # Round all results to 4 decimal places.\n    U = np.around(U, 4)\n    S = np.around(S, 4)\n    V = np.around(V, 4)\n    \n    # IMPORTANT NOTE:\n    # In standard SVD, A = U \u00b7 S \u00b7 V\u1d40.\n    # The code above returns V (whose columns are the right\u2013singular vectors),\n    # so that to reconstruct A you need to use V transposed:\n    #    A_reconstructed = U @ S @ V.T\n    # If you require A = U \u00b7 S \u00b7 V then simply return V.T (and adjust rounding accordingly).\n\n    # Finally, convert the numpy arrays to Python lists.\n    return U.tolist(), S.tolist(), V.tolist()\n\n\n# Example usage:\nif __name__ == '__main__':\n    # Example 2x2 matrix\n    A = np.array([[3, 1],\n                  [1, 3]], dtype=float)\n    \n    U, S, V = svd_2x2(A)\n    print(\"U =\", U)\n    print(\"S =\", S)\n    print(\"V =\", V)\n    \n    # To check the decomposition (note the standard formula A = U S V\u1d40):\n    U_mat = np.array(U)\n    S_mat = np.array(S)\n    V_mat = np.array(V)\n    A_reconstructed = U_mat @ S_mat @ V_mat.T\n    A_reconstructed = np.around(A_reconstructed, 4)\n    print(\"A_reconstructed =\", A_reconstructed.tolist())\n--------------------------------------------------\n\nExplanation of Key Steps:\n\n1. We compute A\u1d40A and then use np.linalg.eigh because A\u1d40A is symmetric. Since eigh returns the eigen-values in ascending order, we reverse the order so that the singular values (\u03c3) come out in descending order.\n\n2. The right\u2013singular matrix V is composed of the eigen\u2013vectors of A\u1d40A. Then, for every nonzero singular value, we compute the corresponding left\u2013singular vector as u = (1/\u03c3)\u00b7(A \u00b7 v).\n\n3. Finally, we round every number to 4 decimals and convert the matrices to lists using the tolist() method.\n\nYou can run this code to compute the SVD for any 2x2 matrix."}
{"task_id": 29, "completion_id": 0, "completion": "import numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in numpy arrays X and y while preserving correspondence,\n    then return the shuffled arrays as Python lists using tolist().\n    \n    Parameters:\n    X (np.array): Array containing the input features.\n    y (np.array): Array containing the target values.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: (shuffled_X_list, shuffled_y_list) where both are Python lists.\n    \"\"\"\n    # Set the random seed if provided for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n        \n    # Ensure that X and y have the same number of samples\n    if len(X) != len(y):\n        raise ValueError(\"X and y must have the same number of samples.\")\n    \n    # Generate a permutation of indices and shuffle the arrays\n    indices = np.random.permutation(len(X))\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    \n    # Convert the shuffled numpy arrays to Python lists using tolist() and return them\n    return shuffled_X.tolist(), shuffled_y.tolist()"}
{"task_id": 30, "completion_id": 0, "completion": "import numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    # Get the number of samples in X\n    num_samples = X.shape[0]\n    \n    # Iterate over the data in steps of batch_size\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[start_idx:end_idx].tolist()  # convert batch to list\n        \n        # If y is provided, yield a pair (X_batch, y_batch)\n        if y is not None:\n            y_batch = y[start_idx:end_idx].tolist()  # convert batch to list\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n\n# Example usage:\nif __name__ == '__main__':\n    # Create a dummy dataset with 10 samples and 2 features\n    X = np.arange(20).reshape(10, 2)\n    y = np.arange(10)\n    \n    # Use batch_iterator with batch size 3 for X only\n    print(\"Batches (X only):\")\n    for batch in batch_iterator(X, batch_size=3):\n        print(batch)\n    \n    # Use batch_iterator with batch size 4 for (X, y) pair\n    print(\"\\nBatches (X and y):\")\n    for batch in batch_iterator(X, y=y, batch_size=4):\n        print(batch)"}
{"task_id": 31, "completion_id": 0, "completion": "import numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    # Create a boolean mask where the specified feature is >= threshold\n    mask = X[:, feature_i] >= threshold\n    \n    # Split the dataset based on the mask\n    subset_meeting = X[mask]\n    subset_not_meeting = X[~mask]\n    \n    # Convert the numpy arrays to Python lists using tolist() and return them\n    return subset_meeting.tolist(), subset_not_meeting.tolist()"}
{"task_id": 32, "completion_id": 0, "completion": "Below is one valid solution:\n\n------------------------------------------------------------\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for array X up to the specified degree.\n    \n    Parameters:\n      X      : 2D numpy array of shape (n_samples, n_features)\n      degree : integer, maximum degree of polynomial features to generate.\n      \n    Returns:\n      A python list (generated by np.array(...).tolist()) containing the new features.\n      The features include the bias term (i.e. constant 1) corresponding to degree 0.\n      \n    Example:\n      If X = [[x1, x2]] and degree=2, the generated features will be:\n      [1, x1, x2, x1*x1, x1*x2, x2*x2]\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Precompute all index combinations for degrees 0 up to degree,\n    # where degree==0 yields the empty tuple (for the constant feature)\n    comb_indices = []\n    for d in range(degree + 1):\n        comb_indices.extend(combinations_with_replacement(range(n_features), d))\n    \n    # Prepare the new features for each sample.\n    # There will be len(comb_indices) features per sample.\n    # For each combination, multiply the corresponding feature values.\n    output = np.empty((n_samples, len(comb_indices)))\n    \n    for i in range(n_samples):\n        feature_list = []\n        for indices in comb_indices:\n            # For the empty tuple, the corresponding \"product\" is 1.\n            if len(indices) == 0:\n                product = 1\n            else:\n                product = 1\n                for idx in indices:\n                    product *= X[i, idx]\n            feature_list.append(product)\n        output[i, :] = feature_list\n        \n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data: 2 samples, 2 features\n    X = np.array([[2, 3],\n                  [4, 5]])\n    deg = 2\n    poly_out = polynomial_features(X, deg)\n    print(poly_out)\n------------------------------------------------------------\n\nExplanation:\n1. We first compute all combinations (with replacement) of feature indices for degrees 0 to degree. Note that for degree 0 there is one combination\u2014the empty tuple\u2014which corresponds to the constant feature 1.\n2. For each sample in X, for each combination we compute the product of the features corresponding to the indices in the combination.\n3. We store the resulting features in a numpy array and then convert it to a Python list via .tolist() before returning.\n\nThis meets the problem specifications."}
{"task_id": 33, "completion_id": 0, "completion": "Below is the complete Python function that meets your requirements:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    \n    subsets = []\n    for _ in range(n_subsets):\n        # Choose indices based on whether we sample with or without replacement.\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        \n        # Select subsets from X and y\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Convert numpy arrays to lists using .tolist()\n        subset_tuple = (X_subset.tolist(), y_subset.tolist())\n        subsets.append(subset_tuple)\n        \n    return subsets\n\n# Example usage:\nif __name__ == '__main__':\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    # Generate 3 random subsets with replacements\n    subsets_with_replacement = get_random_subsets(X, y, 3, replacements=True)\n    print(\"Subsets with Replacement:\")\n    for subset in subsets_with_replacement:\n        print(subset)\n    \n    # Generate 3 random subsets without replacements\n    subsets_without_replacement = get_random_subsets(X, y, 3, replacements=False)\n    print(\"\\nSubsets without Replacement:\")\n    for subset in subsets_without_replacement:\n        print(subset)\n------------------------------------------------------------\n\nExplanation:\n1. We initialize the random seed using np.random.seed(seed) for reproducibility.\n2. For each subset, we generate n_samples indices using np.random.choice. If replacements is True, sampling is done with replacement; otherwise (when replacements is False) sampling is done without replacement.\n3. We extract the corresponding rows from X and elements from y, then convert them to Python lists using the .tolist() method.\n4. Finally, we return a list of tuple pairs (X_subset, y_subset).\n\nThis function will generate the desired output as specified."}
{"task_id": 34, "completion_id": 0, "completion": "import numpy as np\ndef to_categorical(x, n_col=None):\n    # Ensure x is a numpy array\n    x = np.array(x)\n    \n    # If number of columns isn't provided, determine it from the data by taking the max value plus one.\n    if n_col is None:\n        n_col = int(np.max(x)) + 1\n    \n    # Initialize a 2D array filled with zeros.\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n    \n    # Fill in the appropriate positions with 1's.\n    one_hot[np.arange(x.shape[0]), x] = 1\n    \n    # Return the one-hot encoded array as a list.\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "completion": "import numpy as np\ndef make_diagonal(x):\n    # Create a diagonal matrix using numpy's diag function\n    diag_matrix = np.diag(x)\n    # Convert the numpy array to a python list and return it\n    return diag_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "completion": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    # Count matches between true and predicted labels\n    correct_predictions = np.sum(y_true == y_pred)\n    # Calculate the accuracy as a fraction of correct predictions\n    accuracy = correct_predictions / len(y_true)\n    # Round the result to the nearest 4th decimal place\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "completion": "Below is the complete Python function that computes the correlation matrix as described. If Y is not provided, it computes the correlation matrix of X with itself (i.e. correlation among the variables in X). Otherwise, it computes the cross-correlation between the columns of X and Y. All the results are rounded to the nearest fourth decimal, and the final output is converted to a Python list using the tolist() method.\n\n------------------------------------------------------------\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    # If Y is not provided: compute correlation matrix for X with itself.\n    if Y is None:\n        # np.corrcoef by default computes correlations between rows.\n        # Since each row is an observation and each column a variable, set rowvar=False.\n        corr = np.corrcoef(X, rowvar=False)\n        # Round to 4 decimals and convert to a Python list.\n        return np.around(corr, 4).tolist()\n    else:\n        # Ensure X and Y have the same number of observations (rows)\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X and Y must have the same number of rows (observations)\")\n        \n        # Compute means for every variable (columns)\n        mean_X = np.mean(X, axis=0)\n        mean_Y = np.mean(Y, axis=0)\n        \n        # Center the datasets by subtracting the mean.\n        X_centered = X - mean_X\n        Y_centered = Y - mean_Y\n        \n        # Compute the standard deviations for each variable.\n        # Using ddof=0 to be consistent with np.corrcoef's computation.\n        std_X = np.std(X_centered, axis=0, ddof=0)\n        std_Y = np.std(Y_centered, axis=0, ddof=0)\n        \n        # Compute the covariance matrix between X and Y:\n        # cov_ij = (1/n) * sum_k (X[k,i] - mean_X[i]) * (Y[k,j] - mean_Y[j])\n        n = X.shape[0]\n        cov = (X_centered.T @ Y_centered) / n\n        \n        # Compute the outer product of standard deviations for normalization.\n        denominator = np.outer(std_X, std_Y)\n        \n        # To avoid division by zero, we handle positions where denominator is 0.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            corr = np.where(denominator == 0, 0, cov / denominator)\n        \n        # Round the correlation matrix to 4 decimals and convert to a Python list.\n        return np.around(corr, 4).tolist()\n\n------------------------------------------------------------\n\n# Example Usage:\nif __name__ == '__main__':\n    # Generate a random dataset X of shape (100, 3)\n    np.random.seed(42)\n    X = np.random.randn(100, 3)\n    \n    # When Y is None: correlation matrix within X.\n    corr_matrix = calculate_correlation_matrix(X)\n    print(\"Correlation matrix for X with itself:\")\n    print(corr_matrix)\n    \n    # Generate another dataset Y of shape (100, 2)\n    Y = np.random.randn(100, 2)\n    cross_corr_matrix = calculate_correlation_matrix(X, Y)\n    print(\"\\nCross-correlation matrix for X and Y:\")\n    print(cross_corr_matrix)\n\nThis implementation meets the task requirements and ensures that the results are rounded and returned as a Python list."}
{"task_id": 38, "completion_id": 0, "completion": "Below is one complete solution. In this solution we use simple decision stumps as weak classifiers. For each classifier, we loop through every feature and several candidate thresholds (mid\u2010points between unique sorted values, as well as some boundary candidates) and over both possible polarities. With the best stump (the one having the smallest weighted error), we compute its weight (alpha), update the sample weights, and then finally return a list of dictionaries, each with rounded values (to 4 decimals).\n\nBelow is the code:\n\n------------------------------------------------------------\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    # Initialize uniform weights\n    D = np.ones(n_samples) / n_samples\n    classifiers = []\n    \n    # AdaBoost iterations\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_clf = {}\n        best_pred = None\n        \n        # search for the best decision stump (a threshold on one feature)\n        for j in range(n_features):\n            feature_values = X[:, j]\n            # Get unique sorted candidate values\n            unique_vals = np.unique(feature_values)\n            candidates = []\n            \n            # If there is only one unique value, use it as candidate threshold\n            if unique_vals.size == 1:\n                candidates.append(unique_vals[0])\n            else:\n                # Use midpoints between consecutive unique values as thresholds\n                for k in range(len(unique_vals) - 1):\n                    candidates.append((unique_vals[k] + unique_vals[k+1]) / 2)\n                # Also test thresholds slightly lower than the minimum and higher than the maximum.\n                candidates.append(unique_vals[0] - 1)\n                candidates.append(unique_vals[-1] + 1)\n  \n            # Try every threshold and polarity\n            for threshold in candidates:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples)\n                    # The decision stump: if polarity*(x_j - threshold) < 0 predict -1\n                    predictions[polarity * (feature_values - threshold) < 0] = -1\n\n                    # Calculate weighted error\n                    misclassified = (predictions != y)\n                    error = np.sum(D[misclassified])\n                    # Round the error for output constraints (and for storage/comparison uniformity)\n                    error_rounded = round(error, 4)\n                    \n                    if error_rounded < best_error:\n                        best_error = error_rounded\n                        best_clf = {'feature_index': j, \n                                    'threshold': round(threshold, 4), \n                                    'polarity': polarity}\n                        best_pred = predictions.copy()\n        \n        # Avoid division by zero if the error is 0: set error very small\n        if best_error == 0:\n            best_error = 1e-10\n        \n        # AdaBoost classifier weight (alpha)\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        alpha = round(alpha, 4)\n        best_clf['alpha'] = alpha\n        classifiers.append(best_clf)\n        \n        # Update sample weights; misclassified samples get increased weight\n        D = D * np.exp(-alpha * y * best_pred)\n        D = D / np.sum(D)   # Normalize to sum to one\n        \n    return classifiers\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Toy dataset: four samples, single feature, labels {-1, 1}\n    X = np.array([[1], [2], [3], [4]])\n    y = np.array([1, 1, -1, -1])\n    clf_list = adaboost_fit(X, y, 3)\n    print(clf_list)\n------------------------------------------------------------\n\nExplanation of key parts:\n1. We initialize each sample\u2019s weight D (uniformly).\n2. For each of the n_clf iterations we look for the best decision stump:\n  \u2022 For each feature we select a set of candidate thresholds.\n  \u2022 For each candidate threshold we test both polarities (which determine on which side of the threshold the positive label is predicted).\n  \u2022 We compute the weighted error and select the classifier (feature index, threshold, polarity) that minimizes the error.\n3. We compute the classifier weight (alpha) as 0.5 * ln((1 - error)/error). (A small value is used if error is 0 to avoid division by zero.)\n4. The sample weights are then updated by D = D * exp(-alpha * y * prediction) and normalized.\n5. Classifier parameters (with numerical values rounded to 4 decimal places) are stored in a Python list and returned.\n\nThis solution meets the problem\u2019s constraints and output formatting requirements."}
{"task_id": 39, "completion_id": 0, "completion": "import numpy as np\n\ndef log_softmax(scores: list):\n    # Convert input list to a numpy array\n    scores_arr = np.array(scores)\n    \n    # Subtract the maximum score for numerical stability\n    stabilized_scores = scores_arr - np.max(scores_arr)\n    \n    # Compute the exponentials of the stabilized scores\n    exp_scores = np.exp(stabilized_scores)\n    \n    # Sum of exponentials\n    sum_exp = np.sum(exp_scores)\n    \n    # Compute log softmax: subtract log(sum_exp) from each stabilized score\n    log_softmax_values = stabilized_scores - np.log(sum_exp)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    \n    # Return as a Python list using tolist() method\n    return rounded_log_softmax.tolist()"}
{"task_id": 40, "completion_id": 0, "completion": "Below is one complete implementation. In this example we also define a simple SGD optimizer that has an update() method. (In your real framework you may replace it with any optimizer you wish.) Note that in both forward_pass() and backward_pass() the computed arrays are rounded to 4\u2010decimals and then converted to a python list using tolist().\n\n------------------------------------------------\n#!/usr/bin/env python\nimport numpy as np\nimport copy\nimport math\n\n# A simple SGD optimizer for demonstration purposes.\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n        \n    def update(self, param, grad):\n        # Basic SGD update rule.\n        return param - self.lr * grad\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape  # expects a shape tuple (n_features,)\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None       # Weight matrix\n        self.w0 = None      # Bias vector\n        # Placeholders for optimizers. They will be initialized in initialize().\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"input_shape must be provided to initialize the Dense layer.\")\n        # Using uniform initialization in the range [-limit, limit]\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        # Initialize optimizers (using SGD here with a chosen learning rate)\n        self.W_optimizer = SGD(learning_rate=0.01)\n        self.w0_optimizer = SGD(learning_rate=0.01)\n\n    def parameters(self):\n        # Return the total number of trainable parameters in W and w0.\n        if self.W is None or self.w0 is None:\n            return 0\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        # Save the input for backward pass.\n        self.layer_input = X\n        # Compute output = X.dot(W) + w0.\n        # Ensure that w0 is added to every row (using numpy broadcasting)\n        output = np.dot(X, self.W) + self.w0\n        # Round the results to the nearest 4th decimal and return as a python list.\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        # accum_grad is the gradient with respect to the outputs of this layer.\n        # Compute the gradient with respect to the input: dX = accum_grad dot W^T\n        grad_input = np.dot(accum_grad, self.W.T)\n        \n        # Compute the gradients with respect to weights and bias.\n        # Weight gradient: X^T dot accum_grad\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        # Bias gradient: sum over the batch dimension.\n        grad_w0 = np.sum(accum_grad, axis=0)\n        \n        if self.trainable:\n            # Update weights and bias using their respective optimizers.\n            self.W = self.W_optimizer.update(self.W, grad_W)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_w0)\n        \n        # Return the gradient with respect to the input, rounded to 4 decimals as a python list.\n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        # Return the output shape produced by the forward pass.\n        return (self.n_units,)\n\n# -----------------------\n# Example usage:\n\nif __name__ == \"__main__\":\n    # Suppose we have 3 features as input and want 2 neurons in our Dense layer.\n    input_shape = (3,)\n    dense_layer = Dense(n_units=2, input_shape=input_shape)\n    dense_layer.initialize()\n    \n    # Create some dummy input data (batch_size=4, features=3)\n    X = np.array([[0.1, 0.2, 0.3],\n                  [0.4, 0.5, 0.6],\n                  [0.7, 0.8, 0.9],\n                  [1.0, 1.1, 1.2]])\n    \n    # Forward pass\n    output = dense_layer.forward_pass(X, training=True)\n    print(\"Forward pass output (rounded, as list):\")\n    print(output)\n    \n    # Suppose we get a gradient from next layer (same shape as output: (4,2))\n    accum_grad = np.array([[0.01, -0.02],\n                           [0.03, -0.04],\n                           [0.05, -0.06],\n                           [0.07, -0.08]])\n    \n    # Backward pass, we get gradient with respect to input.\n    grad_input = dense_layer.backward_pass(accum_grad)\n    print(\"\\nBackward pass gradient with respect to input (rounded, as list):\")\n    print(grad_input)\n    \n    # Check the number of parameters.\n    print(\"\\nTrainable parameters count:\")\n    print(dense_layer.parameters())\n    \n    # Output shape of layer\n    print(\"\\nOutput shape of layer:\")\n    print(dense_layer.output_shape())\n    \n# When running this module, you should see outputs rounded to 4 decimals and expressed as Python lists.\n------------------------------------------------\n\nExplanation:\n1. The initialize() method sets up the weight matrix W using a uniform distribution between \u2013limit and limit, where limit = 1/sqrt(n_features). Biases are set to zeros.\n2. The forward_pass() computes X.dot(W) + w0, rounds the results, and returns a python list.\n3. The backward_pass() calculates the gradient with respect to inputs, weights, and bias. When the layer is trainable the weights and bias are updated via SGD.\n4. The output_shape() returns the tuple (n_units,).\n\nAll outputs (forward results and gradients) are rounded to 4th decimal and then converted to a python list using .tolist()."}
{"task_id": 41, "completion_id": 0, "completion": "Below is the complete Python implementation of the simple 2D convolutional layer:\n\n-----------------------------------------------------------\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Get kernel shape\n    k_h, k_w = kernel.shape\n    \n    # Pad the input matrix\n    padded_matrix = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    \n    # Determine output dimensions\n    out_height = (padded_matrix.shape[0] - k_h) // stride + 1\n    out_width = (padded_matrix.shape[1] - k_w) // stride + 1\n    \n    # Initialize output matrix\n    output = np.zeros((out_height, out_width))\n    \n    # Perform convolution\n    for i in range(out_height):\n        for j in range(out_width):\n            # Calculate the region of interest\n            vert_start = i * stride\n            vert_end = vert_start + k_h\n            horiz_start = j * stride\n            horiz_end = horiz_start + k_w\n            current_region = padded_matrix[vert_start:vert_end, horiz_start:horiz_end]\n            \n            # Element-wise multiplication and sum over the kernel area\n            output[i, j] = np.sum(current_region * kernel)\n    \n    # Round the results to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Return output as python list\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define an example input matrix and kernel\n    input_matrix = np.array([[1, 2, 3, 0],\n                             [0, 1, 2, 3],\n                             [3, 0, 1, 2],\n                             [2, 3, 0, 1]])\n                             \n    kernel = np.array([[1, 0],\n                       [0, -1]])\n                       \n    padding = 1\n    stride = 1\n    result = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(\"Convolved Output:\")\n    for row in result:\n        print(row)\n-----------------------------------------------------------\n\nExplanation:\n1. The function begins by calculating the necessary padding on the input matrix.\n2. The output dimensions are computed based on the padded input shape, kernel size, and stride.\n3. A nested loop iterates through the padded matrix to apply the kernel and compute the sum of element-wise products for each output position.\n4. The result is rounded to four decimal places and then converted to a Python list using the tolist() method.\n\nThis implementation meets all the specified constraints."}
{"task_id": 42, "completion_id": 0, "completion": "def relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0.0\n\n# Example usage and simple tests:\nif __name__ == \"__main__\":\n    test_values = [-3.0, 0.0, 2.5, -0.1, 4.2]\n    for value in test_values:\n        print(f\"relu({value}) = {relu(value)}\")"}
{"task_id": 43, "completion_id": 0, "completion": "import numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Compute predictions using the linear model\n    y_pred = X.dot(w)\n    # Calculate Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    # Calculate regularization term (L2 penalty)\n    reg_term = alpha * np.sum(w ** 2)\n    # Total Ridge loss is the sum of MSE and regularization term\n    total_loss = mse + reg_term\n    # Return the loss rounded to the nearest 4 decimals\n    return round(total_loss, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample feature matrix (3 samples, 2 features)\n    X_sample = np.array([[1, 2], [3, 4], [5, 6]])\n    # Sample coefficients\n    w_sample = np.array([0.1, 0.2])\n    # Sample true labels\n    y_true_sample = np.array([1, 2, 3])\n    # Regularization parameter\n    alpha_sample = 0.5\n    \n    loss = ridge_loss(X_sample, w_sample, y_true_sample, alpha_sample)\n    print(\"Ridge Regression Loss:\", loss)"}
{"task_id": 44, "completion_id": 0, "completion": "def leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float|int: The result of the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-10, -1, 0, 1, 10]\n    for value in test_values:\n        print(f\"leaky_relu({value}) = {leaky_relu(value)}\")"}
{"task_id": 45, "completion_id": 0, "completion": "import numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "completion": "import numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    \n    Precision is defined as:\n        precision = TP / (TP + FP)\n    \n    where:\n        TP (true positives) are the cases where y_true == 1 and y_pred == 1.\n        FP (false positives) are the cases where y_true == 0 and y_pred == 1.\n        \n    Parameters:\n        y_true (np.array): numpy array of true binary labels (0s and 1s).\n        y_pred (np.array): numpy array of predicted binary labels (0s and 1s).\n        \n    Returns:\n        float: The precision metric. If there are no positive predictions,\n               returns 0.0.\n    \"\"\"\n    # Ensure input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false positives\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Avoid division by zero: if (tp + fp) == 0, then precision is defined as 0.0\n    if (tp + fp) == 0:\n        return 0.0\n    return tp / (tp + fp)\n\n# Example usage:\nif __name__ == '__main__':\n    # Define example arrays\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0])\n    y_pred = np.array([1, 1, 1, 0, 0, 1, 0])\n    \n    # Calculate precision\n    prec = precision(y_true, y_pred)\n    print(\"Precision:\", prec)"}
{"task_id": 47, "completion_id": 0, "completion": "Below is one correct solution. In this implementation the function uses the passed\u2010in parameter method to decide whether to perform batch, stochastic (i.e. single sample) or mini-batch gradient descent. At every iteration the mean squared error (MSE) gradient is computed and the weights are updated. Finally, the function computes the final predictions (X \u00b7 weights), rounds them to 4 decimal places, reshapes them (if necessary) and returns a plain Python list using numpy\u2019s tolist().\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    n_samples = len(y)\n    \n    for i in range(n_iterations):\n        if method == 'batch':\n            # Use the whole dataset to update weights\n            predictions = X.dot(weights)\n            errors = predictions - y\n            # Gradient for MSE: (2/n) * X^T (predictions - y)\n            gradient = 2 * X.T.dot(errors) / n_samples\n            weights = weights - learning_rate * gradient\n            \n        elif method == 'stochastic':\n            # Update weights on a single sample chosen at random\n            idx = np.random.randint(n_samples)\n            # Use slicing to keep dimensional consistency:\n            X_i = X[idx:idx+1]\n            y_i = y[idx:idx+1]\n            prediction = X_i.dot(weights)\n            error = prediction - y_i\n            gradient = 2 * X_i.T.dot(error)  # note: single sample so division by 1 is omitted\n            weights = weights - learning_rate * gradient\n            \n        elif method == 'mini-batch':\n            # Sample a mini-batch of size `batch_size` without replacement\n            if batch_size > n_samples:\n                raise ValueError(\"batch_size should not exceed number of samples.\")\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n            predictions = X_batch.dot(weights)\n            errors = predictions - y_batch\n            gradient = 2 * X_batch.T.dot(errors) / batch_size\n            weights = weights - learning_rate * gradient\n            \n        else:\n            raise ValueError(\"Method should be 'batch', 'stochastic', or 'mini-batch'\")\n    \n    # After finishing iterations, calculate the final predictions over the entire dataset.\n    final_predictions = X.dot(weights)\n    # Round the results to 4 decimal places:\n    final_predictions = np.round(final_predictions, 4)\n    # Return as a python list.\n    return final_predictions.tolist()\n\n# Example of usage:\nif __name__ == '__main__':\n    # Create some synthetic data\n    np.random.seed(42)\n    X = 2 * np.random.rand(100, 1)\n    y = 4 + 3 * X[:, 0] + np.random.randn(100)\n    # Reshape y to a column vector (alternatively, work with 1-D array if you prefer)\n    y = y.reshape(-1, 1)\n    \n    # Initialize weights (including for the intercept if desired)\n    weights = np.random.randn(1, 1)\n    \n    # Run gradient descent (for example, using batch gradient descent)\n    predictions = gradient_descent(X, y, weights, learning_rate=0.01, n_iterations=1000, method='batch')\n    print(\"Predictions:\", predictions[:5])\n------------------------------------------------------------\n\nExplanation:\n\n1. The function first determines n_samples.\n2. For each iteration:\n   \u2022 When method == 'batch': uses the entire dataset to compute the gradient.\n   \u2022 When method == 'stochastic': picks a single random sample (using slicing to preserve matrix dimensions) and computes the gradient from that sample.\n   \u2022 When method == 'mini-batch': picks a random set of indices (of size batch_size) and computes the gradient over that mini-batch.\n3. After training, final output predictions (X \u00b7 weights) are computed, rounded to 4 decimal places using np.round, and converted to a Python list using tolist().\n4. A simple example is provided under if __name__ == '__main__'.\n\nThis solution meets the task requirements."}
{"task_id": 48, "completion_id": 0, "completion": "import numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Compute the Reduced Row Echelon Form of the given matrix.\n    The function converts the matrix (either as a list of lists or a numpy array)\n    into its reduced row echelon form (RREF) and returns the result as a Python list.\n    \n    Parameters:\n        matrix (list or numpy.ndarray): The input matrix.\n    \n    Returns:\n        list: The RREF of the matrix as a list of lists.\n    \"\"\"\n    # Convert input to a numpy array of type float (for precision in division)\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    lead = 0  # column index for leading entry\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        # Find the pivot row for the current column (i.e., first row with a non-zero element in the 'lead' column)\n        while np.abs(A[i, lead]) < 1e-12:\n            i += 1\n            if i == rows:\n                # if no pivot found in the column, reset i to r and move to next column\n                i = r\n                lead += 1\n                if lead >= cols:\n                    break\n        if lead >= cols:\n            break\n        # Swap the current row with the row with the pivot element\n        A[[r, i]] = A[[i, r]]\n        # Normalize the pivot row such that the pivot becomes 1\n        pivot = A[r, lead]\n        if np.abs(pivot) > 1e-12:  # avoid division by zero\n            A[r] = A[r] / pivot\n        # Eliminate all other entries in the lead column\n        for i in range(rows):\n            if i != r:\n                factor = A[i, lead]\n                A[i] = A[i] - factor * A[r]\n        lead += 1  # move to the next column\n    \n    return A.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example matrix\n    mat = [\n        [1, 2, -1, -4],\n        [2, 3, -1, -11],\n        [-2, 0, -3, 22]\n    ]\n    print(\"Original matrix:\")\n    for row in mat:\n        print(row)\n\n    rref_mat = rref(mat)\n    print(\"\\nRREF:\")\n    for row in rref_mat:\n        print(row)"}
{"task_id": 49, "completion_id": 0, "completion": "import numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Adam Optimizer to minimize the function f.\n    \n    Parameters:\n        f (function): Objective function.\n        grad (function): Function to compute the gradient of f.\n        x0 (array-like): Initial parameter values.\n        learning_rate (float): Step size.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): Small constant for numerical stability.\n        num_iterations (int): Number of iterations.\n        \n    Returns:\n        list: Optimized parameters rounded to four decimals.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    # Main optimization loop\n    for t in range(1, num_iterations + 1):\n        g = grad(x)  # Compute gradient at current x\n        m = beta1 * m + (1 - beta1) * g        # Update biased first moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)   # Update biased second raw moment estimate\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n        # Compute bias-corrected second moment estimate\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to the nearest 4th decimal and convert to python list\n    return np.around(x, decimals=4).tolist()"}
{"task_id": 50, "completion_id": 0, "completion": "Below is one valid implementation:\n\n-------------------------------------------------------\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Number of training examples and number of features\n    n, p = X.shape\n    \n    # Initialize weights and bias\n    w = np.zeros(p)\n    b = 0.0\n    \n    # Gradient descent loop\n    for i in range(max_iter):\n        # Compute predictions: y_hat = Xw + b\n        y_pred = np.dot(X, w) + b\n        \n        # Compute error vector\n        error = y_pred - y  # shape: (n,)\n        \n        # Compute gradient for weights (loss part) and add L1 penalty gradient: sign(w)\n        grad_w = (1 / n) * np.dot(X.T, error) + alpha * np.sign(w)\n        # Compute gradient for bias (no regularization on bias)\n        grad_b = (1 / n) * np.sum(error)\n        \n        # Update parameters\n        new_w = w - learning_rate * grad_w\n        new_b = b - learning_rate * grad_b\n        \n        # Check for convergence (based on parameter change)\n        if np.linalg.norm(new_w - w, ord=1) < tol and abs(new_b - b) < tol:\n            w, b = new_w, new_b\n            break\n        \n        w, b = new_w, new_b\n\n    # Round the results to the nearest 4th decimal\n    w = np.round(w, 4)\n    b = round(b, 4)\n    \n    # Return the weights as a Python list and the bias\n    return (w.tolist(), b)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [2, 3], [3, 4]])\n    y = np.array([3, 4, 5])\n    \n    weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n-------------------------------------------------------\n\nExplanation:\n1. We initialize the weight vector (w) for p features to zeros and the bias (b) to 0.\n2. In each iteration, we compute the predicted value y_pred as the dot-product plus bias.\n3. The error (difference between prediction and actual) is used to compute gradients.\n4. For weights, we add the gradient from the squared-error loss (scaled by 1/n) and the subgradient from the L1 penalty (alpha * sign(w)). Note that the sign function (np.sign) returns the subgradient for the L1 norm.\n5. The parameters are updated and convergence is checked using the L1 norm of change for weights and the absolute change for the bias.\n6. Once complete, weights are rounded to 4 decimal places and converted to a Python list using tolist().\n\nThis implementation meets the requirements given."}
{"task_id": 51, "completion_id": 0, "completion": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m, n = len(source), len(target)\n    # Create a (m+1) x (n+1) matrix for dynamic programming\n    D = np.zeros((m + 1, n + 1), dtype=int)\n    \n    # Base cases: transforming a string into an empty string (or vice versa)\n    for i in range(m + 1):\n        D[i, 0] = i\n    for j in range(n + 1):\n        D[0, j] = j\n\n    # Fill the matrix with the minimal edit distances\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Cost is 0 if characters match, otherwise 1 for substitution\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            \n            # Operations: deletion, insertion, substitution\n            deletion = D[i - 1, j] + 1\n            insertion = D[i, j - 1] + 1\n            substitution = D[i - 1, j - 1] + cost\n            \n            D[i, j] = min(deletion, insertion, substitution)\n            \n            # Check for possible transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                D[i, j] = min(D[i, j], D[i - 2, j - 2] + 1)\n\n    return D[m, n]\n\n# Example usage:\nif __name__ == \"__main__\":\n    s1 = \"caper\"\n    s2 = \"acer\"\n    print(\"OSA distance between '{}' and '{}' is: {}\".format(s1, s2, OSA(s1, s2)))"}
{"task_id": 52, "completion_id": 0, "completion": "import numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert lists to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP) and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Handle the case of division by zero\n    if TP + FN == 0:\n        return 0.0\n    \n    recall_value = TP / (TP + FN)\n    \n    # Round the result to three decimal places\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "completion": "import numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Compute queries, keys, and values\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Scale factor based on the depth of the queries\n    d_k = Q.shape[-1]\n    scaling_factor = np.sqrt(d_k)\n    \n    # Compute raw attention scores (dot product between queries and keys)\n    scores = np.dot(Q, K.T) / scaling_factor\n    \n    # Apply softmax to the attention scores in a numerically stable way\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    \n    # Compute the weighted sum of the values\n    attention_output = np.dot(softmax_scores, V)\n    \n    # Round results to 4 decimal places and convert to a python list\n    rounded_output = np.round(attention_output, 4)\n    \n    return rounded_output.tolist()"}
{"task_id": 54, "completion_id": 0, "completion": "import numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], \n                initial_hidden_state: list[float], \n                Wx: list[list[float]], \n                Wh: list[list[float]], \n                b: list[float]) -> list[float]:\n    \"\"\"\n    Processes a sequence of input vectors through a simple RNN cell and returns the final hidden state.\n    Each hidden state update is computed using the tanh activation function:\n    \n        h_t = tanh(Wx * x_t + Wh * h_{t-1} + b)\n    \n    All results are rounded to 4 decimal places.\n    \n    Parameters:\n    - input_sequence: A list of input vectors (each input vector is a list of floats).\n    - initial_hidden_state: The initial hidden state vector (a list of floats).\n    - Wx: The weight matrix for input-to-hidden connections.\n    - Wh: The weight matrix for hidden-to-hidden connections.\n    - b: The bias vector.\n    \n    Returns:\n    - A list of floats representing the final hidden state (rounded to 4 decimal places) after processing the entire sequence.\n    \"\"\"\n    \n    # Convert inputs to numpy arrays for vectorized operations.\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Process the sequence of inputs.\n    for x in input_sequence:\n        x = np.array(x)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    \n    # Round the final hidden state to 4 decimal places and convert to a Python list.\n    final_hidden_state = np.round(h, 4).tolist()\n    return final_hidden_state"}
{"task_id": 55, "completion_id": 0, "completion": "import numpy as np\ndef translate_object(points, tx, ty):\n    # Convert list of points to a numpy array\n    points_np = np.array(points)  # Shape: (n_points, 2)\n    \n    # Create a column of ones to convert points to homogeneous coordinates\n    ones = np.ones((points_np.shape[0], 1))\n    points_homogeneous = np.hstack([points_np, ones])  # Shape: (n_points, 3)\n    \n    # Define the 3x3 translation matrix for 2D translation\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Apply the translation matrix using matrix multiplication\n    # Transpose to multiply and then transpose back\n    transformed_homogeneous = (translation_matrix @ points_homogeneous.T).T  # Shape: (n_points, 3)\n    \n    # Extract the x and y coordinates from the homogeneous coordinates\n    transformed_points = transformed_homogeneous[:, :2]\n    \n    # Convert the numpy array back to a python list using tolist()\n    return transformed_points.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define an object as a list of [x, y] points\n    points = [[1, 2], [3, 4], [5, 6]]\n    \n    # Define translation distances in x and y directions\n    tx = 10\n    ty = 20\n    \n    # Apply the translation\n    translated_points = translate_object(points, tx, ty)\n    \n    print(\"Original points:\", points)\n    print(\"Translated points:\", translated_points)"}
{"task_id": 56, "completion_id": 0, "completion": "import numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence between two normal distributions:\n      P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2)\n\n    Parameters:\n      mu_p   : Mean of distribution P.\n      sigma_p: Standard deviation of distribution P.\n      mu_q   : Mean of distribution Q.\n      sigma_q: Standard deviation of distribution Q.\n\n    Returns:\n      The KL divergence D_KL(P || Q) as a float.\n      \n    The formula used is:\n      D_KL(P || Q) = log(sigma_q/sigma_p) + (sigma_p^2 + (mu_p - mu_q)**2) / (2*sigma_q**2) - 0.5\n    \"\"\"\n    # Calculate each part of the formula:\n    log_term = np.log(sigma_q / sigma_p)\n    var_ratio_term = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = log_term + var_ratio_term - 0.5\n    return kl_div\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define parameters for two normal distributions\n    mu_p, sigma_p = 0.0, 1.0  # Distribution P ~ N(0, 1^2)\n    mu_q, sigma_q = 1.0, 2.0  # Distribution Q ~ N(1, 2^2)\n    \n    # Compute KL divergence for P || Q\n    divergence = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(\"KL Divergence between P and Q:\", divergence)"}
{"task_id": 57, "completion_id": 0, "completion": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Convert inputs to numpy arrays of float type for consistency\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    \n    # Initialize x with the provided guess or zeros\n    if x_ini is not None:\n        x = np.array(x_ini, dtype=float)\n    else:\n        x = np.zeros(N, dtype=float)\n    \n    # Perform Gauss-Seidel iterations\n    for _ in range(n):\n        for i in range(N):\n            # Calculate the sum of A[i, j] * x[j] for j != i\n            # For j < i, we have already updated x in this iteration\n            # For j > i, use x from the previous iteration\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i+1:], x[i+1:])\n            # Update x[i]\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    \n    # Round the solution to 4 decimal places and return as a list\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "completion": "import numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert inputs to float numpy arrays to avoid integer division issues.\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = A.shape[0]\n\n    # Forward elimination with partial pivoting.\n    for i in range(n):\n        # Partial pivoting: find the row with the maximum element in current column (from i to n) \n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        # Swap the current row with the max_row in A and b\n        if max_row != i:\n            A[[i, max_row]] = A[[max_row, i]]\n            b[[i, max_row]] = b[[max_row, i]]\n        \n        # Check for zero pivot element (singular matrix)\n        if np.abs(A[i, i]) < 1e-12:\n            raise ValueError(\"Matrix is singular or nearly singular!\")\n        \n        # Eliminate the entries below the pivot.\n        for j in range(i+1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n    \n    # Back substitution to solve for x.\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        if np.abs(A[i, i]) < 1e-12:\n            raise ValueError(\"Matrix is singular or nearly singular during back substitution!\")\n        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n    \n    # Round the solution to 4 decimal places and convert to a list.\n    x = np.round(x, 4)\n    return x.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # System:\n    # 2x + 3y -  z = 5\n    # 4x + 4y - 3z = 3\n    # -2x + 3y + 2z = 4\n    A = [[2, 3, -1],\n         [4, 4, -3],\n         [-2, 3, 2]]\n    b = [5, 3, 4]\n    \n    solution = gaussian_elimination(A, b)\n    print(\"Solution:\", solution)"}
{"task_id": 59, "completion_id": 0, "completion": "Below is the complete Python implementation of the LSTM class as specified:\n\n------------------------------------------------------------\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights for forget, input, candidate cell, and output gates.\n        # The weight matrices multiply a concatenated vector of [previous_hidden_state; current_input]\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        # Biases are initialized to zero.\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n    \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns:\n          - A list of hidden states for each time step (each hidden state is a list after rounding).\n          - The final hidden state (rounded to 4 decimals and converted to a list).\n          - The final cell state (rounded to 4 decimals and converted to a list).\n\n        Parameters:\n          x: numpy array of shape (T, input_size) where T is number of time steps.\n          initial_hidden_state: numpy array of shape (hidden_size, 1)\n          initial_cell_state: numpy array of shape (hidden_size, 1)\n        \"\"\"\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []  # to store hidden state at every time step\n\n        # Process each time step\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)  # shape (input_size, 1)\n            # Concatenate the previous hidden state and the current input\n            concat = np.concatenate((h_prev, x_t), axis=0)  # shape (hidden_size + input_size, 1)\n\n            # Forget gate\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            # Input gate\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            # Candidate cell state\n            c_bar = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            # Update cell state\n            c_t = f_t * c_prev + i_t * c_bar\n            # Output gate\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            # Update hidden state\n            h_t = o_t * np.tanh(c_t)\n\n            # Round the hidden state to the nearest 4th decimal and append to list\n            hidden_states.append(np.round(h_t, 4).tolist())\n\n            # Update previous states for next iteration\n            h_prev = h_t\n            c_prev = c_t\n\n        # Return hidden states list along with final hidden and cell states rounded and converted to lists\n        final_hidden_state = np.round(h_prev, 4).tolist()\n        final_cell_state = np.round(c_prev, 4).tolist()\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage:\nif __name__ == '__main__':\n    input_size = 3\n    hidden_size = 4\n    T = 5  # number of time steps\n\n    # Create some random input sequence with T time steps.\n    x = np.random.randn(T, input_size)\n\n    # Initialize the hidden state and cell state to zeros.\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n\n    lstm = LSTM(input_size, hidden_size)\n    hidden_states, final_hidden, final_cell = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    print(\"Hidden States at each time step:\")\n    for i, h in enumerate(hidden_states):\n        print(f\"Time step {i+1}: {h}\")\n\n    print(\"\\nFinal Hidden State:\")\n    print(final_hidden)\n\n    print(\"\\nFinal Cell State:\")\n    print(final_cell)\n------------------------------------------------------------\n\nExplanation:\n1. The __init__ method sets up the weights and biases for four gates using random initialization (for weights) and zero initialization (for biases).\n2. The forward method iterates over each time step of input x. For each time step:\n   - The previous hidden and current input are concatenated.\n   - The forget gate, input gate, candidate cell state, and output gate are computed.\n   - The hidden and cell state are updated.\n3. All results are rounded to 4 decimal places using np.round and subsequently reshaped as lists via tolist().\n4. The example usage at the bottom demonstrates how to instantiate and run the LSTM."}
{"task_id": 60, "completion_id": 0, "completion": "Below is the complete Python code that implements the TF-IDF calculation as described:\n\n------------------------------------------------------------\nimport numpy as np\nfrom math import log\n\ndef compute_tf_idf(corpus, query):\n    # Check for empty corpus: if corpus is empty, return an empty list.\n    if not corpus:\n        return []\n    \n    # Number of documents in the corpus.\n    N = len(corpus)\n    \n    # Pre-calculate document frequency (df) for every query term.\n    # df is the number of documents that contain the term.\n    df = {}\n    for term in query:\n        count = 0\n        for doc in corpus:\n            if term in doc:\n                count += 1\n        df[term] = count\n    \n    # Compute IDF for each query term using smoothing.\n    # We use a smoothed idf: idf = log((N + 1) / (df(t) + 1)) + 1\n    idf = {}\n    for term in query:\n        # Even if a term does not appear in any doc (df = 0), smoothing ensures no division by zero.\n        idf[term] = log((N + 1) / (df[term] + 1)) + 1\n    \n    # Create an empty list to store tf-idf scores of each document.\n    tf_idf_scores = []\n    \n    # For each document in the corpus, calculate the TF-IDF for query terms.\n    for doc in corpus:\n        doc_result = []\n        # Total number of words in this document.\n        total_words = len(doc)\n        \n        # For each query term, calculate its term frequency (tf) and then tf-idf.\n        for term in query:\n            # If the document is empty, we set the tf as 0.\n            tf = 0.0\n            if total_words > 0:\n                tf = doc.count(term) / total_words\n            \n            # Compute the tf-idf by multiplying tf and the pre-calculated idf.\n            tf_idf = tf * idf[term]\n            \n            # Round to the nearest 4th decimal.\n            doc_result.append(round(tf_idf, 4))\n        \n        tf_idf_scores.append(doc_result)\n    \n    # Convert the results to a numpy array then back to a python list using tolist().\n    result_array = np.array(tf_idf_scores)\n    return result_array.tolist()\n\n# Example usage:\nif __name__ == '__main__':\n    corpus = [\n        ['data', 'science', 'machine', 'learning'],\n        ['machine', 'learning', 'deep', 'learning'],\n        [],\n        ['data', 'mining']\n    ]\n    query = ['data', 'learning', 'deep']\n    print(compute_tf_idf(corpus, query))\n------------------------------------------------------------\n\nExplanation:\n\n1. The function first checks whether the corpus is empty, returning an empty list if so.\n2. It calculates the document frequency (df) for each term in the query.\n3. Then, it uses a smoothed IDF formula: IDF(term) = log((N + 1) / (df(term) + 1)) + 1 \u2013 this ensures no division by zero.\n4. For every document in the corpus, it calculates term frequency (tf) for each query word as number_of_occurrences / total_words_in_doc. In case of empty documents, tf remains 0.\n5. Each tf is multiplied by its corresponding idf value and then rounded to 4 decimal places.\n6. Finally, the entire result is converted into a numpy array and then converted back to a Python list using the tolist() method, as required.\n\nThis implementation robustly handles edge cases including empty documents and query terms that may not appear in any document."}
{"task_id": 61, "completion_id": 0, "completion": "import numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays in case they are not\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate precision and recall, handling division by zero\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # Calculate F-score using the formula: F = (1+beta^2) * (precision * recall) / (beta^2 * precision + recall)\n    if precision == 0 and recall == 0:\n        fscore = 0.0\n    else:\n        fscore = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n        \n    # Return the fscore rounded to three decimal places\n    return round(fscore, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample test\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0])\n    y_pred = np.array([1, 0, 0, 1, 0, 1, 1])\n    \n    print(\"F1 Score:\", f_score(y_true, y_pred, beta=1))\n    print(\"F2 Score:\", f_score(y_true, y_pred, beta=2))"}
{"task_id": 62, "completion_id": 0, "completion": "Below is one complete implementation of a simple RNN using NumPy. In this code the network is initialized with small random weights and zero biases, the forward method processes the input sequence storing the hidden states and outputs, and the backward (BPTT) method computes gradients at every time step and updates the weights accordingly. You can modify how the inputs and expected outputs are passed (we assume here that they are lists of column\u2010vectors).\n\nBelow is the code:\n\n------------------------------------------------------------\n#!/usr/bin/env python3\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights (scaled by 0.01) and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        # Initialize weights\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        \n        # Initialize biases\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n    \n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        input_sequence: list of np.ndarray, each of shape (input_size, 1)\n\n        Returns:\n          outputs: list of np.ndarray, each of shape (output_size, 1) for each time step.\n          last_inputs: dictionary mapping time step to input (needed for BPTT).\n          last_hiddens: dictionary mapping time step to hidden state (needed for BPTT).\n        \"\"\"\n        # Initialize the hidden state as zero vector.\n        h_prev = np.zeros((self.hidden_size, 1))\n        \n        # We keep track of all inputs and hidden states for backprop\n        last_inputs = {}\n        last_hiddens = {}\n        outputs = []\n\n        # Process each time step in the input sequence.\n        for t, x in enumerate(input_sequence):\n            last_inputs[t] = x\n            # Compute hidden state: h = tanh(W_xh*x + W_hh*h_prev + b_h)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            last_hiddens[t] = h\n\n            # Compute the output: y = W_hy*h + b_y (no activation function is applied)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n\n            h_prev = h  # update the hidden state for next time step\n\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=1e-3):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the RNN weights.\n\n        input_sequence: list of np.ndarray, each with shape (input_size, 1)\n        expected_output: list of np.ndarray, each with shape (output_size, 1). Each entry is the expected output at that time step.\n        outputs: list of np.ndarray, output produced during forward pass.\n        last_inputs: dictionary from forward pass (maps t -> x).\n        last_hiddens: dictionary from forward pass (maps t -> h at time t).\n        learning_rate: learning rate for the update.\n\n        The loss function is defined as: Loss = (1/2)*sum_t[ || expected[t] - output[t] ||^2 ].\n        \"\"\"\n        # Initialize gradient matrices with zeros (same shape as the parameters)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        # This will store the gradient flowing into the hidden layer from the next time step.\n        dh_next = np.zeros((self.hidden_size, 1))\n        \n        # Number of time steps\n        T = len(input_sequence)\n        \n        # Backpropagate through time (iterate backwards)\n        for t in reversed(range(T)):\n            # Compute derivative of loss with respect to y at time t.\n            # Loss L = 1/2*(y - expected)**2 -> derivative = (y - expected)\n            dy = outputs[t] - expected_output[t]  # shape: (output_size, 1)\n            \n            # Gradients for weight and bias of the output layer.\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n\n            # Backpropagate into hidden state through output layer.\n            # Combine gradient from current output and from subsequent hidden state (dh_next).\n            dh = np.dot(self.W_hy.T, dy) + dh_next  # shape: (hidden_size, 1)\n\n            # Derivative through tanh non-linearity: (1 - h^2)\n            # Here h denotes the hidden state at time t.\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh  # elementwise multiplication\n\n            # Accumulate gradients for weights connecting input to hidden.\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            \n            # For the recurrent weights, we need the hidden state from the previous time step.\n            # For t = 0, we assume h_previous is a zero vector.\n            h_prev = np.zeros((self.hidden_size, 1)) if t == 0 else last_hiddens[t - 1]\n            dW_hh += np.dot(dh_raw, h_prev.T)\n            \n            # Accumulate gradients for the hidden bias.\n            db_h += dh_raw\n            \n            # Propagate gradient to the previous time step for the hidden state.\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Optionally, clip gradients here to mitigate exploding gradients.\n\n        # Update weights and biases using gradient descent.\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h  -= learning_rate * db_h\n        self.b_y  -= learning_rate * db_y\n\n        # For reporting, you can compute and return the loss.\n        loss = 0.0\n        for t in range(T):\n            # 1/2 * (expected - output)^2 for each time step.\n            loss += 0.5 * np.sum((expected_output[t] - outputs[t]) ** 2)\n        return loss\n\n\n# Example usage:\nif __name__ == '__main__':\n    # Define dimensions\n    input_size = 3\n    hidden_size = 5\n    output_size = 2\n\n    # Create an instance of SimpleRNN.\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n\n    # Create a dummy input sequence (list of column vectors) and expected outputs.\n    # Here we use 4 time steps.\n    T = 4\n    input_sequence = [np.random.randn(input_size, 1) for _ in range(T)]\n    expected_output = [np.random.randn(output_size, 1) for _ in range(T)]\n\n    # Forward pass.\n    outputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n    \n    # Compute the loss before the backward pass.\n    loss_before = 0.0\n    for t in range(T):\n        loss_before += 0.5 * np.sum((expected_output[t] - outputs[t])**2)\n    print(\"Loss before backward pass:\", loss_before)\n    \n    # Backward pass and update.\n    loss = rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=1e-2)\n    print(\"Loss computed during backward pass:\", loss)\n    \n    # Do another forward pass to see if loss has reduced.\n    outputs_new, _, _ = rnn.forward(input_sequence)\n    loss_after = 0.0\n    for t in range(T):\n        loss_after += 0.5 * np.sum((expected_output[t] - outputs_new[t])**2)\n    print(\"Loss after backward pass:\", loss_after)\n    \n\"\"\"\nExplanation:\n\n1. __init__: \n   - Initializes the weight matrices (W_xh, W_hh, and W_hy) using small random values.\n   - Biases (b_h and b_y) are initialized to zeros.\n\n2. forward:\n   - Iterates through each time step.\n   - Computes the hidden state using the tanh activation.\n   - Computes the output at that time step.\n   - Records and returns the outputs, the inputs, and the hidden states (needed for BPTT).\n\n3. backward:\n   - Runs backward through time from T-1 down to 0.\n   - Uses the derivative of the squared error loss (dy = output - expected) for each step.\n   - Computes gradients for the output weights and biases.\n   - Propagates the gradient into the hidden state (using the derivative of tanh, which is (1 - h^2)).\n   - Accumulates the gradients for the input-hidden (W_xh) and hidden-hidden (W_hh) transitions.\n   - Updates the weights and biases with gradient descent.\n   - Finally, computes and returns the aggregate loss over all time steps.\n   \nThis code demonstrates a barebones RNN with BPTT suitable for sequence prediction tasks. In practice, you might add gradient clipping, momentum, or use an optimizer from a deep learning library.\n"}
{"task_id": 63, "completion_id": 0, "completion": "import numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n    \n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to a list)\n    \"\"\"\n    # If no initial guess is provided, start with a zero vector\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x0.astype(float)\n    \n    # Initial residual: r = b - A*x\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = np.dot(A, p)\n        # Compute step size alpha\n        alpha = rs_old / np.dot(p, Ap)\n        # Update the solution\n        x = x + alpha * p\n        # Update the residual\n        r = r - alpha * Ap\n        \n        # Check for convergence\n        if np.linalg.norm(r) < tol:\n            break\n        \n        rs_new = np.dot(r, r)\n        # Compute the conjugate direction coefficient beta\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n    \n    # Round the solution to 8 decimal places and convert to list before returning\n    return np.around(x, decimals=8).tolist()"}
{"task_id": 64, "completion_id": 0, "completion": "import numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # If there are no samples, then the impurity is 0.\n    if len(y) == 0:\n        return 0.0\n    \n    # Count the occurrences of each class label.\n    classes, counts = np.unique(y, return_counts=True)\n    # Calculate probability of each class.\n    probabilities = counts / len(y)\n    # Calculate the Gini impurity.\n    impurity = 1 - np.sum(probabilities ** 2)\n    \n    # Round the result to three decimal places and return.\n    return round(impurity, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    labels = [1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n    print(\"Gini Impurity:\", gini_impurity(labels))"}
{"task_id": 65, "completion_id": 0, "completion": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    # List to store non-zero values.\n    values = []\n    # List to store corresponding column indices of these non-zero values.\n    col_indices = []\n    # Row pointer array, starts with 0.\n    row_ptr = [0]\n    \n    # Process each row in the dense matrix.\n    for row in dense_matrix:\n        # For each element in the current row, check if it's non-zero.\n        for j, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(j)\n        # Append next row pointer: previous pointer value + count of non-zero in current row.\n        row_ptr.append(len(values))\n    \n    return (values, col_indices, row_ptr)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # A simple example dense matrix:\n    dense = [\n        [0, 0, 3, 0],\n        [22, 0, 0, 0],\n        [7, 0, 5, 1],\n        [0, 0, 0, 0]\n    ]\n    \n    csr = compressed_row_sparse_matrix(dense)\n    print(\"Values:\", csr[0])\n    print(\"Column Indices:\", csr[1])\n    print(\"Row Pointer:\", csr[2])"}
{"task_id": 66, "completion_id": 0, "completion": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (a list of numbers)\n    :param L: The line vector defining the direction of projection (a list of numbers)\n    :return: List representing the projection of v onto L, with each component rounded to three decimals\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_vL = sum(vi * Li for vi, Li in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_LL = sum(Li * Li for Li in L)\n    \n    # If L is a zero vector, projection is undefined; handle by returning None or similar.\n    if dot_LL == 0:\n        raise ValueError(\"The line vector L must be non-zero.\")\n    \n    # Compute the projection coefficients: (v dot L) / (L dot L)\n    coeff = dot_vL / dot_LL\n    \n    # Compute the projection vector component-wise and round each component\n    proj = [round(coeff * Li, 3) for Li in L]\n    \n    return proj\n\n# Example Usage:\nif __name__ == \"__main__\":\n    v = [3, 4]\n    L = [1, 2]\n    projection = orthogonal_projection(v, L)\n    print(\"Projection of\", v, \"onto\", L, \"is:\", projection)"}
{"task_id": 67, "completion_id": 0, "completion": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Check if matrix is empty\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    \n    values = []        # Stores the non-zero values in column-major order\n    row_indices = []   # Stores the corresponding row indices for each non-zero value\n    col_pointer = [0]  # Start pointer for each column; first index is always 0\n\n    # Iterate over each column of the dense matrix\n    for col in range(num_cols):\n        # For each column, iterate over each row\n        for row in range(num_rows):\n            element = dense_matrix[row][col]\n            if element != 0:\n                values.append(element)\n                row_indices.append(row)\n        # Append the current length of values list as end of this column data\n        col_pointer.append(len(values))\n    \n    return (values, row_indices, col_pointer)\n\n# Example usage:\nif __name__ == \"__main__\":\n    dense = [\n        [0, 5, 0],\n        [1, 0, 0],\n        [0, 2, 3]\n    ]\n    vals, rows, col_ptr = compressed_col_sparse_matrix(dense)\n    print(\"Values:\", vals)           # Output: [1, 5, 2, 3]\n    print(\"Row Indices:\", rows)      # Output: [1, 0, 2, 2]\n    print(\"Column Pointer:\", col_ptr)  # Output: [0, 1, 3, 4]"}
{"task_id": 68, "completion_id": 0, "completion": "Below is one correct solution. In the code we use a simple Gaussian\u2010elimination approach to \u201cmark\u201d the pivot columns. The column space (or image) of A is given by the original columns corresponding to those pivot positions. Finally, the output matrix is rounded to 8 decimal places and converted to a list.\n\nBelow is the complete code:\n\n------------------------------------------------\nimport numpy as np\n\ndef matrix_image(A):\n    A = np.array(A, dtype=float)  # Ensure working with a numpy array of floats\n    m, n = A.shape\n    pivot_row = 0\n    pivot_columns = []\n    # Create a copy of A for elimination (we don't want to modify original A)\n    A_copy = A.copy()\n    \n    # Perform Gaussian elimination to identify pivot columns.\n    for col in range(n):\n        # Find the pivot row for current column:\n        pivot_found = None\n        for r in range(pivot_row, m):\n            if abs(A_copy[r, col]) > 1e-12:  # treat near-zero as zero\n                pivot_found = r\n                break\n        if pivot_found is None:\n            # No pivot in this column, move on to next column.\n            continue\n        \n        # Swap the current row with the pivot row (if necessary)\n        if pivot_found != pivot_row:\n            A_copy[[pivot_row, pivot_found]] = A_copy[[pivot_found, pivot_row]]\n        # Record the current column as a pivot column (independent column in original A)\n        pivot_columns.append(col)\n        \n        # Normalize the pivot row\n        pivot_val = A_copy[pivot_row, col]\n        A_copy[pivot_row] = A_copy[pivot_row] / pivot_val\n        \n        # Eliminate all entries below the pivot in the current column.\n        for r in range(pivot_row + 1, m):\n            factor = A_copy[r, col]\n            A_copy[r] = A_copy[r] - factor * A_copy[pivot_row]\n        \n        pivot_row += 1\n        if pivot_row == m:\n            break\n\n    # The basis for the image (column space) is given by the original matrix's\n    # columns corresponding to the pivot_columns.\n    basis = A[:, pivot_columns]\n   \n    # Round the answer to 8 decimal places and convert to list.\n    return np.round(basis, 8).tolist()\n\n\n# Example usage:\nif __name__ == '__main__':\n    A = [[1, 2, 3],\n         [4, 5, 6],\n         [7, 8, 9]]\n    basis = matrix_image(A)\n    print(\"Basis for the column space of A is:\")\n    print(basis)\n\n------------------------------------------------\n\nExplanation:\n\n1. We loop over each column of A_copy and look for a nonzero element (taking care of numerical precision by using a threshold).\n2. Once a pivot is found in a column, we swap rows so that the pivot is at the current pivot_row.\n3. We record that column index, normalize the pivot row, and then eliminate entries below it.\n4. Finally, we extract the columns from the original matrix A corresponding to the pivot indices. This set forms a basis for the image (column space) of A.\n5. The result is rounded to 8 decimal places and returned as a Python nested list."}
{"task_id": 69, "completion_id": 0, "completion": "import numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Convert input to numpy arrays for vectorized operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate total sum of squares (denomimator)\n    ss_total = np.sum((y_true - np.mean(y_true))**2)\n    \n    # Calculate residual sum of squares (numerator)\n    ss_res = np.sum((y_true - y_pred)**2)\n    \n    # Compute R-squared value\n    r2 = 1 - (ss_res / ss_total)\n    \n    # Return the value rounded to three decimal places\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "completion": "def calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n\n    # Check if the rows in the matrix have inconsistent lengths\n    # and validate pixel values for each row.\n    expected_row_length = len(img[0])\n    for row in img:\n        if len(row) != expected_row_length:\n            return -1\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                return -1\n\n    # Calculate total brightness and count the number of pixels.\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        num_pixels += len(row)\n        total_brightness += sum(row)\n\n    # Calculate average brightness.\n    average_brightness = total_brightness / num_pixels\n\n    # Return the average brightness rounded to two decimal places.\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "completion": "import numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual values (y_true) and predicted values (y_pred).\n    \n    Parameters:\n      y_true (array-like): Actual values.\n      y_pred (array-like): Predicted values.\n      \n    Returns:\n      float: The RMSE value rounded to three decimal places.\n      \n    Raises:\n      ValueError: If the input arrays have mismatched shapes, are empty, or cannot be converted to numpy arrays.\n    \"\"\"\n    # Attempt to convert inputs to numpy arrays\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n    except Exception as e:\n        raise ValueError(\"Input values must be array-like and convertible to a numpy array.\") from e\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    \n    # Check for shape mismatch\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate RMSE: sqrt(mean(square(difference)))\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    \n    # Return the RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test case 1: Valid inputs\n    true_values = [1, 2, 3, 4]\n    predicted_values = [1.1, 1.9, 3.2, 3.8]\n    print(\"RMSE:\", rmse(true_values, predicted_values))\n\n    # Test case 2: Mismatched shapes (should raise an error)\n    try:\n        print(rmse([1, 2, 3], [1, 2, 3, 4]))\n    except ValueError as e:\n        print(\"Error:\", e)\n\n    # Test case 3: Empty arrays (should raise an error)\n    try:\n        print(rmse([], []))\n    except ValueError as e:\n        print(\"Error:\", e)"}
{"task_id": 72, "completion_id": 0, "completion": "import numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Convert inputs to numpy arrays if they aren't already.\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Compute the intersection and union for binary arrays.\n    intersection = np.logical_and(y_true == 1, y_pred == 1).sum()\n    union = np.logical_or(y_true == 1, y_pred == 1).sum()\n    \n    # Handle the edge case where both arrays contain only zeros.\n    if union == 0:\n        return 1.000  # Define Jaccard Index as 1 when both are all zeros.\n    \n    # Calculate the Jaccard Index.\n    jaccard = intersection / union\n    # Return rounded value to 3 decimal places.\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "completion": "import numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true binary labels.\n    y_pred (np.ndarray): Array of predicted binary labels.\n    \n    Returns:\n    float: Dice score rounded to 3 decimal places.\n    \n    Note:\n    - If both y_true and y_pred have no positives (i.e., sums are zero), the function returns 1.0\n      since there is a perfect match (nothing predicted, nothing true).\n    \"\"\"\n    \n    # Ensure the inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Compute number of true positives (intersection)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Count total positives in y_true and y_pred\n    sum_true = np.sum(y_true == 1)\n    sum_pred = np.sum(y_pred == 1)\n    \n    # Edge case: if both are 0, return perfect score.\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    \n    # Compute Dice Score using the formula\n    dice = 2 * intersection / (sum_true + sum_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "completion": "Below is one complete solution. In this example we \u201cbind\u201d the representations for each feature by element\u2010wise multiplication of two bipolar hypervectors\u2013one for the feature name and one for the feature value\u2013and then \u201cbundle\u201d (i.e. add) these bound vectors across features. (You can later choose to \u201cclean up\u201d the row\u2013level hypervector by taking its sign, which is a typical majority\u2010vote binarization in HDC.)\n\nTo keep reproducibility we use the provided random seed for each feature. For the value hypervector we want different (but reproducible) random numbers than for the feature name. One simple approach is to compute a small integer offset from the feature\u2019s value (first converting it to a string and then computing a hash) and then use a seed that is the sum of the feature\u2019s base seed and that offset. (Of course, many different design choices are possible.) \n\nBelow is the implementation:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a dataset row using Hyperdimensional Computing.\n    \n    For each feature in the row:\n      1. Generate a bipolar hypervector for the feature name using the provided random seed.\n      2. Generate a bipolar hypervector for the feature value using a modified seed based on the feature seed \n         and the value (converted to string and hashed), so that the same value always leads to the same hypervector.\n      3. Bind them using element-wise multiplication.\n    \n    Then, bundle (i.e., sum) all bound hypervectors across features.\n    \n    Optionally, you may binarize the bundled vector by taking the sign (i.e. majority vote) so that each dimension is +1 or -1.\n    \n    Args:\n      row (dict): Dictionary representing a row with feature key and value.\n      dim (int): Dimensionality of the hypervectors.\n      random_seeds (dict): Dictionary with feature names as keys and integer seeds as values.\n    \n    Returns: \n      list: The composite hypervector (as a list) representing the row.\n    \"\"\"\n    bound_vectors = []\n    \n    for feature, value in row.items():\n        # Get the seed for the feature. (Assuming the random_seeds has an entry for every feature.)\n        base_seed = random_seeds[feature]\n        \n        # Generate feature name hypervector (bipolar representation: values in {-1, 1})\n        rs_name = np.random.RandomState(base_seed)\n        name_hv = rs_name.choice([-1, 1], size=dim)\n        \n        # For the feature value, derive a seed based on the feature seed and the feature value.\n        # We convert the value to a string, then use its hash.\n        value_str = str(value)\n        # The abs(hash(...)) ensures a nonnegative offset\n        offset = abs(hash(value_str)) % 10000  # mod to keep the offset in a reasonable range\n        value_seed = base_seed + offset\n        \n        rs_value = np.random.RandomState(value_seed)\n        value_hv = rs_value.choice([-1, 1], size=dim)\n        \n        # Bind the two hypervectors via element-wise multiplication (which, for bipolar vectors, is a common binding operation)\n        bound = name_hv * value_hv\n        bound_vectors.append(bound)\n    \n    # Bundle all bound hypervectors by summation.\n    composite = np.sum(bound_vectors, axis=0)\n    \n    # Optional: Binarize the bundled hypervector using sign (majority vote).\n    # Here we use np.where to treat zeros as +1.\n    composite_bipolar = np.where(composite >= 0, 1, -1)\n    \n    # Return the composite hypervector as a list (using tolist())\n    return composite_bipolar.tolist()\n\n# Example usage:\nif __name__ == '__main__':\n    # Example data row\n    row = {\n        \"color\": \"red\",\n        \"size\": 5,\n        \"shape\": \"circle\"\n    }\n    \n    # Dimensionality of hypervectors\n    dim = 10000\n    \n    # Random seeds for each feature (ensuring reproducibility)\n    random_seeds = {\n        \"color\": 42,\n        \"size\": 123,\n        \"shape\": 7\n    }\n    \n    hv = create_row_hv(row, dim, random_seeds)\n    print(\"Composite Hypervector (first 20 dims):\", hv[:20])\n    \n# End of code.\n------------------------------------------------------------\n\nExplanation:\n\n1. For each feature we generate two random bipolar hypervectors: one for the feature name and one for the feature value. To ensure that the value HV is reproducible and different from the name HV, we add an offset (derived from the hashed string version of the value) to the feature\u2019s random seed.\n\n2. We bind these two hypervectors using elementwise multiplication.\n\n3. After processing all features, we bundle (sum) the resulting bound hypervectors.\n\n4. We then binarize the sum by taking the sign (with zeros chosen as +1) and finally convert the result to a list with tolist().\n\nThis solution meets the requirements of generating a composite hypervector using HDC."}
{"task_id": 75, "completion_id": 0, "completion": "from collections import Counter\ndef confusion_matrix(data):\n    # Define counters for the four outcomes\n    # True Negatives (TN), False Positives (FP), False Negatives (FN), True Positives (TP)\n    counts = Counter()\n    \n    # Iterate over each observation in the data\n    for y_true, y_pred in data:\n        if y_true == 0 and y_pred == 0:\n            counts['TN'] += 1\n        elif y_true == 0 and y_pred == 1:\n            counts['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            counts['FN'] += 1\n        elif y_true == 1 and y_pred == 1:\n            counts['TP'] += 1\n            \n    # Construct the confusion matrix\n    # Matrix layout:\n    #          Predicted\n    # Actual   0       1\n    #   0    [TN,   FP]\n    #   1    [FN,   TP]\n    return [\n        [counts.get('TN', 0), counts.get('FP', 0)],\n        [counts.get('FN', 0), counts.get('TP', 0)]\n    ]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample input: list of observations [y_true, y_pred]\n    data = [\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n        [0, 0],\n        [1, 1]\n    ]\n    matrix = confusion_matrix(data)\n    print(\"Confusion Matrix:\")\n    for row in matrix:\n        print(row)"}
{"task_id": 76, "completion_id": 0, "completion": "import numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Check if both vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both input vectors must have the same shape.\")\n    \n    # Check if vectors are empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    \n    # Calculate the norm (magnitude) of each vector\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Check if either vector has zero magnitude\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Compute the cosine similarity\n    similarity = np.dot(v1, v2) / (norm_v1 * norm_v2)\n    \n    # Return the similarity rounded to three decimal places\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "completion": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n\n    # Initialize counts for True Positives, False Positives, True Negatives, and False Negatives.\n    TP = FP = TN = FN = 0\n\n    # Loop through actual and predicted values and count outcomes.\n    for act, pred in zip(actual, predicted):\n        if act not in (0, 1) or pred not in (0, 1):\n            raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n        if act == 1 and pred == 1:\n            TP += 1\n        elif act == 0 and pred == 1:\n            FP += 1\n        elif act == 1 and pred == 0:\n            FN += 1\n        elif act == 0 and pred == 0:\n            TN += 1\n\n    # Build confusion matrix in the format:\n    # [[True Negative, False Positive],\n    #  [False Negative, True Positive]]\n    confusion_matrix = [[TN, FP],\n                        [FN, TP]]\n\n    total = TP + TN + FP + FN\n\n    # Calculate Accuracy\n    accuracy = (TP + TN) / total if total != 0 else 0\n\n    # Calculate F1 Score. \n    # F1 = 2*TP / (2*TP + FP + FN)\n    if (2 * TP + FP + FN) == 0:\n        f1_score = 0\n    else:\n        f1_score = (2 * TP) / (2 * TP + FP + FN)\n\n    # Calculate Specificity = TN / (TN + FP)\n    if (TN + FP) == 0:\n        specificity = 0\n    else:\n        specificity = TN / (TN + FP)\n\n    # Calculate Negative Predictive Value = TN / (TN + FN)\n    if (TN + FN) == 0:\n        negative_predictive_value = 0\n    else:\n        negative_predictive_value = TN / (TN + FN)\n\n    # Round the computed metrics to three decimal places.\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\nif __name__ == \"__main__\":\n    actual_labels = [0, 1, 0, 1, 1, 0, 1]\n    predicted_labels = [0, 1, 0, 0, 1, 0, 1]\n    results = performance_metrics(actual_labels, predicted_labels)\n    print(\"Confusion Matrix:\", results[0])\n    print(\"Accuracy:\", results[1])\n    print(\"F1 Score:\", results[2])\n    print(\"Specificity:\", results[3])\n    print(\"Negative Predictive Value:\", results[4])"}
{"task_id": 78, "completion_id": 0, "completion": "Below is one correct solution:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef descriptive_statistics(data):\n    # Convert input to NumPy array for efficient computation\n    data = np.array(data)\n    \n    # Calculate mean and median\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    # Calculate mode: in case of multiple values with the same highest frequency,\n    # we choose the smallest one.\n    unique_vals, counts = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    modes = unique_vals[counts == max_count]\n    mode_val = np.min(modes)\n    \n    # Compute variance and standard deviation (population statistics, ddof=0)\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    \n    # Compute percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)  # this is equivalent to median\n    percentile_75 = np.percentile(data, 75)\n    \n    # Compute Interquartile Range (IQR)\n    interquartile_range = percentile_75 - percentile_25\n\n    # Build the results dictionary.\n    # Except for statistics that are directly drawn from the data,\n    # all aggregated statistics are rounded to four decimal places.\n    # (Here we round every computed metric.)\n    results = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(float(mode_val), 4),\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_data = [1, 2, 2, 3, 4, 5, 5, 5, 6]\n    stats = descriptive_statistics(sample_data)\n    print(stats)\n------------------------------------------------------------\n\nExplanation:\n1. The function first converts the input data to a NumPy array.\n2. It calculates the mean, median, variance, and standard deviation.\n3. For the mode, the function finds unique values and their counts. In cases of multiple modes (ties), it returns the smallest value.\n4. It computes the 25th, 50th, and 75th percentiles, then calculates the interquartile range (IQR).\n5. Finally, it rounds the relevant aggregated statistics to four decimal places as required and returns them in a dictionary."}
{"task_id": 79, "completion_id": 0, "completion": "import math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials (rounded to five decimal places)\n    \"\"\"\n    # Calculate the binomial coefficient: n choose k\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability components\n    success_prob = p ** k\n    failure_prob = (1 - p) ** (n - k)\n    # Binomial probability using the formula: C(n, k) * p^k * (1-p)^(n-k)\n    probability = binom_coeff * success_prob * failure_prob\n    # Return probability rounded to five decimal places\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "completion": "import math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(exponent)\n    return round(pdf_value, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 0\n    mean = 0\n    std_dev = 1\n    print(\"PDF at x =\", x, \":\", normal_pdf(x, mean, std_dev))"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    # Calculate the probability using the Poisson formula:\n    # P(k; lam) = (lam^k * exp(-lam)) / k!\n    probability = (lam ** k * math.exp(-lam)) / math.factorial(k)\n    return round(probability, 5)\n    \n# Example usage:\nif __name__ == \"__main__\":\n    # Example: probability of observing exactly 3 events when the average rate is 2.5\n    k_value = 3\n    lam_value = 2.5\n    result = poisson_probability(k_value, lam_value)\n    print(\"P(3 events | lam=2.5):\", result)\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image, computed as the difference between the \n               maximum and minimum pixel values.\n    \"\"\"\n    # Validate input type\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"The input image must be a numpy.ndarray.\")\n\n    # Validate that the input is a 2D array\n    if img.ndim != 2:\n        raise ValueError(\"The input image must be a 2D array representing a grayscale image.\")\n    \n    # Calculate contrast as the difference between the maximum and minimum pixel values\n    contrast = np.max(img) - np.min(img)\n    \n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create an example grayscale image (2D numpy array)\n    example_img = np.array([[100, 150, 200],\n                            [50, 125, 175],\n                            [25, 75, 225]])\n\n    contrast_value = calculate_contrast(example_img)\n    print(\"Contrast of the image:\", contrast_value)\n```"}
{"task_id": 83, "completion_id": 0, "completion": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    # Optionally, you can add a check to ensure both vectors are 1D and of the same length.\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both vec1 and vec2 must be 1D arrays.\")\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Both vectors must be of the same length.\")\n\n    return np.dot(vec1, vec2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n    result = calculate_dot_product(v1, v2)\n    print(\"Dot product:\", result)"}
{"task_id": 84, "completion_id": 0, "completion": "import numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features \n                           of the corresponding data point, each rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    transformed = []\n    # For each data point, compute [x^0, x^1, ..., x^degree]\n    for x in data:\n        features = []\n        for power in range(degree + 1):\n            features.append(round(x**power, 8))\n        transformed.append(features)\n    \n    return transformed\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1.0, 2.0, 3.0]\n    degree = 3\n    print(phi_transform(data, degree))\n    # Expected output:\n    # [\n    #  [1.0, 1.0, 1.0, 1.0],\n    #  [1.0, 2.0, 4.0, 8.0],\n    #  [1.0, 3.0, 9.0, 27.0]\n    # ]"}
{"task_id": 85, "completion_id": 0, "completion": "import numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    # Check for invalid inputs\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create an array of positions (shape: [position, 1])\n    positions = np.arange(position).reshape(-1, 1)\n    \n    # Create an array of dimensions (shape: [1, d_model])\n    dims = np.arange(d_model).reshape(1, -1)\n    \n    # Compute the angle rates: using the formula 1 / (10000^(2 * (i//2) / d_model))\n    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / np.float32(d_model))\n    \n    # Calculate the angles by multiplying positions and angle_rates (broadcast shapes)\n    angle_rads = positions * angle_rates\n    \n    # Initialize the positional encoding matrix\n    pos_encoding = np.empty_like(angle_rads)\n    \n    # Apply sine to even indices in the array; cosine to odd indices.\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    # Convert the numpy array to np.float16 and then to a list before returning.\n    return pos_encoding.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "completion": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, 1 if overfitting, -1 if underfitting, or 0 if a good fit.\n    \"\"\"\n    # Check for overfitting: training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n\n    # Check for underfitting: both training and test accuracies are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n\n    # If neither condition is met, the model has a good fit.\n    return 0\n\n# Example tests:\nif __name__ == \"__main__\":\n    # Overfitting\n    print(model_fit_quality(0.95, 0.70))  # Expected output: 1 (overfitting)\n    \n    # Underfitting\n    print(model_fit_quality(0.65, 0.60))  # Expected output: -1 (underfitting)\n    \n    # Good fit\n    print(model_fit_quality(0.85, 0.80))  # Expected output: 0 (good fit)"}
{"task_id": 87, "completion_id": 0, "completion": "import numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value (scalar or numpy array)\n    :param grad: Current gradient (scalar or numpy array)\n    :param m: First moment estimate (scalar or numpy array)\n    :param v: Second moment estimate (scalar or numpy array)\n    :param t: Current timestep (int)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v),\n             where each element is rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already (this allows scalar and array inputs)\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    \n    # Update biased first moment estimate:\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate:\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate:\n    m_hat = m / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate:\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters:\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round each element to 5 decimal places and convert to list.\n    updated_parameter = np.around(updated_parameter, decimals=5).tolist()\n    m = np.around(m, decimals=5).tolist()\n    v = np.around(v, decimals=5).tolist()\n    \n    return updated_parameter, m, v\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Scalar example\n    param, grad, m_init, v_init, t = 1.0, 0.1, 0.0, 0.0, 1\n    updated_param, updated_m, updated_v = adam_optimizer(param, grad, m_init, v_init, t)\n    print(\"Updated scalar parameter:\", updated_param)\n    print(\"Updated m:\", updated_m)\n    print(\"Updated v:\", updated_v)\n\n    # Array example\n    param_arr = np.array([1.0, 2.0])\n    grad_arr = np.array([0.1, 0.2])\n    m_init_arr = np.array([0.0, 0.0])\n    v_init_arr = np.array([0.0, 0.0])\n    t = 1\n    updated_param_arr, updated_m_arr, updated_v_arr = adam_optimizer(param_arr, grad_arr, m_init_arr, v_init_arr, t)\n    print(\"Updated array parameter:\", updated_param_arr)\n    print(\"Updated m (array):\", updated_m_arr)\n    print(\"Updated v (array):\", updated_v_arr)"}
{"task_id": 88, "completion_id": 0, "completion": "Below is one possible solution that builds a simplified GPT\u20112\u2013like text generator using token and positional embeddings, a dummy \u201cattention\u201d mechanism, a feed\u2011forward layer, and layer normalization. In this example the multi\u2011head attention and feed\u2011forward network are very simplified; we simulate \u201cattention\u201d by averaging over the context embeddings and then pass that result through a tanh nonlinearity before normalizing and converting it to logits. Finally we sample the next token from the softmax distribution over the dummy vocabulary.\n\nNote that this is not a full implementation of GPT\u20112 but only a minimal illustration to help understand the components behind autoregressive generation.\n\n--------------------------------------------------\n#!/usr/bin/env python3\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    # Dummy Byte Pair Encoder (BPE) implementation\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n    \n    # Model hyperparameters (for this simplified version)\n    hparams = {\n        \"n_ctx\": 1024,  # maximum context length\n        \"n_head\": 12,   # number of attention heads (not used in depth here)\n    }\n    \n    # Dummy model parameters:\n    # wte: token embedding table: shape (vocab_size=3, embed_dim=10)\n    # wpe: positional embedding: shape (n_ctx, embed_dim)\n    # ln_f: final layer norm parameters (scale and bias)\n    params = {\n        \"wte\": np.random.rand(3, 10),         # token embeddings (3 tokens in vocabulary)\n        \"wpe\": np.random.rand(1024, 10),        # positional embeddings for each position\n        \"blocks\": [],                         # empty list for blocks (not used in our simple version)\n        \"ln_f\": {\n            \"g\": np.ones(10),                  # layer norm gain vector\n            \"b\": np.zeros(10),                 # layer norm bias vector\n        }\n    }\n    \n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    \n    # Load the dummy encoder, hyperparameters, and parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt into token ids\n    tokens = encoder.encode(prompt)\n    \n    # For each new token to generate\n    for i in range(n_tokens_to_generate):\n        # Guard: do not exceed the maximum context length\n        if len(tokens) >= hparams[\"n_ctx\"]:\n            break\n\n        # Build the context embeddings for the current sequence\n        context_embeddings = []\n        for pos, token_id in enumerate(tokens):\n            # Token embedding from \"wte\"\n            token_emb = params[\"wte\"][token_id]         # shape (10,)\n            # Positional embedding corresponding to this position\n            pos_emb = params[\"wpe\"][pos]                 # shape (10,)\n            # Combine (sum) token and positional embeddings\n            context_embeddings.append(token_emb + pos_emb)\n        context = np.stack(context_embeddings, axis=0)   # shape (seq_len, 10)\n        \n        # --- Simulated Multi-head Attention ---\n        # For our simplified version, we \u201cattend\u201d all positions equally by taking the average.\n        attention_output = context.mean(axis=0)          # shape (10,)\n        \n        # --- Simulated Feed-Forward Network ---\n        # Apply a simple non-linearity (tanh) as a stand-in for a more complex feed\u2011forward network.\n        ff_output = np.tanh(attention_output)            # shape (10,)\n        \n        # --- Layer Normalization ---\n        epsilon = 1e-5\n        mean_val = ff_output.mean()\n        std_val = ff_output.std() + epsilon\n        norm_output = (ff_output - mean_val) / std_val\n        # Scale and shift with ln_f parameters\n        norm_output = norm_output * params[\"ln_f\"][\"g\"] + params[\"ln_f\"][\"b\"]\n        \n        # --- Compute Next Token Probabilities ---\n        # Use the normalized output as an implicit \u201clogit\u201d vector.\n        # We compute the dot product with the token embeddings table's transpose to produce a score for each token.\n        logits = norm_output.dot(params[\"wte\"].T)   # shape (vocab_size,)\n        \n        # Apply softmax to convert logits to probabilities.\n        exp_logits = np.exp(logits - np.max(logits))\n        probs = exp_logits / exp_logits.sum()\n        \n        # Sample a token from the probability distribution\n        next_token = np.random.choice(len(probs), p=probs)\n        tokens.append(int(next_token))\n    \n    # Decode the generated tokens to text and return the result\n    generated_text = encoder.decode(tokens)\n    return generated_text\n\n# Example usage:\nif __name__ == \"__main__\":\n    prompt_text = \"hello world\"\n    output = gen_text(prompt_text, n_tokens_to_generate=20)\n    print(\"Generated Text:\")\n    print(output)\n\n--------------------------------------------------\n\nExplanation:\n1. load_encoder_hparams_and_params sets up a dummy encoder along with two embedding tables: \n   \u2022 wte: maps token IDs to 10-dimensional vectors.\n   \u2022 wpe: provides a positional embedding for every possible position up to 1024.\n2. In gen_text, the prompt is first encoded into a list of token IDs.\n3. For each token generation step:\n   \u2022 The embeddings for the entire current sequence are constructed.\n   \u2022 A very simple \u201cattention\u201d function is simulated by averaging these embeddings.\n   \u2022 The result is passed through a feed-forward transformation (tanh) followed by layer normalization.\n   \u2022 The normalized output is used (via a dot product with the token embeddings) to build logits.\n   \u2022 A softmax produces token probabilities from which the next token is sampled.\n4. Finally, the list of token IDs is decoded back into text.\n\nThis code thus illustrates a minimal autoregressive text generation process with key GPT\u20112\u2013inspired components."}
{"task_id": 89, "completion_id": 0, "completion": "We can solve the problem by \u201csimulating\u201d a self\u2010attention mechanism on a list of crystal values. In this simplified version we will treat each crystal\u2019s value as its \u201cembedding\u201d (i.e. used for query, key, and value) and we form the attention scores using the dot\u2010product between every pair of crystal values. In order to do so we will:\n\n1. Define a softmax helper function (with a stability trick);\n2. Compute the dot\u2010product relationship (i.e. score) between every crystal and every other crystal. In normal self\u2010attention, you also scale by the square root of the embedding dimension; here that dimension is given by the parameter \u201cdimension\u201d so we scale the dot product by 1/\u221a(dimension);\n3. For each crystal i, compute the softmax over its \u201cscore vector\u201d (the list of scores between crystal i and each crystal j);\n4. Calculate the weighted sum of the crystal values (using the attention weights) to give the enhanced pattern value for crystal i;\n5. Round each result to 4 decimal places and return the list.\n\nBelow is the complete Python code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Use a stability trick by subtracting the max value\n        shifted = values - np.max(values)\n        exps = np.exp(shifted)\n        return exps / np.sum(exps)\n    \n    # Convert the list to a numpy array of floats\n    crystals = np.array(crystal_values, dtype=float)\n    \n    # Compute the relationship scores between every pair of crystals.\n    # Here we use the dot product (crystal_i * crystal_j)\n    # and scale by the square root of the dimension.\n    # This represents (query \u00b7 key) / sqrt(dimension) in self-attention.\n    scores = np.outer(crystals, crystals) / np.sqrt(dimension)\n    \n    # For each crystal, compute the weighted sum of the crystal values\n    # using the attention weights from the softmax of its score row.\n    result = []\n    for i in range(n):\n        attention_weights = softmax(scores[i])\n        weighted_pattern = np.dot(attention_weights, crystals)\n        result.append(round(weighted_pattern, 4))\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Suppose we have 4 crystals with the following values and use dimension 3\n    n = 4\n    crystal_values = [1.5, 2.0, 3.0, 4.0]\n    dimension = 3\n    patterns = pattern_weaver(n, crystal_values, dimension)\n    print(patterns)\n\n------------------------------------------------------------\n\nExplanation:\n\n\u2022 We first define the softmax function so that for any list (or np.array) of scores it produces a normalized probability distribution.\n\n\u2022 The outer product (np.outer) creates a matrix where the (i, j)th entry is the product of crystal_values[i] and crystal_values[j]. This stands in for the \u201cquery-key\u201d interaction. We then scale by \u221a(dimension).\n\n\u2022 For each crystal i, we take the i\u2013th row of the score matrix, compute its softmax (attention distribution), and then use these weights to form a weighted sum of all crystal values. This sum is the \u201cenhanced pattern\u201d for crystal i.\n\n\u2022 Finally, we round each value to the fourth decimal place as required and return the list of weighted patterns.\n\nFeel free to adjust or experiment with the computation as needed."}
{"task_id": 90, "completion_id": 0, "completion": "Below is one possible solution:\n\n------------------------------------------------------------\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus given the query.\n    \n    Parameters:\n      corpus: list of documents (each document is a string).\n      query: string with query words.\n      k1: tuning parameter for term frequency scaling.\n      b: tuning parameter for document length normalization.\n      \n    Returns:\n      A list of BM25 scores (rounded to three decimals) for each document in the corpus.\n    \"\"\"\n    \n    # Tokenize documents: here we simply use split() assuming words separated by whitespace.\n    tokenized_corpus = [doc.split() for doc in corpus]\n    N = len(tokenized_corpus)\n    \n    # Compute the average document length\n    doc_lengths = [len(doc) for doc in tokenized_corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    \n    # Tokenize the query\n    query_terms = query.split()\n    \n    # Precompute document frequency for each query term (number of docs that contain the term)\n    doc_freq = {}\n    for term in query_terms:\n        count = sum(1 for doc in tokenized_corpus if term in set(doc))\n        doc_freq[term] = count\n    \n    # Function to calculate BM25 term weight for a given term in a document\n    def bm25_term_score(term, freq, doc_len, df):\n        # BM25 idf component\n        idf = np.log((N - df + 0.5) / (df + 0.5) + 1)\n        # BM25 frequency component\n        score = idf * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * (doc_len / avg_doc_length)))\n        return score\n    \n    scores = []\n    # Calculate BM25 score for each document in the corpus\n    for doc in tokenized_corpus:\n        score = 0.0\n        doc_len = len(doc)\n        term_freqs = Counter(doc)\n        for term in query_terms:\n            freq = term_freqs.get(term, 0)\n            # Only contribute to the score if the term is present (or consider it 0)\n            if freq > 0:\n                score += bm25_term_score(term, freq, doc_len, doc_freq[term])\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage:\nif __name__ == \"__main__\":\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog barked at the cat\",\n        \"the quick brown fox jumped over the lazy dog\"\n    ]\n    query = \"cat dog\"\n    print(calculate_bm25_scores(corpus, query))\n------------------------------------------------------------\n\nExplanation:\n\n1. The code begins by tokenizing each document (assuming space-separated words).\n2. It then computes the average document length, which is used for normalization.\n3. It calculates the document frequency (df) of each query term.\n4. For each document, it computes the BM25 score by summing over each query term's contribution:\n   \u2022 The term\u2019s idf is calculated using the BM25 idf formula.\n   \u2022 The term's frequency in the document is adjusted using the BM25 frequency formula.\n5. Scores are then rounded to three decimal places and returned.\n\nThis implementation adheres to the BM25 ranking function as described."}
{"task_id": 91, "completion_id": 0, "completion": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    \n    # Calculate true positives, false positives, false negatives\n    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n    predicted_positives = sum(1 for pred in y_pred if pred == 1)\n    actual_positives = sum(1 for true in y_true if true == 1)\n    \n    # Calculate precision\n    if predicted_positives == 0:\n        precision = 0.0\n    else:\n        precision = tp / predicted_positives\n\n    # Calculate recall\n    if actual_positives == 0:\n        recall = 0.0\n    else:\n        recall = tp / actual_positives\n\n    # Calculate F1 Score\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    \n    return round(f1, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test case\n    y_true = [1, 0, 1, 1, 0, 1, 0]\n    y_pred = [1, 0, 1, 0, 0, 1, 1]\n    print(\"F1 Score:\", calculate_f1_score(y_true, y_pred))\n    \n# This function computes the F1 score, which is the harmonic mean of precision and recall,\n# and returns the result rounded to three decimal places."}
{"task_id": 92, "completion_id": 0, "completion": "Below is one acceptable implementation:\n\n--------------------------------------------------\n#!/usr/bin/env python3\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Number of data points (days 1 to 10)\n    n = len(consumption_data)\n    \n    # 1) Remove the known fluctuation from each data point.\n    # The fluctuation is defined as: 10 * sin(2\u03c0 * day / 10)\n    detrended = []\n    for i in range(1, n+1):\n        # Calculate fluctuation for day i\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        # Remove the fluctuation from the measured consumption\n        base_usage = consumption_data[i-1] - fluctuation\n        detrended.append(base_usage)\n\n    # 2) Perform linear regression on the detrended data.\n    # We assume a model: y = a * x + b, where x is day number (1..10)\n    # Calculate sums needed for the regression formulas.\n    sum_x = 0\n    sum_y = 0\n    sum_xy = 0\n    sum_x2 = 0\n    for i in range(1, n+1):\n        x = i\n        y = detrended[i-1]\n        sum_x += x\n        sum_y += y\n        sum_xy += x * y\n        sum_x2 += x * x\n\n    # using the formulas:\n    # slope a = (n * \u03a3(xy) - \u03a3x * \u03a3y) / (n * \u03a3(x^2) - (\u03a3x)^2)\n    # intercept b = (\u03a3y - a * \u03a3x) / n\n    a = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n    b = (sum_y - a * sum_x) / n\n\n    # 3) Predict day 15's base consumption using the linear model.\n    day_predict = 15\n    predicted_base = a * day_predict + b\n\n    # 4) Add back the known fluctuation on day 15.\n    fluctuation_day15 = 10 * math.sin(2 * PI * day_predict / 10)\n    predicted_total = predicted_base + fluctuation_day15\n\n    # 5) Include a 5% safety margin.\n    # Multiply the prediction by 1.05 then round up to the nearest integer.\n    safe_prediction = math.ceil(predicted_total * 1.05)\n\n    # 6) Return the final integer forecast.\n    return safe_prediction\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Suppose these are the consumption measurements for days 1 through 10.\n    consumption = [100, 110, 120, 130, 140, 150, 160, 170, 180, 190]\n    forecast = power_grid_forecast(consumption)\n    print(\"Forecast for day 15 (with safety margin):\", forecast)\n--------------------------------------------------\n\nExplanation:\n\n1. We first remove the periodic fluctuation from each daily reading by subtracting 10 * sin(2\u03c0 * i/10).\n\n2. We perform linear regression on the detrended data points using the standard formulas for the slope (a) and intercept (b).\n\n3. We use the regression model to predict day 15\u2019s base consumption.\n\n4. We then add back the fluctuation (10 * sin(2\u03c0 * 15/10)) for day 15.\n\n5. Finally, we apply a 5% safety margin by multiplying by 1.05 and then taking the ceiling (rounding up) to ensure an integer value that meets the colony\u2019s requirements.\n\nThis code meets the task requirements."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute errors between predictions and true values.\n    abs_errors = np.abs(y_true - y_pred)\n    \n    # Compute the mean of the absolute errors.\n    mean_abs_error = np.mean(abs_errors)\n    \n    # Return the result rounded to 3 decimal places.\n    return round(mean_abs_error, 3)\n  \n# Example usage:\nif __name__ == \"__main__\":\n    # Example arrays of true and predicted values.\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    \n    # Calculate MAE.\n    result = mae(y_true, y_pred)\n    print(\"Mean Absolute Error:\", result)\n```"}
{"task_id": 94, "completion_id": 0, "completion": "Below is one complete solution implementation:\n\n------------------------------------------------------------\n#!/usr/bin/env python3\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute and return the Query (Q), Key (K) and Value (V) matrices.\n    \n    Arguments:\n      X     : (seq_len, d_model) input matrix.\n      W_q   : (d_model, d_model) query projection matrix.\n      W_k   : (d_model, d_model) key projection matrix.\n      W_v   : (d_model, d_model) value projection matrix.\n    \n    Returns:\n      A tuple (Q, K, V) each of shape (seq_len, d_model).  \n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    # rounding the results to nearest 4th decimal as intermediate result\n    Q = np.around(Q, decimals=4)\n    K = np.around(K, decimals=4)\n    V = np.around(V, decimals=4)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray):\n    \"\"\"\n    Compute self-attention using scaled dot-product attention.\n    \n    Arguments:\n      Q: (seq_len, head_dim) Query matrix.\n      K: (seq_len, head_dim) Key matrix.\n      V: (seq_len, head_dim) Value matrix.\n    \n    Returns:\n      Attention output: (seq_len, head_dim) matrix.\n    \"\"\"\n    # Get the dimensionality of head vectors.\n    d = Q.shape[-1]\n    \n    # Scaled dot-product: (seq_len, head_dim) @ (head_dim, seq_len) -> (seq_len, seq_len)\n    logits = Q @ K.T / np.sqrt(d)\n    \n    # For numerical stability, subtract the max in each row before computing softmax.\n    logits = logits - np.max(logits, axis=1, keepdims=True)\n    \n    # Compute softmax for each row.\n    exp_logits = np.exp(logits)\n    softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Compute attention as weighted sum of values.\n    attention = softmax @ V\n    # rounding the results to the nearest 4th decimal.\n    attention = np.around(attention, decimals=4)\n    return attention\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention for the input X.\n    \n    Arguments:\n      X      : (seq_len, d_model) input matrix.\n      W_q    : (d_model, d_model) query projection matrix.\n      W_k    : (d_model, d_model) key projection matrix.\n      W_v    : (d_model, d_model) value projection matrix.\n      n_heads: number of attention heads.\n    \n    Returns:\n      Multi-head attention output as a python list (after rounding to 4 decimals).\n    \"\"\"\n    # 1. Compute the Query (Q), Key (K), and Value (V) matrices.\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    seq_len, d_model = Q.shape\n    if d_model % n_heads != 0:\n        raise ValueError(\"d_model must be divisible by n_heads\")\n    \n    head_dim = d_model // n_heads\n    \n    # 2. Split Q, K, V for multiple heads.\n    # New shape: (seq_len, n_heads, head_dim) then transpose to (n_heads, seq_len, head_dim)\n    def split_heads(X_mat):\n        X_reshaped = X_mat.reshape(seq_len, n_heads, head_dim)\n        return X_reshaped.transpose(1, 0, 2)\n    \n    Q_split = split_heads(Q)\n    K_split = split_heads(K)\n    V_split = split_heads(V)\n    \n    # 3. Compute attention for each head using the self_attention function.\n    heads_output = []\n    for i in range(n_heads):\n        q_head = Q_split[i]   # shape: (seq_len, head_dim)\n        k_head = K_split[i]   # shape: (seq_len, head_dim)\n        v_head = V_split[i]   # shape: (seq_len, head_dim)\n        head_out = self_attention(q_head, k_head, v_head)\n        heads_output.append(head_out)\n    \n    # 4. Concatenate outputs from all heads.\n    # heads_output is a list of arrays of shape (seq_len, head_dim),\n    # we first stack on axis 0 -> shape: (n_heads, seq_len, head_dim)\n    concat = np.concatenate(heads_output, axis=-1)  # along the last dim --> (seq_len, n_heads * head_dim)\n    \n    # Round final output\n    concat = np.around(concat, decimals=4)\n    \n    # Return the list representation.\n    return concat.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example dimensions:\n    seq_len = 3\n    d_model = 8\n    n_heads = 2\n    \n    # Random input X and weights (fixed seed for reproducibility)\n    np.random.seed(42)\n    X = np.random.rand(seq_len, d_model)\n    W_q = np.random.rand(d_model, d_model)\n    W_k = np.random.rand(d_model, d_model)\n    W_v = np.random.rand(d_model, d_model)\n    \n    output = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(\"Multi-head attention output:\")\n    print(output)\n------------------------------------------------------------\n\nExplanation:\n1. compute_qkv multiplies the input X with provided weight matrices to get Q, K, and V.\n2. self_attention computes scaled dot-product self attention after applying a softmax on the scaled similarity scores.\n3. multi_head_attention first computes Q, K, V; then splits the result over the number of heads (ensuring head dimension d_model/n_heads); applies self attention for each head; and finally concatenates the results. All intermediate and final outputs are rounded to the 4th decimal and converted to a python list using .tolist().\n\nThis solution fulfills the task requirements."}
{"task_id": 95, "completion_id": 0, "completion": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Ensure both lists have the same length\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n    \n    # Initialize counts for contingency table\n    n11 = n10 = n01 = n00 = 0\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n        else:\n            raise ValueError(\"Lists must contain only binary values (0 or 1).\")\n    \n    # Compute the numerator and denominator for the Phi coefficient\n    numerator = (n11 * n00) - (n10 * n01)\n    # Calculate parts of the denominator\n    denom_part1 = n11 + n10\n    denom_part2 = n01 + n00\n    denom_part3 = n11 + n01\n    denom_part4 = n10 + n00\n    denominator = (denom_part1 * denom_part2 * denom_part3 * denom_part4) ** 0.5\n    \n    # Avoid division by zero\n    if denominator == 0:\n        return 0.0\n    \n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Hard sigmoid formula: f(x) = max(0, min(1, 0.2*x + 0.5))\n    return max(0.0, min(1.0, 0.2 * x + 0.5))\n```"}
{"task_id": 97, "completion_id": 0, "completion": "import math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-3.0, -1.0, 0.0, 1.0, 3.0]\n    for value in test_values:\n        print(f\"elu({value}) = {elu(value)}\")\n"}
{"task_id": 98, "completion_id": 0, "completion": "def prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-3.0, -1.0, 0.0, 1.0, 3.0]\n    for val in test_values:\n        print(\"prelu({0}) = {1}\".format(val, prelu(val)))\n"}
{"task_id": 99, "completion_id": 0, "completion": "import math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    # To improve numerical stability:\n    # For x > 0, we use the identity: softplus(x) = x + log(1 + e^{-x})\n    # For x <= 0, we use the identity: softplus(x) = log(1 + e^x)\n    if x > 0:\n        result = x + math.log1p(math.exp(-x))\n    else:\n        result = math.log1p(math.exp(x))\n \n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-100, -1, 0, 1, 100]\n    for val in test_values:\n        print(f\"softplus({val}) =\", softplus(val))"}
{"task_id": 100, "completion_id": 0, "completion": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "completion": "Below is one valid implementation. In this version we assume that the GRPO objective is very similar to the PPO surrogate objective:\n1. We compute the surrogate term as the minimum between the unclipped advantage (rho * A) and the clipped one.\n2. We then subtract a penalty term computed as the KL divergence between the old policy and a reference policy.\n3. Finally, we return the mean surrogate (across the batch) minus the KL penalty, rounded to 6 decimals.\n\nYou can adjust the aggregation (sum vs. mean) if required by your application. For example:\n\n--------------------------------------------------\n\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n        \n    Returns:\n        The computed GRPO objective value (rounded to the nearest 6th decimal).\n    \"\"\"\n    # Convert lists into numpy arrays for vectorized operations.\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_theta_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.array(pi_theta_ref, dtype=np.float64)\n    \n    # Apply clipping: compute the clipped likelihood ratios.\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the surrogate term: minimum between the unclipped and clipped terms.\n    surrogate = np.minimum(rhos * A, rhos_clipped * A)\n    \n    # We take average over the sample batch:\n    surrogate_mean = np.mean(surrogate)\n    \n    # Compute the KL divergence between old and reference policy distributions.\n    # Adding a tiny constant (1e-8) for numerical stability.\n    kl_div = np.sum(pi_theta_old * np.log(pi_theta_old / (pi_theta_ref + 1e-8)))\n    \n    # Compute the final objective: surrogate advantage minus the KL penalty.\n    objective = surrogate_mean - beta * kl_div\n    \n    # Round the final result to 6 decimals.\n    return round(objective, 6)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dummy example values:\n    rhos = [1.1, 0.9, 1.05, 0.95]\n    A = [0.5, -0.3, 0.2, 0.1]\n    pi_theta_old = [0.25, 0.25, 0.25, 0.25]\n    pi_theta_ref = [0.3, 0.2, 0.3, 0.2]\n    \n    objective_value = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\n    print(\"GRPO objective:\", objective_value)\n\n--------------------------------------------------\n\nExplanation:\n\u2022 We convert input lists to numpy arrays.\n\u2022 The likelihood ratio rhos is clipped between [1 - epsilon, 1 + epsilon].\n\u2022 The surrogate is computed element wise as the minimum between the unclipped and clipped term multiplied by the advantage estimates.\n\u2022 The KL divergence is computed as \u03a3[ pi_theta_old * log(pi_theta_old / pi_theta_ref) ]. A small constant is added to avoid division by zero.\n\u2022 The final objective subtracts (beta times the KL divergence) from the average surrogate advantage.\n\u2022 The returned result is rounded to 6 decimal places.\n\nThis implementation meets the task requirements."}
{"task_id": 102, "completion_id": 0, "completion": "import math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    # Compute the sigmoid function\n    sigmoid = 1 / (1 + math.exp(-x))\n    # Compute swish\n    swish_value = x * sigmoid\n    # Round the result to the 4th decimal\n    return round(swish_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for value in inputs:\n        print(f\"swish({value}) = {swish(value)}\")"}
{"task_id": 103, "completion_id": 0, "completion": "import math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n    \n    Args:\n        x: Input value\n        \n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x >= 0:\n        result = scale * x\n    else:\n        # Use math.expm1 for numerical stability in computing exp(x) - 1\n        result = scale * (alpha * math.expm1(x))\n    \n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-1.0, 0.0, 1.0, 2.0]\n    for val in test_values:\n        print(f\"selu({val}) = {selu(val)}\")"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a python list.\n    \"\"\"\n    # Compute the linear combination of inputs and weights, then add bias\n    linear_output = np.dot(X, weights) + bias\n    \n    # Sigmoid function to compute probabilities\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    \n    # Convert probabilities to binary predictions using a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return predictions as a Python list\n    return predictions.tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "Below is one correct implementation. In this solution we initialize the coefficient matrix B with zeros (of shape [num_features, num_classes]), and then run a loop for the given number of iterations. In each iteration we compute the logits, normalize with the Softmax function (using a numerical stability trick), compute the cross\u2010entropy loss for the batch, derive the gradient, and perform the parameter update with gradient descent. Finally, we round B (and each loss) to 4 decimal places and return them as lists via tolist().\n\nHere is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Determine number of samples, features, and classes.\n    N, n_features = X.shape\n    # Here, we assume the labels are in the range 0...M-1.\n    num_classes = int(np.max(y)) + 1\n\n    # Initialize weights B: shape (n_features, num_classes)\n    B = np.zeros((n_features, num_classes))\n    losses = []\n\n    for i in range(iterations):\n        # Compute scores (logits): shape (N, num_classes)\n        scores = np.dot(X, B)\n        # For numerical stability, subtract max for each sample.\n        scores -= np.max(scores, axis=1, keepdims=True)\n        \n        # Compute softmax probabilities.\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # shape (N, num_classes)\n        \n        # Compute the cross entropy loss.\n        # Extract the probabilities corresponding to the correct classes.\n        correct_logprobs = -np.log(probs[np.arange(N), y])\n        loss = np.mean(correct_logprobs)\n        losses.append(round(loss, 4))\n        \n        # Compute the gradient on scores.\n        dscores = probs.copy()\n        dscores[np.arange(N), y] -= 1\n        dscores /= N\n\n        # Backpropagate the gradient to the parameters B\n        dB = np.dot(X.T, dscores)  # shape (n_features, num_classes)\n\n        # Perform parameter update.\n        B = B - learning_rate * dB\n\n    # Round final parameter matrix B to 4 decimals.\n    B = np.round(B, 4)\n    \n    # Convert B to list and return as tuple with the loss list.\n    return B.tolist(), losses\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Simple toy example: 4 samples, 3 features, 2 classes\n    X = np.array([[1.0, 2.0, 3.0],\n                  [4.0, 5.0, 6.0],\n                  [7.0, 8.0, 9.0],\n                  [1.0, 3.0, 5.0]])\n    y = np.array([0, 1, 0, 1])\n    learning_rate = 0.01\n    iterations = 100\n\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    print(\"Optimized Coefficients (B):\")\n    print(B)\n    print(\"Loss over iterations:\")\n    print(losses)\n\n------------------------------------------------------------\n\nExplanation:\n\n1. We subtract the maximum per sample from the scores to avoid overflow when exponentiating.\n2. Softmax probabilities are calculated and used both for computing the cross entropy loss and for computing the gradient.\n3. The gradient, dB, is computed by taking the dot product between the transpose of X and the difference between the predicted probabilities and the one-hot encoded true labels.\n4. The parameter B is updated using the gradient and the given learning rate.\n5. Finally, both the parameter matrix and loss values are rounded to 4 decimal places and converted to standard Python lists before returning."}
{"task_id": 106, "completion_id": 0, "completion": "import numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression,\n    optimizing parameters with Binary Cross Entropy loss.\n    Returns a tuple containing the list of loss values over iterations\n    and the final coefficient values (rounded to the 4th decimal).\n    \"\"\"\n    m, n = X.shape\n    # Initialize parameters (theta) as zeros with shape (n,1)\n    theta = np.zeros((n, 1))\n    \n    # Ensure y is a column vector\n    y = y.reshape(-1, 1)\n    \n    # To store loss values after each iteration\n    loss_history = []\n    \n    # Define sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Small epsilon to avoid log(0) errors\n    epsilon = 1e-15\n    \n    for i in range(iterations):\n        # Compute linear combination (m,1)\n        z = np.dot(X, theta)\n        \n        # Compute predicted probabilities (m,1)\n        y_hat = sigmoid(z)\n        # Clip predictions to avoid log(0) in loss calculation\n        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n        \n        # Calculate Binary Cross Entropy loss\n        loss = (-1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n        # Append the rounded loss (4 decimal places)\n        loss_history.append(round(loss, 4))\n        \n        # Compute the gradient (n,1)\n        gradient = (1 / m) * np.dot(X.T, (y_hat - y))\n        \n        # Update parameters theta using gradient descent\n        theta = theta - learning_rate * gradient\n    \n    # Round the final coefficients to 4 decimals and flatten to 1D list\n    final_coefficients = np.round(theta, 4).flatten().tolist()\n    \n    return loss_history, final_coefficients\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dummy dataset: 4 samples, 2 features\n    X = np.array([[1, 2],\n                  [1, -1],\n                  [-1, 2],\n                  [-1, -1]])\n    y = np.array([1, 0, 1, 0])\n    \n    # Train the logistic regression\n    losses, coefficients = train_logreg(X, y, learning_rate=0.1, iterations=100)\n    print(\"Loss history (first 5):\", losses[:5])\n    print(\"Final coefficients:\", coefficients)"}
{"task_id": 107, "completion_id": 0, "completion": "Below is one correct implementation. In this code we first compute Q, K, V from the input X and weight matrices. Then we define the function masked_attention where we compute the scaled dot product between Q and K\u1d40, apply an additive mask (setting positions we do not want to attend to to a very small value), and then compute the softmax over each row. Finally we multiply the resulting attention weights by V and return the result as a Python list using np.tolist().\n\nNote: We assume that the provided mask is a binary mask with 1\u2019s for allowed positions and 0\u2019s for masked-out ones. One common practice is to replace masked-out attention scores with a very small value (like -1e9) so that the subsequent softmax yields near-zero values.\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Steps:\n    1. Compute the scaled dot-product between Q and K\u1d40.\n    2. Apply the mask: the positions where mask == 0 get a large negative value.\n    3. Apply softmax to get the attention weights.\n    4. Multiply the attention weights with V.\n    5. Return the output as a Python list.\n    \"\"\"\n    # Obtain the dimension of Q (and K) for scaling\n    d_k = Q.shape[-1]\n    \n    # 1. Scaled dot-product: (batch or sequence length) x (sequence length)\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # 2. Apply mask: assume mask is 1 (keep) and 0 (mask out)\n    # We use a large negative value so that masked logits are nearly zero after softmax.\n    masked_scores = np.where(mask == 0, -1e9, scores)\n    \n    # 3. Compute softmax along the last axis\n    # To improve numerical stability, subtract the max from each row before exponentiating.\n    max_per_row = np.max(masked_scores, axis=1, keepdims=True)\n    exp_scores = np.exp(masked_scores - max_per_row)\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    \n    # 4. Multiply the softmax scores with V to get the attended results.\n    output = np.dot(softmax_scores, V)\n    \n    # 5. Convert the result to a Python list and return\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Let's assume a small sequence of 4 tokens and embedding dimension of 3.\n    X = np.array([[0.1, 0.2, 0.3],\n                  [0.4, 0.5, 0.6],\n                  [0.7, 0.8, 0.9],\n                  [1.0, 1.1, 1.2]])\n    \n    # Define random weight matrices for Q, K, V:\n    np.random.seed(42)\n    W_q = np.random.rand(3, 3)\n    W_k = np.random.rand(3, 3)\n    W_v = np.random.rand(3, 3)\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Create an attention mask for a sequence of length 4.\n    # For example, a lower-triangular mask for causal/auto-regressive attention:\n    seq_len = X.shape[0]\n    mask = np.tril(np.ones((seq_len, seq_len), dtype=np.int32))\n    \n    # Compute masked attention output:\n    output = masked_attention(Q, K, V, mask)\n    print(\"Masked Attention Output:\")\n    print(output)\n    \n# When you run this script, it will print the output of the masked attention as a Python list.\n------------------------------------------------------------\n\nExplanation:\n\n1. Function compute_qkv computes Q, K, V by taking the dot product of the input X with the corresponding weight matrices.\n2. In masked_attention, we compute the attention scores using Q @ K\u1d40, scale by 1/sqrt(d_k) to mitigate high variance.\n3. The mask is applied via np.where: wherever mask is 0, the score becomes -1e9 (a very low number), ensuring the softmax assigns almost zero weight there.\n4. The softmax is computed in a numerically stable manner.\n5. Finally, the attended output is obtained by multiplying the attention weights with V, and the result is returned as a list after calling tolist()."}
{"task_id": 108, "completion_id": 0, "completion": "We can measure disorder using many approaches. One simple and natural one is to compute the Shannon entropy of the colors. This entropy is zero when all apples are the same (since there is no uncertainty) and increases as the relative frequencies of the different colors become more balanced. For example, for the basket [1,1,0,0] the two colors are equally likely (probability 0.5 each), which gives an entropy of 1 bit. For [0,1,2,3] (four equally common colors) the entropy is 2 bits, and similarly for [0,0,1,1,2,2,3,3] (four colors with equal probability), the entropy is 2 bits. Notice that although the latter two baskets yield the same entropy, the basket [0,0,1,1,2,2,3,3] has twice as many apples as [0,1,2,3]. If you want a measure that increases not only with the variety of colors but also with the absolute evenness among many apples, you might tweak the measure further. However, according to our requirements the basic entropy (which distinguishes [1,1,0,0] from [0,1,2,3] and yields a higher value for [0,0,1,1,2,2,3] than for [0,0,0,0,0,1,2,3]) works fine.\n\nBelow is the Python code that implements such an approach using base\u20112 logarithms. All results are rounded to the nearest fourth decimal:\n\n--------------------------------------------------\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy (base 2) as the metric.\n    The more balanced and varied the colors, the higher the disorder.\n    \"\"\"\n    if not apples:\n        return 0.0\n\n    # Count the occurrences of each color.\n    freq = {}\n    for color in apples:\n        freq[color] = freq.get(color, 0) + 1\n\n    total = len(apples)\n    entropy = 0.0\n\n    # Compute Shannon entropy: -sum(p_i * log2(p_i))\n    import math\n    for count in freq.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    \n    # Return the entropy rounded to 4 decimal places.\n    return round(entropy, 4)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    baskets = [\n        [0, 0, 0, 0],\n        [1, 1, 0, 0],\n        [0, 1, 2, 3],\n        [0, 0, 1, 1, 2, 2, 3, 3],\n        [0, 0, 0, 0, 0, 1, 2, 3],\n    ]\n    for basket in baskets:\n        print(f\"Basket: {basket} => Disorder: {disorder(basket)}\")\n\n--------------------------------------------------\n\nExplanation of the examples:\n\n\u2022 [0,0,0,0]:\n  All apples have the same color so probability is 1, hence entropy = 0.\n\n\u2022 [1,1,0,0]:\n  Two colors with equal probability of 0.5 yield entropy = \u20132\u00d7(0.5\u00d7log2(0.5)) = 1.0.\n\n\u2022 [0,1,2,3]:\n  Four unique colors (each with probability 0.25) yield entropy = 2.0.\n\n\u2022 [0,0,1,1,2,2,3,3]:\n  Again, four unique colors equally represented so entropy = 2.0.\n  \n\u2022 [0,0,0,0,0,1,2,3]:\n  The color 0 appears 5 times and the others appear once each. The resulting entropy is lower than 2.0 (approximately 1.5488).\n\nThis meets all the problem properties."}
{"task_id": 109, "completion_id": 0, "completion": "import numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute mean and variance along the feature axis (last dimension)\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input: subtract mean and scale by the standard deviation\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift: gamma and beta should broadcast over the feature dimension\n    out = X_normalized * gamma + beta\n    \n    # Round the output to 5 decimal places and convert to list\n    out_rounded = np.round(out, 5)\n    return out_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create an example input: batch size = 2, sequence length = 3, feature dimension = 4\n    X = np.array([[[1, 2, 3, 4],\n                   [2, 3, 4, 5],\n                   [3, 4, 5, 6]],\n                  \n                  [[4, 5, 6, 7],\n                   [5, 6, 7, 8],\n                   [6, 7, 8, 9]]], dtype=np.float32)\n    \n    # gamma and beta parameters for scaling and shifting; matched with feature dimension (4)\n    gamma = np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)\n    beta = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n    \n    # Call layer normalization\n    normalized_output = layer_normalization(X, gamma, beta)\n    print(normalized_output)"}
{"task_id": 110, "completion_id": 0, "completion": "Below is one acceptable solution. In this implementation we first tokenize the reference and candidate strings. Then we \u201calign\u201d candidate unigrams with a (greedy) occurrence in the reference in a way that preserves the word order (so that only matches with increasing reference\u2010indices are accepted). Once an alignment is built we count the number m of matched unigrams and also count the number of \u201cchunks\u201d (a chunk is defined as a contiguous block in the reference according to our alignment; every time two consecutive matched indices are not consecutive, we start a new chunk). Finally we compute precision, recall and a weighted harmonic mean (F_mean) and we subtract a penalty computed as gamma \u00d7 (chunks/m)^beta. (All parameters are passed as default parameters.) Finally, the score is returned as a float rounded to 3\u2010decimal places.\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\nfrom collections import defaultdict\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # tokenize the input strings (here we simply split on whitespace)\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Create a dictionary mapping each token in the reference to a list of its positions (sorted)\n    ref_positions = defaultdict(list)\n    for idx, token in enumerate(ref_tokens):\n        ref_positions[token].append(idx)\n    \n    # We perform a greedy alignment: for each candidate token, if it appears in the reference,\n    # we try to match it to an occurrence whose position is > than the position of the previous match.\n    alignment = []  # list of chosen matched positions from reference\n    last_assigned = -1  # last reference index used in the alignment\n    \n    # To avoid matching the same occurrence more than once, we create a copy of ref_positions lists.\n    ref_available = {token: list(indices) for token, indices in ref_positions.items()}\n    \n    for tok in cand_tokens:\n        if tok in ref_available and ref_available[tok]:\n            # In the available positions for this token, find the smallest one that is greater than last_assigned.\n            chosen_index = None\n            for pos in ref_available[tok]:\n                if pos > last_assigned:\n                    chosen_index = pos\n                    break\n            if chosen_index is not None:\n                alignment.append(chosen_index)\n                last_assigned = chosen_index\n                # Remove the chosen index from available list\n                ref_available[tok].remove(chosen_index)\n            # else: no available occurrence that can maintain order, so skip.\n    \n    m = len(alignment)  # number of matched tokens\n    if m == 0:\n        return 0.0  # no match => score is 0\n    \n    # Count chunks: start with one chunk and each time consecutive indices are not adjacent in reference,\n    # we count a new chunk.\n    chunks = 1\n    for i in range(1, len(alignment)):\n        if alignment[i] != alignment[i-1] + 1:\n            chunks += 1\n    \n    # Compute precision and recall\n    precision = m / len(cand_tokens)\n    recall = m / len(ref_tokens)\n    \n    # Compute the weighted harmonic mean (F_mean) using the parameter alpha.\n    # F_mean = (P * R) / ((1 - alpha) * R + alpha * P)\n    denominator = ((1 - alpha) * recall + alpha * precision)\n    if denominator == 0:\n        f_mean = 0\n    else:\n        f_mean = (precision * recall) / denominator\n    \n    # Compute penalty: penalty = gamma * (chunks/m)**beta\n    penalty = gamma * ((chunks / m) ** beta)\n    \n    # Final METEOR score is defined as: score = F_mean * (1 - penalty)\n    score = f_mean * (1 - penalty)\n    \n    return round(score, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    ref = \"the cat is on the mat\"\n    cand = \"cat the mat is on\"\n    print(\"METEOR score:\", meteor_score(ref, cand))\n    \n# For the provided example you might see an output similar to:\n# METEOR score: 0.433\n\n------------------------------------------------------------\n\nExplanation:\n\n1. We split the reference and candidate strings into tokens.\n2. We build an index mapping for the reference.\n3. We then go through the candidate tokens and try to match each token to an occurrence in the reference that comes after the previous match. This yields our alignment.\n4. We count the number of chunks. A new chunk is started whenever two consecutive aligned indices are not consecutive numbers.\n5. Precision and recall are computed from the number of aligned tokens (m).\n6. We compute F_mean using the given parameter alpha.\n7. A penalty term is computed as gamma * (chunks/m)^beta.\n8. Finally the score is given by F_mean * (1 - penalty) and is returned rounded to three decimal places.\n\nThis meets the task requirements."}
{"task_id": 111, "completion_id": 0, "completion": "Below is the Python implementation that calculates PMI:\n\n--------------------------------------------------\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n    \n    PMI = log [P(x,y) / (P(x)*P(y))]\n        = log [ (joint_counts / total_samples) / ((total_counts_x/total_samples) * (total_counts_y/total_samples)) ]\n        = log [ (joint_counts * total_samples) / (total_counts_x * total_counts_y) ]\n    \n    Args:\n        joint_counts (int or float): The count of simultaneous occurrences of events x and y.\n        total_counts_x (int or float): The count of event x occurrences.\n        total_counts_y (int or float): The count of event y occurrences.\n        total_samples (int or float): The total number of samples.\n    \n    Returns:\n        float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Compute PMI using the formula\n    pmi = np.log((joint_counts * total_samples) / (total_counts_x * total_counts_y))\n    return round(pmi, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    joint_counts = 5\n    total_counts_x = 20\n    total_counts_y = 30\n    total_samples = 100\n    print(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n--------------------------------------------------\n\nNotes:\n\u2022 The formula used utilizes the natural logarithm (np.log).\n\u2022 The result is rounded to 3 decimals before being returned."}
{"task_id": 112, "completion_id": 0, "completion": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []  # Return an empty list if input is empty.\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Calculate the range. Avoid division by zero if all numbers are the same.\n    range_val = max_val - min_val\n    if range_val == 0:\n        # If all numbers are identical, return a list of 0.0's.\n        return [0.0 for _ in x]\n    \n    # Normalize each value and round to 4 decimal places.\n    normalized = [round((value - min_val) / range_val, 4) for value in x]\n    return normalized\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_data = [10, 20, 30, 40, 50]\n    print(min_max(sample_data))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]"}
{"task_id": 113, "completion_id": 0, "completion": "import numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # Define the ReLU activation function.\n    def relu(z):\n        return np.maximum(0, z)\n\n    # First weight layer and ReLU.\n    z1 = np.dot(w1, x)\n    a1 = relu(z1)\n\n    # Second weight layer and ReLU.\n    z2 = np.dot(w2, a1)\n\n    # Add the shortcut (the original input) and apply the final ReLU.\n    output = relu(z2 + x)\n\n    # Round the results to 4 decimal places and convert to list.\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "completion": "import numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform global average pooling on a 3D numpy array representing feature maps.\n    \n    Parameters:\n    x (np.ndarray): Input array with shape (height, width, channels)\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    # Compute the mean over the height and width dimensions (axis 0 and 1)\n    return np.mean(x, axis=(0, 1))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample 3D array with shape (4, 4, 3)\n    sample_input = np.random.rand(4, 4, 3)\n    \n    # Apply global average pooling\n    output = global_avg_pool(sample_input)\n    \n    print(\"Input shape:\", sample_input.shape)\n    print(\"Output (global average pooled) shape:\", output.shape)\n    print(\"Output values:\\n\", output)"}
{"task_id": 115, "completion_id": 0, "completion": "Below is one correct implementation:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute mean and variance across batch, height, and width dimensions for each channel.\n    # Keep dimensions so that broadcasting over X works directly.\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape gamma and beta to broadcast along BCHW dimensions.\n    gamma = gamma.reshape(1, -1, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)\n    \n    # Scale and shift.\n    X_out = gamma * X_norm + beta\n    \n    # Round the result to 4 decimal places and convert to list.\n    return np.round(X_out, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a random BCHW tensor: (batch size, channels, height, width) = (2, 3, 4, 4)\n    np.random.seed(0)\n    X = np.random.randn(2, 3, 4, 4)\n    gamma = np.array([1.0, 1.5, 2.0])\n    beta = np.array([0.5, -0.5, 1.0])\n    \n    result = batch_normalization(X, gamma, beta)\n    print(result)\n------------------------------------------------------------\n\nExplanation:\n1. Mean and variance are computed across dimensions 0 (batch), 2 (height), and 3 (width) for each channel.\n2. We normalize X with X_norm = (X - mean) / sqrt(var + epsilon) for numerical stability.\n3. gamma and beta are reshaped to (1, channels, 1, 1) so that they can be broadcasted properly when scaling and shifting X_norm.\n4. The final output is rounded to 4 decimal places and converted to a Python list with tolist()."}
{"task_id": 116, "completion_id": 0, "completion": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term c * x^n using the power rule.\n    \n    Parameters:\n      c (float): the coefficient of the term.\n      x (float): the point at which to evaluate the derivative.\n      n (float): the exponent in the term.\n      \n    Returns:\n      float: the derivative evaluated at x, rounded to 4 decimal places.\n    \"\"\"\n    # For n=0, the term is a constant and its derivative is 0.\n    if n == 0:\n        return 0.0\n    \n    # Compute derivative using power rule: d/dx (c * x^n) = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1))\n    \n    # Round the result to 4 decimal places.\n    return round(derivative, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: For the term 3 * x^2, the derivative is 6 * x.\n    # At x = 2, derivative should be 12.\n    result1 = poly_term_derivative(3, 2, 2)\n    print(\"Derivative of 3*x^2 at x=2 is:\", result1)  # Expected output: 12.0\n\n    # Example 2: For the term 5 * x^3, the derivative is 15 * x^2.\n    # At x = 1.5, derivative should be 15 * (1.5)^2.\n    result2 = poly_term_derivative(5, 1.5, 3)\n    print(\"Derivative of 5*x^3 at x=1.5 is:\", result2)  # Expected output: 33.75 (rounded to 33.7500)\n\n    # Example 3: For a constant term (n=0), the derivative is 0.\n    result3 = poly_term_derivative(10, 100, 0)\n    print(\"Derivative of 10*x^0 (a constant) at x=100 is:\", result3)  # Expected output: 0.0\n"}
{"task_id": 117, "completion_id": 0, "completion": "import numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by given 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: List of 2D vectors (each vector itself is a list of floats).\n    - tol: Tolerance used to decide if a vector is linearly independent.\n    \n    Returns:\n    - A list of orthonormal vectors (each as a list of 4-decimal rounded floats) that span the same subspace.\n    \"\"\"\n    # List to store the orthonormal vectors (as numpy arrays)\n    ortho_basis = []\n    \n    for v in vectors:\n        # Convert list to numpy array of float type\n        v_vec = np.array(v, dtype=float)\n        \n        # Subtract components in the directions of already found basis vectors\n        for b in ortho_basis:\n            proj = np.dot(v_vec, b) * b  # projection of v_vec on b\n            v_vec = v_vec - proj\n        \n        # If the vector is not nearly zero, normalize it\n        norm_v = np.linalg.norm(v_vec)\n        if norm_v > tol:\n            # Normalize the vector\n            v_vec = v_vec / norm_v\n            ortho_basis.append(v_vec)\n    \n    # Round each vector in the basis to 4 decimal places and convert it to list\n    ortho_basis_rounded = [np.round(vec, 4).tolist() for vec in ortho_basis]\n    return ortho_basis_rounded\n\n# Example usage:\nif __name__ == '__main__':\n    vectors = [[1, 2], [2, 4], [1, 0]]\n    basis = orthonormal_basis(vectors)\n    print(\"Orthonormal Basis:\", basis)"}
{"task_id": 118, "completion_id": 0, "completion": "import numpy as np\ndef cross_product(a, b):\n    # Convert input lists to numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product using numpy's cross function\n    result = np.cross(a, b)\n    \n    # Round the results to 4 decimal places\n    result = np.around(result, decimals=4)\n    \n    # Convert the numpy array to a list and return\n    return result.tolist()"}
{"task_id": 119, "completion_id": 0, "completion": "import numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays (in case they're not already) with float type for precision\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Calculate determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is close to zero, there is no unique solution; return -1\n    if np.isclose(det_A, 0):\n        return -1\n    \n    n = A.shape[0]\n    solution = np.zeros(n)\n    \n    # Use Cramer's Rule: for each variable, replace the corresponding column with vector b\n    # and compute the determinant of the modified matrix.\n    for i in range(n):\n        A_copy = A.copy()\n        A_copy[:, i] = b\n        det_Ai = np.linalg.det(A_copy)\n        solution[i] = det_Ai / det_A\n\n    # Round the results to 4 decimal places\n    solution = np.round(solution, 4)\n    \n    # Convert the solution numpy array to a Python list using .tolist() and return it\n    return solution.tolist()\n\n\n# Example usage:\n# A = [[2, -1, 3],\n#      [1,  0, 2],\n#      [3,  1, 4]]\n# b = [5,  4, 10]\n# print(cramers_rule(A, b))  # Should output the solution vector rounded to 4 decimals"}
{"task_id": 120, "completion_id": 0, "completion": "import numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check for empty lists or different lengths\n    if not p or not q or len(p) != len(q):\n        return 0.0\n\n    # Convert lists to numpy arrays for element-wise computation\n    p_arr = np.array(p)\n    q_arr = np.array(q)\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n    \n    # Handle edge case: if the coefficient is 0, the log would be -inf.\n    if bc <= 0:\n        return float('inf')\n    \n    # Compute the Bhattacharyya distance\n    distance = -np.log(bc)\n    \n    # Return the distance rounded to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    p = [0.2, 0.5, 0.3]\n    q = [0.1, 0.7, 0.2]\n    print(bhattacharyya_distance(p, q))  # Expected output: Bhattacharyya distance rounded to 4 decimal places"}
{"task_id": 121, "completion_id": 0, "completion": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n\n    return [x + y for x, y in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "completion": "Below is the complete Python code that implements the REINFORCE policy gradient estimator for a softmax policy. For each state\u2013action step in each episode, we first compute the return (i.e. sum of rewards from that step onward), then compute the gradient of the log-policy \u2013 which, under the softmax parameterization, is one-hot for the taken action minus the probability distribution \u2013 and scale it by the return. Finally, we average these gradients over episodes and round the result to 4 decimal places before converting the array to a Python list.\n\nBelow is the complete code:\n\n------------------------------------------------\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    # Get the dimensions from theta\n    num_states, num_actions = theta.shape\n    # Initialize a gradient array of same shape as theta\n    grad_total = np.zeros_like(theta, dtype=np.float64)\n    \n    # Iterate over episodes\n    for episode in episodes:\n        # Pre-compute returns for each time step in the episode.\n        # For each time step t, return is sum_{i=t}^{T-1} r_i.\n        T = len(episode)\n        returns = np.zeros(T)\n        G = 0.0\n        # Iterate backwards to compute return-to-go for each time step.\n        for t in reversed(range(T)):\n            # Each element: (state, action, reward)\n            _, _, reward = episode[t]\n            G += reward\n            returns[t] = G\n            \n        # Now accumulate gradient contributions for this episode.\n        for t, (state, action, _) in enumerate(episode):\n            # Compute softmax probabilities for the current state.\n            exps = np.exp(theta[state, :])\n            probs = exps / np.sum(exps)\n            \n            # The gradient of log pi(a|s) is one-hot for the selected action minus the probabilities.\n            # Create an array that is the gradient for the given state.\n            grad_log = np.zeros(num_actions, dtype=np.float64)\n            grad_log[action] = 1.0  # one-hot for the taken action.\n            grad_log -= probs   # subtract the probabilities.\n            \n            # Multiply by the return from this time step (the REINFORCE weight)\n            grad_total[state, :] += grad_log * returns[t]\n    \n    # Average the gradient over episodes.\n    grad_avg = grad_total / len(episodes)\n    \n    # Round the results to the nearest 4th decimal.\n    grad_avg = np.around(grad_avg, decimals=4)\n    \n    # Return the result as a Python list after reshaping.\n    return grad_avg.tolist()\n\n\n# Example usage:\nif __name__ == '__main__':\n    # Define a simple theta for 3 states and 2 actions.\n    theta = np.array([[0.2, 0.8],\n                      [0.5, 0.5],\n                      [0.9, 0.1]])\n    \n    # Define sample episodes: each episode is a list of (state, action, reward)\n    episodes = [\n        [(0, 1, 1.0), (1, 0, 0.5), (2, 0, -0.2)],\n        [(1, 1, 0.7), (0, 0, 1.2)]\n    ]\n    \n    grad_list = compute_policy_gradient(theta, episodes)\n    print(grad_list)\n------------------------------------------------\n\nExplanation:\n\n1. For each episode, we compute returns by iterating backward so that for time step t, returns[t] = r_t + r_(t+1) + ... .\n\n2. For each state in the episode, we compute the softmax probabilities using the current theta. The gradient for the log-policy is then computed as:\n\u2003\u2003grad_log = (one_hot vector for the taken action) - (softmax probabilities)\n\n3. We multiply the gradient by the return from that time step and accumulate it in grad_total for the corresponding state.\n\n4. After processing all episodes, we average the gradient across episodes, round to 4 decimals, and finally convert the NumPy array into a Python list with the tolist() method.\n\nThis implementation meets the requirements as described."}
{"task_id": 123, "completion_id": 0, "completion": "Below is one way to solve the problem. One common interpretation is that a dense layer with n_experts would need to perform a weight multiplication using every expert\u2019s weight (i.e. n_experts \u00b7 d_in \u00b7 d_out FLOPs per forward pass for the experts\u2019 parts), whereas an MoE layer only evaluates k_active experts (i.e. k_active \u00b7 d_in \u00b7 d_out FLOPs). Then, the savings percentage is calculated as:\n\n\u2003\u2003savings (%) = [(Dense FLOPs \u2013 MoE FLOPs) / Dense FLOPs] \u00d7 100\n\nHere\u2019s the complete code:\n\n------------------------------------------------\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # Compute FLOPs for the dense layer: every expert is used.\n    flops_dense = n_experts * d_in * d_out\n    \n    # Compute FLOPs for the Mixture-of-Experts layer: only k_active experts are active.\n    flops_moe = k_active * d_in * d_out\n    \n    # Calculate the savings percentage.\n    # If every expert were used (dense), and we only use k_active, then\n    # savings = (dense - moe) / dense * 100.\n    savings_percentage = ((flops_dense - flops_moe) / flops_dense) * 100 if flops_dense > 0 else 0\n    \n    # Round all results to the nearest 1th decimal.\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_percentage = round(savings_percentage, 1)\n    \n    return flops_dense, flops_moe, savings_percentage\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Example parameters:\n    n_experts = 10     # total number of experts in dense layer\n    k_active = 2       # number of experts that are active in the MoE layer\n    d_in = 512         # input dimension\n    d_out = 1024       # output dimension\n\n    dense_flops, moe_flops, savings = compute_efficiency(n_experts, k_active, d_in, d_out)\n    print(\"Dense layer FLOPs:\", dense_flops)\n    print(\"MoE layer FLOPs:\", moe_flops)\n    print(\"Savings percentage:\", savings, \"%\")\n------------------------------------------------\n\nExplanation:\n1. The dense layer requires FLOPs_dense = n_experts * d_in * d_out.\n2. The MoE layer only computes over k_active experts giving FLOPs_moe = k_active * d_in * d_out.\n3. The savings is the relative reduction in FLOPs: percentage_gain = ((FLOPs_dense - FLOPs_moe) / FLOPs_dense) \u00d7 100.\n4. All results are rounded to one decimal place."}
{"task_id": 124, "completion_id": 0, "completion": "Below is one complete solution. In this code we assume that X is a (batch_size, input_dim) matrix and that the gating weights W_g and noise weights W_noise are of shape (input_dim, num_experts). The pre\u2010sampled noise N is assumed to match the shape of logits (i.e. (batch_size, num_experts)). The procedure is as follows:\n\n1. Compute the gating logits:\u2003logits = X \u00b7 W_g  \n2. Compute the noise standard deviation (or \u201cgate noise\u201d) with a parallel weight matrix:\u2003noise_std = X \u00b7 W_noise  \n3. Compute the noisy logits as:\u2003noisy_logits = logits + noise_std * N  \n4. For each sample we pick the top\u2011k experts using the noisy logits. For the other experts the logits are replaced with \u2013\u221e so that their softmax contribution is exactly zero.  \n5. A numerically stable softmax is computed over each row and the result is rounded to 4 decimals and returned using tolist().\n\nBelow is the complete implementation:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Step 1: Compute the raw logits\n    logits = np.dot(X, W_g)  # shape: (batch_size, num_experts)\n\n    # Step 2: Compute noise std (or noise scaling factor)\n    noise_std = np.dot(X, W_noise)  # shape: (batch_size, num_experts)\n\n    # Step 3: Add noise to obtain noisy logits.\n    noisy_logits = logits + noise_std * N\n\n    # Step 4: For each sample, mask out the indices that are not in top-k.\n    # First, we will choose the top-k indices per row.\n    batch_size, num_experts = noisy_logits.shape\n\n    # Create a mask array of False with the shape of logits.\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n\n    # Loop over rows to fill in the mask.\n    # (Alternatively, one could vectorize this if desired.)\n    for i in range(batch_size):\n        # argsort returns indices in ascending order.\n        # We want the highest k values so we take the last k indices.\n        topk_indices = np.argsort(noisy_logits[i])[-k:]\n        mask[i, topk_indices] = True\n\n    # Replace values not in top-k with negative infinity so that they contribute zero in exp.\n    masked_logits = np.where(mask, noisy_logits, -np.inf)\n\n    # Step 5: Numerically stable softmax only on top-k indices\n    # subtract the max to enhance numerical stability.\n    # The max for each row in masked_logits (remember that -np.inf will not affect max).\n    row_max = np.max(masked_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(masked_logits - row_max)\n    # Sum of exps along experts dimension.\n    sum_exps = np.sum(exp_logits, axis=1, keepdims=True)\n    # Compute probabilities.\n    probs = exp_logits / sum_exps\n\n    # Round the results to 4 decimal places and return as python list.\n    probs_rounded = np.round(probs, 4)\n    return probs_rounded.tolist()\n\n# Example test run:\nif __name__ == \"__main__\":\n    # Define a small random example for demonstration.\n    np.random.seed(0)\n    batch_size = 3\n    input_dim = 4\n    num_experts = 5\n    k = 2\n\n    X = np.random.randn(batch_size, input_dim)\n    W_g = np.random.randn(input_dim, num_experts)\n    W_noise = np.random.randn(input_dim, num_experts)\n    # Pre-sampled noise matching shape (batch_size, num_experts)\n    N = np.random.randn(batch_size, num_experts)\n\n    probs = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(\"Final gating probabilities:\")\n    print(probs)\n------------------------------------------------------------\n\nExplanation:\n- We first compute logits and add noise scaled by noise_std multiplied by the provided noise matrix N.\n- By masking out (setting to \u2013\u221e) all but the top k experts for each sample, computing the softmax yields nonzero probabilities only for those experts.\n- Finally, the resulting probabilities are rounded and returned as a python list.\n\nThis implementation follows the specification and rounds all floating\u2010point values to the nearest fourth decimal."}
{"task_id": 125, "completion_id": 0, "completion": "Below is one acceptable solution. In this example we assume the following shapes:\n\n\u2022 x has shape (batch_size, input_dim).  \n\u2022 Wg is a gating weight matrix with shape (input_dim, n_experts).  \n\u2022 We is a 3D numpy array with shape (n_experts, input_dim, output_dim) containing all experts\u2019 weight matrices.  \n\nFor each token (row in x) we first compute gating logits and then apply softmax. Next we select the top\u2011k experts (i.e. the indices with the highest gating probabilities), renormalize those probabilities (so that they sum to 1) and then compute the expert output (using x dot Expert_weight). Finally the token\u2019s final output is computed as the weighted sum of the top\u2011k experts\u2019 outputs. All results are rounded to the fourth decimal and returned as a Python list.\n\nBelow is the complete code:\n\n------------------------------------------------------------\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture of Experts (MoE) layer.\n    \n    Parameters:\n      x        : Input tensor of shape (batch_size, input_dim)\n      We       : Expert weight matrices with shape (n_experts, input_dim, output_dim)\n      Wg       : Gating weight matrix with shape (input_dim, n_experts)\n      n_experts: Number of experts (should match the first dimension of We)\n      top_k    : Number of experts to select per token\n      \n    Returns:\n      A python list (via numpy's tolist()) representing the MoE output, where the result is rounded \n      to the nearest 4th decimal.\n    \"\"\"\n    # Step 1: Compute gating logits and apply softmax gating\n    # logits shape: (batch_size, n_experts)\n    logits = np.dot(x, Wg)\n    # Use a numerically stable softmax: subtract max along each row\n    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    gate_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    batch_size, input_dim = x.shape\n    output_dim = We.shape[2]  # assuming shape (n_experts, input_dim, output_dim)\n    \n    # Step 2: Compute all experts outputs for every token.\n    # We'll compute for each expert i: output = x dot We[i]\n    # The resulting shape will be (batch_size, n_experts, output_dim).\n    expert_outputs = []\n    for i in range(n_experts):\n        expert_output = np.dot(x, We[i])\n        expert_outputs.append(expert_output)\n    expert_outputs = np.stack(expert_outputs, axis=1)  # (batch_size, n_experts, output_dim)\n    \n    # Step 3: For each token, select top_k experts and combine weighted outputs.\n    final_output = np.zeros((batch_size, output_dim))\n    \n    for i in range(batch_size):\n        # Get indices of the top_k experts (largest gating probabilities)\n        topk_indices = np.argsort(gate_probs[i])[::-1][:top_k]\n        topk_gate_probs = gate_probs[i][topk_indices]\n        # Renormalize the top_k probabilities so they sum to 1\n        norm_factor = np.sum(topk_gate_probs)\n        if norm_factor == 0:\n            normalized_probs = np.zeros_like(topk_gate_probs)\n        else:\n            normalized_probs = topk_gate_probs / norm_factor\n            \n        # Gather the corresponding expert outputs for current token.\n        outputs = expert_outputs[i, topk_indices, :]  # shape: (top_k, output_dim)\n        # Compute weighted sum of the experts' outputs.\n        combined = np.dot(normalized_probs, outputs)  # shape: (output_dim,)\n        final_output[i] = combined\n    \n    # Step 4: Round the final output to 4 decimals and return as python list.\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()\n\n# Example usage:\nif __name__ == '__main__':\n    # Setting up a toy example:\n    np.random.seed(0)\n    batch_size = 3\n    input_dim = 4\n    output_dim = 5\n    n_experts = 6\n    top_k = 2\n    \n    # Random input\n    x = np.random.randn(batch_size, input_dim)\n    # Random expert weights so that We has shape (n_experts, input_dim, output_dim)\n    We = np.random.randn(n_experts, input_dim, output_dim)\n    # Random gating weights with shape (input_dim, n_experts)\n    Wg = np.random.randn(input_dim, n_experts)\n    \n    moe_output = moe(x, We, Wg, n_experts, top_k)\n    print(\"MoE output:\")\n    print(moe_output)\n------------------------------------------------------------\n\nExplanation:\n\n1. The gating logits are computed by using a dot product between x and Wg. The softmax is calculated (with a subtraction of the maximum value in every row for stability).\n2. All expert outputs for every input token (row in x) are computed and stacked.\n3. For each token, the indices for the top\u2011k experts (largest gating probabilities) are determined. Their probabilities are renormalized (so they sum to 1), and then each expert\u2019s output is weighted and summed.\n4. The final tensor is rounded to 4 decimals and ultimately reshaped into a list using tolist().\n\nYou can adapt the input shapes if you use a different convention for We or x."}
{"task_id": 126, "completion_id": 0, "completion": "Below is the complete implementation:\n\n--------------------------------------------------\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # X shape is (B, C, H, W)\n    B, C, H, W = X.shape\n    # Verify that channels can be divided into groups\n    assert C % num_groups == 0, \"The number of channels must be divisible by num_groups\"\n    channels_per_group = C // num_groups\n    \n    # Reshape the input to (B, num_groups, channels_per_group, H, W)\n    X_group = X.reshape(B, num_groups, channels_per_group, H, W)\n    \n    # Compute mean and variance over the grouped channels and spatial dimensions\n    mean = X_group.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_group.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize each group\n    X_group_normalized = (X_group - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original dimensions (B, C, H, W)\n    X_normalized = X_group_normalized.reshape(B, C, H, W)\n    \n    # Apply learned scale (gamma) and shift (beta). \n    # We assume gamma and beta are 1D arrays of shape (C,) and can be broadcast to (B, C, H, W)\n    out = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n    \n    # Round all results to the nearest 4th decimal and convert to a python list.\n    rounded_out = np.around(out, decimals=4)\n    return rounded_out.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a dummy tensor of shape (B, C, H, W) = (2, 4, 3, 3)\n    np.random.seed(0)\n    X = np.random.randn(2, 4, 3, 3)\n    # Create dummy gamma and beta of shape (C,)\n    gamma = np.ones(4)\n    beta = np.zeros(4)\n    num_groups = 2\n    \n    result = group_normalization(X, gamma, beta, num_groups)\n    print(result)\n--------------------------------------------------\n\nExplanation:\n1. The input tensor X is reshaped to (B, num_groups, channels_per_group, H, W) so that normalization statistics (mean and variance) are computed over the grouped channels and spatial dimensions.\n2. After normalization, the tensor is reshaped back to (B, C, H, W) and then the learned scale (gamma) and shift (beta) are applied.\n3. Finally, the output is rounded to 4 decimal places with np.around and converted into a Python list using tolist()."}
{"task_id": 127, "completion_id": 0, "completion": "import numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function f(x) and its derivative f'(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n\n    def grad(x):\n        return 4*x**3 - 9*x**2\n\n    x = start_x\n    for _ in range(max_iters):\n        gradient = grad(x)\n        new_x = x - learning_rate * gradient\n\n        # Check for convergence: if step is small enough, break out\n        if abs(new_x - x) < tolerance:\n            x = new_x\n            break\n        \n        x = new_x\n\n    # Return x rounded to the nearest 4th decimal\n    return round(x, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Starting from a random initial point, e.g., start_x = -5.0, 0.0, 5.0 to test various initial positions\n    start_x = np.random.uniform(-10, 10)\n    treasure_x = find_treasure(start_x)\n    print(\"Starting from {:.4f}, the treasure is found at x = {:.4f}\".format(start_x, treasure_x))\n    \n    # For reference, the analytical local minimum of f(x) = x^4 - 3x^3 + 2 is at x = 9/4 = 2.25\n    print(\"Analytical treasure position: x = 2.2500\")"}
{"task_id": 128, "completion_id": 0, "completion": "Below is one acceptable solution. In this implementation we define DyT as a scaled and shifted tanh function. In other words, given an input x, we compute\n\n\u2003\u2003y = \u03b3 \u00b7 tanh(\u03b1 \u00b7 x + \u03b2)\n\nwhich preserves the tanh \u201csquashing\u201d behavior but now with the extra parameters \u03b3 and \u03b2 (replacing the role of layer\u2010normalization\u2019s learned scale and bias). Finally, we round each output value to four decimals and return the result as a Python list.\n\nBelow is the code:\n\n------------------------------------------------\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Compute the dynamic tanh using the formula:\n    #   y = gamma * tanh(alpha * x + beta)\n    # This activation preserves the squashing (nonlinearity)\n    # while incorporating learnable dynamics.\n    y = gamma * np.tanh(alpha * x + beta)\n    \n    # Round each element to the nearest fourth decimal.\n    y_rounded = np.around(y, decimals=4)\n    \n    # In case x is multidimensional, flatten the result and convert to a Python list.\n    return y_rounded.flatten().tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create an example input array.\n    x = np.linspace(-3, 3, 10)\n    # Choose example parameters alpha, gamma, beta.\n    alpha = 1.0\n    gamma = 1.0\n    beta = 0.0\n    \n    output = dynamic_tanh(x, alpha, gamma, beta)\n    print(\"Input:\", x)\n    print(\"Output:\", output)\n------------------------------------------------\n\nExplanation:\n1. The function computes y = gamma * tanh(alpha * x + beta).\n2. np.around rounds every element to 4 decimals.\n3. We flatten the result (if necessary) and convert it to a Python list via the tolist() method.\n4. A short example under the if __name__ == '__main__' block demonstrates how to use the function. \n\nThis implementation meets the task requirements."}
