{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the product of a matrix 'a' and a vector 'b'. \n    Returns:\n        -1 if dimensions are incompatible,\n         a list of dot products (one for each row of 'a') otherwise.\n    \"\"\"\n    if not a:\n        return []\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Returns the transpose of a given matrix 'a'.\n    The matrix is represented as a list of lists.\n    \n    Example:\n        Input: [[1, 2, 3], \n                [4, 5, 6]]\n        Output: [[1, 4], \n                 [2, 5], \n                 [3, 6]]\n    \"\"\"\n    rows = len(a)\n    columns = len(a[0]) if rows > 0 else 0\n    transposed = [[a[row_index][col_index] for row_index in range(rows)] for col_index in range(columns)]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a 2D list 'a' into the specified new_shape (rows, cols) using numpy.\n    If the total number of elements doesn't match new_shape, return an empty list.\n\n    :param a: Original 2D list of integers/floats\n    :param new_shape: Tuple specifying the new shape (rows, cols)\n    :return: Reshaped 2D list (if possible) or an empty list otherwise\n    \"\"\"\n    total_elements = sum((len(row) for row in a))\n    required_elements = new_shape[0] * new_shape[1]\n    if total_elements != required_elements:\n        return []\n    arr = np.array(a)\n    reshaped_arr = arr.reshape(new_shape)\n    return reshaped_arr.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    :param matrix: A list of lists representing the matrix.\n    :param mode: A string specifying the mode, either 'row' or 'column'.\n    :return: A list of mean values, one for each row (if mode='row')\n             or for each column (if mode='column').\n    \"\"\"\n    if not matrix or not isinstance(matrix[0], list):\n        return []\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    if mode == 'column':\n        n_rows = len(matrix)\n        n_cols = len(matrix[0])\n        return [sum((matrix[i][j] for i in range(n_rows))) / n_rows for j in range(n_cols)]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a 2D matrix by a scalar value.\n\n    :param matrix: A list of lists, where each inner list represents a row in the matrix.\n    :param scalar: The scalar (int or float) to multiply each element of the matrix by.\n    :return: A new matrix (list of lists) with elements multiplied by the given scalar.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them in descending order.\n\n    Args:\n        matrix (list[list[float|int]]): A 2x2 matrix represented as a nested list.\n\n    Returns:\n        list[float]: A list of eigenvalues in descending order.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = discriminant ** 0.5\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using T^{-1} * A * S, where T and S are invertible matrices.\n    - First validates if T and S are square and invertible.\n    - If not invertible or shapes are incompatible, returns -1.\n    - Otherwise, computes T^{-1} * A * S, rounds to 4th decimal, and returns as a python list.\n    \"\"\"\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        _ = np.linalg.inv(S_np)\n        result = T_inv @ A_np @ S_np\n        result_rounded = np.round(result, 4)\n        return result_rounded.tolist()\n    except (np.linalg.LinAlgError, ValueError):\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    The matrix is expected to be in the form:\n    [\n        [a, b],\n        [c, d]\n    ]\n    \n    Returns:\n        A 2x2 list of lists representing the inverse matrix,\n        or None if the matrix is not invertible (determinant = 0).\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    return [[d / det, -b / det], [-c / det, a / det]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices a and b (2D lists of numeric types), returning the product.\n    If the number of columns in a does not match the number of rows in b, returns -1.\n  \n    Parameters:\n    a (list[list[int|float]]): The first matrix.\n    b (list[list[int|float]]): The second matrix.\n\n    Returns:\n    list[list[int|float]]: The product of matrix a and matrix b, or -1 if not compatible.\n    \"\"\"\n    if not a or not b:\n        return -1\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            total = 0\n            for k in range(cols_a):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of feature vectors.\n    Each inner list in 'vectors' represents one feature (dimension), \n    and its elements are observations of that feature.\n\n    Args:\n        vectors (list[list[float]]): List of features, where each feature is a list\n                                     of observations (all features must have the same length).\n\n    Returns:\n        list[list[float]]: Covariance matrix of size (num_features x num_features).\n\n    Raises:\n        ValueError: If the features do not have the same number of observations,\n                    or if there are fewer than 2 observations per feature.\n    \"\"\"\n    num_features = len(vectors)\n    if num_features == 0:\n        return []\n    num_observations = len(vectors[0])\n    for feature in vectors:\n        if len(feature) != num_observations:\n            raise ValueError('All features must have the same number of observations.')\n    if num_observations < 2:\n        raise ValueError('At least two observations are required to compute covariance.')\n    means = [sum(feature) / num_observations for feature in vectors]\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(num_features):\n            cov_sum = 0.0\n            for k in range(num_observations):\n                cov_sum += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance_matrix[i][j] = cov_sum / (num_observations - 1)\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    -----------\n    A : np.ndarray\n        Coefficient matrix (2D array).\n    b : np.ndarray\n        Right-hand side vector (1D array).\n    n : int\n        The number of iterations to perform.\n    \n    Returns:\n    --------\n    list\n        The approximate solution vector (1D list), with each entry rounded\n        to four decimal places.\n    \"\"\"\n    A = A.astype(float)\n    b = b.astype(float)\n    num_rows = A.shape[0]\n    x = np.zeros(num_rows, dtype=float)\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(num_rows):\n            sigma = 0.0\n            for j in range(num_rows):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            new_x[i] = (b[i] - sigma) / A[i, i]\n        x = np.round(new_x, decimals=4)\n    return x.reshape(-1).tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix A via the Jacobi method.\n    Returns a tuple of the two singular values, rounded to 4 decimals.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    M = A.T @ A\n    (m11, m12) = (M[0, 0], M[0, 1])\n    (m21, m22) = (M[1, 0], M[1, 1])\n    phi = 0.5 * np.arctan2(2.0 * m12, m11 - m22)\n    c = np.cos(phi)\n    s = np.sin(phi)\n    lam1 = c ** 2 * m11 - 2 * c * s * m12 + s ** 2 * m22\n    lam2 = s ** 2 * m11 + 2 * c * s * m12 + c ** 2 * m22\n    svals = np.array([np.sqrt(abs(lam1)), np.sqrt(abs(lam2))])\n    svals.sort()\n    svals = svals[::-1]\n    svals_rounded = tuple(np.round(svals, 4))\n    return svals_rounded"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Computes the determinant of a 4x4 matrix using Laplace's expansion (cofactor expansion).\n    This function expands along the first row and recursively computes the determinant \n    of 3x3 minors using Laplace's expansion as well.\n\n    :param matrix: A 4x4 matrix represented as a list of lists (elements can be int or float).\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n\n    def get_submatrix(mat: list[list[int | float]], row_to_remove: int, col_to_remove: int) -> list[list[int | float]]:\n        \"\"\"\n        Returns the submatrix obtained by removing the specified row and column.\n        \"\"\"\n        return [[mat[r][c] for c in range(len(mat)) if c != col_to_remove] for r in range(len(mat)) if r != row_to_remove]\n\n    def determinant_2x2(m2: list[list[int | float]]) -> float:\n        \"\"\"Determinant of a 2x2 matrix.\"\"\"\n        return m2[0][0] * m2[1][1] - m2[0][1] * m2[1][0]\n\n    def determinant_3x3(m3: list[list[int | float]]) -> float:\n        \"\"\"\n        Recursively computes the determinant of a 3x3 matrix using Laplace's expansion\n        along the first row.\n        \"\"\"\n        if len(m3) != 3 or any((len(row) != 3 for row in m3)):\n            raise ValueError('Minor matrix must be 3x3 for determinant_3x3.')\n        det_val = 0.0\n        for col in range(3):\n            minor_2x2 = get_submatrix(m3, 0, col)\n            cofactor = (-1) ** (0 + col) * determinant_2x2(minor_2x2)\n            det_val += m3[0][col] * cofactor\n        return det_val\n    det_4x4 = 0.0\n    for col in range(4):\n        minor_3x3 = get_submatrix(matrix, 0, col)\n        cofactor = (-1) ** (0 + col) * determinant_3x3(minor_3x3)\n        det_4x4 += matrix[0][col] * cofactor\n    return det_4x4"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    theta = np.linalg.pinv(X_arr.T @ X_arr) @ (X_arr.T @ y_arr)\n    theta_rounded = [round(coef, 4) for coef in theta]\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept (shape: m x n)\n    y (np.ndarray): Target vector (shape: m,) or (m x 1)\n    alpha (float): Learning rate\n    iterations (int): Number of gradient descent iterations\n    \n    Returns:\n    np.ndarray: Model coefficients (n,) as a Python list, rounded to 4 decimal places.\n                The return value is obtained using numpy.ndarray.tolist().\n    \"\"\"\n    (m, n) = X.shape\n    y = y.reshape(-1, 1)\n    theta = np.zeros((n, 1))\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = 1 / m * (X.T @ errors)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.flatten().tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data (rows = samples, columns = features) using:\n    1. Standardization: (x - mean) / std\n    2. Min-Max Normalization: (x - min) / (max - min)\n    \n    Handles zero std or zero range by substituting with 1 to avoid division by zero.\n    \n    Returns:\n        standardized_list, minmax_list\n        Both are 2D Python lists (rounded to 4 decimal places).\n    \"\"\"\n    means = data.mean(axis=0)\n    stds = data.std(axis=0)\n    stds[stds == 0] = 1\n    standardized_data = (data - means) / stds\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    range_vals = max_vals - min_vals\n    range_vals[range_vals == 0] = 1\n    minmax_data = (data - min_vals) / range_vals\n    standardized_data = np.round(standardized_data, 4)\n    minmax_data = np.round(minmax_data, 4)\n    return (standardized_data.tolist(), minmax_data.tolist())"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-Means clustering algorithm.\n\n    :param points: A list of points, where each point is a tuple of coordinates (e.g., (x, y))\n    :param k: The number of clusters to form\n    :param initial_centroids: A list of initial centroid points, each a tuple of coordinates\n    :param max_iterations: The maximum number of iterations to perform\n    :return: A list of the final centroids of the clusters, where each centroid is rounded to 4 decimals\n    \"\"\"\n    data = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.copy(centroids)\n        for cluster_index in range(k):\n            cluster_points = data[cluster_assignments == cluster_index]\n            if len(cluster_points) > 0:\n                new_centroids[cluster_index] = cluster_points.mean(axis=0)\n        if np.allclose(centroids, new_centroids, atol=1e-07):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    final_centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature dataset.\n    y : np.ndarray\n        Labels or target values.\n    k : int, default=5\n        Number of folds to split the data into.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int or None, default=None\n        Random seed for reproducible shuffling.\n\n    Returns:\n    --------\n    list of tuples (train_index, test_index)\n        Each element is a tuple containing:\n            - train_index: 1D array of indices for training.\n            - test_index: 1D array of indices for testing.\n    \"\"\"\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.default_rng(seed=random_seed)\n        rng.shuffle(indices)\n    folds = np.array_split(indices, k)\n    splits = []\n    for i in range(k):\n        test_index = folds[i]\n        train_index = np.concatenate([fold for (j, fold) in enumerate(folds) if j != i])\n        splits.append((train_index, test_index))\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the input data matrix.\n\n    Parameters:\n    -----------\n    data : np.ndarray\n        A 2D array where each row represents a data sample and each column represents a feature.\n    k : int\n        The number of principal components to return.\n\n    Returns:\n    --------\n    list[list[float]]\n        A 2D list of shape (k, n_features), where each row contains one principal component\n        (an eigenvector of the covariance matrix). Values are rounded to 4 decimal places.\n    \"\"\"\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=1)\n    data_std = (data - means) / stds\n    n_samples = data_std.shape[0]\n    covariance_matrix = data_std.T @ data_std / (n_samples - 1)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    top_k_indices = sorted_indices[:k]\n    principal_components = eigenvectors[:, top_k_indices]\n    principal_components = principal_components.T\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef _entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Compute the entropy of the target attribute in the given examples.\n    \"\"\"\n    value_counts = Counter((example[target_attr] for example in examples))\n    if not examples:\n        return 0.0\n    total = len(examples)\n    entropy_value = 0.0\n    for count in value_counts.values():\n        p = count / total\n        entropy_value -= p * math.log2(p)\n    return entropy_value\ndef _information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Compute the information gain by splitting on `attribute`.\n    IG = entropy(parent) - sum( (|Dj| / |D|) * entropy(Dj) ) over each subset Dj.\n    \"\"\"\n    parent_entropy = _entropy(examples, target_attr)\n    subsets = {}\n    for example in examples:\n        value = example[attribute]\n        subsets.setdefault(value, []).append(example)\n    total = len(examples)\n    weighted_entropy = 0.0\n    for subset_examples in subsets.values():\n        weighted_entropy += len(subset_examples) / total * _entropy(subset_examples, target_attr)\n    return parent_entropy - weighted_entropy\ndef _majority_value(examples: list[dict], target_attr: str):\n    \"\"\"\n    Return the most common value for the target attribute among the provided examples.\n    \"\"\"\n    value_counts = Counter((example[target_attr] for example in examples))\n    return value_counts.most_common(1)[0][0]\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree using recursive splitting based on entropy and information gain.\n\n    :param examples: A list of examples, each a dict of {attribute: value, ...}.\n    :param attributes: List of attribute names (excluding the target).\n    :param target_attr: Name of the target attribute to classify.\n    :return: A nested dictionary representing the learned decision tree.\n    \"\"\"\n    if not examples:\n        return {}\n    first_label = examples[0][target_attr]\n    if all((example[target_attr] == first_label for example in examples)):\n        return first_label\n    if not attributes:\n        return _majority_value(examples, target_attr)\n    gains = [(attribute, _information_gain(examples, attribute, target_attr)) for attribute in attributes]\n    (best_attr, _) = max(gains, key=lambda x: x[1])\n    tree = {best_attr: {}}\n    values = set((example[best_attr] for example in examples))\n    for val in values:\n        subset = [ex for ex in examples if ex[best_attr] == val]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        if not subset:\n            tree[best_attr][val] = _majority_value(examples, target_attr)\n        else:\n            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n            tree[best_attr][val] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Trains a binary SVM classifier using a deterministic (batch) Pegasos algorithm with a specified kernel.\n    \n    Parameters:\n    -----------\n    data        : np.ndarray, shape (n_samples, n_features)\n                  Training data.\n    labels      : np.ndarray, shape (n_samples,)\n                  Labels corresponding to the training data. Must be in {+1, -1}.\n    kernel      : str, optional, default='linear'\n                  Specifies the kernel type. Choices: 'linear', 'rbf'.\n    lambda_val  : float, optional, default=0.01\n                  Regularization parameter.\n    iterations  : int, optional, default=100\n                  Number of batch Pegasos iterations.\n    sigma       : float, optional, default=1.0\n                  Parameter for RBF kernel (\u03c3).\n    \n    Returns:\n    --------\n    alpha_list  : list of lists\n                  Learned dual coefficients, rounded to 4 decimals, reshaped to (n_samples, 1) then converted to list.\n    bias_list   : list of one element\n                  Learned bias term, rounded to 4 decimals and returned as a single-element list.\n    \"\"\"\n    n_samples = data.shape[0]\n    if kernel == 'linear':\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2.0 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        alpha *= 1.0 - 1.0 / t\n        f = (alpha * labels).dot(K) + b\n        mis_idx = np.where(labels * f < 1)[0]\n        if len(mis_idx) > 0:\n            alpha[mis_idx] += 1.0 / (lambda_val * t * n_samples)\n        if len(mis_idx) > 0:\n            b += 1.0 / (lambda_val * t * n_samples) * np.sum(labels[mis_idx])\n    alpha_rounded = np.round(alpha, 4).reshape(-1, 1)\n    b_rounded = float(np.round(b, 4))\n    return (alpha_rounded.tolist(), [b_rounded])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for the input z.\n    Returns the result rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation values for a given list of scores.\n    Each returned value is rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_values = [math.exp(s - max_score) for s in scores]\n    total = sum(exp_values)\n    softmax_values = [round(ev / total, 4) for ev in exp_values]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Parameters:\n    -----------\n    features : list of list of floats\n        Each inner list represents the feature vector for one example.\n    labels : list of ints\n        True binary labels (0 or 1) corresponding to each feature vector.\n    weights : list of floats\n        Weights for each feature.\n    bias : float\n        Bias term for the neuron.\n\n    Returns:\n    --------\n    (predicted_probabilities, mse)\n        predicted_probabilities : python list (shape: n x 1) after reshaping, each element rounded to 4 decimal places\n        mse : float\n            Mean squared error between predicted probabilities and true labels, rounded to 4 decimal places.\n    \"\"\"\n    X = np.array(features, dtype=float)\n    y = np.array(labels, dtype=float)\n    w = np.array(weights, dtype=float)\n    z = np.dot(X, w) + bias\n    predictions = 1.0 / (1.0 + np.exp(-z))\n    mse = np.mean((predictions - y) ** 2)\n    predictions = np.round(predictions, 4)\n    mse = round(mse, 4)\n    predictions = predictions.reshape(-1, 1).tolist()\n    return (predictions, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.astype(float).copy()\n    bias = float(initial_bias)\n    n_samples = features.shape[0]\n    mse_history = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        output = 1.0 / (1.0 + np.exp(-z))\n        errors = output - labels\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        d_sigmoid = output * (1.0 - output)\n        w_grad = np.dot(features.T, errors * d_sigmoid) / n_samples\n        b_grad = np.sum(errors * d_sigmoid) / n_samples\n        weights -= learning_rate * w_grad\n        bias -= learning_rate * b_grad\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = round(bias, 4)\n    weights_list = weights_rounded.tolist()\n    return (weights_list, bias_rounded, mse_history)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_topo(child)\n                topo.append(node)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P that takes coordinates from basis B to basis C in R^3.\n\n    If B and C are each lists of three vectors in R^3, interpreted as rows:\n        B = [\n          [b1x, b1y, b1z],\n          [b2x, b2y, b2z],\n          [b3x, b3y, b3z]\n        ]\n        C = [\n          [c1x, c1y, c1z],\n          [c2x, c2y, c2z],\n          [c3x, c3y, c3z]\n        ]\n\n    Then the columns of each matrix represent the basis vectors. The transformation\n    from B-coordinates to C-coordinates is given by P = C_inv * B, where B and C \n    are the matrices formed by using these vectors as columns.\n\n    :param B: list of list of ints, basis B\n    :param C: list of list of ints, basis C\n    :return: list of list of floats (transformation matrix, rounded to 4 decimals)\n    \"\"\"\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n    C_inv = np.linalg.inv(C_mat)\n    P = C_inv.dot(B_mat)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using the method described:\n      1) Compute A^T A and find its eigen-decomposition A^T A = V \u039b V^T\n      2) Singular values are sqrt of the eigenvalues (\u039b)\n      3) U = A V \u03a3^(-1) (handling zero singular values carefully)\n    Finally, enforce det(U) >= 0 and det(V) >= 0 to keep a standard orientation.\n\n    Returns (U, S, V) as lists of lists (each 2x2) rounded to 4 decimal places,\n    satisfying A \u2248 U S V^T in the usual SVD sense.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    ATA = A.T @ A\n    (eigvals, eigvecs) = np.linalg.eigh(ATA)\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n    s_vals = np.sqrt(np.clip(eigvals, 0, None))\n    V = eigvecs\n    U = np.zeros((2, 2), dtype=np.float64)\n    for i in range(2):\n        if abs(s_vals[i]) > 1e-14:\n            U[:, i] = A @ V[:, i] / s_vals[i]\n        elif i == 0:\n            U[:, i] = [1.0, 0.0]\n        else:\n            col0 = U[:, 0]\n            if np.linalg.norm(col0) > 1e-14:\n                perp = np.array([col0[1], -col0[0]])\n                perp /= np.linalg.norm(perp)\n                U[:, i] = perp\n            else:\n                U[:, i] = [0.0, 1.0]\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n    if np.linalg.det(V) < 0:\n        V[:, 1] = -V[:, 1]\n    S = np.diag(s_vals)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays X and y in unison, maintaining correspondence\n    between their rows. Returns the shuffled data as Python lists.\n\n    Parameters:\n    X (numpy.ndarray): Feature array\n    y (numpy.ndarray): Labels array\n    seed (int, optional): Random seed for reproducibility\n\n    Returns:\n    (list, list): Shuffled X and y converted to Python lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from X and optionally y.\n    Each batch is returned as a Python list after reshaping by using numpy's tolist() method.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Numpy array containing input data.\n    y : np.ndarray, optional\n        Numpy array containing labels (default is None).\n    batch_size : int, optional\n        Number of samples per batch (default is 64).\n    \n    Yields:\n    -------\n    If y is provided, yields (X_batch, y_batch) where both are Python lists.\n    Otherwise, yields X_batch as a Python list.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        x_batch = X[start:end]\n        x_batch_list = x_batch.tolist()\n        if y is not None:\n            y_batch = y[start:end]\n            y_batch_list = y_batch.tolist()\n            yield (x_batch_list, y_batch_list)\n        else:\n            yield x_batch_list"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature\n    is greater than or equal to the given threshold.\n    \n    Parameters:\n    X (numpy.ndarray): The input dataset (2D numpy array).\n    feature_i (int): The index of the feature to split on.\n    threshold (float): The threshold value for the feature.\n    \n    Returns:\n    (list, list): Two lists of samples: the first for samples where the \n    feature_i >= threshold, and the second for samples where the \n    feature_i < threshold.\n    \"\"\"\n    mask = X[:, feature_i] >= threshold\n    X_meet_condition = X[mask]\n    X_not_meet_condition = X[~mask]\n    return (X_meet_condition.tolist(), X_not_meet_condition.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree for the input 2D array X.\n\n    Parameters:\n    ------------\n    X : numpy.ndarray\n        2D array of shape (n_samples, n_features).\n    degree : int\n        The maximum polynomial degree.\n\n    Returns:\n    ------------\n    list\n        Python list of shape (n_samples, number_of_polynomial_features),\n        containing all polynomial features up to the specified degree.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    feature_list = []\n    for d in range(degree + 1):\n        for combo in combinations_with_replacement(range(n_features), d):\n            if len(combo) == 0:\n                new_feature = np.ones(n_samples)\n            else:\n                new_feature = np.prod(X[:, combo], axis=1)\n            feature_list.append(new_feature)\n    return np.array(feature_list).T.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of the dataset (X, y).\n    If replacements is True, sample with replacement; otherwise, sample without replacement.\n\n    :param X: 2D numpy array, shape (n_samples, n_features)\n    :param y: 1D numpy array, shape (n_samples,)\n    :param n_subsets: int, number of random subsets to generate\n    :param replacements: bool, if True use sampling with replacement\n    :param seed: int, random seed for reproducibility\n    \n    :return: list of tuples (X_subset, y_subset), each as Python lists\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.randint(0, n_samples, size=n_samples)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices, :]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding of a 1D numpy array of integer values.\n\n    Parameters:\n    x (numpy.ndarray): 1D array of integer values representing categories.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If None, it is automatically determined from x.\n\n    Returns:\n    list: One-hot encoded representation of x as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n    for (i, val) in enumerate(x):\n        one_hot[i, val] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array x into a diagonal matrix.\n    \n    Parameters:\n    x (numpy.ndarray): 1D input array.\n    \n    Returns:\n    list: 2D python list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates and returns the accuracy score of model predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels\n    y_pred (numpy.ndarray): 1D array of predicted labels\n\n    Returns:\n    float: Accuracy score = (number of correct predictions) / (total number of predictions)\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return float(accuracy)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset X and, optionally,\n    another dataset Y. If Y is not provided, the correlation matrix is computed\n    for X with itself.\n    \n    Args:\n        X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features_X).\n        Y (numpy.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_Y).\n            If None, Y is set to be X.\n    \n    Returns:\n        list: A 2D python list representing the correlation matrix, rounded to 4 decimals.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    Y_centered = Y - Y.mean(axis=0, keepdims=True)\n    cov = X_centered.T @ Y_centered / (X.shape[0] - 1)\n    std_x = X_centered.std(axis=0, ddof=1)\n    std_y = Y_centered.std(axis=0, ddof=1)\n    corr_matrix = cov / np.outer(std_x, std_y)\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fits an AdaBoost classifier using decision stumps for a given dataset X and labels y.\n    \n    Args:\n        X (numpy.ndarray): 2D array of shape (n_samples, n_features)\n        y (numpy.ndarray): 1D array of shape (n_samples,) with labels in {+1, -1}\n        n_clf (int): Number of weak classifiers (decision stumps) to train\n\n    Returns:\n        list: A list of dictionaries, each containing parameters of a learned weak classifier:\n              [\n                {\n                  \"feature\": <feature index>,\n                  \"threshold\": <float, rounded to 4 decimals>,\n                  \"alpha\": <float, rounded to 4 decimals>,\n                  \"direction\": <'lt' or 'gt'>\n                },\n                ...\n              ]\n    \"\"\"\n    if set(np.unique(y)) == {0, 1}:\n        y = np.where(y == 0, -1, 1)\n    (n_samples, n_features) = X.shape\n    w = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    def stump_predict(feature, threshold, direction, X_data):\n        \"\"\"\n        Predict labels (+1 or -1) for a given feature, threshold, and direction.\n        direction == 'lt': predict +1 if x < threshold else -1\n        direction == 'gt': predict +1 if x > threshold else -1\n        \"\"\"\n        if direction == 'lt':\n            return np.where(X_data[:, feature] < threshold, 1, -1)\n        else:\n            return np.where(X_data[:, feature] > threshold, 1, -1)\n    for _ in range(n_clf):\n        (best_feature, best_threshold) = (None, None)\n        best_direction = None\n        best_error = float('inf')\n        best_pred = None\n        for feature in range(n_features):\n            x_vals = np.unique(X[:, feature])\n            if len(x_vals) == 1:\n                thresholds = x_vals\n            else:\n                thresholds = (x_vals[:-1] + x_vals[1:]) / 2.0\n            for threshold in thresholds:\n                for direction in ['lt', 'gt']:\n                    pred = stump_predict(feature, threshold, direction, X)\n                    misclassified = pred != y\n                    error = np.sum(w[misclassified])\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_direction = direction\n                        best_pred = pred\n        eps = 1e-10\n        best_error = max(best_error, eps)\n        best_error = min(best_error, 1 - eps)\n        alpha = 0.5 * np.log((1.0 - best_error) / best_error)\n        w *= np.exp(-alpha * y * best_pred)\n        w /= np.sum(w)\n        classifiers.append({'feature': best_feature, 'threshold': float(np.round(best_threshold, 4)), 'alpha': float(np.round(alpha, 4)), 'direction': best_direction})\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    x = np.array(scores, dtype=np.float64)\n    x_shifted = x - np.max(x)\n    log_sum_exp = np.log(np.sum(np.exp(x_shifted)))\n    log_softmax_vals = x_shifted - log_sum_exp\n    log_softmax_vals = np.round(log_softmax_vals, 4)\n    return log_softmax_vals.reshape(-1, 1).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        \"\"\"\n        1. Initialization:\n           - Define the layer with a specified number of neurons (n_units) and an optional input shape.\n           - Create placeholders for the layer's weights (W), biases (w0), \n             as well as placeholders for possible optimizers (W_opt, w0_opt).\n        \"\"\"\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.layer_input = None\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer=None):\n        \"\"\"\n        2. Weight Initialization:\n           - Initialize W using a uniform distribution with limit = 1 / sqrt(input_shape[0]).\n           - Initialize biases (w0) to zero.\n           - Optionally copy the passed optimizer to W_opt and w0_opt.\n        \"\"\"\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        if optimizer is not None:\n            self.W_opt = copy.deepcopy(optimizer)\n            self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        \"\"\"\n        3. Parameter Count:\n           - Number of trainable parameters = size of W + size of w0.\n        \"\"\"\n        return self.input_shape[0] * self.n_units + self.n_units\n\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        4. Forward Pass:\n           - Compute the layer output as a matrix multiplication between X and W plus the bias w0.\n           - Round to 4 decimal places and return as a Python list.\n        \"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        output = np.round(output, 4)\n        return output.tolist()\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        5. Backward Pass:\n           - Compute gradients with respect to W and w0.\n           - If trainable, update W and w0 using the optimizer's update rule.\n           - Compute and return the gradient wrt. the input (dX).\n           - All returned/updated values should be rounded to 4 decimals, \n             and the gradient returned as a Python list.\n        \"\"\"\n        accum_grad = np.array(accum_grad)\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            if self.W_opt is not None:\n                self.W = self.W_opt.update(self.W, dW)\n            else:\n                self.W -= 0.01 * dW\n            if self.w0_opt is not None:\n                self.w0 = self.w0_opt.update(self.w0, dw0)\n            else:\n                self.w0 -= 0.01 * dw0\n            self.W = np.round(self.W, 4)\n            self.w0 = np.round(self.w0, 4)\n        grad_input = np.dot(accum_grad, self.W.T)\n        grad_input = np.round(grad_input, 4)\n        return grad_input.tolist()\n\n    def output_shape(self):\n        \"\"\"\n        6. Output Shape:\n           - Return the shape of the output = (n_units,).\n        \"\"\"\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Computes a 2D convolution operation on the input matrix using the provided kernel,\n    with specified padding and stride values. The results are rounded to 4 decimal places.\n\n    Parameters:\n        input_matrix (np.ndarray): 2D array representing the input.\n        kernel (np.ndarray): 2D array representing the convolution kernel.\n        padding (int): Number of zero-padding layers added symmetrically.\n        stride (int): Stride step size for the sliding convolution.\n\n    Returns:\n        list: Python list representation of the 2D convolution result,\n              rounded to 4 decimal places.\n    \"\"\"\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n    out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((out_height, out_width), dtype=float)\n    for i in range(out_height):\n        for j in range(out_width):\n            row_start = i * stride\n            col_start = j * stride\n            region = padded_input[row_start:row_start + kernel_height, col_start:col_start + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): 2D feature matrix of shape (n_samples, n_features)\n    w (np.ndarray): 1D array of coefficients of shape (n_features,)\n    y_true (np.ndarray): 1D array of true labels of shape (n_samples,)\n    alpha (float): Regularization parameter\n\n    Returns:\n    float: Ridge loss (MSE + alpha * sum(w^2)), rounded to 4 decimal places\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg = alpha * np.sum(w ** 2)\n    loss = mse + reg\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n    float|int: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    Precision = TP / (TP + FP)\n    \n    Parameters:\n    y_true (np.array): Ground truth binary labels\n    y_pred (np.array): Predicted binary labels\n\n    Returns:\n    float: The precision value\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    return tp / denominator"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent using MSE loss in one of three variants:\n      1) Batch Gradient Descent\n      2) Stochastic Gradient Descent (SGD)\n      3) Mini-Batch Gradient Descent\n\n    Parameters:\n    X             : np.ndarray, shape (n_samples, n_features)\n                   The input data.\n    y             : np.ndarray, shape (n_samples,)\n                   The target values.\n    weights       : np.ndarray, shape (n_features,)\n                   Initial weights for the model.\n    learning_rate : float\n                   Learning rate for the gradient descent updates.\n    n_iterations  : int\n                   Number of iterations (updates) to perform.\n    batch_size    : int, optional (default=1)\n                   Size of each mini-batch (only used if method='mini-batch').\n    method        : str, optional (default='batch')\n                   Specifies which gradient descent variant to use.\n                     'batch'       -> Batch Gradient Descent\n                     'sgd'         -> Stochastic Gradient Descent\n                     'mini-batch'  -> Mini-Batch Gradient Descent\n\n    Returns:\n    A python list of the final weight values, each rounded to the 4th decimal place.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            predictions = X.dot(weights)\n            errors = predictions - y\n            grad = 2 / n_samples * X.T.dot(errors)\n            weights -= learning_rate * grad\n        elif method == 'sgd':\n            idx = np.random.randint(n_samples)\n            (X_i, y_i) = (X[idx], y[idx])\n            prediction = X_i.dot(weights)\n            error = prediction - y_i\n            grad = 2 * X_i * error\n            weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                X_batch = X_shuffled[start:end]\n                y_batch = y_shuffled[start:end]\n                predictions = X_batch.dot(weights)\n                errors = predictions - y_batch\n                grad = 2 / X_batch.shape[0] * X_batch.T.dot(errors)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be one of: 'batch', 'sgd', or 'mini-batch'\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF) and return the result as a Python list.\n    \n    Parameters:\n    matrix (list of lists or numpy array): The input 2D matrix to convert into RREF.\n    \n    Returns:\n    list: The RREF of the input matrix as a nested Python list.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    pivot_row = 0\n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n        pivot = pivot_row\n        while pivot < rows and abs(A[pivot, col]) < 1e-12:\n            pivot += 1\n        if pivot == rows:\n            continue\n        if pivot != pivot_row:\n            A[[pivot_row, pivot], :] = A[[pivot, pivot_row], :]\n        pivot_val = A[pivot_row, col]\n        if abs(pivot_val) > 1e-12:\n            A[pivot_row, :] = A[pivot_row, :] / pivot_val\n        for r in range(rows):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > 1e-12:\n                    A[r, :] = A[r, :] - factor * A[pivot_row, :]\n        pivot_row += 1\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam (Adaptive Moment Estimation) optimization algorithm.\n    \n    Parameters:\n    -----------\n    f : callable\n        The objective function to be optimized.\n    grad : callable\n        A function that computes the gradient of f.\n    x0 : array_like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size (default: 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default: 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default: 10).\n        \n    Returns:\n    --------\n    list\n        The optimized parameters, rounded to the nearest 4 decimals, returned as a Python list.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x, dtype=np.float64)\n    v = np.zeros_like(x, dtype=np.float64)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression (L1) using Gradient Descent and return (weights, bias).\n    \n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target array of shape (n_samples, ).\n        alpha (float): Regularization parameter for L1 penalty.\n        learning_rate (float): The learning rate for gradient descent.\n        max_iter (int): Maximum number of iterations for gradient descent.\n        tol (float): Tolerance for stopping criterion.\n    \n    Returns:\n        (list, float): \n            - list of final weights rounded to 4 decimal places.\n            - bias (float) rounded to 4 decimal places.\n    \"\"\"\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        residual = y_pred - y\n        grad_w_mse = 1.0 / n * X.T.dot(residual)\n        grad_w_l1 = alpha * np.sign(w)\n        grad_w_l1[w == 0] = 0.0\n        grad_w = grad_w_mse + grad_w_l1\n        w_new = w - learning_rate * grad_w\n        grad_b = 1.0 / n * np.sum(residual)\n        b_new = b - learning_rate * grad_b\n        if np.linalg.norm(w_new - w) < tol:\n            w = w_new\n            b = b_new\n            break\n        w = w_new\n        b = b_new\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment distance between two strings,\n    allowing insertions, deletions, substitutions, and transposition of\n    adjacent characters (each at cost 1).\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    return round(tp / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Implements the self-attention mechanism:\n      1) Compute the dot product of Q and K^T.\n      2) Scale by sqrt(d_k) where d_k is the dimension of the key.\n      3) Apply the softmax function to get the attention weights.\n      4) Multiply the attention weights by V to get the output.\n\n    Parameters:\n        Q, K, V (numpy.ndarray): \n            - Shape: (sequence_length, feature_dimension).\n            - sequence_length must be the same for K and V,\n              and feature_dimension must be the same for Q, K, and V.\n\n    Returns:\n        list: The self-attention output after rounding to 4 decimal places\n              and converted to a Python list.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    exps = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exps / np.sum(exps, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    x_seq = np.array(input_sequence, dtype=float)\n    h_prev = np.array(initial_hidden_state, dtype=float)\n    Wx_arr = np.array(Wx, dtype=float)\n    Wh_arr = np.array(Wh, dtype=float)\n    b_arr = np.array(b, dtype=float)\n    for x_t in x_seq:\n        h_prev = np.tanh(np.dot(x_t, Wx_arr) + np.dot(h_prev, Wh_arr) + b_arr)\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=float)\n    ones = np.ones((points_array.shape[0], 1), dtype=float)\n    homogeneous_points = np.hstack((points_array, ones))\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=float)\n    translated_points_homogeneous = homogeneous_points @ translation_matrix.T\n    translated_points = translated_points_homogeneous[:, :2]\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the KL divergence between two univariate normal distributions:\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2).\n\n    Parameters\n    ----------\n    mu_p : float\n        Mean of the first normal distribution (P).\n    sigma_p : float\n        Standard deviation of the first normal distribution (P).\n    mu_q : float\n        Mean of the second normal distribution (Q).\n    sigma_q : float\n        Standard deviation of the second normal distribution (Q).\n\n    Returns\n    -------\n    float\n        The KL divergence D_KL(P || Q).\n    \"\"\"\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    return term1 + term2 + term3"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations A x = b using the Gauss-Seidel method.\n\n    Parameters\n    ----------\n    A : (N, N) array_like\n        Coefficient matrix.\n    b : (N,) array_like\n        Right-hand side vector.\n    n : int\n        Number of iterations.\n    x_ini : (N,) array_like, optional\n        Initial guess. If None, a zero vector will be used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(len(b)):\n            sum_ = b[i]\n            for j in range(len(b)):\n                if j != i:\n                    sum_ -= A[i, j] * x[j]\n            x[i] = sum_ / A[i, i]\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Performs Gaussian elimination with partial pivoting to solve the system A x = b.\n\n    Parameters:\n    -----------\n    A : np.ndarray\n        A 2D square numpy array (coefficient matrix).\n    b : np.ndarray\n        A 1D numpy array (right-hand side vector).\n\n    Returns:\n    --------\n    list\n        Solution vector x as a Python list, values rounded to the 4th decimal.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    for i in range(n):\n        pivot_row = i + np.argmax(np.abs(A[i:, i]))\n        if pivot_row != i:\n            A[[i, pivot_row]] = A[[pivot_row, i]]\n            (b[i], b[pivot_row]) = (b[pivot_row], b[i])\n        if abs(A[i, i]) < 1e-14:\n            raise ValueError('Matrix is singular or nearly singular.')\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        sum_ax = np.dot(A[i, i + 1:], x[i + 1:])\n        x[i] = (b[i] - sum_ax) / A[i, i]\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        All returned values are rounded to the 4th decimal place and converted to Python lists.\n        \n        Args:\n            x (np.ndarray): Input sequence of shape (T, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n\n        Returns:\n            tuple: (hidden_states_list, final_hidden_state_list, final_cell_state_list)\n                   Each element is a Python list version of the numpy array rounded to 4 decimals.\n        \"\"\"\n\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n        T = x.shape[0]\n        ht = initial_hidden_state\n        ct = initial_cell_state\n        hidden_states = []\n        for t in range(T):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((ht, xt))\n            ft = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ot = sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ct = ft * ct + it * c_tilde\n            ht = ot * np.tanh(ct)\n            hidden_states.append(ht.copy())\n        hidden_states_np = np.concatenate(hidden_states, axis=1).T\n        final_hidden_state = ht\n        final_cell_state = ct\n        hidden_states_np = np.round(hidden_states_np, 4)\n        final_hidden_state = np.round(final_hidden_state, 4)\n        final_cell_state = np.round(final_cell_state, 4)\n        hidden_states_list = hidden_states_np.tolist()\n        final_hidden_state_list = final_hidden_state.reshape(-1).tolist()\n        final_cell_state_list = final_cell_state.reshape(-1).tolist()\n        return (hidden_states_list, final_hidden_state_list, final_cell_state_list)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores of the given `query` terms against a `corpus` of documents.\n    \n    :param corpus: List of documents, where each document is a list of words (tokens).\n    :param query: List of words for which TF-IDF scores are to be computed.\n    :return: A list of lists of TF-IDF scores for each document (rows) and each query term (columns).\n    \"\"\"\n    if not corpus:\n        return []\n    if not query:\n        return []\n    N = len(corpus)\n    doc_freq = {term: 0 for term in query}\n    for doc in corpus:\n        unique_words = set(doc)\n        for term in query:\n            if term in unique_words:\n                doc_freq[term] += 1\n    idf = {}\n    for term in query:\n        idf[term] = np.log10((N + 1) / (doc_freq[term] + 1))\n    tf_idf_matrix = []\n    for doc in corpus:\n        doc_len = len(doc)\n        if doc_len == 0:\n            tf_idf_matrix.append([0.0] * len(query))\n            continue\n        word_count = {}\n        for w in doc:\n            word_count[w] = word_count.get(w, 0) + 1\n        row_scores = []\n        for term in query:\n            tf = word_count.get(term, 0) / doc_len\n            row_scores.append(tf * idf[term])\n        tf_idf_matrix.append(row_scores)\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n    if precision == 0 and recall == 0:\n        return 0.0\n    f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x as a list (rounded to 8 decimal places)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x0.astype(float)\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        if np.linalg.norm(r) < tol:\n            break\n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        current_count = len(values)\n        for (col_index, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of floats/ints)\n    :param L: The line vector defining the direction of projection (list of floats/ints)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    dot_v_L = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_L_L = sum((Li * Li for Li in L))\n    if abs(dot_L_L) < 1e-15:\n        return [0.0] * len(L)\n    scalar = dot_v_L / dot_L_L\n    projection = [round(scalar * Li, 3) for Li in L]\n    return projection"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n    \n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row_indices, col_ptr)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        col_ptr[col] = len(values)\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n    col_ptr[num_cols] = len(values)\n    return (values, row_indices, col_ptr)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space (image) of matrix A using row echelon form.\n    The columns corresponding to pivot positions from the original matrix\n    form a basis for the column space.\n\n    Parameters:\n        A (numpy.ndarray): A 2D array (matrix).\n\n    Returns:\n        list: A 2D list whose columns are the basis vectors for the column\n              space of A, rounded to 8 decimal places.\n    \"\"\"\n    A = A.astype(float)\n    (n_rows, n_cols) = A.shape\n    A_copy = A.copy()\n    pivot_cols = []\n    pivot_row = 0\n    for col in range(n_cols):\n        if pivot_row >= n_rows:\n            break\n        max_row = pivot_row + np.argmax(np.abs(A_copy[pivot_row:, col]))\n        if abs(A_copy[max_row, col]) < 1e-15:\n            continue\n        if max_row != pivot_row:\n            A_copy[[pivot_row, max_row]] = A_copy[[max_row, pivot_row]]\n        pivot_cols.append(col)\n        for r in range(pivot_row + 1, n_rows):\n            if abs(A_copy[r, col]) > 1e-15:\n                factor = A_copy[r, col] / A_copy[pivot_row, col]\n                A_copy[r, col:] -= factor * A_copy[pivot_row, col:]\n        pivot_row += 1\n        if pivot_row == n_rows:\n            break\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n    return basis.tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute R-squared (coefficient of determination).\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,)\n    - y_pred: array-like of shape (n_samples,)\n\n    Returns:\n    - float: R-squared value rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    sst = np.sum((y_true - np.mean(y_true)) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - sse / sst\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    if not img[0]:\n        return -1\n    row_length = len(img[0])\n    total_sum = 0\n    total_count = 0\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_sum += pixel\n            total_count += 1\n    average_brightness = total_sum / total_count\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Computes the Root Mean Square Error (RMSE) between the actual values y_true \n    and the predicted values y_pred.\n\n    Parameters:\n    y_true (array-like): True/actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    TypeError: If inputs are not array-like (numpy array, list, or tuple).\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    \"\"\"\n    if not isinstance(y_true, (np.ndarray, list, tuple)) or not isinstance(y_pred, (np.ndarray, list, tuple)):\n        raise TypeError('y_true and y_pred must be array-like (numpy arrays, lists, or tuples).')\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must match.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary numpy arrays.\n    \n    Parameters:\n    -----------\n    y_true : numpy.ndarray\n        The ground truth binary labels.\n    y_pred : numpy.ndarray\n        The predicted binary labels.\n    \n    Returns:\n    --------\n    float\n        The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice Score between two binary arrays y_true and y_pred.\n\n    Parameters:\n    -----------\n    y_true : array-like of shape (n_samples,)\n        Ground truth (correct) labels as a binary array.\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels as a binary array.\n\n    Returns:\n    --------\n    float\n        Dice Score, a value between 0 and 1, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given row using Hyperdimensional Computing.\n\n    Args:\n        row (dict): A dictionary representing a dataset row, \n                    where keys are feature names and values are their corresponding values.\n        dim (int): Dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary mapping feature names to their reproducible random seeds.\n    \n    Returns:\n        list: The composite hypervector (as a list) representing the entire row.\n    \"\"\"\n\n    def stable_hash(value):\n        s = str(value)\n        h = 0\n        for char in s:\n            h = h * 31 + ord(char) & 4294967295\n        return h\n    composite_sum = np.zeros(dim, dtype=int)\n    n_features = len(row)\n    for (feature, value) in row.items():\n        np.random.seed(random_seeds[feature])\n        feature_hv = np.random.choice([0, 1], size=dim)\n        value_seed = random_seeds[feature] + stable_hash(value) & 4294967295\n        np.random.seed(value_seed)\n        value_hv = np.random.choice([0, 1], size=dim)\n        bound_hv = np.logical_xor(feature_hv, value_hv).astype(int)\n        composite_sum += bound_hv\n    threshold = n_features / 2.0\n    composite_hv = (composite_sum >= threshold).astype(int)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification from the given data.\n\n    Parameters:\n    data (List[List[int]]): A list where each element is [y_true, y_pred].\n\n    Returns:\n    List[List[int]]: A 2x2 confusion matrix represented as [[TN, FP], [FN, TP]].\n    \"\"\"\n    counter = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    TN = counter[0, 0]\n    FP = counter[0, 1]\n    FN = counter[1, 0]\n    TP = counter[1, 1]\n    return [[TN, FP], [FN, TP]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors must not be empty.')\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Vectors must not have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    return round(cos_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Computes performance metrics for a binary classification problem, including:\n    - Confusion Matrix (2x2)\n    - Accuracy\n    - F1 Score\n    - Specificity\n    - Negative Predictive Value\n\n    Parameters:\n    -----------\n    actual : list[int]\n        The actual class labels (1 for positive, 0 for negative).\n    predicted : list[int]\n        The predicted class labels (1 for positive, 0 for negative).\n\n    Returns:\n    --------\n    tuple:\n        A tuple containing:\n        - confusion_matrix: list[list[int]] (2x2 matrix)\n        - accuracy: float (rounded to three decimal places)\n        - f1_score: float (rounded to three decimal places)\n        - specificity: float (rounded to three decimal places)\n        - negative_predictive_value: float (rounded to three decimal places)\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"Length of 'actual' and 'predicted' must be the same.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be 0 or 1.\")\n    TP = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    TN = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    FP = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    FN = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    precision = TP / (TP + FP) if TP + FP != 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN != 0 else 0.0\n    if precision + recall != 0:\n        f1_score = 2 * precision * recall / (precision + recall)\n    else:\n        f1_score = 0.0\n    specificity = TN / (TN + FP) if TN + FP != 0 else 0.0\n    negative_predictive_value = TN / (TN + FN) if TN + FN != 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Returns a dictionary of descriptive statistics for the given dataset:\n    mean, median, mode, variance, standard deviation, 25th percentile,\n    50th percentile, 75th percentile, and interquartile range (IQR).\n    All numeric results are rounded to four decimal places. The mode \n    is returned as a single number if unique, or as a list if multiple modes.\n    \"\"\"\n    data = np.array(data, dtype=float)\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    (unique_vals, counts) = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    all_modes = unique_vals[counts == max_count]\n    if len(all_modes) == 1:\n        mode_val = all_modes[0]\n    else:\n        mode_val = all_modes.tolist()\n    variance_val = np.var(data, ddof=1)\n    std_val = np.std(data, ddof=1)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr = p75 - p25\n    mean_val = round(mean_val, 4)\n    median_val = round(median_val, 4)\n    variance_val = round(variance_val, 4)\n    std_val = round(std_val, 4)\n    p25 = round(p25, 4)\n    p50 = round(p50, 4)\n    p75 = round(p75, 4)\n    iqr = round(iqr, 4)\n    if isinstance(mode_val, float):\n        mode_val = round(mode_val, 4)\n    else:\n        mode_val = [round(m, 4) for m in mode_val]\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val, 'variance': variance_val, 'standard_deviation': std_val, '25th_percentile': p25, '50th_percentile': p50, '75th_percentile': p75, 'interquartile_range': iqr}"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    combinations = math.comb(n, k)\n    probability = combinations * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    two_pi = 2 * math.pi\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(two_pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError('k must be a non-negative integer.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image by taking the difference\n    between the maximum and the minimum pixel values.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image \n                             with pixel values between 0 and 255.\n\n    Returns:\n        int or float: The contrast value (max pixel - min pixel).\n    \"\"\"\n    if len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D array.')\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D numpy arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must have the same number of elements.')\n    return float(np.dot(vec1, vec2))"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial\n                           features of the corresponding data point. Each value is\n                           rounded to 8 decimal places. If degree is less than 0,\n                           an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        transformed_row = []\n        for d in range(degree + 1):\n            transformed_row.append(round(x ** d, 8))\n        transformed_data.append(transformed_row)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos = np.arange(position)[:, np.newaxis]\n    dim = np.arange(d_model)[np.newaxis, :]\n    angle_rates = pos / np.power(10000, 2 * (dim // 2) / np.float32(d_model))\n    pos_enc = np.zeros((position, d_model), dtype=np.float32)\n    pos_enc[:, 0::2] = np.sin(angle_rates[:, 0::2])\n    pos_enc[:, 1::2] = np.cos(angle_rates[:, 1::2])\n    return pos_enc.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value (scalar or np.ndarray)\n    :param grad: Current gradient (scalar or np.ndarray)\n    :param m: First moment estimate (scalar or np.ndarray)\n    :param v: Second moment estimate (scalar or np.ndarray)\n    :param t: Current timestep (integer)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter_list, updated_m_list, updated_v_list)\n             Each returned array is rounded to 5 decimals and converted to a list.\n    \"\"\"\n    parameter = np.atleast_1d(parameter)\n    grad = np.atleast_1d(grad)\n    m = np.atleast_1d(m)\n    v = np.atleast_1d(v)\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    parameter_new = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter_new = np.round(parameter_new, 5).tolist()\n    m_new = np.round(m_new, 5).tolist()\n    v_new = np.round(v_new, 5).tolist()\n    return (parameter_new, m_new, v_new)"}
{"task_id": 88, "completion_id": 0, "solution": "def load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generate text using a simplified GPT-2-like process:\n      1. Convert prompt to token IDs (token + positional embeddings).\n      2. Pass through a tiny \"Transformer block\" with:\n         - Single-head self-attention.\n         - Feed-forward network.\n         - Layer normalization.\n      3. Compute logits for the next token and sample from them.\n      4. Repeat for the requested number of tokens.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    token_ids = encoder.encode(prompt)\n\n    def layer_norm(x, g, b, eps=1e-05):\n        \"\"\"Applies layer normalization to tensor x.\"\"\"\n        mean = x.mean(axis=-1, keepdims=True)\n        var = x.var(axis=-1, keepdims=True)\n        return g * (x - mean) / np.sqrt(var + eps) + b\n    embed_dim = params['wte'].shape[1]\n    Q_w = np.random.randn(embed_dim, embed_dim)\n    K_w = np.random.randn(embed_dim, embed_dim)\n    V_w = np.random.randn(embed_dim, embed_dim)\n    ff_w1 = np.random.randn(embed_dim, embed_dim * 4)\n    ff_b1 = np.random.randn(embed_dim * 4)\n    ff_w2 = np.random.randn(embed_dim * 4, embed_dim)\n    ff_b2 = np.random.randn(embed_dim)\n    proj_w = np.random.randn(embed_dim, 3)\n    proj_b = np.random.randn(3)\n\n    def single_head_self_attention(x):\n        \"\"\"\n        A single-head self-attention over the sequence x.\n        x shape: [seq_len, embed_dim].\n        \"\"\"\n        q = x @ Q_w\n        k = x @ K_w\n        v = x @ V_w\n        d_k = np.sqrt(k.shape[-1])\n        scores = q @ k.transpose() / d_k\n        seq_len = x.shape[0]\n        causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n        scores[causal_mask] = -1000000000.0\n        att_weights = np.exp(scores - scores.max(axis=-1, keepdims=True))\n        att_weights /= att_weights.sum(axis=-1, keepdims=True)\n        return att_weights @ v\n\n    def feed_forward(h):\n        \"\"\"A tiny feed-forward network.\"\"\"\n        h1 = np.maximum(0, h @ ff_w1 + ff_b1)\n        return h1 @ ff_w2 + ff_b2\n\n    def transformer_block(x):\n        \"\"\"\n        A minimal, single-layer Transformer block:\n          - self-attention\n          - add & layer norm\n          - feed-forward\n          - add & layer norm\n        \"\"\"\n        attn_out = single_head_self_attention(x)\n        x = layer_norm(x + attn_out, g=params['ln_f']['g'], b=params['ln_f']['b'])\n        ff_out = feed_forward(x)\n        x = layer_norm(x + ff_out, g=params['ln_f']['g'], b=params['ln_f']['b'])\n        return x\n\n    def logits_for_next_token(x):\n        \"\"\"\n        Given the final hidden state for each token,\n        produce logits for next-token prediction\n        using a linear projection to the vocabulary size (3).\n        We'll use the hidden state of the last position.\n        \"\"\"\n        last_token_hidden = x[-1]\n        return last_token_hidden @ proj_w + proj_b\n\n    def sample_from_logits(logits):\n        \"\"\"Sample from the given logits (3 possible tokens).\"\"\"\n        probs = np.exp(logits - np.max(logits))\n        probs = probs / probs.sum()\n        return np.random.choice(len(probs), p=probs)\n    for _ in range(n_tokens_to_generate):\n        if len(token_ids) > n_ctx:\n            token_ids = token_ids[-n_ctx:]\n        x = []\n        for (i, tid) in enumerate(token_ids):\n            x.append(params['wte'][tid] + params['wpe'][i])\n        x = np.array(x)\n        hidden_states = transformer_block(x)\n        logits = logits_for_next_token(hidden_states)\n        next_token_id = sample_from_logits(logits)\n        token_ids.append(next_token_id)\n    generated_text = encoder.decode(token_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        shifted = values - np.max(values)\n        exp_vals = np.exp(shifted)\n        return exp_vals / np.sum(exp_vals)\n    crystals = np.array(crystal_values, dtype=float)\n    sqrt_d = np.sqrt(dimension)\n    att_score = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            att_score[i, j] = sqrt_d * (crystals[i] * crystals[j])\n    result = []\n    for i in range(n):\n        row_attention = softmax(att_score[i])\n        weighted_sum = 0.0\n        for j in range(n):\n            weighted_sum += row_attention[j] * crystals[j]\n        final_value = dimension * weighted_sum\n        result.append(round(final_value, 4))\n    return result"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query against a given corpus.\n\n    Parameters:\n    -----------\n    corpus : list of str\n        The corpus, where each element is a document (string).\n    query : str\n        The query string.\n    k1 : float, optional\n        Term frequency saturation parameter in BM25.\n    b : float, optional\n        Document length normalization parameter in BM25.\n\n    Returns:\n    --------\n    list of float\n        A list containing the BM25 score for each document, rounded to three decimals.\n    \"\"\"\n    tokenized_corpus = [doc.split() for doc in corpus]\n    query_terms = query.split()\n    N = len(tokenized_corpus)\n    doc_word_counts = []\n    df_counts = Counter()\n    for doc_tokens in tokenized_corpus:\n        counts = Counter(doc_tokens)\n        doc_word_counts.append(counts)\n        for term in set(doc_tokens):\n            df_counts[term] += 1\n    doc_lengths = [sum(counts.values()) for counts in doc_word_counts]\n    avg_doc_length = np.mean(doc_lengths) if doc_lengths else 0.0\n\n    def idf(term):\n        df = df_counts.get(term, 0)\n        return np.log((N - df + 0.5) / (df + 0.5) + 1)\n    scores = []\n    for (i, counts) in enumerate(doc_word_counts):\n        score = 0.0\n        doc_len = doc_lengths[i]\n        for term in query_terms:\n            f_td = counts.get(term, 0)\n            if f_td > 0:\n                numerator = f_td * (k1 + 1)\n                denominator = f_td + k1 * (1 - b + b * (doc_len / avg_doc_length))\n                score += idf(term) * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels (binary classification).\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            tp += 1\n        elif true_label == 0 and pred_label == 1:\n            fp += 1\n        elif true_label == 1 and pred_label == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * precision * recall / (precision + recall)\n    return round(f1_score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = list(range(1, 11))\n    detrended = []\n    for i in days:\n        fluctuation = 10 * math.sin(2 * PI * i / 10.0)\n        detrended_value = consumption_data[i - 1] - fluctuation\n        detrended.append(detrended_value)\n    n = len(days)\n    x_mean = sum(days) / n\n    y_mean = sum(detrended) / n\n    numerator = sum(((x - x_mean) * (y - y_mean) for (x, y) in zip(days, detrended)))\n    denominator = sum(((x - x_mean) ** 2 for x in days))\n    slope = numerator / denominator if denominator != 0 else 0.0\n    intercept = y_mean - slope * x_mean\n    day_15_base = intercept + slope * 15\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10.0)\n    day_15_prediction = day_15_base + fluctuation_15\n    day_15_rounded = round(day_15_prediction)\n    final_forecast = math.ceil(day_15_rounded * 1.05)\n    return final_forecast"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute scaled dot-product attention:\n      scores = QK^T / sqrt(d_k)\n      weights = softmax(scores)\n      output = weights @ V\n    Q, K, V shapes: (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = weights @ V\n    return output\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Split Q, K, V into multiple heads, apply self_attention for each head,\n    then concatenate the results. Q, K, V shapes: (m, d). \n    We assume d is divisible by n_heads.\n    \"\"\"\n    (m, d) = Q.shape\n    d_head = d // n_heads\n    Q_heads = Q.reshape(m, n_heads, d_head)\n    K_heads = K.reshape(m, n_heads, d_head)\n    V_heads = V.reshape(m, n_heads, d_head)\n    outputs = []\n    for i in range(n_heads):\n        Q_i = Q_heads[:, i, :]\n        K_i = K_heads[:, i, :]\n        V_i = V_heads[:, i, :]\n        head_out = self_attention(Q_i, K_i, V_i)\n        outputs.append(head_out)\n    return np.concatenate(outputs, axis=-1)"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n        x (list[int]): A list of binary values (0 or 1).\n        y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n        float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n_11 = n_10 = n_01 = n_00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n_11 += 1\n        elif xi == 1 and yi == 0:\n            n_10 += 1\n        elif xi == 0 and yi == 1:\n            n_01 += 1\n        elif xi == 0 and yi == 0:\n            n_00 += 1\n    n_1_ = n_11 + n_10\n    n_0_ = n_01 + n_00\n    n__1 = n_11 + n_01\n    n__0 = n_10 + n_00\n    numerator = n_11 * n_00 - n_10 * n_01\n    denominator = (n_1_ * n_0_ * n__1 * n__0) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    softplus(x) = log(1 + e^x)\n\n    This function handles edge cases to prevent numerical overflow or underflow\n    by using a piecewise approach.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value, rounded to 4 decimal places.\n    \"\"\"\n    if x < 0:\n        result = math.log1p(math.exp(x))\n    else:\n        result = x + math.log1p(math.exp(-x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value (rounded to 6 decimals).\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_ref = np.array(pi_theta_ref, dtype=np.float64)\n    unclipped_obj = rhos * A\n    clipped_obj = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon) * A\n    clipped_advantage = np.minimum(unclipped_obj, clipped_obj)\n    clipped_advantage_sum = np.sum(clipped_advantage)\n    pi_theta_unnormalized = rhos * pi_old\n    normalization = np.sum(pi_theta_unnormalized)\n    pi_theta = pi_theta_unnormalized / (normalization + 1e-10)\n    eps = 1e-10\n    kl_div = np.sum(pi_theta * np.log((pi_theta + eps) / (pi_ref + eps)))\n    objective = clipped_advantage_sum - beta * kl_div\n    return float(np.round(objective, 6))"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function: x * sigmoid(x)\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid_x\n    return round(swish_val, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * math.expm1(x)\n    return round(selu_value, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a Python list\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-logits))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, C) = X.shape\n    M = np.max(y) + 1\n    Y = np.eye(M)[y]\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exps = np.exp(scores)\n        probs = exps / np.sum(exps, axis=1, keepdims=True)\n        loss = -np.sum(Y * np.log(probs + 1e-10)) / N\n        losses.append(loss)\n        grad = X.T @ (probs - Y) / N\n        B -= learning_rate * grad\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(val, 4) for val in losses]\n    return (B_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters \n    with Binary Cross Entropy loss.\n    All returned values are rounded to the 4th decimal place.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Labels array of shape (n_samples, ).\n    learning_rate : float\n        Step size for gradient descent updates.\n    iterations : int\n        Number of gradient descent steps.\n    \n    Returns:\n    --------\n    tuple[list[float], list[float]]\n        A tuple of:\n        - the final fitted parameters as a list of floats\n        - the history of loss values over iterations as a list of floats\n    \"\"\"\n    y = y.flatten()\n    X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n    (n_samples, n_features) = X_ext.shape\n    params = np.zeros(n_features, dtype=float)\n    epsilon = 1e-10\n    loss_values = []\n    for _ in range(iterations):\n        z = X_ext @ params\n        y_pred = 1 / (1 + np.exp(-z))\n        loss = -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n        loss_values.append(round(loss, 4))\n        grad = 1 / n_samples * (X_ext.T @ (y_pred - y))\n        params -= learning_rate * grad\n    params = np.round(params, 4).tolist()\n    return (params, loss_values)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    Q, K, V: shape (seq_length, d_model) or similar\n    mask: shape (seq_length, seq_length) \n          with 1's for valid positions and 0's (or False) for masked positions\n    Returns attention outputs as a list (seq_length, d_model).\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    large_negative = -1000000000.0\n    scores = np.where(mask > 0, scores, large_negative)\n    scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    scores_exp_sum = np.sum(scores_exp, axis=-1, keepdims=True)\n    attention_weights = scores_exp / (scores_exp_sum + 1e-09)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Disorder is measured using the Shannon entropy of the color distribution.\n    The disorder value is 0 if all apples are the same color, and increases with more variety.\n    Results are rounded to the nearest 4th decimal.\n    \"\"\"\n    from math import log2\n    if len(apples) <= 1:\n        return 0.0\n    color_counts = {}\n    for color in apples:\n        color_counts[color] = color_counts.get(color, 0) + 1\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total_apples\n        entropy -= p * log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Layer Normalization on the given 3D tensor X with shape \n    (batch_size, sequence_length, feature_dim).\n    \n    Normalizes X across the last dimension (feature dimension) for each element \n    in the batch and sequence, then applies the supplied scaling (gamma) and \n    shifting (beta) parameters.\n\n    The output is rounded to 5 decimal places and returned as a nested list.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scale parameter of shape (feature_dim,).\n    beta (np.ndarray): Shift parameter of shape (feature_dim,).\n    epsilon (float): A small constant to avoid division by zero.\n\n    Returns:\n    list: A Python list containing the normalized and scaled data, \n          rounded to 5 decimal places.\n    \"\"\"\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized * gamma + beta\n    X_out = np.round(X_normalized, 5).tolist()\n    return X_out"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for a single reference and candidate translation.\n\n    Parameters:\n    reference (str or list): Reference translation (either a tokenized list or space-delimited string).\n    candidate (str or list): Candidate translation (either a tokenized list or space-delimited string).\n    alpha (float): Weight parameter (default: 0.9).\n    beta (float): Penalty exponent (default: 3).\n    gamma (float): Penalty coefficient (default: 0.5).\n\n    Returns:\n    float: METEOR score rounded to three decimal places.\n    \"\"\"\n    if isinstance(reference, str):\n        reference_tokens = reference.strip().split()\n    else:\n        reference_tokens = reference\n    if isinstance(candidate, str):\n        candidate_tokens = candidate.strip().split()\n    else:\n        candidate_tokens = candidate\n    ref_counts = Counter(reference_tokens)\n    cand_counts = Counter(candidate_tokens)\n    common_words = set(ref_counts.keys()) & set(cand_counts.keys())\n    matches = sum((min(ref_counts[w], cand_counts[w]) for w in common_words))\n    if matches == 0:\n        return 0.0\n    precision = matches / len(candidate_tokens)\n    recall = matches / len(reference_tokens)\n    f_mean = precision * recall / ((1 - alpha) * precision + alpha * recall)\n    ref_positions = {}\n    for (i, w) in enumerate(reference_tokens):\n        ref_positions.setdefault(w, []).append(i)\n    ref_counts_for_alignment = Counter(ref_counts)\n    aligned_positions = []\n    for w in candidate_tokens:\n        if ref_counts_for_alignment[w] > 0:\n            aligned_positions.append(ref_positions[w][0])\n            ref_positions[w].pop(0)\n            ref_counts_for_alignment[w] -= 1\n    chunks = 1\n    for i in range(1, len(aligned_positions)):\n        if aligned_positions[i] != aligned_positions[i - 1] + 1:\n            chunks += 1\n    penalty = gamma * (chunks / matches) ** beta\n    penalty = min(penalty, 1.0)\n    meteor = (1 - penalty) * f_mean\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events X and Y.\n\n    PMI(X, Y) = log2( p(X,Y) / ( p(X) * p(Y) ) )\n              = log2( (joint_counts / total_samples) / \n                      ((total_counts_x / total_samples) * (total_counts_y / total_samples)) )\n\n    :param joint_counts: Number of times X and Y jointly occur\n    :param total_counts_x: Number of times X occurs\n    :param total_counts_y: Number of times Y occurs\n    :param total_samples: Total number of samples\n    :return: PMI value rounded to 3 decimal places (float)\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return float(round(pmi, 3))"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n    \n    Returns a list of floats, each rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = []\n    for val in x:\n        norm_val = (val - min_val) / (max_val - min_val)\n        normalized.append(round(norm_val, 4))\n    return normalized"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple residual block that takes a 1D input array x,\n    applies two dense layers with ReLU activations, adds\n    the original input x (shortcut), and applies a final ReLU.\n    \n    The final output is rounded to 4 decimal places and\n    returned as a list.\n    \"\"\"\n    relu = lambda z: np.maximum(0, z)\n    z1 = x @ w1\n    a1 = relu(z1)\n    z2 = a1 @ w2\n    a2 = relu(z2)\n    out = relu(a2 + x)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    The input shape is (height, width, channels).\n    The returned 1D array has shape (channels,), \n    where each element is the average of all values \n    in the corresponding feature map.\n    \"\"\"\n    assert len(x.shape) == 3, 'Input should be a 3D array (height, width, channels).'\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D input (BCHW).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width).\n    gamma : np.ndarray\n        Scale parameters (one per channel).\n    beta : np.ndarray\n        Shift parameters (one per channel).\n    epsilon : float, optional\n        A small value added to variance for numerical stability. Default is 1e-5.\n\n    Returns:\n    --------\n    list\n        The batch-normalized array rounded to 4 decimal places, converted to a Python list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    gamma_reshaped = gamma.reshape((1, -1, 1, 1))\n    beta_reshaped = beta.reshape((1, -1, 1, 1))\n    out = gamma_reshaped * X_norm + beta_reshaped\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term (c * x^n) at the point x.\n    \n    :param c: Coefficient of the polynomial term\n    :param x: The point at which to evaluate the derivative\n    :param n: Exponent of the polynomial term\n    :return: The derivative value rounded to 4 decimal places\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by the given 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors, e.g. [[x1, y1], [x2, y2], ...].\n    tol (float): Tolerance for determining linear independence.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (each rounded to 4 decimals) \n                       that spans the same subspace as 'vectors'.\n    \"\"\"\n    arrs = [np.array(vec, dtype=float) for vec in vectors]\n    basis = []\n    for v in arrs:\n        w = v.copy()\n        for b in basis:\n            w -= np.dot(w, b) * b\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            basis.append(w / norm_w)\n    return [np.round(vec, 4).tolist() for vec in basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a, dtype=float)\n    b = np.array(b, dtype=float)\n    cross_prod = np.cross(a, b)\n    cross_prod_rounded = np.round(cross_prod, 4)\n    return cross_prod_rounded.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    detA = np.linalg.det(A)\n    if np.isclose(detA, 0.0):\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        detA_i = np.linalg.det(A_i)\n        x_val = detA_i / detA\n        x.append(x_val)\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n    bc = sum((np.sqrt(pi * qi) for (pi, qi) in zip(p, q)))\n    if bc <= 0:\n        return 0.0\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Returns the element-wise sum of two vectors (lists).\n    If the vectors have different lengths, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Implements the REINFORCE policy gradient estimator.\n    \n    The policy is parameterized by theta, which has shape (num_states, num_actions).\n    We compute a softmax over each row theta[s, :] to get the action probabilities\n    for state s. For each episode and time step, we multiply the gradient of the log\n    probability of the taken action by the return from that time step onward. Then we\n    average across all episodes.\n\n    Parameters:\n    -----------\n    theta    : np.ndarray\n               Array of shape (num_states, num_actions), the policy parameters.\n    episodes : list of lists\n               Each element is an episode, which is a list of (state, action, reward) tuples.\n\n    Returns:\n    --------\n    list\n        The averaged gradient (same shape as theta), rounded to 4 decimals, returned as a Python list.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    grad = np.zeros_like(theta)\n\n    def softmax(x):\n        shifted_x = x - np.max(x)\n        exp_x = np.exp(shifted_x)\n        return exp_x / np.sum(exp_x)\n    for episode in episodes:\n        T = len(episode)\n        returns = np.zeros(T)\n        G = 0.0\n        for t in reversed(range(T)):\n            G += episode[t][2]\n            returns[t] = G\n        for t in range(T):\n            (s, a, _) = episode[t]\n            p = softmax(theta[s])\n            for a_prime in range(num_actions):\n                indicator = 1.0 if a_prime == a else 0.0\n                grad[s, a_prime] += returns[t] * (indicator - p[a_prime])\n    grad /= len(episodes)\n    grad = np.round(grad, 4)\n    return grad.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Computes the FLOPs of a large dense layer vs. a Mixture-of-Experts layer\n    (as in \"Outrageously Large Neural Networks: The Sparsely-Gated\n    Mixture-of-Experts Layer\") and returns the percentage savings.\n\n    Assumes:\n      - The \"dense\" baseline has dimensions [d_in, n_experts * d_out]\n        to match the total capacity of n_experts experts.\n      - The MoE layer has n_experts experts, each [d_in, d_out] in size,\n        but only k_active experts are used per input.\n\n    Parameters:\n      n_experts (int): number of experts in the MoE layer\n      k_active  (int): number of active (selected) experts per input\n      d_in      (int): input dimension\n      d_out     (int): output dimension (per expert)\n\n    Returns:\n      dict: {\n         'dense_flops': float (one-decimal),\n         'moe_flops': float (one-decimal),\n         'savings_percent': float (one-decimal)\n      }\n    \"\"\"\n    dense_flops = d_in * (n_experts * d_out)\n    moe_flops = d_in * (k_active * d_out)\n    savings_percent = 100.0 * (dense_flops - moe_flops) / dense_flops\n    return {'dense_flops': round(dense_flops, 1), 'moe_flops': round(moe_flops, 1), 'savings_percent': round(savings_percent, 1)}"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements Noisy Top-K gating used in Mixture-of-Experts.\n\n    Args:\n        X (np.ndarray): Input of shape (batch_size, input_dim).\n        W_g (np.ndarray): Gating weights of shape (input_dim, num_experts).\n        W_noise (np.ndarray): Noise weights of shape (input_dim, num_experts).\n        N (np.ndarray): Pre-sampled noise of shape (batch_size, num_experts).\n        k (int): Sparsity constraint - number of active experts per sample.\n\n    Returns:\n        list: A list (converted from np.ndarray) containing the gating probabilities\n              of shape (batch_size, num_experts). Each row sums to 1 among its\n              top-k positions; other positions are 0.\n    \"\"\"\n    base_logits = X.dot(W_g)\n    noise_logits = X.dot(W_noise) * N\n    gating_logits = base_logits + noise_logits\n    (batch_size, num_experts) = gating_logits.shape\n    gating_probs = np.zeros((batch_size, num_experts), dtype=np.float32)\n    for i in range(batch_size):\n        row = gating_logits[i]\n        topk_indices = np.argsort(row)[-k:]\n        topk_logits = row[topk_indices]\n        shifted_logits = topk_logits - np.max(topk_logits)\n        exp_logits = np.exp(shifted_logits)\n        denom = np.sum(exp_logits)\n        gating_probs[i, topk_indices] = exp_logits / denom\n    gating_probs = np.round(gating_probs, 4)\n    return gating_probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture of Experts (MoE) layer.\n    \n    Parameters:\n    -----------\n    x : np.ndarray\n        Input data of shape (batch_size, d_in).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, d_in, d_out).\n    Wg : np.ndarray\n        Gating weight matrix of shape (d_in, n_experts).\n    n_experts : int\n        Number of experts.\n    top_k : int\n        Number of top experts to select per token.\n    \n    Returns:\n    --------\n    List[List[float]]\n        The output of the MoE layer, rounded to 4 decimal places,\n        converted to a nested Python list.\n    \"\"\"\n    gating_logits = x @ Wg\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    stable_logits = gating_logits - max_logits\n    exp_logits = np.exp(stable_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    batch_size = x.shape[0]\n    d_out = We.shape[2]\n    output = np.zeros((batch_size, d_out))\n    for i in range(batch_size):\n        top_indices = np.argpartition(gating_probs[i], -top_k)[-top_k:]\n        for e in top_indices:\n            expert_out = x[i] @ We[e]\n            output[i] += gating_probs[i, e] * expert_out\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor X with shape (B, C, H, W).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input data of shape (B, C, H, W).\n    gamma : np.ndarray\n        Scale parameter of shape (C,).\n    beta : np.ndarray\n        Shift parameter of shape (C,).\n    num_groups : int\n        Number of channel groups to split into.\n    epsilon : float\n        A small constant for numerical stability.\n\n    Returns:\n    --------\n    List (Python list):\n        The normalized output, converted to a Python list\n        after rounding to 4 decimal places.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    group_size = C // num_groups\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_scaled = X_normalized * gamma.reshape(1, C, 1, 1) + beta.reshape(1, C, 1, 1)\n    X_rounded = np.round(X_scaled, 4)\n    return X_rounded.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x that minimizes\n    f(x) = x^4 - 3x^3 + 2.\n    Returns the result rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def derivative(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        grad = derivative(current_x)\n        next_x = current_x - learning_rate * grad\n        if abs(next_x - current_x) < tolerance:\n            break\n        current_x = next_x\n    return round(current_x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, a normalization-free transformation\n    inspired by the Tanh function. This function replaces layer normalization in\n    Transformer architectures while preserving squashing behavior and enabling stable training.\n\n    Args:\n        x (np.ndarray): Input array.\n        alpha (float): Scaling parameter.\n        gamma (float): Gain parameter.\n        beta (float): Shift parameter.\n\n    Returns:\n        list[float]: Python list of the output array after applying the DyT function,\n                     rounded to 4 decimal places.\n    \"\"\"\n    y = alpha * np.tanh(gamma * x + beta)\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()"}
