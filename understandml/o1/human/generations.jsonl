{"task_id": 1, "completion_id": 0, "completion": "def matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the product of a matrix 'a' and a vector 'b'. \n    Returns:\n        -1 if dimensions are incompatible,\n         a list of dot products (one for each row of 'a') otherwise.\n    \"\"\"\n    if not a:\n        # If the matrix is empty, we can consider it 0 x m. \n        # Multiplying it by any vector can be interpreted \n        # to yield an empty list (0-dimensional result).\n        return []\n\n    # Verify column count of 'a' matches length of 'b'\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Returns the transpose of a given matrix 'a'.\n    The matrix is represented as a list of lists.\n    \n    Example:\n        Input: [[1, 2, 3], \n                [4, 5, 6]]\n        Output: [[1, 4], \n                 [2, 5], \n                 [3, 6]]\n    \"\"\"\n    # The number of rows in 'a' is len(a)\n    # The number of columns in 'a' is len(a[0]) (assuming a is not empty)\n    rows = len(a)\n    columns = len(a[0]) if rows > 0 else 0\n    \n    # Build a new 2D list where its dimensions are swapped\n    # We iterate over columns to form the rows of the transposed matrix\n    transposed = [\n        [a[row_index][col_index] for row_index in range(rows)]\n        for col_index in range(columns)\n    ]\n    \n    return transposed\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape a 2D list 'a' into the specified new_shape (rows, cols) using numpy.\n    If the total number of elements doesn't match new_shape, return an empty list.\n\n    :param a: Original 2D list of integers/floats\n    :param new_shape: Tuple specifying the new shape (rows, cols)\n    :return: Reshaped 2D list (if possible) or an empty list otherwise\n    \"\"\"\n    # Calculate the total number of elements in the original list\n    total_elements = sum(len(row) for row in a)\n    \n    # Calculate the total number of elements required by new_shape\n    required_elements = new_shape[0] * new_shape[1]\n    \n    # If the total elements do not match, return an empty list\n    if total_elements != required_elements:\n        return []\n    \n    # Convert to numpy array and reshape\n    arr = np.array(a)\n    reshaped_arr = arr.reshape(new_shape)\n    \n    # Convert back to a standard Python list\n    return reshaped_arr.tolist()\n```\n"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    :param matrix: A list of lists representing the matrix.\n    :param mode: A string specifying the mode, either 'row' or 'column'.\n    :return: A list of mean values, one for each row (if mode='row')\n             or for each column (if mode='column').\n    \"\"\"\n    # Check for empty matrix or invalid mode\n    if not matrix or not isinstance(matrix[0], list):\n        return []\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n\n    # Compute means by rows\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n\n    # Compute means by columns\n    # Assuming all rows have the same number of columns\n    if mode == 'column':\n        n_rows = len(matrix)\n        n_cols = len(matrix[0])\n        # sum_col_j across all rows, then divide by n_rows\n        return [\n            sum(matrix[i][j] for i in range(n_rows)) / n_rows\n            for j in range(n_cols)\n        ]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of a 2D matrix by a scalar value.\n\n    :param matrix: A list of lists, where each inner list represents a row in the matrix.\n    :param scalar: The scalar (int or float) to multiply each element of the matrix by.\n    :return: A new matrix (list of lists) with elements multiplied by the given scalar.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them in descending order.\n\n    Args:\n        matrix (list[list[float|int]]): A 2x2 matrix represented as a nested list.\n\n    Returns:\n        list[float]: A list of eigenvalues in descending order.\n    \"\"\"\n    # Unpack the elements of the 2x2 matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the terms needed for the characteristic equation\n    trace = a + d\n    determinant = a * d - b * c\n\n    # Calculate the eigenvalues using the quadratic formula for \u03bb\u00b2 - (a+d)\u03bb + (ad - bc) = 0\n    discriminant = (trace ** 2) - 4 * determinant\n    sqrt_discriminant = discriminant ** 0.5\n\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n\n    # Sort in descending order\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], \n                     T: list[list[int|float]], \n                     S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms matrix A using T^{-1} * A * S, where T and S are invertible matrices.\n    - First validates if T and S are square and invertible.\n    - If not invertible or shapes are incompatible, returns -1.\n    - Otherwise, computes T^{-1} * A * S, rounds to 4th decimal, and returns as a python list.\n    \"\"\"\n    try:\n        # Convert inputs to numpy arrays\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        \n        # Check if T and S are square\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        \n        # Attempt to find inverses for T and S to confirm invertibility\n        # (We only actually need T^{-1} for the computation, but verifying S is invertible as well)\n        T_inv = np.linalg.inv(T_np)\n        _ = np.linalg.inv(S_np)  # Just to check invertibility of S\n        \n        # Compute transformation T^{-1} A S\n        # If there's a shape mismatch in multiplication, this will raise ValueError\n        result = T_inv @ A_np @ S_np\n        \n        # Round the result to 4 decimal places\n        result_rounded = np.round(result, 4)\n        \n        # Convert back to python list\n        return result_rounded.tolist()\n    \n    except (np.linalg.LinAlgError, ValueError):\n        # np.linalg.LinAlgError: raised if T or S is not invertible or shape mismatch\n        # ValueError: raised if matrices can't be multiplied due to dimension mismatch\n        return -1\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    The matrix is expected to be in the form:\n    [\n        [a, b],\n        [c, d]\n    ]\n    \n    Returns:\n        A 2x2 list of lists representing the inverse matrix,\n        or None if the matrix is not invertible (determinant = 0).\n    \"\"\"\n    # Extract elements\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate determinant\n    det = a * d - b * c\n    \n    # Check if determinant is zero (not invertible)\n    if det == 0:\n        return None\n    \n    # Calculate inverse\n    return [\n        [ d / det, -b / det],\n        [-c / det,  a / det]\n    ]\n```\n"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int|float]],\n              b: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies two matrices a and b (2D lists of numeric types), returning the product.\n    If the number of columns in a does not match the number of rows in b, returns -1.\n  \n    Parameters:\n    a (list[list[int|float]]): The first matrix.\n    b (list[list[int|float]]): The second matrix.\n\n    Returns:\n    list[list[int|float]]: The product of matrix a and matrix b, or -1 if not compatible.\n    \"\"\"\n    if not a or not b:\n        # If either matrix is empty, define \"not compatible\" for safety\n        return -1\n\n    # Number of rows & columns in a\n    rows_a = len(a)\n    cols_a = len(a[0])\n\n    # Number of rows & columns in b\n    rows_b = len(b)\n    cols_b = len(b[0])\n\n    # Check if a can be multiplied by b\n    if cols_a != rows_b:\n        return -1\n\n    # Initialize result matrix with zeros (dimensions rows_a x cols_b)\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n\n    # Perform multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            # Calculate the single entry result[i][j]\n            total = 0\n            for k in range(cols_a):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n\n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of feature vectors.\n    Each inner list in 'vectors' represents one feature (dimension), \n    and its elements are observations of that feature.\n\n    Args:\n        vectors (list[list[float]]): List of features, where each feature is a list\n                                     of observations (all features must have the same length).\n\n    Returns:\n        list[list[float]]: Covariance matrix of size (num_features x num_features).\n\n    Raises:\n        ValueError: If the features do not have the same number of observations,\n                    or if there are fewer than 2 observations per feature.\n    \"\"\"\n    # Number of features:\n    num_features = len(vectors)\n    if num_features == 0:\n        return []\n\n    # Validate all features have the same number of observations\n    num_observations = len(vectors[0])\n    for feature in vectors:\n        if len(feature) != num_observations:\n            raise ValueError(\"All features must have the same number of observations.\")\n    if num_observations < 2:\n        raise ValueError(\"At least two observations are required to compute covariance.\")\n\n    # Calculate the mean of each feature\n    means = [sum(feature) / num_observations for feature in vectors]\n\n    # Initialize a matrix to hold the covariance results\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n\n    # Compute the covariance for each pair of features\n    for i in range(num_features):\n        for j in range(num_features):\n            cov_sum = 0.0\n            for k in range(num_observations):\n                cov_sum += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            # Sample covariance uses (N - 1) in the denominator\n            covariance_matrix[i][j] = cov_sum / (num_observations - 1)\n\n    return covariance_matrix\n\nif __name__ == \"__main__\":\n    # -------------------------------------------------------------------------\n    # Test Case 1: Simple 2-feature example\n    # -------------------------------------------------------------------------\n    # Feature 1: [1, 2, 3]\n    # Feature 2: [2, 4, 6]\n    # The two features are perfectly correlated (feature2 = 2 * feature1).\n    # Let's check if the covariance reflects that relationship.\n    vectors_2 = [\n        [1, 2, 3],\n        [2, 4, 6]\n    ]\n    print(\"Test Case 1: 2-feature perfect correlation\")\n    cov_matrix_2 = calculate_covariance_matrix(vectors_2)\n    print(\"Covariance Matrix:\", cov_matrix_2)\n    # Expected covariance > 0 and consistent with perfect correlation\n\n    # -------------------------------------------------------------------------\n    # Test Case 2: Three features with 4 observations each\n    # -------------------------------------------------------------------------\n    vectors_3 = [\n        [2, 4, 6, 8],     # Feature 1\n        [1, 3, 5, 7],     # Feature 2\n        [10, 12, 13, 15]  # Feature 3\n    ]\n    print(\"\\nTest Case 2: 3-feature example\")\n    cov_matrix_3 = calculate_covariance_matrix(vectors_3)\n    print(\"Covariance Matrix:\", cov_matrix_3)\n\n    # -------------------------------------------------------------------------\n    # Test Case 3: Verify correctness against numpy\n    # (Requires numpy; uncomment the following test if you wish to check equality)\n    # -------------------------------------------------------------------------\n    \"\"\"\n    import numpy as np\n    np_array = np.array(vectors_3)\n    numpy_cov = np.cov(np_array, ddof=1).tolist()\n    print(\"\\nNumpy Covariance Matrix:\", numpy_cov)\n\n    # Compare each element\n    for i in range(len(cov_matrix_3)):\n        for j in range(len(cov_matrix_3)):\n            assert abs(cov_matrix_3[i][j] - numpy_cov[i][j]) < 1e-9, \\\n                f\"Mismatch at element ({i}, {j})\"\n    print(\"Covariance matrix matches NumPy's result within numerical tolerance.\")\n    \"\"\"\n    \n    # -------------------------------------------------------------------------\n    # Test Case 4: Edge case - only two observations per feature\n    # -------------------------------------------------------------------------\n    # This is still valid, but it is the smallest number of observations that\n    # can compute a sample covariance (n-1 in the denominator).\n    print(\"\\nTest Case 4: Edge case with two observations\")\n    vectors_2obs = [\n        [5, 7],\n        [10, 14]\n    ]\n    cov_matrix_2obs = calculate_covariance_matrix(vectors_2obs)\n    print(\"Covariance Matrix:\", cov_matrix_2obs)\n\n    # -------------------------------------------------------------------------\n    # Test Case 5: Invalid input example - Mismatch in number of observations\n    # -------------------------------------------------------------------------\n    # This should raise ValueError\n    print(\"\\nTest Case 5: Mismatched number of observations (should raise ValueError)\")\n    try:\n        invalid_vectors = [\n            [1, 2, 3],\n            [4, 5]  # fewer observations\n        ]\n        _ = calculate_covariance_matrix(invalid_vectors)\n    except ValueError as e:\n        print(\"Caught expected ValueError:\", e)\n```\n\nExplanation of Key Steps:\n1. Validation:\n   \u2022 Checks whether each feature has the same number of observations.  \n   \u2022 Ensures there are at least two observations (otherwise, sample covariance is undefined).\n\n2. Mean Calculation:\n   \u2022 Calculates the mean for each feature.\n\n3. Covariance Computation:\n   \u2022 For each pair of features (i, j), sums up the product of deviations from their respective means.  \n   \u2022 Divides by (n \u2212 1) to get the sample covariance.\n\n4. Testing:\n   \u2022 Several test cases are provided, including a verification (commented out) that compares against NumPy\u2019s covariance function."}
{"task_id": 11, "completion_id": 0, "completion": "import numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    -----------\n    A : np.ndarray\n        Coefficient matrix (2D array).\n    b : np.ndarray\n        Right-hand side vector (1D array).\n    n : int\n        The number of iterations to perform.\n    \n    Returns:\n    --------\n    list\n        The approximate solution vector (1D list), with each entry rounded\n        to four decimal places.\n    \"\"\"\n    # Convert inputs to float for safety\n    A = A.astype(float)\n    b = b.astype(float)\n    \n    # Number of equations\n    num_rows = A.shape[0]\n    \n    # Initialize the solution vector x with zeros\n    x = np.zeros(num_rows, dtype=float)\n    \n    # Perform n Jacobi iterations\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        \n        for i in range(num_rows):\n            # Sum over all j != i\n            sigma = 0.0\n            for j in range(num_rows):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            \n            # Update the i-th variable\n            new_x[i] = (b[i] - sigma) / A[i, i]\n        \n        # Round each component of the solution to 4 decimal places\n        x = np.round(new_x, decimals=4)\n    \n    # Return the solution as a Python list\n    return x.reshape(-1).tolist()"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix A via the Jacobi method.\n    Returns a tuple of the two singular values, rounded to 4 decimals.\n    \"\"\"\n    # Safety check that A is 2x2:\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n\n    # Step 1: Form the symmetric matrix M = A^T * A\n    M = A.T @ A\n\n    # Extract elements for convenience\n    m11, m12 = M[0, 0], M[0, 1]\n    m21, m22 = M[1, 0], M[1, 1]\n\n    # The off-diagonal entries of M should be m12 = m21\n    # Jacobi rotation angle\n    # Use arctan2 for better numerical stability.\n    # phi = 1/2 * arctan(2*m12 / (m11 - m22)), but we can use arctan2 in Python\n    phi = 0.5 * np.arctan2(2.0 * m12, m11 - m22)\n\n    # Cosine and sine of rotation\n    c = np.cos(phi)\n    s = np.sin(phi)\n\n    # Step 2: Apply the rotation to diagonalize M\n    # M' = R^T * M * R where R is the Jacobi rotation\n    # For a 2x2, the diagonal elements of M' are directly computed:\n    # M'[0,0] and M'[1,1] are the eigenvalues of M\n    lam1 = c**2 * m11 - 2*c*s*m12 + s**2 * m22\n    lam2 = s**2 * m11 + 2*c*s*m12 + c**2 * m22\n\n    # Step 3: The singular values are the sqrt of these eigenvalues\n    # (M = A^T A is positive semi-definite, so lambdas should be >= 0)\n    # Sort them in descending order just for convention\n    svals = np.array([np.sqrt(abs(lam1)), np.sqrt(abs(lam2))])\n    svals.sort()\n    svals = svals[::-1]  # descending\n\n    # Round to 4 decimals\n    svals_rounded = tuple(np.round(svals, 4))\n\n    return svals_rounded\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Computes the determinant of a 4x4 matrix using Laplace's expansion (cofactor expansion).\n    This function expands along the first row and recursively computes the determinant \n    of 3x3 minors using Laplace's expansion as well.\n\n    :param matrix: A 4x4 matrix represented as a list of lists (elements can be int or float).\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n\n    # Safety check: validate that 'matrix' is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix.\")\n\n    def get_submatrix(mat: list[list[int|float]], row_to_remove: int, col_to_remove: int) -> list[list[int|float]]:\n        \"\"\"\n        Returns the submatrix obtained by removing the specified row and column.\n        \"\"\"\n        return [\n            [mat[r][c] for c in range(len(mat)) if c != col_to_remove]\n            for r in range(len(mat)) if r != row_to_remove\n        ]\n    \n    def determinant_2x2(m2: list[list[int|float]]) -> float:\n        \"\"\"Determinant of a 2x2 matrix.\"\"\"\n        return m2[0][0] * m2[1][1] - m2[0][1] * m2[1][0]\n\n    def determinant_3x3(m3: list[list[int|float]]) -> float:\n        \"\"\"\n        Recursively computes the determinant of a 3x3 matrix using Laplace's expansion\n        along the first row.\n        \"\"\"\n        # Safety check: validate that 'm3' is 3x3\n        if len(m3) != 3 or any(len(row) != 3 for row in m3):\n            raise ValueError(\"Minor matrix must be 3x3 for determinant_3x3.\")\n\n        det_val = 0.0\n        for col in range(3):\n            # Build the 2x2 submatrix\n            minor_2x2 = get_submatrix(m3, 0, col)\n            # Cofactor includes the sign factor (-1)^(row+col)\n            cofactor = ((-1) ** (0 + col)) * determinant_2x2(minor_2x2)\n            # Add contribution to determinant\n            det_val += m3[0][col] * cofactor\n        return det_val\n\n    # Now compute the 4x4 determinant by expanding along the first row\n    det_4x4 = 0.0\n    for col in range(4):\n        # Build the 3x3 submatrix (minor) after removing row 0, column col\n        minor_3x3 = get_submatrix(matrix, 0, col)\n        # Cofactor with sign\n        cofactor = ((-1) ** (0 + col)) * determinant_3x3(minor_3x3)\n        # Update determinant\n        det_4x4 += matrix[0][col] * cofactor\n\n    return det_4x4\n```\n\nExplanation of the key parts:\n1. We first check that the input matrix is indeed 4x4.  \n2. The helper function get_submatrix is used to form the minor matrix by deleting the specified row and column.  \n3. determinant_2x2 computes the 2x2 determinant directly by the standard formula.  \n4. determinant_3x3 expands along the first row (Laplace's expansion) and recursively calls determinant_2x2 on each minor.  \n5. Finally, determinant_4x4 calls determinant_3x3 to handle the minors in its Laplace expansion."}
{"task_id": 14, "completion_id": 0, "completion": "import numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert Python lists to NumPy arrays\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=float)\n\n    # Compute the normal equation: theta = (X^T X)^-1 X^T y\n    # Use pinv (pseudo-inverse) to handle potential singularity\n    theta = np.linalg.pinv(X_arr.T @ X_arr) @ (X_arr.T @ y_arr)\n    \n    # Round each coefficient to 4 decimal places, allowing -0.0 if applicable\n    theta_rounded = [round(coef, 4) for coef in theta]\n\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "completion": "import numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept (shape: m x n)\n    y (np.ndarray): Target vector (shape: m,) or (m x 1)\n    alpha (float): Learning rate\n    iterations (int): Number of gradient descent iterations\n    \n    Returns:\n    np.ndarray: Model coefficients (n,) as a Python list, rounded to 4 decimal places.\n                The return value is obtained using numpy.ndarray.tolist().\n    \"\"\"\n    m, n = X.shape\n    # Ensure y is a column vector\n    y = y.reshape(-1, 1)  \n    \n    # Initialize parameters (coefficients) to zeros\n    theta = np.zeros((n, 1))\n    \n    for _ in range(iterations):\n        # Compute the predictions\n        predictions = X @ theta\n        \n        # Compute the error\n        errors = predictions - y\n        \n        # Compute the gradient\n        gradient = (1 / m) * (X.T @ errors)\n        \n        # Update the parameters\n        theta -= alpha * gradient\n    \n    # Round the resulting coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Flatten and convert to list\n    return theta_rounded.flatten().tolist()"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data (rows = samples, columns = features) using:\n    1. Standardization: (x - mean) / std\n    2. Min-Max Normalization: (x - min) / (max - min)\n    \n    Handles zero std or zero range by substituting with 1 to avoid division by zero.\n    \n    Returns:\n        standardized_list, minmax_list\n        Both are 2D Python lists (rounded to 4 decimal places).\n    \"\"\"\n    # Calculate mean and standard deviation per feature\n    means = data.mean(axis=0)\n    stds = data.std(axis=0)\n    \n    # Prevent division by zero in standardization\n    stds[stds == 0] = 1\n    \n    # Standardize the data\n    standardized_data = (data - means) / stds\n    \n    # Calculate min and max per feature\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    \n    # Prevent division by zero in min-max normalization\n    range_vals = max_vals - min_vals\n    range_vals[range_vals == 0] = 1\n    \n    # Min-Max normalize the data\n    minmax_data = (data - min_vals) / range_vals\n    \n    # Round to 4 decimal places\n    standardized_data = np.round(standardized_data, 4)\n    minmax_data = np.round(minmax_data, 4)\n    \n    # Convert to Python lists\n    return standardized_data.tolist(), minmax_data.tolist()\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], \n                       k: int, \n                       initial_centroids: list[tuple[float, float]], \n                       max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-Means clustering algorithm.\n\n    :param points: A list of points, where each point is a tuple of coordinates (e.g., (x, y))\n    :param k: The number of clusters to form\n    :param initial_centroids: A list of initial centroid points, each a tuple of coordinates\n    :param max_iterations: The maximum number of iterations to perform\n    :return: A list of the final centroids of the clusters, where each centroid is rounded to 4 decimals\n    \"\"\"\n\n    # Convert the points and initial centroids to NumPy arrays\n    data = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n\n    for _ in range(max_iterations):\n        # Compute the distance of each point to each centroid\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        # Assign each point to the cluster with the nearest centroid\n        cluster_assignments = np.argmin(distances, axis=1)\n\n        new_centroids = np.copy(centroids)\n        \n        # Update each centroid based on the mean of points assigned to it\n        for cluster_index in range(k):\n            # Find the points that belong to this cluster\n            cluster_points = data[cluster_assignments == cluster_index]\n            # If there are points assigned to this cluster, update the centroid\n            if len(cluster_points) > 0:\n                new_centroids[cluster_index] = cluster_points.mean(axis=0)\n\n        # Check if centroids have effectively stopped changing (within a small threshold)\n        if np.allclose(centroids, new_centroids, atol=1e-7):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # Round the centroid coordinates to 4 decimals and convert back to list of tuples\n    final_centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    return final_centroids\n```"}
{"task_id": 18, "completion_id": 0, "completion": "import numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature dataset.\n    y : np.ndarray\n        Labels or target values.\n    k : int, default=5\n        Number of folds to split the data into.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int or None, default=None\n        Random seed for reproducible shuffling.\n\n    Returns:\n    --------\n    list of tuples (train_index, test_index)\n        Each element is a tuple containing:\n            - train_index: 1D array of indices for training.\n            - test_index: 1D array of indices for testing.\n    \"\"\"\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n\n    # Shuffle the indices if requested\n    if shuffle:\n        rng = np.random.default_rng(seed=random_seed)\n        rng.shuffle(indices)\n\n    # Split indices into k roughly equal-sized folds\n    folds = np.array_split(indices, k)\n\n    # Create a list to hold (train_index, test_index) for each fold\n    splits = []\n    for i in range(k):\n        test_index = folds[i]\n        # Concatenate all other folds to create the training set\n        train_index = np.concatenate([fold for j, fold in enumerate(folds) if j != i])\n        splits.append((train_index, test_index))\n\n    return splits"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the input data matrix.\n\n    Parameters:\n    -----------\n    data : np.ndarray\n        A 2D array where each row represents a data sample and each column represents a feature.\n    k : int\n        The number of principal components to return.\n\n    Returns:\n    --------\n    list[list[float]]\n        A 2D list of shape (k, n_features), where each row contains one principal component\n        (an eigenvector of the covariance matrix). Values are rounded to 4 decimal places.\n    \"\"\"\n\n    # 1. Standardize the data\n    #   Subtract the mean and divide by the standard deviation for each feature\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=1)  # ddof=1 for sample standard deviation\n    data_std = (data - means) / stds\n\n    # 2. Compute the covariance matrix\n    #   The shape of data_std is (n_samples, n_features),\n    #   so the covariance matrix will have shape (n_features, n_features).\n    n_samples = data_std.shape[0]\n    covariance_matrix = (data_std.T @ data_std) / (n_samples - 1)\n\n    # 3. Find eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # 4. Sort eigenvalues in descending order, and sort eigenvectors accordingly\n    sorted_indices = np.argsort(eigenvalues)[::-1]  # descending order\n    top_k_indices = sorted_indices[:k]\n\n    # 5. Select the top k eigenvectors (principal components)\n    #   Each column of 'eigenvectors' is an eigenvector, so we gather the columns of interest\n    principal_components = eigenvectors[:, top_k_indices]\n\n    # 6. For clarity, we return principal components as a (k, n_features) matrix.\n    #    Each row becomes one eigenvector.\n    principal_components = principal_components.T  # shape: (k, n_features)\n\n    # 7. Round the results to the 4th decimal place and convert to a list of lists\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()\n```\n"}
{"task_id": 20, "completion_id": 0, "completion": "import math\nfrom collections import Counter\n\ndef _entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Compute the entropy of the target attribute in the given examples.\n    \"\"\"\n    # Count how many times each class (target_attr value) appears\n    value_counts = Counter(example[target_attr] for example in examples)\n\n    # If there are no examples, entropy is 0\n    if not examples:\n        return 0.0\n\n    # Calculate entropy = -sum( p_i * log2(p_i) )\n    total = len(examples)\n    entropy_value = 0.0\n    for count in value_counts.values():\n        p = count / total\n        entropy_value -= p * math.log2(p)\n    return entropy_value\n\n\ndef _information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Compute the information gain by splitting on `attribute`.\n    IG = entropy(parent) - sum( (|Dj| / |D|) * entropy(Dj) ) over each subset Dj.\n    \"\"\"\n    # Entropy before splitting\n    parent_entropy = _entropy(examples, target_attr)\n\n    # Group examples by the value of the chosen attribute\n    subsets = {}\n    for example in examples:\n        value = example[attribute]\n        subsets.setdefault(value, []).append(example)\n\n    # Calculate weighted sum of entropies\n    total = len(examples)\n    weighted_entropy = 0.0\n    for subset_examples in subsets.values():\n        weighted_entropy += (len(subset_examples) / total) * _entropy(subset_examples, target_attr)\n\n    # Information Gain\n    return parent_entropy - weighted_entropy\n\n\ndef _majority_value(examples: list[dict], target_attr: str):\n    \"\"\"\n    Return the most common value for the target attribute among the provided examples.\n    \"\"\"\n    value_counts = Counter(example[target_attr] for example in examples)\n    return value_counts.most_common(1)[0][0]\n\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree using recursive splitting based on entropy and information gain.\n\n    :param examples: A list of examples, each a dict of {attribute: value, ...}.\n    :param attributes: List of attribute names (excluding the target).\n    :param target_attr: Name of the target attribute to classify.\n    :return: A nested dictionary representing the learned decision tree.\n    \"\"\"\n    # Base cases\n    if not examples:\n        # No examples, return an empty tree or None\n        return {}\n\n    # If all examples have the same class label, return that label\n    first_label = examples[0][target_attr]\n    if all(example[target_attr] == first_label for example in examples):\n        return first_label\n\n    # If there are no remaining attributes to split on, return the majority class\n    if not attributes:\n        return _majority_value(examples, target_attr)\n\n    # 1. Choose the attribute with the highest information gain for the split\n    gains = [(attribute, _information_gain(examples, attribute, target_attr)) for attribute in attributes]\n    best_attr, _ = max(gains, key=lambda x: x[1])\n\n    # Create the tree structure. For convenience, represent as {best_attr: {value: subtree}}\n    tree = {best_attr: {}}\n\n    # 2. Split the dataset by each possible value of the best attribute and recurse\n    #    (This is a multiway split \u2014 if you'd like strictly \"binary\" splits, you'd need more logic\n    #     to choose thresholds for continuous attributes or partition sets for discrete attributes.)\n    values = set(example[best_attr] for example in examples)\n    for val in values:\n        # Examples that match this value for the best_attr\n        subset = [ex for ex in examples if ex[best_attr] == val]\n\n        # Remove the chosen attribute from the remaining list\n        remaining_attrs = [a for a in attributes if a != best_attr]\n\n        if not subset:\n            # If no examples for this value, assign the majority label of the original set\n            tree[best_attr][val] = _majority_value(examples, target_attr)\n        else:\n            # Recurse on the subset\n            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n            tree[best_attr][val] = subtree\n\n    return tree"}
{"task_id": 21, "completion_id": 0, "completion": "import numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, \n                       labels: np.ndarray, \n                       kernel='linear', \n                       lambda_val=0.01, \n                       iterations=100, \n                       sigma=1.0):\n    \"\"\"\n    Trains a binary SVM classifier using a deterministic (batch) Pegasos algorithm with a specified kernel.\n    \n    Parameters:\n    -----------\n    data        : np.ndarray, shape (n_samples, n_features)\n                  Training data.\n    labels      : np.ndarray, shape (n_samples,)\n                  Labels corresponding to the training data. Must be in {+1, -1}.\n    kernel      : str, optional, default='linear'\n                  Specifies the kernel type. Choices: 'linear', 'rbf'.\n    lambda_val  : float, optional, default=0.01\n                  Regularization parameter.\n    iterations  : int, optional, default=100\n                  Number of batch Pegasos iterations.\n    sigma       : float, optional, default=1.0\n                  Parameter for RBF kernel (\u03c3).\n    \n    Returns:\n    --------\n    alpha_list  : list of lists\n                  Learned dual coefficients, rounded to 4 decimals, reshaped to (n_samples, 1) then converted to list.\n    bias_list   : list of one element\n                  Learned bias term, rounded to 4 decimals and returned as a single-element list.\n    \"\"\"\n    \n    # Number of samples\n    n_samples = data.shape[0]\n    \n    # ---------------------\n    # Build the Kernel Matrix\n    # ---------------------\n    if kernel == 'linear':\n        # Linear Kernel: K(x_i, x_j) = x_i dot x_j\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        # RBF Kernel: K(x_i, x_j) = exp(-||x_i - x_j||^2 / (2*sigma^2))\n        # Build the full kernel matrix of shape (n_samples, n_samples)\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2.0 * sigma**2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n    \n    # ---------------------\n    # Initialize alpha and bias\n    # ---------------------\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    \n    # ---------------------\n    # Pegasos Updates (batch mode)\n    # ---------------------\n    # We use 1-based iteration indexing for Pegasos\n    for t in range(1, iterations + 1):\n        # Scale alpha by (1 - 1/t)\n        alpha *= (1.0 - 1.0 / t)\n        \n        # Compute decision function for all samples:\n        # f_i = sum_j(alpha_j * labels_j * K[j,i]) + b\n        # Instead of summation by loop, we can do a vectorized form:\n        # (alpha * labels) is shape (n_samples,)\n        # Then dot(...) with K => shape (n_samples,)\n        f = (alpha * labels).dot(K) + b\n        \n        # Identify misclassified or margin-violating samples\n        # i.e., those for which y_i * f_i < 1\n        mis_idx = np.where(labels * f < 1)[0]\n        \n        # Update alpha for each misclassified sample\n        # alpha[i] += (1 / (lambda_val * t * n_samples)) for i in mis_idx\n        if len(mis_idx) > 0:\n            alpha[mis_idx] += 1.0 / (lambda_val * t * n_samples)\n        \n        # Update bias term\n        # Subgradient wrt b is sum of (-y_i) for samples that violate, \n        # so the update is b += step_size * mean_of(y_i for violating samples)\n        # step_size = 1 / (lambda_val * t), batch size = n_samples => average factor = 1/n_samples\n        if len(mis_idx) > 0:\n            b += (1.0 / (lambda_val * t * n_samples)) * np.sum(labels[mis_idx])\n\n    # ---------------------\n    # Round results and return\n    # ---------------------\n    alpha_rounded = np.round(alpha, 4).reshape(-1, 1)  # shape (n_samples,1)\n    b_rounded = float(np.round(b, 4))\n\n    return alpha_rounded.tolist(), [b_rounded]  # return both as Python lists"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for the input z.\n    Returns the result rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation values for a given list of scores.\n    Each returned value is rounded to four decimal places.\n    \"\"\"\n    # For numerical stability, subtract the maximum score before exponentiating\n    max_score = max(scores)\n    exp_values = [math.exp(s - max_score) for s in scores]\n    \n    total = sum(exp_values)\n    softmax_values = [round(ev / total, 4) for ev in exp_values]\n    \n    return softmax_values\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Parameters:\n    -----------\n    features : list of list of floats\n        Each inner list represents the feature vector for one example.\n    labels : list of ints\n        True binary labels (0 or 1) corresponding to each feature vector.\n    weights : list of floats\n        Weights for each feature.\n    bias : float\n        Bias term for the neuron.\n\n    Returns:\n    --------\n    (predicted_probabilities, mse)\n        predicted_probabilities : python list (shape: n x 1) after reshaping, each element rounded to 4 decimal places\n        mse : float\n            Mean squared error between predicted probabilities and true labels, rounded to 4 decimal places.\n    \"\"\"\n\n    # Convert inputs to numpy arrays\n    X = np.array(features, dtype=float)\n    y = np.array(labels, dtype=float)\n    w = np.array(weights, dtype=float)\n\n    # Linear combination: z = Xw + b\n    z = np.dot(X, w) + bias\n\n    # Sigmoid activation\n    predictions = 1.0 / (1.0 + np.exp(-z))\n\n    # Compute Mean Squared Error (MSE) with unrounded predictions\n    mse = np.mean((predictions - y) ** 2)\n\n    # Round predictions and MSE to 4 decimal places\n    predictions = np.round(predictions, 4)\n    mse = round(mse, 4)\n\n    # Reshape predictions to (n, 1) and convert to python list\n    predictions = predictions.reshape(-1, 1).tolist()\n\n    return predictions, mse\n```\n"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, \n                 labels: np.ndarray, \n                 initial_weights: np.ndarray, \n                 initial_bias: float, \n                 learning_rate: float, \n                 epochs: int) -> (np.ndarray, float, list[float]):\n\n    # Copy initial weights to avoid modifying external references\n    weights = initial_weights.astype(float).copy()\n    bias = float(initial_bias)\n\n    n_samples = features.shape[0]\n\n    # To store the MSE for each epoch\n    mse_history = []\n\n    for _ in range(epochs):\n        # Forward pass: compute linear combination, then apply sigmoid\n        z = np.dot(features, weights) + bias\n        output = 1.0 / (1.0 + np.exp(-z))  # Sigmoid activation\n\n        # Compute MSE loss\n        errors = output - labels\n        mse = np.mean(errors ** 2)\n        # Round and store MSE\n        mse_history.append(round(mse, 4))\n\n        # Backpropagation\n        # Derivative of sigmoid (output) wrt z\n        d_sigmoid = output * (1.0 - output)\n        # Compute gradients (averaged over batch)\n        w_grad = np.dot(features.T, errors * d_sigmoid) / n_samples\n        b_grad = np.sum(errors * d_sigmoid) / n_samples\n\n        # Update weights and bias\n        weights -= learning_rate * w_grad\n        bias   -= learning_rate * b_grad\n\n    # Round final weights and bias to 4 decimals\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = round(bias, 4)\n\n    # Convert weights to a Python list\n    weights_list = weights_rounded.tolist()\n\n    return weights_list, bias_rounded, mse_history\n```"}
{"task_id": 26, "completion_id": 0, "completion": "Below is a minimal implementation of a Value class that supports scalar autograd for addition, multiplication, and ReLU. It also includes a \u201cbackward\u201d method to propagate gradients through the graph:\n\n--------------------------------------------------------------------------------\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        # Ensure 'other' is a Value\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        # radd just calls __add__\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        # Ensure 'other' is a Value\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        # rmul just calls __mul__\n        return self.__mul__(other)\n\n    def relu(self):\n        # ReLU(x) = max(0, x)\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            # Gradient is passed only if self.data > 0\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        # Perform a topological sort of the graph and then apply the chain rule\n        topo = []\n        visited = set()\n\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_topo(child)\n                topo.append(node)\n\n        build_topo(self)\n\n        # The gradient of the output w.r.t. itself is 1\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n--------------------------------------------------------------------------------\n\nExplanation of key methods:\n\n\u2022 __add__ and __mul__: Implement addition and multiplication between two Value objects (or a Value and a Python scalar). They create a new Value that references its children and define a _backward function that updates self.grad and other.grad.\n\n\u2022 relu: Implements the ReLU activation. The forward pass is max(0, self.data). In backward, the gradient flows only if self.data > 0.\n\n\u2022 backward: Performs a depth-first search (build_topo) to compute a topological ordering of all Value nodes that influence the final output. Then it propagates gradients from the output back to all dependencies via each node\u2019s _backward function.\n\nThis minimal approach should be sufficient for scalar automatic differentiation and can be extended for more complex use cases."}
{"task_id": 27, "completion_id": 0, "completion": "import numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P that takes coordinates from basis B to basis C in R^3.\n\n    If B and C are each lists of three vectors in R^3, interpreted as rows:\n        B = [\n          [b1x, b1y, b1z],\n          [b2x, b2y, b2z],\n          [b3x, b3y, b3z]\n        ]\n        C = [\n          [c1x, c1y, c1z],\n          [c2x, c2y, c2z],\n          [c3x, c3y, c3z]\n        ]\n\n    Then the columns of each matrix represent the basis vectors. The transformation\n    from B-coordinates to C-coordinates is given by P = C_inv * B, where B and C \n    are the matrices formed by using these vectors as columns.\n\n    :param B: list of list of ints, basis B\n    :param C: list of list of ints, basis C\n    :return: list of list of floats (transformation matrix, rounded to 4 decimals)\n    \"\"\"\n\n    # Convert B and C into numpy arrays; interpret each row as a vector => transpose\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n\n    # Compute the transformation P = C^(-1) * B\n    C_inv = np.linalg.inv(C_mat)\n    P = C_inv.dot(B_mat)\n\n    # Round to 4 decimals\n    P_rounded = np.round(P, 4)\n\n    # Convert back to a list of lists and return\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using the method described:\n      1) Compute A^T A and find its eigen-decomposition A^T A = V \u039b V^T\n      2) Singular values are sqrt of the eigenvalues (\u039b)\n      3) U = A V \u03a3^(-1) (handling zero singular values carefully)\n    Finally, enforce det(U) >= 0 and det(V) >= 0 to keep a standard orientation.\n\n    Returns (U, S, V) as lists of lists (each 2x2) rounded to 4 decimal places,\n    satisfying A \u2248 U S V^T in the usual SVD sense.\n    \"\"\"\n\n    # Ensure A is 2x2\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n\n    # Step 1: Compute A^T A\n    ATA = A.T @ A\n\n    # Step 2: Eigen-decomposition of A^T A\n    # eigh() for symmetric matrices; eigenvalues returned in ascending order\n    eigvals, eigvecs = np.linalg.eigh(ATA)\n\n    # Sort eigenvalues/eigenvectors in descending order\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n\n    # Step 3: Singular values are sqrt of eigenvalues (clamp small negatives to 0)\n    s_vals = np.sqrt(np.clip(eigvals, 0, None))\n\n    # Step 4: Construct V from eigenvectors (these are the right singular vectors)\n    V = eigvecs\n\n    # Step 5: Construct U. Each column i of U = (A @ V[:,i]) / s_vals[i]\n    # Handle the case where a singular value may be zero (rank-deficient).\n    U = np.zeros((2, 2), dtype=np.float64)\n    for i in range(2):\n        if abs(s_vals[i]) > 1e-14:\n            U[:, i] = (A @ V[:, i]) / s_vals[i]\n        else:\n            # If s_vals[i] == 0, we pick any vector orthonormal to the already found column (if any).\n            if i == 0:\n                # If the biggest singular value is zero => A is the zero matrix\n                # so set a default basis for U\n                U[:, i] = [1.0, 0.0]\n            else:\n                # Orthogonalize to the first column\n                col0 = U[:, 0]\n                if np.linalg.norm(col0) > 1e-14:\n                    # A perpendicular vector in 2D is (y, -x)\n                    perp = np.array([col0[1], -col0[0]])\n                    perp /= np.linalg.norm(perp)\n                    U[:, i] = perp\n                else:\n                    # If the first column was also zero, pick a standard basis vector\n                    U[:, i] = [0.0, 1.0]\n\n    # Ensure U has a positive determinant (fix orientation)\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n\n    # Similarly ensure V has a positive determinant\n    if np.linalg.det(V) < 0:\n        V[:, 1] = -V[:, 1]\n\n    # Step 6: Form the diagonal S matrix\n    S = np.diag(s_vals)\n\n    # Round to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    # Return as Python lists\n    return U.tolist(), S.tolist(), V.tolist()\n```"}
{"task_id": 29, "completion_id": 0, "completion": "import numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays X and y in unison, maintaining correspondence\n    between their rows. Returns the shuffled data as Python lists.\n\n    Parameters:\n    X (numpy.ndarray): Feature array\n    y (numpy.ndarray): Labels array\n    seed (int, optional): Random seed for reproducibility\n\n    Returns:\n    (list, list): Shuffled X and y converted to Python lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate an array of indices and shuffle it\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    \n    # Shuffle X and y using the shuffled indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Return the results as Python lists\n    return X_shuffled.tolist(), y_shuffled.tolist()"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from X and optionally y.\n    Each batch is returned as a Python list after reshaping by using numpy's tolist() method.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Numpy array containing input data.\n    y : np.ndarray, optional\n        Numpy array containing labels (default is None).\n    batch_size : int, optional\n        Number of samples per batch (default is 64).\n    \n    Yields:\n    -------\n    If y is provided, yields (X_batch, y_batch) where both are Python lists.\n    Otherwise, yields X_batch as a Python list.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        \n        x_batch = X[start:end]\n        x_batch_list = x_batch.tolist()  # convert to Python list\n        \n        if y is not None:\n            y_batch = y[start:end]\n            y_batch_list = y_batch.tolist()  # convert to Python list\n            yield x_batch_list, y_batch_list\n        else:\n            yield x_batch_list\n```"}
{"task_id": 31, "completion_id": 0, "completion": "import numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature\n    is greater than or equal to the given threshold.\n    \n    Parameters:\n    X (numpy.ndarray): The input dataset (2D numpy array).\n    feature_i (int): The index of the feature to split on.\n    threshold (float): The threshold value for the feature.\n    \n    Returns:\n    (list, list): Two lists of samples: the first for samples where the \n    feature_i >= threshold, and the second for samples where the \n    feature_i < threshold.\n    \"\"\"\n    # Boolean mask for the condition\n    mask = X[:, feature_i] >= threshold\n    \n    # Split the dataset\n    X_meet_condition = X[mask]\n    X_not_meet_condition = X[~mask]\n    \n    # Convert to Python lists\n    return X_meet_condition.tolist(), X_not_meet_condition.tolist()"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree for the input 2D array X.\n\n    Parameters:\n    ------------\n    X : numpy.ndarray\n        2D array of shape (n_samples, n_features).\n    degree : int\n        The maximum polynomial degree.\n\n    Returns:\n    ------------\n    list\n        Python list of shape (n_samples, number_of_polynomial_features),\n        containing all polynomial features up to the specified degree.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # This will hold arrays of shape (n_samples,)\n    feature_list = []\n\n    # For each total degree d from 0..degree (0 corresponds to the intercept)\n    for d in range(degree + 1):\n        # Generate all combinations of feature indices (with replacement) of length d\n        for combo in combinations_with_replacement(range(n_features), d):\n            if len(combo) == 0:\n                # Intercept term (all ones)\n                new_feature = np.ones(n_samples)\n            else:\n                # Multiply together the columns specified by combo\n                new_feature = np.prod(X[:, combo], axis=1)\n            feature_list.append(new_feature)\n    \n    # Stack all features column-wise, then convert to Python list\n    return np.array(feature_list).T.tolist()\n```"}
{"task_id": 33, "completion_id": 0, "completion": "import numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of the dataset (X, y).\n    If replacements is True, sample with replacement; otherwise, sample without replacement.\n\n    :param X: 2D numpy array, shape (n_samples, n_features)\n    :param y: 1D numpy array, shape (n_samples,)\n    :param n_subsets: int, number of random subsets to generate\n    :param replacements: bool, if True use sampling with replacement\n    :param seed: int, random seed for reproducibility\n    \n    :return: list of tuples (X_subset, y_subset), each as Python lists\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            # Sample indices with replacement\n            indices = np.random.randint(0, n_samples, size=n_samples)\n        else:\n            # Sample indices without replacement\n            # This picks a random permutation of n_samples\n            indices = np.random.permutation(n_samples)\n\n        X_subset = X[indices, :]\n        y_subset = y[indices]\n\n        # Convert subsets to Python lists\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n\n    return subsets"}
{"task_id": 34, "completion_id": 0, "completion": "import numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding of a 1D numpy array of integer values.\n\n    Parameters:\n    x (numpy.ndarray): 1D array of integer values representing categories.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If None, it is automatically determined from x.\n\n    Returns:\n    list: One-hot encoded representation of x as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1  # Determine number of columns from data if not provided\n\n    # Initialize one-hot encoded array\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n\n    # Set the corresponding positions to 1\n    for i, val in enumerate(x):\n        one_hot[i, val] = 1\n\n    # Convert to Python list\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array x into a diagonal matrix.\n    \n    Parameters:\n    x (numpy.ndarray): 1D input array.\n    \n    Returns:\n    list: 2D python list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates and returns the accuracy score of model predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels\n    y_pred (numpy.ndarray): 1D array of predicted labels\n\n    Returns:\n    float: Accuracy score = (number of correct predictions) / (total number of predictions)\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return float(accuracy)\n```"}
{"task_id": 37, "completion_id": 0, "completion": "import numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset X and, optionally,\n    another dataset Y. If Y is not provided, the correlation matrix is computed\n    for X with itself.\n    \n    Args:\n        X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features_X).\n        Y (numpy.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_Y).\n            If None, Y is set to be X.\n    \n    Returns:\n        list: A 2D python list representing the correlation matrix, rounded to 4 decimals.\n    \"\"\"\n    if Y is None:\n        Y = X  # Compute correlation of X with itself\n    \n    # Center the data (subtract column means)\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    Y_centered = Y - Y.mean(axis=0, keepdims=True)\n    \n    # Compute sample covariance\n    cov = (X_centered.T @ Y_centered) / (X.shape[0] - 1)\n    \n    # Compute standard deviations for each column\n    std_x = X_centered.std(axis=0, ddof=1)\n    std_y = Y_centered.std(axis=0, ddof=1)\n    \n    # Compute correlation matrix\n    corr_matrix = cov / np.outer(std_x, std_y)\n    \n    # Round to 4 decimal places and convert to Python list\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()"}
{"task_id": 38, "completion_id": 0, "completion": "import numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fits an AdaBoost classifier using decision stumps for a given dataset X and labels y.\n    \n    Args:\n        X (numpy.ndarray): 2D array of shape (n_samples, n_features)\n        y (numpy.ndarray): 1D array of shape (n_samples,) with labels in {+1, -1}\n        n_clf (int): Number of weak classifiers (decision stumps) to train\n\n    Returns:\n        list: A list of dictionaries, each containing parameters of a learned weak classifier:\n              [\n                {\n                  \"feature\": <feature index>,\n                  \"threshold\": <float, rounded to 4 decimals>,\n                  \"alpha\": <float, rounded to 4 decimals>,\n                  \"direction\": <'lt' or 'gt'>\n                },\n                ...\n              ]\n    \"\"\"\n    # Ensure y contains only +1 or -1 (if needed, convert from {0,1} to {-1,1})\n    # This step is only necessary if your labels are in {0,1}. Otherwise, remove or modify as needed.\n    if set(np.unique(y)) == {0, 1}:\n        y = np.where(y == 0, -1, 1)\n\n    n_samples, n_features = X.shape\n    \n    # Initialize sample weights to 1/n\n    w = np.ones(n_samples) / n_samples\n    \n    # Store our weak classifiers\n    classifiers = []\n    \n    def stump_predict(feature, threshold, direction, X_data):\n        \"\"\"\n        Predict labels (+1 or -1) for a given feature, threshold, and direction.\n        direction == 'lt': predict +1 if x < threshold else -1\n        direction == 'gt': predict +1 if x > threshold else -1\n        \"\"\"\n        if direction == 'lt':\n            return np.where(X_data[:, feature] < threshold, 1, -1)\n        else:\n            return np.where(X_data[:, feature] > threshold, 1, -1)\n\n    for _ in range(n_clf):\n        best_feature, best_threshold = None, None\n        best_direction = None\n        best_error = float(\"inf\")\n        best_pred = None\n        \n        # Try each feature to find best threshold\n        for feature in range(n_features):\n            # Extract all unique sorted values for this feature\n            x_vals = np.unique(X[:, feature])\n            \n            # Candidate thresholds: midpoints between consecutive unique values\n            if len(x_vals) == 1:\n                # If there's only one unique value, skip or handle separately\n                # E.g., threshold = x_vals[0]\n                thresholds = x_vals\n            else:\n                thresholds = (x_vals[:-1] + x_vals[1:]) / 2.0\n\n            for threshold in thresholds:\n                for direction in ['lt', 'gt']:\n                    # Get predictions based on this threshold/direction\n                    pred = stump_predict(feature, threshold, direction, X)\n                    \n                    # Compute weighted error: sum of weights where pred != y\n                    misclassified = (pred != y)\n                    error = np.sum(w[misclassified])\n                    \n                    # Track the best feature/threshold/direction\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_direction = direction\n                        best_pred = pred\n        \n        # Compute alpha (weak classifier weight)\n        # Avoid log(0) issues if best_error is extremely small or extremely close to 1\n        eps = 1e-10\n        best_error = max(best_error, eps)\n        best_error = min(best_error, 1 - eps)\n        \n        alpha = 0.5 * np.log((1.0 - best_error) / best_error)\n        \n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h(x_i))\n        w *= np.exp(-alpha * y * best_pred)\n        \n        # Normalize weights\n        w /= np.sum(w)\n        \n        # Store the classifier parameters (rounded as requested)\n        classifiers.append({\n            \"feature\": best_feature,\n            \"threshold\": float(np.round(best_threshold, 4)),\n            \"alpha\": float(np.round(alpha, 4)),\n            \"direction\": best_direction\n        })\n\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert scores to a numpy array\n    x = np.array(scores, dtype=np.float64)\n    \n    # Shift scores by subtracting the max value for numerical stability\n    x_shifted = x - np.max(x)\n    \n    # Compute log of the normalization term\n    log_sum_exp = np.log(np.sum(np.exp(x_shifted)))\n    \n    # Compute log-softmax values\n    log_softmax_vals = x_shifted - log_sum_exp\n    \n    # Round the result to the nearest 4th decimal\n    log_softmax_vals = np.round(log_softmax_vals, 4)\n    \n    # Reshape to 2D array with one column and convert to list\n    return log_softmax_vals.reshape(-1, 1).tolist()\n```"}
{"task_id": 40, "completion_id": 0, "completion": "import numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        \"\"\"\n        1. Initialization:\n           - Define the layer with a specified number of neurons (n_units) and an optional input shape.\n           - Create placeholders for the layer's weights (W), biases (w0), \n             as well as placeholders for possible optimizers (W_opt, w0_opt).\n        \"\"\"\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.layer_input = None\n\n        # This layer should be trainable\n        self.trainable = True\n\n        # Placeholders for weights, biases, and their optimizers\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer=None):\n        \"\"\"\n        2. Weight Initialization:\n           - Initialize W using a uniform distribution with limit = 1 / sqrt(input_shape[0]).\n           - Initialize biases (w0) to zero.\n           - Optionally copy the passed optimizer to W_opt and w0_opt.\n        \"\"\"\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n        # If an optimizer is provided, copy it for W and w0\n        if optimizer is not None:\n            self.W_opt = copy.deepcopy(optimizer)\n            self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        \"\"\"\n        3. Parameter Count:\n           - Number of trainable parameters = size of W + size of w0.\n        \"\"\"\n        return (self.input_shape[0] * self.n_units) + self.n_units\n\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        4. Forward Pass:\n           - Compute the layer output as a matrix multiplication between X and W plus the bias w0.\n           - Round to 4 decimal places and return as a Python list.\n        \"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        output = np.round(output, 4)      # Round to 4 decimal places\n        return output.tolist()           # Return as Python list\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        5. Backward Pass:\n           - Compute gradients with respect to W and w0.\n           - If trainable, update W and w0 using the optimizer's update rule.\n           - Compute and return the gradient wrt. the input (dX).\n           - All returned/updated values should be rounded to 4 decimals, \n             and the gradient returned as a Python list.\n        \"\"\"\n        # Convert incoming gradient to np array just in case\n        accum_grad = np.array(accum_grad)\n\n        # Gradient wrt. W and w0\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n        # If the layer is trainable, update weights and biases\n        if self.trainable:\n            if self.W_opt is not None:\n                self.W = self.W_opt.update(self.W, dW)\n            else:\n                self.W -= 0.01 * dW  # Simple fallback update (e.g., SGD)\n\n            if self.w0_opt is not None:\n                self.w0 = self.w0_opt.update(self.w0, dw0)\n            else:\n                self.w0 -= 0.01 * dw0  # Simple fallback update (e.g., SGD)\n\n            # Round updates to 4 decimals\n            self.W = np.round(self.W, 4)\n            self.w0 = np.round(self.w0, 4)\n\n        # Gradient wrt. the input\n        grad_input = np.dot(accum_grad, self.W.T)\n\n        # Round and return as list\n        grad_input = np.round(grad_input, 4)\n        return grad_input.tolist()\n\n    def output_shape(self):\n        \"\"\"\n        6. Output Shape:\n           - Return the shape of the output = (n_units,).\n        \"\"\"\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "completion": "import numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Computes a 2D convolution operation on the input matrix using the provided kernel,\n    with specified padding and stride values. The results are rounded to 4 decimal places.\n\n    Parameters:\n        input_matrix (np.ndarray): 2D array representing the input.\n        kernel (np.ndarray): 2D array representing the convolution kernel.\n        padding (int): Number of zero-padding layers added symmetrically.\n        stride (int): Stride step size for the sliding convolution.\n\n    Returns:\n        list: Python list representation of the 2D convolution result,\n              rounded to 4 decimal places.\n    \"\"\"\n\n    # Pad the input matrix with zeros\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Determine sizes\n    in_height, in_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n\n    # Calculate output dimensions\n    out_height = ((in_height + 2*padding - kernel_height) // stride) + 1\n    out_width = ((in_width + 2*padding - kernel_width) // stride) + 1\n\n    # Initialize the output\n    output = np.zeros((out_height, out_width), dtype=float)\n\n    # Convolution operation\n    for i in range(out_height):\n        for j in range(out_width):\n            # Extract the region from the padded input\n            row_start = i * stride\n            col_start = j * stride\n\n            region = padded_input[row_start: row_start + kernel_height,\n                                  col_start: col_start + kernel_width]\n\n            # Perform element-wise multiplication and sum\n            output[i, j] = np.sum(region * kernel)\n\n    # Round to 4 decimal places\n    output = np.round(output, 4)\n\n    # Convert to Python list\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```"}
{"task_id": 43, "completion_id": 0, "completion": "import numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): 2D feature matrix of shape (n_samples, n_features)\n    w (np.ndarray): 1D array of coefficients of shape (n_features,)\n    y_true (np.ndarray): 1D array of true labels of shape (n_samples,)\n    alpha (float): Regularization parameter\n\n    Returns:\n    float: Ridge loss (MSE + alpha * sum(w^2)), rounded to 4 decimal places\n    \"\"\"\n    # Compute predictions\n    y_pred = X @ w\n    \n    # Compute Mean Squared Error\n    mse = np.mean((y_pred - y_true) ** 2)\n    \n    # Compute the L2 regularization term\n    reg = alpha * np.sum(w ** 2)\n    \n    # Compute the Ridge loss\n    loss = mse + reg\n    \n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n    float|int: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    Precision = TP / (TP + FP)\n    \n    Parameters:\n    y_true (np.array): Ground truth binary labels\n    y_pred (np.array): Predicted binary labels\n\n    Returns:\n    float: The precision value\n    \"\"\"\n    # True positives: predicted 1 and is actually 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    # False positives: predicted 1 but is actually 0\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n\n    # Avoid division by zero if there are no predicted positives\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    \n    return tp / denominator\n```"}
{"task_id": 47, "completion_id": 0, "completion": "import numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent using MSE loss in one of three variants:\n      1) Batch Gradient Descent\n      2) Stochastic Gradient Descent (SGD)\n      3) Mini-Batch Gradient Descent\n\n    Parameters:\n    X             : np.ndarray, shape (n_samples, n_features)\n                   The input data.\n    y             : np.ndarray, shape (n_samples,)\n                   The target values.\n    weights       : np.ndarray, shape (n_features,)\n                   Initial weights for the model.\n    learning_rate : float\n                   Learning rate for the gradient descent updates.\n    n_iterations  : int\n                   Number of iterations (updates) to perform.\n    batch_size    : int, optional (default=1)\n                   Size of each mini-batch (only used if method='mini-batch').\n    method        : str, optional (default='batch')\n                   Specifies which gradient descent variant to use.\n                     'batch'       -> Batch Gradient Descent\n                     'sgd'         -> Stochastic Gradient Descent\n                     'mini-batch'  -> Mini-Batch Gradient Descent\n\n    Returns:\n    A python list of the final weight values, each rounded to the 4th decimal place.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    for iteration in range(n_iterations):\n        if method == 'batch':\n            # Use the entire dataset for each update\n            predictions = X.dot(weights)\n            errors = predictions - y\n            grad = (2 / n_samples) * X.T.dot(errors)\n            weights -= learning_rate * grad\n\n        elif method == 'sgd':\n            # Update using one random sample at a time\n            idx = np.random.randint(n_samples)\n            X_i, y_i = X[idx], y[idx]\n            prediction = X_i.dot(weights)\n            error = prediction - y_i\n            # Gradient for a single sample: 2 * X_i * (prediction - y_i)\n            grad = 2 * X_i * error\n            weights -= learning_rate * grad\n\n        elif method == 'mini-batch':\n            # Shuffle data and iterate in mini-batches\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            \n            # Go through each mini-batch\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                X_batch = X_shuffled[start:end]\n                y_batch = y_shuffled[start:end]\n                predictions = X_batch.dot(weights)\n                errors = predictions - y_batch\n                grad = (2 / X_batch.shape[0]) * X_batch.T.dot(errors)\n                weights -= learning_rate * grad\n\n        else:\n            raise ValueError(\"method must be one of: 'batch', 'sgd', or 'mini-batch'\")\n\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "completion": "import numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF) and return the result as a Python list.\n    \n    Parameters:\n    matrix (list of lists or numpy array): The input 2D matrix to convert into RREF.\n    \n    Returns:\n    list: The RREF of the input matrix as a nested Python list.\n    \"\"\"\n    # Convert to numpy array (float type to handle divisions)\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    \n    # pivot_row tracks the row we are setting the pivot in\n    pivot_row = 0\n    \n    # Iterate over each column to find and set pivots\n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n        \n        # Find a pivot in the current column at or below pivot_row\n        pivot = pivot_row\n        while pivot < rows and abs(A[pivot, col]) < 1e-12:  # treat very small values as zero\n            pivot += 1\n        \n        # If no pivot in this column, move to the next column\n        if pivot == rows:\n            continue\n        \n        # If pivot is not the current pivot_row, swap them\n        if pivot != pivot_row:\n            A[[pivot_row, pivot], :] = A[[pivot, pivot_row], :]\n        \n        # Normalize the pivot row by dividing by the pivot element\n        pivot_val = A[pivot_row, col]\n        if abs(pivot_val) > 1e-12:\n            A[pivot_row, :] = A[pivot_row, :] / pivot_val\n        \n        # Eliminate this column in all other rows\n        for r in range(rows):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > 1e-12:\n                    A[r, :] = A[r, :] - factor * A[pivot_row, :]\n        \n        pivot_row += 1\n    \n    # Return the resulting array as a Python list\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Implements the Adam (Adaptive Moment Estimation) optimization algorithm.\n    \n    Parameters:\n    -----------\n    f : callable\n        The objective function to be optimized.\n    grad : callable\n        A function that computes the gradient of f.\n    x0 : array_like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size (default: 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default: 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default: 10).\n        \n    Returns:\n    --------\n    list\n        The optimized parameters, rounded to the nearest 4 decimals, returned as a Python list.\n    \"\"\"\n    \n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x, dtype=np.float64)\n    v = np.zeros_like(x, dtype=np.float64)\n    \n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**t)\n        \n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2**t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 4 decimals, then convert to Python list\n    x_rounded = np.round(x, 4)\n    \n    return x_rounded.tolist()\n```\n"}
{"task_id": 50, "completion_id": 0, "completion": "import numpy as np\n\ndef l1_regularization_gradient_descent(\n    X: np.array, \n    y: np.array, \n    alpha: float = 0.1, \n    learning_rate: float = 0.01, \n    max_iter: int = 1000, \n    tol: float = 1e-4\n) -> tuple:\n    \"\"\"\n    Perform Lasso Regression (L1) using Gradient Descent and return (weights, bias).\n    \n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target array of shape (n_samples, ).\n        alpha (float): Regularization parameter for L1 penalty.\n        learning_rate (float): The learning rate for gradient descent.\n        max_iter (int): Maximum number of iterations for gradient descent.\n        tol (float): Tolerance for stopping criterion.\n    \n    Returns:\n        (list, float): \n            - list of final weights rounded to 4 decimal places.\n            - bias (float) rounded to 4 decimal places.\n    \"\"\"\n    # Number of samples (n) and features (p)\n    n, p = X.shape\n    \n    # Initialize weights and bias to zeros\n    w = np.zeros(p)\n    b = 0.0\n    \n    for _ in range(max_iter):\n        # Predict y using current weights and bias\n        y_pred = X.dot(w) + b\n        \n        # Compute residuals\n        residual = y_pred - y\n        \n        # Gradient of the MSE part w.r.t. w:\n        # Because of 1/(2n)*sum(...)^2, derivative is (1/n)*X^T(y_pred - y)\n        grad_w_mse = (1.0 / n) * (X.T.dot(residual))\n        \n        # Subgradient of the L1 penalty alpha * sum(|w|)\n        # sign(w_j) if w_j != 0, else 0 for subgradient at w_j = 0\n        grad_w_l1 = alpha * np.sign(w)\n        grad_w_l1[w == 0] = 0.0  # one valid subgradient at 0 is 0\n        \n        # Total gradient for weights\n        grad_w = grad_w_mse + grad_w_l1\n        \n        # Update weights\n        w_new = w - learning_rate * grad_w\n        \n        # Gradient w.r.t bias (no regularization on bias)\n        grad_b = (1.0 / n) * np.sum(residual)\n        \n        # Update bias\n        b_new = b - learning_rate * grad_b\n        \n        # Check for convergence (based on weight update size)\n        if np.linalg.norm(w_new - w) < tol:\n            w = w_new\n            b = b_new\n            break\n        \n        w = w_new\n        b = b_new\n    \n    # Round and convert to list\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    \n    return w_rounded, b_rounded"}
{"task_id": 51, "completion_id": 0, "completion": "import numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment distance between two strings,\n    allowing insertions, deletions, substitutions, and transposition of\n    adjacent characters (each at cost 1).\n    \"\"\"\n    m, n = len(source), len(target)\n\n    # Create a 2D DP array where dp[i][j] represents the OSA distance\n    # between source[:i] and target[:j].\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the base cases:\n    # dp[i][0] = i (cost of deleting all characters up to i)\n    # dp[0][j] = j (cost of inserting all characters up to j)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n\n    # Fill the DP array\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n\n            # Consider the operations:\n            # 1. Deletion: dp[i-1][j] + 1\n            # 2. Insertion: dp[i][j-1] + 1\n            # 3. Substitution: dp[i-1][j-1] + cost\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,\n                dp[i][j - 1] + 1,\n                dp[i - 1][j - 1] + cost\n            )\n\n            # 4. Transposition of adjacent characters:\n            # If source[i-1] == target[j-2] and source[i-2] == target[j-1],\n            # we consider dp[i-2][j-2] + 1 (swap cost).\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert inputs to NumPy arrays (in case they aren't already)\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP): Predicted 1, Actual 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    # Calculate False Negatives (FN): Predicted 0, Actual 1\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    denominator = tp + fn\n    # Avoid division by zero\n    if denominator == 0:\n        return 0.0\n    \n    # Compute recall (TP / (TP + FN)) and round to three decimals\n    return round(tp / denominator, 3)\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Implements the self-attention mechanism:\n      1) Compute the dot product of Q and K^T.\n      2) Scale by sqrt(d_k) where d_k is the dimension of the key.\n      3) Apply the softmax function to get the attention weights.\n      4) Multiply the attention weights by V to get the output.\n\n    Parameters:\n        Q, K, V (numpy.ndarray): \n            - Shape: (sequence_length, feature_dimension).\n            - sequence_length must be the same for K and V,\n              and feature_dimension must be the same for Q, K, and V.\n\n    Returns:\n        list: The self-attention output after rounding to 4 decimal places\n              and converted to a Python list.\n    \"\"\"\n\n    # 1. Compute d_k\n    d_k = K.shape[-1]  # feature_dimension\n\n    # 2. Compute the scaled dot product: Q * K^T / sqrt(d_k)\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n\n    # 3. Apply softmax along the last axis\n    #    (i.e., for each row in scores, softmax over all columns)\n    #    to get the attention weights\n    exps = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exps / np.sum(exps, axis=-1, keepdims=True)\n\n    # 4. Multiply attention weights by V to get the output\n    output = np.matmul(attention_weights, V)\n\n    # 5. Round to 4 decimal places\n    output = np.round(output, 4)\n\n    # 6. Convert to a Python list and return\n    return output.tolist()\n```\n"}
{"task_id": 54, "completion_id": 0, "completion": "import numpy as np\n\ndef rnn_forward(\n    input_sequence: list[list[float]],\n    initial_hidden_state: list[float],\n    Wx: list[list[float]],\n    Wh: list[list[float]],\n    b: list[float]\n) -> list[float]:\n\n    # Convert all inputs to numpy arrays\n    x_seq = np.array(input_sequence, dtype=float)     # Shape: (T, D)\n    h_prev = np.array(initial_hidden_state, dtype=float)  # Shape: (H,)\n    Wx_arr = np.array(Wx, dtype=float)               # Shape: (D, H)\n    Wh_arr = np.array(Wh, dtype=float)               # Shape: (H, H)\n    b_arr = np.array(b, dtype=float)                 # Shape: (H,)\n\n    # Iterate through the sequence\n    for x_t in x_seq:\n        # Update hidden state using the RNN formula:\n        # h_t = tanh(x_t * Wx + h_{t-1} * Wh + b)\n        h_prev = np.tanh(np.dot(x_t, Wx_arr) + np.dot(h_prev, Wh_arr) + b_arr)\n\n    # Round final hidden state to 4 decimal places and return as a list\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Convert the input points to a NumPy array\n    points_array = np.array(points, dtype=float)  # Shape (N, 2)\n    \n    # Create a column of ones to convert to homogeneous coordinates\n    ones = np.ones((points_array.shape[0], 1), dtype=float)  # Shape (N, 1)\n    homogeneous_points = np.hstack((points_array, ones))     # Shape (N, 3)\n    \n    # Define the 2D translation matrix in homogeneous form\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1 ]\n    ], dtype=float)  # Shape (3, 3)\n    \n    # Apply the translation matrix\n    # Note: (N, 3) @ (3, 3) -> (N, 3)\n    translated_points_homogeneous = homogeneous_points @ translation_matrix.T\n    \n    # Convert back from homogeneous coordinates to 2D\n    translated_points = translated_points_homogeneous[:, :2]  # Shape (N, 2)\n    \n    # Convert NumPy array back to a Python list\n    return translated_points.tolist()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the KL divergence between two univariate normal distributions:\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2).\n\n    Parameters\n    ----------\n    mu_p : float\n        Mean of the first normal distribution (P).\n    sigma_p : float\n        Standard deviation of the first normal distribution (P).\n    mu_q : float\n        Mean of the second normal distribution (Q).\n    sigma_q : float\n        Standard deviation of the second normal distribution (Q).\n\n    Returns\n    -------\n    float\n        The KL divergence D_KL(P || Q).\n    \"\"\"\n    # Formula for KL divergence between two univariate normal distributions:\n    # KL(P || Q) = log(sigma_q / sigma_p) \n    #              + [sigma_p^2 + (mu_p - mu_q)^2] / (2 * sigma_q^2)\n    #              - 1/2\n\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    term3 = -0.5\n\n    return term1 + term2 + term3\n```"}
{"task_id": 57, "completion_id": 0, "completion": "import numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations A x = b using the Gauss-Seidel method.\n\n    Parameters\n    ----------\n    A : (N, N) array_like\n        Coefficient matrix.\n    b : (N,) array_like\n        Right-hand side vector.\n    n : int\n        Number of iterations.\n    x_ini : (N,) array_like, optional\n        Initial guess. If None, a zero vector will be used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n\n    # If initial guess is not provided, use a zero vector\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n\n    # Gauss-Seidel iterative process\n    for _ in range(n):\n        for i in range(len(b)):\n            # Start with the constant term\n            sum_ = b[i]\n            # Subtract the known contributions from all other variables\n            for j in range(len(b)):\n                if j != i:\n                    sum_ -= A[i, j] * x[j]\n            # Update the variable using the most recent values\n            x[i] = sum_ / A[i, i]\n\n    # Round results to the 4th decimal\n    x_rounded = np.round(x, 4)\n\n    # Return as a Python list\n    return x_rounded.tolist()"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Performs Gaussian elimination with partial pivoting to solve the system A x = b.\n\n    Parameters:\n    -----------\n    A : np.ndarray\n        A 2D square numpy array (coefficient matrix).\n    b : np.ndarray\n        A 1D numpy array (right-hand side vector).\n\n    Returns:\n    --------\n    list\n        Solution vector x as a Python list, values rounded to the 4th decimal.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n\n    n = A.shape[0]\n\n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Partial pivot: find the row with the largest pivot in column i\n        pivot_row = i + np.argmax(np.abs(A[i:, i]))\n        # Swap rows if a larger pivot is found\n        if pivot_row != i:\n            A[[i, pivot_row]] = A[[pivot_row, i]]\n            b[i], b[pivot_row] = b[pivot_row], b[i]\n\n        # If the pivot is zero, matrix is singular or nearly singular\n        if abs(A[i, i]) < 1e-14:\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n\n        # Eliminate entries below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n\n    # Backward substitution\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        # Subtract known contributions from already solved variables\n        sum_ax = np.dot(A[i, i+1:], x[i+1:])\n        x[i] = (b[i] - sum_ax) / A[i, i]\n\n    # Round to 4 decimals and return as Python list\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        All returned values are rounded to the 4th decimal place and converted to Python lists.\n        \n        Args:\n            x (np.ndarray): Input sequence of shape (T, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n\n        Returns:\n            tuple: (hidden_states_list, final_hidden_state_list, final_cell_state_list)\n                   Each element is a Python list version of the numpy array rounded to 4 decimals.\n        \"\"\"\n\n        # Sigmoid function\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n\n        # Get sequence length T\n        T = x.shape[0]\n\n        # Initialize ho and co\n        ht = initial_hidden_state\n        ct = initial_cell_state\n\n        # Store hidden states at each timestep\n        hidden_states = []\n\n        # Process each time step\n        for t in range(T):\n            # Current input\n            xt = x[t].reshape(-1, 1)  # shape (input_size, 1)\n\n            # Concatenate ht and xt\n            concat = np.vstack((ht, xt))  # shape (input_size + hidden_size, 1)\n\n            # Forget gate\n            ft = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            # Input gate\n            it = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            # Candidate cell state\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            # Output gate\n            ot = sigmoid(np.dot(self.Wo, concat) + self.bo)\n\n            # Next cell state\n            ct = ft * ct + it * c_tilde\n            # Next hidden state\n            ht = ot * np.tanh(ct)\n\n            hidden_states.append(ht.copy())\n\n        # Convert to numpy arrays for consistent manipulation\n        hidden_states_np = np.concatenate(hidden_states, axis=1).T  # shape (T, hidden_size)\n        final_hidden_state = ht\n        final_cell_state = ct\n\n        # Round to 4 decimals\n        hidden_states_np = np.round(hidden_states_np, 4)\n        final_hidden_state = np.round(final_hidden_state, 4)\n        final_cell_state = np.round(final_cell_state, 4)\n\n        # Convert to Python lists\n        hidden_states_list = hidden_states_np.tolist()\n        final_hidden_state_list = final_hidden_state.reshape(-1).tolist()\n        final_cell_state_list = final_cell_state.reshape(-1).tolist()\n\n        return hidden_states_list, final_hidden_state_list, final_cell_state_list\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores of the given `query` terms against a `corpus` of documents.\n    \n    :param corpus: List of documents, where each document is a list of words (tokens).\n    :param query: List of words for which TF-IDF scores are to be computed.\n    :return: A list of lists of TF-IDF scores for each document (rows) and each query term (columns).\n    \"\"\"\n    \n    # Edge case: If the corpus is empty, return an empty list\n    if not corpus:\n        return []\n    \n    # Edge case: If the query is empty, return a list of lists of zeros or just empty. \n    # We will return empty list for consistency.\n    if not query:\n        return []\n    \n    # Number of documents\n    N = len(corpus)\n    \n    # Precompute Document Frequencies (df) for the query terms\n    # We'll do a smoothing approach in the IDF to avoid division by zero\n    doc_freq = {term: 0 for term in query}\n    \n    for doc in corpus:\n        # Convert doc to a set for fast membership checking\n        unique_words = set(doc)\n        for term in query:\n            if term in unique_words:\n                doc_freq[term] += 1\n    \n    # Compute IDF for each query term using log10((N + 1) / (df + 1))\n    idf = {}\n    for term in query:\n        idf[term] = np.log10((N + 1) / (doc_freq[term] + 1))\n    \n    # Compute TF-IDF\n    # For each document, compute TF for each query term, then multiply by IDF\n    tf_idf_matrix = []\n    for doc in corpus:\n        doc_len = len(doc)\n        # Edge case: If document is empty, all tf-idf values for this doc are zero\n        # to avoid division by zero for TF\n        if doc_len == 0:\n            tf_idf_matrix.append([0.0] * len(query))\n            continue\n            \n        # Count frequencies of terms in the doc\n        word_count = {}\n        for w in doc:\n            word_count[w] = word_count.get(w, 0) + 1\n        \n        row_scores = []\n        for term in query:\n            tf = word_count.get(term, 0) / doc_len\n            row_scores.append(tf * idf[term])\n        \n        tf_idf_matrix.append(row_scores)\n    \n    # Round results to 4 decimals\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()\n```\n\nExplanation of Key Steps:\n1. If the corpus or query is empty, the function returns an empty list to handle edge cases gracefully.\n2. Document Frequency (df) is computed by counting how many different documents each query term appears in. A set of each document\u2019s words is used for fast membership checks.\n3. Inverse Document Frequency (IDF) uses smoothing to prevent division by zero: log10((N + 1) / (df + 1)).\n4. Term Frequency (TF) is computed as the count of the term in a document divided by the total number of words in that document.\n5. TF-IDF is calculated as (TF \u00d7 IDF) for each term in each document.\n6. The results are rounded to 4 decimal places and returned as a Python list."}
{"task_id": 61, "completion_id": 0, "completion": "import numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives, False Positives, False Negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n    \n    # F-score using the formula: (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)\n    if precision == 0 and recall == 0:\n        return 0.0  # avoids division by zero\n    \n    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta, 3)"}
{"task_id": 62, "completion_id": 0, "completion": "Below is an example implementation of a simple RNN with backpropagation through time (BPTT). It uses \u00bd\u00b7MSE as the loss function and sums losses across all time steps. Each input in the sequence is assumed to be a NumPy array of shape (input_size,), and each output is a NumPy array of shape (output_size,).\n\n--------------------------------------------------------------------------------\nimport numpy as np\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    \"\"\"\n    Initializes the RNN weights:\n      W_xh: (hidden_size, input_size)\n      W_hh: (hidden_size, hidden_size)\n      W_hy: (output_size, hidden_size)\n      b_h: (hidden_size,)\n      b_y: (output_size,)\n    All weights are set to small random values, and biases are zeros.\n    \"\"\"\n    W_xh = 0.01 * np.random.randn(hidden_size, input_size)\n    W_hh = 0.01 * np.random.randn(hidden_size, hidden_size)\n    W_hy = 0.01 * np.random.randn(output_size, hidden_size)\n    b_h = np.zeros((hidden_size,))\n    b_y = np.zeros((output_size,))\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    \"\"\"\n    Processes a sequence of inputs through the RNN.\n    Returns:\n      outputs: list of outputs for each time step\n      last_inputs: list of inputs at each time step\n      last_hiddens: list of hidden states at each time step\n    \"\"\"\n    h_prev = np.zeros((hidden_size,))\n    outputs = []\n    last_inputs = []\n    last_hiddens = []\n\n    for x_t in input_sequence:\n        # Compute new hidden state\n        h_t = np.tanh(W_xh @ x_t + W_hh @ h_prev + b_h)\n        # Compute output\n        y_t = W_hy @ h_t + b_y\n\n        outputs.append(y_t)\n        last_inputs.append(x_t)\n        last_hiddens.append(h_t)\n\n        # Update hidden for next time step\n        h_prev = h_t\n\n    return outputs, last_inputs, last_hiddens\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y,\n                 hidden_size, input_sequence, expected_output,\n                 outputs, last_inputs, last_hiddens, learning_rate):\n    \"\"\"\n    Performs backpropagation through time (BPTT) to adjust the weights.\n    Uses 1/2 * MSE loss summed over all time steps.\n    \"\"\"\n    # Initialize gradients to zero\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n\n    # For backprop through the hidden states\n    dh_next = np.zeros((hidden_size,))\n\n    # BPTT through time (from last to first)\n    T = len(input_sequence)\n    for t in reversed(range(T)):\n        y_t = outputs[t]                 # Predicted output\n        y_true = expected_output[t]      # Ground truth\n        x_t = last_inputs[t]             # Input at time t\n        h_t = last_hiddens[t]            # Hidden state at time t\n        h_prev = last_hiddens[t-1] if t > 0 else np.zeros((hidden_size,))\n\n        # \u2202(Loss)/\u2202(y_t) = (y_t - y_true), \n        # since Loss_t = 1/2 * (y_t - y_true)^2 and d/dy (1/2*(y-y*)\u00b2)= (y-y*)\n        dy_t = (y_t - y_true)\n\n        # Gradients w.r.t output layer\n        dW_hy += np.outer(dy_t, h_t)\n        db_y += dy_t\n\n        # Backprop to hidden\n        dh_t = W_hy.T @ dy_t + dh_next  # includes grad from next time step\n        # derivative of tanh(z) = 1 - tanh^2(z)\n        dtanh = (1 - h_t**2) * dh_t\n\n        # Gradients w.r.t hidden layer parameters\n        dW_xh += np.outer(dtanh, x_t)\n        dW_hh += np.outer(dtanh, h_prev)\n        db_h += dtanh\n\n        # Pass the gradient back in time\n        dh_next = W_hh.T @ dtanh\n\n    # Update parameters using gradient descent\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef entrypoint(input_sequence, expected_output, input_size,\n               hidden_size, output_size, learning_rate=0.01,\n               epochs=100, random_seed=42):\n    \"\"\"\n    Trains the simple RNN on the given input_sequence and expected_output\n    for a specified number of epochs, then returns the final predictions.\n    \"\"\"\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(\n            W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence,\n            expected_output, outputs, last_inputs, last_hiddens, learning_rate\n        )\n\n    # Return the final predictions, rounded to 5 decimal places\n    return np.round(np.array(outputs), 5).tolist()\n\n--------------------------------------------------------------------------------\n\nExplanation of key steps:\n\n\u2022 initialize_weights:  \n  - Randomly initializes weight matrices W_xh, W_hh, W_hy to small values (multiplied by 0.01) and sets bias vectors b_h and b_y to zero.  \n  - Shapes:  \n      W_xh is [hidden_size, input_size]  \n      W_hh is [hidden_size, hidden_size]  \n      W_hy is [output_size, hidden_size]  \n      b_h is [hidden_size,]  \n      b_y is [output_size,]  \n\n\u2022 rnn_forward:  \n  - Initializes the hidden state h_prev as a zero vector of size [hidden_size].  \n  - For each x_t in the input sequence:  \n    1) Computes h_t = tanh(W_xh\u00b7x_t + W_hh\u00b7h_prev + b_h).  \n    2) Computes output y_t = W_hy\u00b7h_t + b_y.  \n    3) Stores the hidden state and the input for backpropagation.  \n    4) Updates h_prev = h_t.  \n\n\u2022 rnn_backward (BPTT):  \n  - Initializes gradient accumulators (dW_xh, dW_hh, dW_hy, db_h, db_y) to zero.  \n  - Traverses the sequence in reverse order.  \n    1) Let dy_t = (y_t - y_true).  \n    2) Accumulate dW_hy, db_y.  \n    3) Propagate dy_t back to hidden state (dh_t = W_hy^T\u00b7dy_t + dh_next), then apply derivative of tanh.  \n    4) Update gradients for W_xh, W_hh, and b_h.  \n    5) Propagate gradient to dh_next for the next iteration in time.  \n  - After summing gradients from all time steps, update parameters by subtracting learning_rate \u00d7 gradient.  \n\n\u2022 entrypoint:  \n  1) Sets a random seed (for reproducibility) and calls initialize_weights.  \n  2) Runs rnn_forward to get outputs and intermediate variables.  \n  3) Calls rnn_backward to perform weight updates.  \n  4) After epochs of training, returns the rounded predictions.  \n\nUse this code as a reference to construct and train a simple RNN on sequence data using Backpropagation Through Time."}
{"task_id": 63, "completion_id": 0, "completion": "import numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x as a list (rounded to 8 decimal places)\n    \"\"\"\n    # If no initial guess is provided, start with a zero vector\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x0.astype(float)\n\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n\n        # Check for convergence\n        if np.linalg.norm(r) < tol:\n            break\n        \n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n\n    # Round the solution to 8 decimal places and convert to list\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Get the unique classes and their respective counts\n    unique_classes, counts = np.unique(y, return_counts=True)\n\n    # Calculate probabilities of each class\n    probabilities = counts / len(y)\n\n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n\n    # Return Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n```"}
{"task_id": 65, "completion_id": 0, "completion": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]  # Start with 0, will accumulate counts of non-zero elements per row\n\n    for row in dense_matrix:\n        # Record how many non-zero elements seen so far before this row\n        current_count = len(values)\n        for col_index, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(col_index)\n        # The row pointer for this row is the total non-zero count thus far\n        row_pointers.append(len(values))\n\n    return values, col_indices, row_pointers"}
{"task_id": 66, "completion_id": 0, "completion": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of floats/ints)\n    :param L: The line vector defining the direction of projection (list of floats/ints)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    # Compute the dot product of v and L\n    dot_v_L = sum(vi * Li for vi, Li in zip(v, L))\n    \n    # Compute the dot product of L with itself\n    dot_L_L = sum(Li * Li for Li in L)\n\n    # If L is a zero vector (or very close), return a zero vector of the same dimension as L\n    if abs(dot_L_L) < 1e-15:\n        return [0.0] * len(L)\n    \n    # Calculate the scalar multiplier for the projection\n    scalar = dot_v_L / dot_L_L\n    \n    # Compute the projection of v onto L and round to 3 decimal places\n    projection = [round(scalar * Li, 3) for Li in L]\n    \n    return projection"}
{"task_id": 67, "completion_id": 0, "completion": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n    \n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row_indices, col_ptr)\n    \"\"\"\n    if not dense_matrix:\n        # Handle empty matrix\n        return [], [], []\n    \n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    col_ptr = [0] * (num_cols + 1)\n    \n    # Build CSC by traversing columns first\n    for col in range(num_cols):\n        # Mark where this column's non-zero values will start in 'values'\n        col_ptr[col] = len(values)\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n    \n    # The last entry of the column pointer is the total number of non-zero elements\n    col_ptr[num_cols] = len(values)\n    \n    return values, row_indices, col_ptr"}
{"task_id": 68, "completion_id": 0, "completion": "import numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space (image) of matrix A using row echelon form.\n    The columns corresponding to pivot positions from the original matrix\n    form a basis for the column space.\n\n    Parameters:\n        A (numpy.ndarray): A 2D array (matrix).\n\n    Returns:\n        list: A 2D list whose columns are the basis vectors for the column\n              space of A, rounded to 8 decimal places.\n    \"\"\"\n    # Convert A to a float type (copy) to avoid altering the original array\n    A = A.astype(float)\n    n_rows, n_cols = A.shape\n\n    # We'll work on a copy of A for row-echelon form\n    # but we need to remember which columns are pivot columns.\n    A_copy = A.copy()\n\n    pivot_cols = []  # To store the indices of pivot columns\n    pivot_row = 0\n\n    for col in range(n_cols):\n        if pivot_row >= n_rows:\n            break\n        \n        # Partial pivot: find row with largest abs value in the current column from pivot_row downward\n        max_row = pivot_row + np.argmax(np.abs(A_copy[pivot_row:, col]))\n        if abs(A_copy[max_row, col]) < 1e-15:\n            # If the pivot is effectively zero, skip this column\n            continue\n        \n        # Swap the pivot row if needed\n        if max_row != pivot_row:\n            A_copy[[pivot_row, max_row]] = A_copy[[max_row, pivot_row]]\n        \n        # Now we have our pivot in row pivot_row, column col\n        pivot_cols.append(col)\n\n        # Eliminate below pivot\n        for r in range(pivot_row + 1, n_rows):\n            if abs(A_copy[r, col]) > 1e-15:\n                factor = A_copy[r, col] / A_copy[pivot_row, col]\n                A_copy[r, col:] -= factor * A_copy[pivot_row, col:]\n        \n        pivot_row += 1\n        # If we've used all rows, done.\n        if pivot_row == n_rows:\n            break\n\n    # The basis for the image is given by the pivot columns of the original matrix\n    # Extract those columns from the original A\n    basis = A[:, pivot_cols]\n\n    # Round to 8 decimal places\n    basis = np.round(basis, 8)\n\n    # Convert to list and return\n    return basis.tolist()"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute R-squared (coefficient of determination).\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,)\n    - y_pred: array-like of shape (n_samples,)\n\n    Returns:\n    - float: R-squared value rounded to three decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays for reliable math operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the total sum of squares (SST)\n    sst = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Calculate the sum of squares of residuals (SSE)\n    sse = np.sum((y_true - y_pred) ** 2)\n    \n    # Compute R-squared\n    r2 = 1 - (sse / sst)\n    \n    # Return R-squared rounded to three decimal places\n    return round(r2, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "def calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n    \n    # Check if the first row is empty\n    if not img[0]:\n        return -1\n    \n    # Determine the length of the first row\n    row_length = len(img[0])\n    \n    # Validate each row in the image\n    total_sum = 0\n    total_count = 0\n    \n    for row in img:\n        # Check for inconsistent row lengths\n        if len(row) != row_length:\n            return -1\n        \n        for pixel in row:\n            # Check if pixel is within the valid range\n            if not (0 <= pixel <= 255):\n                return -1\n            total_sum += pixel\n            total_count += 1\n    \n    # Calculate the average and round to two decimals\n    average_brightness = total_sum / total_count\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "completion": "import numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Computes the Root Mean Square Error (RMSE) between the actual values y_true \n    and the predicted values y_pred.\n\n    Parameters:\n    y_true (array-like): True/actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    TypeError: If inputs are not array-like (numpy array, list, or tuple).\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    \"\"\"\n\n    # Ensure the inputs are array-like\n    if not isinstance(y_true, (np.ndarray, list, tuple)) or not isinstance(y_pred, (np.ndarray, list, tuple)):\n        raise TypeError(\"y_true and y_pred must be array-like (numpy arrays, lists, or tuples).\")\n\n    # Convert inputs to numpy arrays of floats\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n\n    # Check for shape mismatch\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"The shape of y_true and y_pred must match.\")\n\n    # Check for empty arrays\n    if y_true.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    # Calculate the RMSE\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n\n    # Round to three decimal places\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "completion": "import numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary numpy arrays.\n    \n    Parameters:\n    -----------\n    y_true : numpy.ndarray\n        The ground truth binary labels.\n    y_pred : numpy.ndarray\n        The predicted binary labels.\n    \n    Returns:\n    --------\n    float\n        The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Ensure y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection (logical AND) and union (logical OR)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    \n    # Handle edge cases:\n    # If the union is zero, this means both arrays are all zeros,\n    # implying no positive labels in either array -> perfect match (Jaccard = 1)\n    if union == 0:\n        return 1.0\n    \n    # Calculate the Jaccard Index\n    jaccard = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice Score between two binary arrays y_true and y_pred.\n\n    Parameters:\n    -----------\n    y_true : array-like of shape (n_samples,)\n        Ground truth (correct) labels as a binary array.\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels as a binary array.\n\n    Returns:\n    --------\n    float\n        Dice Score, a value between 0 and 1, rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to boolean arrays for safety\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n\n    # Handle edge case: if there are no true positives and no predicted positives\n    if sum_true + sum_pred == 0:\n        # If both are empty (all zeros), they are effectively the same => score = 1.0\n        return 1.0\n\n    dice = (2.0 * intersection) / (sum_true + sum_pred)\n    return round(dice, 3)\n```\n\nExplanation:\n1. We ensure both inputs are binary (boolean) arrays.\n2. We compute the intersection (elements that are positive in both y_true and y_pred) via logical AND and take the sum of the result.\n3. We compute sums of y_true and y_pred to get the total number of positive elements in each.\n4. If both sums are zero, the arrays are effectively the same (both contain no positives), so we return 1.0 as the score.\n5. Otherwise, we use the standard Dice formula: dice = 2 * intersection / (sum_true + sum_pred).\n6. We round the result to three decimal places before returning."}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given row using Hyperdimensional Computing.\n\n    Args:\n        row (dict): A dictionary representing a dataset row, \n                    where keys are feature names and values are their corresponding values.\n        dim (int): Dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary mapping feature names to their reproducible random seeds.\n    \n    Returns:\n        list: The composite hypervector (as a list) representing the entire row.\n    \"\"\"\n    \n    # A simple stable hash function (deterministic for the same string representation)\n    def stable_hash(value):\n        s = str(value)\n        h = 0\n        for char in s:\n            h = (h * 31 + ord(char)) & 0xffffffff\n        return h\n\n    # We'll accumulate bound hypervectors by summation,\n    # then threshold at the end to get a binary hypervector.\n    composite_sum = np.zeros(dim, dtype=int)\n    n_features = len(row)\n\n    for feature, value in row.items():\n        # Use the seed for the feature to generate a random hypervector for the feature name.\n        np.random.seed(random_seeds[feature])\n        feature_hv = np.random.choice([0, 1], size=dim)\n\n        # Create a reproducible seed for the value by adding a stable hash of the value.\n        value_seed = (random_seeds[feature] + stable_hash(value)) & 0xffffffff\n        np.random.seed(value_seed)\n        value_hv = np.random.choice([0, 1], size=dim)\n\n        # Bind feature name HV and value HV by XOR.\n        bound_hv = np.logical_xor(feature_hv, value_hv).astype(int)\n\n        # Bundle (accumulate) all feature HVs into the composite hypervector\n        composite_sum += bound_hv\n\n    # Threshold to get a final binary hypervector (majority rule).\n    threshold = n_features / 2.0\n    composite_hv = (composite_sum >= threshold).astype(int)\n\n    # Convert numpy array to a list before returning\n    return composite_hv.tolist()\n```\n\nExplanation of the main steps:\n1. We create a simple stable_hash function to ensure the same numeric result for the same value, ensuring reproducibility.\n2. For each feature:\n   \u2022 Use the preset random seed to generate a hypervector for the feature name.  \n   \u2022 Combine this seed with a stable hash of the feature's value to generate a hypervector for the value.  \n   \u2022 Bind (XOR) these two hypervectors to get a feature-specific hypervector.  \n   \u2022 Accumulate (bundle) all feature hypervectors by summation in composite_sum.  \n3. Finally, threshold composite_sum to retrieve a binary hypervector and convert it to a Python list."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification from the given data.\n\n    Parameters:\n    data (List[List[int]]): A list where each element is [y_true, y_pred].\n\n    Returns:\n    List[List[int]]: A 2x2 confusion matrix represented as [[TN, FP], [FN, TP]].\n    \"\"\"\n    # Count the occurrences for each (y_true, y_pred) pair\n    counter = Counter((y_true, y_pred) for y_true, y_pred in data)\n\n    # Extract counts for each category\n    TN = counter[(0, 0)]\n    FP = counter[(0, 1)]\n    FN = counter[(1, 0)]\n    TP = counter[(1, 1)]\n\n    return [[TN, FP],\n            [FN, TP]]\n```\n\nExplanation:\n1. We iterate over the list (data) to build a Counter of (y_true, y_pred) pairs.\n2. Based on binary labels (0 or 1), we gather the counts for:\n   \u2022 True Negatives (TN): (y_true=0, y_pred=0)  \n   \u2022 False Positives (FP): (y_true=0, y_pred=1)  \n   \u2022 False Negatives (FN): (y_true=1, y_pred=0)  \n   \u2022 True Positives (TP): (y_true=1, y_pred=1)  \n3. The function returns a 2x2 list of lists representing the confusion matrix as [[TN, FP], [FN, TP]]."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure both vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape.\")\n    \n    # Check that vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Vectors must not be empty.\")\n    \n    # Compute the magnitudes (norms)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Check that norms are not zero\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Vectors must not have zero magnitude.\")\n    \n    # Calculate dot product\n    dot_product = np.dot(v1, v2)\n    \n    # Compute cosine similarity\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    \n    # Return the result rounded to three decimal places\n    return round(cos_sim, 3)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Computes performance metrics for a binary classification problem, including:\n    - Confusion Matrix (2x2)\n    - Accuracy\n    - F1 Score\n    - Specificity\n    - Negative Predictive Value\n\n    Parameters:\n    -----------\n    actual : list[int]\n        The actual class labels (1 for positive, 0 for negative).\n    predicted : list[int]\n        The predicted class labels (1 for positive, 0 for negative).\n\n    Returns:\n    --------\n    tuple:\n        A tuple containing:\n        - confusion_matrix: list[list[int]] (2x2 matrix)\n        - accuracy: float (rounded to three decimal places)\n        - f1_score: float (rounded to three decimal places)\n        - specificity: float (rounded to three decimal places)\n        - negative_predictive_value: float (rounded to three decimal places)\n    \"\"\"\n\n    # Ensure both lists are of equal length\n    if len(actual) != len(predicted):\n        raise ValueError(\"Length of 'actual' and 'predicted' must be the same.\")\n    \n    # Ensure all elements are 0 or 1\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be 0 or 1.\")\n\n    # Calculate elements of the confusion matrix\n    TP = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n    TN = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n    FP = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n    FN = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n\n    # Build confusion matrix: [[TN, FP], [FN, TP]]\n    confusion_matrix = [[TN, FP], [FN, TP]]\n\n    # Calculate metrics\n    total = TP + TN + FP + FN\n\n    # To avoid division by zero (in case of strange input),\n    # we handle each metric carefully:\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    precision = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n    \n    # F1 Score\n    if (precision + recall) != 0:\n        f1_score = 2 * precision * recall / (precision + recall)\n    else:\n        f1_score = 0.0\n    \n    # Specificity (True Negative Rate)\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0.0\n    \n    # Negative Predictive Value\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) != 0 else 0.0\n\n    # Round the metrics\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "import numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Returns a dictionary of descriptive statistics for the given dataset:\n    mean, median, mode, variance, standard deviation, 25th percentile,\n    50th percentile, 75th percentile, and interquartile range (IQR).\n    All numeric results are rounded to four decimal places. The mode \n    is returned as a single number if unique, or as a list if multiple modes.\n    \"\"\"\n    # Ensure data is a NumPy array\n    data = np.array(data, dtype=float)\n    \n    # Calculate descriptive statistics\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    \n    # Calculate mode(s)\n    unique_vals, counts = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    all_modes = unique_vals[counts == max_count]\n    # If there's exactly one mode, return it as a float; otherwise, return a list\n    if len(all_modes) == 1:\n        mode_val = all_modes[0]\n    else:\n        mode_val = all_modes.tolist()\n    \n    # Sample variance and standard deviation (using Bessel's correction, ddof=1)\n    variance_val = np.var(data, ddof=1)\n    std_val = np.std(data, ddof=1)\n    \n    # Percentiles\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    \n    # Interquartile range\n    iqr = p75 - p25\n    \n    # Round numeric values to four decimals\n    mean_val = round(mean_val, 4)\n    median_val = round(median_val, 4)\n    variance_val = round(variance_val, 4)\n    std_val = round(std_val, 4)\n    p25 = round(p25, 4)\n    p50 = round(p50, 4)\n    p75 = round(p75, 4)\n    iqr = round(iqr, 4)\n    \n    # If mode is a single float, round it; if it's a list, round each element\n    if isinstance(mode_val, float):\n        mode_val = round(mode_val, 4)\n    else:\n        mode_val = [round(m, 4) for m in mode_val]\n\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'mode': mode_val,\n        'variance': variance_val,\n        'standard_deviation': std_val,\n        '25th_percentile': p25,\n        '50th_percentile': p50,\n        '75th_percentile': p75,\n        'interquartile_range': iqr\n    }"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    # Using the binomial coefficient (n choose k), p^k, and (1-p)^(n-k)\n    combinations = math.comb(n, k)\n    probability = combinations * (p ** k) * ((1 - p) ** (n - k))\n    return round(probability, 5)\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Mathematical constant for 2*pi\n    two_pi = 2 * math.pi\n    \n    # Calculate the exponent term\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    \n    # Calculate the coefficient for the PDF\n    coefficient = 1 / (std_dev * math.sqrt(two_pi))\n    \n    # Calculate the PDF\n    pdf_value = coefficient * math.exp(exponent)\n    \n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf_value, 5)\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    # Ensure k is a non-negative integer\n    if k < 0 or not isinstance(k, int):\n        raise ValueError(\"k must be a non-negative integer.\")\n\n    # Poisson distribution formula: P(k; \u03bb) = (\u03bb^k * e^(-\u03bb)) / k!\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n\n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image by taking the difference\n    between the maximum and the minimum pixel values.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image \n                             with pixel values between 0 and 255.\n\n    Returns:\n        int or float: The contrast value (max pixel - min pixel).\n    \"\"\"\n    # Ensure the input is a 2D array\n    if len(img.shape) != 2:\n        raise ValueError(\"Input image must be a 2D array.\")\n    \n    # Calculate contrast\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n\n    return contrast\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Ensure both vectors are 1D\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D numpy arrays.\")\n\n    # Ensure both vectors have the same size\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Both vectors must have the same number of elements.\")\n\n    # Calculate and return the dot product\n    return float(np.dot(vec1, vec2))\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial\n                           features of the corresponding data point. Each value is\n                           rounded to 8 decimal places. If degree is less than 0,\n                           an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    transformed_data = []\n    for x in data:\n        transformed_row = []\n        for d in range(degree + 1):\n            transformed_row.append(round(x**d, 8))\n        transformed_data.append(transformed_row)\n    \n    return transformed_data\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    # Return -1 if position is 0 or d_model <= 0\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create array of positions and array of dimensions\n    pos = np.arange(position)[:, np.newaxis]\n    dim = np.arange(d_model)[np.newaxis, :]\n\n    # Compute angle rates by dividing position by 10000^(2*(dim//2)/d_model)\n    angle_rates = pos / np.power(10000, (2 * (dim // 2)) / np.float32(d_model))\n\n    # Initialize the positional encoding matrix\n    pos_enc = np.zeros((position, d_model), dtype=np.float32)\n\n    # Apply sine to even indices (0, 2, 4, ...)\n    pos_enc[:, 0::2] = np.sin(angle_rates[:, 0::2])\n\n    # Apply cosine to odd indices (1, 3, 5, ...)\n    pos_enc[:, 1::2] = np.cos(angle_rates[:, 1::2])\n\n    # Convert to float16 and return as list\n    return pos_enc.astype(np.float16).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n\n    # Check for overfitting\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    \n    # Check for underfitting\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n\n    # Otherwise, it's a good fit\n    return 0\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value (scalar or np.ndarray)\n    :param grad: Current gradient (scalar or np.ndarray)\n    :param m: First moment estimate (scalar or np.ndarray)\n    :param v: Second moment estimate (scalar or np.ndarray)\n    :param t: Current timestep (integer)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter_list, updated_m_list, updated_v_list)\n             Each returned array is rounded to 5 decimals and converted to a list.\n    \"\"\"\n    # Ensure inputs are at least 1D arrays to have consistent .tolist() output\n    parameter = np.atleast_1d(parameter)\n    grad = np.atleast_1d(grad)\n    m = np.atleast_1d(m)\n    v = np.atleast_1d(v)\n    \n    # Update first moment estimate\n    m_new = beta1 * m + (1 - beta1) * grad\n    \n    # Update second moment estimate\n    v_new = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first and second moment estimates\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter_new = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 5 decimals and convert to list\n    parameter_new = np.round(parameter_new, 5).tolist()\n    m_new = np.round(m_new, 5).tolist()\n    v_new = np.round(v_new, 5).tolist()\n    \n    return parameter_new, m_new, v_new\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    # Minimal set of hyperparameters for demonstration\n    hparams = {\n        \"n_ctx\": 1024,  # maximum sequence length\n        \"n_head\": 12    # number of attention heads (not strictly used here)\n    }\n\n    # Dummy model parameters\n    # wte: word token embedding, wpe: positional embedding\n    # blocks: transformer blocks (empty for this demo), ln_f: final layer norm params\n    params = {\n        \"wte\": np.random.rand(3, 10),       # 3 tokens in our dummy vocab, embedding dim=10\n        \"wpe\": np.random.rand(1024, 10),    # positional embeddings (for up to 1024 positions)\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    \"\"\"\n    Generate text using a simplified GPT-2-like process:\n      1. Convert prompt to token IDs (token + positional embeddings).\n      2. Pass through a tiny \"Transformer block\" with:\n         - Single-head self-attention.\n         - Feed-forward network.\n         - Layer normalization.\n      3. Compute logits for the next token and sample from them.\n      4. Repeat for the requested number of tokens.\n    \"\"\"\n    np.random.seed(42)  # Fixed seed for reproducibility\n\n    # -----------------\n    # 1. Load encoder & parameters\n    # -----------------\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    n_ctx = hparams[\"n_ctx\"]\n    \n    # -----------------\n    # 2. Tokenize prompt\n    # -----------------\n    token_ids = encoder.encode(prompt)\n    \n    # -----------------\n    # Helpful sub-functions\n    # -----------------\n    \n    def layer_norm(x, g, b, eps=1e-5):\n        \"\"\"Applies layer normalization to tensor x.\"\"\"\n        mean = x.mean(axis=-1, keepdims=True)\n        var = x.var(axis=-1, keepdims=True)\n        return g * (x - mean) / np.sqrt(var + eps) + b\n\n    # Create random Q, K, V, FF weights (tiny demonstration; real GPT-2 has many more params).\n    embed_dim = params[\"wte\"].shape[1]\n    Q_w = np.random.randn(embed_dim, embed_dim)\n    K_w = np.random.randn(embed_dim, embed_dim)\n    V_w = np.random.randn(embed_dim, embed_dim)\n    \n    # Feed-forward layers\n    ff_w1 = np.random.randn(embed_dim, embed_dim * 4)\n    ff_b1 = np.random.randn(embed_dim * 4)\n    ff_w2 = np.random.randn(embed_dim * 4, embed_dim)\n    ff_b2 = np.random.randn(embed_dim)\n    \n    # Projection from hidden dimension to vocab (3 tokens: hello, world, <UNK>)\n    proj_w = np.random.randn(embed_dim, 3)\n    proj_b = np.random.randn(3)\n\n    def single_head_self_attention(x):\n        \"\"\"\n        A single-head self-attention over the sequence x.\n        x shape: [seq_len, embed_dim].\n        \"\"\"\n        q = x @ Q_w  # [seq_len, embed_dim]\n        k = x @ K_w  # [seq_len, embed_dim]\n        v = x @ V_w  # [seq_len, embed_dim]\n\n        # Compute attention scores\n        d_k = np.sqrt(k.shape[-1])\n        scores = (q @ k.transpose()) / d_k  # [seq_len, seq_len]\n\n        # Causal mask to prevent attending to future tokens\n        seq_len = x.shape[0]\n        causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n        scores[causal_mask] = -1e9  # block future info\n\n        # Softmax\n        att_weights = np.exp(scores - scores.max(axis=-1, keepdims=True))\n        att_weights /= att_weights.sum(axis=-1, keepdims=True)\n\n        # Weighted sum of values\n        return att_weights @ v  # [seq_len, embed_dim]\n\n    def feed_forward(h):\n        \"\"\"A tiny feed-forward network.\"\"\"\n        h1 = np.maximum(0, h @ ff_w1 + ff_b1)  # ReLU\n        return h1 @ ff_w2 + ff_b2\n\n    def transformer_block(x):\n        \"\"\"\n        A minimal, single-layer Transformer block:\n          - self-attention\n          - add & layer norm\n          - feed-forward\n          - add & layer norm\n        \"\"\"\n        # Self-attention\n        attn_out = single_head_self_attention(x)\n        # Add & Norm\n        x = layer_norm(x + attn_out, g=params[\"ln_f\"][\"g\"], b=params[\"ln_f\"][\"b\"])\n        # Feed forward\n        ff_out = feed_forward(x)\n        # Add & Norm\n        x = layer_norm(x + ff_out, g=params[\"ln_f\"][\"g\"], b=params[\"ln_f\"][\"b\"])\n        return x\n\n    def logits_for_next_token(x):\n        \"\"\"\n        Given the final hidden state for each token,\n        produce logits for next-token prediction\n        using a linear projection to the vocabulary size (3).\n        We'll use the hidden state of the last position.\n        \"\"\"\n        last_token_hidden = x[-1]  # shape: [embed_dim]\n        return last_token_hidden @ proj_w + proj_b  # shape: [3]\n\n    def sample_from_logits(logits):\n        \"\"\"Sample from the given logits (3 possible tokens).\"\"\"\n        # Softmax\n        probs = np.exp(logits - np.max(logits))\n        probs = probs / probs.sum()\n        return np.random.choice(len(probs), p=probs)\n\n    # -----------------\n    # 3. Autoregressive generation loop\n    # -----------------\n    for _ in range(n_tokens_to_generate):\n        # If we exceed context window, truncate from the left\n        if len(token_ids) > n_ctx:\n            token_ids = token_ids[-n_ctx:]\n\n        # Build embeddings for current tokens\n        # Each token: wte[token_id] + wpe[position]\n        x = []\n        for i, tid in enumerate(token_ids):\n            x.append(params[\"wte\"][tid] + params[\"wpe\"][i])\n        x = np.array(x)  # shape: [seq_len, embed_dim]\n\n        # Pass through our tiny Transformer block\n        hidden_states = transformer_block(x)\n\n        # Get logits for the next token\n        logits = logits_for_next_token(hidden_states)\n\n        # Sample the next token id\n        next_token_id = sample_from_logits(logits)\n\n        # Append it to our generated sequence\n        token_ids.append(next_token_id)\n\n    # -----------------\n    # 4. Decode the final sequence\n    # -----------------\n    generated_text = encoder.decode(token_ids)\n    return generated_text\n```\n\nExplanation:\n1. We load dummy parameters and an encoder that maps a small vocabulary (\"hello\", \"world\", \"<UNK>\") to indices.  \n2. We encode the prompt into token IDs.  \n3. For each new token to generate, we:  \n   \u2022 Construct embeddings by adding token embeddings (wte) and positional embeddings (wpe).  \n   \u2022 Pass them through a simplified single-layer \"Transformer block\" which includes:  \n     \u2013 Self-Attention (with a causal mask).  \n     \u2013 Add & Layer Norm.  \n     \u2013 Simple Feed-Forward Network.  \n     \u2013 Add & Layer Norm again.  \n   \u2022 Project the last hidden state to the vocabulary dimension to get logits for the next token.  \n   \u2022 Sample the next token based on a softmax over those logits.  \n4. Finally, we decode the full sequence of token IDs back into text.  \n\nThis is, of course, a vastly simplified version of GPT-2, but it illustrates the core concepts of token/positional embeddings, (single-head) self-attention, feed-forward networks, and layer normalization in an autoregressive text generation loop."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Standard 1D softmax for numerical stability\n        shifted = values - np.max(values)\n        exp_vals = np.exp(shifted)\n        return exp_vals / np.sum(exp_vals)\n    \n    crystals = np.array(crystal_values, dtype=float)\n    sqrt_d = np.sqrt(dimension)\n    \n    # Compute the raw attention scores\n    # att_score[i, j] = sqrt(dimension) * (crystals[i] * crystals[j])\n    att_score = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            att_score[i, j] = sqrt_d * (crystals[i] * crystals[j])\n    \n    # For each crystal i:\n    #   1) softmax over its row (att_score[i])\n    #   2) compute weighted sum of crystal values\n    #   3) multiply by dimension to get final pattern\n    result = []\n    for i in range(n):\n        row_attention = softmax(att_score[i])\n        weighted_sum = 0.0\n        for j in range(n):\n            weighted_sum += row_attention[j] * crystals[j]\n        # Final pattern is dimension * weighted_sum\n        final_value = dimension * weighted_sum\n        result.append(round(final_value, 4))\n    \n    return result\n```\n\nExplanation of the simplified self-attention steps:\n1. We first define a softmax function for numerical stability.  \n2. We interpret each crystal\u2019s \u201cquery\u201d and \u201ckey\u201d as its value scaled by \u221a(dimension), so the attention score between crystals i and j becomes \u221a(dimension) \u00d7 (value_i \u00d7 value_j).  \n3. For each crystal i, we apply softmax to its row of scores, producing a distribution across all crystals j.  \n4. We then form a weighted sum of the original crystal values using these attention weights.  \n5. Finally, we multiply by \u201cdimension\u201d to produce the enhanced pattern for crystal i.  \n6. The result is a list of floats (length N), each rounded to four decimal places."}
{"task_id": 90, "completion_id": 0, "completion": "import numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query against a given corpus.\n\n    Parameters:\n    -----------\n    corpus : list of str\n        The corpus, where each element is a document (string).\n    query : str\n        The query string.\n    k1 : float, optional\n        Term frequency saturation parameter in BM25.\n    b : float, optional\n        Document length normalization parameter in BM25.\n\n    Returns:\n    --------\n    list of float\n        A list containing the BM25 score for each document, rounded to three decimals.\n    \"\"\"\n\n    # 1. Tokenize the corpus and query\n    tokenized_corpus = [doc.split() for doc in corpus]\n    query_terms = query.split()\n    \n    # 2. Compute document frequencies (df) for all terms in the corpus\n    #    and store word counts for each document\n    N = len(tokenized_corpus)\n    doc_word_counts = []\n    df_counts = Counter()\n    \n    for doc_tokens in tokenized_corpus:\n        counts = Counter(doc_tokens)\n        doc_word_counts.append(counts)\n        # Update document frequency counts only once per term per doc\n        for term in set(doc_tokens):\n            df_counts[term] += 1\n    \n    # 3. Precompute average document length\n    doc_lengths = [sum(counts.values()) for counts in doc_word_counts]\n    avg_doc_length = np.mean(doc_lengths) if doc_lengths else 0.0\n\n    # 4. Function to calculate IDF for a term\n    def idf(term):\n        df = df_counts.get(term, 0)\n        # BM25 IDF formula\n        return np.log((N - df + 0.5) / (df + 0.5) + 1)\n\n    # 5. Calculate scores for each document\n    scores = []\n    for i, counts in enumerate(doc_word_counts):\n        score = 0.0\n        doc_len = doc_lengths[i]\n        for term in query_terms:\n            # term frequency in the document\n            f_td = counts.get(term, 0)\n            if f_td > 0:                \n                numerator = f_td * (k1 + 1)\n                denominator = f_td + k1 * (1 - b + b * (doc_len / avg_doc_length))\n                score += idf(term) * (numerator / denominator)\n        scores.append(round(score, 3))\n\n    return scores"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels (binary classification).\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    \n    # Initialize counts\n    tp = 0  # True Positives\n    fp = 0  # False Positives\n    fn = 0  # False Negatives\n\n    # Count TP, FP, FN\n    for true_label, pred_label in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            tp += 1\n        elif true_label == 0 and pred_label == 1:\n            fp += 1\n        elif true_label == 1 and pred_label == 0:\n            fn += 1\n\n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall = tp / (tp + fn) if (tp + fn) else 0.0\n\n    # Calculate F1 score\n    if (precision + recall) == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * precision * recall / (precision + recall)\n    \n    return round(f1_score, 3)\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # consumption_data is a list of length 10 with daily measurements for days 1..10.\n    \n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    days = list(range(1, 11))  # [1, 2, ..., 10]\n    detrended = []\n    for i in days:\n        fluctuation = 10 * math.sin(2 * PI * i / 10.0)\n        detrended_value = consumption_data[i - 1] - fluctuation\n        detrended.append(detrended_value)\n    \n    # 2) Perform linear regression on the detrended data.\n    #    We'll use the standard least squares formula for slope and intercept.\n    n = len(days)\n    x_mean = sum(days) / n\n    y_mean = sum(detrended) / n\n    \n    # Numerator and denominator for slope\n    numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(days, detrended))\n    denominator = sum((x - x_mean) ** 2 for x in days)\n    slope = numerator / denominator if denominator != 0 else 0.0\n    intercept = y_mean - slope * x_mean\n    \n    # 3) Predict day 15's base consumption.\n    day_15_base = intercept + slope * 15\n    \n    # 4) Add the day 15 fluctuation back (10 * sin(2\u03c0 * 15 / 10)).\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10.0)\n    day_15_prediction = day_15_base + fluctuation_15\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    day_15_rounded = round(day_15_prediction)\n    final_forecast = math.ceil(day_15_rounded * 1.05)\n    \n    # 6) Return the final integer.\n    return final_forecast\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute errors\n    absolute_errors = np.abs(y_true - y_pred)\n    # Compute the mean of these absolute errors\n    mean_absolute_error = np.mean(absolute_errors)\n    # Return the result rounded to 3 decimal places\n    return round(mean_absolute_error, 3)\n```"}
{"task_id": 94, "completion_id": 0, "completion": "Below is a complete implementation following the prompt. The key functions are:\n\n1) compute_qkv(X, W_q, W_k, W_v)  \n2) self_attention(Q, K, V)  \n3) multi_head_attention(Q, K, V, n_heads)  \n\nNote that we assume n is divisible by n_heads for simplicity.\n\n--------------------------------------------------------------------------------\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute Q, K, V as simple linear transformations of input X:\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute scaled dot-product attention:\n      scores = QK^T / sqrt(d_k)\n      weights = softmax(scores)\n      output = weights @ V\n    Q, K, V shapes: (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    # QK^T -> shape: (m, m)\n    scores = Q @ K.T / np.sqrt(d_k)\n    # Softmax row-wise\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    # Output: (m, d)\n    output = weights @ V\n    return output\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Split Q, K, V into multiple heads, apply self_attention for each head,\n    then concatenate the results. Q, K, V shapes: (m, d). \n    We assume d is divisible by n_heads.\n    \"\"\"\n    m, d = Q.shape\n    d_head = d // n_heads\n    \n    # Split Q, K, V into multiple heads:\n    # new shape: (m, n_heads, d_head)\n    Q_heads = Q.reshape(m, n_heads, d_head)\n    K_heads = K.reshape(m, n_heads, d_head)\n    V_heads = V.reshape(m, n_heads, d_head)\n    \n    # Process each head separately\n    outputs = []\n    for i in range(n_heads):\n        Q_i = Q_heads[:, i, :]  # (m, d_head)\n        K_i = K_heads[:, i, :]  # (m, d_head)\n        V_i = V_heads[:, i, :]  # (m, d_head)\n        head_out = self_attention(Q_i, K_i, V_i)  # (m, d_head)\n        outputs.append(head_out)\n    \n    # Concatenate along the last dimension: (m, d)\n    return np.concatenate(outputs, axis=-1)\n\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()"}
{"task_id": 95, "completion_id": 0, "completion": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n        x (list[int]): A list of binary values (0 or 1).\n        y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n        float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Ensure x and y have the same length\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n    \n    # Count occurrences in the 2x2 contingency table\n    n_11 = n_10 = n_01 = n_00 = 0\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            n_11 += 1\n        elif xi == 1 and yi == 0:\n            n_10 += 1\n        elif xi == 0 and yi == 1:\n            n_01 += 1\n        elif xi == 0 and yi == 0:\n            n_00 += 1\n    \n    # Calculate row and column sums\n    n_1_ = n_11 + n_10  # x=1\n    n_0_ = n_01 + n_00  # x=0\n    n__1 = n_11 + n_01  # y=1\n    n__0 = n_10 + n_00  # y=0\n    \n    # Compute Phi coefficient\n    numerator = (n_11 * n_00) - (n_10 * n_01)\n    denominator = (n_1_ * n_0_ * n__1 * n__0) ** 0.5\n    \n    # Handle case where denominator is 0\n    if denominator == 0:\n        return 0.0\n    \n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return (0.2 * x) + 0.5\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    softplus(x) = log(1 + e^x)\n\n    This function handles edge cases to prevent numerical overflow or underflow\n    by using a piecewise approach.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value, rounded to 4 decimal places.\n    \"\"\"\n    if x < 0:\n        # For negative x, directly compute to avoid overflow from e^x when x is large negative\n        result = math.log1p(math.exp(x))\n    else:\n        # For non-negative x, re-factor to avoid overflow in e^x\n        result = x + math.log1p(math.exp(-x))\n\n    return round(result, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value (rounded to 6 decimals).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_ref = np.array(pi_theta_ref, dtype=np.float64)\n\n    # 1. Compute the clipped-objective portion (similar to PPO)\n    #    We take the minimum of the \"unclipped\" objective and the \"clipped\" objective\n    unclipped_obj = rhos * A\n    clipped_obj = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon) * A\n    clipped_advantage = np.minimum(unclipped_obj, clipped_obj)\n    clipped_advantage_sum = np.sum(clipped_advantage)\n\n    # 2. Compute pi_theta from rhos and pi_theta_old\n    #    pi_theta[i] = (rhos[i] * pi_old[i]) / Z, where Z is the normalization term\n    pi_theta_unnormalized = rhos * pi_old\n    normalization = np.sum(pi_theta_unnormalized)\n    # Add a small epsilon to avoid division by zero\n    pi_theta = pi_theta_unnormalized / (normalization + 1e-10)\n\n    # 3. Compute the KL(pi_theta || pi_theta_ref)\n    #    KL = sum_i [ pi_theta[i] * log( pi_theta[i] / pi_theta_ref[i] ) ]\n    #    We add a small epsilon inside the log to avoid numerical issues.\n    eps = 1e-10\n    kl_div = np.sum(pi_theta * np.log((pi_theta + eps) / (pi_ref + eps)))\n\n    # 4. Final GRPO objective: clipped advantage - beta * KL\n    objective = clipped_advantage_sum - beta * kl_div\n\n    # 5. Return the result rounded to 6 decimals\n    return float(np.round(objective, 6))\n```\n\nExplanation of key steps:\n\u2022 We first compute the \u201cclipped\u201d vs. \u201cunclipped\u201d portions of the surrogate objective (as done in PPO), using the likelihood ratios (rhos) and advantage estimates (A).  \n\u2022 We then construct the new policy distribution \u03c0\u03b8 by multiplying the old policy probabilities by rhos and normalizing.  \n\u2022 We compute the KL divergence with respect to the reference policy \u03c0\u03b8_ref, and apply the penalty term (-\u03b2 \u00d7 KL).  \n\u2022 The final objective is the sum of the clipped surrogate minus the KL penalty term.  \n\u2022 The result is returned with a numerical precision of 6 digits after the decimal point."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function: x * sigmoid(x)\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid_x\n    return round(swish_val, 4)\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    # Use expm1 for better numerical stability when x is negative\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * math.expm1(x)\n    \n    # Round to 4 decimal places\n    return round(selu_value, 4)\n```"}
{"task_id": 104, "completion_id": 0, "completion": "import numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a Python list\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    logits = np.dot(X, weights) + bias\n\n    # Apply the sigmoid function\n    probabilities = 1 / (1 + np.exp(-logits))\n\n    # Generate binary predictions with a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n\n    # Return predictions as a Python list\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of samples (N) and number of features (C)\n    N, C = X.shape\n\n    # Number of classes (M)\n    M = np.max(y) + 1\n\n    # Convert labels to one-hot encoding (N x M)\n    Y = np.eye(M)[y]\n\n    # Initialize parameter matrix B (C x M) with zeros\n    B = np.zeros((C, M))\n\n    # List to store loss values\n    losses = []\n\n    for _ in range(iterations):\n        # Forward pass: compute raw scores (N x M)\n        scores = X @ B\n        \n        # Compute softmax probabilities (N x M)\n        exps = np.exp(scores)\n        probs = exps / np.sum(exps, axis=1, keepdims=True)\n\n        # Compute Cross Entropy loss\n        # add a small value in log to avoid numerical issues\n        loss = -np.sum(Y * np.log(probs + 1e-10)) / N\n        losses.append(loss)\n\n        # Compute gradient for B\n        grad = (X.T @ (probs - Y)) / N\n\n        # Update the parameters\n        B -= learning_rate * grad\n\n    # Round results\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(val, 4) for val in losses]\n\n    return B_rounded, losses_rounded\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters \n    with Binary Cross Entropy loss.\n    All returned values are rounded to the 4th decimal place.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Labels array of shape (n_samples, ).\n    learning_rate : float\n        Step size for gradient descent updates.\n    iterations : int\n        Number of gradient descent steps.\n    \n    Returns:\n    --------\n    tuple[list[float], list[float]]\n        A tuple of:\n        - the final fitted parameters as a list of floats\n        - the history of loss values over iterations as a list of floats\n    \"\"\"\n    # Ensure y is a 1D array\n    y = y.flatten()\n\n    # Add a column of ones to X for the intercept term\n    X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n    n_samples, n_features = X_ext.shape\n    \n    # Initialize parameters to zeros\n    params = np.zeros(n_features, dtype=float)\n    \n    # To avoid taking log(0)\n    epsilon = 1e-10\n    \n    # Container for loss values\n    loss_values = []\n\n    for _ in range(iterations):\n        # Compute prediction using sigmoid\n        z = X_ext @ params\n        y_pred = 1 / (1 + np.exp(-z))\n        \n        # Compute loss (Binary Cross Entropy)\n        # Add a small epsilon to avoid log(0)\n        loss = - np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n        \n        # Store rounded loss\n        loss_values.append(round(loss, 4))\n        \n        # Compute gradient\n        grad = (1 / n_samples) * (X_ext.T @ (y_pred - y))\n        \n        # Update parameters\n        params -= learning_rate * grad\n    \n    # Round parameters to 4 decimal places and convert to list\n    params = np.round(params, 4).tolist()\n    \n    return params, loss_values\n```"}
{"task_id": 107, "completion_id": 0, "completion": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    Q, K, V: shape (seq_length, d_model) or similar\n    mask: shape (seq_length, seq_length) \n          with 1's for valid positions and 0's (or False) for masked positions\n    Returns attention outputs as a list (seq_length, d_model).\n    \"\"\"\n    # Compute raw attention scores: (seq_len, d_k) dot (d_k, seq_len) -> (seq_len, seq_len)\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply mask by setting masked positions to a large negative number\n    # so that they are essentially zero after softmax.\n    # Assuming mask has the same shape as scores (seq_len, seq_len)\n    large_negative = -1e9\n    scores = np.where(mask > 0, scores, large_negative)\n    \n    # Compute softmax along the last dimension\n    # (for each row in scores, subtract max to ensure numerical stability)\n    scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    scores_exp_sum = np.sum(scores_exp, axis=-1, keepdims=True)\n    attention_weights = scores_exp / (scores_exp_sum + 1e-9)\n    \n    # Multiply attention weights by V to get output\n    # (seq_len, seq_len) dot (seq_len, d_v) -> (seq_len, d_v)\n    output = np.dot(attention_weights, V)\n    \n    # Return output as a Python list\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "completion": "```python\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Disorder is measured using the Shannon entropy of the color distribution.\n    The disorder value is 0 if all apples are the same color, and increases with more variety.\n    Results are rounded to the nearest 4th decimal.\n    \"\"\"\n    from math import log2\n    \n    # If the list is empty or all of one color, disorder is 0\n    if len(apples) <= 1:\n        return 0.0\n    \n    # Count occurrences of each color\n    color_counts = {}\n    for color in apples:\n        color_counts[color] = color_counts.get(color, 0) + 1\n    \n    # Calculate total number of apples\n    total_apples = len(apples)\n    \n    # Compute Shannon entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total_apples\n        entropy -= p * log2(p)\n    \n    # Round to 4 decimal places\n    return round(entropy, 4)\n```"}
{"task_id": 109, "completion_id": 0, "completion": "import numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Layer Normalization on the given 3D tensor X with shape \n    (batch_size, sequence_length, feature_dim).\n    \n    Normalizes X across the last dimension (feature dimension) for each element \n    in the batch and sequence, then applies the supplied scaling (gamma) and \n    shifting (beta) parameters.\n\n    The output is rounded to 5 decimal places and returned as a nested list.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scale parameter of shape (feature_dim,).\n    beta (np.ndarray): Shift parameter of shape (feature_dim,).\n    epsilon (float): A small constant to avoid division by zero.\n\n    Returns:\n    list: A Python list containing the normalized and scaled data, \n          rounded to 5 decimal places.\n    \"\"\"\n    # Compute the mean and variance along the last dimension\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    \n    # Normalize\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    X_normalized = X_normalized * gamma + beta\n    \n    # Round to 5 decimal places and convert to list\n    X_out = np.round(X_normalized, 5).tolist()\n    \n    return X_out"}
{"task_id": 110, "completion_id": 0, "completion": "import numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for a single reference and candidate translation.\n\n    Parameters:\n    reference (str or list): Reference translation (either a tokenized list or space-delimited string).\n    candidate (str or list): Candidate translation (either a tokenized list or space-delimited string).\n    alpha (float): Weight parameter (default: 0.9).\n    beta (float): Penalty exponent (default: 3).\n    gamma (float): Penalty coefficient (default: 0.5).\n\n    Returns:\n    float: METEOR score rounded to three decimal places.\n    \"\"\"\n\n    # If the inputs are strings, split them on whitespace to get tokens.\n    if isinstance(reference, str):\n        reference_tokens = reference.strip().split()\n    else:\n        reference_tokens = reference\n\n    if isinstance(candidate, str):\n        candidate_tokens = candidate.strip().split()\n    else:\n        candidate_tokens = candidate\n\n    # Count unigrams in reference and candidate\n    ref_counts = Counter(reference_tokens)\n    cand_counts = Counter(candidate_tokens)\n\n    # Total number of unigram matches\n    common_words = set(ref_counts.keys()) & set(cand_counts.keys())\n    matches = sum(min(ref_counts[w], cand_counts[w]) for w in common_words)\n\n    # If no matches, return zero\n    if matches == 0:\n        return 0.0\n\n    # Compute precision and recall\n    precision = matches / len(candidate_tokens)\n    recall = matches / len(reference_tokens)\n\n    # Compute F-mean\n    f_mean = (precision * recall) / ((1 - alpha) * precision + alpha * recall)\n\n    # Compute the fragmentation penalty:\n    # We need to align matched words in the candidate to their positions in the reference.\n    # Then count how many contiguous segments (chunks) of matched words there are.\n    # penalty = gamma * (chunks / matches)^beta\n\n    # Make local copies to track usage\n    ref_positions = {}\n    for i, w in enumerate(reference_tokens):\n        ref_positions.setdefault(w, []).append(i)\n\n    # We'll pop positions from ref_positions as we align them\n    ref_counts_for_alignment = Counter(ref_counts)  # copy for alignment\n\n    aligned_positions = []\n    for w in candidate_tokens:\n        if ref_counts_for_alignment[w] > 0:\n            aligned_positions.append(ref_positions[w][0])  # take the first available position\n            ref_positions[w].pop(0)                       # remove that position\n            ref_counts_for_alignment[w] -= 1\n\n    # Count how many chunks (contiguous in reference order)\n    chunks = 1\n    for i in range(1, len(aligned_positions)):\n        if aligned_positions[i] != aligned_positions[i - 1] + 1:\n            chunks += 1\n\n    penalty = gamma * ((chunks / matches) ** beta)\n    penalty = min(penalty, 1.0)  # penalty should not exceed 1.0\n\n    # Final METEOR score\n    meteor = (1 - penalty) * f_mean\n\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events X and Y.\n\n    PMI(X, Y) = log2( p(X,Y) / ( p(X) * p(Y) ) )\n              = log2( (joint_counts / total_samples) / \n                      ((total_counts_x / total_samples) * (total_counts_y / total_samples)) )\n\n    :param joint_counts: Number of times X and Y jointly occur\n    :param total_counts_x: Number of times X occurs\n    :param total_counts_y: Number of times Y occurs\n    :param total_samples: Total number of samples\n    :return: PMI value rounded to 3 decimal places (float)\n    \"\"\"\n    # Convert counts to probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n\n    # If the joint probability is zero, PMI is negative infinity by definition\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')\n\n    # Compute PMI using log base 2\n    pmi = np.log2(p_xy / (p_x * p_y))\n\n    # Return PMI rounded to 3 decimal places\n    return float(round(pmi, 3))\n```"}
{"task_id": 112, "completion_id": 0, "completion": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n    \n    Returns a list of floats, each rounded to 4 decimal places.\n    \"\"\"\n    if not x:  # handle empty list case (optional)\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # If all values are the same, return all zeros\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    \n    normalized = []\n    for val in x:\n        norm_val = (val - min_val) / (max_val - min_val)\n        normalized.append(round(norm_val, 4))\n    \n    return normalized"}
{"task_id": 113, "completion_id": 0, "completion": "import numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple residual block that takes a 1D input array x,\n    applies two dense layers with ReLU activations, adds\n    the original input x (shortcut), and applies a final ReLU.\n    \n    The final output is rounded to 4 decimal places and\n    returned as a list.\n    \"\"\"\n    # Define ReLU\n    relu = lambda z: np.maximum(0, z)\n    \n    # First layer + ReLU\n    z1 = x @ w1\n    a1 = relu(z1)\n    \n    # Second layer + ReLU\n    z2 = a1 @ w2\n    a2 = relu(z2)\n    \n    # Shortcut connection + final ReLU\n    out = relu(a2 + x)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    The input shape is (height, width, channels).\n    The returned 1D array has shape (channels,), \n    where each element is the average of all values \n    in the corresponding feature map.\n    \"\"\"\n    # Ensure x is 3D\n    assert len(x.shape) == 3, \"Input should be a 3D array (height, width, channels).\"\n    \n    # Compute the mean across the height and width dimensions\n    return np.mean(x, axis=(0, 1))\n```"}
{"task_id": 115, "completion_id": 0, "completion": "import numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Batch Normalization on a 4D input (BCHW).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width).\n    gamma : np.ndarray\n        Scale parameters (one per channel).\n    beta : np.ndarray\n        Shift parameters (one per channel).\n    epsilon : float, optional\n        A small value added to variance for numerical stability. Default is 1e-5.\n\n    Returns:\n    --------\n    list\n        The batch-normalized array rounded to 4 decimal places, converted to a Python list.\n    \"\"\"\n\n    # Compute mean and variance across batch + spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n\n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n\n    # Reshape gamma and beta to match X's dimensions for broadcasting\n    gamma_reshaped = gamma.reshape((1, -1, 1, 1))\n    beta_reshaped = beta.reshape((1, -1, 1, 1))\n\n    # Apply scale and shift\n    out = gamma_reshaped * X_norm + beta_reshaped\n\n    # Round to 4 decimal places and convert to list\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term (c * x^n) at the point x.\n    \n    :param c: Coefficient of the polynomial term\n    :param x: The point at which to evaluate the derivative\n    :param n: Exponent of the polynomial term\n    :return: The derivative value rounded to 4 decimal places\n    \"\"\"\n    derivative_value = c * n * (x ** (n - 1))\n    return round(derivative_value, 4)\n```"}
{"task_id": 117, "completion_id": 0, "completion": "import numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by the given 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors, e.g. [[x1, y1], [x2, y2], ...].\n    tol (float): Tolerance for determining linear independence.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (each rounded to 4 decimals) \n                       that spans the same subspace as 'vectors'.\n    \"\"\"\n    # Convert input to numpy arrays\n    arrs = [np.array(vec, dtype=float) for vec in vectors]\n    basis = []\n\n    for v in arrs:\n        # Make a copy to modify\n        w = v.copy()\n        # Subtract projections onto the existing basis vectors\n        for b in basis:\n            w -= np.dot(w, b) * b\n\n        # Normalize if above tolerance\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            basis.append(w / norm_w)\n\n    # Round to 4 decimals and convert each to a list\n    return [np.round(vec, 4).tolist() for vec in basis]"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Convert inputs to numpy arrays\n    a = np.array(a, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Compute the cross product\n    cross_prod = np.cross(a, b)\n    \n    # Round the result to 4 decimal places\n    cross_prod_rounded = np.round(cross_prod, 4)\n    \n    # Convert to list and return\n    return cross_prod_rounded.tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "import numpy as np\n\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Calculate the determinant of A\n    detA = np.linalg.det(A)\n    \n    # Check if determinant is (close to) zero, implying no unique solution\n    if np.isclose(detA, 0.0):\n        return -1\n    \n    n = A.shape[0]\n    x = []\n    \n    # Apply Cramer's Rule\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b  # Replace the i-th column with vector b\n        detA_i = np.linalg.det(A_i)\n        x_val = detA_i / detA\n        x.append(x_val)\n    \n    # Round results to 4 decimals and convert to Python list\n    x = np.round(x, 4).tolist()\n    \n    return x"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if lists are empty or have different lengths\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n    \n    # Compute the Bhattacharyya coefficient: sum of sqrt(p_i * q_i)\n    bc = sum(np.sqrt(pi * qi) for pi, qi in zip(p, q))\n    \n    # Bhattacharyya distance is -ln(bc)\n    # If bc is 0 or negative (which theoretically shouldn't happen if p and q are valid distributions),\n    # let it return a large number or handle gracefully. Here we simply check bc <= 0.\n    if bc <= 0:\n        return 0.0  # or float('inf'), depending on the interpretation\n    \n    distance = -np.log(bc)\n    \n    # Return the distance rounded to 4 decimal places\n    return round(distance, 4)\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    '''\n    Returns the element-wise sum of two vectors (lists).\n    If the vectors have different lengths, returns -1.\n    '''\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n```"}
{"task_id": 122, "completion_id": 0, "completion": "import numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Implements the REINFORCE policy gradient estimator.\n    \n    The policy is parameterized by theta, which has shape (num_states, num_actions).\n    We compute a softmax over each row theta[s, :] to get the action probabilities\n    for state s. For each episode and time step, we multiply the gradient of the log\n    probability of the taken action by the return from that time step onward. Then we\n    average across all episodes.\n\n    Parameters:\n    -----------\n    theta    : np.ndarray\n               Array of shape (num_states, num_actions), the policy parameters.\n    episodes : list of lists\n               Each element is an episode, which is a list of (state, action, reward) tuples.\n\n    Returns:\n    --------\n    list\n        The averaged gradient (same shape as theta), rounded to 4 decimals, returned as a Python list.\n    \"\"\"\n\n    num_states, num_actions = theta.shape\n    grad = np.zeros_like(theta)\n\n    # Function to compute softmax for a single state\n    def softmax(x):\n        # Subtract max(x) for numerical stability\n        shifted_x = x - np.max(x)\n        exp_x = np.exp(shifted_x)\n        return exp_x / np.sum(exp_x)\n    \n    # Accumulate gradients over all episodes\n    for episode in episodes:\n        T = len(episode)\n        # Compute returns from each time step to the end\n        returns = np.zeros(T)\n        G = 0.0\n        for t in reversed(range(T)):\n            G += episode[t][2]\n            returns[t] = G\n\n        # For each time step, compute log-prob gradient times return\n        for t in range(T):\n            s, a, _ = episode[t]\n            p = softmax(theta[s])\n            for a_prime in range(num_actions):\n                # Gradient of log \u03c0(a|s) = (1_{a_prime==a} - p[a_prime])\n                indicator = 1.0 if a_prime == a else 0.0\n                grad[s, a_prime] += returns[t] * (indicator - p[a_prime])\n\n    # Average the gradient by the number of episodes\n    grad /= len(episodes)\n\n    # Round and convert to list\n    grad = np.round(grad, 4)\n    return grad.tolist()"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Computes the FLOPs of a large dense layer vs. a Mixture-of-Experts layer\n    (as in \"Outrageously Large Neural Networks: The Sparsely-Gated\n    Mixture-of-Experts Layer\") and returns the percentage savings.\n\n    Assumes:\n      - The \"dense\" baseline has dimensions [d_in, n_experts * d_out]\n        to match the total capacity of n_experts experts.\n      - The MoE layer has n_experts experts, each [d_in, d_out] in size,\n        but only k_active experts are used per input.\n\n    Parameters:\n      n_experts (int): number of experts in the MoE layer\n      k_active  (int): number of active (selected) experts per input\n      d_in      (int): input dimension\n      d_out     (int): output dimension (per expert)\n\n    Returns:\n      dict: {\n         'dense_flops': float (one-decimal),\n         'moe_flops': float (one-decimal),\n         'savings_percent': float (one-decimal)\n      }\n    \"\"\"\n\n    # Cost of a \"dense\" layer matching total MoE capacity\n    dense_flops = d_in * (n_experts * d_out)\n\n    # Cost of an MoE layer (only k_active experts used instead of all n_experts)\n    moe_flops = d_in * (k_active * d_out)\n\n    # Percentage savings: how much we reduce compute compared to the full dense usage\n    savings_percent = 100.0 * (dense_flops - moe_flops) / dense_flops\n\n    return {\n        'dense_flops': round(dense_flops, 1),\n        'moe_flops': round(moe_flops, 1),\n        'savings_percent': round(savings_percent, 1)\n    }\n```\n\nExplanation of the calculations:\n\n\u2022 Dense FLOPs:  \n  We assume an equally large dense layer has dimensions [d_in, n_experts * d_out], giving d_in \u00d7 (n_experts \u00d7 d_out) FLOPs.  \n\n\u2022 MoE FLOPs:  \n  The MoE has n_experts experts, each sized [d_in, d_out]. However, due to sparsity, only k_active experts are used per input. Hence, the total compute is d_in \u00d7 (k_active \u00d7 d_out).  \n\n\u2022 Savings Percentage:  \n  The compute reduction is tracked by (1 \u2212 (k_active / n_experts)), expressed as:  \n  100% \u00d7 (Dense_FLOPs \u2212 MoE_FLOPs) / Dense_FLOPs.  \n\nAll outputs are rounded to one decimal place."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements Noisy Top-K gating used in Mixture-of-Experts.\n\n    Args:\n        X (np.ndarray): Input of shape (batch_size, input_dim).\n        W_g (np.ndarray): Gating weights of shape (input_dim, num_experts).\n        W_noise (np.ndarray): Noise weights of shape (input_dim, num_experts).\n        N (np.ndarray): Pre-sampled noise of shape (batch_size, num_experts).\n        k (int): Sparsity constraint - number of active experts per sample.\n\n    Returns:\n        list: A list (converted from np.ndarray) containing the gating probabilities\n              of shape (batch_size, num_experts). Each row sums to 1 among its\n              top-k positions; other positions are 0.\n    \"\"\"\n\n    # 1) Compute base gating logits: (batch_size, num_experts)\n    base_logits = X.dot(W_g)\n\n    # 2) Compute noise-based logits: (batch_size, num_experts)\n    noise_logits = X.dot(W_noise) * N\n\n    # 3) Compute combined gating logits\n    gating_logits = base_logits + noise_logits\n\n    batch_size, num_experts = gating_logits.shape\n\n    # 4) For each batch sample, pick top-k experts and compute normalized gating\n    gating_probs = np.zeros((batch_size, num_experts), dtype=np.float32)\n\n    for i in range(batch_size):\n        row = gating_logits[i]\n\n        # Get indices of the top k logits\n        topk_indices = np.argsort(row)[-k:]\n\n        # Extract top-k logits\n        topk_logits = row[topk_indices]\n\n        # Apply a stable softmax over only top-k entries\n        # Subtract max for numerical stability\n        shifted_logits = topk_logits - np.max(topk_logits)\n        exp_logits = np.exp(shifted_logits)\n        denom = np.sum(exp_logits)\n\n        # Compute partial softmax probabilities only in top-k positions\n        gating_probs[i, topk_indices] = exp_logits / denom\n\n    # 5) Round to 4 decimal places\n    gating_probs = np.round(gating_probs, 4)\n\n    # 6) Convert to Python list\n    return gating_probs.tolist()\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture of Experts (MoE) layer.\n    \n    Parameters:\n    -----------\n    x : np.ndarray\n        Input data of shape (batch_size, d_in).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, d_in, d_out).\n    Wg : np.ndarray\n        Gating weight matrix of shape (d_in, n_experts).\n    n_experts : int\n        Number of experts.\n    top_k : int\n        Number of top experts to select per token.\n    \n    Returns:\n    --------\n    List[List[float]]\n        The output of the MoE layer, rounded to 4 decimal places,\n        converted to a nested Python list.\n    \"\"\"\n\n    # 1. Compute gating logits: shape (batch_size, n_experts)\n    gating_logits = x @ Wg  # (B, n_experts)\n    \n    # 2. Stable softmax across experts\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    stable_logits = gating_logits - max_logits\n    exp_logits = np.exp(stable_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # (B, n_experts)\n    \n    # Prepare output\n    batch_size = x.shape[0]\n    d_out = We.shape[2]\n    output = np.zeros((batch_size, d_out))\n    \n    # 3. For each token in the batch, pick top-k experts\n    for i in range(batch_size):\n        # Indices of the top-k experts (no need to sort them fully, just partition)\n        top_indices = np.argpartition(gating_probs[i], -top_k)[-top_k:]\n        \n        # 4. Compute the weighted sum from the top-k experts\n        for e in top_indices:\n            expert_out = x[i] @ We[e]       # (d_out,)\n            output[i] += gating_probs[i, e] * expert_out\n    \n    # 5. Round to 4 decimals and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n```\n"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor X with shape (B, C, H, W).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input data of shape (B, C, H, W).\n    gamma : np.ndarray\n        Scale parameter of shape (C,).\n    beta : np.ndarray\n        Shift parameter of shape (C,).\n    num_groups : int\n        Number of channel groups to split into.\n    epsilon : float\n        A small constant for numerical stability.\n\n    Returns:\n    --------\n    List (Python list):\n        The normalized output, converted to a Python list\n        after rounding to 4 decimal places.\n    \"\"\"\n\n    # Get shape\n    B, C, H, W = X.shape\n    \n    # Compute the size of each group\n    group_size = C // num_groups\n    \n    # Reshape X to group the channels\n    # New shape = (B, num_groups, group_size, H, W)\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n    \n    # Compute mean and variance over the grouped axes (2, 3, 4)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply gamma and beta\n    # Broadcast gamma, beta to match (B, C, H, W)\n    X_scaled = X_normalized * gamma.reshape(1, C, 1, 1) + beta.reshape(1, C, 1, 1)\n    \n    # Round to 4 decimal places\n    X_rounded = np.round(X_scaled, 4)\n    \n    # Convert to list and return\n    return X_rounded.tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "import numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x that minimizes\n    f(x) = x^4 - 3x^3 + 2.\n    Returns the result rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Derivative of f(x): f'(x) = 4x^3 - 9x^2\n    def derivative(x):\n        return 4*x**3 - 9*x**2\n    \n    current_x = start_x\n    \n    for _ in range(max_iters):\n        grad = derivative(current_x)\n        next_x = current_x - learning_rate * grad\n        \n        # Stop if we've moved less than the tolerance\n        if abs(next_x - current_x) < tolerance:\n            break\n        \n        current_x = next_x\n    \n    # Round to 4 decimal places\n    return round(current_x, 4)"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, a normalization-free transformation\n    inspired by the Tanh function. This function replaces layer normalization in\n    Transformer architectures while preserving squashing behavior and enabling stable training.\n\n    Args:\n        x (np.ndarray): Input array.\n        alpha (float): Scaling parameter.\n        gamma (float): Gain parameter.\n        beta (float): Shift parameter.\n\n    Returns:\n        list[float]: Python list of the output array after applying the DyT function,\n                     rounded to 4 decimal places.\n    \"\"\"\n    # Compute the DyT activation\n    y = alpha * np.tanh(gamma * x + beta)\n    \n    # Round to 4 decimals\n    y_rounded = np.round(y, 4)\n\n    # Return as python list\n    return y_rounded.tolist()\n```"}
