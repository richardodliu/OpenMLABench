{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the product of a matrix 'a' and a vector 'b'. \n    Returns:\n        -1 if dimensions are incompatible,\n         a list of dot products (one for each row of 'a') otherwise.\n    \"\"\"\n    if not a:\n        return []\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Returns the transpose of a given matrix 'a'.\n    The matrix is represented as a list of lists.\n    \n    Example:\n        Input: [[1, 2, 3], \n                [4, 5, 6]]\n        Output: [[1, 4], \n                 [2, 5], \n                 [3, 6]]\n    \"\"\"\n    rows = len(a)\n    columns = len(a[0]) if rows > 0 else 0\n    transposed = [[a[row_index][col_index] for row_index in range(rows)] for col_index in range(columns)]\n    return transposed\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a 2D list 'a' into the specified new_shape (rows, cols) using numpy.\n    If the total number of elements doesn't match new_shape, return an empty list.\n\n    :param a: Original 2D list of integers/floats\n    :param new_shape: Tuple specifying the new shape (rows, cols)\n    :return: Reshaped 2D list (if possible) or an empty list otherwise\n    \"\"\"\n    total_elements = sum((len(row) for row in a))\n    required_elements = new_shape[0] * new_shape[1]\n    if total_elements != required_elements:\n        return []\n    arr = np.array(a)\n    reshaped_arr = arr.reshape(new_shape)\n    return reshaped_arr.tolist()\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    :param matrix: A list of lists representing the matrix.\n    :param mode: A string specifying the mode, either 'row' or 'column'.\n    :return: A list of mean values, one for each row (if mode='row')\n             or for each column (if mode='column').\n    \"\"\"\n    if not matrix or not isinstance(matrix[0], list):\n        return []\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    if mode == 'column':\n        n_rows = len(matrix)\n        n_cols = len(matrix[0])\n        return [sum((matrix[i][j] for i in range(n_rows))) / n_rows for j in range(n_cols)]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a 2D matrix by a scalar value.\n\n    :param matrix: A list of lists, where each inner list represents a row in the matrix.\n    :param scalar: The scalar (int or float) to multiply each element of the matrix by.\n    :return: A new matrix (list of lists) with elements multiplied by the given scalar.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them in descending order.\n\n    Args:\n        matrix (list[list[float|int]]): A 2x2 matrix represented as a nested list.\n\n    Returns:\n        list[float]: A list of eigenvalues in descending order.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = discriminant ** 0.5\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using T^{-1} * A * S, where T and S are invertible matrices.\n    - First validates if T and S are square and invertible.\n    - If not invertible or shapes are incompatible, returns -1.\n    - Otherwise, computes T^{-1} * A * S, rounds to 4th decimal, and returns as a python list.\n    \"\"\"\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        _ = np.linalg.inv(S_np)\n        result = T_inv @ A_np @ S_np\n        result_rounded = np.round(result, 4)\n        return result_rounded.tolist()\n    except (np.linalg.LinAlgError, ValueError):\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    The matrix is expected to be in the form:\n    [\n        [a, b],\n        [c, d]\n    ]\n    \n    Returns:\n        A 2x2 list of lists representing the inverse matrix,\n        or None if the matrix is not invertible (determinant = 0).\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    return [[d / det, -b / det], [-c / det, a / det]]\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices a and b (2D lists of numeric types), returning the product.\n    If the number of columns in a does not match the number of rows in b, returns -1.\n  \n    Parameters:\n    a (list[list[int|float]]): The first matrix.\n    b (list[list[int|float]]): The second matrix.\n\n    Returns:\n    list[list[int|float]]: The product of matrix a and matrix b, or -1 if not compatible.\n    \"\"\"\n    if not a or not b:\n        return -1\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            total = 0\n            for k in range(cols_a):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of feature vectors.\n    Each inner list in 'vectors' represents one feature (dimension), \n    and its elements are observations of that feature.\n\n    Args:\n        vectors (list[list[float]]): List of features, where each feature is a list\n                                     of observations (all features must have the same length).\n\n    Returns:\n        list[list[float]]: Covariance matrix of size (num_features x num_features).\n\n    Raises:\n        ValueError: If the features do not have the same number of observations,\n                    or if there are fewer than 2 observations per feature.\n    \"\"\"\n    num_features = len(vectors)\n    if num_features == 0:\n        return []\n    num_observations = len(vectors[0])\n    for feature in vectors:\n        if len(feature) != num_observations:\n            raise ValueError('All features must have the same number of observations.')\n    if num_observations < 2:\n        raise ValueError('At least two observations are required to compute covariance.')\n    means = [sum(feature) / num_observations for feature in vectors]\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(num_features):\n            cov_sum = 0.0\n            for k in range(num_observations):\n                cov_sum += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance_matrix[i][j] = cov_sum / (num_observations - 1)\n    return covariance_matrix\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    -----------\n    A : np.ndarray\n        Coefficient matrix (2D array).\n    b : np.ndarray\n        Right-hand side vector (1D array).\n    n : int\n        The number of iterations to perform.\n    \n    Returns:\n    --------\n    list\n        The approximate solution vector (1D list), with each entry rounded\n        to four decimal places.\n    \"\"\"\n    A = A.astype(float)\n    b = b.astype(float)\n    num_rows = A.shape[0]\n    x = np.zeros(num_rows, dtype=float)\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(num_rows):\n            sigma = 0.0\n            for j in range(num_rows):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            new_x[i] = (b[i] - sigma) / A[i, i]\n        x = np.round(new_x, decimals=4)\n    return x.reshape(-1).tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix A via the Jacobi method.\n    Returns a tuple of the two singular values, rounded to 4 decimals.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    M = A.T @ A\n    (m11, m12) = (M[0, 0], M[0, 1])\n    (m21, m22) = (M[1, 0], M[1, 1])\n    phi = 0.5 * np.arctan2(2.0 * m12, m11 - m22)\n    c = np.cos(phi)\n    s = np.sin(phi)\n    lam1 = c ** 2 * m11 - 2 * c * s * m12 + s ** 2 * m22\n    lam2 = s ** 2 * m11 + 2 * c * s * m12 + c ** 2 * m22\n    svals = np.array([np.sqrt(abs(lam1)), np.sqrt(abs(lam2))])\n    svals.sort()\n    svals = svals[::-1]\n    svals_rounded = tuple(np.round(svals, 4))\n    return svals_rounded\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Computes the determinant of a 4x4 matrix using Laplace's expansion (cofactor expansion).\n    This function expands along the first row and recursively computes the determinant \n    of 3x3 minors using Laplace's expansion as well.\n\n    :param matrix: A 4x4 matrix represented as a list of lists (elements can be int or float).\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n\n    def get_submatrix(mat: list[list[int | float]], row_to_remove: int, col_to_remove: int) -> list[list[int | float]]:\n        \"\"\"\n        Returns the submatrix obtained by removing the specified row and column.\n        \"\"\"\n        return [[mat[r][c] for c in range(len(mat)) if c != col_to_remove] for r in range(len(mat)) if r != row_to_remove]\n\n    def determinant_2x2(m2: list[list[int | float]]) -> float:\n        \"\"\"Determinant of a 2x2 matrix.\"\"\"\n        return m2[0][0] * m2[1][1] - m2[0][1] * m2[1][0]\n\n    def determinant_3x3(m3: list[list[int | float]]) -> float:\n        \"\"\"\n        Recursively computes the determinant of a 3x3 matrix using Laplace's expansion\n        along the first row.\n        \"\"\"\n        if len(m3) != 3 or any((len(row) != 3 for row in m3)):\n            raise ValueError('Minor matrix must be 3x3 for determinant_3x3.')\n        det_val = 0.0\n        for col in range(3):\n            minor_2x2 = get_submatrix(m3, 0, col)\n            cofactor = (-1) ** (0 + col) * determinant_2x2(minor_2x2)\n            det_val += m3[0][col] * cofactor\n        return det_val\n    det_4x4 = 0.0\n    for col in range(4):\n        minor_3x3 = get_submatrix(matrix, 0, col)\n        cofactor = (-1) ** (0 + col) * determinant_3x3(minor_3x3)\n        det_4x4 += matrix[0][col] * cofactor\n    return det_4x4\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    theta = np.linalg.pinv(X_arr.T @ X_arr) @ (X_arr.T @ y_arr)\n    theta_rounded = [round(coef, 4) for coef in theta]\n    return theta_rounded\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept (shape: m x n)\n    y (np.ndarray): Target vector (shape: m,) or (m x 1)\n    alpha (float): Learning rate\n    iterations (int): Number of gradient descent iterations\n    \n    Returns:\n    np.ndarray: Model coefficients (n,) as a Python list, rounded to 4 decimal places.\n                The return value is obtained using numpy.ndarray.tolist().\n    \"\"\"\n    (m, n) = X.shape\n    y = y.reshape(-1, 1)\n    theta = np.zeros((n, 1))\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = 1 / m * (X.T @ errors)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.flatten().tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data (rows = samples, columns = features) using:\n    1. Standardization: (x - mean) / std\n    2. Min-Max Normalization: (x - min) / (max - min)\n    \n    Handles zero std or zero range by substituting with 1 to avoid division by zero.\n    \n    Returns:\n        standardized_list, minmax_list\n        Both are 2D Python lists (rounded to 4 decimal places).\n    \"\"\"\n    means = data.mean(axis=0)\n    stds = data.std(axis=0)\n    stds[stds == 0] = 1\n    standardized_data = (data - means) / stds\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    range_vals = max_vals - min_vals\n    range_vals[range_vals == 0] = 1\n    minmax_data = (data - min_vals) / range_vals\n    standardized_data = np.round(standardized_data, 4)\n    minmax_data = np.round(minmax_data, 4)\n    return (standardized_data.tolist(), minmax_data.tolist())\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-Means clustering algorithm.\n\n    :param points: A list of points, where each point is a tuple of coordinates (e.g., (x, y))\n    :param k: The number of clusters to form\n    :param initial_centroids: A list of initial centroid points, each a tuple of coordinates\n    :param max_iterations: The maximum number of iterations to perform\n    :return: A list of the final centroids of the clusters, where each centroid is rounded to 4 decimals\n    \"\"\"\n    data = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.copy(centroids)\n        for cluster_index in range(k):\n            cluster_points = data[cluster_assignments == cluster_index]\n            if len(cluster_points) > 0:\n                new_centroids[cluster_index] = cluster_points.mean(axis=0)\n        if np.allclose(centroids, new_centroids, atol=1e-07):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    final_centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    return final_centroids\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature dataset.\n    y : np.ndarray\n        Labels or target values.\n    k : int, default=5\n        Number of folds to split the data into.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int or None, default=None\n        Random seed for reproducible shuffling.\n\n    Returns:\n    --------\n    list of tuples (train_index, test_index)\n        Each element is a tuple containing:\n            - train_index: 1D array of indices for training.\n            - test_index: 1D array of indices for testing.\n    \"\"\"\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.default_rng(seed=random_seed)\n        rng.shuffle(indices)\n    folds = np.array_split(indices, k)\n    splits = []\n    for i in range(k):\n        test_index = folds[i]\n        train_index = np.concatenate([fold for (j, fold) in enumerate(folds) if j != i])\n        splits.append((train_index, test_index))\n    return splits\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the input data matrix.\n\n    Parameters:\n    -----------\n    data : np.ndarray\n        A 2D array where each row represents a data sample and each column represents a feature.\n    k : int\n        The number of principal components to return.\n\n    Returns:\n    --------\n    list[list[float]]\n        A 2D list of shape (k, n_features), where each row contains one principal component\n        (an eigenvector of the covariance matrix). Values are rounded to 4 decimal places.\n    \"\"\"\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=1)\n    data_std = (data - means) / stds\n    n_samples = data_std.shape[0]\n    covariance_matrix = data_std.T @ data_std / (n_samples - 1)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    top_k_indices = sorted_indices[:k]\n    principal_components = eigenvectors[:, top_k_indices]\n    principal_components = principal_components.T\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef _entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Compute the entropy of the target attribute in the given examples.\n    \"\"\"\n    value_counts = Counter((example[target_attr] for example in examples))\n    if not examples:\n        return 0.0\n    total = len(examples)\n    entropy_value = 0.0\n    for count in value_counts.values():\n        p = count / total\n        entropy_value -= p * math.log2(p)\n    return entropy_value\ndef _information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Compute the information gain by splitting on `attribute`.\n    IG = entropy(parent) - sum( (|Dj| / |D|) * entropy(Dj) ) over each subset Dj.\n    \"\"\"\n    parent_entropy = _entropy(examples, target_attr)\n    subsets = {}\n    for example in examples:\n        value = example[attribute]\n        subsets.setdefault(value, []).append(example)\n    total = len(examples)\n    weighted_entropy = 0.0\n    for subset_examples in subsets.values():\n        weighted_entropy += len(subset_examples) / total * _entropy(subset_examples, target_attr)\n    return parent_entropy - weighted_entropy\ndef _majority_value(examples: list[dict], target_attr: str):\n    \"\"\"\n    Return the most common value for the target attribute among the provided examples.\n    \"\"\"\n    value_counts = Counter((example[target_attr] for example in examples))\n    return value_counts.most_common(1)[0][0]\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree using recursive splitting based on entropy and information gain.\n\n    :param examples: A list of examples, each a dict of {attribute: value, ...}.\n    :param attributes: List of attribute names (excluding the target).\n    :param target_attr: Name of the target attribute to classify.\n    :return: A nested dictionary representing the learned decision tree.\n    \"\"\"\n    if not examples:\n        return {}\n    first_label = examples[0][target_attr]\n    if all((example[target_attr] == first_label for example in examples)):\n        return first_label\n    if not attributes:\n        return _majority_value(examples, target_attr)\n    gains = [(attribute, _information_gain(examples, attribute, target_attr)) for attribute in attributes]\n    (best_attr, _) = max(gains, key=lambda x: x[1])\n    tree = {best_attr: {}}\n    values = set((example[best_attr] for example in examples))\n    for val in values:\n        subset = [ex for ex in examples if ex[best_attr] == val]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        if not subset:\n            tree[best_attr][val] = _majority_value(examples, target_attr)\n        else:\n            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n            tree[best_attr][val] = subtree\n    return tree\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Trains a binary SVM classifier using a deterministic (batch) Pegasos algorithm with a specified kernel.\n    \n    Parameters:\n    -----------\n    data        : np.ndarray, shape (n_samples, n_features)\n                  Training data.\n    labels      : np.ndarray, shape (n_samples,)\n                  Labels corresponding to the training data. Must be in {+1, -1}.\n    kernel      : str, optional, default='linear'\n                  Specifies the kernel type. Choices: 'linear', 'rbf'.\n    lambda_val  : float, optional, default=0.01\n                  Regularization parameter.\n    iterations  : int, optional, default=100\n                  Number of batch Pegasos iterations.\n    sigma       : float, optional, default=1.0\n                  Parameter for RBF kernel (\u03c3).\n    \n    Returns:\n    --------\n    alpha_list  : list of lists\n                  Learned dual coefficients, rounded to 4 decimals, reshaped to (n_samples, 1) then converted to list.\n    bias_list   : list of one element\n                  Learned bias term, rounded to 4 decimals and returned as a single-element list.\n    \"\"\"\n    n_samples = data.shape[0]\n    if kernel == 'linear':\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2.0 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        alpha *= 1.0 - 1.0 / t\n        f = (alpha * labels).dot(K) + b\n        mis_idx = np.where(labels * f < 1)[0]\n        if len(mis_idx) > 0:\n            alpha[mis_idx] += 1.0 / (lambda_val * t * n_samples)\n        if len(mis_idx) > 0:\n            b += 1.0 / (lambda_val * t * n_samples) * np.sum(labels[mis_idx])\n    alpha_rounded = np.round(alpha, 4).reshape(-1, 1)\n    b_rounded = float(np.round(b, 4))\n    return (alpha_rounded.tolist(), [b_rounded])\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for the input z.\n    Returns the result rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation values for a given list of scores.\n    Each returned value is rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_values = [math.exp(s - max_score) for s in scores]\n    total = sum(exp_values)\n    softmax_values = [round(ev / total, 4) for ev in exp_values]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Parameters:\n    -----------\n    features : list of list of floats\n        Each inner list represents the feature vector for one example.\n    labels : list of ints\n        True binary labels (0 or 1) corresponding to each feature vector.\n    weights : list of floats\n        Weights for each feature.\n    bias : float\n        Bias term for the neuron.\n\n    Returns:\n    --------\n    (predicted_probabilities, mse)\n        predicted_probabilities : python list (shape: n x 1) after reshaping, each element rounded to 4 decimal places\n        mse : float\n            Mean squared error between predicted probabilities and true labels, rounded to 4 decimal places.\n    \"\"\"\n    X = np.array(features, dtype=float)\n    y = np.array(labels, dtype=float)\n    w = np.array(weights, dtype=float)\n    z = np.dot(X, w) + bias\n    predictions = 1.0 / (1.0 + np.exp(-z))\n    mse = np.mean((predictions - y) ** 2)\n    predictions = np.round(predictions, 4)\n    mse = round(mse, 4)\n    predictions = predictions.reshape(-1, 1).tolist()\n    return (predictions, mse)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.astype(float).copy()\n    bias = float(initial_bias)\n    n_samples = features.shape[0]\n    mse_history = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        output = 1.0 / (1.0 + np.exp(-z))\n        errors = output - labels\n        mse = np.mean(errors ** 2)\n        mse_history.append(round(mse, 4))\n        d_sigmoid = output * (1.0 - output)\n        w_grad = np.dot(features.T, errors * d_sigmoid) / n_samples\n        b_grad = np.sum(errors * d_sigmoid) / n_samples\n        weights -= learning_rate * w_grad\n        bias -= learning_rate * b_grad\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = round(bias, 4)\n    weights_list = weights_rounded.tolist()\n    return (weights_list, bias_rounded, mse_history)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_topo(child)\n                topo.append(node)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P that takes coordinates from basis B to basis C in R^3.\n\n    If B and C are each lists of three vectors in R^3, interpreted as rows:\n        B = [\n          [b1x, b1y, b1z],\n          [b2x, b2y, b2z],\n          [b3x, b3y, b3z]\n        ]\n        C = [\n          [c1x, c1y, c1z],\n          [c2x, c2y, c2z],\n          [c3x, c3y, c3z]\n        ]\n\n    Then the columns of each matrix represent the basis vectors. The transformation\n    from B-coordinates to C-coordinates is given by P = C_inv * B, where B and C \n    are the matrices formed by using these vectors as columns.\n\n    :param B: list of list of ints, basis B\n    :param C: list of list of ints, basis C\n    :return: list of list of floats (transformation matrix, rounded to 4 decimals)\n    \"\"\"\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n    C_inv = np.linalg.inv(C_mat)\n    P = C_inv.dot(B_mat)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using the method described:\n      1) Compute A^T A and find its eigen-decomposition A^T A = V \u039b V^T\n      2) Singular values are sqrt of the eigenvalues (\u039b)\n      3) U = A V \u03a3^(-1) (handling zero singular values carefully)\n    Finally, enforce det(U) >= 0 and det(V) >= 0 to keep a standard orientation.\n\n    Returns (U, S, V) as lists of lists (each 2x2) rounded to 4 decimal places,\n    satisfying A \u2248 U S V^T in the usual SVD sense.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    ATA = A.T @ A\n    (eigvals, eigvecs) = np.linalg.eigh(ATA)\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n    s_vals = np.sqrt(np.clip(eigvals, 0, None))\n    V = eigvecs\n    U = np.zeros((2, 2), dtype=np.float64)\n    for i in range(2):\n        if abs(s_vals[i]) > 1e-14:\n            U[:, i] = A @ V[:, i] / s_vals[i]\n        elif i == 0:\n            U[:, i] = [1.0, 0.0]\n        else:\n            col0 = U[:, 0]\n            if np.linalg.norm(col0) > 1e-14:\n                perp = np.array([col0[1], -col0[0]])\n                perp /= np.linalg.norm(perp)\n                U[:, i] = perp\n            else:\n                U[:, i] = [0.0, 1.0]\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n    if np.linalg.det(V) < 0:\n        V[:, 1] = -V[:, 1]\n    S = np.diag(s_vals)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays X and y in unison, maintaining correspondence\n    between their rows. Returns the shuffled data as Python lists.\n\n    Parameters:\n    X (numpy.ndarray): Feature array\n    y (numpy.ndarray): Labels array\n    seed (int, optional): Random seed for reproducibility\n\n    Returns:\n    (list, list): Shuffled X and y converted to Python lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from X and optionally y.\n    Each batch is returned as a Python list after reshaping by using numpy's tolist() method.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Numpy array containing input data.\n    y : np.ndarray, optional\n        Numpy array containing labels (default is None).\n    batch_size : int, optional\n        Number of samples per batch (default is 64).\n    \n    Yields:\n    -------\n    If y is provided, yields (X_batch, y_batch) where both are Python lists.\n    Otherwise, yields X_batch as a Python list.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        x_batch = X[start:end]\n        x_batch_list = x_batch.tolist()\n        if y is not None:\n            y_batch = y[start:end]\n            y_batch_list = y_batch.tolist()\n            yield (x_batch_list, y_batch_list)\n        else:\n            yield x_batch_list\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature\n    is greater than or equal to the given threshold.\n    \n    Parameters:\n    X (numpy.ndarray): The input dataset (2D numpy array).\n    feature_i (int): The index of the feature to split on.\n    threshold (float): The threshold value for the feature.\n    \n    Returns:\n    (list, list): Two lists of samples: the first for samples where the \n    feature_i >= threshold, and the second for samples where the \n    feature_i < threshold.\n    \"\"\"\n    mask = X[:, feature_i] >= threshold\n    X_meet_condition = X[mask]\n    X_not_meet_condition = X[~mask]\n    return (X_meet_condition.tolist(), X_not_meet_condition.tolist())\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree for the input 2D array X.\n\n    Parameters:\n    ------------\n    X : numpy.ndarray\n        2D array of shape (n_samples, n_features).\n    degree : int\n        The maximum polynomial degree.\n\n    Returns:\n    ------------\n    list\n        Python list of shape (n_samples, number_of_polynomial_features),\n        containing all polynomial features up to the specified degree.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    feature_list = []\n    for d in range(degree + 1):\n        for combo in combinations_with_replacement(range(n_features), d):\n            if len(combo) == 0:\n                new_feature = np.ones(n_samples)\n            else:\n                new_feature = np.prod(X[:, combo], axis=1)\n            feature_list.append(new_feature)\n    return np.array(feature_list).T.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of the dataset (X, y).\n    If replacements is True, sample with replacement; otherwise, sample without replacement.\n\n    :param X: 2D numpy array, shape (n_samples, n_features)\n    :param y: 1D numpy array, shape (n_samples,)\n    :param n_subsets: int, number of random subsets to generate\n    :param replacements: bool, if True use sampling with replacement\n    :param seed: int, random seed for reproducibility\n    \n    :return: list of tuples (X_subset, y_subset), each as Python lists\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.randint(0, n_samples, size=n_samples)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices, :]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding of a 1D numpy array of integer values.\n\n    Parameters:\n    x (numpy.ndarray): 1D array of integer values representing categories.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If None, it is automatically determined from x.\n\n    Returns:\n    list: One-hot encoded representation of x as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n    for (i, val) in enumerate(x):\n        one_hot[i, val] = 1\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array x into a diagonal matrix.\n    \n    Parameters:\n    x (numpy.ndarray): 1D input array.\n    \n    Returns:\n    list: 2D python list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates and returns the accuracy score of model predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels\n    y_pred (numpy.ndarray): 1D array of predicted labels\n\n    Returns:\n    float: Accuracy score = (number of correct predictions) / (total number of predictions)\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return float(accuracy)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset X and, optionally,\n    another dataset Y. If Y is not provided, the correlation matrix is computed\n    for X with itself.\n    \n    Args:\n        X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features_X).\n        Y (numpy.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_Y).\n            If None, Y is set to be X.\n    \n    Returns:\n        list: A 2D python list representing the correlation matrix, rounded to 4 decimals.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    Y_centered = Y - Y.mean(axis=0, keepdims=True)\n    cov = X_centered.T @ Y_centered / (X.shape[0] - 1)\n    std_x = X_centered.std(axis=0, ddof=1)\n    std_y = Y_centered.std(axis=0, ddof=1)\n    corr_matrix = cov / np.outer(std_x, std_y)\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fits an AdaBoost classifier using decision stumps for a given dataset X and labels y.\n    \n    Args:\n        X (numpy.ndarray): 2D array of shape (n_samples, n_features)\n        y (numpy.ndarray): 1D array of shape (n_samples,) with labels in {+1, -1}\n        n_clf (int): Number of weak classifiers (decision stumps) to train\n\n    Returns:\n        list: A list of dictionaries, each containing parameters of a learned weak classifier:\n              [\n                {\n                  \"feature\": <feature index>,\n                  \"threshold\": <float, rounded to 4 decimals>,\n                  \"alpha\": <float, rounded to 4 decimals>,\n                  \"direction\": <'lt' or 'gt'>\n                },\n                ...\n              ]\n    \"\"\"\n    if set(np.unique(y)) == {0, 1}:\n        y = np.where(y == 0, -1, 1)\n    (n_samples, n_features) = X.shape\n    w = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    def stump_predict(feature, threshold, direction, X_data):\n        \"\"\"\n        Predict labels (+1 or -1) for a given feature, threshold, and direction.\n        direction == 'lt': predict +1 if x < threshold else -1\n        direction == 'gt': predict +1 if x > threshold else -1\n        \"\"\"\n        if direction == 'lt':\n            return np.where(X_data[:, feature] < threshold, 1, -1)\n        else:\n            return np.where(X_data[:, feature] > threshold, 1, -1)\n    for _ in range(n_clf):\n        (best_feature, best_threshold) = (None, None)\n        best_direction = None\n        best_error = float('inf')\n        best_pred = None\n        for feature in range(n_features):\n            x_vals = np.unique(X[:, feature])\n            if len(x_vals) == 1:\n                thresholds = x_vals\n            else:\n                thresholds = (x_vals[:-1] + x_vals[1:]) / 2.0\n            for threshold in thresholds:\n                for direction in ['lt', 'gt']:\n                    pred = stump_predict(feature, threshold, direction, X)\n                    misclassified = pred != y\n                    error = np.sum(w[misclassified])\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_direction = direction\n                        best_pred = pred\n        eps = 1e-10\n        best_error = max(best_error, eps)\n        best_error = min(best_error, 1 - eps)\n        alpha = 0.5 * np.log((1.0 - best_error) / best_error)\n        w *= np.exp(-alpha * y * best_pred)\n        w /= np.sum(w)\n        classifiers.append({'feature': best_feature, 'threshold': float(np.round(best_threshold, 4)), 'alpha': float(np.round(alpha, 4)), 'direction': best_direction})\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    x = np.array(scores, dtype=np.float64)\n    x_shifted = x - np.max(x)\n    log_sum_exp = np.log(np.sum(np.exp(x_shifted)))\n    log_softmax_vals = x_shifted - log_sum_exp\n    log_softmax_vals = np.round(log_softmax_vals, 4)\n    return log_softmax_vals.reshape(-1, 1).tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        \"\"\"\n        1. Initialization:\n           - Define the layer with a specified number of neurons (n_units) and an optional input shape.\n           - Create placeholders for the layer's weights (W), biases (w0), \n             as well as placeholders for possible optimizers (W_opt, w0_opt).\n        \"\"\"\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.layer_input = None\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer=None):\n        \"\"\"\n        2. Weight Initialization:\n           - Initialize W using a uniform distribution with limit = 1 / sqrt(input_shape[0]).\n           - Initialize biases (w0) to zero.\n           - Optionally copy the passed optimizer to W_opt and w0_opt.\n        \"\"\"\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        if optimizer is not None:\n            self.W_opt = copy.deepcopy(optimizer)\n            self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        \"\"\"\n        3. Parameter Count:\n           - Number of trainable parameters = size of W + size of w0.\n        \"\"\"\n        return self.input_shape[0] * self.n_units + self.n_units\n\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        4. Forward Pass:\n           - Compute the layer output as a matrix multiplication between X and W plus the bias w0.\n           - Round to 4 decimal places and return as a Python list.\n        \"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        output = np.round(output, 4)\n        return output.tolist()\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        5. Backward Pass:\n           - Compute gradients with respect to W and w0.\n           - If trainable, update W and w0 using the optimizer's update rule.\n           - Compute and return the gradient wrt. the input (dX).\n           - All returned/updated values should be rounded to 4 decimals, \n             and the gradient returned as a Python list.\n        \"\"\"\n        accum_grad = np.array(accum_grad)\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            if self.W_opt is not None:\n                self.W = self.W_opt.update(self.W, dW)\n            else:\n                self.W -= 0.01 * dW\n            if self.w0_opt is not None:\n                self.w0 = self.w0_opt.update(self.w0, dw0)\n            else:\n                self.w0 -= 0.01 * dw0\n            self.W = np.round(self.W, 4)\n            self.w0 = np.round(self.w0, 4)\n        grad_input = np.dot(accum_grad, self.W.T)\n        grad_input = np.round(grad_input, 4)\n        return grad_input.tolist()\n\n    def output_shape(self):\n        \"\"\"\n        6. Output Shape:\n           - Return the shape of the output = (n_units,).\n        \"\"\"\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Computes a 2D convolution operation on the input matrix using the provided kernel,\n    with specified padding and stride values. The results are rounded to 4 decimal places.\n\n    Parameters:\n        input_matrix (np.ndarray): 2D array representing the input.\n        kernel (np.ndarray): 2D array representing the convolution kernel.\n        padding (int): Number of zero-padding layers added symmetrically.\n        stride (int): Stride step size for the sliding convolution.\n\n    Returns:\n        list: Python list representation of the 2D convolution result,\n              rounded to 4 decimal places.\n    \"\"\"\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n    out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((out_height, out_width), dtype=float)\n    for i in range(out_height):\n        for j in range(out_width):\n            row_start = i * stride\n            col_start = j * stride\n            region = padded_input[row_start:row_start + kernel_height, col_start:col_start + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): 2D feature matrix of shape (n_samples, n_features)\n    w (np.ndarray): 1D array of coefficients of shape (n_features,)\n    y_true (np.ndarray): 1D array of true labels of shape (n_samples,)\n    alpha (float): Regularization parameter\n\n    Returns:\n    float: Ridge loss (MSE + alpha * sum(w^2)), rounded to 4 decimal places\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg = alpha * np.sum(w ** 2)\n    loss = mse + reg\n    return round(loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n    float|int: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    Precision = TP / (TP + FP)\n    \n    Parameters:\n    y_true (np.array): Ground truth binary labels\n    y_pred (np.array): Predicted binary labels\n\n    Returns:\n    float: The precision value\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    return tp / denominator\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: method must be one of: 'batch', 'sgd', or 'mini-batch'", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent using MSE loss in one of three variants:\n      1) Batch Gradient Descent\n      2) Stochastic Gradient Descent (SGD)\n      3) Mini-Batch Gradient Descent\n\n    Parameters:\n    X             : np.ndarray, shape (n_samples, n_features)\n                   The input data.\n    y             : np.ndarray, shape (n_samples,)\n                   The target values.\n    weights       : np.ndarray, shape (n_features,)\n                   Initial weights for the model.\n    learning_rate : float\n                   Learning rate for the gradient descent updates.\n    n_iterations  : int\n                   Number of iterations (updates) to perform.\n    batch_size    : int, optional (default=1)\n                   Size of each mini-batch (only used if method='mini-batch').\n    method        : str, optional (default='batch')\n                   Specifies which gradient descent variant to use.\n                     'batch'       -> Batch Gradient Descent\n                     'sgd'         -> Stochastic Gradient Descent\n                     'mini-batch'  -> Mini-Batch Gradient Descent\n\n    Returns:\n    A python list of the final weight values, each rounded to the 4th decimal place.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            predictions = X.dot(weights)\n            errors = predictions - y\n            grad = 2 / n_samples * X.T.dot(errors)\n            weights -= learning_rate * grad\n        elif method == 'sgd':\n            idx = np.random.randint(n_samples)\n            (X_i, y_i) = (X[idx], y[idx])\n            prediction = X_i.dot(weights)\n            error = prediction - y_i\n            grad = 2 * X_i * error\n            weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                X_batch = X_shuffled[start:end]\n                y_batch = y_shuffled[start:end]\n                predictions = X_batch.dot(weights)\n                errors = predictions - y_batch\n                grad = 2 / X_batch.shape[0] * X_batch.T.dot(errors)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be one of: 'batch', 'sgd', or 'mini-batch'\")\n    return np.round(weights, 4).tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF) and return the result as a Python list.\n    \n    Parameters:\n    matrix (list of lists or numpy array): The input 2D matrix to convert into RREF.\n    \n    Returns:\n    list: The RREF of the input matrix as a nested Python list.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    pivot_row = 0\n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n        pivot = pivot_row\n        while pivot < rows and abs(A[pivot, col]) < 1e-12:\n            pivot += 1\n        if pivot == rows:\n            continue\n        if pivot != pivot_row:\n            A[[pivot_row, pivot], :] = A[[pivot, pivot_row], :]\n        pivot_val = A[pivot_row, col]\n        if abs(pivot_val) > 1e-12:\n            A[pivot_row, :] = A[pivot_row, :] / pivot_val\n        for r in range(rows):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > 1e-12:\n                    A[r, :] = A[r, :] - factor * A[pivot_row, :]\n        pivot_row += 1\n    return A.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam (Adaptive Moment Estimation) optimization algorithm.\n    \n    Parameters:\n    -----------\n    f : callable\n        The objective function to be optimized.\n    grad : callable\n        A function that computes the gradient of f.\n    x0 : array_like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size (default: 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default: 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default: 10).\n        \n    Returns:\n    --------\n    list\n        The optimized parameters, rounded to the nearest 4 decimals, returned as a Python list.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x, dtype=np.float64)\n    v = np.zeros_like(x, dtype=np.float64)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression (L1) using Gradient Descent and return (weights, bias).\n    \n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target array of shape (n_samples, ).\n        alpha (float): Regularization parameter for L1 penalty.\n        learning_rate (float): The learning rate for gradient descent.\n        max_iter (int): Maximum number of iterations for gradient descent.\n        tol (float): Tolerance for stopping criterion.\n    \n    Returns:\n        (list, float): \n            - list of final weights rounded to 4 decimal places.\n            - bias (float) rounded to 4 decimal places.\n    \"\"\"\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        residual = y_pred - y\n        grad_w_mse = 1.0 / n * X.T.dot(residual)\n        grad_w_l1 = alpha * np.sign(w)\n        grad_w_l1[w == 0] = 0.0\n        grad_w = grad_w_mse + grad_w_l1\n        w_new = w - learning_rate * grad_w\n        grad_b = 1.0 / n * np.sum(residual)\n        b_new = b - learning_rate * grad_b\n        if np.linalg.norm(w_new - w) < tol:\n            w = w_new\n            b = b_new\n            break\n        w = w_new\n        b = b_new\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment distance between two strings,\n    allowing insertions, deletions, substitutions, and transposition of\n    adjacent characters (each at cost 1).\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    return round(tp / denominator, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: self_attention() takes 3 positional arguments but 4 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Implements the self-attention mechanism:\n      1) Compute the dot product of Q and K^T.\n      2) Scale by sqrt(d_k) where d_k is the dimension of the key.\n      3) Apply the softmax function to get the attention weights.\n      4) Multiply the attention weights by V to get the output.\n\n    Parameters:\n        Q, K, V (numpy.ndarray): \n            - Shape: (sequence_length, feature_dimension).\n            - sequence_length must be the same for K and V,\n              and feature_dimension must be the same for Q, K, and V.\n\n    Returns:\n        list: The self-attention output after rounding to 4 decimal places\n              and converted to a Python list.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    exps = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exps / np.sum(exps, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    x_seq = np.array(input_sequence, dtype=float)\n    h_prev = np.array(initial_hidden_state, dtype=float)\n    Wx_arr = np.array(Wx, dtype=float)\n    Wh_arr = np.array(Wh, dtype=float)\n    b_arr = np.array(b, dtype=float)\n    for x_t in x_seq:\n        h_prev = np.tanh(np.dot(x_t, Wx_arr) + np.dot(h_prev, Wh_arr) + b_arr)\n    return np.round(h_prev, 4).tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=float)\n    ones = np.ones((points_array.shape[0], 1), dtype=float)\n    homogeneous_points = np.hstack((points_array, ones))\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=float)\n    translated_points_homogeneous = homogeneous_points @ translation_matrix.T\n    translated_points = translated_points_homogeneous[:, :2]\n    return translated_points.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the KL divergence between two univariate normal distributions:\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2).\n\n    Parameters\n    ----------\n    mu_p : float\n        Mean of the first normal distribution (P).\n    sigma_p : float\n        Standard deviation of the first normal distribution (P).\n    mu_q : float\n        Mean of the second normal distribution (Q).\n    sigma_q : float\n        Standard deviation of the second normal distribution (Q).\n\n    Returns\n    -------\n    float\n        The KL divergence D_KL(P || Q).\n    \"\"\"\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    return term1 + term2 + term3\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations A x = b using the Gauss-Seidel method.\n\n    Parameters\n    ----------\n    A : (N, N) array_like\n        Coefficient matrix.\n    b : (N,) array_like\n        Right-hand side vector.\n    n : int\n        Number of iterations.\n    x_ini : (N,) array_like, optional\n        Initial guess. If None, a zero vector will be used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(len(b)):\n            sum_ = b[i]\n            for j in range(len(b)):\n                if j != i:\n                    sum_ -= A[i, j] * x[j]\n            x[i] = sum_ / A[i, i]\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Performs Gaussian elimination with partial pivoting to solve the system A x = b.\n\n    Parameters:\n    -----------\n    A : np.ndarray\n        A 2D square numpy array (coefficient matrix).\n    b : np.ndarray\n        A 1D numpy array (right-hand side vector).\n\n    Returns:\n    --------\n    list\n        Solution vector x as a Python list, values rounded to the 4th decimal.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    for i in range(n):\n        pivot_row = i + np.argmax(np.abs(A[i:, i]))\n        if pivot_row != i:\n            A[[i, pivot_row]] = A[[pivot_row, i]]\n            (b[i], b[pivot_row]) = (b[pivot_row], b[i])\n        if abs(A[i, i]) < 1e-14:\n            raise ValueError('Matrix is singular or nearly singular.')\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        sum_ax = np.dot(A[i, i + 1:], x[i + 1:])\n        x[i] = (b[i] - sum_ax) / A[i, i]\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        All returned values are rounded to the 4th decimal place and converted to Python lists.\n        \n        Args:\n            x (np.ndarray): Input sequence of shape (T, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n\n        Returns:\n            tuple: (hidden_states_list, final_hidden_state_list, final_cell_state_list)\n                   Each element is a Python list version of the numpy array rounded to 4 decimals.\n        \"\"\"\n\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n        T = x.shape[0]\n        ht = initial_hidden_state\n        ct = initial_cell_state\n        hidden_states = []\n        for t in range(T):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((ht, xt))\n            ft = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ot = sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ct = ft * ct + it * c_tilde\n            ht = ot * np.tanh(ct)\n            hidden_states.append(ht.copy())\n        hidden_states_np = np.concatenate(hidden_states, axis=1).T\n        final_hidden_state = ht\n        final_cell_state = ct\n        hidden_states_np = np.round(hidden_states_np, 4)\n        final_hidden_state = np.round(final_hidden_state, 4)\n        final_cell_state = np.round(final_cell_state, 4)\n        hidden_states_list = hidden_states_np.tolist()\n        final_hidden_state_list = final_hidden_state.reshape(-1).tolist()\n        final_cell_state_list = final_cell_state.reshape(-1).tolist()\n        return (hidden_states_list, final_hidden_state_list, final_cell_state_list)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores of the given `query` terms against a `corpus` of documents.\n    \n    :param corpus: List of documents, where each document is a list of words (tokens).\n    :param query: List of words for which TF-IDF scores are to be computed.\n    :return: A list of lists of TF-IDF scores for each document (rows) and each query term (columns).\n    \"\"\"\n    if not corpus:\n        return []\n    if not query:\n        return []\n    N = len(corpus)\n    doc_freq = {term: 0 for term in query}\n    for doc in corpus:\n        unique_words = set(doc)\n        for term in query:\n            if term in unique_words:\n                doc_freq[term] += 1\n    idf = {}\n    for term in query:\n        idf[term] = np.log10((N + 1) / (doc_freq[term] + 1))\n    tf_idf_matrix = []\n    for doc in corpus:\n        doc_len = len(doc)\n        if doc_len == 0:\n            tf_idf_matrix.append([0.0] * len(query))\n            continue\n        word_count = {}\n        for w in doc:\n            word_count[w] = word_count.get(w, 0) + 1\n        row_scores = []\n        for term in query:\n            tf = word_count.get(term, 0) / doc_len\n            row_scores.append(tf * idf[term])\n        tf_idf_matrix.append(row_scores)\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n    if precision == 0 and recall == 0:\n        return 0.0\n    f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: name 'SimpleRNN' is not defined", "solution": "import numpy as np\nimport numpy as np\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x as a list (rounded to 8 decimal places)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x0.astype(float)\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        if np.linalg.norm(r) < tol:\n            break\n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        current_count = len(values)\n        for (col_index, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of floats/ints)\n    :param L: The line vector defining the direction of projection (list of floats/ints)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    dot_v_L = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_L_L = sum((Li * Li for Li in L))\n    if abs(dot_L_L) < 1e-15:\n        return [0.0] * len(L)\n    scalar = dot_v_L / dot_L_L\n    projection = [round(scalar * Li, 3) for Li in L]\n    return projection\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n    \n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row_indices, col_ptr)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        col_ptr[col] = len(values)\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n    col_ptr[num_cols] = len(values)\n    return (values, row_indices, col_ptr)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space (image) of matrix A using row echelon form.\n    The columns corresponding to pivot positions from the original matrix\n    form a basis for the column space.\n\n    Parameters:\n        A (numpy.ndarray): A 2D array (matrix).\n\n    Returns:\n        list: A 2D list whose columns are the basis vectors for the column\n              space of A, rounded to 8 decimal places.\n    \"\"\"\n    A = A.astype(float)\n    (n_rows, n_cols) = A.shape\n    A_copy = A.copy()\n    pivot_cols = []\n    pivot_row = 0\n    for col in range(n_cols):\n        if pivot_row >= n_rows:\n            break\n        max_row = pivot_row + np.argmax(np.abs(A_copy[pivot_row:, col]))\n        if abs(A_copy[max_row, col]) < 1e-15:\n            continue\n        if max_row != pivot_row:\n            A_copy[[pivot_row, max_row]] = A_copy[[max_row, pivot_row]]\n        pivot_cols.append(col)\n        for r in range(pivot_row + 1, n_rows):\n            if abs(A_copy[r, col]) > 1e-15:\n                factor = A_copy[r, col] / A_copy[pivot_row, col]\n                A_copy[r, col:] -= factor * A_copy[pivot_row, col:]\n        pivot_row += 1\n        if pivot_row == n_rows:\n            break\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n    return basis.tolist()\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute R-squared (coefficient of determination).\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,)\n    - y_pred: array-like of shape (n_samples,)\n\n    Returns:\n    - float: R-squared value rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    sst = np.sum((y_true - np.mean(y_true)) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - sse / sst\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    if not img:\n        return -1\n    if not img[0]:\n        return -1\n    row_length = len(img[0])\n    total_sum = 0\n    total_count = 0\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_sum += pixel\n            total_count += 1\n    average_brightness = total_sum / total_count\n    return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Computes the Root Mean Square Error (RMSE) between the actual values y_true \n    and the predicted values y_pred.\n\n    Parameters:\n    y_true (array-like): True/actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    TypeError: If inputs are not array-like (numpy array, list, or tuple).\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    \"\"\"\n    if not isinstance(y_true, (np.ndarray, list, tuple)) or not isinstance(y_pred, (np.ndarray, list, tuple)):\n        raise TypeError('y_true and y_pred must be array-like (numpy arrays, lists, or tuples).')\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must match.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary numpy arrays.\n    \n    Parameters:\n    -----------\n    y_true : numpy.ndarray\n        The ground truth binary labels.\n    y_pred : numpy.ndarray\n        The predicted binary labels.\n    \n    Returns:\n    --------\n    float\n        The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice Score between two binary arrays y_true and y_pred.\n\n    Parameters:\n    -----------\n    y_true : array-like of shape (n_samples,)\n        Ground truth (correct) labels as a binary array.\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels as a binary array.\n\n    Returns:\n    --------\n    float\n        Dice Score, a value between 0 and 1, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given row using Hyperdimensional Computing.\n\n    Args:\n        row (dict): A dictionary representing a dataset row, \n                    where keys are feature names and values are their corresponding values.\n        dim (int): Dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary mapping feature names to their reproducible random seeds.\n    \n    Returns:\n        list: The composite hypervector (as a list) representing the entire row.\n    \"\"\"\n\n    def stable_hash(value):\n        s = str(value)\n        h = 0\n        for char in s:\n            h = h * 31 + ord(char) & 4294967295\n        return h\n    composite_sum = np.zeros(dim, dtype=int)\n    n_features = len(row)\n    for (feature, value) in row.items():\n        np.random.seed(random_seeds[feature])\n        feature_hv = np.random.choice([0, 1], size=dim)\n        value_seed = random_seeds[feature] + stable_hash(value) & 4294967295\n        np.random.seed(value_seed)\n        value_hv = np.random.choice([0, 1], size=dim)\n        bound_hv = np.logical_xor(feature_hv, value_hv).astype(int)\n        composite_sum += bound_hv\n    threshold = n_features / 2.0\n    composite_hv = (composite_sum >= threshold).astype(int)\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification from the given data.\n\n    Parameters:\n    data (List[List[int]]): A list where each element is [y_true, y_pred].\n\n    Returns:\n    List[List[int]]: A 2x2 confusion matrix represented as [[TN, FP], [FN, TP]].\n    \"\"\"\n    counter = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    TN = counter[0, 0]\n    FP = counter[0, 1]\n    FN = counter[1, 0]\n    TP = counter[1, 1]\n    return [[TN, FP], [FN, TP]]\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors must not be empty.')\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Vectors must not have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    return round(cos_sim, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Computes performance metrics for a binary classification problem, including:\n    - Confusion Matrix (2x2)\n    - Accuracy\n    - F1 Score\n    - Specificity\n    - Negative Predictive Value\n\n    Parameters:\n    -----------\n    actual : list[int]\n        The actual class labels (1 for positive, 0 for negative).\n    predicted : list[int]\n        The predicted class labels (1 for positive, 0 for negative).\n\n    Returns:\n    --------\n    tuple:\n        A tuple containing:\n        - confusion_matrix: list[list[int]] (2x2 matrix)\n        - accuracy: float (rounded to three decimal places)\n        - f1_score: float (rounded to three decimal places)\n        - specificity: float (rounded to three decimal places)\n        - negative_predictive_value: float (rounded to three decimal places)\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"Length of 'actual' and 'predicted' must be the same.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be 0 or 1.\")\n    TP = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    TN = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    FP = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    FN = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    precision = TP / (TP + FP) if TP + FP != 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN != 0 else 0.0\n    if precision + recall != 0:\n        f1_score = 2 * precision * recall / (precision + recall)\n    else:\n        f1_score = 0.0\n    specificity = TN / (TN + FP) if TN + FP != 0 else 0.0\n    negative_predictive_value = TN / (TN + FN) if TN + FN != 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Returns a dictionary of descriptive statistics for the given dataset:\n    mean, median, mode, variance, standard deviation, 25th percentile,\n    50th percentile, 75th percentile, and interquartile range (IQR).\n    All numeric results are rounded to four decimal places. The mode \n    is returned as a single number if unique, or as a list if multiple modes.\n    \"\"\"\n    data = np.array(data, dtype=float)\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    (unique_vals, counts) = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    all_modes = unique_vals[counts == max_count]\n    if len(all_modes) == 1:\n        mode_val = all_modes[0]\n    else:\n        mode_val = all_modes.tolist()\n    variance_val = np.var(data, ddof=1)\n    std_val = np.std(data, ddof=1)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr = p75 - p25\n    mean_val = round(mean_val, 4)\n    median_val = round(median_val, 4)\n    variance_val = round(variance_val, 4)\n    std_val = round(std_val, 4)\n    p25 = round(p25, 4)\n    p50 = round(p50, 4)\n    p75 = round(p75, 4)\n    iqr = round(iqr, 4)\n    if isinstance(mode_val, float):\n        mode_val = round(mode_val, 4)\n    else:\n        mode_val = [round(m, 4) for m in mode_val]\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val, 'variance': variance_val, 'standard_deviation': std_val, '25th_percentile': p25, '50th_percentile': p50, '75th_percentile': p75, 'interquartile_range': iqr}\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    combinations = math.comb(n, k)\n    probability = combinations * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    two_pi = 2 * math.pi\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(two_pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError('k must be a non-negative integer.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image by taking the difference\n    between the maximum and the minimum pixel values.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image \n                             with pixel values between 0 and 255.\n\n    Returns:\n        int or float: The contrast value (max pixel - min pixel).\n    \"\"\"\n    if len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D array.')\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D numpy arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must have the same number of elements.')\n    return float(np.dot(vec1, vec2))\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial\n                           features of the corresponding data point. Each value is\n                           rounded to 8 decimal places. If degree is less than 0,\n                           an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        transformed_row = []\n        for d in range(degree + 1):\n            transformed_row.append(round(x ** d, 8))\n        transformed_data.append(transformed_row)\n    return transformed_data\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos = np.arange(position)[:, np.newaxis]\n    dim = np.arange(d_model)[np.newaxis, :]\n    angle_rates = pos / np.power(10000, 2 * (dim // 2) / np.float32(d_model))\n    pos_enc = np.zeros((position, d_model), dtype=np.float32)\n    pos_enc[:, 0::2] = np.sin(angle_rates[:, 0::2])\n    pos_enc[:, 1::2] = np.cos(angle_rates[:, 1::2])\n    return pos_enc.astype(np.float16).tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value (scalar or np.ndarray)\n    :param grad: Current gradient (scalar or np.ndarray)\n    :param m: First moment estimate (scalar or np.ndarray)\n    :param v: Second moment estimate (scalar or np.ndarray)\n    :param t: Current timestep (integer)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter_list, updated_m_list, updated_v_list)\n             Each returned array is rounded to 5 decimals and converted to a list.\n    \"\"\"\n    parameter = np.atleast_1d(parameter)\n    grad = np.atleast_1d(grad)\n    m = np.atleast_1d(m)\n    v = np.atleast_1d(v)\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    parameter_new = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter_new = np.round(parameter_new, 5).tolist()\n    m_new = np.round(m_new, 5).tolist()\n    v_new = np.round(v_new, 5).tolist()\n    return (parameter_new, m_new, v_new)\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generate text using a simplified GPT-2-like process:\n      1. Convert prompt to token IDs (token + positional embeddings).\n      2. Pass through a tiny \"Transformer block\" with:\n         - Single-head self-attention.\n         - Feed-forward network.\n         - Layer normalization.\n      3. Compute logits for the next token and sample from them.\n      4. Repeat for the requested number of tokens.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    token_ids = encoder.encode(prompt)\n\n    def layer_norm(x, g, b, eps=1e-05):\n        \"\"\"Applies layer normalization to tensor x.\"\"\"\n        mean = x.mean(axis=-1, keepdims=True)\n        var = x.var(axis=-1, keepdims=True)\n        return g * (x - mean) / np.sqrt(var + eps) + b\n    embed_dim = params['wte'].shape[1]\n    Q_w = np.random.randn(embed_dim, embed_dim)\n    K_w = np.random.randn(embed_dim, embed_dim)\n    V_w = np.random.randn(embed_dim, embed_dim)\n    ff_w1 = np.random.randn(embed_dim, embed_dim * 4)\n    ff_b1 = np.random.randn(embed_dim * 4)\n    ff_w2 = np.random.randn(embed_dim * 4, embed_dim)\n    ff_b2 = np.random.randn(embed_dim)\n    proj_w = np.random.randn(embed_dim, 3)\n    proj_b = np.random.randn(3)\n\n    def single_head_self_attention(x):\n        \"\"\"\n        A single-head self-attention over the sequence x.\n        x shape: [seq_len, embed_dim].\n        \"\"\"\n        q = x @ Q_w\n        k = x @ K_w\n        v = x @ V_w\n        d_k = np.sqrt(k.shape[-1])\n        scores = q @ k.transpose() / d_k\n        seq_len = x.shape[0]\n        causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n        scores[causal_mask] = -1000000000.0\n        att_weights = np.exp(scores - scores.max(axis=-1, keepdims=True))\n        att_weights /= att_weights.sum(axis=-1, keepdims=True)\n        return att_weights @ v\n\n    def feed_forward(h):\n        \"\"\"A tiny feed-forward network.\"\"\"\n        h1 = np.maximum(0, h @ ff_w1 + ff_b1)\n        return h1 @ ff_w2 + ff_b2\n\n    def transformer_block(x):\n        \"\"\"\n        A minimal, single-layer Transformer block:\n          - self-attention\n          - add & layer norm\n          - feed-forward\n          - add & layer norm\n        \"\"\"\n        attn_out = single_head_self_attention(x)\n        x = layer_norm(x + attn_out, g=params['ln_f']['g'], b=params['ln_f']['b'])\n        ff_out = feed_forward(x)\n        x = layer_norm(x + ff_out, g=params['ln_f']['g'], b=params['ln_f']['b'])\n        return x\n\n    def logits_for_next_token(x):\n        \"\"\"\n        Given the final hidden state for each token,\n        produce logits for next-token prediction\n        using a linear projection to the vocabulary size (3).\n        We'll use the hidden state of the last position.\n        \"\"\"\n        last_token_hidden = x[-1]\n        return last_token_hidden @ proj_w + proj_b\n\n    def sample_from_logits(logits):\n        \"\"\"Sample from the given logits (3 possible tokens).\"\"\"\n        probs = np.exp(logits - np.max(logits))\n        probs = probs / probs.sum()\n        return np.random.choice(len(probs), p=probs)\n    for _ in range(n_tokens_to_generate):\n        if len(token_ids) > n_ctx:\n            token_ids = token_ids[-n_ctx:]\n        x = []\n        for (i, tid) in enumerate(token_ids):\n            x.append(params['wte'][tid] + params['wpe'][i])\n        x = np.array(x)\n        hidden_states = transformer_block(x)\n        logits = logits_for_next_token(hidden_states)\n        next_token_id = sample_from_logits(logits)\n        token_ids.append(next_token_id)\n    generated_text = encoder.decode(token_ids)\n    return generated_text\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        shifted = values - np.max(values)\n        exp_vals = np.exp(shifted)\n        return exp_vals / np.sum(exp_vals)\n    crystals = np.array(crystal_values, dtype=float)\n    sqrt_d = np.sqrt(dimension)\n    att_score = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            att_score[i, j] = sqrt_d * (crystals[i] * crystals[j])\n    result = []\n    for i in range(n):\n        row_attention = softmax(att_score[i])\n        weighted_sum = 0.0\n        for j in range(n):\n            weighted_sum += row_attention[j] * crystals[j]\n        final_value = dimension * weighted_sum\n        result.append(round(final_value, 4))\n    return result\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'split'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query against a given corpus.\n\n    Parameters:\n    -----------\n    corpus : list of str\n        The corpus, where each element is a document (string).\n    query : str\n        The query string.\n    k1 : float, optional\n        Term frequency saturation parameter in BM25.\n    b : float, optional\n        Document length normalization parameter in BM25.\n\n    Returns:\n    --------\n    list of float\n        A list containing the BM25 score for each document, rounded to three decimals.\n    \"\"\"\n    tokenized_corpus = [doc.split() for doc in corpus]\n    query_terms = query.split()\n    N = len(tokenized_corpus)\n    doc_word_counts = []\n    df_counts = Counter()\n    for doc_tokens in tokenized_corpus:\n        counts = Counter(doc_tokens)\n        doc_word_counts.append(counts)\n        for term in set(doc_tokens):\n            df_counts[term] += 1\n    doc_lengths = [sum(counts.values()) for counts in doc_word_counts]\n    avg_doc_length = np.mean(doc_lengths) if doc_lengths else 0.0\n\n    def idf(term):\n        df = df_counts.get(term, 0)\n        return np.log((N - df + 0.5) / (df + 0.5) + 1)\n    scores = []\n    for (i, counts) in enumerate(doc_word_counts):\n        score = 0.0\n        doc_len = doc_lengths[i]\n        for term in query_terms:\n            f_td = counts.get(term, 0)\n            if f_td > 0:\n                numerator = f_td * (k1 + 1)\n                denominator = f_td + k1 * (1 - b + b * (doc_len / avg_doc_length))\n                score += idf(term) * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels (binary classification).\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            tp += 1\n        elif true_label == 0 and pred_label == 1:\n            fp += 1\n        elif true_label == 1 and pred_label == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * precision * recall / (precision + recall)\n    return round(f1_score, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = list(range(1, 11))\n    detrended = []\n    for i in days:\n        fluctuation = 10 * math.sin(2 * PI * i / 10.0)\n        detrended_value = consumption_data[i - 1] - fluctuation\n        detrended.append(detrended_value)\n    n = len(days)\n    x_mean = sum(days) / n\n    y_mean = sum(detrended) / n\n    numerator = sum(((x - x_mean) * (y - y_mean) for (x, y) in zip(days, detrended)))\n    denominator = sum(((x - x_mean) ** 2 for x in days))\n    slope = numerator / denominator if denominator != 0 else 0.0\n    intercept = y_mean - slope * x_mean\n    day_15_base = intercept + slope * 15\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10.0)\n    day_15_prediction = day_15_base + fluctuation_15\n    day_15_rounded = round(day_15_prediction)\n    final_forecast = math.ceil(day_15_rounded * 1.05)\n    return final_forecast\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: multi_head_attention() takes 4 positional arguments but 5 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute scaled dot-product attention:\n      scores = QK^T / sqrt(d_k)\n      weights = softmax(scores)\n      output = weights @ V\n    Q, K, V shapes: (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = weights @ V\n    return output\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Split Q, K, V into multiple heads, apply self_attention for each head,\n    then concatenate the results. Q, K, V shapes: (m, d). \n    We assume d is divisible by n_heads.\n    \"\"\"\n    (m, d) = Q.shape\n    d_head = d // n_heads\n    Q_heads = Q.reshape(m, n_heads, d_head)\n    K_heads = K.reshape(m, n_heads, d_head)\n    V_heads = V.reshape(m, n_heads, d_head)\n    outputs = []\n    for i in range(n_heads):\n        Q_i = Q_heads[:, i, :]\n        K_i = K_heads[:, i, :]\n        V_i = V_heads[:, i, :]\n        head_out = self_attention(Q_i, K_i, V_i)\n        outputs.append(head_out)\n    return np.concatenate(outputs, axis=-1)\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n        x (list[int]): A list of binary values (0 or 1).\n        y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n        float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n_11 = n_10 = n_01 = n_00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n_11 += 1\n        elif xi == 1 and yi == 0:\n            n_10 += 1\n        elif xi == 0 and yi == 1:\n            n_01 += 1\n        elif xi == 0 and yi == 0:\n            n_00 += 1\n    n_1_ = n_11 + n_10\n    n_0_ = n_01 + n_00\n    n__1 = n_11 + n_01\n    n__0 = n_10 + n_00\n    numerator = n_11 * n_00 - n_10 * n_01\n    denominator = (n_1_ * n_0_ * n__1 * n__0) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    softplus(x) = log(1 + e^x)\n\n    This function handles edge cases to prevent numerical overflow or underflow\n    by using a piecewise approach.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value, rounded to 4 decimal places.\n    \"\"\"\n    if x < 0:\n        result = math.log1p(math.exp(x))\n    else:\n        result = x + math.log1p(math.exp(-x))\n    return round(result, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value (rounded to 6 decimals).\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_ref = np.array(pi_theta_ref, dtype=np.float64)\n    unclipped_obj = rhos * A\n    clipped_obj = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon) * A\n    clipped_advantage = np.minimum(unclipped_obj, clipped_obj)\n    clipped_advantage_sum = np.sum(clipped_advantage)\n    pi_theta_unnormalized = rhos * pi_old\n    normalization = np.sum(pi_theta_unnormalized)\n    pi_theta = pi_theta_unnormalized / (normalization + 1e-10)\n    eps = 1e-10\n    kl_div = np.sum(pi_theta * np.log((pi_theta + eps) / (pi_ref + eps)))\n    objective = clipped_advantage_sum - beta * kl_div\n    return float(np.round(objective, 6))\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function: x * sigmoid(x)\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid_x\n    return round(swish_val, 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * math.expm1(x)\n    return round(selu_value, 4)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a Python list\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-logits))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, C) = X.shape\n    M = np.max(y) + 1\n    Y = np.eye(M)[y]\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exps = np.exp(scores)\n        probs = exps / np.sum(exps, axis=1, keepdims=True)\n        loss = -np.sum(Y * np.log(probs + 1e-10)) / N\n        losses.append(loss)\n        grad = X.T @ (probs - Y) / N\n        B -= learning_rate * grad\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(val, 4) for val in losses]\n    return (B_rounded, losses_rounded)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters \n    with Binary Cross Entropy loss.\n    All returned values are rounded to the 4th decimal place.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Labels array of shape (n_samples, ).\n    learning_rate : float\n        Step size for gradient descent updates.\n    iterations : int\n        Number of gradient descent steps.\n    \n    Returns:\n    --------\n    tuple[list[float], list[float]]\n        A tuple of:\n        - the final fitted parameters as a list of floats\n        - the history of loss values over iterations as a list of floats\n    \"\"\"\n    y = y.flatten()\n    X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n    (n_samples, n_features) = X_ext.shape\n    params = np.zeros(n_features, dtype=float)\n    epsilon = 1e-10\n    loss_values = []\n    for _ in range(iterations):\n        z = X_ext @ params\n        y_pred = 1 / (1 + np.exp(-z))\n        loss = -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n        loss_values.append(round(loss, 4))\n        grad = 1 / n_samples * (X_ext.T @ (y_pred - y))\n        params -= learning_rate * grad\n    params = np.round(params, 4).tolist()\n    return (params, loss_values)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    Q, K, V: shape (seq_length, d_model) or similar\n    mask: shape (seq_length, seq_length) \n          with 1's for valid positions and 0's (or False) for masked positions\n    Returns attention outputs as a list (seq_length, d_model).\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    large_negative = -1000000000.0\n    scores = np.where(mask > 0, scores, large_negative)\n    scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    scores_exp_sum = np.sum(scores_exp, axis=-1, keepdims=True)\n    attention_weights = scores_exp / (scores_exp_sum + 1e-09)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Disorder is measured using the Shannon entropy of the color distribution.\n    The disorder value is 0 if all apples are the same color, and increases with more variety.\n    Results are rounded to the nearest 4th decimal.\n    \"\"\"\n    from math import log2\n    if len(apples) <= 1:\n        return 0.0\n    color_counts = {}\n    for color in apples:\n        color_counts[color] = color_counts.get(color, 0) + 1\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total_apples\n        entropy -= p * log2(p)\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Layer Normalization on the given 3D tensor X with shape \n    (batch_size, sequence_length, feature_dim).\n    \n    Normalizes X across the last dimension (feature dimension) for each element \n    in the batch and sequence, then applies the supplied scaling (gamma) and \n    shifting (beta) parameters.\n\n    The output is rounded to 5 decimal places and returned as a nested list.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scale parameter of shape (feature_dim,).\n    beta (np.ndarray): Shift parameter of shape (feature_dim,).\n    epsilon (float): A small constant to avoid division by zero.\n\n    Returns:\n    list: A Python list containing the normalized and scaled data, \n          rounded to 5 decimal places.\n    \"\"\"\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized * gamma + beta\n    X_out = np.round(X_normalized, 5).tolist()\n    return X_out\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for a single reference and candidate translation.\n\n    Parameters:\n    reference (str or list): Reference translation (either a tokenized list or space-delimited string).\n    candidate (str or list): Candidate translation (either a tokenized list or space-delimited string).\n    alpha (float): Weight parameter (default: 0.9).\n    beta (float): Penalty exponent (default: 3).\n    gamma (float): Penalty coefficient (default: 0.5).\n\n    Returns:\n    float: METEOR score rounded to three decimal places.\n    \"\"\"\n    if isinstance(reference, str):\n        reference_tokens = reference.strip().split()\n    else:\n        reference_tokens = reference\n    if isinstance(candidate, str):\n        candidate_tokens = candidate.strip().split()\n    else:\n        candidate_tokens = candidate\n    ref_counts = Counter(reference_tokens)\n    cand_counts = Counter(candidate_tokens)\n    common_words = set(ref_counts.keys()) & set(cand_counts.keys())\n    matches = sum((min(ref_counts[w], cand_counts[w]) for w in common_words))\n    if matches == 0:\n        return 0.0\n    precision = matches / len(candidate_tokens)\n    recall = matches / len(reference_tokens)\n    f_mean = precision * recall / ((1 - alpha) * precision + alpha * recall)\n    ref_positions = {}\n    for (i, w) in enumerate(reference_tokens):\n        ref_positions.setdefault(w, []).append(i)\n    ref_counts_for_alignment = Counter(ref_counts)\n    aligned_positions = []\n    for w in candidate_tokens:\n        if ref_counts_for_alignment[w] > 0:\n            aligned_positions.append(ref_positions[w][0])\n            ref_positions[w].pop(0)\n            ref_counts_for_alignment[w] -= 1\n    chunks = 1\n    for i in range(1, len(aligned_positions)):\n        if aligned_positions[i] != aligned_positions[i - 1] + 1:\n            chunks += 1\n    penalty = gamma * (chunks / matches) ** beta\n    penalty = min(penalty, 1.0)\n    meteor = (1 - penalty) * f_mean\n    return round(meteor, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events X and Y.\n\n    PMI(X, Y) = log2( p(X,Y) / ( p(X) * p(Y) ) )\n              = log2( (joint_counts / total_samples) / \n                      ((total_counts_x / total_samples) * (total_counts_y / total_samples)) )\n\n    :param joint_counts: Number of times X and Y jointly occur\n    :param total_counts_x: Number of times X occurs\n    :param total_counts_y: Number of times Y occurs\n    :param total_samples: Total number of samples\n    :return: PMI value rounded to 3 decimal places (float)\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_xy == 0 or p_x == 0 or p_y == 0:\n        return float('-inf')\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return float(round(pmi, 3))\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n    \n    Returns a list of floats, each rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = []\n    for val in x:\n        norm_val = (val - min_val) / (max_val - min_val)\n        normalized.append(round(norm_val, 4))\n    return normalized\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple residual block that takes a 1D input array x,\n    applies two dense layers with ReLU activations, adds\n    the original input x (shortcut), and applies a final ReLU.\n    \n    The final output is rounded to 4 decimal places and\n    returned as a list.\n    \"\"\"\n    relu = lambda z: np.maximum(0, z)\n    z1 = x @ w1\n    a1 = relu(z1)\n    z2 = a1 @ w2\n    a2 = relu(z2)\n    out = relu(a2 + x)\n    return np.round(out, 4).tolist()\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    The input shape is (height, width, channels).\n    The returned 1D array has shape (channels,), \n    where each element is the average of all values \n    in the corresponding feature map.\n    \"\"\"\n    assert len(x.shape) == 3, 'Input should be a 3D array (height, width, channels).'\n    return np.mean(x, axis=(0, 1))\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D input (BCHW).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width).\n    gamma : np.ndarray\n        Scale parameters (one per channel).\n    beta : np.ndarray\n        Shift parameters (one per channel).\n    epsilon : float, optional\n        A small value added to variance for numerical stability. Default is 1e-5.\n\n    Returns:\n    --------\n    list\n        The batch-normalized array rounded to 4 decimal places, converted to a Python list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    gamma_reshaped = gamma.reshape((1, -1, 1, 1))\n    beta_reshaped = beta.reshape((1, -1, 1, 1))\n    out = gamma_reshaped * X_norm + beta_reshaped\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term (c * x^n) at the point x.\n    \n    :param c: Coefficient of the polynomial term\n    :param x: The point at which to evaluate the derivative\n    :param n: Exponent of the polynomial term\n    :return: The derivative value rounded to 4 decimal places\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by the given 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors, e.g. [[x1, y1], [x2, y2], ...].\n    tol (float): Tolerance for determining linear independence.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (each rounded to 4 decimals) \n                       that spans the same subspace as 'vectors'.\n    \"\"\"\n    arrs = [np.array(vec, dtype=float) for vec in vectors]\n    basis = []\n    for v in arrs:\n        w = v.copy()\n        for b in basis:\n            w -= np.dot(w, b) * b\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            basis.append(w / norm_w)\n    return [np.round(vec, 4).tolist() for vec in basis]\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    a = np.array(a, dtype=float)\n    b = np.array(b, dtype=float)\n    cross_prod = np.cross(a, b)\n    cross_prod_rounded = np.round(cross_prod, 4)\n    return cross_prod_rounded.tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    detA = np.linalg.det(A)\n    if np.isclose(detA, 0.0):\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        detA_i = np.linalg.det(A_i)\n        x_val = detA_i / detA\n        x.append(x_val)\n    x = np.round(x, 4).tolist()\n    return x\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n    bc = sum((np.sqrt(pi * qi) for (pi, qi) in zip(p, q)))\n    if bc <= 0:\n        return 0.0\n    distance = -np.log(bc)\n    return round(distance, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Returns the element-wise sum of two vectors (lists).\n    If the vectors have different lengths, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Implements the REINFORCE policy gradient estimator.\n    \n    The policy is parameterized by theta, which has shape (num_states, num_actions).\n    We compute a softmax over each row theta[s, :] to get the action probabilities\n    for state s. For each episode and time step, we multiply the gradient of the log\n    probability of the taken action by the return from that time step onward. Then we\n    average across all episodes.\n\n    Parameters:\n    -----------\n    theta    : np.ndarray\n               Array of shape (num_states, num_actions), the policy parameters.\n    episodes : list of lists\n               Each element is an episode, which is a list of (state, action, reward) tuples.\n\n    Returns:\n    --------\n    list\n        The averaged gradient (same shape as theta), rounded to 4 decimals, returned as a Python list.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    grad = np.zeros_like(theta)\n\n    def softmax(x):\n        shifted_x = x - np.max(x)\n        exp_x = np.exp(shifted_x)\n        return exp_x / np.sum(exp_x)\n    for episode in episodes:\n        T = len(episode)\n        returns = np.zeros(T)\n        G = 0.0\n        for t in reversed(range(T)):\n            G += episode[t][2]\n            returns[t] = G\n        for t in range(T):\n            (s, a, _) = episode[t]\n            p = softmax(theta[s])\n            for a_prime in range(num_actions):\n                indicator = 1.0 if a_prime == a else 0.0\n                grad[s, a_prime] += returns[t] * (indicator - p[a_prime])\n    grad /= len(episodes)\n    grad = np.round(grad, 4)\n    return grad.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Computes the FLOPs of a large dense layer vs. a Mixture-of-Experts layer\n    (as in \"Outrageously Large Neural Networks: The Sparsely-Gated\n    Mixture-of-Experts Layer\") and returns the percentage savings.\n\n    Assumes:\n      - The \"dense\" baseline has dimensions [d_in, n_experts * d_out]\n        to match the total capacity of n_experts experts.\n      - The MoE layer has n_experts experts, each [d_in, d_out] in size,\n        but only k_active experts are used per input.\n\n    Parameters:\n      n_experts (int): number of experts in the MoE layer\n      k_active  (int): number of active (selected) experts per input\n      d_in      (int): input dimension\n      d_out     (int): output dimension (per expert)\n\n    Returns:\n      dict: {\n         'dense_flops': float (one-decimal),\n         'moe_flops': float (one-decimal),\n         'savings_percent': float (one-decimal)\n      }\n    \"\"\"\n    dense_flops = d_in * (n_experts * d_out)\n    moe_flops = d_in * (k_active * d_out)\n    savings_percent = 100.0 * (dense_flops - moe_flops) / dense_flops\n    return {'dense_flops': round(dense_flops, 1), 'moe_flops': round(moe_flops, 1), 'savings_percent': round(savings_percent, 1)}\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements Noisy Top-K gating used in Mixture-of-Experts.\n\n    Args:\n        X (np.ndarray): Input of shape (batch_size, input_dim).\n        W_g (np.ndarray): Gating weights of shape (input_dim, num_experts).\n        W_noise (np.ndarray): Noise weights of shape (input_dim, num_experts).\n        N (np.ndarray): Pre-sampled noise of shape (batch_size, num_experts).\n        k (int): Sparsity constraint - number of active experts per sample.\n\n    Returns:\n        list: A list (converted from np.ndarray) containing the gating probabilities\n              of shape (batch_size, num_experts). Each row sums to 1 among its\n              top-k positions; other positions are 0.\n    \"\"\"\n    base_logits = X.dot(W_g)\n    noise_logits = X.dot(W_noise) * N\n    gating_logits = base_logits + noise_logits\n    (batch_size, num_experts) = gating_logits.shape\n    gating_probs = np.zeros((batch_size, num_experts), dtype=np.float32)\n    for i in range(batch_size):\n        row = gating_logits[i]\n        topk_indices = np.argsort(row)[-k:]\n        topk_logits = row[topk_indices]\n        shifted_logits = topk_logits - np.max(topk_logits)\n        exp_logits = np.exp(shifted_logits)\n        denom = np.sum(exp_logits)\n        gating_probs[i, topk_indices] = exp_logits / denom\n    gating_probs = np.round(gating_probs, 4)\n    return gating_probs.tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: index 3 is out of bounds for axis 1 with size 3", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture of Experts (MoE) layer.\n    \n    Parameters:\n    -----------\n    x : np.ndarray\n        Input data of shape (batch_size, d_in).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, d_in, d_out).\n    Wg : np.ndarray\n        Gating weight matrix of shape (d_in, n_experts).\n    n_experts : int\n        Number of experts.\n    top_k : int\n        Number of top experts to select per token.\n    \n    Returns:\n    --------\n    List[List[float]]\n        The output of the MoE layer, rounded to 4 decimal places,\n        converted to a nested Python list.\n    \"\"\"\n    gating_logits = x @ Wg\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    stable_logits = gating_logits - max_logits\n    exp_logits = np.exp(stable_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    batch_size = x.shape[0]\n    d_out = We.shape[2]\n    output = np.zeros((batch_size, d_out))\n    for i in range(batch_size):\n        top_indices = np.argpartition(gating_probs[i], -top_k)[-top_k:]\n        for e in top_indices:\n            expert_out = x[i] @ We[e]\n            output[i] += gating_probs[i, e] * expert_out\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor X with shape (B, C, H, W).\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input data of shape (B, C, H, W).\n    gamma : np.ndarray\n        Scale parameter of shape (C,).\n    beta : np.ndarray\n        Shift parameter of shape (C,).\n    num_groups : int\n        Number of channel groups to split into.\n    epsilon : float\n        A small constant for numerical stability.\n\n    Returns:\n    --------\n    List (Python list):\n        The normalized output, converted to a Python list\n        after rounding to 4 decimal places.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    group_size = C // num_groups\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_scaled = X_normalized * gamma.reshape(1, C, 1, 1) + beta.reshape(1, C, 1, 1)\n    X_rounded = np.round(X_scaled, 4)\n    return X_rounded.tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x that minimizes\n    f(x) = x^4 - 3x^3 + 2.\n    Returns the result rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def derivative(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        grad = derivative(current_x)\n        next_x = current_x - learning_rate * grad\n        if abs(next_x - current_x) < tolerance:\n            break\n        current_x = next_x\n    return round(current_x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, a normalization-free transformation\n    inspired by the Tanh function. This function replaces layer normalization in\n    Transformer architectures while preserving squashing behavior and enabling stable training.\n\n    Args:\n        x (np.ndarray): Input array.\n        alpha (float): Scaling parameter.\n        gamma (float): Gain parameter.\n        beta (float): Shift parameter.\n\n    Returns:\n        list[float]: Python list of the output array after applying the DyT function,\n                     rounded to 4 decimal places.\n    \"\"\"\n    y = alpha * np.tanh(gamma * x + beta)\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
