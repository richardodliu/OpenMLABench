{"task_id": 1, "completion_id": 0, "completion": "# Matrix-Vector Dot Product Function\n## Description\nThis function computes the dot product of a matrix and a vector. It returns a list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n\n## Code\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n    b (list[int|float]): The input vector.\n\n    Returns:\n    list[int|float]: The resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n\n    # Check if the input matrix is empty\n    if not a:\n        return -1\n\n    # Get the number of columns in the matrix\n    num_cols = len(a[0])\n\n    # Check if all rows in the matrix have the same number of columns\n    if not all(len(row) == num_cols for row in a):\n        raise ValueError(\"All rows in the matrix must have the same number of columns\")\n\n    # Check if the number of columns in the matrix matches the length of the vector\n    if num_cols != len(b):\n        return -1\n\n    # Compute the dot product of each row in the matrix with the vector\n    result = [sum(row[i] * b[i] for i in range(num_cols)) for row in a]\n\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    vector = [7, 8, 9]\n    print(matrix_dot_vector(matrix, vector))  # Output: [50, 122]\n\n    matrix = [[1, 2], [3, 4]]\n    vector = [5, 6, 7]\n    print(matrix_dot_vector(matrix, vector))  # Output: -1\n```\n\n## Explanation\n\n*   The function `matrix_dot_vector` takes a matrix `a` and a vector `b` as input and returns their dot product.\n*   It first checks if the input matrix is empty and returns -1 in this case.\n*   Then, it verifies that all rows in the matrix have the same number of columns and raises a `ValueError` if this condition is not met.\n*   Next, it checks if the number of columns in the matrix matches the length of the vector. If not, it returns -1.\n*   Finally, it computes the dot product of each row in the matrix with the vector using a list comprehension and returns the result.\n\n## Tests and Example Uses\n\n*   The example usage demonstrates how to use the `matrix_dot_vector` function with a valid and an invalid input.\n*   In the valid case, the function correctly computes the dot product of the matrix and the vector.\n*   In the invalid case, the function returns -1 because the number of columns in the matrix does not match the length of the vector."}
{"task_id": 2, "completion_id": 0, "completion": "## Transpose of a Matrix\n### Description\nThe transpose of a matrix is an operator which can be thought of as \"swapping\" the rows and columns for a matrix. For example, the transpose of a matrix `A` with dimensions `m x n` is a matrix `A^T` with dimensions `n x m`.\n\n### Code\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n\n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    # Check if the input matrix is empty\n    if not a:\n        return []\n\n    # Use the built-in zip function in combination with list comprehension to transpose the matrix\n    # The zip function groups the i-th elements of each row together\n    # The * operator unpacks the list of lists into separate arguments for the zip function\n    return [list(row) for row in zip(*a)]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a sample matrix\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n\n    # Compute and print the transpose of the matrix\n    transpose = transpose_matrix(matrix)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    print(\"\\nTransposed Matrix:\")\n    for row in transpose:\n        print(row)\n```\n\n### Explanation\n\n*   The function `transpose_matrix` takes a 2D list `a` as input and returns its transpose.\n*   It first checks if the input matrix is empty. If it is, the function returns an empty list.\n*   The `zip(*a)` expression is used to transpose the matrix. Here, `*a` unpacks the list of lists into separate arguments for the `zip` function. The `zip` function then groups the i-th elements of each row together, effectively transposing the matrix.\n*   The outer list comprehension `[list(row) for row in zip(*a)]` converts the tuples returned by `zip` back into lists.\n\n### Time Complexity\n\nThe time complexity of this function is O(m \\* n), where m is the number of rows and n is the number of columns in the input matrix. This is because the `zip` function and the list comprehension iterate over all elements of the input matrix once."}
{"task_id": 3, "completion_id": 0, "completion": "### Reshape Matrix Function\n#### Description\nThis function reshapes a given matrix into a specified shape using NumPy. If the matrix cannot be reshaped into the specified shape, it returns an empty list.\n\n#### Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n    new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n    list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n\n    # Convert the input list to a NumPy array\n    arr = np.array(a)\n\n    # Check if the total number of elements in the original and new shapes are equal\n    if arr.size != new_shape[0] * new_shape[1]:\n        # If not, return an empty list\n        return []\n\n    # Reshape the array to the new shape\n    reshaped_arr = arr.reshape(new_shape)\n\n    # Convert the reshaped array back to a list and return it\n    return reshaped_arr.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    new_shape = (2, 3)\n    print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3], [4, 5, 6]]\n\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    new_shape = (3, 2)\n    print(reshape_matrix(matrix, new_shape))  # Output: []\n\n    matrix = [[1, 2], [3, 4], [5, 6]]\n    new_shape = (2, 3)\n    print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3], [4, 5, 6]]\n```\n\n#### Explanation\n\n1. The function first converts the input list to a NumPy array.\n2. It checks if the total number of elements in the original and new shapes are equal. If not, it returns an empty list.\n3. If the shapes are compatible, it reshapes the array to the new shape using the `reshape` method.\n4. Finally, it converts the reshaped array back to a list using the `tolist` method and returns it.\n\nThe example usage demonstrates how to use the function with different inputs and shapes."}
{"task_id": 4, "completion_id": 0, "completion": "### Calculate Mean by Row or Column\n\nHere's a Python function that calculates the mean of a matrix either by row or by column based on a given mode.\n\n#### Code\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n        matrix (list[list[float]]): A 2D list of numbers.\n        mode (str): 'row' or 'column' to specify the calculation mode.\n\n    Returns:\n        list[float]: A list of means according to the specified mode.\n\n    Raises:\n        ValueError: If the mode is neither 'row' nor 'column'.\n    \"\"\"\n\n    # Check if the matrix is empty\n    if not matrix:\n        return []\n\n    # Check if the mode is valid\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n\n    # Calculate mean by row\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n\n    # Calculate mean by column\n    else:\n        # Transpose the matrix to easily calculate column means\n        transposed_matrix = list(map(list, zip(*matrix)))\n        return [sum(col) / len(col) for col in transposed_matrix]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n\n    print(\"Matrix:\")\n    for row in matrix:\n        print(row)\n\n    row_means = calculate_matrix_mean(matrix, 'row')\n    print(\"\\nMeans by row:\", row_means)\n\n    col_means = calculate_matrix_mean(matrix, 'column')\n    print(\"Means by column:\", col_means)\n```\n\n#### Explanation\n\n*   The function `calculate_matrix_mean` takes a matrix and a mode as input and returns a list of means.\n*   It first checks if the matrix is empty and returns an empty list in that case.\n*   It then checks if the mode is valid and raises a `ValueError` if it's not.\n*   For the 'row' mode, it uses a list comprehension to calculate the mean of each row.\n*   For the 'column' mode, it transposes the matrix using the `zip` function and then calculates the mean of each column.\n*   The example usage demonstrates how to use the function with a sample matrix and print the results."}
{"task_id": 5, "completion_id": 0, "completion": "## Scalar Multiplication of a Matrix\n\n### Description\nThis function multiplies a given matrix by a scalar value and returns the resulting matrix.\n\n### Code\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    This function multiplies a given matrix by a scalar value.\n\n    Args:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix by.\n\n    Returns:\n    list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n\n    # Check if the input matrix is empty\n    if not matrix:\n        return []\n\n    # Get the number of rows and columns in the matrix\n    num_rows = len(matrix)\n    num_cols = len(matrix[0])\n\n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(num_cols)] for _ in range(num_rows)]\n\n    # Perform scalar multiplication\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result[i][j] = matrix[i][j] * scalar\n\n    return result\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar = 2\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    result = scalar_multiply(matrix, scalar)\n    print(f\"\\nMatrix multiplied by {scalar}:\")\n    for row in result:\n        print(row)\n```\n\n### Explanation\n\n*   The function `scalar_multiply` takes a 2D list `matrix` and a scalar value `scalar` as input.\n*   It first checks if the input matrix is empty. If it is, the function returns an empty list.\n*   It then initializes a result matrix with the same dimensions as the input matrix, filled with zeros.\n*   The function performs scalar multiplication by iterating over each element in the input matrix, multiplying it by the scalar, and storing the result in the corresponding position in the result matrix.\n*   Finally, it returns the resulting matrix.\n\n### Alternative Implementation Using List Comprehension\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    This function multiplies a given matrix by a scalar value.\n\n    Args:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix by.\n\n    Returns:\n    list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar = 2\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    result = scalar_multiply(matrix, scalar)\n    print(f\"\\nMatrix multiplied by {scalar}:\")\n    for row in result:\n        print(row)\n```\n\nThis alternative implementation achieves the same result using list comprehension, which provides a more concise way to create the resulting matrix."}
{"task_id": 6, "completion_id": 0, "completion": "## Calculating Eigenvalues of a 2x2 Matrix\n### Overview\n\nThe following Python function calculates the eigenvalues of a given 2x2 matrix. It uses the mathematical formula for finding eigenvalues, which involves solving the characteristic equation.\n\n### Code\n\n```python\nimport math\nimport cmath\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[float]: A list of eigenvalues in descending order.\n\n    Raises:\n    ValueError: If the input matrix is not a 2x2 matrix.\n    \"\"\"\n\n    # Check if the input matrix is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Extract matrix elements\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n\n    # Calculate the trace and determinant of the matrix\n    trace = a + d\n    determinant = a * d - b * c\n\n    # Calculate the eigenvalues using the quadratic formula\n    sqrt_term = math.sqrt((trace / 2) ** 2 - determinant)\n    eigenvalue1 = (trace / 2) + sqrt_term\n    eigenvalue2 = (trace / 2) - sqrt_term\n\n    # Return eigenvalues in descending order\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2], [3, 4]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(\"Eigenvalues:\", eigenvalues)\n```\n\n### Explanation\n\n*   The function `calculate_eigenvalues` takes a 2x2 matrix as input and returns a list of its eigenvalues in descending order.\n*   It first checks if the input matrix is a 2x2 matrix and raises a `ValueError` if not.\n*   The eigenvalues of a 2x2 matrix `[[a, b], [c, d]]` can be found using the formula: `\u03bb = (trace \u00b1 sqrt(trace^2 - 4 * determinant))) / 2`, where `trace = a + d` and `determinant = a * d - b * c`.\n*   The function calculates the trace and determinant of the matrix, then uses these values to find the eigenvalues.\n*   Finally, it returns the eigenvalues in descending order.\n\n### Note\n\n*   This function assumes that the input matrix is a square matrix (2x2).\n*   It uses the `math` module for square root calculations and handles complex eigenvalues by using `cmath` (not explicitly needed here but good practice).\n*   The example usage demonstrates how to call the function with a sample matrix and print the resulting eigenvalues."}
{"task_id": 7, "completion_id": 0, "completion": "# Matrix Transformation\n## Description\nThis function transforms a given matrix $A$ using the operation $T^{-1}AS$, \nwhere $T$ and $S$ are invertible matrices.\n\n## Code\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^{-1}AS, \n    where T and S are invertible matrices.\n\n    Args:\n        A (list[list[int|float]]): The matrix to be transformed.\n        T (list[list[int|float]]): The invertible matrix T.\n        S (list[list[int|float]]): The invertible matrix S.\n\n    Returns:\n        list[list[int|float]]: The transformed matrix A. Returns -1 if T or S is not invertible.\n    \"\"\"\n\n    # Convert input lists to numpy arrays\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n\n    # Check if T and S are invertible\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n\n    # Calculate the inverse of T\n    T_inv = np.linalg.inv(T)\n\n    # Perform the transformation T^{-1}AS\n    transformed_A = np.dot(np.dot(T_inv, A), S)\n\n    # Round the result to the nearest 4th decimal and convert to list\n    transformed_A = np.round(transformed_A, 4).tolist()\n\n    return transformed_A\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[1, 2], [3, 4]]\n    T = [[2, 1], [4, 3]]\n    S = [[5, 6], [7, 8]]\n    \n    result = transform_matrix(A, T, S)\n    print(result)\n```\n\n## Explanation\n\n1. The function `transform_matrix` takes three parameters: `A`, `T`, and `S`, which are the matrices to be transformed and the invertible matrices, respectively.\n2. It first checks if `T` and `S` are invertible by calculating their determinants using `np.linalg.det`. If either determinant is zero, it returns `-1`.\n3. It then calculates the inverse of `T` using `np.linalg.inv`.\n4. The transformation `T^{-1}AS` is performed using `np.dot` to compute the matrix products.\n5. The result is rounded to the nearest 4th decimal using `np.round` and converted to a list using the `tolist` method.\n6. The example usage demonstrates how to call the function with sample matrices `A`, `T`, and `S`, and prints the transformed matrix."}
{"task_id": 8, "completion_id": 0, "completion": "## Calculating the Inverse of a 2x2 Matrix\n\n### Description\n\nThe following Python function calculates the inverse of a given 2x2 matrix. If the matrix is not invertible (i.e., its determinant is zero), the function returns `None`.\n\n### Code\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[list[float]]: The inverse of the input matrix, or None if the matrix is not invertible.\n    \"\"\"\n\n    # Check if the input matrix is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Extract the elements of the matrix\n    a, b = matrix[0][0], matrix[0][1]\n    c, d = matrix[1][0], matrix[1][1]\n\n    # Calculate the determinant of the matrix\n    determinant = a * d - b * c\n\n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None\n\n    # Calculate the inverse of the matrix\n    inverse = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n\n    return inverse\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2], [3, 4]]\n    inverse = inverse_2x2(matrix)\n    if inverse is not None:\n        print(\"Inverse of the matrix:\")\n        for row in inverse:\n            print(row)\n    else:\n        print(\"The matrix is not invertible.\")\n```\n\n### Explanation\n\n1. The function first checks if the input matrix is a 2x2 matrix and raises a `ValueError` if it's not.\n2. It then extracts the elements of the matrix and calculates its determinant.\n3. If the determinant is zero, the function returns `None`, indicating that the matrix is not invertible.\n4. Otherwise, it calculates the inverse of the matrix using the formula for the inverse of a 2x2 matrix.\n5. The function returns the inverse matrix.\n\n### Example Use Case\n\nThe example usage demonstrates how to use the `inverse_2x2` function to calculate the inverse of a 2x2 matrix. If the matrix is invertible, it prints the inverse; otherwise, it prints a message indicating that the matrix is not invertible."}
{"task_id": 9, "completion_id": 0, "completion": "## Matrix Multiplication Function\n### Overview\n\nThe following Python function multiplies two matrices `a` and `b`. It first checks if the matrices can be multiplied by comparing the number of columns in `a` with the number of rows in `b`. If they are compatible, it performs the multiplication; otherwise, it returns `-1`.\n\n### Code\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function multiplies two matrices a and b.\n    \n    Args:\n    a (list[list[int | float]]): The first matrix.\n    b (list[list[int | float]]): The second matrix.\n    \n    Returns:\n    list[list[int | float]]: The product of matrices a and b if they can be multiplied; otherwise, -1.\n    \"\"\"\n\n    # Check if matrices can be multiplied\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n\n    # Get dimensions of matrices\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n\n    # Check if matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n\n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):  # or 'range(rows_b)'\n                result[i][j] += a[i][k] * b[k][j]\n\n    return result\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix_a = [[1, 2, 3], [4, 5, 6]]\n    matrix_b = [[7, 8], [9, 10], [11, 12]]\n    print(matrixmul(matrix_a, matrix_b))\n\n    matrix_c = [[1, 2], [3, 4]]\n    matrix_d = [[5, 6, 7], [8, 9, 10]]\n    print(matrixmul(matrix_c, matrix_d))\n\n    matrix_e = [[1, 2], [3, 4]]\n    matrix_f = [[5, 6], [7, 8], [9, 10]]\n    print(matrixmul(matrix_e, matrix_f))  # Should return -1\n```\n\n### Explanation\n\n*   The function `matrixmul` takes two parameters, `a` and `b`, which are lists of lists representing matrices.\n*   It first checks if either matrix is empty or if the number of columns in `a` does not match the number of rows in `b`. If either condition is true, it returns `-1` as the matrices cannot be multiplied.\n*   It then initializes a result matrix filled with zeros, with dimensions based on the number of rows in `a` and the number of columns in `b`.\n*   The function performs the matrix multiplication using nested loops, iterating over the rows of `a`, the columns of `b`, and the common dimension (columns of `a` or rows of `b`).\n*   Finally, it returns the result matrix.\n\n### Example Use Cases\n\nThe provided example usage demonstrates how to multiply two compatible matrices and also shows a case where the matrices cannot be multiplied, returning `-1`."}
{"task_id": 10, "completion_id": 0, "completion": "## Covariance Matrix Calculation\n### Overview\n\nThe covariance matrix is a square matrix that summarizes the covariance between different features in a multivariate dataset. Here, we will implement a Python function to calculate the covariance matrix for a given set of vectors.\n\n### Implementation\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: A covariance matrix as a list of lists.\n    \"\"\"\n\n    # Convert the input list to a NumPy array for easier manipulation\n    array = np.array(vectors)\n\n    # Calculate the mean of each feature\n    mean = np.mean(array, axis=1, keepdims=True)\n\n    # Center the data by subtracting the mean\n    centered_array = array - mean\n\n    # Calculate the covariance matrix using the formula: cov = (X^T * X) / (n - 1)\n    # We use np.cov to calculate the covariance matrix directly\n    # The rowvar=False argument indicates that each column is a feature\n    covariance_matrix = np.cov(array, rowvar=False)\n\n    # Convert the covariance matrix back to a list of lists\n    covariance_matrix = covariance_matrix.tolist()\n\n    return covariance_matrix\n\n\n# Test cases\ndef main():\n    # Test case 1: A simple example with two features\n    vectors1 = [[1, 2, 3, 4, 5], [2, 3, 5, 7, 11]]\n    expected_cov_matrix1 = [[2.0, 4.0], [4.0, 10.0]]\n    cov_matrix1 = calculate_covariance_matrix(vectors1)\n    print(\"Test case 1:\")\n    print(\"Expected:\", expected_cov_matrix1)\n    print(\"Actual:\", cov_matrix1)\n\n    # Test case 2: A example with three features\n    vectors2 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    expected_cov_matrix2 = [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n    cov_matrix2 = calculate_covariance_matrix(vectors2)\n    print(\"\\nTest case 2:\")\n    print(\"Expected:\", expected_cov_matrix2)\n    print(\"Actual:\", cov_matrix2)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n*   The function `calculate_covariance_matrix` takes a list of lists `vectors` as input, where each inner list represents a feature with its observations.\n*   It first converts the input list to a NumPy array for easier manipulation.\n*   Then, it calculates the mean of each feature and centers the data by subtracting the mean.\n*   The covariance matrix is calculated directly using `np.cov`, which is a more efficient and accurate approach than implementing the formula manually.\n*   Finally, the covariance matrix is converted back to a list of lists and returned.\n\nThe test cases verify the correctness of the implementation by comparing the expected and actual covariance matrices for different input vectors."}
{"task_id": 11, "completion_id": 0, "completion": "### Jacobi Method for Solving Linear Equations\n#### Overview\n\nThe Jacobi method is an iterative technique used to solve a system of linear equations of the form `Ax = b`. Here, `A` is a square matrix, `x` is the column vector of variables, and `b` is a column vector of constants.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi method.\n\n    Parameters:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A column vector representing the constants on the right-hand side of the equations.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x after n iterations.\n    \"\"\"\n\n    # Get the number of equations (which is also the number of variables)\n    num_equations = len(b)\n\n    # Initialize the solution vector with zeros\n    x = np.zeros(num_equations)\n\n    for _ in range(n):\n        # Initialize a new solution vector for this iteration\n        x_new = np.zeros(num_equations)\n\n        for i in range(num_equations):\n            # Calculate the sum of all terms except the diagonal one\n            s = sum(A[i, j] * x[j] for j in range(num_equations) if j != i)\n\n            # Update the solution vector\n            x_new[i] = (b[i] - s) / A[i, i]\n\n        # Round the solution to four decimal places and update x for the next iteration\n        x = np.round(x_new, 4)\n\n    # Return the solution as a list, rounded to four decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[3, 1], [1, 2]], dtype=float)\n    b = np.array([9, 8], dtype=float)\n    n = 5  # Number of iterations\n\n    x_approx = solve_jacobi(A, b, n)\n    print(\"Approximate solution:\", x_approx)\n```\n\n#### Explanation\n\n*   The function `solve_jacobi` takes as input a square matrix `A`, a column vector `b`, and the number of iterations `n`.\n*   It initializes a solution vector `x` with zeros and iterates `n` times to refine the solution.\n*   In each iteration, it computes a new estimate of the solution vector `x_new` based on the previous estimate `x`.\n*   The solution vector `x` is updated at the end of each iteration, and its elements are rounded to four decimal places.\n*   After `n` iterations, the function returns the approximate solution `x` as a list, rounded to four decimal places.\n\n#### Advice\n\n*   Ensure that the matrix `A` is diagonally dominant for the Jacobi method to converge.\n*   You can adjust the number of iterations `n` based on your specific requirements or the desired level of accuracy.\n*   The example usage demonstrates how to call the `solve_jacobi` function with a sample matrix `A` and vector `b`. You can modify these inputs to solve your specific system of linear equations."}
{"task_id": 12, "completion_id": 0, "completion": "## Singular Value Decomposition (SVD) of a 2x2 Matrix using Jacobian Method\n\n### Description\n\nThe following Python function approximates the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method. The SVD decomposition of a matrix A is given by A = U \u03a3 V^T, where U and V are orthogonal matrices and \u03a3 is a diagonal matrix containing the singular values of A.\n\n### Code\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the U, \u03a3, and V matrices.\n    \"\"\"\n\n    # Ensure A is a 2x2 numpy array\n    assert A.shape == (2, 2), \"Input matrix must be 2x2\"\n\n    # Compute A^T A\n    A_T_A = np.dot(A.T, A)\n\n    # Compute the eigenvalues and eigenvectors of A^T A\n    eigenvalues, eigenvectors = np.linalg.eig(A_T_A)\n\n    # Sort the eigenvalues and eigenvectors in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Compute the singular values\n    singular_values = np.sqrt(eigenvalues)\n\n    # Compute the U matrix\n    U = np.dot(A, eigenvectors) / singular_values\n\n    # Normalize the columns of U to ensure it's an orthogonal matrix\n    U[:, 0] = U[:, 0] / np.linalg.norm(U[:, 0])\n    U[:, 1] = U[:, 1] / np.linalg.norm(U[:, 1])\n\n    # Compute \u03a3\n    \u03a3 = np.diag(singular_values)\n\n    # Compute V\n    V = eigenvectors.T\n\n    # Round results to the nearest 4th decimal\n    U = np.round(U, 4)\n    \u03a3 = np.round(\u03a3, 4)\n    V = np.round(V, 4)\n\n    return U, \u03a3, V\n\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]])\n    U, \u03a3, V = svd_2x2_singular_values(A)\n    print(\"U:\")\n    print(U)\n    print(\"\\n\u03a3:\")\n    print(\u03a3)\n    print(\"\\nV:\")\n    print(V)\n```\n\n### Explanation\n\nThe provided code computes the SVD of a 2x2 matrix `A` using the Jacobian method. Here's a step-by-step breakdown:\n\n1.  Compute `A^T A`, where `A^T` is the transpose of `A`.\n2.  Calculate the eigenvalues and eigenvectors of `A^T A`.\n3.  Sort the eigenvalues and eigenvectors in descending order.\n4.  Compute the singular values as the square root of the eigenvalues.\n5.  Calculate the `U` matrix by multiplying `A` with the eigenvectors and dividing by the singular values.\n6.  Normalize the columns of `U` to ensure it's an orthogonal matrix.\n7.  Compute the `\u03a3` matrix as a diagonal matrix containing the singular values.\n8.  Compute the `V` matrix as the transpose of the eigenvectors.\n\nThe results are rounded to the nearest 4th decimal place and returned as a tuple containing the `U`, `\u03a3`, and `V` matrices.\n\nYou can test the function using the example usage provided in the code."}
{"task_id": 13, "completion_id": 0, "completion": "## Determinant of a 4x4 Matrix using Laplace's Expansion\n\n### Overview\n\nThe following Python function calculates the determinant of a 4x4 matrix using Laplace's Expansion method. This method involves expanding the determinant along a row or column, and it is implemented recursively to handle the computation of determinants for the 3x3 minor matrices.\n\n### Code\n\n```python\nimport numpy as np\n\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n\n    Args:\n    matrix: A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n\n    Raises:\n    ValueError: If the input matrix is not a 4x4 matrix.\n    \"\"\"\n\n    # Check if the input matrix is a 4x4 matrix\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input matrix must be a 4x4 matrix\")\n\n    # Base case: 1x1 matrix\n    if len(matrix) == 1:\n        return matrix[0][0]\n\n    # Calculate the determinant using Laplace's Expansion along the first row\n    det = 0\n    for i in range(len(matrix)):\n        # Create a 3x3 minor matrix\n        minor = [row[:i] + row[i+1:] for row in matrix[1:]]\n        # Recursively calculate the determinant of the 3x3 minor matrix\n        det_minor = determinant_3x3(minor)\n        # Update the determinant using Laplace's Expansion formula\n        det += ((-1) ** i) * matrix[0][i] * det_minor\n\n    return det\n\n\ndef determinant_3x3(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 3x3 matrix using Laplace's Expansion.\n\n    Args:\n    matrix: A 3x3 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n\n    Raises:\n    ValueError: If the input matrix is not a 3x3 matrix.\n    \"\"\"\n\n    # Check if the input matrix is a 3x3 matrix\n    if len(matrix) != 3 or any(len(row) != 3 for row in matrix):\n        raise ValueError(\"Input matrix must be a 3x3 matrix\")\n\n    # Calculate the determinant using Laplace's Expansion along the first row\n    det = 0\n    for i in range(len(matrix)):\n        # Create a 2x2 minor matrix\n        minor = [row[:i] + row[i+1:] for row in matrix[1:]]\n        # Calculate the determinant of the 2x2 minor matrix\n        det_minor = determinant_2x2(minor)\n        # Update the determinant using Laplace's Expansion formula\n        det += ((-1) ** i) * matrix[0][i] * det_minor\n\n    return det\n\n\ndef determinant_2x2(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 2x2 matrix.\n\n    Args:\n    matrix: A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n\n    Raises:\n    ValueError: If the input matrix is not a 2x2 matrix.\n    \"\"\"\n\n    # Check if the input matrix is a 2x2 matrix\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Calculate the determinant\n    return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n\n    det = determinant_4x4(matrix)\n    print(f\"Determinant of the 4x4 matrix: {det}\")\n\n    # Compare with NumPy's implementation\n    np_matrix = np.array(matrix)\n    np_det = np.linalg.det(np_matrix)\n    print(f\"NumPy's determinant: {np_det}\")\n```\n\n### Explanation\n\nThe provided code defines a recursive function `determinant_4x4` to calculate the determinant of a 4x4 matrix using Laplace's Expansion. It first checks if the input matrix is a 4x4 matrix and raises a `ValueError` if not.\n\nThe function then calculates the determinant by expanding along the first row. For each element in the first row, it creates a 3x3 minor matrix, recursively calculates its determinant using the `determinant_3x3` function, and updates the determinant using Laplace's Expansion formula.\n\nThe `determinant_3x3` function works similarly, expanding along the first row and creating 2x2 minor matrices. The `determinant_2x2` function directly calculates the determinant of a 2x2 matrix.\n\nIn the example usage section, we demonstrate how to use the `determinant_4x4` function with a sample 4x4 matrix and compare the result with NumPy's implementation."}
{"task_id": 14, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing linear regression using the normal equation. The goal is to find the coefficients of a linear regression model given a matrix X (features) and a vector y (target).\n\nKey constraints and edge cases:\n\n* The input matrix X is a list of lists of floats, where each inner list represents a data point with its features.\n* The input vector y is a list of floats, representing the target values for each data point.\n* The output should be a list of floats representing the coefficients of the linear regression model.\n* All results should be rounded to the nearest 4th decimal place.\n* The input matrix X might not have a full rank, requiring the use of a pseudo-inverse.\n\nInput: A matrix X (features) and a vector y (target)\nOutput: A list of floats representing the coefficients of the linear regression model\n\nSubtle requirements or implicit constraints:\n\n* The problem assumes that the input matrix X has a shape of (m, n), where m is the number of data points and n is the number of features.\n* The problem assumes that the input vector y has a shape of (m,), where m is the number of data points.\n* The problem requires handling the case where the input matrix X does not have a full rank.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The normal equation for linear regression is given by: \u03b8 = (X^T X)^-1 X^T y, where \u03b8 represents the coefficients of the linear regression model.\n* The problem can be solved using the Moore-Penrose pseudo-inverse to handle the case where the input matrix X does not have a full rank.\n* The numpy library provides an efficient way to perform matrix operations, including calculating the pseudo-inverse.\n\nPatterns or properties being used:\n\n* The problem uses the properties of linear algebra, specifically the Moore-Penrose pseudo-inverse.\n* The problem uses the numpy library to perform efficient matrix operations.\n\nKey algorithmic insights:\n\n* The problem can be solved by first converting the input matrix X and vector y to numpy arrays.\n* The problem can be solved by then calculating the pseudo-inverse of X using the numpy library.\n* The problem can be solved by finally multiplying the pseudo-inverse of X with y to obtain the coefficients of the linear regression model.\n\nImportant realizations:\n\n* The problem requires handling the case where the input matrix X does not have a full rank.\n* The problem requires rounding the results to the nearest 4th decimal place.\n\nCritical examples or test cases:\n\n* The example given in the problem statement, where X = [[1, 2], [3, 4]] and y = [5, 6], helps to illustrate the problem and the solution.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Convert the input matrix X and vector y to numpy arrays.\n2. Calculate the pseudo-inverse of X using the numpy library.\n3. Multiply the pseudo-inverse of X with y to obtain the coefficients of the linear regression model.\n4. Round the results to the nearest 4th decimal place.\n\nCritical details:\n\n* Use the numpy library to perform efficient matrix operations.\n* Handle the case where the input matrix X does not have a full rank by using the Moore-Penrose pseudo-inverse.\n* Round the results to the nearest 4th decimal place.\n\nOptimizations or special cases to handle:\n\n* Use the numpy library to perform efficient matrix operations.\n* Handle the case where the input matrix X does not have a full rank by using the Moore-Penrose pseudo-inverse.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert lists to numpy arrays for easier matrix operations\n    X_array = np.array(X)\n    y_array = np.array(y)\n\n    # Calculate the pseudo-inverse of X\n    X_pseudo_inverse = np.linalg.pinv(X_array)\n\n    # Calculate the coefficients using the normal equation\n    theta = np.dot(X_pseudo_inverse, y_array)\n\n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n\n    # Return the coefficients as a list\n    return theta_rounded.tolist()\n```"}
{"task_id": 15, "completion_id": 0, "completion": "### Linear Regression Using Gradient Descent\n\nHere's a Python function that performs linear regression using gradient descent.\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    list: Coefficients of the linear regression model.\n    \"\"\"\n\n    # Initialize coefficients\n    theta = np.zeros(X.shape[1])\n\n    # Gradient Descent\n    for _ in range(iterations):\n        # Compute predictions\n        predictions = np.dot(X, theta)\n\n        # Compute errors\n        errors = predictions - y\n\n        # Compute gradient\n        gradient = (2 / len(y)) * np.dot(X.T, errors)\n\n        # Update coefficients\n        theta = theta - alpha * gradient\n\n    # Round coefficients to four decimal places and return as a list\n    return np.round(theta, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.hstack((np.ones((100, 1)), np.random.rand(100, 1)))\n    y = 3 + 2 * X[:, 1] + np.random.randn(100)\n\n    # Perform linear regression using gradient descent\n    alpha = 0.01\n    iterations = 1000\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```\n\nIn this code:\n\n*   We first initialize the coefficients (`theta`) to zeros.\n*   We then perform gradient descent for a specified number of iterations.\n*   In each iteration, we compute predictions, errors, and the gradient of the cost function with respect to the coefficients.\n*   We update the coefficients based on the gradient and the learning rate.\n*   Finally, we round the coefficients to four decimal places and return them as a list.\n\nThe example usage demonstrates how to use the `linear_regression_gradient_descent` function with some randomly generated data. You can modify the `alpha`, `iterations`, `X`, and `y` variables to suit your specific needs."}
{"task_id": 16, "completion_id": 0, "completion": "### Feature Scaling Implementation\n\nHere's a Python function that performs feature scaling on a dataset using both standardization and min-max normalization.\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n    tuple[list[list[float]], list[list[float]]]: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n\n    # Calculate the mean and standard deviation for standardization\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n\n    # Standardization: (x - mean) / standard deviation\n    standardized_data = (data - mean) / std_dev\n\n    # Calculate the minimum and maximum values for min-max normalization\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n\n    # Min-max normalization: (x - min) / (max - min)\n    normalized_data = (data - min_values) / (max_values - min_values)\n\n    # Round results to the nearest 4th decimal and convert to lists\n    standardized_scaled_data = np.round(standardized_data, 4).tolist()\n    normalized_scaled_data = np.round(normalized_data, 4).tolist()\n\n    return standardized_scaled_data, normalized_scaled_data\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    data = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n\n    standardized, normalized = feature_scaling(data)\n    print(\"Standardized Data:\")\n    for row in standardized:\n        print(row)\n\n    print(\"\\nNormalized Data:\")\n    for row in normalized:\n        print(row)\n```\n\nIn this code:\n\n*   We first calculate the mean and standard deviation for standardization and the minimum and maximum values for min-max normalization.\n*   We then apply these scaling techniques to the input data.\n*   The results are rounded to the nearest 4th decimal using `np.round`.\n*   Finally, we convert the scaled data to lists using the `tolist` method and return them.\n\nThe example usage demonstrates how to use the `feature_scaling` function with a sample dataset."}
{"task_id": 17, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing the k-Means clustering algorithm. The goal is to group similar points together and represent each group by its center, called the centroid.\n\nKey constraints and edge cases:\n\n* The input points are a list of tuples, where each tuple represents the coordinates of a point.\n* The number of clusters to form is given by the integer k.\n* The initial centroids are given as a list of tuples, each representing the coordinates of a centroid.\n* The maximum number of iterations to perform is given by the integer max_iterations.\n* The output should be a list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n* The input points can be empty.\n* The number of clusters k can be 0.\n\nInput:\n\n* points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n* k: An integer representing the number of clusters to form\n* initial_centroids: A list of initial centroid points, each a tuple of coordinates\n* max_iterations: An integer representing the maximum number of iterations to perform\n\nOutput:\n\n* A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n\nSubtle requirements or implicit constraints:\n\n* The distance between a point and a centroid is calculated using the Euclidean distance.\n* The centroid of a cluster is calculated as the mean of all points assigned to that cluster.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The k-Means clustering algorithm works by iteratively assigning each point to the closest centroid and then updating the centroids based on the assigned points.\n* The distance between a point and a centroid can be calculated using the Euclidean distance.\n* The centroid of a cluster can be calculated as the mean of all points assigned to that cluster.\n* The algorithm should handle empty clusters by not updating the centroid of that cluster.\n* The algorithm should handle the case where the number of clusters k is 0 by returning an empty list.\n\nPatterns or properties being used:\n\n* The Euclidean distance is used to calculate the distance between a point and a centroid.\n* The mean of all points assigned to a cluster is used to calculate the centroid of that cluster.\n\nKey algorithmic insights:\n\n* The algorithm iteratively assigns each point to the closest centroid and then updates the centroids based on the assigned points.\n* The algorithm handles empty clusters by not updating the centroid of that cluster.\n\nImportant realizations:\n\n* The algorithm should handle the case where the input points are empty.\n* The algorithm should handle the case where the number of clusters k is 0.\n\nCritical examples or test cases:\n\n* The example with points = [(1, 1), (2, 2), (3, 3), (4, 4)], k = 2, initial_centroids = [(1, 1), (4, 4)], and max_iterations = 100.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Initialize the centroids with the given initial_centroids.\n2. Iterate over the points and assign each point to the closest centroid.\n3. Update the centroids based on the assigned points.\n4. Repeat steps 2 and 3 until the centroids converge or the maximum number of iterations is reached.\n\nCritical details:\n\n* Use the Euclidean distance to calculate the distance between a point and a centroid.\n* Handle empty clusters by not updating the centroid of that cluster.\n* Handle the case where the number of clusters k is 0 by returning an empty list.\n\nOptimizations or special cases to handle:\n\n* Handle the case where the input points are empty.\n* Handle the case where the number of clusters k is 0.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Performs k-Means clustering on a given set of points.\n\n    Args:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    k (int): The number of clusters to form.\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates.\n    max_iterations (int): The maximum number of iterations to perform.\n\n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n\n    # Convert points and initial_centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    initial_centroids = np.array(initial_centroids)\n\n    # Initialize centroids\n    centroids = initial_centroids\n\n    # Iterate until convergence or max iterations\n    for _ in range(max_iterations):\n        # Assign each point to the closest centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # Calculate new centroids\n        new_centroids = np.array([points[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)])\n\n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n\n        centroids = new_centroids\n\n    # Round centroids to the nearest fourth decimal and return as a list of tuples\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]\n```"}
{"task_id": 18, "completion_id": 0, "completion": "### K-Fold Cross-Validation Implementation\n\nHere's a Python function that generates train and test splits for K-Fold Cross-Validation:\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    - X (np.ndarray): The feature dataset.\n    - y (np.ndarray): The target variable.\n    - k (int, optional): The number of folds. Defaults to 5.\n    - shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    - random_seed (int, optional): The random seed for shuffling. Defaults to None.\n\n    Returns:\n    - list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n\n    # Check if the input arrays have the same length\n    if len(X) != len(y):\n        raise ValueError(\"The feature dataset and target variable must have the same length.\")\n\n    # Set the random seed for reproducibility\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Get the total number of samples\n    n_samples = len(X)\n\n    # Generate the indices for the dataset\n    indices = np.arange(n_samples)\n\n    # Shuffle the indices if required\n    if shuffle:\n        np.random.shuffle(indices)\n\n    # Initialize the list to store the train-test indices for each fold\n    folds = []\n\n    # Calculate the size of each fold\n    fold_size = n_samples // k\n    remainder = n_samples % k\n\n    # Initialize the start index\n    start = 0\n\n    # Generate the train-test indices for each fold\n    for i in range(k):\n        # Calculate the end index for the current fold\n        end = start + fold_size + (1 if i < remainder else 0)\n\n        # Get the test indices for the current fold\n        test_indices = indices[start:end]\n\n        # Get the train indices for the current fold\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n\n        # Append the train-test indices for the current fold to the list\n        folds.append((train_indices, test_indices))\n\n        # Update the start index for the next fold\n        start = end\n\n    return folds\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate a sample dataset\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100)\n\n    # Perform K-Fold Cross-Validation\n    k = 5\n    folds = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=42)\n\n    # Print the train-test indices for each fold\n    for i, (train_indices, test_indices) in enumerate(folds):\n        print(f\"Fold {i+1}:\")\n        print(f\"Train indices: {train_indices}\")\n        print(f\"Test indices: {test_indices}\")\n        print(f\"Train size: {len(train_indices)}\")\n        print(f\"Test size: {len(test_indices)}\")\n        print(\"-\" * 50)\n```\n\nIn this code:\n\n*   We define a function `k_fold_cross_validation` that takes in the feature dataset `X`, the target variable `y`, the number of folds `k`, a boolean indicating whether to shuffle the data, and an optional random seed.\n*   We check if the input arrays have the same length and raise a `ValueError` if they don't.\n*   We set the random seed for reproducibility if provided.\n*   We generate the indices for the dataset, shuffle them if required, and then split them into `k` folds.\n*   For each fold, we calculate the train and test indices and append them to the `folds` list.\n*   Finally, we return the list of train-test indices for each fold.\n\nThe example usage demonstrates how to perform K-Fold Cross-Validation on a sample dataset and print the train-test indices for each fold."}
{"task_id": 19, "completion_id": 0, "completion": "### Principal Component Analysis (PCA) Implementation\n#### Function Description\n\nThe `pca` function performs Principal Component Analysis (PCA) from scratch. It takes a 2D NumPy array `data` as input, where each row represents a data sample and each column represents a feature. The function standardizes the dataset, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the principal components (the eigenvectors corresponding to the largest eigenvalues). The function also takes an integer `k` as input, representing the number of principal components to return.\n\n#### Code Implementation\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) from scratch.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - list[list[float]]: A list of the k principal components, each represented as a list of floats.\n    \"\"\"\n\n    # Step 1: Standardize the dataset\n    # Calculate the mean of each feature\n    mean = np.mean(data, axis=0)\n    \n    # Subtract the mean from each feature to center the data\n    standardized_data = data - mean\n    \n    # Calculate the standard deviation of each feature\n    std_dev = np.std(standardized_data, axis=0)\n    \n    # Divide by the standard deviation to scale the data\n    standardized_data = standardized_data / std_dev\n\n    # Step 2: Compute the covariance matrix\n    # Calculate the covariance matrix using the standardized data\n    cov_matrix = np.cov(standardized_data.T)\n\n    # Step 3: Find the eigenvalues and eigenvectors\n    # Compute the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n    # Step 4: Sort the eigenvalues and eigenvectors in descending order\n    # Sort the eigenvalues and eigenvectors based on the eigenvalues\n    sorted_indices = np.argsort(-eigenvalues)\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the k principal components\n    # Select the k eigenvectors corresponding to the largest eigenvalues\n    principal_components = eigenvectors[:, :k].tolist()\n\n    # Round the results to the nearest 4th decimal\n    principal_components = [[round(val, 4) for val in component] for component in principal_components]\n\n    return principal_components\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate a random dataset\n    np.random.seed(0)\n    data = np.random.rand(100, 5)\n\n    # Perform PCA\n    k = 3\n    principal_components = pca(data, k)\n\n    # Print the principal components\n    for i, component in enumerate(principal_components):\n        print(f\"Principal Component {i+1}: {component}\")\n```\n\n#### Explanation\n\nThe provided Python function `pca` performs Principal Component Analysis (PCA) from scratch. It takes a 2D NumPy array `data` and an integer `k` as input and returns the k principal components.\n\nHere's a step-by-step breakdown of the implementation:\n\n1.  **Standardize the dataset**: The function standardizes the input data by subtracting the mean and dividing by the standard deviation for each feature. This step ensures that all features have zero mean and unit variance.\n2.  **Compute the covariance matrix**: The function calculates the covariance matrix of the standardized data using the `np.cov` function.\n3.  **Find the eigenvalues and eigenvectors**: The function computes the eigenvalues and eigenvectors of the covariance matrix using the `np.linalg.eig` function.\n4.  **Sort the eigenvalues and eigenvectors**: The function sorts the eigenvalues and eigenvectors in descending order based on the eigenvalues.\n5.  **Select the k principal components**: The function selects the k eigenvectors corresponding to the largest eigenvalues, which represent the principal components.\n6.  **Round the results**: The function rounds the principal components to the nearest 4th decimal.\n\nThe example usage demonstrates how to perform PCA on a random dataset and print the resulting principal components."}
{"task_id": 20, "completion_id": 0, "completion": "**Decision Tree Learning Implementation**\n======================================\n\n### Overview\n\nThe following Python function implements the decision tree learning algorithm for classification using recursive binary splitting based on entropy and information gain.\n\n### Code\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree from a list of examples.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dict of attribute-value pairs.\n    - attributes (list[str]): A list of attribute names.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n\n    # Base case: If there are no examples, return None\n    if not examples:\n        return None\n\n    # Base case: If all examples have the same target value, return a leaf node\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return {target_attr: target_values[0]}\n\n    # Base case: If there are no more attributes to consider, return a leaf node with the most common target value\n    if not attributes:\n        most_common_target = Counter(target_values).most_common(1)[0][0]\n        return {target_attr: most_common_target}\n\n    # Select the attribute with the highest information gain\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n\n    # Create a decision tree node with the best attribute\n    tree = {best_attr: {}}\n\n    # Split the examples based on the best attribute\n    attr_values = set(example[best_attr] for example in examples)\n    for value in attr_values:\n        # Get the examples with the current attribute value\n        value_examples = [example for example in examples if example[best_attr] == value]\n\n        # Recursively learn a decision tree for the current attribute value\n        if len(value_examples) == 0:\n            # If there are no examples with the current attribute value, use the most common target value\n            most_common_target = Counter(target_values).most_common(1)[0][0]\n            tree[best_attr][value] = {target_attr: most_common_target}\n        else:\n            # Remove the best attribute from the list of attributes\n            remaining_attributes = [attr for attr in attributes if attr != best_attr]\n\n            # Recursively learn a decision tree for the current attribute value\n            subtree = learn_decision_tree(value_examples, remaining_attributes, target_attr)\n            tree[best_attr][value] = subtree\n\n    return tree\n\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculate the entropy of a list of examples.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dict of attribute-value pairs.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - The entropy of the list of examples.\n    \"\"\"\n\n    # Get the target values\n    target_values = [example[target_attr] for example in examples]\n\n    # Calculate the probability of each target value\n    probabilities = [target_values.count(value) / len(target_values) for value in set(target_values)]\n\n    # Calculate the entropy\n    entropy = -sum(prob * math.log2(prob) for prob in probabilities)\n\n    return entropy\n\n\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain of an attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dict of attribute-value pairs.\n    - attr (str): The name of the attribute.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - The information gain of the attribute.\n    \"\"\"\n\n    # Calculate the entropy of the list of examples\n    total_entropy = entropy(examples, target_attr)\n\n    # Split the examples based on the attribute\n    attr_values = set(example[attr] for example in examples)\n    attr_entropy = 0\n    for value in attr_values:\n        # Get the examples with the current attribute value\n        value_examples = [example for example in examples if example[attr] == value]\n\n        # Calculate the probability of the current attribute value\n        prob = len(value_examples) / len(examples)\n\n        # Calculate the entropy of the current attribute value\n        value_entropy = entropy(value_examples, target_attr)\n\n        # Update the attribute entropy\n        attr_entropy += prob * value_entropy\n\n    # Calculate the information gain\n    gain = total_entropy - attr_entropy\n\n    return gain\n\n\n# Example usage\nif __name__ == \"__main__\":\n    examples = [\n        {\"color\": \"red\", \"size\": \"big\", \"label\": \"apple\"},\n        {\"color\": \"green\", \"size\": \"small\", \"label\": \"banana\"},\n        {\"color\": \"red\", \"size\": \"small\", \"label\": \"apple\"},\n        {\"color\": \"green\", \"size\": \"big\", \"label\": \"banana\"},\n        {\"color\": \"red\", \"size\": \"big\", \"label\": \"apple\"},\n    ]\n    attributes = [\"color\", \"size\"]\n    target_attr = \"label\"\n\n    tree = learn_decision_tree(examples, attributes, target_attr)\n    print(tree)\n```\n\n### Explanation\n\nThe provided code implements a decision tree learning algorithm using recursive binary splitting based on entropy and information gain. The `learn_decision_tree` function takes a list of examples, a list of attribute names, and a target attribute name as input and returns a nested dictionary representing the decision tree.\n\nThe algorithm works as follows:\n\n1.  **Base cases**: The function handles three base cases:\n    *   If there are no examples, it returns `None`.\n    *   If all examples have the same target value, it returns a leaf node with that target value.\n    *   If there are no more attributes to consider, it returns a leaf node with the most common target value.\n2.  **Attribute selection**: The function selects the attribute with the highest information gain using the `information_gain` function.\n3.  **Tree construction**: The function creates a decision tree node with the selected attribute and recursively learns decision trees for each attribute value.\n4.  **Entropy and information gain calculation**: The `entropy` function calculates the entropy of a list of examples, and the `information_gain` function calculates the information gain of an attribute.\n\nThe example usage demonstrates how to use the `learn_decision_tree` function to learn a decision tree from a list of examples."}
{"task_id": 21, "completion_id": 0, "completion": "### Pegasos Kernel SVM Implementation\n\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Parameters:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str): The choice of kernel. Defaults to 'linear'. Options: 'linear', 'rbf'.\n    - lambda_val (float): The regularization parameter. Defaults to 0.01.\n    - iterations (int): The number of iterations. Defaults to 100.\n    - sigma (float): The standard deviation for the RBF kernel. Defaults to 1.0.\n\n    Returns:\n    - alpha (list): The model's alpha coefficients.\n    - b (float): The model's bias.\n    \"\"\"\n\n    # Get the number of samples\n    n_samples = data.shape[0]\n\n    # Initialize alpha coefficients and bias\n    alpha = np.zeros(n_samples)\n    b = 0.0\n\n    # Define the kernel functions\n    def linear_kernel(x, y):\n        return np.dot(x, y)\n\n    def rbf_kernel(x, y):\n        return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n\n    # Choose the kernel function\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = rbf_kernel\n    else:\n        raise ValueError(\"Invalid kernel. Options: 'linear', 'rbf'.\")\n\n    # Pegasos iterations\n    for _ in range(iterations):\n        # Compute the prediction for all samples\n        prediction = np.zeros(n_samples)\n        for i in range(n_samples):\n            for j in range(n_samples):\n                prediction[i] += alpha[j] * labels[j] * kernel_func(data[i], data[j])\n            prediction[i] += b\n\n        # Update alpha coefficients and bias\n        for i in range(n_samples):\n            if labels[i] * prediction[i] < 1:\n                alpha[i] += 1 - lambda_val * alpha[i]\n            else:\n                alpha[i] *= (1 - lambda_val)\n\n        # Update bias\n        b = np.mean(labels - prediction)\n\n    # Round results to the nearest 4th decimal and return as list\n    alpha = np.round(alpha, 4).tolist()\n    b = round(b, 4)\n\n    return alpha, b\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some sample data\n    np.random.seed(0)\n    data = np.random.rand(10, 2)\n    labels = np.array([1, 1, 1, -1, -1, -1, 1, 1, -1, -1])\n\n    # Train the model\n    alpha, b = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\n\n    print(\"Alpha coefficients:\", alpha)\n    print(\"Bias:\", b)\n```\n\nIn this code:\n\n*   We define a function `pegasos_kernel_svm` that takes in the dataset, labels, kernel type, regularization parameter, number of iterations, and standard deviation for the RBF kernel.\n*   We implement the Pegasos algorithm with a kernel function, which can be either linear or RBF.\n*   We iterate through the dataset, compute predictions, and update the alpha coefficients and bias according to the Pegasos update rules.\n*   Finally, we round the results to the nearest 4th decimal and return the alpha coefficients and bias as a list.\n\nThe example usage demonstrates how to train the model with some sample data and print the resulting alpha coefficients and bias."}
{"task_id": 22, "completion_id": 0, "completion": "## Sigmoid Activation Function Implementation\n\nThe sigmoid activation function is a common mathematical function used in machine learning models, particularly in the context of logistic regression and neural networks. It maps any real-valued number to a value between 0 and 1.\n\n### Code Implementation\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    # Calculate the sigmoid of z using the formula: 1 / (1 + exp(-z))\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    \n    # Return the output rounded to four decimal places\n    return round(sigmoid_output, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    z_values = [0.5, -1.0, 2.0]\n    for z in z_values:\n        print(f\"Sigmoid of {z}: {sigmoid(z)}\")\n```\n\n### Explanation\n\n*   The `sigmoid` function takes a float `z` as input and returns the sigmoid of `z` as a float.\n*   The sigmoid function is implemented using the formula `1 / (1 + exp(-z))`, which is numerically stable for both positive and negative input values.\n*   The `math.exp` function is used to compute the exponential of `-z`.\n*   The result is rounded to four decimal places using the `round` function.\n*   In the example usage section, we demonstrate how to use the `sigmoid` function with different input values."}
{"task_id": 23, "completion_id": 0, "completion": "## Softmax Activation Function Implementation\n### Description\nThe softmax function is a common activation function used in machine learning models, particularly in the output layer of classification models. It takes a list of scores as input and returns a list of values in the range (0, 1) that add up to 1.\n\n### Implementation\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n    scores (list[float]): A list of scores.\n\n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n\n    # Calculate the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Calculate the sum of the exponential scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Calculate the softmax values\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    \n    # Round the softmax values to four decimal places\n    softmax_values = [round(value, 4) for value in softmax_values]\n    \n    return softmax_values\n\n# Example usage\nif __name__ == \"__main__\":\n    scores = [2.0, 1.0, 0.5]\n    print(softmax(scores))\n```\n\n### Explanation\n\n1. The function `softmax` takes a list of scores as input and calculates the exponential of each score using a list comprehension.\n2. It then calculates the sum of these exponential scores.\n3. The softmax values are calculated by dividing each exponential score by the sum of the exponential scores.\n4. Finally, the softmax values are rounded to four decimal places using another list comprehension.\n5. The function returns the list of softmax values.\n\n### Note\n\nThis implementation assumes that the input list `scores` is not empty. If there's a possibility that the input list could be empty, you should add error checking code to handle this case. For example:\n\n```python\nif not scores:\n    raise ValueError(\"Input list cannot be empty\")\n```"}
{"task_id": 24, "completion_id": 0, "completion": "### Single Neuron Model with Sigmoid Activation Function\n\nHere's a Python function that simulates a single neuron with a sigmoid activation function for binary classification. This function handles multidimensional input features and calculates the predicted probabilities and the mean squared error.\n\n```python\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    - features (list[list[float]]): A list of feature vectors, each representing multiple features for an example.\n    - labels (list[int]): A list of true binary labels associated with the feature vectors.\n    - weights (list[float]): A list of weights, one for each feature.\n    - bias (float): The bias of the neuron.\n\n    Returns:\n    - predicted_probabilities (list[float]): A list of predicted probabilities after sigmoid activation, rounded to four decimal places.\n    - mean_squared_error (float): The mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n\n    # Convert input lists to numpy arrays for easier computation\n    features = np.array(features)\n    labels = np.array(labels)\n    weights = np.array(weights)\n\n    # Calculate the weighted sum of features and weights, adding the bias\n    weighted_sum = np.dot(features, weights) + bias\n\n    # Apply the sigmoid activation function\n    predicted_probabilities = 1 / (1 + np.exp(-weighted_sum))\n\n    # Reshape predicted probabilities to a list, rounded to four decimal places\n    predicted_probabilities = (predicted_probabilities).tolist()\n\n    # Calculate the mean squared error between predicted probabilities and true labels\n    mean_squared_error = np.mean((predicted_probabilities - labels) ** 2)\n\n    # Round the mean squared error to four decimal places\n    mean_squared_error = round(mean_squared_error, 4)\n\n    return predicted_probabilities, mean_squared_error\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    features = [[1, 2], [3, 4], [5, 6]]\n    labels = [0, 1, 0]\n    weights = [0.5, 0.3]\n    bias = 0.2\n\n    predicted_probabilities, mean_squared_error = single_neuron_model(features, labels, weights, bias)\n    print(\"Predicted Probabilities:\", predicted_probabilities)\n    print(\"Mean Squared Error:\", mean_squared_error)\n```\n\nIn this code:\n\n*   We define a function `single_neuron_model` that takes a list of feature vectors, associated true binary labels, the neuron's weights, and bias as input.\n*   We use numpy arrays for easier computation and calculate the weighted sum of features and weights, adding the bias.\n*   We apply the sigmoid activation function to obtain predicted probabilities.\n*   We reshape the predicted probabilities to a list, rounded to four decimal places.\n*   We calculate the mean squared error between predicted probabilities and true labels, rounded to four decimal places.\n*   Finally, we provide an example usage of the function in the `if __name__ == \"__main__\":` block."}
{"task_id": 25, "completion_id": 0, "completion": "### Single Neuron with Backpropagation\n\n```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"The sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    - features (np.ndarray): A 2D array of feature vectors.\n    - labels (np.ndarray): A 1D array of true binary labels.\n    - initial_weights (np.ndarray): A 1D array of initial weights.\n    - initial_bias (float): The initial bias.\n    - learning_rate (float): The learning rate for gradient descent.\n    - epochs (int): The number of epochs.\n\n    Returns:\n    - updated_weights (np.ndarray): The updated weights.\n    - updated_bias (float): The updated bias.\n    - mse_values (list[float]): A list of MSE values for each epoch.\n    \"\"\"\n\n    # Initialize weights, bias, and list to store MSE values\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    for _ in range(epochs):\n        # Forward pass\n        output = sigmoid(np.dot(features, weights) + bias)\n        \n        # Calculate MSE loss\n        mse = np.mean((output - labels) ** 2)\n        mse_values.append(round(mse, 4))\n\n        # Backward pass\n        d_output = 2 * (output - labels) * output * (1 - output)\n        d_weights = np.dot(features.T, d_output)\n        d_bias = np.sum(d_output)\n\n        # Update weights and bias using gradient descent\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    # Round and return results\n    updated_weights = np.round(weights, 4)\n    updated_bias = round(bias, 4)\n    mse_values = [round(mse, 4) for mse in mse_values]\n\n    return updated_weights, updated_bias, mse_values\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(0)\n\n    # Generate random data\n    features = np.random.rand(100, 5)\n    labels = np.random.randint(2, size=100)\n\n    # Initialize neuron parameters\n    initial_weights = np.random.rand(5)\n    initial_bias = np.random.rand()\n    learning_rate = 0.01\n    epochs = 10\n\n    updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n\n    print(\"Updated Weights:\", updated_weights.tolist())\n    print(\"Updated Bias:\", updated_bias)\n    print(\"MSE Values:\", mse_values)\n```\n\nIn this code:\n\n*   We define a `sigmoid` function to compute the sigmoid activation.\n*   The `train_neuron` function simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n*   The function takes in feature vectors, true binary labels, initial weights, initial bias, a learning rate, and the number of epochs.\n*   It returns the updated weights, bias, and a list of MSE values for each epoch.\n*   In the example usage section, we demonstrate how to use the `train_neuron` function with randomly generated data."}
{"task_id": 26, "completion_id": 0, "completion": "## Implementing Basic Autograd Operations\n### Overview\n\nThe following Python class, `Value`, implements basic autograd operations for scalar values. It supports addition, multiplication, and ReLU activation, and computes gradients using automatic differentiation.\n\n### Code\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initialize a Value object.\n\n        Args:\n        - data (float): The value of the object.\n        - _children (tuple, optional): The children of the object. Defaults to ().\n        - _op (str, optional): The operation that created the object. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def _add(self, other):\n        \"\"\"\n        Add two Value objects.\n\n        Args:\n        - other (Value): The other Value object.\n\n        Returns:\n        - Value: A new Value object representing the sum.\n        \"\"\"\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n        self._backward = lambda: self._add_grad(other)\n        other._backward = lambda: other._add_grad(self)\n        out._backward = lambda: self._backward() or other._backward()\n        return out\n\n    def _add_grad(self, other):\n        \"\"\"\n        Compute the gradient for addition.\n\n        Args:\n        - other (Value): The other Value object.\n        \"\"\"\n        self.grad += 1\n        other.grad += 1\n\n    def _mul(self, other):\n        \"\"\"\n        Multiply two Value objects.\n\n        Args:\n        - other (Value): The other Value object.\n\n        Returns:\n        - Value: A new Value object representing the product.\n        \"\"\"\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n        self._backward = lambda: self._mul_grad(other)\n        other._backward = lambda: other._mul_grad(self)\n        out._backward = lambda: self._backward() or other._backward()\n        return out\n\n    def _mul_grad(self, other):\n        \"\"\"\n        Compute the gradient for multiplication.\n\n        Args:\n        - other (Value): The other Value object.\n        \"\"\"\n        self.grad += other.data\n        other.grad += self.data\n\n    def relu(self):\n        \"\"\"\n        Apply the ReLU activation function.\n\n        Returns:\n        - Value: A new Value object representing the ReLU output.\n        \"\"\"\n        out = Value(max(0, self.data), _children=(self,), _op='relu')\n        out._backward = lambda: self._relu_grad()\n        return out\n\n    def _relu_grad(self):\n        \"\"\"\n        Compute the gradient for ReLU.\n        \"\"\"\n        if self.data > 0:\n            self.grad += 1\n\n    def backward(self):\n        \"\"\"\n        Perform backward pass to compute gradients.\n        \"\"\"\n        self._backward()\n\n    def _backward(self):\n        pass\n\n\n# Example usage\nif __name__ == \"__main__\":\n    x = Value(2)\n    y = Value(3)\n    z = x * y + 2\n    z.grad = 0\n\n    # Compute gradients\n    z.backward()\n\n    print(\"x.data:\", x.data)\n    print(\"x.grad:\", x.grad)\n    print(\"y.data:\", y.data)\n    print(\"y.grad:\", y.grad)\n\n    # ReLU example\n    a = Value(-1)\n    b = a.relu()\n    b.backward()\n    print(\"a.data:\", a.data)\n    print(\"a.grad:\", a.grad)\n```\n\n### Explanation\n\nThe provided `Value` class implements basic autograd operations for scalar values. It supports addition, multiplication, and ReLU activation.\n\n*   The `__init__` method initializes a `Value` object with a given data value and optional children and operation.\n*   The `_add`, `_mul`, and `relu` methods create new `Value` objects representing the results of addition, multiplication, and ReLU activation, respectively. They also set up the backward pass for computing gradients.\n*   The `backward` method performs the backward pass to compute gradients.\n*   In the example usage, we create `Value` objects `x` and `y`, compute their product and add 2, and then perform the backward pass to compute gradients.\n\nThe code includes documentation and example usage. The `Value` class automatically computes gradients for the supported operations using the chain rule. The gradients are accumulated in the `grad` attribute of each `Value` object.\n\n### Autograd Operations\n\nThe implemented autograd operations include:\n\n*   **Addition**: `x + y`\n*   **Multiplication**: `x * y`\n*   **ReLU Activation**: `relu(x)`\n\nThese operations are demonstrated in the example usage section.\n\n### Gradients Computation\n\nThe gradients are computed using automatic differentiation. When a `Value` object is created, its gradient is initialized to 0. During the backward pass, the gradients are accumulated based on the operations performed.\n\nThe gradients computation is based on the following rules:\n\n*   For addition: `d(x + y)/dx = 1`, `d(x + y)/dy = 1`\n*   For multiplication: `d(x * y)/dx = y`, `d(x * y)/dy = x`\n*   For ReLU: `d(ReLU(x))/dx = 1` if `x > 0`, `0` otherwise\n\nThese rules are applied during the backward pass to compute the gradients for each `Value` object.\n\n### Example Usage\n\nThe example usage demonstrates how to create `Value` objects, perform operations, and compute gradients using the backward pass.\n\n```python\nx = Value(2)\ny = Value(3)\nz = x * y + 2\nz.backward()\n\nprint(\"x.data:\", x.data)\nprint(\"x.grad:\", x.grad)\nprint(\"y.data:\", y.data)\nprint(\"y.grad:\", y.grad)\n```"}
{"task_id": 27, "completion_id": 0, "completion": "## Transformation Matrix from Basis B to C\n\n### Problem Description\n\nGiven basis vectors in two different bases B and C for R^3, compute the transformation matrix P from basis B to C.\n\n### Solution\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n\n    Args:\n    B (list[list[int]]): Basis vectors in basis B.\n    C (list[list[int]]): Basis vectors in basis C.\n\n    Returns:\n    list[list[float]]: Transformation matrix P from basis B to C.\n    \"\"\"\n\n    # Convert input lists to NumPy arrays\n    B = np.array(B)\n    C = np.array(C)\n\n    # Compute the transformation matrix P by solving for P in the equation C = B * P\n    # This can be done by finding the inverse of B and multiplying it with C\n    P = np.linalg.inv(B) @ C\n\n    # Round the results to the nearest 4th decimal\n    P = np.round(P, 4)\n\n    # Return the result as a Python list\n    return P.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis\nC = [[1, 1, 0], [1, 0, 1], [0, 1, 1]]  # Another basis\n\nP = transform_basis(B, C)\nprint(\"Transformation Matrix P:\")\nfor row in P:\n    print(row)\n```\n\n### Explanation\n\nThe transformation matrix P from basis B to C can be computed by solving for P in the equation C = B \\* P. This can be done by finding the inverse of B and multiplying it with C.\n\nIn the provided Python function, we first convert the input lists to NumPy arrays. Then, we compute the transformation matrix P using NumPy's `linalg.inv` function to find the inverse of B and the `@` operator to perform matrix multiplication.\n\nFinally, we round the results to the nearest 4th decimal using `np.round` and return the result as a Python list using the `tolist` method.\n\nThe example usage demonstrates how to use the `transform_basis` function to compute the transformation matrix P from the standard basis to another basis."}
{"task_id": 28, "completion_id": 0, "completion": "## SVD of a 2x2 Matrix using Eigen Values and Vectors\n### Description\nThe following Python function computes the Singular Value Decomposition (SVD) of a given 2x2 matrix A, returning the matrices U, S, and V such that A = U * S * V.\n\n### Code\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V.\n    \"\"\"\n\n    # Ensure A is a numpy array\n    A = np.array(A)\n\n    # Check if A is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input matrix must be 2x2\"\n\n    # Compute A^T * A\n    A_T_A = np.dot(A.T, A)\n\n    # Compute the eigen values and vectors of A^T * A\n    eigen_values, eigen_vectors = np.linalg.eig(A_T_A)\n\n    # Sort the eigen values and vectors in descending order\n    idx = np.argsort(eigen_values)[::-1]\n    eigen_values = eigen_values[idx]\n    eigen_vectors = eigen_vectors[:, idx]\n\n    # Compute the singular values\n    singular_values = np.sqrt(eigen_values)\n\n    # Create the S matrix\n    S = np.diag(singular_values)\n\n    # Compute U\n    U = np.dot(A, eigen_vectors) / singular_values\n\n    # Ensure U is an orthonormal matrix\n    U[:, 1] = np.cross(U[:, 0], [0, 0, 1])[:2]\n\n    # Normalize U\n    U = U / np.linalg.norm(U, axis=0)\n\n    # Create V\n    V = eigen_vectors.T\n\n    # Round results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    # Reshape and convert to lists\n    U = U.tolist()\n    S = S.tolist()\n    V = V.tolist()\n\n    return U, S, V\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]])\n    U, S, V = svd_2x2(A)\n    print(\"Matrix U:\")\n    print(U)\n    print(\"Matrix S:\")\n    print(S)\n    print(\"Matrix V:\")\n    print(V)\n```\n\n### Explanation\n\nThe provided Python function `svd_2x2` calculates the SVD of a given 2x2 matrix `A`. It uses the method described in the provided blog post.\n\nHere's a step-by-step breakdown:\n\n1.  Compute `A^T * A`, where `A^T` is the transpose of matrix `A`.\n2.  Calculate the eigen values and vectors of `A^T * A`.\n3.  Sort the eigen values and vectors in descending order.\n4.  Compute the singular values by taking the square root of the eigen values.\n5.  Create the `S` matrix as a diagonal matrix with the singular values.\n6.  Compute `U` by dividing the product of `A` and the eigen vectors by the singular values.\n7.  Ensure `U` is an orthonormal matrix by normalizing its columns.\n8.  Create `V` as the transpose of the eigen vectors.\n\nThe function returns the matrices `U`, `S`, and `V` such that `A = U * S * V`. The results are rounded to the nearest 4th decimal and returned as lists.\n\nYou can test the function using the example usage provided in the code."}
{"task_id": 29, "completion_id": 0, "completion": "### Random Shuffle of Dataset\n\nHere's a Python function that performs a random shuffle of the samples in two numpy arrays, `X` and `y`, while maintaining the corresponding order between them. The function has an optional `seed` parameter for reproducibility.\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, \n    while maintaining the corresponding order between them.\n\n    Args:\n    - X (numpy array): Feature data\n    - y (numpy array): Target data\n    - seed (int, optional): Seed for the random number generator. Defaults to None.\n\n    Returns:\n    - list: A list of two numpy arrays, X and y, after shuffling and reshaping.\n    \"\"\"\n\n    # Set the seed for the random number generator if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Combine X and y into a single array of pairs\n    pairs = np.column_stack((X, y))\n\n    # Shuffle the pairs\n    np.random.shuffle(pairs)\n\n    # Split the pairs back into X and y\n    X_shuffled, y_shuffled = pairs[:, :-1], pairs[:, -1]\n\n    # Reshape and return as a list\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample data\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 0, 1, 1])\n\n    # Shuffle the data with a seed\n    shuffled_X, shuffled_y = shuffle_data(X, y, seed=42)\n    print(\"Shuffled data with seed 42:\")\n    print(\"X:\", shuffled_X)\n    print(\"y:\", shuffled_y)\n\n    # Shuffle the data without a seed\n    shuffled_X, shuffled_y = shuffle_data(X, y)\n    print(\"\\nShuffled data without seed:\")\n    print(\"X:\", shuffled_X)\n    print(\"y:\", shuffled_y)\n```\n\nIn this code:\n\n*   We define a function `shuffle_data` that takes two numpy arrays `X` and `y`, and an optional `seed` parameter.\n*   We set the seed for the random number generator if provided, ensuring reproducibility.\n*   We combine `X` and `y` into a single array of pairs using `np.column_stack`.\n*   We shuffle the pairs using `np.random.shuffle`.\n*   We split the pairs back into `X` and `y`.\n*   We reshape the shuffled arrays using the `tolist()` method and return them as a list.\n\nThe example usage demonstrates how to use the `shuffle_data` function with and without a seed."}
{"task_id": 30, "completion_id": 0, "completion": "### Batch Iterator for Dataset\n\nThe following Python function implements a batch iterable that samples from a numpy array `X` and an optional numpy array `y`. It yields batches of a specified size.\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterable function that samples from a numpy array X and an optional numpy array y.\n\n    Args:\n        X (numpy array): The input data.\n        y (numpy array, optional): The target data. Defaults to None.\n        batch_size (int, optional): The size of each batch. Defaults to 64.\n\n    Yields:\n        tuple or numpy array: Batches of (X, y) pairs if y is provided; otherwise, batches of X only.\n    \"\"\"\n\n    # Check if X is a numpy array\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"X must be a numpy array\")\n\n    # Check if y is a numpy array or None\n    if y is not None and not isinstance(y, np.ndarray):\n        raise ValueError(\"y must be a numpy array or None\")\n\n    # Check if the batch size is a positive integer\n    if not isinstance(batch_size, int) or batch_size <= 0:\n        raise ValueError(\"batch_size must be a positive integer\")\n\n    # Check if the shapes of X and y are compatible\n    if y is not None and X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n\n    # Calculate the number of batches\n    num_batches = int(np.ceil(X.shape[0] / batch_size))\n\n    # Iterate over the batches\n    for i in range(num_batches):\n        # Calculate the start and end indices of the current batch\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, X.shape[0])\n\n        # Slice the current batch from X and y (if provided)\n        batch_X = X[start_idx:end_idx]\n        batch_y = y[start_idx:end_idx] if y is not None else None\n\n        # Yield the batch\n        if batch_y is not None:\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample data\n    X = np.random.rand(100, 10)\n    y = np.random.randint(0, 2, 100)\n\n    # Create a batch iterator\n    iterator = batch_iterator(X, y, batch_size=32)\n\n    # Iterate over the batches\n    for batch in iterator:\n        print(batch)\n```\n\nIn this code:\n\n*   We define a function `batch_iterator` that takes in a numpy array `X`, an optional numpy array `y`, and a batch size.\n*   We perform error checking to ensure that `X` is a numpy array, `y` is a numpy array or `None`, and the batch size is a positive integer.\n*   We calculate the number of batches based on the shape of `X` and the batch size.\n*   We iterate over the batches, slicing the current batch from `X` and `y` (if provided).\n*   We yield the batch, either as a tuple of `(X, y)` or as `X` only.\n*   In the example usage, we create sample data, create a batch iterator, and iterate over the batches, printing each batch."}
{"task_id": 31, "completion_id": 0, "completion": "### Divide Dataset Based on Feature Threshold\n\nHere's a Python function that divides a dataset into two subsets based on a specified feature threshold.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature_threshold(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset into two subsets based on a feature threshold.\n\n    Parameters:\n    - X (numpy array): The input dataset.\n    - feature_i (int): The index of the feature to consider.\n    - threshold (float): The threshold value.\n\n    Returns:\n    - A list of two numpy arrays: \n      1. The subset where feature values are greater than or equal to the threshold.\n      2. The subset where feature values are less than the threshold.\n    \"\"\"\n\n    # Ensure feature index is within bounds\n    if feature_i < 0 or feature_i >= X.shape[1]:\n        raise ValueError(\"Feature index out of bounds\")\n\n    # Create a mask for samples that meet the condition\n    mask = X[:, feature_i] >= threshold\n\n    # Create subsets based on the mask\n    subset_meet_condition = X[mask].tolist()\n    subset_not_meet_condition = X[~mask].tolist()\n\n    return [subset_meet_condition, subset_not_meet_condition]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample dataset\n    X = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n        [10, 11, 12],\n        [13, 14, 15]\n    ])\n\n    feature_i = 1  # Consider the second feature\n    threshold = 10\n\n    result = divide_on_feature_threshold(X, feature_i, threshold)\n    print(\"Subset where feature values are >= threshold:\")\n    print(result[0])\n    print(\"\\nSubset where feature values are < threshold:\")\n    print(result[1])\n```\n\n#### Explanation\n\n*   The function `divide_on_feature_threshold` takes in a dataset `X`, a feature index `feature_i`, and a threshold value.\n*   It first checks if the feature index is within the bounds of the dataset.\n*   Then, it creates a mask to identify samples where the feature value is greater than or equal to the threshold.\n*   Based on this mask, it creates two subsets of the dataset: one where the condition is met and another where it is not.\n*   Finally, it returns these subsets as a list of two numpy arrays, reshaped using the `tolist()` method.\n\nYou can modify the example usage in the `if __name__ == \"__main__\":` block to test the function with your own datasets and feature thresholds."}
{"task_id": 32, "completion_id": 0, "completion": "### Generating Polynomial Features\n#### Function Description\n\nThe `polynomial_features` function generates polynomial features for a given dataset. It takes in a 2D numpy array `X` and an integer `degree`, and returns a new 2D numpy array with polynomial features up to the specified degree.\n\n#### Code\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (numpy array): 2D numpy array of shape (n_samples, n_features)\n    degree (int): Degree of polynomial features to generate\n\n    Returns:\n    list: A list of polynomial features\n    \"\"\"\n    # Get the number of features in the input dataset\n    n_features = X.shape[1]\n\n    # Initialize a list to store the polynomial features\n    poly_features = []\n\n    # Generate all combinations of features up to the specified degree\n    for r in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), r):\n            # Initialize a feature vector for the current combination\n            feature = np.ones(X.shape[0])\n\n            # Compute the feature value for each sample\n            for index in combo:\n                feature *= X[:, index]\n\n            # Add the feature to the list of polynomial features\n            poly_features.append(feature)\n\n    # Stack the polynomial features into a single array\n    poly_features = np.stack(poly_features, axis=1)\n\n    # Return the polynomial features as a list\n    return poly_features.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n\n    # Generate polynomial features of degree 2\n    degree = 2\n    poly_features = polynomial_features(X, degree)\n    print(\"Polynomial Features:\")\n    print(poly_features)\n```\n\n#### Explanation\n\n1. The function `polynomial_features` takes in a 2D numpy array `X` and an integer `degree`.\n2. It generates all combinations of features up to the specified degree using `combinations_with_replacement`.\n3. For each combination, it computes the feature value for each sample by multiplying the corresponding feature values.\n4. The polynomial features are stacked into a single array using `np.stack`.\n5. Finally, the function returns the polynomial features as a list using `tolist()`."}
{"task_id": 33, "completion_id": 0, "completion": "**Generating Random Subsets of a Dataset**\n=============================================\n\n### Function Description\n\nThe `get_random_subsets` function generates random subsets of a given dataset. It takes in a 2D numpy array `X`, a 1D numpy array `y`, an integer `n_subsets`, a boolean `replacements`, and an optional integer `seed`. The function returns a list of `n_subsets` random subsets of the dataset, where each subset is a tuple of `(X_subset, y_subset)`.\n\n### Code\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): 2D numpy array representing the dataset features.\n    y (numpy.ndarray): 1D numpy array representing the dataset labels.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool, optional): Whether to create subsets with replacements. Defaults to True.\n    seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    list: List of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Check if replacements is True or False and generate subsets accordingly\n    if replacements:\n        # Generate random indices with replacements\n        indices = np.random.randint(0, len(X), size=(n_subsets, len(X)))\n    else:\n        # Generate random indices without replacements\n        indices = np.random.choice(len(X), size=(n_subsets, len(X)), replace=False)\n\n    # Initialize an empty list to store the subsets\n    subsets = []\n\n    # Generate subsets\n    for idx in indices:\n        X_subset = X[idx]\n        y_subset = y[idx]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n\n    # Generate 2 random subsets with replacements\n    subsets_with_replacements = get_random_subsets(X, y, 2, replacements=True)\n    print(\"Subsets with replacements:\")\n    for i, subset in enumerate(subsets_with_replacements):\n        print(f\"Subset {i+1}: X = {subset[0]}, y = {subset[1]}\")\n\n    # Generate 2 random subsets without replacements\n    subsets_without_replacements = get_random_subsets(X, y, 2, replacements=False)\n    print(\"\\nSubsets without replacements:\")\n    for i, subset in enumerate(subsets_without_replacements):\n        print(f\"Subset {i+1}: X = {subset[0]}, y = {subset[1]}\")\n```\n\n### Explanation\n\n*   The function `get_random_subsets` takes in a 2D numpy array `X`, a 1D numpy array `y`, an integer `n_subsets`, a boolean `replacements`, and an optional integer `seed`.\n*   It sets the random seed for reproducibility using `np.random.seed(seed)`.\n*   If `replacements` is `True`, it generates random indices with replacements using `np.random.randint`. Otherwise, it generates random indices without replacements using `np.random.choice`.\n*   It then generates subsets by indexing into `X` and `y` using the generated indices.\n*   Finally, it returns a list of subsets, where each subset is a tuple of `(X_subset, y_subset)`.\n\n### Example Usage\n\nThe example usage demonstrates how to generate random subsets of a sample dataset with and without replacements. It prints out the generated subsets for illustration purposes."}
{"task_id": 34, "completion_id": 0, "completion": "### One-Hot Encoding of Nominal Values\n\n#### Function Description\n\nThe `to_categorical` function performs one-hot encoding of nominal values. It takes in a 1D numpy array `x` of integer values and an optional integer `n_col` representing the number of columns for the one-hot encoded array. If `n_col` is not provided, it is automatically determined from the input array.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encoding of nominal values.\n\n    Args:\n    - x (numpy.ndarray): A 1D numpy array of integer values.\n    - n_col (int, optional): The number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    - list: A python list representing the one-hot encoded array.\n    \"\"\"\n    # Check if input is a numpy array\n    if not isinstance(x, np.ndarray):\n        raise ValueError(\"Input must be a numpy array\")\n\n    # Check if input array is 1D\n    if len(x.shape) != 1:\n        raise ValueError(\"Input array must be 1D\")\n\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n\n    # Perform one-hot encoding using numpy's one-hot encoding function\n    encoded = np.eye(n_col)[x]\n\n    # Return the encoded array as a python list\n    return encoded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([0, 1, 2, 0, 1])\n    print(to_categorical(x))\n    print(to_categorical(x, n_col=3))\n```\n\n#### Explanation\n\n1. The function first checks if the input is a numpy array and if it's 1D. If not, it raises a `ValueError`.\n2. If `n_col` is not provided, it determines the number of columns by finding the maximum value in the input array and adding 1. This assumes that the input array contains integer values starting from 0.\n3. The function uses numpy's `eye` function to create an identity matrix with `n_col` columns. It then uses advanced indexing to select the rows corresponding to the values in the input array `x`.\n4. Finally, the function returns the encoded array as a python list using the `tolist` method.\n\n#### Example Use Cases\n\n* `to_categorical(np.array([0, 1, 2, 0, 1]))`: One-hot encodes the input array with automatically determined number of columns.\n* `to_categorical(np.array([0, 1, 2, 0, 1]), n_col=3)`: One-hot encodes the input array with 3 columns."}
{"task_id": 35, "completion_id": 0, "completion": "### Convert Vector to Diagonal Matrix\n\nHere's a Python function that uses NumPy to convert a 1D numpy array into a diagonal matrix.\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Args:\n        x (numpy.ndarray): A 1D numpy array.\n\n    Returns:\n        list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Ensure input is a numpy array\n    x = np.asarray(x)\n\n    # Check if input is 1D\n    if x.ndim != 1:\n        raise ValueError(\"Input must be a 1D numpy array\")\n\n    # Create a diagonal matrix using np.diag\n    diagonal_matrix = np.diag(x)\n\n    # Convert the diagonal matrix to a list and return\n    return diagonal_matrix.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = np.array([1, 2, 3, 4, 5])\n    print(\"Input Vector:\")\n    print(x)\n    print(\"\\nDiagonal Matrix:\")\n    print(make_diagonal(x))\n```\n\nIn this code:\n\n*   We define a function `make_diagonal` that takes a 1D numpy array `x` as input.\n*   We use `np.diag` to create a diagonal matrix from the input array.\n*   Finally, we convert the diagonal matrix to a list using the `tolist` method and return it.\n\nWhen you run this code with the example input `[1, 2, 3, 4, 5]`, it will output the corresponding diagonal matrix as a list:\n\n```\nInput Vector:\n[1 2 3 4 5]\n\nDiagonal Matrix:\n[[1, 0, 0, 0, 0],\n [0, 2, 0, 0, 0],\n [0, 0, 3, 0, 0],\n [0, 0, 0, 4, 0],\n [0, 0, 0, 0, 5]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "### Accuracy Score Calculation Function\n#### Overview\n\nThe following Python function calculates the accuracy score of a model's predictions by comparing the true labels (`y_true`) with the predicted labels (`y_pred`).\n\n#### Code\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Args:\n    - y_true (numpy array): 1D array containing the true labels.\n    - y_pred (numpy array): 1D array containing the predicted labels.\n\n    Returns:\n    - accuracy (float): The accuracy score rounded to the nearest 4th decimal.\n\n    Raises:\n    - ValueError: If y_true and y_pred are not of the same length.\n    \"\"\"\n\n    # Check if y_true and y_pred are of the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"y_true and y_pred must be of the same length\")\n\n    # Compare true labels with predicted labels\n    correct_predictions = np.sum(y_true == y_pred)\n\n    # Calculate accuracy score\n    accuracy = correct_predictions / len(y_true)\n\n    # Round accuracy score to the nearest 4th decimal\n    accuracy = round(accuracy, 4)\n\n    return accuracy\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 2, 3, 4])\n    y_pred = np.array([0, 1, 2, 3, 4])\n    print(accuracy_score(y_true, y_pred))  # Output: 1.0\n\n    y_true = np.array([0, 1, 2, 3, 4])\n    y_pred = np.array([0, 1, 2, 5, 4])\n    print(accuracy_score(y_true, y_pred))  # Output: 0.8\n```\n\n#### Explanation\n\n*   The function `accuracy_score` takes two 1D numpy arrays `y_true` and `y_pred` as input and returns the accuracy score as a float.\n*   It first checks if `y_true` and `y_pred` are of the same length. If not, it raises a `ValueError`.\n*   Then, it compares the true labels with the predicted labels using `np.sum(y_true == y_pred)`, which counts the number of correct predictions.\n*   The accuracy score is calculated by dividing the number of correct predictions by the total number of predictions.\n*   Finally, the accuracy score is rounded to the nearest 4th decimal using the `round` function and returned.\n\nYou can use the example usage section as a starting point to test the function with your own data."}
{"task_id": 37, "completion_id": 0, "completion": "### Calculating Correlation Matrix\n#### Function Description\n\nThe `calculate_correlation_matrix` function calculates the correlation matrix for a given dataset. It takes in a 2D numpy array `X` and an optional 2D numpy array `Y`. If `Y` is not provided, the function calculates the correlation matrix of `X` with itself.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Args:\n    - X (2D numpy array): The input dataset.\n    - Y (2D numpy array, optional): The second dataset for correlation calculation. Defaults to None.\n\n    Returns:\n    - list: The correlation matrix as a 2D list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Check if X is a 2D numpy array\n    if not isinstance(X, np.ndarray) or len(X.shape) != 2:\n        raise ValueError(\"X must be a 2D numpy array\")\n\n    # If Y is not provided, calculate the correlation matrix of X with itself\n    if Y is None:\n        # Calculate the correlation matrix of X with itself\n        correlation_matrix = np.corrcoef(X.T)\n    else:\n        # Check if Y is a 2D numpy array\n        if not isinstance(Y, np.ndarray) or len(Y.shape) != 2:\n            raise ValueError(\"Y must be a 2D numpy array\")\n\n        # Ensure X and Y have the same number of rows\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X and Y must have the same number of rows\")\n\n        # Calculate the correlation matrix of X and Y\n        correlation_matrix = np.corrcoef(X.T, Y.T)[:X.shape[1], X.shape[1]:]\n\n    # Round the correlation matrix to the nearest 4th decimal and convert to list\n    correlation_matrix = np.round(correlation_matrix, 4).tolist()\n\n    return correlation_matrix\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate a random 2D numpy array\n    np.random.seed(0)\n    X = np.random.rand(100, 3)\n    Y = np.random.rand(100, 2)\n\n    # Calculate the correlation matrix of X with itself\n    correlation_matrix_X = calculate_correlation_matrix(X)\n    print(\"Correlation Matrix of X with itself:\")\n    for row in correlation_matrix_X:\n        print(row)\n\n    # Calculate the correlation matrix of X and Y\n    correlation_matrix_XY = calculate_correlation_matrix(X, Y)\n    print(\"\\nCorrelation Matrix of X and Y:\")\n    for row in correlation_matrix_XY:\n        print(row)\n```\n\n#### Explanation\n\n*   The function first checks if `X` is a 2D numpy array and raises a `ValueError` if not.\n*   If `Y` is not provided, it calculates the correlation matrix of `X` with itself using `np.corrcoef(X.T)`.\n*   If `Y` is provided, it checks if `Y` is a 2D numpy array and if `X` and `Y` have the same number of rows. It then calculates the correlation matrix of `X` and `Y` using `np.corrcoef(X.T, Y.T)`.\n*   The correlation matrix is rounded to the nearest 4th decimal using `np.round(correlation_matrix, 4)` and converted to a list using the `tolist()` method.\n*   The example usage demonstrates how to use the function with a random 2D numpy array `X` and an optional array `Y`."}
{"task_id": 38, "completion_id": 0, "completion": "### AdaBoost Fit Method Implementation\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fit method for an AdaBoost classifier.\n\n    Parameters:\n    - X (numpy array): 2D array of shape (n_samples, n_features) representing the dataset.\n    - y (numpy array): 1D array of shape (n_samples,) representing the labels.\n    - n_clf (int): The number of classifiers.\n\n    Returns:\n    - A list of classifiers with their parameters.\n    \"\"\"\n\n    # Initialize sample weights\n    n_samples = X.shape[0]\n    weights = np.full(n_samples, 1 / n_samples)\n\n    # Initialize list to store classifiers\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize best classifier parameters\n        best_feature = None\n        best_threshold = None\n        best_error = np.inf\n        best_classifier = None\n\n        # Iterate over features\n        for feature in range(X.shape[1]):\n            # Get feature values and labels\n            feature_values = X[:, feature]\n\n            # Sort feature values and labels\n            sorted_indices = np.argsort(feature_values)\n            sorted_feature_values = feature_values[sorted_indices]\n            sorted_labels = y[sorted_indices]\n            sorted_weights = weights[sorted_indices]\n\n            # Initialize sum of weights for positive and negative labels\n            sum_pos_weights = np.sum(sorted_weights[sorted_labels == 1])\n            sum_neg_weights = np.sum(sorted_weights[sorted_labels == -1])\n\n            # Initialize best threshold for current feature\n            best_threshold_feature = None\n            best_error_feature = np.inf\n\n            # Iterate over possible thresholds\n            for i in range(1, len(sorted_feature_values)):\n                # Calculate threshold\n                threshold = (sorted_feature_values[i - 1] + sorted_feature_values[i]) / 2\n\n                # Calculate error for current threshold\n                error = sum_pos_weights * (sorted_feature_values[i - 1] < threshold) + sum_neg_weights * (sorted_feature_values[i - 1] >= threshold)\n\n                # Update best threshold and error for current feature\n                if error < best_error_feature:\n                    best_error_feature = error\n                    best_threshold_feature = threshold\n\n                # Update sum of weights for positive and negative labels\n                if sorted_labels[i - 1] == 1:\n                    sum_pos_weights -= sorted_weights[i - 1]\n                else:\n                    sum_neg_weights -= sorted_weights[i - 1]\n\n            # Update best classifier parameters\n            if best_error_feature < best_error:\n                best_error = best_error_feature\n                best_feature = feature\n                best_threshold = best_threshold_feature\n                best_classifier = (best_feature, best_threshold)\n\n        # Update weights\n        classifiers.append(best_classifier)\n        predictions = np.where(X[:, best_classifier[0]] < best_classifier[1], 1, -1)\n        error = np.sum(weights * (predictions != y))\n        alpha = 0.5 * math.log((1 - error) / error)\n        weights *= np.exp(-alpha * y * predictions)\n        weights /= np.sum(weights)\n\n    return classifiers\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, -1, 1, -1])\n    n_clf = 2\n    classifiers = adaboost_fit(X, y, n_clf)\n    print(\"Classifiers:\", classifiers)\n```\n\n### Explanation\n\nThe provided Python function `adaboost_fit` implements the fit method for an AdaBoost classifier. It takes in a 2D numpy array `X` representing the dataset, a 1D numpy array `y` representing the labels, and an integer `n_clf` representing the number of classifiers.\n\nThe function initializes sample weights, iterates over the specified number of classifiers, and for each classifier, it finds the best feature and threshold that results in the lowest error. The error is calculated based on the sample weights and the predicted labels.\n\nAfter finding the best classifier, the function updates the sample weights based on the error and the predicted labels. The updated weights are then used for the next classifier.\n\nThe function returns a list of classifiers with their parameters, where each classifier is represented as a tuple containing the feature index and the threshold value.\n\nIn the example usage, we create a sample dataset `X` and labels `y`, and specify the number of classifiers `n_clf`. We then call the `adaboost_fit` function and print the resulting list of classifiers."}
{"task_id": 39, "completion_id": 0, "completion": "### Implementation of Log Softmax Function\n\nHere's a Python function that calculates the log-softmax of a given 1D numpy array of scores.\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list) -> list:\n    \"\"\"\n    Compute the log-softmax of a given 1D array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list representing the log-softmax of the input scores, rounded to 4 decimal places.\n    \"\"\"\n\n    # Convert the input list to a numpy array\n    scores = np.array(scores)\n\n    # Calculate the maximum score for numerical stability\n    max_score = np.max(scores)\n\n    # Calculate the softmax\n    exp_scores = np.exp(scores - max_score)\n    softmax = exp_scores / np.sum(exp_scores)\n\n    # Calculate the log-softmax\n    log_softmax_scores = np.log(softmax)\n\n    # Round the results to 4 decimal places and convert to a list\n    log_softmax_scores = np.round(log_softmax_scores, 4).tolist()\n\n    return log_softmax_scores\n\n# Example usage:\nif __name__ == \"__main__\":\n    scores = [2.0, 1.0, 0.1]\n    result = log_softmax(scores)\n    print(\"Log Softmax:\", result)\n```\n\n### Explanation\n\n*   The function `log_softmax` takes a list of scores as input and returns the log-softmax of these scores as a list.\n*   It first converts the input list to a numpy array for easier manipulation.\n*   To ensure numerical stability when computing the softmax of large numbers, it subtracts the maximum score from all scores before calculating the exponential.\n*   The softmax is then calculated by dividing the exponential of each score by the sum of the exponentials of all scores.\n*   The log-softmax is calculated by taking the natural logarithm of the softmax.\n*   Finally, the results are rounded to 4 decimal places and converted to a list using numpy's `round` and `tolist` methods.\n\n### Alternative Implementation Using Vectorized Operations\n\nYou can also implement the log-softmax function using vectorized operations, which is more concise and efficient:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list) -> list:\n    \"\"\"\n    Compute the log-softmax of a given 1D array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list representing the log-softmax of the input scores, rounded to 4 decimal places.\n    \"\"\"\n\n    # Convert the input list to a numpy array\n    scores = np.array(scores)\n\n    # Calculate the log-softmax\n    log_softmax_scores = scores - np.log(np.sum(np.exp(scores)))\n\n    # Round the results to 4 decimal places and convert to a list\n    log_softmax_scores = np.round(log_softmax_scores - np.max(log_softmax_scores), 4).tolist()\n\n    # Adjust for the numerically stable method\n    log_softmax_scores = [x - np.log(np.sum(np.exp(np.array(log_softmax_scores)))) + np.log(np.sum(np.exp(scores - np.max(scores)))) for x in log_softmax_scores]\n\n    # However an even cleaner solution exists.\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    log_softmax_scores = scores - max_score - np.log(np.sum(np.exp(scores - max_score)))\n    return np.round(log_softmax_scores, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    scores = [2.0, 1.0, 0.1]\n    result = log_softmax(scores)\n    print(\"Log Softmax:\", result)\n```"}
{"task_id": 40, "completion_id": 0, "completion": "### Implementing a Custom Dense Layer in Python\n\nBelow is the implementation of the `Dense` class, which extends the `Layer` class to create a fully connected neural network layer.\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        \"\"\"\n        Initializes the Dense layer.\n\n        Args:\n        - n_units (int): The number of units (neurons) in the layer.\n        - input_shape (tuple, optional): The input shape to the layer. Defaults to None.\n        \"\"\"\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self):\n        \"\"\"\n        Initializes the weights and biases for the layer.\n        \"\"\"\n        if self.input_shape is not None:\n            # Initialize weights and biases\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n            self.w0 = np.zeros((1, self.n_units)))\n            # Initialize optimizers\n            self.optimizer = {\n                'W': np.zeros_like(self.W),\n                'w0': np.zeros_like(self.w0)\n            }\n\n    def parameters(self):\n        \"\"\"\n        Returns the total number of trainable parameters in the layer.\n\n        Returns:\n        - int: The total number of trainable parameters.\n        \"\"\"\n        if self.W is not None and self.w0 is not None:\n            return self.W.size + self.w0.size\n        else:\n            return 0\n\n    def forward_pass(self, X, training):\n        \"\"\"\n        Computes the output of the layer.\n\n        Args:\n        - X (numpy array): The input to the layer.\n        - training (bool): Whether the layer is in training mode.\n\n        Returns:\n        - numpy array: The output of the layer.\n        \"\"\"\n        if self.W is None or self.w0 is None:\n            raise ValueError(\"Weights and biases must be initialized before the forward pass.\")\n        \n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad, learning_rate=0.01):\n        \"\"\"\n        Computes the gradients and updates the weights and biases.\n\n        Args:\n        - accum_grad (numpy array): The accumulated gradients.\n        - learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.01.\n\n        Returns:\n        - numpy array: The gradient with respect to the input.\n        \"\"\"\n        # Compute gradients\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n        # Update weights and biases\n        if self.optimizer is not None:\n            self.W -= learning_rate * self.optimizer['W']\n            self.w0 -= learning_rate * self.optimizer['w0']\n\n        # Return gradients with respect to input\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        \"\"\"\n        Returns the shape of the output produced by the forward pass.\n\n        Returns:\n        - tuple: The shape of the output.\n        \"\"\"\n        return (self.n_units,)\n\n    def set_weights(self, W, w0):\n        self.W = W\n        self.w0 = w0\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a Dense layer with 5 units and input shape (3,)\n    dense_layer = Dense(5, input_shape=(3,))\n\n    # Initialize weights and biases\n    dense_layer.W = np.random.rand(3, 5)\n    dense_layer.w0 = np.zeros((1, 5)))\n\n    # Set input shape\n    dense_layer.set_input_shape((3,))\n\n    # Create a random input\n    X = np.random.rand(1, 3)\n\n    # Perform forward pass\n    output = dense_layer.forward_pass(X, training=True)\n\n    print(\"Output shape:\", output.shape)\n\n    # Perform backward pass\n    accum_grad = np.random.rand(1, 5)\n    grad = dense_layer.backward_pass(accum_grad=accum_grad)\n\n    print(\"Gradient shape:\", grad.shape)\n\n    # Get the number of parameters\n    num_params = dense_layer.parameters()\n    print(\"Number of parameters:\", num_params)\n\n    # Get the output shape\n    output_shape = dense_layer.output_shape()\n    print(\"Output shape:\", output_shape)\n```\n\n### Explanation\n\nThe provided code implements a custom `Dense` layer in Python, which is a subclass of the `Layer` class. \n\nThe `__init__` method initializes the `Dense` layer with a specified number of units (`n_units`) and an optional input shape.\n\nThe `initialize` method sets up the weights (`W`) and biases (`w0`) for the layer using a uniform distribution. It also initializes optimizers for `W` and `w0`.\n\nThe `parameters` method returns the total number of trainable parameters in the layer.\n\nThe `forward_pass` method computes the output of the layer by performing a dot product between the input `X` and the weight matrix `W`, and then adding the bias `w0`.\n\nThe `backward_pass` method calculates and returns the gradient with respect to the input and updates the weights and biases using the optimizer's update rule.\n\nThe `output_shape` method returns the shape of the output produced by the forward pass.\n\n### Usage\n\nTo use this `Dense` class, create an instance and call the methods as shown in the example usage section.\n\nThe code defines a `Dense` layer with 5 units and an input shape of (3,). It then initializes the weights and biases, sets the input shape, and performs a forward pass with a random input.\n\nFinally, it computes the gradients and updates the weights and biases using the `backward_pass` method.\n\nMake sure to round results to the nearest 4th decimal. \n\nHere is a reformatted version of your code with corrections and improvements:\n\n```python\nimport numpy as np\nimport math\n\nclass Layer:\n    def __init__(self):\n        pass\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        \"\"\"\n        Initializes the Dense layer.\n\n        Args:\n        - n_units (int): The number of units (neurons) in the layer.\n        - input_shape (tuple, optional): The input shape to the layer. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        \"\"\"\n        Initializes the weights and biases for the layer.\n        \"\"\"\n        if self.input_shape is not None:\n            # Initialize weights and biases\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units)))\n            self.w0 = np.zeros((1, self.n_units)))\n            # Initialize optimizers\n            self.optimizer_W = np.zeros_like(self.W)\n            self.optimizer_w0 = np.zeros_like(self.w0)\n\n    def parameters(self):\n        \"\"\"\n        Returns the total number of trainable parameters in the layer.\n\n        Returns:\n        - int: The total number of trainable parameters.\n        \"\"\"\n        if self.W is not None and self.w0 is not None:\n            return self.W.size + self.w0.size\n        else:\n            return 0\n\n    def forward_pass(self, X, training):\n        \"\"\"\n        Computes the output of the layer.\n\n        Args:\n        - X (numpy array): The input to the layer.\n        - training (bool): Whether the layer is in training mode.\n\n        Returns:\n        - numpy array: The output of the layer.\n        \"\"\"\n        if self.W is None or self.w0 is None:\n            raise ValueError(\"Weights and biases must be initialized before the forward pass.\")\n        \n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad, learning_rate=0.01):\n        \"\"\"\n        Computes the gradients and updates the weights and biases.\n\n        Args:\n        - accum_grad (numpy array): The accumulated gradients.\n        - learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.01.\n\n        Returns:\n        - numpy array: The gradient with respect to the input.\n        \"\"\"\n        # Compute gradients\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n        # Update weights and biases\n        self.W -= learning_rate * self.optimizer_W\n        self.w0 -="}
{"task_id": 41, "completion_id": 0, "completion": "### Simple Convolutional 2D Layer Implementation\n\nHere's a Python function implementing a 2D convolutional layer using NumPy. This function takes an input matrix, a convolutional kernel, padding, and stride as parameters.\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolutional layer to the input matrix using the specified kernel, padding, and stride.\n\n    Args:\n    - input_matrix (np.ndarray): The input matrix to be convolved.\n    - kernel (np.ndarray): The convolutional kernel.\n    - padding (int): The amount of padding to apply to the input matrix.\n    - stride (int): The stride of the convolution operation.\n\n    Returns:\n    - A list representing the output of the convolutional layer, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Get the dimensions of the input matrix and kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n\n    # Calculate the output height and width\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n\n    # Pad the input matrix\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Initialize the output matrix\n    output = np.zeros((output_height, output_width))\n\n    # Perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            # Extract the region of interest from the padded input matrix\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n\n            # Perform the convolution operation for this region\n            output[i, j] = np.sum(region * kernel)\n\n    # Round the output to the nearest 4th decimal and convert to a list\n    output = np.round(output, 4).tolist()\n\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    padding = 1\n    stride = 1\n\n    output = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(output)\n```\n\nIn this code:\n\n*   We first calculate the dimensions of the output matrix based on the input matrix, kernel, padding, and stride.\n*   We pad the input matrix using NumPy's `pad` function.\n*   We then perform the convolution operation by iterating over the output matrix, extracting the region of interest from the padded input matrix, and convolving it with the kernel.\n*   Finally, we round the output to the nearest 4th decimal and convert it to a list using NumPy's `round` and `tolist` functions.\n\nThe example usage demonstrates how to use the `simple_conv2d` function with a sample input matrix, kernel, padding, and stride."}
{"task_id": 42, "completion_id": 0, "completion": "# Implementing ReLU Activation Function\n## Overview\n\nThe Rectified Linear Unit (ReLU) activation function is a widely used activation function in deep learning models. It maps all negative values to 0 and all positive values to the same value.\n\n## Code\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    This function implements the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The value after applying the ReLU function.\n    \"\"\"\n    # If the input is greater than 0, return the input; otherwise, return 0\n    return max(0, z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(relu(5.0))  # Expected output: 5.0\n    print(relu(-3.0))  # Expected output: 0\n```\n\n## Explanation\n\n*   The `relu` function takes a single float `z` as input and returns the result after applying the ReLU function.\n*   The `max` function is used with 0 and `z` as arguments to implement the ReLU logic. If `z` is greater than 0, `max` returns `z`; otherwise, it returns 0.\n\n## Alternative Implementation\n\nYou can also implement ReLU using a simple if-else statement:\n\n```python\ndef relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0\n```\n\nHowever, the `max` function approach is more concise and efficient."}
{"task_id": 43, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing the Ridge Regression loss function in Python. The function should take a 2D numpy array `X` representing the feature matrix, a 1D numpy array `w` representing the coefficients, a 1D numpy array `y_true` representing the true labels, and a float `alpha` representing the regularization parameter. The function should return the Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n\nKey constraints and edge cases:\n\n* The input arrays `X`, `w`, and `y_true` are numpy arrays.\n* `X` is a 2D array, `w` is a 1D array, and `y_true` is a 1D array.\n* The regularization parameter `alpha` is a float.\n* The output should be rounded to the nearest 4th decimal.\n\nInput:\n\n* `X`: 2D numpy array representing the feature matrix\n* `w`: 1D numpy array representing the coefficients\n* `y_true`: 1D numpy array representing the true labels\n* `alpha`: float representing the regularization parameter\n\nOutput:\n\n* The Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n\nSubtle requirements or implicit constraints:\n\n* The Ridge loss function combines the Mean Squared Error (MSE) and a regularization term.\n* The regularization term is the L2 norm of the coefficients `w`, multiplied by `alpha`.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The Ridge loss function can be broken down into two parts: the Mean Squared Error (MSE) and the regularization term.\n* The MSE can be calculated as the mean of the squared differences between the predicted labels and the true labels.\n* The regularization term is the L2 norm of the coefficients `w`, multiplied by `alpha`.\n* The L2 norm of `w` can be calculated as the square root of the sum of the squared values of `w`.\n\nPatterns or properties being used:\n\n* The Mean Squared Error (MSE) is a common loss function for regression problems.\n* The L2 norm is a common regularization term for Ridge regression.\n\nKey algorithmic insights:\n\n* The predicted labels can be calculated by taking the dot product of the feature matrix `X` and the coefficients `w`.\n* The MSE can be calculated using the predicted labels and the true labels.\n* The regularization term can be calculated using the L2 norm of `w` and `alpha`.\n\nImportant realizations:\n\n* The Ridge loss function combines the MSE and the regularization term.\n* The regularization term is a penalty term that encourages smaller values of `w`.\n\nCritical examples or test cases:\n\n* None mentioned explicitly, but the problem statement implies that the function should work for any valid input.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Calculate the predicted labels by taking the dot product of the feature matrix `X` and the coefficients `w`.\n2. Calculate the Mean Squared Error (MSE) using the predicted labels and the true labels.\n3. Calculate the L2 norm of the coefficients `w`.\n4. Calculate the regularization term by multiplying the L2 norm of `w` by `alpha`.\n5. Calculate the Ridge loss by adding the MSE and the regularization term.\n6. Round the result to the nearest 4th decimal.\n\nCritical details:\n\n* Use numpy arrays to represent the input data.\n* Use the `np.dot` function to calculate the dot product of `X` and `w`.\n* Use the `np.mean` function to calculate the mean of the squared differences between the predicted labels and the true labels.\n* Use the `np.linalg.norm` function to calculate the L2 norm of `w`.\n\nOptimizations or special cases:\n\n* None mentioned explicitly.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function.\n\n    Parameters:\n    X (np.ndarray): 2D array representing the feature matrix.\n    w (np.ndarray): 1D array representing the coefficients.\n    y_true (np.ndarray): 1D array representing the true labels.\n    alpha (float): Regularization parameter.\n\n    Returns:\n    float: The Ridge loss.\n    \"\"\"\n    # Calculate the predicted labels\n    y_pred = np.dot(X, w)\n\n    # Calculate the Mean Squared Error (MSE)\n    mse = np.mean((y_pred - y_true) ** 2)\n\n    # Calculate the regularization term\n    reg_term = alpha * np.sum(w ** 2)\n\n    # Calculate the Ridge loss\n    ridge_loss = mse + reg_term\n\n    return round(ridge_loss, 4)\n```"}
{"task_id": 44, "completion_id": 0, "completion": "# Leaky ReLU Activation Function\n## Overview\n\nThe Leaky Rectified Linear Unit (Leaky ReLU) is a variation of the Rectified Linear Unit (ReLU) activation function. It is used in deep learning models to introduce non-linearity. Unlike ReLU, Leaky ReLU allows a small fraction of the input to pass through, even if it is negative.\n\n## Implementation\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n    float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    # If z is greater than or equal to 0, return z (ReLU for positive inputs)\n    if z >= 0:\n        return z\n    # If z is less than 0, return alpha * z (leaky part for negative inputs)\n    else:\n        return alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(leaky_relu(2.0))  # Expected output: 2.0\n    print(leaky_relu(-2.0))  # Expected output: -0.02 (with default alpha)\n    print(leaky_relu(-2.0, alpha=0.1))  # Expected output: -0.2\n```\n\n## Alternative Implementation Using NumPy\n\nIf you're working with large arrays or vectors, consider using NumPy for vectorized operations:\n\n```python\nimport numpy as np\n\ndef leaky_relu_numpy(z: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function using NumPy.\n\n    Args:\n    z (np.ndarray): The input array.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n    np.ndarray: The array after applying the Leaky ReLU function.\n    \"\"\"\n    return np.where(z >= 0, z, alpha * z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    z_array = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    print(leaky_relu_numpy(z_array))  # Expected output: [-0.02 -0.01  0.    1.    2.   ]\n```"}
{"task_id": 45, "completion_id": 0, "completion": "### Linear Kernel Function\n\nThe linear kernel function computes the dot product (inner product) of two input vectors.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors.\n\n    Args:\n        x1 (numpy.ndarray): The first input vector.\n        x2 (numpy.ndarray): The second input vector.\n\n    Returns:\n        float: The linear kernel value.\n\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    # Check if input vectors are numpy arrays\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n\n    # Check if input vectors have the same length\n    if len(x1) != len(x2):\n        raise ValueError(\"Input vectors must be of the same length\")\n\n    # Compute the linear kernel using the dot product\n    kernel_value = np.dot(x1, x2)\n\n    return kernel_value\n\n# Example usage\nif __name__ == \"__main__\":\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n    print(kernel_function(x1, x2))  # Output: 32\n```\n\n#### Explanation\n\n*   The function `kernel_function` takes two input vectors `x1` and `x2` and returns their dot product.\n*   It first checks if the input vectors are of the same length and raises a `ValueError` if they are not.\n*   The `np.dot` function is used to compute the dot product of the two input vectors.\n*   The example usage demonstrates how to use the `kernel_function` with two sample vectors."}
{"task_id": 46, "completion_id": 0, "completion": "## Precision Metric Implementation\n### Overview\n\nThe precision metric is a measure of the accuracy of a binary classification model. It is defined as the ratio of true positives to the sum of true positives and false positives.\n\n### Code\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given two numpy arrays: y_true and y_pred.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The precision metric.\n\n    Raises:\n    ValueError: If the input arrays are not of the same shape.\n    ZeroDivisionError: If the denominator of the precision calculation is zero.\n    \"\"\"\n\n    # Check if input arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape\")\n\n    # Calculate true positives and false positives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n\n    # Check for division by zero\n    if tp + fp == 0:\n        raise ZeroDivisionError(\"Cannot calculate precision with zero true positives and false positives\")\n\n    # Calculate precision\n    precision_metric = tp / (tp + fp)\n\n    return precision_metric\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0, 1, 0, 0, 1, 1])\n    print(precision(y_true, y_pred))\n```\n\n### Explanation\n\n*   The function `precision` takes two numpy arrays `y_true` and `y_pred` as input and returns the precision metric.\n*   It first checks if the input arrays have the same shape and raises a `ValueError` if they do not.\n*   It then calculates the true positives (`tp`) and false positives (`fp`) by comparing the true and predicted labels.\n*   The function checks for division by zero and raises a `ZeroDivisionError` if the denominator of the precision calculation is zero.\n*   Finally, it calculates the precision metric as the ratio of true positives to the sum of true positives and false positives.\n\n### Example Use Case\n\nThe example usage demonstrates how to use the `precision` function with sample true and predicted labels. The output will be the precision metric for the given classification results."}
{"task_id": 47, "completion_id": 0, "completion": "### Gradient Descent Variants with MSE Loss\n\nHere's a Python function that implements Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function.\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs Gradient Descent Variants (SGD, Batch GD, Mini-Batch GD) with MSE Loss.\n\n    Args:\n    - X (numpy array): Feature matrix.\n    - y (numpy array): Target vector.\n    - weights (numpy array): Initial model weights.\n    - learning_rate (float): Learning rate for gradient descent.\n    - n_iterations (int): Number of iterations for gradient descent.\n    - batch_size (int, optional): Batch size for mini-batch gradient descent. Defaults to 1.\n    - method (str, optional): Gradient descent variant. Options: 'stochastic', 'batch', 'mini-batch'. Defaults to 'batch'.\n\n    Returns:\n    - list: A list of weights after each iteration, rounded to 4 decimal places.\n    \"\"\"\n\n    # Validate method parameter\n    if method not in ['stochastic', 'batch', 'mini-batch']:\n        raise ValueError(\"Invalid method. Choose from 'stochastic', 'batch', or 'mini-batch'.\")\n\n    # Initialize weights history\n    weights_history = [weights.tolist()]\n\n    # Perform gradient descent\n    for _ in range(n_iterations):\n        if method == 'stochastic':\n            # Stochastic Gradient Descent\n            for i in range(len(y)):\n                # Compute predictions and errors\n                prediction = np.dot(X[i], weights)\n                error = prediction - y[i]\n\n                # Update weights\n                weights -= learning_rate * error * X[i]\n\n        elif method == 'batch':\n            # Batch Gradient Descent\n            # Compute predictions and errors\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n\n            # Update weights\n            weights -= learning_rate * (2 * np.dot(X.T, errors) / len(y))\n\n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            n_samples = len(y)\n            for i in range(0, n_samples, batch_size):\n                batch_X = X[i:i+batch_size]\n                batch_y = y[i:i+batch_size]\n\n                # Compute predictions and errors\n                predictions = np.dot(batch_X, weights)\n                errors = predictions - batch_y\n\n                # Update weights\n                weights -= learning_rate * (2 * np.dot(batch_X.T, errors) / batch_size)\n\n        # Append current weights to history\n        weights_history.append(np.round(weights, 4).tolist())\n\n    return weights_history\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate random data\n    np.random.seed(0)\n    X = np.random.rand(100, 3)\n    y = 3 * X[:, 0] + 2 * X[:, 1] - 4 * X[:, 2] + np.random.randn(100)\n\n    # Initialize weights\n    weights = np.random.rand(3)\n\n    # Perform gradient descent\n    learning_rate = 0.01\n    n_iterations = 1000\n    batch_size = 10\n\n    # Stochastic Gradient Descent\n    weights_sgd = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n    print(\"Stochastic Gradient Descent Weights:\")\n    print(weights_sgd[-1])\n\n    # Batch Gradient Descent\n    weights_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n    print(\"\\nBatch Gradient Descent Weights:\")\n    print(weights_batch[-1])\n\n    # Mini-Batch Gradient Descent\n    weights_mini_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini-batch')\n    print(\"\\nMini-Batch Gradient Descent Weights:\")\n    print(weights_mini_batch[-1])\n```\n\nThe provided Python function `gradient_descent` implements three variants of gradient descent: Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent. The function takes in the feature matrix `X`, target vector `y`, initial model weights `weights`, learning rate `learning_rate`, number of iterations `n_iterations`, batch size `batch_size`, and the method to use.\n\nThe function returns a list of weights after each iteration, rounded to 4 decimal places. It first validates the `method` parameter to ensure it's one of the supported variants. Then, it performs the specified gradient descent variant and updates the weights accordingly.\n\nIn the example usage section, we demonstrate how to use the `gradient_descent` function with sample data and print the final weights for each gradient descent variant."}
{"task_id": 48, "completion_id": 0, "completion": "# Reduced Row Echelon Form (RREF) Function Implementation\n\n## Overview\n\nThe following Python function implements the Reduced Row Echelon Form (RREF) algorithm for a given matrix. This algorithm transforms a matrix into its RREF, where each leading entry in a row is 1, and all other elements in the column containing the leading 1 are zeros.\n\n## Code\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Compute the Reduced Row Echelon Form (RREF) of a given matrix.\n\n    Args:\n    matrix (list of lists or numpy array): Input matrix.\n\n    Returns:\n    list of lists: RREF of the input matrix.\n    \"\"\"\n\n    # Convert input matrix to numpy array for easier manipulation\n    matrix = np.array(matrix, dtype=float)\n\n    # Get the number of rows and columns in the matrix\n    num_rows, num_cols = matrix.shape\n\n    # Initialize the current row and column indices\n    current_row = 0\n    current_col = 0\n\n    # Iterate through the matrix\n    while current_row < num_rows and current_col < num_cols:\n        # Find the row with the largest absolute value in the current column\n        max_row = np.argmax(np.abs(matrix[current_row:, current_col])) + current_row\n\n        # If the maximum value is zero, move to the next column\n        if matrix[max_row, current_col] == 0:\n            current_col += 1\n            continue\n\n        # Swap the current row with the row having the maximum value\n        matrix[[current_row, max_row]] = matrix[[max_row, current_row]]\n\n        # Normalize the current row\n        matrix[current_row] = matrix[current_row] / matrix[current_row, current_col]\n\n        # Eliminate the current column's values in other rows\n        for row in range(num_rows):\n            if row != current_row:\n                matrix[row] = matrix[row] - matrix[row, current_col] * matrix[current_row]\n\n        # Move to the next row and column\n        current_row += 1\n        current_col += 1\n\n    # Return the RREF matrix as a list of lists\n    return matrix.tolist()\n\n# Example usage:\nmatrix = [\n    [2, 1, -1, 8],\n    [-3, -1, 2, -11],\n    [-2, 1, 2, -3]\n]\n\nrref_matrix = rref(matrix)\nprint(\"RREF Matrix:\")\nfor row in rref_matrix:\n    print(row)\n```\n\n## Explanation\n\nThe provided code implements the RREF algorithm using Gaussian elimination with partial pivoting. Here's a step-by-step breakdown:\n\n1.  **Initialization**: The input matrix is converted to a numpy array for easier manipulation. The number of rows and columns in the matrix is determined.\n2.  **Iteration**: The algorithm iterates through the matrix using two indices, `current_row` and `current_col`, starting from the top-left corner.\n3.  **Finding the Pivot Row**: For each column, the algorithm finds the row with the largest absolute value (the pivot row). If the maximum value is zero, it moves to the next column.\n4.  **Row Swapping and Normalization**: The current row is swapped with the pivot row, and then the current row is normalized by dividing all its elements by the pivot value (making the pivot element equal to 1).\n5.  **Elimination**: The algorithm eliminates the current column's values in other rows by subtracting a multiple of the current row from each row.\n6.  **Iteration Continuation**: The algorithm moves to the next row and column and repeats the process until all rows or columns have been processed.\n7.  **Result**: The final RREF matrix is returned as a list of lists using the `tolist()` method.\n\nThe example usage demonstrates how to use the `rref()` function with a sample matrix and print the resulting RREF matrix."}
{"task_id": 49, "completion_id": 0, "completion": "### Adam Optimization Algorithm Implementation\n#### Overview\nThe Adam optimization algorithm is a popular stochastic gradient descent optimizer that adapts the learning rate for each parameter based on the magnitude of the gradient. Here, we implement the Adam algorithm in Python.\n\n#### Code\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Optimizes the objective function using the Adam algorithm.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of `f`.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 1000.\n\n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n\n    # Initialize variables\n    x = x0\n    m = np.zeros_like(x0)  # First moment estimate\n    v = np.zeros_like(x0)  # Second moment estimate\n    t = 0  # Iteration counter\n\n    for _ in range(num_iterations):\n        # Compute gradient\n        g = grad(x)\n\n        # Update iteration counter\n        t += 1\n\n        # Update first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n\n        # Update second moment estimate\n        v = beta2 * v + (1 - beta2) * g**2\n\n        # Compute bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n\n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Return optimized parameters as a list, rounded to 4 decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage:\ndef objective_function(x):\n    return x[0]**2 + x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([1.0, 2.0])\noptimized_params = adam_optimizer(objective_function, gradient, x0)\nprint(optimized_params)\n```\n\n#### Explanation\n\n*   The `adam_optimizer` function takes in the objective function `f`, its gradient `grad`, initial parameters `x0`, and various hyperparameters.\n*   It initializes the first and second moment estimates `m` and `v`, as well as the iteration counter `t`.\n*   The function then iterates `num_iterations` times, computing the gradient, updating the moment estimates, and adjusting the parameters using the Adam update rule.\n*   Finally, it returns the optimized parameters as a list, rounded to 4 decimal places.\n\n#### Example Usage\n\nThe example usage demonstrates how to use the `adam_optimizer` function to minimize a simple objective function `x[0]**2 + x[1]**2`. The optimized parameters are printed to the console."}
{"task_id": 50, "completion_id": 0, "completion": "### Lasso Regression using Gradient Descent\n\nHere's the implementation of Lasso Regression using Gradient Descent in Python:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Lasso Regression using Gradient Descent.\n\n    Args:\n    - X (np.array): Feature matrix.\n    - y (np.array): Target vector.\n    - alpha (float, optional): Regularization parameter. Defaults to 0.1.\n    - learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n    - max_iter (int, optional): Maximum number of iterations. Defaults to 1000.\n    - tol (float, optional): Tolerance for convergence. Defaults to 1e-4.\n\n    Returns:\n    - tuple: A tuple containing the optimized weights and bias.\n    \"\"\"\n\n    # Initialize weights and bias\n    n_features = X.shape[1]\n    weights = np.zeros(n_features)\n    bias = 0\n\n    # Gradient descent iterations\n    for _ in range(max_iter):\n        # Predict\n        y_pred = np.dot(X, weights) + bias\n\n        # Compute gradients\n        dw = (-1 / X.shape[0]) * np.dot(X.T, (y - y_pred)) + (alpha / X.shape[0]) * np.sign(weights)\n        db = (-1 / X.shape[0]) * np.sum(y - y_pred)\n\n        # Update weights and bias\n        new_weights = weights - learning_rate * dw\n        new_bias = bias - learning_rate * db\n\n        # Check convergence\n        if np.linalg.norm(weights - new_weights) < tol and np.abs(bias - new_bias) < tol:\n            weights = new_weights\n            bias = new_bias\n            break\n\n        weights = new_weights\n        bias = new_bias\n\n    # Round results to 4 decimal places and return as list\n    return (np.round(weights, 4).tolist(), np.round(bias, 4))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n\n    # Run Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n\n    print(\"Optimized Weights:\", weights)\n    print(\"Optimized Bias:\", bias)\n```\n\nIn this code:\n\n*   We define the `l1_regularization_gradient_descent` function that takes in the feature matrix `X`, target vector `y`, regularization parameter `alpha`, learning rate, maximum iterations, and tolerance.\n*   We initialize the weights and bias, then perform gradient descent iterations until convergence or maximum iterations are reached.\n*   In each iteration, we compute the predictions, gradients of the loss function with respect to the weights and bias, and update the weights and bias using the gradients and learning rate.\n*   We apply the L1 penalty to the weights during the update step.\n*   Finally, we round the optimized weights and bias to 4 decimal places and return them as a list.\n\nThe example usage demonstrates how to use the `l1_regularization_gradient_descent` function with some randomly generated data. You can modify the `alpha`, `learning_rate`, `max_iter`, and `tol` parameters to see how they affect the optimization process."}
{"task_id": 51, "completion_id": 0, "completion": "# Optimal String Alignment Distance\n\n## Problem Description\n\nThe Optimal String Alignment (OSA) distance represents the minimum number of edits required to transform one string into another. The allowed edit operations are:\n\n* Insert a character\n* Delete a character\n* Substitute a character\n* Transpose two adjacent characters\n\nEach of these operations costs 1 unit.\n\n## Implementation\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two given strings.\n\n    Args:\n    source (str): The source string.\n    target (str): The target string.\n\n    Returns:\n    int: The minimum number of edits needed to convert the source string into the target string.\n    \"\"\"\n\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the base cases\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i, j] = min(\n                dp[i - 1, j] + 1,      # Deletion\n                dp[i, j - 1] + 1,      # Insertion\n                dp[i - 1, j - 1] + cost  # Substitution or no operation\n            )\n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n\n    return dp[m, n]\n\n# Example usage:\nif __name__ == \"__main__\":\n    source_str = \"caper\"\n    target_str = \"acer\"\n    distance = OSA(source_str, target_str)\n    print(f\"The OSA distance between '{source_str}' and '{target_str}' is: {distance}\")\n```\n\n## Explanation\n\nThe provided code implements a dynamic programming approach to calculate the Optimal String Alignment (OSA) distance between two strings.\n\n*   We first initialize a 2D array `dp` with dimensions `(m + 1) x (n + 1)`, where `m` and `n` are the lengths of the source and target strings, respectively.\n*   The base cases are initialized as follows:\n\n    *   `dp[i, 0] = i` represents the OSA distance when the target string is empty, which is equal to the length of the source string (all deletions).\n    *   `dp[0, j] = j` represents the OSA distance when the source string is empty, which is equal to the length of the target string (all insertions).\n*   We then fill in the rest of the `dp` array using the following rules:\n\n    *   For each cell `dp[i, j]`, we consider three operations:\n\n        *   Deletion: `dp[i - 1, j] + 1`\n        *   Insertion: `dp[i, j - 1] + 1`\n        *   Substitution or no operation: `dp[i - 1, j - 1] + cost`, where `cost` is 0 if the current characters in the source and target strings match, and 1 otherwise\n    *   We also check for transposition:\n\n        *   If the current characters in the source and target strings are swapped with the previous characters, we consider a transposition operation: `dp[i - 2, j - 2] + 1`\n*   Finally, the OSA distance is stored in the bottom-right cell of the `dp` array: `dp[m, n]`."}
{"task_id": 52, "completion_id": 0, "completion": "## Recall Metric Implementation in Binary Classification\n\n### Overview\n\nThe recall metric in binary classification evaluates a model's ability to identify all positive instances from the actual positive cases in a dataset. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in a binary classification setting.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n\n    # Convert input lists to NumPy arrays for efficient computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate true positives (TP) and false negatives (FN)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Calculate the denominator (TP + FN) to avoid division by zero\n    denominator = tp + fn\n\n    # If the denominator is zero, return 0.0 as per the problem statement\n    if denominator == 0:\n        return 0.0\n\n    # Calculate recall and round to three decimal places\n    recall_value = tp / denominator\n    return round(recall_value, 3)\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]\nprint(recall(y_true, y_pred))  # Output: 0.667\n```\n\n### Explanation\n\n1. The function `recall(y_true, y_pred)` takes two lists as input: `y_true` for true binary labels and `y_pred` for predicted binary labels.\n2. It converts the input lists to NumPy arrays for efficient computation.\n3. True positives (TP) are calculated by finding the intersection of `y_true == 1` and `y_pred == 1`.\n4. False negatives (FN) are calculated by finding the intersection of `y_true == 1` and `y_pred == 0`.\n5. The denominator for the recall calculation is `TP + FN`. If this denominator is zero, the function returns 0.0 to avoid division by zero.\n6. Finally, the recall value is calculated as `TP / (TP + FN)` and rounded to three decimal places before being returned."}
{"task_id": 53, "completion_id": 0, "completion": "### Self-Attention Mechanism Implementation\n\n#### Overview\n\nThe self-attention mechanism is a key component of transformer models. It allows a model to attend to different parts of the input sequence simultaneously and weigh their importance. This implementation takes in input sequence `X`, and learnable weight matrices `W_q`, `W_k`, `W_v` for query, key, and value projections respectively.\n\n#### Implementation\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes self-attention output.\n\n    Parameters:\n    - X (numpy array): Input sequence of shape (n, d)\n    - W_q (numpy array): Weight matrix for query projection of shape (d, d)\n    - W_k (numpy array): Weight matrix for key projection of shape (d, d)\n    - W_v (numpy array): Weight matrix for value projection of shape (d, d)\n\n    Returns:\n    - Self-attention output as a numpy array, reshaped and converted to a list.\n    \"\"\"\n\n    # Compute query, key, and value projections\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[1])\n\n    # Compute attention weights\n    attention_weights = np.softmax(attention_scores, axis=-1)\n\n    # Compute self-attention output\n    output = np.dot(attention_weights, V)\n\n    # Round results to the nearest 4th decimal\n    output = np.round(output, 4)\n\n    # Reshape and convert to list\n    output = output.tolist()\n\n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input sequence and weight matrices\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    W_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n    W_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n    W_v = np.array([[0.5, 0.6, 0.7], [0.2, 0.3, 0.4], [0.1, 0.2, 0.3]])\n\n    # Compute self-attention output\n    output = self_attention(X, W_q, W_k, W_v)\n    print(output)\n```\n\n#### Explanation\n\nThe provided code implements the self-attention mechanism. Here's a step-by-step breakdown:\n\n1.  **Query, Key, and Value Projections**: The input sequence `X` is projected into query `Q`, key `K`, and value `V` spaces using learnable weight matrices `W_q`, `W_k`, and `W_v` respectively.\n2.  **Attention Scores Computation**: Attention scores are computed by taking the dot product of `Q` and `K.T` and scaling by the square root of the dimensionality of `Q`.\n3.  **Attention Weights Computation**: Attention weights are obtained by applying the softmax function to the attention scores along the last axis.\n4.  **Self-Attention Output Computation**: The self-attention output is computed by taking the dot product of attention weights and `V`.\n5.  **Result Processing**: The output is rounded to the nearest 4th decimal, reshaped, and converted to a list using numpy's `tolist()` method.\n\nThe example usage demonstrates how to use the `self_attention` function with sample input sequence and weight matrices."}
{"task_id": 54, "completion_id": 0, "completion": "# Implementing a Simple RNN\n\n## Description\nThis code implements a simple Recurrent Neural Network (RNN) cell in Python. The function processes a sequence of input vectors and produces the final hidden state.\n\n## Code\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Args:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n\n    # Convert input lists to numpy arrays for easier computation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n\n    # Initialize the hidden state with the initial hidden state\n    hidden_state = initial_hidden_state\n\n    # Process the input sequence\n    for input_vector in input_sequence:\n        # Compute the hidden state update\n        hidden_state_update = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n        \n        # Update the hidden state\n        hidden_state = hidden_state_update\n\n    # Return the final hidden state, rounded to four decimal places, as a python list\n    return np.round(hidden_state, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_sequence = [[0.1, 0.2], [0.3, 0.4]]\n    initial_hidden_state = [0.5, 0.6]\n    Wx = [[0.7, 0.8], [0.9, 1.0]]\n    Wh = [[1.1, 1.2], [1.3, 1.4]]\n    b = [0.1, 0.2]\n\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(final_hidden_state)\n```\n\n## Explanation\n\n*   The function `rnn_forward` takes as inputs the sequence of input vectors `input_sequence`, the initial hidden state `initial_hidden_state`, the weight matrices for input-to-hidden and hidden-to-hidden connections `Wx` and `Wh`, and the bias vector `b`.\n*   It processes the input sequence and produces the final hidden state using the tanh activation function for the hidden state updates.\n*   The function returns the final hidden state after processing the entire sequence, rounded to four decimal places, as a python list.\n*   In the example usage, we demonstrate how to use the `rnn_forward` function with sample inputs."}
{"task_id": 55, "completion_id": 0, "completion": "## 2D Translation Matrix Implementation\n\n### Description\nThis code implements a function that applies a 2D translation matrix to a set of points. The translation matrix is used to move points in 2D space by a specified distance in the x and y directions.\n\n### Code\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n    points (list): A list of [x, y] coordinates.\n    tx (float): The translation distance in the x direction.\n    ty (float): The translation distance in the y direction.\n\n    Returns:\n    list: A new list of points after applying the translation matrix.\n    \"\"\"\n    # Convert the list of points to a numpy array for easier manipulation\n    points_array = np.array(points)\n\n    # Create a translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                  [0, 1, ty],\n                                  [0, 0, 1]])\n\n    # Add a column of ones to the points array to make it compatible with the translation matrix\n    homogeneous_points = np.hstack((points_array, np.ones((len(points_array), 1))))\n\n    # Apply the translation matrix to the points\n    translated_points = np.dot(translation_matrix, homogeneous_points.T).T\n\n    # Remove the last column (which is all ones) and return the result as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\nprint(translate_object(points, tx, ty))\n```\n\n### Explanation\n\n1. The function `translate_object` takes a list of points and translation distances `tx` and `ty` as input.\n2. It converts the list of points to a numpy array for easier manipulation.\n3. A translation matrix is created with the given translation distances.\n4. The points are converted to homogeneous coordinates by adding a column of ones.\n5. The translation matrix is applied to the points using matrix multiplication.\n6. The translated points are converted back to non-homogeneous coordinates and returned as a list.\n\n### Output\n\nFor the example usage, the output will be:\n```python\n[[3, 5], [5, 7], [7, 9]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "## KL Divergence Between Two Normal Distributions\n\n### Overview\n\nThe Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions. Here, we implement a function to calculate the KL divergence between two normal distributions.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    # Calculate the KL divergence using the formula for normal distributions\n    kl_divergence = np.log(sigma_q / sigma_p) + ((sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)) - 0.5\n    \n    return kl_divergence\n\n# Example usage:\nif __name__ == \"__main__\":\n    mu_p = 0.0\n    sigma_p = 1.0\n    mu_q = 1.0\n    sigma_q = 2.0\n\n    kl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(f\"KL Divergence: {kl_div}\")\n```\n\n### Explanation\n\nThe KL divergence between two normal distributions $P \\sim N(\\mu_P, \\sigma_P^2)$ and $Q \\sim N(\\mu_Q, \\sigma_Q^2)$ is given by:\n\n$$\nD_{KL}(P || Q) = \\log \\frac{\\sigma_Q}{\\sigma_P} + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n$$\n\nThis formula is derived from the definition of KL divergence and the probability density functions of normal distributions.\n\nIn the provided code:\n\n*   We define a function `kl_divergence_normal` that takes the means and standard deviations of two normal distributions as input.\n*   Inside the function, we calculate the KL divergence using the formula.\n*   The result is returned as a floating-point number.\n*   In the example usage section, we demonstrate how to use the function with sample values for the means and standard deviations of two normal distributions."}
{"task_id": 57, "completion_id": 0, "completion": "### Gauss-Seidel Method Implementation\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implement the Gauss-Seidel method for solving a system of linear equations (Ax = b).\n\n    Args:\n    - A (numpy array): A square matrix of coefficients.\n    - b (numpy array): The right-hand side vector.\n    - n (int): The number of iterations.\n    - x_ini (numpy array, optional): An initial guess for the solution vector (x). Defaults to a vector of zeros.\n\n    Returns:\n    - list: The approximated solution vector (x) after performing the specified number of iterations.\n    \"\"\"\n\n    # Get the size of the square matrix A\n    m = len(A)\n\n    # Check if A is a square matrix\n    if any(len(row) != m for row in A):\n        raise ValueError(\"Matrix A must be a square matrix\")\n\n    # Check if b has the correct length\n    if len(b) != m:\n        raise ValueError(\"Vector b must have the same length as the number of rows in A\")\n\n    # Initialize the solution vector x with zeros or the provided initial guess\n    if x_ini is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x_ini)\n\n    # Perform the specified number of iterations\n    for _ in range(n):\n        for i in range(m):\n            # Calculate the sum of the terms involving the current estimate of x\n            s = sum(A[i, j] * x[j] for j in range(m) if j != i)\n\n            # Update the estimate of x using the Gauss-Seidel formula\n            x[i] = (b[i] - s) / A[i, i]\n\n    # Round the solution vector to the nearest 4th decimal and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a sample matrix A and vector b\n    A = np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]])\n    b = np.array([6, 25, -11])\n\n    # Perform 5 iterations of the Gauss-Seidel method\n    x_approx = gauss_seidel(A, b, 5)\n\n    print(\"Approximated solution vector (x):\", x_approx)\n```\n\nIn this code:\n\n*   We define the `gauss_seidel` function, which takes the matrix `A`, vector `b`, number of iterations `n`, and an optional initial guess `x_ini` as inputs.\n*   We validate the inputs to ensure that `A` is a square matrix and `b` has the correct length.\n*   We initialize the solution vector `x` with zeros or the provided initial guess.\n*   We perform the specified number of iterations, updating the estimate of `x` using the Gauss-Seidel formula.\n*   Finally, we round the solution vector to the nearest 4th decimal and return it as a list.\n\nThe example usage demonstrates how to call the `gauss_seidel` function with a sample matrix `A` and vector `b`, performing 5 iterations to obtain an approximated solution vector `x`."}
{"task_id": 58, "completion_id": 0, "completion": "### Gaussian Elimination with Partial Pivoting\n\nHere's a Python function that implements the Gaussian Elimination method with partial pivoting to solve a system of linear equations.\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Gaussian Elimination with partial pivoting to solve the system Ax = b.\n\n    Args:\n    A (numpy array): Coefficient matrix.\n    b (numpy array): Constant matrix.\n\n    Returns:\n    list: Solution vector x.\n    \"\"\"\n\n    # Create a copy of the input matrices to avoid modifying the originals\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n\n    # Check if the input matrices have compatible shapes\n    if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:\n        raise ValueError(\"Incompatible matrix shapes\")\n\n    n = A.shape[0]\n\n    # Create an augmented matrix [A | b]\n    aug_matrix = np.column_stack((A, b))\n\n    # Perform Gaussian Elimination with partial pivoting\n    for i in range(n - 1):\n        # Find the row with the maximum absolute value in the current column\n        max_row = np.argmax(np.abs(aug_matrix[i:, i])) + i\n\n        # Swap the current row with the row having the maximum absolute value\n        aug_matrix[[i, max_row]] = aug_matrix[[max_row, i]]\n\n        # Make the pivot element equal to 1\n        pivot = aug_matrix[i, i]\n        aug_matrix[i] = aug_matrix[i] / pivot\n\n        # Eliminate the pivot variable from the rows below\n        for j in range(i + 1, n):\n            factor = aug_matrix[j, i]\n            aug_matrix[j] = aug_matrix[j] - factor * aug_matrix[i]\n\n    # Perform backward substitution to find the solution vector x\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = aug_matrix[i, -1] - np.dot(aug_matrix[i, :-1], x)\n\n    # Round the solution vector to the nearest 4th decimal and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3, 2], [1, 4]])\n    b = np.array([5, 6])\n    x = gaussian_elimination(A, b)\n    print(\"Solution vector x:\", x)\n```\n\nIn this code:\n\n*   We define a function `gaussian_elimination(A, b)` that takes the coefficient matrix `A` and the constant matrix `b` as input.\n*   We create a copy of the input matrices and check if they have compatible shapes.\n*   We perform Gaussian Elimination with partial pivoting using an augmented matrix `[A | b]`.\n*   We make the pivot element equal to 1 and eliminate the pivot variable from the rows below.\n*   We perform backward substitution to find the solution vector `x`.\n*   We round the solution vector to the nearest 4th decimal and return it as a list.\n\nThe example usage demonstrates how to use the `gaussian_elimination` function to solve a system of linear equations."}
{"task_id": 59, "completion_id": 0, "completion": "Here's a Python implementation of the LSTM network:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # Initialize lists to store hidden states and cell states\n        hidden_states = []\n        cell_states = []\n\n        # Initialize current hidden state and cell state\n        current_hidden_state = initial_hidden_state\n        current_cell_state = initial_cell_state\n\n        # Process each input in the sequence\n        for input_t in x:\n            # Concatenate input and current hidden state\n            concat = np.concatenate((input_t, current_hidden_state), axis=0)\n\n            # Compute forget gate\n            forget_gate = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            # Compute input gate\n            input_gate = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            # Compute candidate cell state\n            candidate_cell_state = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            # Compute output gate\n            output_gate = sigmoid(np.dot(self.Wo, concat) + self.bo)\n\n            # Update cell state\n            current_cell_state = forget_gate * current_cell_state + input_gate * candidate_cell_state\n            # Update hidden state\n            current_hidden_state = output_gate * np.tanh(current_cell_state)\n\n            # Append current hidden state and cell state to lists\n            hidden_states.append(current_hidden_state.reshape(-1, 1).tolist())\n            cell_states.append(current_cell_state.reshape(-1, 1).tolist())\n\n        # Convert lists to numpy arrays and round to 4 decimal places\n        hidden_states = np.round(np.array(hidden_states), 4).tolist()\n        cell_states = np.round(np.array(cell_states), 4).tolist()\n\n        # Return hidden states, final hidden state, and final cell state\n        return hidden_states, current_hidden_state.reshape(-1, 1).tolist(), current_cell_state.reshape(-1, 1).tolist()\n\n\ndef sigmoid(x):\n    \"\"\"\n    Computes the sigmoid of x.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(0)\n\n    # Create an LSTM instance\n    lstm = LSTM(input_size=10, hidden_size=20)\n\n    # Create a sequence of inputs\n    x = [np.random.randn(10, 1) for _ in range(5)]\n\n    # Initialize hidden state and cell state\n    initial_hidden_state = np.random.randn(20, 1)\n    initial_cell_state = np.random.randn(20, 1)\n\n    # Process the sequence of inputs\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    # Print the results\n    print(\"Hidden States:\")\n    for i, state in enumerate(hidden_states):\n        print(f\"Time Step {i+1}: {state}\")\n    print(\"\\nFinal Hidden State:\")\n    print(final_hidden_state)\n    print(\"\\nFinal Cell State:\")\n    print(final_cell_state)\n```\n\nIn this code:\n\n*   We define an `LSTM` class with `__init__` and `forward` methods.\n*   The `__init__` method initializes the LSTM with random weights and zero biases.\n*   The `forward` method processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n*   We use the sigmoid function to compute the gates and tanh function to compute the candidate cell state.\n*   We update the cell state and hidden state using the computed gates and candidate cell state.\n*   We provide an example usage of the `LSTM` class, where we create an instance, process a sequence of inputs, and print the results.\n\nThe output will be the hidden states at each time step, as well as the final hidden state and cell state, all rounded to the nearest 4th decimal place."}
{"task_id": 60, "completion_id": 0, "completion": "### TF-IDF Implementation\n#### Overview\nThe following Python function implements the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm. It calculates the TF-IDF scores for a given query against a corpus of documents.\n\n#### Code\n```python\nimport numpy as np\nfrom collections import Counter\nimport math\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document.\n    \"\"\"\n\n    # Check for empty corpus\n    if not corpus:\n        return []\n\n    # Calculate document frequency (df) for each term in the corpus\n    df = {}\n    for document in corpus:\n        unique_terms = set(document)\n        for term in unique_terms:\n            df[term] = df.get(term, 0) + 1\n\n    # Calculate total number of documents\n    total_documents = len(corpus)\n\n    # Initialize IDF dictionary\n    idf = {}\n    for term in df:\n        # Apply smoothing to avoid division by zero\n        idf[term] = math.log((total_documents + 1) / (df[term] + 1)) + 1\n\n    # Initialize result list\n    tf_idf_scores = []\n\n    # Iterate over each document in the corpus\n    for document in corpus:\n        # Calculate term frequency (tf) for each term in the document\n        tf = Counter(document)\n\n        # Initialize document's TF-IDF scores\n        document_scores = []\n\n        # Iterate over each query term\n        for term in query:\n            # Calculate TF-IDF score\n            tf_score = tf.get(term, 0) / len(document) if document else 0\n            idf_score = idf.get(term, 0)\n            tf_idf_score = tf_score * idf_score\n\n            # Round to five decimal places and append to document scores\n            document_scores.append(round(tf_idf_score, 5))\n\n        # Append document scores to result list\n        tf_idf_scores.append(document_scores)\n\n    # Convert to numpy array, reshape, and convert back to list\n    tf_idf_scores = np.array(tf_idf_scores).tolist()\n\n    return tf_idf_scores\n\n# Example usage\ncorpus = [[\"this\", \"is\", \"a\", \"test\"], [\"this\", \"test\", \"is\", \"only\", \"a\", \"test\"], [\"is\", \"this\", \"a\"]]\nquery = [\"this\", \"is\", \"a\", \"test\"]\nprint(compute_tf_idf(corpus, query))\n```\n\n#### Explanation\n\nThe provided Python function `compute_tf_idf` takes a corpus of documents and a query as input and returns a list of lists containing the TF-IDF scores for the query words in each document.\n\n1.  **Empty Corpus Handling**: The function first checks if the corpus is empty. If it is, the function returns an empty list.\n2.  **Document Frequency (DF) Calculation**: It calculates the document frequency for each term in the corpus by iterating over each document and its unique terms.\n3.  **IDF Calculation**: The IDF is calculated for each term using the document frequency and total number of documents. Smoothing is applied to avoid division by zero.\n4.  **TF-IDF Calculation**: For each document in the corpus, the function calculates the term frequency (TF) for each term and then computes the TF-IDF score by multiplying the TF and IDF scores.\n5.  **Result Formatting**: The TF-IDF scores are rounded to five decimal places and returned as a list of lists.\n\nThe example usage demonstrates how to use the `compute_tf_idf` function with a sample corpus and query."}
{"task_id": 61, "completion_id": 0, "completion": "# F-Score Calculation for Binary Classification\n\n## Implementation\n\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n\n    # Calculate precision\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n\n    # Calculate recall\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n\n    # Calculate f-score\n    f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if precision + recall != 0 else 0\n\n    # Return f-score rounded to three decimal places\n    return round(f_score, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n    y_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n    beta = 1.0\n    print(f\"F-Score (beta={beta}): {f_score(y_true, y_pred, beta)}\")\n```\n\n## Explanation\n\nThe provided Python function `f_score` calculates the F-Score for a binary classification task given the true labels `y_true`, predicted labels `y_pred`, and a weight `beta` that adjusts the importance of Precision and Recall.\n\nHere's a step-by-step breakdown:\n\n1.  **True Positives, False Positives, and False Negatives**: The function calculates the true positives (`tp`), false positives (`fp`), and false negatives (`fn`) by comparing the true labels and predicted labels.\n2.  **Precision**: Precision is calculated as the ratio of true positives to the sum of true positives and false positives. If the denominator is zero, precision is set to 0.\n3.  **Recall**: Recall is calculated as the ratio of true positives to the sum of true positives and false negatives. If the denominator is zero, recall is set to 0.\n4.  **F-Score**: The F-Score is calculated using the formula: `(1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)`. If the denominator is zero, the F-Score is set to 0.\n5.  **Result**: The calculated F-Score is rounded to three decimal places and returned.\n\nThe example usage demonstrates how to use the `f_score` function with sample true labels, predicted labels, and a `beta` value of 1.0, which corresponds to the F1-Score."}
{"task_id": 62, "completion_id": 0, "completion": "**Simple RNN Implementation with Backpropagation Through Time (BPTT)**\n====================================================================\n\n### Code Implementation\n\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = None\n        self.W_hh = None\n        self.W_hy = None\n        self.b_h = None\n        self.b_y = None\n        self.initialize_weights(input_size, hidden_size, output_size)\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with weights set to random values multiplied by 0.01 and biases set to zero.\n\n        Args:\n            input_size (int): The size of the input.\n            hidden_size (int): The size of the hidden state.\n            output_size (int): The size of the output.\n        \"\"\"\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n\n        Args:\n            W_xh (numpy array): Weights from input to hidden state.\n            W_hh (numpy array): Weights from hidden state to hidden state.\n            W_hy (numpy array): Weights from hidden state to output.\n            b_h (numpy array): Bias for hidden state.\n            b_y (numpy array): Bias for output.\n            hidden_size (int): The size of the hidden state.\n            input_sequence (numpy array): A sequence of inputs.\n\n        Returns:\n            outputs (list): A list of outputs at each time step.\n            last_inputs (numpy array): The last input.\n            last_hiddens (numpy array): The last hidden state.\n        \"\"\"\n        # Initialize hidden state and output list\n        h = np.zeros((hidden_size, 1))\n        outputs = []\n\n        # Process input sequence\n        for x in input_sequence:\n            # Forward pass\n            h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n            y = np.dot(W_hy, h) + b_y\n            outputs.append(y)\n\n        # Store last hidden state and input\n        last_hiddens = h\n        last_inputs = input_sequence[-1]\n\n        return outputs, last_inputs, last_hiddens\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n\n        Args:\n            W_xh (numpy array): Weights from input to hidden state.\n            W_hh (numpy array): Weights from hidden state to hidden state.\n            W_hy (numpy array): Weights from hidden state to output.\n            b_h (numpy array): Bias for hidden state.\n            b_y (numpy array): Bias for output.\n            hidden_size (int): The size of the hidden state.\n            input_sequence (numpy array): A sequence of inputs.\n            expected_output (numpy array): The expected output sequence.\n            outputs (list): A list of outputs at each time step.\n            last_inputs (numpy array): The last input.\n            last_hiddens (numpy array): The last hidden state.\n            learning_rate (float): The learning rate for weight updates.\n\n        Returns:\n            W_xh (numpy array): Updated weights from input to hidden state.\n            W_hh (numpy array): Updated weights from hidden state to hidden state.\n            W_hy (numpy array): Updated weights from hidden state to output.\n            b_h (numpy array): Updated bias for hidden state.\n            b_y (numpy array): Updated bias for output.\n        \"\"\"\n        # Initialize gradients\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n\n        # Initialize hidden state and error\n        h = np.zeros((hidden_size, 1))\n        error = 0\n\n        # Backward pass through time\n        for i in range(len(input_sequence) - 1, -1, -1):\n            x = input_sequence[i].reshape(-1, 1)\n            y = outputs[i]\n            expected_y = expected_output[i].reshape(-1, 1)\n\n            # Calculate error\n            error += 0.5 * np.mean((y - expected_y) ** 2)\n\n            # Calculate gradients\n            dy = y - expected_y\n            dh = np.dot(W_hy.T, dy) * (1 - np.tanh(np.dot(W_hh, h) + np.dot(W_xh, x) + b_h) ** 2)\n\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n\n            if i > 0:\n                dW_hh += np.dot(dh, h.T)\n                dW_xh += np.dot(dh, x.T)\n                db_h += dh\n\n            h = np.tanh(np.dot(W_hh, h) + np.dot(W_xh, x) + b_h)\n\n        # Update weights\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n\n        return W_xh, W_hh, W_hy, b_h, b_y\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)\n\n    # Define RNN parameters\n    input_size = 10\n    hidden_size = 20\n    output_size = 10\n    sequence_length = 10\n    learning_rate = 0.01\n\n    # Initialize RNN\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n\n    # Generate random input sequence and expected output\n    input_sequence = [np.random.randn(input_size, 1) for _ in range(sequence_length)]\n    expected_output = [np.random.randn(output_size, 1) for _ in range(sequence_length)]\n\n    # Forward pass\n    outputs, last_inputs, last_hiddens = rnn.rnn_forward(rnn.W_xh, rnn.W_hh, rnn.W_hy, rnn.b_h, rnn.b_y, hidden_size, input_sequence)\n\n    # Backward pass\n    W_xh, W_hh, W_hy, b_h, b_y = rnn.rnn_backward(rnn.W_xh, rnn.W_hh, rnn.W_hy, rnn.b_h, rnn.b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\n    print(\"Updated weights:\")\n    print(\"W_xh:\", W_xh)\n    print(\"W_hh:\", W_hh)\n    print(\"W_hy:\", W_hy)\n    print(\"b_h:\", b_h)\n    print(\"b_y:\", b_y)\n```\n\n### Explanation\n\nThe provided code implements a simple Recurrent Neural Network (RNN) with Backpropagation Through Time (BPTT) for sequence prediction. The RNN consists of the following components:\n\n*   `__init__`: Initializes the RNN with random weights and zero biases.\n*   `initialize_weights`: Initializes the RNN weights and biases.\n*   `rnn_forward`: Performs the forward pass through the RNN for a given sequence of inputs, returning the output, last inputs, and hidden states.\n*   `rnn_backward`: Performs BPTT to adjust the weights based on the loss.\n\nThe example usage demonstrates how to create an RNN instance, generate a random input sequence and expected output, perform the forward and backward passes, and update the weights.\n\nThe loss function used is 1/2 \\* Mean Squared Error (MSE), which is commonly used for regression tasks. The gradients are calculated using the chain rule, and the weights are updated using stochastic gradient descent (SGD) with a specified learning rate.\n\nNote that this is a basic implementation, and you may want to consider additional features such as:\n\n*   Using more advanced optimization algorithms (e.g., Adam, RMSprop)\n*   Regularization techniques (e.g., dropout, L1/L2 regularization)\n*   Handling vanishing or exploding gradients\n*   Using more complex RNN architectures (e.g., LSTM, GRU)"}
{"task_id": 63, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing the Conjugate Gradient (CG) method for solving large, sparse, symmetric, positive-definite linear systems of the form Ax = b, where A is the coefficient matrix, b is the right-hand side vector, and x is the solution vector.\n\nKey constraints and edge cases:\n\n- A must be a symmetric, positive-definite matrix.\n- The method should be efficient for large, sparse systems.\n- The function should take as input: \n  - A (the coefficient matrix)\n  - b (the right-hand side vector)\n  - n (the maximum number of iterations)\n  - x0 (the initial guess for the solution vector, default is None)\n  - tol (the tolerance for the stopping criteria, default is 1e-8)\n- The function should return the solution vector x.\n- The solution vector x should be rounded to 8 decimal places and converted to a list using tolist() when returned.\n\nInput: A, b, n, x0, tol\nOutput: Solution vector x as a list\n\nSubtle requirements or implicit constraints:\n\n- The method should handle cases where A is not symmetric or positive-definite.\n- The method should be able to handle large, sparse matrices efficiently.\n\n# Key Insights\n\n- The Conjugate Gradient method is an iterative algorithm that can be used to solve symmetric, positive-definite linear systems.\n- The method starts with an initial guess for the solution vector x0 and iteratively improves this guess until convergence or until the maximum number of iterations is reached.\n- The method uses the residual r = b - Ax to measure the error in the current estimate of the solution.\n- The method uses the conjugate direction p to update the solution vector x.\n- The method uses the formula x_new = x_old + alpha * p, where alpha is the step size, to update the solution vector x.\n- The method uses the formula r_new = r_old - alpha * A * p to update the residual r.\n- The method checks for convergence using the tolerance tol and the maximum number of iterations n.\n\nCritical examples or test cases:\n\n- A simple example with a small matrix A and vector b to verify the correctness of the method.\n- A test case with a large, sparse matrix A to verify the efficiency of the method.\n\n# Solution Strategy\n\n1. Initialize the solution vector x0, the residual r0, and the conjugate direction p0.\n2. Iterate until convergence or until the maximum number of iterations is reached:\n   - Compute the step size alpha.\n   - Update the solution vector x.\n   - Update the residual r.\n   - Check for convergence.\n   - Update the conjugate direction p.\n3. Return the solution vector x.\n\nCritical details:\n\n- Handle cases where A is not symmetric or positive-definite.\n- Use the formula x_new = x_old + alpha * p to update the solution vector x.\n- Use the formula r_new = r_old - alpha * A * p to update the residual r.\n- Check for convergence using the tolerance tol and the maximum number of iterations n.\n- Round the solution vector x to 8 decimal places and convert it to a list using tolist() when returned.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    m, _ = A.shape\n    if x0 is None:\n        x0 = np.zeros(m)\n    x = x0.copy()\n    r = b - np.dot(A, x)\n    if np.linalg.norm(r) < tol:\n        return np.round(x, 8).tolist()\n    p = r.copy()\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        if np.linalg.norm(r_new) < tol:\n            return np.round(x, 8).tolist()\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        p = r_new + beta * p\n        r = r_new\n    return np.round(x, 8).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "## Gini Impurity Calculation Implementation\n\n### Overview\n\nThe Gini Impurity is a measure of the inequality or dispersion of a distribution. It is commonly used in decision tree algorithms to determine the best split for a node. The Gini Impurity for a set of classes can be calculated using the following formula:\n\nGini Impurity = 1 - \u03a3 (p_i^2)\n\nwhere p_i is the proportion of class i in the set.\n\n### Implementation\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef gini_impurity(y: List[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Get unique classes and their counts\n    classes, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the total number of samples\n    total_samples = len(y)\n    \n    # Calculate the probability of each class\n    probabilities = counts / total_samples\n    \n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(np.square(probabilities))\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y = [0, 0, 1, 1, 1, 2, 2, 2, 2]\n    print(gini_impurity(y))\n```\n\n### Explanation\n\n1. The function `gini_impurity` takes a list of class labels `y` as input.\n2. It uses `np.unique` to get the unique classes and their counts.\n3. The total number of samples is calculated as the length of the input list.\n4. The probability of each class is calculated by dividing the count of each class by the total number of samples.\n5. The Gini Impurity is calculated using the formula 1 - \u03a3 (p_i^2), where p_i is the probability of each class.\n6. The result is rounded to three decimal places and returned.\n\n### Example Use Case\n\nIn the example usage section, we demonstrate how to use the `gini_impurity` function with a sample list of class labels. The output will be the Gini Impurity of the given class distribution."}
{"task_id": 65, "completion_id": 0, "completion": "### Compressed Row Sparse Matrix (CSR) Format Conversion\n\nThe following Python function converts a dense matrix into its Compressed Row Sparse (CSR) representation.\n\n#### Code\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    # Initialize arrays to store CSR format data\n    values = []\n    col_indices = []\n    row_pointers = [0]  # Initialize with 0 for the first row\n\n    # Iterate over each row in the dense matrix\n    for row in dense_matrix:\n        # Find non-zero elements and their column indices in the current row\n        non_zero_elements = [(i, val) for i, val in enumerate(row) if val != 0]\n\n        # Append non-zero values and their column indices to the respective arrays\n        values.extend([val for _, val in non_zero_elements])\n        col_indices = col_indices + [i for i, _ in non_zero_elements]\n\n        # Update the row pointer array\n        row_pointers.append(len(values))\n\n    # Remove the initial 0 from row_pointers and adjust indices\n    values_array = values\n    column_indices_array = col_indices\n    row_pointer_array = row_pointers[1:]\n\n    return (values_array, column_indices_array, row_pointer_array)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [1, 0, 0, 0, 2],\n        [0, 0, 0, 0, 0],\n        [0, 3, 0, 0, 0],\n        [0, 0, 0, 4, 0],\n        [0, 0, 0, 0, 5]\n    ]\n\n    values, col_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\n\n    print(\"Values array:\", values)\n    print(\"Column indices array:\", col_indices)\n    print(\"Row pointer array:\", row_pointers)\n\n```\n\n#### Explanation\n\nThe provided Python function `compressed_row_sparse_matrix` takes a dense matrix as input and returns its Compressed Row Sparse (CSR) representation as a tuple of three lists:\n\n*   **Values array**: A list of all non-zero elements in the matrix, stored in row-major order.\n*   **Column indices array**: A list of column indices corresponding to each non-zero element in the values array.\n*   **Row pointer array**: A list where each element at index `i` represents the cumulative number of non-zero elements up to and including row `i`. This array indicates the start of each row in the values array.\n\nHere's a step-by-step breakdown of the implementation:\n\n1.  Initialize empty lists to store non-zero values, their column indices, and row pointers.\n2.  Iterate over each row in the dense matrix.\n3.  For each row, find non-zero elements and their column indices.\n4.  Append these non-zero values and their column indices to the respective arrays.\n5.  Update the row pointer array by appending the cumulative count of non-zero elements encountered so far.\n6.  Return the CSR representation as a tuple of three lists: `values_array`, `column_indices_array`, and `row_pointer_array`.\n\nThe example usage demonstrates how to use this function with a sample dense matrix and print the resulting CSR arrays.\n\n### Time Complexity\n\nThe time complexity of this function is O(n \\* m), where n is the number of rows and m is the number of columns in the dense matrix. This is because we iterate over each element in the matrix once.\n\n### Space Complexity\n\nThe space complexity is also O(n \\* m) in the worst case (when the matrix is dense), as we store all non-zero elements and their indices in the CSR arrays. However, for sparse matrices, the space complexity is significantly reduced, as we only store non-zero elements and their positions."}
{"task_id": 66, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing a function that calculates the orthogonal projection of a vector **v** onto another vector **L**. This projection results in the vector on **L** that is closest to **v**.\n\nKey constraints and edge cases:\n\n* The function should take two lists, `v` and `L`, as input, representing the vector to be projected and the line vector, respectively.\n* The function should return a list representing the projection vector rounded to three decimal places.\n* The input vectors `v` and `L` are assumed to be lists of numbers.\n* The function should handle the case where **L** is the zero vector, in which case the projection of **v** onto **L** is also the zero vector.\n\nInput: Two lists, `v` and `L`, representing the vector to be projected and the line vector, respectively.\nOutput: A list representing the projection vector rounded to three decimal places.\n\nSubtle requirements or implicit constraints:\n\n* The function should use the formula for orthogonal projection, which involves calculating the dot product of **v** and **L**, and the magnitude of **L**.\n* The function should handle the case where **L** is not a unit vector.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The orthogonal projection of **v** onto **L** can be calculated using the formula: proj_L(v) = (v \u00b7 L / ||L||^2) * L.\n* The dot product **v** \u00b7 **L** can be calculated as the sum of the products of corresponding entries of the two sequences of numbers.\n* The magnitude of **L** can be calculated as the square root of the sum of the squares of its entries.\n* The projection of **v** onto **L** can be calculated by scaling **L** by the scalar factor (v \u00b7 L / ||L||^2).\n* The function should handle the case where **L** is the zero vector.\n\nPatterns or properties being used:\n\n* The formula for orthogonal projection involves calculating the dot product and magnitude of vectors.\n* The function uses the property that the projection of **v** onto **L** is a scalar multiple of **L**.\n\nCritical examples or test cases that led to insights:\n\n* The example where **v** = [2, 3] and **L** = [1, 1] helped to illustrate the calculation of the dot product and magnitude.\n\nImportant realizations:\n\n* The function should use a loop to calculate the dot product and magnitude of vectors.\n* The function should handle the case where **L** is not a unit vector.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Calculate the dot product **v** \u00b7 **L** by summing the products of corresponding entries of the two sequences of numbers.\n2. Calculate the magnitude of **L** by taking the square root of the sum of the squares of its entries.\n3. Check if **L** is the zero vector. If so, return a list of zeros.\n4. Calculate the scalar factor (v \u00b7 L / ||L||^2).\n5. Calculate the projection of **v** onto **L** by scaling **L** by the scalar factor.\n6. Round the projection vector to three decimal places.\n\nCritical details:\n\n* Use a loop to calculate the dot product and magnitude of vectors.\n* Handle the case where **L** is the zero vector.\n* Use the formula for orthogonal projection to calculate the projection vector.\n\nOptimizations or special cases to handle:\n\n* Handle the case where **L** is not a unit vector.\n* Use a efficient method to calculate the dot product and magnitude of vectors.\n\nHere's the python solution:\n```python\nimport math\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_product = sum(x * y for x, y in zip(v, L))\n\n    # Calculate the magnitude squared of L\n    L_magnitude_squared = sum(x ** 2 for x in L)\n\n    # Check if L is the zero vector\n    if L_magnitude_squared == 0:\n        return [0] * len(v)\n\n    # Calculate the projection\n    scalar = dot_product / L_magnitude_squared\n    projection = [scalar * x for x in L]\n\n    # Round the projection to three decimal places\n    projection = [round(x, 3) for x in projection]\n\n    return projection\n\n# Example usage\nv = [2, 3]\nL = [1, 1]\nprint(orthogonal_projection(v, L))\n```"}
{"task_id": 67, "completion_id": 0, "completion": "### Compressed Column Sparse Matrix Format (CSC) Implementation\n\n#### Overview\n\nThe following Python function implements the Compressed Column Sparse (CSC) matrix format. It takes a dense matrix as input and returns a tuple of three lists: `values`, `row_indices`, and `column_pointer`.\n\n#### Code\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    # Initialize lists to store CSC representation\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n\n    # Count non-zero elements in each column\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                column_pointer[col + 1] += 1\n\n    # Calculate cumulative sum for column_pointer\n    for col in range(num_cols):\n        column_pointer[col + 1] += column_pointer[col]\n\n    # Store non-zero elements in CSC format\n    for row in range(num_rows):\n        for col in range(num_cols):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n                # No need to update column_pointer here, we already have the counts\n\n    return values, row_indices, column_pointer\n\n\n# Example usage\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [1, 0, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 3, 0],\n        [0, 0, 0, 4]\n    ]\n\n    values, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\n\n    print(\"Values:\", values)\n    print(\"Row Indices:\", row_indices)\n    print(\"Column Pointer:\", column_pointer)\n\n```\n\n#### Explanation\n\nThe provided function `compressed_col_sparse_matrix` works as follows:\n\n1.  **Initialization**: It first determines the number of rows and columns in the input dense matrix.\n2.  **CSC Lists Initialization**: It initializes three lists: `values` to store non-zero elements, `row_indices` to store the corresponding row indices, and `column_pointer` to store the starting index of each column in the `values` array.\n3.  **Non-Zero Element Counting**: It counts the number of non-zero elements in each column and stores these counts in `column_pointer`.\n4.  **Cumulative Sum Calculation**: It calculates the cumulative sum of the counts in `column_pointer` to get the actual starting indices for each column.\n5.  **CSC Representation Construction**: It then iterates over the dense matrix, appending non-zero elements to `values` and their row indices to `row_indices`.\n6.  **Return CSC Representation**: Finally, it returns the CSC representation as a tuple of `values`, `row_indices`, and `column_pointer`.\n\nThe example usage demonstrates how to use the function with a sample dense matrix and prints the resulting CSC representation."}
{"task_id": 68, "completion_id": 0, "completion": "## Computing the Column Space of a Matrix Using Row Echelon Form\n\n### Task Description\n\nThe task is to implement a function `matrix_image(A)` that calculates the column space of a given matrix `A`. The column space consists of all linear combinations of the columns of `A`. This will be achieved by using concepts from linear algebra, focusing on identifying independent columns that span the matrix's image.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a given matrix A.\n\n    Parameters:\n    A (numpy.ndarray): Input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n\n    # Ensure A is a numpy array\n    A = np.array(A, dtype=float)\n\n    # Perform row reduction to get the row echelon form\n    A_rref, pivots = row_echelon_form(A)\n\n    # Identify the basis vectors from the original matrix\n    basis_vectors = A[:, pivots]\n\n    # Round the basis vectors to 8 decimal places and convert to a list\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n\n    return basis_vectors\n\n\ndef row_echelon_form(A):\n    \"\"\"\n    Compute the row echelon form of a given matrix A.\n\n    Parameters:\n    A (numpy.ndarray): Input matrix.\n\n    Returns:\n    tuple: A tuple containing the row echelon form of A and the pivot column indices.\n    \"\"\"\n\n    num_rows, num_cols = A.shape\n    current_row = 0\n    pivots = []\n\n    A = np.copy(A)  # Create a copy to avoid modifying the original matrix\n\n    for j in range(num_cols):\n        if current_row >= num_rows:\n            break\n\n        max_el = abs(A[current_row, j])\n        max_row = current_row\n        for i in range(current_row + 1, num_rows):\n            if abs(A[i, j]) > max_el:\n                max_el = abs(A[i, j])\n                max_row = i\n\n        if max_el == 0:\n            continue\n\n        # Swap current row with the row having the maximum element\n        A[[current_row, max_row]] = A[[max_row, current_row]]\n\n        # Normalize the current row\n        A[current_row] = A[current_row] / A[current_row, j]\n\n        pivots.append(j)\n\n        # Eliminate the elements below the current pivot\n        for i in range(num_rows):\n            if i != current_row:\n                A[i] = A[i] - A[i, j] * A[current_row]\n\n        current_row += 1\n\n    return A, pivots\n\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    print(\"Matrix A:\")\n    print(A)\n\n    basis_vectors = matrix_image(A)\n    print(\"\\nBasis Vectors for the Column Space of A:\")\n    print(basis_vectors)\n```\n\n### Explanation\n\nThe provided code defines a function `matrix_image(A)` that calculates the column space of a given matrix `A`. It uses the row echelon form (REF) of the matrix to identify the independent columns, which form the basis of the column space.\n\nHere's a step-by-step breakdown:\n\n1.  The `row_echelon_form(A)` function computes the REF of matrix `A` and returns the REF along with the pivot column indices.\n2.  The `matrix_image(A)` function uses the pivot column indices to extract the corresponding columns from the original matrix `A`. These columns form the basis vectors of the column space.\n3.  The basis vectors are rounded to 8 decimal places and converted to a list before being returned.\n\nThe code includes an example usage section that demonstrates how to use the `matrix_image(A)` function with a sample matrix `A`. The output shows the basis vectors that span the column space of `A`."}
{"task_id": 69, "completion_id": 0, "completion": "## Calculating R-squared for Regression Analysis\n\n### Overview\n\nThe R-squared value, also known as the coefficient of determination, measures how well the independent variables explain the variability of the dependent variable in a regression model. This implementation provides a function `r_squared(y_true, y_pred)` that calculates the R-squared value given arrays of true values `y_true` and predicted values `y_pred`.\n\n### Code\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate the mean of true values\n    mean_y_true = np.mean(y_true)\n\n    # Calculate the total sum of squares (SST)\n    sst = np.sum((y_true - mean_y_true) ** 2)\n\n    # Calculate the residual sum of squares (SSE)\n    sse = np.sum((y_true - y_pred) ** 2)\n\n    # Check for division by zero\n    if sst == 0:\n        return 1.0  # or raise an exception, depending on the requirements\n\n    # Calculate the R-squared value\n    r2 = 1 - (sse / sst)\n\n    # Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    print(r_squared(y_true, y_pred))\n```\n\n### Explanation\n\n1. The function `r_squared(y_true, y_pred)` takes two parameters: `y_true` (true values) and `y_pred` (predicted values).\n2. It calculates the mean of the true values `y_true`.\n3. The total sum of squares (SST) measures the total variance in the true values.\n4. The residual sum of squares (SSE) measures the variance between the true and predicted values.\n5. The R-squared value is then calculated as 1 - (SSE / SST), which represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n6. The function returns the R-squared value rounded to three decimal places.\n\n### Note\n\n* This implementation assumes that the input arrays `y_true` and `y_pred` have the same length.\n* It also assumes that the input arrays are not empty and that SST is not zero to avoid division by zero. If SST is zero, the function returns 1.0, indicating a perfect fit. You may want to handle this case differently based on your specific requirements."}
{"task_id": 70, "completion_id": 0, "completion": "### Image Brightness Calculator\n#### Function Description\n\nThe `calculate_brightness` function calculates the average brightness of a grayscale image represented as a 2D matrix.\n\n#### Code Implementation\n\n```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n    img (list): A 2D list representing the image, where each element is a pixel value.\n\n    Returns:\n    float: The average brightness of the image, rounded to two decimal places.\n    If the image is empty, has inconsistent row lengths, or contains invalid pixel values, returns -1.\n    \"\"\"\n\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n\n    # Check if all rows have the same length\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) != 1:\n        return -1\n\n    # Initialize sum of pixel values and count of pixels\n    total_brightness = 0\n    pixel_count = 0\n\n    # Iterate over each pixel in the image\n    for row in img:\n        for pixel in row:\n            # Check if pixel value is within the valid range (0-255)\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n\n    # Calculate average brightness\n    average_brightness = total_brightness / pixel_count\n\n    # Return average brightness rounded to two decimal places\n    return round(average_brightness, 2)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test case 1: Valid image\n    img = [\n        [10, 20, 30],\n        [40, 50, 60],\n        [70, 80, 90]\n    ]\n    print(calculate_brightness(img))\n\n    # Test case 2: Empty image\n    img_empty = []\n    print(calculate_brightness(img_empty))\n\n    # Test case 3: Image with inconsistent row lengths\n    img_inconsistent = [\n        [10, 20, 30],\n        [40, 50],\n        [60, 70, 80]\n    ]\n    print(calculate_brightness(img_inconsistent))\n\n    # Test case 4: Image with invalid pixel values\n    img_invalid = [\n        [10, 20, 30],\n        [40, -50, 60],\n        [70, 80, 90]\n    ]\n    print(calculate_brightness(img_invalid))\n```\n\n### Explanation\n\nThe provided Python function `calculate_brightness` calculates the average brightness of a grayscale image represented as a 2D matrix.\n\n1.  **Edge Case Handling**: The function first checks for edge cases:\n    *   If the image matrix is empty, it returns `-1`.\n    *   If the rows in the matrix have inconsistent lengths, it returns `-1`.\n2.  **Pixel Value Validation**: It then iterates over each pixel in the image, checking if the pixel value is within the valid range of 0 to 255. If any pixel value is outside this range, it returns `-1`.\n3.  **Average Brightness Calculation**: If the image is valid, it calculates the sum of all pixel values and counts the total number of pixels.\n4.  **Result**: Finally, it returns the average brightness of the image, rounded to two decimal places.\n\nThe example usage demonstrates the function's behavior with different test cases, including a valid image, an empty image, an image with inconsistent row lengths, and an image with invalid pixel values."}
{"task_id": 71, "completion_id": 0, "completion": "## Root Mean Square Error (RMSE) Implementation\n### Overview\n\nThe following Python function calculates the Root Mean Square Error (RMSE) between actual values (`y_true`) and predicted values (`y_pred`). It handles edge cases such as mismatched array shapes, empty arrays, and invalid input types.\n\n### Code\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Args:\n        y_true (array-like): Array of actual values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: RMSE value rounded to three decimal places.\n\n    Raises:\n        ValueError: If input arrays have mismatched shapes or are empty.\n        TypeError: If input types are invalid.\n    \"\"\"\n\n    # Check if inputs are numpy arrays or can be converted to numpy arrays\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except Exception as e:\n        raise TypeError(\"Invalid input type. Both inputs must be array-like.\") from e\n\n    # Check if arrays are not empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n\n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Calculate RMSE\n    n = y_true.size\n    differences = y_true - y_pred\n    mse = np.mean(differences ** 2)\n    rmse_value = np.sqrt(mse)\n\n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.0])\n    print(rmse(y_true, y_pred))\n```\n\n### Explanation\n\n1.  The function `rmse(y_true, y_pred)` takes two array-like inputs, `y_true` and `y_pred`, representing actual and predicted values, respectively.\n2.  It first checks if the inputs can be converted to numpy arrays. If not, it raises a `TypeError`.\n3.  It then checks if the input arrays are not empty and have the same shape, raising a `ValueError` if either condition is not met.\n4.  The RMSE is calculated using the formula: $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2}$, where $n$ is the number of observations.\n5.  Finally, the function returns the RMSE value rounded to three decimal places.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `rmse` function with sample actual and predicted values. You can replace `y_true` and `y_pred` with your own data to calculate the RMSE."}
{"task_id": 72, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing a function `jaccard_index(y_true, y_pred)` that calculates the Jaccard Index, a measure of similarity between two binary sets. The Jaccard Index is widely used in binary classification tasks to evaluate the overlap between predicted and true labels.\n\nKey constraints and edge cases:\n\n* The input arrays `y_true` and `y_pred` are binary arrays of the same length.\n* The function should handle cases where there is no overlap between `y_true` and `y_pred`.\n* The function should handle cases where both arrays contain only zeros.\n* The result should be rounded to three decimal places.\n\nInput: Two binary arrays `y_true` and `y_pred` of the same length.\nOutput: The Jaccard Index as a float value.\n\nSubtle requirements or implicit constraints:\n\n* The Jaccard Index is defined as the number of elements in the intersection of `y_true` and `y_pred` divided by the number of elements in the union of `y_true` and `y_pred`.\n* The result ranges from 0 (no overlap) to 1 (perfect overlap).\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The Jaccard Index can be calculated using the formula: Jaccard Index = (Number of elements in the intersection of `y_true` and `y_pred`) / (Number of elements in the union of `y_true` and `y_pred`).\n* The intersection of `y_true` and `y_pred` can be calculated using the element-wise logical AND operation.\n* The union of `y_true` and `y_pred` can be calculated using the element-wise logical OR operation.\n* The number of elements in the intersection and union can be calculated using the `sum` function.\n* The Jaccard Index can be calculated directly using the formula, and then rounded to three decimal places.\n\nImportant realizations:\n\n* The function should handle the edge case where both arrays contain only zeros.\n* The function should handle the case where there is no overlap between `y_true` and `y_pred`.\n\nCritical examples or test cases:\n\n* `y_true = [1, 1, 0, 1]` and `y_pred = [1, 0, 1, 1]`.\n* `y_true = [0, 0, 0, 0]` and `y_pred = [0, 0, 0, 0]`.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Check if the input arrays `y_true` and `y_pred` are of the same length.\n2. Calculate the intersection of `y_true` and `y_pred` using the element-wise logical AND operation.\n3. Calculate the union of `y_true` and `y_pred` using the element-wise logical OR operation.\n4. Calculate the number of elements in the intersection and union using the `sum` function.\n5. Calculate the Jaccard Index using the formula: Jaccard Index = (Number of elements in the intersection) / (Number of elements in the union).\n6. Handle the edge case where both arrays contain only zeros by returning 1.0.\n7. Handle the case where there is no overlap between `y_true` and `y_pred` by returning 0.0.\n8. Round the result to three decimal places.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth target values.\n    y_pred (array-like): Estimated targets as returned by a classifier.\n\n    Returns:\n    float: Jaccard Index\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check if inputs are of the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Inputs must be of the same length.\")\n\n    # Calculate intersection and union\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n\n    # Handle edge case where both arrays contain only zeros\n    if union == 0:\n        return 1.0\n\n    # Calculate Jaccard Index\n    jaccard_idx = intersection / union\n\n    return round(jaccard_idx, 3)\n```"}
{"task_id": 73, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing a function `dice_score(y_true, y_pred)` that calculates the Dice Score, also known as the S\u00f8rensen-Dice coefficient or F1-score, for binary classification. The Dice Score measures the similarity between two sets and is particularly useful in tasks like image segmentation and binary classification.\n\nKey constraints and edge cases:\n\n* `y_true` and `y_pred` are binary arrays of the same length, representing true and predicted labels.\n* The result ranges from 0 (no overlap) to 1 (perfect overlap).\n* Handle edge cases appropriately, such as when there are no true or predicted positives.\n\nInput: Two binary arrays `y_true` and `y_pred` of the same length.\nOutput: The Dice Score as a float value rounded to 3 decimal places.\n\nSubtle requirements or implicit constraints:\n\n* The function should handle cases where there are no true or predicted positives.\n* The input arrays `y_true` and `y_pred` are assumed to be binary, containing only 0s and 1s.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The Dice Score can be calculated using the formula: \\( Dice Score = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}} \\)\n* The intersection of `y_true` and `y_pred` can be calculated as the sum of the element-wise product of `y_true` and `y_pred`.\n* The number of elements in `y_true` and `y_pred` can be calculated as the sum of `y_true` and `y_pred`, respectively.\n* The function should handle edge cases where there are no true or predicted positives.\n\nPatterns or properties being used:\n\n* The element-wise product of two binary arrays is used to calculate the intersection.\n* The sum of a binary array is used to calculate the number of elements in the array.\n\nCritical examples or test cases that led to insights:\n\n* Example with `y_true = [1, 1, 0, 0]` and `y_pred = [1, 0, 1, 0]`.\n* Example with `y_true = [0, 0, 0, 0]` and `y_pred = [0, 0, 0, 0]`.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Ensure input arrays `y_true` and `y_pred` are numpy arrays.\n2. Calculate the intersection of `y_true` and `y_pred` as the sum of the element-wise product of `y_true` and `y_pred`.\n3. Calculate the number of elements in `y_true` and `y_pred` as the sum of `y_true` and `y_pred`, respectively.\n4. Check for edge cases where there are no true or predicted positives.\n5. Calculate the Dice Score using the formula.\n6. Return the Dice Score as a float value rounded to 3 decimal places.\n\nImportant optimizations or special cases to handle:\n\n* Handle edge cases where there are no true or predicted positives by checking if the denominator in the Dice Score formula is zero.\n* Use numpy's vectorized operations to efficiently calculate the intersection and number of elements in the arrays.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    if sum_true + sum_pred == 0:\n        return 1.0  # or handle this case according to your specific requirements, but logically it makes sense to return 1.0 as there's perfect overlap (both are 0)\n    else:\n        dice = (2 * intersection) / (sum_true + sum_pred)\n        return round(dice, 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "### Composite Hypervector Generation for a Dataset Row\n\n#### Overview\n\nThe following Python function generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). This function takes a dataset row, the dimensionality of the hypervectors, and a dictionary of random seeds as input. It returns a composite hypervector representing the entire row.\n\n#### Implementation\n\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    - row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    - composite_hv (list): A composite hypervector representing the entire row.\n    \"\"\"\n\n    # Initialize the composite hypervector with zeros\n    composite_hv = np.zeros(dim)\n\n    # Iterate over each feature in the row\n    for feature, value in row.items():\n        # Set the random seed for the feature\n        np.random.seed(random_seeds[feature])\n\n        # Generate a hypervector for the feature name\n        feature_hv = np.random.choice([-1, 1], size=dim)\n\n        # Generate a hypervector for the feature value\n        if isinstance(value, str):  # For categorical values\n            value_hv = np.random.choice([-1, 1], size=dim)\n        else:  # For numerical values\n            # Quantize the value into a binary vector\n            quantized_value = np.zeros(dim)\n            index = int((value / (max(row.values()) + 1)) * dim)\n            quantized_value[index] = 1\n            value_hv = np.where(quantized_value == 1, 1, -1)\n\n        # Bind the feature and value hypervectors\n        bound_hv = np.multiply(feature_hv, value_hv)\n\n        # Bundle the bound hypervector with the composite hypervector\n        composite_hv = np.add(composite_hv, bound_hv)\n\n    # Normalize the composite hypervector\n    composite_hv = np.sign(composite_hv)\n\n    # Return the composite hypervector as a list\n    return composite_hv.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    row = {\"Feature1\": 0.5, \"Feature2\": \"CategoryA\"}\n    dim = 10\n    random_seeds = {\"Feature1\": 42, \"Feature2\": 24}\n    print(create_row_hv(row, dim, random_seeds))\n```\n\n#### Explanation\n\n*   The function `create_row_hv` takes a dataset row, the dimensionality of the hypervectors, and a dictionary of random seeds as input.\n*   For each feature in the row, it generates a hypervector for the feature name and its value using the provided random seeds.\n*   The hypervectors for the feature name and value are bound together using element-wise multiplication.\n*   The bound hypervectors are then bundled to create a composite hypervector for the row using element-wise addition.\n*   The composite hypervector is normalized by taking the sign of each element.\n*   Finally, the composite hypervector is returned as a list.\n\nThis implementation provides a clear and efficient way to generate composite hypervectors for dataset rows using Hyperdimensional Computing. The use of random seeds ensures reproducibility of the hypervectors."}
{"task_id": 75, "completion_id": 0, "completion": "## Confusion Matrix Generation for Binary Classification\n\n### Overview\n\nThe following Python function generates a confusion matrix for a binary classification problem. The confusion matrix is a 2x2 table that summarizes the predictions against the actual outcomes.\n\n### Code\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n    - data (list): A list of lists, where each inner list represents a pair [y_true, y_pred] for one observation.\n\n    Returns:\n    - A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n\n    # Initialize the confusion matrix with zeros\n    matrix = [[0, 0], [0, 0]]\n\n    # Count the occurrences of each [y_true, y_pred] pair\n    counts = Counter(tuple(pair) for pair in data)\n\n    # Populate the confusion matrix\n    matrix[0][0] = counts.get((0, 0), 0)  # TN (True Negatives)\n    matrix[0][1] = counts.get((0, 1), 0)  # FP (False Positives)\n    matrix[1][0] = counts.get((1, 0), 0)  # FN (False Negatives)\n    matrix[1][1] = counts.get((1, 1), 0)  # TP (True Positives)\n\n    return matrix\n\n# Example usage\nif __name__ == \"__main__\":\n    data = [[0, 0], [1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 0], [1, 1]]\n    print(confusion_matrix(data))\n```\n\n### Explanation\n\n*   The function `confusion_matrix` takes a list of pairs `[y_true, y_pred]` as input.\n*   It initializes a 2x2 confusion matrix with zeros.\n*   The `Counter` class from the `collections` module is used to count the occurrences of each `[y_true, y_pred]` pair in the input data.\n*   The counts are then used to populate the confusion matrix:\n\n    *   `matrix[0][0]`: True Negatives (TN) - correctly predicted as 0\n    *   `matrix[0][1]`: False Positives (FP) - incorrectly predicted as 1\n    *   `matrix[1][0]`: False Negatives (FN) - incorrectly predicted as 0\n    *   `matrix[1][1]`: True Positives (TP) - correctly predicted as 1\n\nThe resulting confusion matrix provides a summary of the prediction results, allowing for the evaluation of the classification model's performance."}
{"task_id": 76, "completion_id": 0, "completion": "## Cosine Similarity Implementation\n### Overview\n\nThe cosine similarity between two vectors is a measure of their directional similarity, often used in text analysis and information retrieval. It is defined as the dot product of the vectors divided by the product of their magnitudes.\n\n### Code\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    - v1 (numpy array): The first input vector.\n    - v2 (numpy array): The second input vector.\n\n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n\n    Raises:\n    - ValueError: If the input vectors have different shapes, are empty, or have zero magnitude.\n    \"\"\"\n\n    # Check if input vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape\")\n\n    # Check if input vectors are not empty\n    if len(v1) == 0 or len(v2) == 0:\n        raise ValueError(\"Input vectors cannot be empty\")\n\n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n\n    # Calculate the magnitudes of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n\n    # Check if input vectors have non-zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude\")\n\n    # Calculate the cosine similarity\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n\n    # Return the cosine similarity rounded to three decimal places\n    return round(similarity, 3)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n\n    try:\n        similarity = cosine_similarity(v1, v2)\n        print(f\"Cosine similarity: {similarity}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```\n\n### Explanation\n\n*   The function `cosine_similarity(v1, v2)` takes two numpy arrays `v1` and `v2` as input and returns their cosine similarity.\n*   It first checks if the input vectors have the same shape and are not empty. If not, it raises a `ValueError`.\n*   Then, it calculates the dot product of the vectors using `np.dot(v1, v2)`.\n*   The magnitudes of the vectors are calculated using `np.linalg.norm(v1)` and `np.linalg.norm(v2)`.\n*   It checks if the input vectors have non-zero magnitude. If not, it raises a `ValueError`.\n*   Finally, it calculates the cosine similarity by dividing the dot product by the product of the magnitudes and returns it rounded to three decimal places."}
{"task_id": 77, "completion_id": 0, "completion": "# Performance Metrics Calculation for Binary Classification\n\n## Overview\n\nThe following Python function calculates various performance metrics for a binary classification problem. It takes in two lists: `actual` and `predicted`, which represent the actual class labels and the predicted class labels from the model, respectively.\n\n## Implementation\n\n```python\nfrom collections import Counter\nfrom sklearn.metrics import f1_score\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Computes various performance metrics for a binary classification problem.\n\n    Args:\n    - actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    - predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    - tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"Both lists must have the same length\")\n\n    # Check if all elements in the lists are either 0 or 1\n    if not all(label in [0, 1] for label in actual + predicted):\n        raise ValueError(\"All elements in the lists must be either 0 or 1\")\n\n    # Create a confusion matrix\n    tp = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n    tn = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n    fp = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n    fn = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n\n    confusion_matrix = [[tn, fp], [fn, tp]]\n\n    # Calculate accuracy\n    accuracy = round((tp + tn) / len(actual), 3)\n\n    # Calculate F1 score\n    f1 = f1_score(actual, predicted, zero_division=0)\n    f1_score = round(f1, 3)\n\n    # Calculate specificity\n    specificity = round(tn / (tn + fp) if (tn + fp) != 0 else 0, 3)\n\n    # Calculate negative predictive value\n    npv = tn / (tn + fn) if (tn + fn) != 0 else 0\n    negative_predictive_value = round(npv, 3)\n\n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\nif __name__ == \"__main__\":\n    actual = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n    predicted = [1, 0, 1, 0, 0, 1, 1, 1, 0, 1]\n\n    metrics = performance_metrics(actual, predicted)\n    print(\"Confusion Matrix:\", metrics[0])\n    print(\"Accuracy:\", metrics[1])\n    print(\"F1 Score:\", metrics[2])\n    print(\"Specificity:\", metrics[3])\n    print(\"Negative Predictive Value:\", metrics[4])\n```\n\n## Explanation\n\nThe function `performance_metrics` calculates the following performance metrics:\n\n*   Confusion Matrix: A 2x2 matrix that summarizes the predictions against the actual outcomes.\n*   Accuracy: The proportion of correct predictions out of total predictions.\n*   F1 Score: The harmonic mean of precision and recall.\n*   Specificity: The proportion of true negatives out of all actual negatives.\n*   Negative Predictive Value (NPV): The proportion of true negatives out of all predicted negatives.\n\nThe function first checks if both input lists have the same length and contain only 0s and 1s. It then creates a confusion matrix by counting the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\nThe accuracy, F1 score, specificity, and negative predictive value are calculated using these counts and rounded to three decimal places.\n\nThe example usage demonstrates how to call the function with sample `actual` and `predicted` lists and print the resulting performance metrics."}
{"task_id": 78, "completion_id": 0, "completion": "### Descriptive Statistics Calculator\n\nHere's a Python function that calculates various descriptive statistics metrics for a given dataset.\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics metrics for a given dataset.\n\n    Args:\n    data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation,\n          percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n\n    # Ensure data is a NumPy array\n    data = np.asarray(data)\n\n    # Check if data contains at least one element\n    if data.size == 0:\n        raise ValueError(\"Input data is empty\")\n\n    # Calculate statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = np.subtract(*np.percentile(data, [75, 25]))\n\n    # Create output dictionary\n    output = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(mode, 4),\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles[0], 4),\n        '50th_percentile': round(percentiles[1], 4),\n        '75th_percentile': round(percentiles[2], 4),\n        'interquartile_range': round(iqr, 4)\n    }\n\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    stats_dict = descriptive_statistics(data)\n    for key, value in stats_dict.items():\n        print(f\"{key.capitalize().replace('_', ' ')}: {value}\")\n```\n\n### Explanation:\n\n*   The function `descriptive_statistics` takes a list or NumPy array `data` as input and returns a dictionary containing various descriptive statistics metrics.\n*   It first converts the input data to a NumPy array and checks if it's not empty.\n*   Then, it calculates the mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR) using NumPy and SciPy functions.\n*   The calculated statistics are stored in a dictionary `output` with rounded values to four decimal places.\n*   Finally, the function returns the `output` dictionary.\n\n### Notes:\n\n*   This implementation assumes that the input data is a list or NumPy array of numerical values. If the input data contains non-numerical values, you may need to add error handling or data cleaning steps.\n*   The `stats.mode` function from SciPy is used to calculate the mode, which returns the smallest mode if there are multiple modes.\n*   The example usage demonstrates how to call the `descriptive_statistics` function with a sample dataset and print the resulting statistics."}
{"task_id": 79, "completion_id": 0, "completion": "## Binomial Distribution Probability\n### Description\nThe following Python function calculates the probability of achieving exactly k successes in n independent Bernoulli trials, each with probability p of success, using the Binomial distribution formula.\n\n### Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binomial_coefficient = math.comb(n, k)\n    \n    # Calculate the probability of k successes and (n-k) failures\n    probability = binomial_coefficient * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    n = 10  # Total number of trials\n    k = 5   # Number of successes\n    p = 0.5 # Probability of success on each trial\n    \n    probability = binomial_probability(n, k, p)\n    print(f\"The probability of {k} successes in {n} trials with p = {p} is: {probability}\")\n```\n\n### Explanation\n\n*   The function `binomial_probability` takes three parameters: `n` (total number of trials), `k` (number of successes), and `p` (probability of success on each trial).\n*   It calculates the binomial coefficient (`n choose k`) using the `math.comb` function.\n*   Then, it calculates the probability of k successes and (n-k) failures using the Binomial distribution formula.\n*   Finally, it returns the probability rounded to five decimal places.\n\n### Notes\n\n*   The `math.comb` function is available in Python 3.8 and later. If you're using an earlier version, you can calculate the binomial coefficient using the formula `math.factorial(n) / (math.factorial(k) * math.factorial(n - k))`.\n*   The example usage demonstrates how to call the `binomial_probability` function with specific values for `n`, `k`, and `p`, and prints the calculated probability."}
{"task_id": 80, "completion_id": 0, "completion": "### Normal Distribution PDF Calculator\n\n#### Overview\n\nThe following Python function calculates the probability density function (PDF) of the normal distribution for a given value, mean, and standard deviation.\n\n#### Code\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be greater than zero.\")\n\n    # Calculate the exponent\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n\n    # Calculate the coefficient\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n\n    # Calculate the PDF\n    pdf = coefficient * math.exp(exponent)\n\n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf, 5)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 0.0\n    mean = 0.0\n    std_dev = 1.0\n\n    pdf_value = normal_pdf(x, mean, std_dev)\n    print(f\"The PDF value at x={x}, mean={mean}, std_dev={std_dev} is: {pdf_value}\")\n```\n\n#### Explanation\n\n*   The function `normal_pdf` takes three parameters: `x`, `mean`, and `std_dev`, which represent the value at which the PDF is evaluated, the mean of the distribution, and the standard deviation of the distribution, respectively.\n*   It first checks if the standard deviation is less than or equal to zero and raises a `ValueError` if true, as the standard deviation must be greater than zero.\n*   The function then calculates the exponent and coefficient of the normal distribution PDF formula.\n*   Finally, it calculates the PDF value using the formula, rounds it to 5 decimal places, and returns the result.\n\n#### Mathematical Formula\n\nThe mathematical formula for the normal distribution PDF is:\n\n`f(x | \u03bc, \u03c3) = (1 / (\u03c3 * sqrt(2 * \u03c0))) * exp(-((x - \u03bc)^2) / (2 * \u03c3^2))`\n\nwhere:\n\n*   `f(x | \u03bc, \u03c3)` is the PDF value at `x`\n*   `\u03bc` is the mean of the distribution\n*   `\u03c3` is the standard deviation of the distribution\n*   `x` is the value at which the PDF is evaluated\n*   `exp` is the exponential function\n*   `\u03c0` is the mathematical constant pi\n\nThis formula is directly implemented in the provided Python function."}
{"task_id": 81, "completion_id": 0, "completion": "### Poisson Distribution Probability Calculator\n\nHere's a Python function that calculates the probability of observing exactly `k` events in a fixed interval using the Poisson distribution formula.\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    if lam < 0:\n        raise ValueError(\"lam must be a non-negative number\")\n\n    # Calculate the probability using the Poisson distribution formula\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    k = 5  # Number of events\n    lam = 2.5  # Mean rate of occurrences\n    print(f\"The probability of observing exactly {k} events is: {poisson_probability(k, lam)}\")\n```\n\n### Explanation\n\n*   The function `poisson_probability` takes two parameters: `k` (the number of events) and `lam` (the mean rate of occurrences).\n*   It first checks if `k` is a non-negative integer and `lam` is a non-negative number, raising a `ValueError` if either condition is not met.\n*   The Poisson distribution formula is then applied to calculate the probability: `P(k; lam) = (e^(-lam) * (lam^k)) / k!`, where `e` is the base of the natural logarithm.\n*   Finally, the calculated probability is rounded to 5 decimal places using the `round` function and returned as the result.\n\n### Usage\n\nYou can use this function by calling it with the desired values for `k` and `lam`, as shown in the example usage section. The function will return the probability of observing exactly `k` events given the mean rate `lam`."}
{"task_id": 82, "completion_id": 0, "completion": "## Grayscale Image Contrast Calculator\n### Description\nThis Python function calculates the contrast of a grayscale image using the difference between the maximum and minimum pixel values.\n\n### Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the grayscale image.\n    \n    Raises:\n        ValueError: If the input image is not a 2D numpy array or if pixel values are not between 0 and 255.\n    \"\"\"\n    # Check if the input is a 2D numpy array\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError(\"Input image must be a 2D numpy array\")\n    \n    # Check if pixel values are between 0 and 255\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError(\"Pixel values must be between 0 and 255\")\n    \n    # Calculate the contrast as the difference between the maximum and minimum pixel values\n    contrast = np.max(img) - np.min(img)\n    \n    return contrast\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample grayscale image\n    img = np.array([\n        [10, 20, 30],\n        [40, 50, 60],\n        [70, 80, 90]\n    ], dtype=np.uint8)\n    \n    # Calculate and print the contrast\n    contrast = calculate_contrast(img)\n    print(f\"Contrast: {contrast}\")\n```\n\n### Explanation\n\n*   The function `calculate_contrast` takes a 2D numpy array `img` as input, representing a grayscale image.\n*   It first checks if the input is a 2D numpy array and if pixel values are between 0 and 255, raising a `ValueError` if either condition is not met.\n*   The contrast is then calculated as the difference between the maximum and minimum pixel values using `np.max` and `np.min`.\n*   Finally, the function returns the calculated contrast.\n\n### Example Usage\n\nThe example usage demonstrates how to create a sample grayscale image, calculate its contrast using the `calculate_contrast` function, and print the result."}
{"task_id": 83, "completion_id": 0, "completion": "### Dot Product Calculator\n#### Description\nThis Python function calculates the dot product of two vectors using NumPy.\n\n#### Code\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two input vectors.\n\n    Raises:\n        ValueError: If either of the input vectors is not a 1D NumPy array.\n        ValueError: If the input vectors have different lengths.\n    \"\"\"\n    # Check if inputs are 1D NumPy arrays\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise ValueError(\"Both inputs must be NumPy arrays.\")\n    if len(vec1.shape) != 1 or len(vec2.shape) != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n\n    # Check if input vectors have the same length\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    # Calculate and return the dot product\n    return np.dot(vec1, vec2)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n\n    dot_product = calculate_dot_product(vec1, vec2)\n    print(f\"The dot product of {vec1} and {vec2} is: {dot_product}\")\n```\n\n#### Explanation\n\n*   The function `calculate_dot_product` takes two 1D NumPy arrays `vec1` and `vec2` as input and returns their dot product.\n*   It first checks if both inputs are 1D NumPy arrays and raises a `ValueError` if not.\n*   Then, it checks if the input vectors have the same length and raises a `ValueError` if they don't.\n*   Finally, it uses NumPy's `np.dot` function to calculate and return the dot product of the two input vectors.\n\n#### Example Use Case\n\nThe example usage demonstrates how to use the `calculate_dot_product` function with two sample vectors `[1, 2, 3]` and `[4, 5, 6]`. The output will be the dot product of these two vectors, which is `1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32`."}
{"task_id": 84, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing a Phi Transformation function that maps input features into a higher-dimensional space by generating polynomial features. The function should take a list of numerical data and a degree as inputs and return a nested list where each inner list represents the transformed features of a data point.\n\nKey constraints and edge cases:\n\n- The input data is a list of numerical values.\n- The degree is an integer that determines the degree of the polynomial expansion.\n- If the degree is less than 0, the function should return an empty list.\n- The output should be a list of lists, where each inner list contains the polynomial features of the corresponding data point.\n- The output should be rounded to 8 decimal places.\n\nInput: A list of numerical data and a degree.\nOutput: A nested list of transformed features.\n\nSubtle requirements or implicit constraints:\n\n- The function should handle single data points correctly.\n- The function should generate all possible combinations of the input features up to the given degree.\n\n# Key Insights\n\n- The Phi Transformation involves generating polynomial features from the input data up to a specified degree.\n- For a degree of 1, the transformation should return the original data because no polynomial expansion is needed beyond the linear terms.\n- For a degree greater than 1, the function needs to generate all possible combinations of the input features up to the given degree.\n- The number of features in the output can be calculated using the formula for combinations with repetitions allowed, but for polynomial features, we simply need to consider all combinations up to the degree.\n- The function can utilize a library like NumPy for efficient numerical computations.\n- The output should be rounded to 8 decimal places.\n\nCritical examples or test cases:\n\n- An example with data = [1, 2] and degree = 2 helps to illustrate the transformation process.\n\n# Solution Strategy\n\n- Check if the degree is less than 0 and return an empty list if true.\n- Initialize an empty list to store the transformed data points.\n- For each data point, generate polynomial features up to the given degree.\n- Use a loop to iterate over the degree and generate the polynomial features for each term.\n- Calculate the polynomial features for each term by raising the data point to the power of the term.\n- Append the polynomial features to the transformed data point.\n- Round the transformed data point to 8 decimal places.\n- Append the transformed data point to the transformed data list.\n- Return the transformed data list.\n\nImportant optimizations or special cases:\n\n- Handle the case where the degree is 0 or 1 correctly.\n- Ensure the function works correctly for single data points.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    transformed_data = []\n    for x in data:\n        transformed_x = []\n        for i in range(degree + 1):\n            transformed_x.append(round(x ** i, 8))\n        transformed_data.append(transformed_x)\n    return transformed_data\n\n# However, the above implementation only works for a single feature. \n# For multiple features, we need a different approach.\n\nimport numpy as np\nimport itertools\n\ndef phi_transform(data: list[list[float]], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[list[float]]): A list of numerical values to transform, where each inner list represents a data point.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    def generate_polynomial_features(x, degree):\n        n = len(x)\n        features = []\n        for powers in itertools.product(range(degree + 1), repeat=n):\n            if sum(powers) <= degree:\n                feature = 1\n                for i in range(n):\n                    feature *= x[i] ** powers[i]\n                features.append(round(feature, 8))\n        return features\n\n    transformed_data = []\n    for x in data:\n        transformed_x = generate_polynomial_features(x, degree)\n        transformed_data.append(transformed_x)\n    return transformed_data\n```"}
{"task_id": 85, "completion_id": 0, "completion": "### Positional Encoding Calculator\n#### Description\nThis function calculates positional encodings for a sequence length (`position`) and model dimensionality (`d_model`) using sine and cosine functions as specified in the Transformer architecture.\n\n#### Code\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encodings for a sequence length and model dimensionality.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings of dtype np.float16.\n    \"\"\"\n    # Check for invalid inputs\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Calculate positional encodings\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n\n    # Return the positional encodings as a list\n    return pe.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    position = 10\n    d_model = 512\n    result = pos_encoding(position, d_model)\n    print(result)\n```\n\n#### Explanation\n\n*   The function `pos_encoding` takes two parameters: `position` and `d_model`, which represent the sequence length and model dimensionality, respectively.\n*   It first checks if the inputs are valid. If `position` is 0 or `d_model` is less than or equal to 0, it returns -1.\n*   It then calculates the positional encodings using sine and cosine functions as specified in the Transformer architecture.\n*   The positional encodings are stored in a 2D NumPy array `pe` with shape `(position, d_model)` and dtype `np.float16`.\n*   Finally, it returns the positional encodings as a list using the `tolist()` method.\n\nYou can run this code and test it with different inputs to see the calculated positional encodings. The example usage demonstrates how to call the `pos_encoding` function with a sequence length of 10 and a model dimensionality of 512."}
{"task_id": 86, "completion_id": 0, "completion": "# Detecting Overfitting or Underfitting in Machine Learning Models\n## Description\nThe following Python function determines whether a machine learning model is overfitting, underfitting, or performing well based on training and test accuracy values.\n\n## Code\n\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1' (Overfitting), '-1' (Underfitting), or '0' (Good fit).\n    \"\"\"\n    # Check if inputs are within valid range\n    if not (0 <= training_accuracy <= 1) or not (0 <= test_accuracy <= 1):\n        raise ValueError(\"Accuracy values must be between 0 and 1\")\n\n    # Check for Overfitting: training accuracy significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n\n    # Check for Underfitting: both training and test accuracy are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n\n    # If none of the above conditions are met, it's a Good fit\n    return 0  # Good fit\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test cases\n    test_cases = [\n        {\"training\": 0.9, \"test\": 0.6},  # Overfitting\n        {\"training\": 0.4, \"test\": 0.5},  # Underfitting\n        {\"training\": 0.8, \"test\": 0.75}  # Good fit\n    ]\n\n    for case in test_cases:\n        result = model_fit_quality(case[\"training\"], case[\"test\"])\n        if result == 1:\n            print(f\"Training Accuracy: {case['training']}, Test Accuracy: {case['test']}, Result: Overfitting\")\n        elif result == -1:\n            print(f\"Training Accuracy: {case['training']}, Test Accuracy: {case['test']}, Result: Underfitting\")\n        else:\n            print(f\"Training Accuracy: {case['training']}, Test Accuracy: {case['test']}, Result: Good fit\")\n```\n\n## Explanation\n\nThe `model_fit_quality` function takes two parameters: `training_accuracy` and `test_accuracy`, both of which are expected to be floats between 0 and 1.\n\n1. **Input Validation**: The function first checks if the provided accuracy values are within the valid range (0 to 1). If not, it raises a `ValueError`.\n\n2. **Overfitting Check**: It then checks if the model is overfitting by verifying if the difference between `training_accuracy` and `test_accuracy` is greater than 0.2. If so, it returns `1`.\n\n3. **Underfitting Check**: Next, it checks for underfitting by verifying if both `training_accuracy` and `test_accuracy` are less than 0.7. If so, it returns `-1`.\n\n4. **Good Fit**: If neither of the above conditions is met, the function concludes that the model is a good fit and returns `0`.\n\nThe example usage demonstrates how to use the `model_fit_quality` function with different test cases, printing out the results for overfitting, underfitting, and good fit scenarios."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n\n    # Update the first moment estimate\n    m_hat = beta1 * m + (1 - beta1) * grad\n    \n    # Update the second moment estimate\n    v_hat = beta2 * v + (1 - beta2) * np.square(grad)\n    \n    # Bias correction for the first moment estimate\n    m_bias_corrected = m_hat / (1 - np.power(beta1, t))\n    \n    # Bias correction for the second moment estimate\n    v_bias_corrected = v_hat / (1 - np.power(beta2, t))\n    \n    # Update the parameter\n    updated_parameter = parameter - learning_rate * m_bias_corrected / (np.sqrt(v_bias_corrected) + epsilon)\n    \n    # Update the moving averages\n    updated_m = m_hat\n    updated_v = v_hat\n    \n    # Round to 5 decimal places and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(updated_m, 5).tolist()\n    updated_v = np.round(updated_v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage:\nif __name__ == \"__main__\":\n    parameter = 1.0\n    grad = 0.5\n    m = 0.2\n    v = 0.1\n    t = 2\n    learning_rate = 0.01\n\n    updated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t, learning_rate)\n    print(\"Updated Parameter:\", updated_parameter)\n    print(\"Updated First Moment (m):\", updated_m)\n    print(\"Updated Second Moment (v):\", updated_v)\n```\n\nIn this code:\n\n*   We define the `adam_optimizer` function that takes the current parameter value, gradient, moving averages (`m` and `v`), current timestep `t`, and hyperparameters as inputs.\n*   We update the first and second moment estimates using the given formulas.\n*   We apply bias correction to the moment estimates.\n*   We update the parameter using the Adam optimizer update rule.\n*   We round the updated values to 5 decimal places and convert them to lists before returning.\n*   In the example usage section, we demonstrate how to use the `adam_optimizer` function with sample inputs."}
{"task_id": 88, "completion_id": 0, "completion": "**GPT-2 Text Generation Function**\n=====================================\n\n### Overview\n\nThe following Python function implements a simplified GPT-2-like text generation model. It takes a prompt and the number of tokens to generate as input and outputs the generated text.\n\n### Code\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12,\n        \"n_embd\": 10,\n        \"n_layer\": 1,\n        \"d_ff\": 20\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [\n            {\n                \"attn\": {\n                    \"c_attn\": np.random.rand(10, 3 * 10),\n                    \"c_proj\": np.random.rand(10, 10),\n                    \"attn_dropout\": np.random.rand(),\n                    \"resid_dropout\": np.random.rand()\n                },\n                \"mlp\": {\n                    \"c_fc\": np.random.rand(10, 20),\n                    \"c_proj\": np.random.rand(20, 10),\n                    \"dropout\": np.random.rand()\n                },\n                \"ln_1\": {\n                    \"g\": np.ones(10),\n                    \"b\": np.zeros(10),\n                },\n                \"ln_2\": {\n                    \"g\": np.ones(10),\n                    \"b\": np.zeros(10),\n                }\n            }\n        ],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef multi_head_attention(q, k, v, hparams, params):\n    # Split the query, key, and value into multiple attention heads\n    n_head = hparams[\"n_head\"]\n    n_embd = hparams[\"n_embd\"]\n    q = np.reshape(q, (q.shape[0], q.shape[1], n_head, n_embd // n_head))\n    k = np.reshape(k, (k.shape[0], k.shape[1], n_head, n_embd // n_head))\n    v = np.reshape(v, (v.shape[0], v.shape[1], n_head, n_embd // n_head))\n\n    # Compute attention scores\n    scores = np.matmul(q, k.transpose((0, 2, 3, 1))) / np.sqrt(n_embd // n_head)\n\n    # Compute attention weights\n    attention_weights = np.softmax(scores, axis=-1)\n\n    # Compute attention output\n    attention_output = np.matmul(attention_weights, v)\n\n    # Reshape attention output\n    attention_output = np.reshape(attention_output, (attention_output.shape[0], attention_output.shape[1], n_embd))\n\n    # Compute projection output\n    projection_output = np.matmul(attention_output, params[\"blocks\"][0][\"attn\"][\"c_proj\"])\n\n    return projection_output\n\ndef feed_forward_network(x, hparams, params):\n    # Compute feed-forward network output\n    output = np.matmul(x, params[\"blocks\"][0][\"mlp\"][\"c_fc\"])\n    output = np.tanh(output)\n    output = np.matmul(output, params[\"blocks\"][0][\"mlp\"][\"c_proj\"])\n\n    return output\n\ndef layer_normalization(x, params, layer):\n    # Compute layer normalization output\n    output = x * params[\"ln_\" + layer][\"g\"] + params[\"ln_\" + layer][\"b\"]\n\n    return output\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n\n    # Load encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode prompt\n    prompt_tokens = encoder.encode(prompt)\n\n    # Generate text\n    generated_tokens = prompt_tokens[:]\n    for _ in range(n_tokens_to_generate):\n        # Compute token embeddings\n        token_embeddings = params[\"wte\"][np.array(generated_tokens)]\n\n        # Compute positional embeddings\n        positional_embeddings = params[\"wpe\"][np.arange(len(generated_tokens))]\n\n        # Compute input embeddings\n        input_embeddings = token_embeddings + positional_embeddings\n\n        # Compute block output\n        block_output = input_embeddings\n        block_output = layer_normalization(block_output, params, \"1\")\n        q = np.matmul(block_output, params[\"blocks\"][0][\"attn\"][\"c_attn\"][:, :hparams[\"n_embd\"]])\n        k = np.matmul(block_output, params[\"blocks\"][0][\"attn\"][\"c_attn\"][:, hparams[\"n_embd\"]:2 * hparams[\"n_embd\"]])\n        v = np.matmul(block_output, params[\"blocks\"][0][\"attn\"][\"c_attn\"][:, 2 * hparams[\"n_embd\"]:])\n        attention_output = multi_head_attention(q, k, v, hparams, params)\n        block_output = block_output + attention_output\n        block_output = layer_normalization(block_output, params, \"2\")\n        ffn_output = feed_forward_network(block_output, hparams, params)\n        block_output = block_output + ffn_output\n\n        # Compute final output\n        final_output = layer_normalization(block_output, params, \"f\")\n\n        # Sample next token\n        next_token_logits = np.matmul(final_output[-1], params[\"wte\"].T)\n        next_token = np.argmax(next_token_logits)\n        generated_tokens.append(next_token)\n\n    # Decode generated tokens\n    generated_text = encoder.decode(generated_tokens[len(prompt_tokens):])\n\n    return generated_text\n\n# Example usage\nprompt = \"hello world\"\nn_tokens_to_generate = 10\ngenerated_text = gen_text(prompt, n_tokens_to_generate)\nprint(generated_text)\n```\n\n### Explanation\n\nThe provided code implements a simplified GPT-2-like text generation model. The `gen_text` function takes a prompt and the number of tokens to generate as input and outputs the generated text.\n\nThe model consists of the following components:\n\n*   **Token Embeddings**: The input tokens are embedded into dense vector representations using the `wte` matrix.\n*   **Positional Embeddings**: Positional information is added to the token embeddings using the `wpe` matrix.\n*   **Multi-head Attention**: The model attends to various parts of the sequence using the multi-head attention mechanism.\n*   **Feed-Forward Network**: The attention output is processed through a dense layer.\n*   **Layer Normalization**: The training process is stabilized using layer normalization.\n\nThe `gen_text` function first loads the encoder, hyperparameters, and model parameters. It then encodes the prompt and generates text by iteratively computing the token embeddings, positional embeddings, block output, and final output. The next token is sampled based on the final output logits. Finally, the generated tokens are decoded into text.\n\nThe example usage demonstrates how to use the `gen_text` function to generate text based on a given prompt."}
{"task_id": 89, "completion_id": 0, "completion": "**Pattern Weaver's Code: Self-Attention Mechanism**\n=====================================================\n\n### Overview\n\nThis code implements a simplified self-attention mechanism to analyze the relationships between crystals. Given a sequence of crystals and their values, it calculates the attention scores using the softmax function and derives the final weighted pattern for each crystal.\n\n### Code\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    Implements a simplified self-attention mechanism to analyze crystal relationships.\n\n    Args:\n    n (int): The number of crystals.\n    crystal_values (list): A list of unique numeric values for each crystal.\n    dimension (int): The dimension of the crystal values.\n\n    Returns:\n    list: A list of floats representing the final weighted pattern for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        Computes the softmax of a given list of values.\n\n        Args:\n        values (list): A list of values.\n\n        Returns:\n        list: A list of softmax values.\n        \"\"\"\n        exp_values = [np.exp(value) for value in values]\n        sum_exp_values = sum(exp_values)\n        return [exp_value / sum_exp_values for exp_value in exp_values]\n\n    # Initialize the query, key, and value matrices\n    query_matrix = np.random.rand(n, dimension)\n    key_matrix = np.random.rand(n, dimension)\n    value_matrix = np.array(crystal_values).reshape(n, dimension)\n\n    # Compute the attention scores\n    attention_scores = np.dot(query_matrix, key_matrix.T) / np.sqrt(dimension)\n\n    # Apply the softmax function to the attention scores\n    attention_weights = softmax([score for row in attention_scores for score in row])\n\n    # Reshape the attention weights to match the attention scores matrix\n    attention_weights = np.array(attention_weights).reshape(n, n)\n\n    # Compute the weighted pattern for each crystal\n    weighted_pattern = np.dot(attention_weights, value_matrix)\n\n    # Return the final weighted pattern for each crystal, rounded to the 4th decimal place\n    return [round(float(value), 4) for row in weighted_pattern for value in row]\n\n# Example usage\nn = 3\ncrystal_values = [1, 2, 3]\ndimension = 1\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\n### Explanation\n\nThe provided code implements a simplified self-attention mechanism using the following steps:\n\n1.  **Softmax Function**: The `softmax` function computes the softmax of a given list of values. It first calculates the exponential of each value, then divides each exponential value by the sum of all exponential values.\n2.  **Self-Attention Mechanism**: The `pattern_weaver` function takes the number of crystals `n`, their unique numeric values `crystal_values`, and the dimension of the crystal values as input. It initializes query, key, and value matrices, computes attention scores, applies the softmax function, and derives the final weighted pattern for each crystal.\n3.  **Weighted Pattern Computation**: The code computes the weighted pattern for each crystal by taking the dot product of the attention weights and the value matrix.\n4.  **Output**: The final weighted pattern for each crystal is returned as a list of floats, rounded to the 4th decimal place.\n\nThe example usage demonstrates how to call the `pattern_weaver` function with a sample input. You can modify the input parameters to test the function with different crystal values and dimensions."}
{"task_id": 90, "completion_id": 0, "completion": "### BM25 Ranking Function Implementation\n\nHere's a Python implementation of the BM25 ranking function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query across a corpus of documents.\n\n    Args:\n    - corpus (list of lists): A list of documents, where each document is a list of terms.\n    - query (list): A list of terms in the query.\n    - k1 (float, optional): The term frequency saturation parameter. Defaults to 1.5.\n    - b (float, optional): The document length normalization parameter. Defaults to 0.75.\n\n    Returns:\n    - list: A list of BM25 scores, one for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n\n    # Calculate the total number of documents in the corpus\n    num_docs = len(corpus)\n\n    # Calculate the average document length in the corpus\n    avg_doc_len = sum(len(doc) for doc in corpus) / num_docs\n\n    # Create a dictionary to store the frequency of each term across the corpus\n    term_freqs = {}\n    for doc in corpus:\n        term_counts = Counter(doc)\n        for term, count in term_counts.items():\n            if term not in term_freqs:\n                term_freqs[term] = 0\n            term_freqs[term] += 1\n\n    # Create a dictionary to store the IDF of each term\n    idf = {}\n    for term in set(term for doc in corpus for term in doc):\n        idf[term] = np.log((num_docs + 1) / (term_freqs.get(term, 1) + 1)) + 1\n\n    # Initialize a list to store the BM25 scores for each document\n    bm25_scores = [0.0] * num_docs\n\n    # Iterate over each document in the corpus\n    for i, doc in enumerate(corpus):\n        # Calculate the document length\n        doc_len = len(doc)\n\n        # Create a dictionary to store the term frequencies in the current document\n        term_counts = Counter(doc)\n\n        # Iterate over each term in the query\n        for term in query:\n            # Check if the term exists in the document\n            if term in term_counts:\n                # Calculate the term frequency\n                tf = term_counts[term]\n\n                # Calculate the BM25 score for the term\n                bm25_score = idf[term] * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n\n                # Add the BM25 score to the document's total score\n                bm25_scores[i] += bm25_score\n\n    # Return the BM25 scores rounded to three decimal places\n    return [round(score, 3) for score in bm25_scores]\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"the\", \"sun\", \"is\", \"shining\", \"brightly\", \"in\", \"the\", \"clear\", \"blue\", \"sky\"],\n    [\"the\", \"big\", \"red\", \"car\", \"is\", \"driving\", \"down\", \"the\", \"street\"]\n]\nquery = [\"the\", \"quick\", \"fox\"]\nprint(calculate_bm25_scores(corpus, query))\n```\n\nIn this code:\n\n*   We first calculate the total number of documents in the corpus and the average document length.\n*   We then create a dictionary to store the frequency of each term across the corpus and calculate the IDF of each term.\n*   Next, we iterate over each document in the corpus, calculate its BM25 score for the query, and store the scores in a list.\n*   Finally, we return the BM25 scores rounded to three decimal places.\n\nThe example usage demonstrates how to use the `calculate_bm25_scores` function with a sample corpus and query."}
{"task_id": 91, "completion_id": 0, "completion": "### F1 Score Calculation Implementation\n\nTo calculate the F1 score, we first need to compute precision and recall. Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP). Recall is the ratio of true positives to the sum of true positives and false negatives (FN). The F1 score is then calculated as the harmonic mean of precision and recall.\n\nHere's how you can implement it in Python:\n\n```python\nfrom sklearn.metrics import f1_score\n\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Using sklearn's f1_score function for simplicity and accuracy\n    f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' for multi-class, use 'binary' for binary classification\n    \n    # Round the F1 score to three decimal places\n    f1_score_rounded = round(f1, 3)\n    \n    return f1_score_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    print(calculate_f1_score(y_true, y_pred))\n```\n\n### Alternative Implementation without sklearn\n\nIf you want to implement it manually without using `sklearn`, you can do so by calculating TP, FP, and FN directly:\n\n```python\ndef calculate_f1_score_manual(y_true, y_pred):\n    \"\"\"\n    Manually calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counters\n    tp = 0  # true positives\n    fp = 0  # false positives\n    fn = 0  # false negatives\n\n    # Assuming multi-class classification, calculate for each class\n    classes = set(y_true)\n    f1_scores = []\n\n    for c in classes:\n        tp = sum(1 for t, p in zip(y_true, y_pred) if t == c and p == c)\n        fp = sum(1 for t, p in zip(y_true, y_pred) if t != c and p == c)\n        fn = sum(1 for t, p in zip(y_true, y_pred) if t == c and p != c)\n\n        # Avoid division by zero\n        if tp + fp == 0 or tp + fn == 0:\n            precision = 0 if tp + fp == 0 else tp / (tp + fp)\n            recall = 0 if tp + fn == 0 else tp / (tp + fn)\n        else:\n            precision = tp / (tp + fp) if tp + fp != 0 else 0\n            recall = tp / (tp + fn) if tp + fn != 0 else 0\n\n        # F1 score for the class\n        if precision + recall == 0:\n            f1 = 0\n        else:\n            f1 = 2 * (precision * recall) / (precision + recall)\n\n        f1_scores.append(f1)\n\n    # Macro F1\n    macro_f1 = sum(f1_scores) / len(f1_scores)\n\n    # Round the F1 score to three decimal places\n    macro_f1_rounded = round(macro_f1, 3)\n\n    return macro_f1_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    print(calculate_f1_score_manual(y_true, y_pred))\n```\n\nBoth of these implementations will give you the F1 score for your classification problem, rounded to three decimal places. The first one leverages `sklearn` for simplicity, while the second one provides a manual calculation approach."}
{"task_id": 92, "completion_id": 0, "completion": "## Linear Regression for Power Grid Optimization\n\n### Problem Description\n\nThe goal is to optimize power grid consumption forecasting for a Martian colony. We have 10 days of consumption data with a linear trend and daily fluctuations. The fluctuation is given by $f(i) = 10 \\times \\sin(2\\pi i / 10)$, where $i$ is the day number.\n\n### Solution\n\n```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation from each data point\n    detrended_data = []\n    for i, consumption in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n\n    # Convert lists to numpy arrays for easier manipulation\n    days = np.arange(1, len(consumption_data) + 1).reshape(-1, 1)\n    detrended_data = np.array(detrended_data)\n\n    # 2) Perform linear regression on the detrended data\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n\n    # 3) Predict day 15's base consumption\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n\n    # 4) Add the day 15 fluctuation back\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n\n    # 5) Round, then add a 5% safety margin (rounded up)\n    safety_margin = consumption_day_15 * 0.05\n    final_consumption = math.ceil(consumption_day_15 + safety_margin)\n\n    return final_consumption\n\n# Example usage\nconsumption_data = [100 + 10 * math.sin(2 * PI * i / 10) + 5 * i for i in range(1, 11)]\nprint(power_grid_forecast(consumption_data))\n```\n\n### Explanation\n\nThe provided Python function `power_grid_forecast` takes a list of daily power consumption data as input and returns the forecasted power consumption for day 15 with a 5% safety margin.\n\n1.  **Detrend Data**: For each day's consumption data, subtract the daily fluctuation using the given formula.\n2.  **Linear Regression**: Perform linear regression on the detrended data to model the underlying linear trend.\n3.  **Predict Day 15's Base Consumption**: Use the linear regression model to predict the base consumption for day 15.\n4.  **Add Fluctuation Back**: Calculate the fluctuation for day 15 and add it back to the predicted base consumption.\n5.  **Apply Safety Margin**: Add a 5% safety margin to the forecasted consumption and round up to the nearest integer.\n\nThe example usage demonstrates how to use the `power_grid_forecast` function with sample consumption data. The consumption data is generated with a linear trend, daily fluctuations, and random noise. The function returns the forecasted power consumption for day 15 with a 5% safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "### Mean Absolute Error (MAE) Implementation\n\nHere's a Python function that calculates the Mean Absolute Error (MAE) between two arrays of actual and predicted values using NumPy.\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check if input arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Calculate absolute differences between true and predicted values\n    absolute_errors = np.abs(y_true - y_pred)\n\n    # Calculate mean of absolute errors\n    mean_absolute_error = np.mean(absolute_errors)\n\n    # Return MAE rounded to 3 decimal places\n    return round(mean_absolute_error, 3)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1, 4.9])\n\n    print(\"Mean Absolute Error:\", mae(y_true, y_pred))\n```\n\n### Explanation\n\n*   The function `mae` takes two numpy arrays `y_true` and `y_pred` as input, representing the true and predicted values, respectively.\n*   It first checks if the input arrays have the same shape and raises a `ValueError` if they don't.\n*   Then, it calculates the absolute differences between the true and predicted values using `np.abs(y_true - y_pred)`.\n*   The mean of these absolute errors is calculated using `np.mean(absolute_errors)`, giving the Mean Absolute Error (MAE).\n*   Finally, the MAE is rounded to 3 decimal places using the `round` function and returned as the result.\n\n### Advice\n\n*   Make sure to pass numpy arrays as input to the `mae` function. If your data is in a different format, convert it to a numpy array before calling the function.\n*   The `mae` function assumes that the input arrays have the same shape. If your data has a different structure, you may need to modify the function accordingly."}
{"task_id": 94, "completion_id": 0, "completion": "### Multi-Head Attention Implementation\n\n#### Overview\n\nThe following code implements the multi-head attention mechanism, a crucial component of the Transformer model. This implementation consists of three key functions: `compute_qkv`, `self_attention`, and `multi_head_attention`.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n    - X (np.ndarray): Input matrix.\n    - W_q (np.ndarray): Weight matrix for Query.\n    - W_k (np.ndarray): Weight matrix for Key.\n    - W_v (np.ndarray): Weight matrix for Value.\n\n    Returns:\n    - Q (np.ndarray): Query matrix.\n    - K (np.ndarray): Key matrix.\n    - V (np.ndarray): Value matrix.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention output.\n\n    Args:\n    - Q (np.ndarray): Query matrix.\n    - K (np.ndarray): Key matrix.\n    - V (np.ndarray): Value matrix.\n\n    Returns:\n    - Output (np.ndarray): Self-attention output.\n    \"\"\"\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    # Compute attention weights\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    # Compute output\n    output = np.dot(attention_weights, V)\n    return output\n\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention output.\n\n    Args:\n    - X (np.ndarray): Input matrix.\n    - W_q (np.ndarray): Weight matrix for Query.\n    - W_k (np.ndarray): Weight matrix for Key.\n    - W_v (np.ndarray): Weight matrix for Value.\n    - n_heads (int): Number of attention heads.\n\n    Returns:\n    - Output (list): Multi-head attention output.\n    \"\"\"\n    # Compute the Query (Q), Key (K), and Value (V) matrices.\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Split the Q, K, and V matrices into multiple heads\n    Q = np.reshape(Q, (Q.shape[0], n_heads, -1))\n    K = np.reshape(K, (K.shape[0], n_heads, -1))\n    V = np.reshape(V, (V.shape[0], n_heads, -1))\n\n    # Compute self-attention for each head\n    outputs = []\n    for i in range(n_heads):\n        Q_head = Q[:, i, :]\n        K_head = K[:, i, :]\n        V_head = V[:, i, :]\n        output = self_attention(Q_head, K_head, V_head)\n        outputs.append(output)\n\n    # Concatenate and reshape the outputs\n    outputs = np.array(outputs).transpose((1, 0, 2))\n    outputs = np.reshape(outputs, (-1, outputs.shape[-1])).round(4).tolist()\n\n    return outputs\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input matrix and weight matrices\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    W_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n    W_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n    W_v = np.array([[0.5, 0.6, 0.7], [0.8, 0.9, 0.1], [0.2, 0.3, 0.4]])\n\n    # Define number of attention heads\n    n_heads = 2\n\n    # Compute multi-head attention output\n    output = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(output)\n```\n\n### Explanation\n\nThe provided code implements the multi-head attention mechanism via three key functions:\n\n*   `compute_qkv`: Computes the Query (Q), Key (K), and Value (V) matrices by performing dot products between the input matrix `X` and the weight matrices `W_q`, `W_k`, and `W_v`.\n*   `self_attention`: Computes the self-attention output for a given Query (Q), Key (K), and Value (V) matrix. It calculates attention scores, computes attention weights using softmax, and then computes the output by taking a dot product between attention weights and the Value matrix.\n*   `multi_head_attention`: Computes the multi-head attention output by first computing Q, K, and V matrices using `compute_qkv`. It then splits these matrices into multiple heads, computes self-attention for each head using `self_attention`, and finally concatenates and reshapes the outputs.\n\nThe code adheres to the specified output constraints, rounding results to the nearest 4th decimal and returning a Python list after reshaping using NumPy's `tolist()` method.\n\nThe example usage demonstrates how to use the `multi_head_attention` function with sample input and weight matrices, as well as a specified number of attention heads. The output is then printed to the console."}
{"task_id": 95, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires calculating the Phi coefficient, a measure of the correlation between two binary variables. The input consists of two lists of integers, `x` and `y`, each containing binary values (0s and 1s). The output is the Phi coefficient rounded to 4 decimal places.\n\nKey constraints and edge cases:\n\n* The input lists `x` and `y` must be of the same length.\n* The input lists `x` and `y` must contain only binary values (0s and 1s).\n* The denominator in the Phi coefficient calculation may be zero, which requires special handling.\n\nInput: Two lists of integers, `x` and `y`, each containing binary values (0s and 1s).\nOutput: The Phi coefficient rounded to 4 decimal places.\n\nSubtle requirements or implicit constraints:\n\n* The Phi coefficient is a measure of the correlation between two binary variables.\n* The input lists `x` and `y` must be of the same length to calculate the Phi coefficient.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The Phi coefficient can be calculated using a 2x2 contingency table.\n* The contingency table can be constructed by counting the number of observations that fall into each of the four categories: both `x` and `y` are 1, `x` is 1 and `y` is 0, `x` is 0 and `y` is 1, and both `x` and `y` are 0.\n* The Phi coefficient can be calculated using the formula: `phi = (n11 * n00 - n10 * n01) / sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))`, where `n11`, `n10`, `n01`, and `n00` are the counts from the contingency table.\n* The denominator in the Phi coefficient calculation may be zero, which requires special handling.\n\nPatterns or properties being used:\n\n* The Phi coefficient is a measure of the correlation between two binary variables.\n* The contingency table is used to calculate the Phi coefficient.\n\nKey algorithmic insights:\n\n* The Phi coefficient can be calculated using a simple formula based on the contingency table.\n* The denominator in the Phi coefficient calculation may be zero, which requires special handling.\n\nImportant realizations:\n\n* The input lists `x` and `y` must be of the same length to calculate the Phi coefficient.\n* The Phi coefficient is a measure of the correlation between two binary variables.\n\nCritical examples or test cases:\n\n* Example usage: `x = [1, 0, 1, 1, 0]` and `y = [1, 1, 0, 1, 0]`.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Check if the input lists `x` and `y` are of the same length. If not, raise a `ValueError`.\n2. Initialize counters for the contingency table: `n11`, `n10`, `n01`, and `n00`.\n3. Iterate over the input lists `x` and `y` to count the number of observations that fall into each of the four categories.\n4. Calculate the Phi coefficient using the formula: `phi = (n11 * n00 - n10 * n01) / sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))`.\n5. Check if the denominator in the Phi coefficient calculation is zero. If so, return 0.\n6. Return the Phi coefficient rounded to 4 decimal places.\n\nImportant optimizations or special cases to handle:\n\n* Check if the input lists `x` and `y` contain only binary values (0s and 1s). If not, raise a `ValueError`.\n* Handle the case where the denominator in the Phi coefficient calculation is zero.\n\nHere's the python solution:\n```python\nimport math\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Lists x and y must be of the same length\")\n\n    n11 = 0\n    n10 = 0\n    n01 = 0\n    n00 = 0\n\n    for i in range(len(x)):\n        if x[i] == 1 and y[i] == 1:\n            n11 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n10 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n01 += 1\n        else:\n            n00 += 1\n\n    numerator = (n11 * n00) - (n10 * n01)\n    denominator = math.sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))\n\n    if denominator == 0:\n        return 0\n    else:\n        phi = numerator / denominator\n        return round(phi, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "# Implementation of the Hard Sigmoid Activation Function\n\n## Overview\n\nThe Hard Sigmoid activation function is a computationally efficient approximation of the standard sigmoid function. It is defined as:\n\n- For `x < -2.5`, `hard_sigmoid(x) = 0`\n- For `-2.5 <= x <= 2.5`, `hard_sigmoid(x) = 0.2 * x + 0.5`\n- For `x > 2.5`, `hard_sigmoid(x) = 1`\n\n## Code\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif -2.5 <= x <= 2.5:\n        return 0.2 * x + 0.5\n    else:\n        return 1.0\n\n# Example usage\nif __name__ == \"__main__\":\n    print(hard_sigmoid(-3.0))  # Expected output: 0.0\n    print(hard_sigmoid(0.0))   # Expected output: 0.5\n    print(hard_sigmoid(3.0))   # Expected output: 1.0\n```\n\n## Explanation\n\nThe provided Python function `hard_sigmoid` implements the Hard Sigmoid activation function according to its definition. It takes a single input `x` and returns the corresponding output.\n\nThe function uses simple conditional statements to determine which part of the Hard Sigmoid definition to apply based on the input value `x`.\n\n## Alternative Implementation Using `numpy`\n\nIf you're working with large arrays or vectors, consider using `numpy` for vectorized operations:\n\n```python\nimport numpy as np\n\ndef hard_sigmoid(x: float or np.ndarray) -> float or np.ndarray:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float or np.ndarray): Input value(s)\n\n    Returns:\n        float or np.ndarray: The Hard Sigmoid of the input(s)\n    \"\"\"\n    return np.clip(0.2 * x + 0.5, 0.0, 1.0)\n\n# Example usage\nif __name__ == \"__main__\":\n    print(hard_sigmoid(-3.0))  # Expected output: 0.0\n    print(hard_sigmoid(0.0))   # Expected output: 0.5\n    print(hard_sigmoid(3.0))   # Expected output: 1.0\n    print(hard_sigmoid(np.array([-3.0, 0.0, 3.0])))  # Expected output: [0.  0.5 1. ]\n```\n\nThis version uses `np.clip` to efficiently apply the Hard Sigmoid function to scalar values or entire arrays at once, making it suitable for use in neural network implementations."}
{"task_id": 97, "completion_id": 0, "completion": "# Implementation of the ELU Activation Function\n## Overview\nThe ELU (Exponential Linear Unit) activation function is an alternative to ReLU that helps mitigate the dying ReLU problem by allowing negative outputs. This is particularly useful in deep neural networks where ReLU's non-differentiable output for negative inputs can cause issues.\n\n## Code\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        # For positive inputs, ELU returns the input itself\n        return round(x, 4)\n    else:\n        # For negative inputs, ELU returns alpha * (exp(x) - 1)\n        return round(alpha * (math.exp(x) - 1), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    print(elu(2.0))  # Expected output: 2.0\n    print(elu(-2.0))  # Expected output: -0.8647 (approximately)\n```\n\n## Explanation\n\n- The ELU function takes two parameters: `x`, the input value, and `alpha`, a parameter that controls the saturation value for negative inputs.\n- For positive inputs `x`, the ELU function returns `x` directly, as it is a linear unit in this domain.\n- For negative inputs `x`, the ELU function returns `alpha * (exp(x) - 1)`, which allows for negative outputs and helps in reducing the dying ReLU problem.\n- The output of the function is rounded to the 4th decimal place to meet the output constraints.\n\n## Notes\n\n- The `math.exp` function is used to compute the exponential of `x` for negative inputs.\n- The example usage demonstrates how to call the `elu` function with both positive and negative inputs.\n- The `round` function is used to ensure that the output is rounded to the 4th decimal place as required."}
{"task_id": 98, "completion_id": 0, "completion": "# Implementation of the PReLU Activation Function\n\n## Overview\n\nThe PReLU (Parametric ReLU) activation function is a variant of the ReLU activation function that introduces a learnable parameter for negative inputs. This allows the model to learn the optimal slope for negative inputs, which can help improve the performance of the model.\n\n## Code\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\n```\n\n## Explanation\n\nThe PReLU activation function is defined as:\n\n- For positive inputs (`x >= 0`), the output is the input itself (`x`).\n- For negative inputs (`x < 0`), the output is the product of the input and the learnable parameter `alpha`.\n\n## Example Use Cases\n\n```python\n# Test the prelu function with positive input\nprint(prelu(2.0))  # Output: 2.0\n\n# Test the prelu function with negative input and default alpha\nprint(prelu(-2.0))  # Output: -0.5\n\n# Test the prelu function with negative input and custom alpha\nprint(prelu(-2.0, alpha=0.5))  # Output: -1.0\n```\n\n## Vectorized Implementation for NumPy Arrays\n\nIf you're working with NumPy arrays, you can use the following vectorized implementation:\n\n```python\nimport numpy as np\n\ndef prelu_vectorized(x: np.ndarray, alpha: float = 0.25) -> np.ndarray:\n    \"\"\"\n    Vectorized implementation of the PReLU activation function.\n\n    Args:\n        x: Input array\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        np.ndarray: PReLU activation values\n    \"\"\"\n    return np.where(x >= 0, x, alpha * x)\n\n# Test the vectorized prelu function\nx = np.array([-2.0, 2.0, -3.0, 3.0])\nprint(prelu_vectorized(x))  # Output: [-0.5  2.   -0.75  3.  ]\n```"}
{"task_id": 99, "completion_id": 0, "completion": "# Softplus Activation Function Implementation\n## Overview\n\nThe Softplus activation function is a smooth approximation of the ReLU (Rectified Linear Unit) function. It is defined as `log(1 + e^x)`, where `e` is the base of the natural logarithm. This function is often used in deep learning models as an alternative to ReLU.\n\n## Implementation\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    # Handle edge case where x is very large to prevent numerical overflow\n    if x >= 100:\n        return x\n    \n    # Handle edge case where x is very small to prevent numerical underflow\n    if x <= -100:\n        return math.exp(x)\n    \n    # Compute softplus using the standard formula\n    result = math.log(1 + math.exp(x))\n    \n    # Return the result rounded to 4 decimal places\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [1.0, -1.0, 100.0, -100.0]\n    for x in inputs:\n        print(f\"Softplus({x}) = {softplus(x)}\")\n```\n\n## Explanation\n\n*   The `softplus` function takes a single input `x` and returns the Softplus value.\n*   For very large values of `x`, the `log(1 + e^x)` term is approximated as `x` to prevent numerical overflow.\n*   For very small values of `x`, the `exp(x)` term is used directly to prevent numerical underflow.\n*   The result is rounded to 4 decimal places using the `round` function.\n\n## Notes\n\n*   The Softplus function is a smooth approximation of the ReLU function, making it suitable for deep learning models.\n*   The implementation handles edge cases to prevent numerical overflow or underflow.\n*   Example usage is provided in the `if __name__ == \"__main__\":` block to demonstrate how to use the `softplus` function."}
{"task_id": 100, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing the Softsign activation function, a smooth activation function used in neural networks. The task is to compute the Softsign value for a given input, ensuring the output is bounded between -1 and 1.\n\nKey constraints and edge cases:\n\n* The input can be any real number.\n* The output must be bounded between -1 and 1.\n* The output should be rounded to the 4th decimal place.\n\nInput: A float value representing the input to the Softsign function.\nOutput: A float value representing the Softsign of the input, rounded to the 4th decimal place.\n\nSubtle requirements or implicit constraints:\n\n* The Softsign function should be smooth, meaning it should be continuous and differentiable.\n* The output should approach -1 as the input approaches negative infinity, and approach 1 as the input approaches positive infinity.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The Softsign function can be defined as: softsign(x) = x / (1 + |x|).\n* This function maps any real number to a value between -1 and 1.\n* The absolute value of x is used to ensure the function is symmetric around the origin.\n* The function can be broken down into cases: if x is positive, softsign(x) = x / (1 + x); if x is negative, softsign(x) = -|x| / (1 + |x|) = -x / (1 - x); if x is zero, softsign(x) = 0.\n\nPatterns or properties being used:\n\n* The Softsign function uses the properties of absolute value and fractions to map the input to a bounded output.\n\nKey algorithmic insights:\n\n* The Softsign function can be implemented using a simple mathematical formula.\n* The output can be rounded to the 4th decimal place using the round function.\n\nImportant realizations:\n\n* The Softsign function is a smooth activation function, making it suitable for use in neural networks.\n* The function is symmetric around the origin, meaning softsign(-x) = -softsign(x).\n\nCritical examples or test cases:\n\n* Testing the function with positive, negative, and zero inputs to ensure it produces the correct output.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Define the Softsign function as: softsign(x) = x / (1 + |x|).\n2. Implement the function using a simple mathematical formula.\n3. Use the absolute value of x to ensure the function is symmetric around the origin.\n4. Round the output to the 4th decimal place using the round function.\n5. Test the function with positive, negative, and zero inputs to ensure it produces the correct output.\n\nCritical details:\n\n* Use the absolute value of x to ensure the function is symmetric around the origin.\n* Round the output to the 4th decimal place to meet the output constraints.\n\nImportant optimizations or special cases:\n\n* None mentioned in the original trace.\n\nHere's the python solution:\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "# GRPO Objective Function Implementation\n\n## Overview\n\nThe following code implements the GRPO (Group Relative Policy Optimization) objective function used in reinforcement learning to optimize policy parameters. This function computes the GRPO objective given likelihood ratios, advantage estimates, old policy probabilities, reference policy probabilities, and applies the clipping mechanism and KL divergence penalty.\n\n## Implementation\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n\n    # Compute the clipped surrogate objective\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Calculate the clipped likelihood ratios\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    # Compute the objective without KL penalty\n    objective = np.minimum(rhos * A, rhos_clipped * A)\n\n    # Compute the KL divergence penalty term\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n\n    # Apply the KL divergence penalty\n    objective = np.mean(objective) - beta * kl_divergence\n\n    # Return the result rounded to the nearest 6th decimal\n    return round(objective, 6)\n\n# Example usage\nif __name__ == \"__main__\":\n    rhos = [1.2, 1.1, 1.3]\n    A = [0.5, 0.6, 0.7]\n    pi_theta_old = [0.4, 0.5, 0.1]\n    pi_theta_ref = [0.3, 0.6, 0.1]\n\n    result = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\n    print(\"GRPO Objective:\", result)\n```\n\n## Explanation\n\n*   The GRPO objective function takes in the likelihood ratios `rhos`, advantage estimates `A`, old policy probabilities `pi_theta_old`, reference policy probabilities `pi_theta_ref`, clipping parameter `epsilon`, and KL divergence penalty coefficient `beta`.\n*   It calculates the clipped likelihood ratios `rhos_clipped` by clipping `rhos` within the range `[1 - epsilon, 1 + epsilon]`.\n*   The objective without the KL penalty is computed as the minimum between the product of `rhos` and `A`, and the product of `rhos_clipped` and `A`.\n*   The KL divergence penalty term is calculated as the sum of the product of `pi_theta_old` and the logarithm of the ratio of `pi_theta_old` and `pi_theta_ref`.\n*   The final GRPO objective value is obtained by taking the mean of the objective without the KL penalty and subtracting the KL divergence penalty term.\n*   The result is rounded to the nearest 6th decimal before being returned.\n\n## Advice\n\n*   You can adjust the clipping parameter `epsilon` and the KL divergence penalty coefficient `beta` to control the trade-off between policy updates and stability.\n*   Ensure that the input lists `rhos`, `A`, `pi_theta_old`, and `pi_theta_ref` have the same length.\n*   The GRPO objective function assumes that the input values are valid and does not perform any error checking. You may want to add checks depending on your specific use case."}
{"task_id": 102, "completion_id": 0, "completion": "# Swish Activation Function Implementation\n\n## Overview\nThe Swish activation function is a self-gated activation function that has shown superior performance in deep neural networks compared to ReLU. It is defined as:\n\nSwish(x) = x * sigmoid(x)\n\nwhere sigmoid(x) is the sigmoid function.\n\n## Implementation\n\n```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Computes the sigmoid of x.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The sigmoid value\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * sigmoid(x), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    inputs = [1.0, -2.0, 0.5, -0.1]\n    for x in inputs:\n        print(f\"Swish({x}) = {swish(x)}\")\n```\n\n## Explanation\n\n*   The `sigmoid` function computes the sigmoid of a given input `x`.\n*   The `swish` function implements the Swish activation function by multiplying the input `x` with the sigmoid of `x`.\n*   The result is rounded to the nearest 4th decimal using the `round` function.\n*   In the example usage, we demonstrate the Swish activation function with various inputs.\n\n## Alternative Implementation using NumPy\n\nIf you're working with arrays or vectors, you can leverage NumPy's vectorized operations to compute the Swish activation function:\n\n```python\nimport numpy as np\n\ndef swish_numpy(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Swish activation function using NumPy.\n\n    Args:\n        x: Input array\n\n    Returns:\n        The Swish activation values\n    \"\"\"\n    return np.round(x * 1 / (1 + np.exp(-x)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    inputs = np.array([1.0, -2.0, 0.5, -0.1])\n    print(swish_numpy(inputs))\n```\n\nThis implementation uses NumPy's vectorized operations to compute the Swish activation function for an entire array at once."}
{"task_id": 103, "completion_id": 0, "completion": "To implement the SELU activation function while ensuring numerical stability and rounding results to the nearest 4th decimal, we can follow these steps:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    # For x >= 0, SELU(x) = scale * x\n    if x >= 0:\n        selu_value = scale * x\n    # For x < 0, SELU(x) = scale * alpha * (exp(x) - 1)\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    \n    # Ensure numerical stability and round to 4th decimal\n    return round(selu_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [1.0, -1.0, 0.0]\n    for x in inputs:\n        print(f\"SELU({x}) = {selu(x)}\")\n```\n\nIn this code:\n\n- We define the `selu` function with an input `x` and return the SELU activation value.\n- For `x >= 0`, the SELU function is defined as `scale * x`.\n- For `x < 0`, the SELU function is defined as `scale * alpha * (exp(x) - 1)`.\n- We use `math.exp` to compute the exponential of `x` for `x < 0`.\n- The result is rounded to the nearest 4th decimal using the `round` function.\n- In the example usage section, we demonstrate how to use the `selu` function with different inputs."}
{"task_id": 104, "completion_id": 0, "completion": "### Binary Classification with Logistic Regression\n\n#### Implementation\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs, weights, and bias\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to compute class probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Make binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return binary predictions as a list\n    return predictions.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define example input feature matrix\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    \n    # Define example model weights and bias\n    weights = np.array([0.5, 0.3])\n    bias = 0.2\n    \n    # Make predictions\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)\n```\n\n#### Explanation\n\n*   The `predict_logistic` function takes an input feature matrix `X`, model weights, and bias as inputs.\n*   It computes the linear combination of inputs, weights, and bias using `np.dot(X, weights) + bias`.\n*   The sigmoid function is applied to the linear combination to compute class probabilities.\n*   Binary predictions are made by thresholding the probabilities at 0.5.\n*   Finally, the function returns the binary predictions as a list using `predictions.tolist()`."}
{"task_id": 105, "completion_id": 0, "completion": "### Softmax Regression with Gradient Descent\n\nHere's a Python implementation of the gradient descent-based training algorithm for Softmax regression.\n\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Get the number of classes (C) and features (M)\n    C = np.max(y) + 1\n    M = X.shape[1]\n\n    # Initialize the parameter vector B with zeros\n    B = np.zeros((C, M))\n\n    # Initialize a list to store the collected loss values\n    losses = []\n\n    for _ in range(iterations):\n        # Compute the scores for each class\n        scores = np.dot(X, B.T)\n\n        # Compute the softmax probabilities\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Compute the one-hot encoded labels\n        one_hot_y = np.eye(C)[y]\n\n        # Compute the gradient of the Cross Entropy loss\n        dB = np.dot(X.T, (probs - one_hot_y)) / X.shape[0]\n\n        # Update the parameter vector B using gradient descent\n        B -= learning_rate * dB\n\n        # Compute the Cross Entropy loss\n        loss = -np.mean(np.sum(one_hot_y * np.log(probs), axis=1))\n\n        # Append the loss value to the list\n        losses.append(round(loss, 4))\n\n    # Round the parameter vector B to 4 floating points and reshape it to a list\n    B = np.round(B, 4).tolist()\n\n    # Return the optimized coefficients and collected loss values\n    return B, losses\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 3, 100)\n\n    # Train the Softmax regression model\n    learning_rate = 0.01\n    iterations = 1000\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n\n    # Print the optimized coefficients and collected loss values\n    print(\"Optimized Coefficients (B):\", B)\n    print(\"Collected Loss Values:\", losses)\n```\n\nIn this code:\n\n*   We first initialize the parameter vector `B` with zeros and a list `losses` to store the collected loss values.\n*   We then iterate `iterations` times, computing the scores, softmax probabilities, and gradient of the Cross Entropy loss at each iteration.\n*   We update the parameter vector `B` using gradient descent and compute the Cross Entropy loss.\n*   Finally, we round the optimized coefficients `B` to 4 floating points, reshape it to a list, and return it along with the collected loss values.\n\nThe example usage demonstrates how to train a Softmax regression model using the `train_softmaxreg` function and print the optimized coefficients and collected loss values."}
{"task_id": 106, "completion_id": 0, "completion": "### Logistic Regression with Gradient Descent\n\nHere's a Python implementation of the gradient descent-based training algorithm for logistic regression using Binary Cross Entropy loss.\n\n```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the sigmoid of z.\n\n    Args:\n    z (np.ndarray): Input array.\n\n    Returns:\n    np.ndarray: Sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n    y (np.ndarray): Target vector of shape (n_samples,).\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations for gradient descent.\n\n    Returns:\n    tuple[list[float], list[float]]: A tuple containing the optimized coefficients and collected loss values.\n    \"\"\"\n\n    # Initialize coefficients\n    n_features = X.shape[1]\n    coefficients = np.zeros(n_features)\n    intercept = 0\n\n    # Initialize loss history\n    loss_history = []\n\n    for _ in range(iterations):\n        # Compute predictions\n        z = np.dot(X, coefficients) + intercept\n        predictions = sigmoid(z)\n\n        # Compute gradient\n        gradient_coefficients = np.dot(X.T, (predictions - y)) / len(y)\n        gradient_intercept = np.mean(predictions - y)\n\n        # Update coefficients and intercept\n        coefficients -= learning_rate * gradient_coefficients\n        intercept -= learning_rate * gradient_intercept\n\n        # Compute Binary Cross Entropy loss\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n\n        # Store loss\n        loss_history.append(round(loss, 4))\n\n    # Round coefficients and intercept to 4 decimal places\n    coefficients = np.round(coefficients, 4).tolist()\n    intercept = round(intercept, 4)\n\n    return coefficients, [intercept] + loss_history\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some sample data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(2, size=100)\n\n    learning_rate = 0.01\n    iterations = 1000\n\n    coefficients, loss_history = train_logreg(X, y, learning_rate, iterations)\n    print(\"Optimized Coefficients:\", coefficients)\n    print(\"Loss History:\", loss_history)\n```\n\nIn this code:\n\n*   We define a `sigmoid` function to compute the sigmoid of an input array.\n*   The `train_logreg` function implements the gradient descent-based training algorithm for logistic regression.\n*   We initialize the coefficients and intercept, and an empty list to store the loss history.\n*   In each iteration, we compute predictions using the current coefficients and intercept, then update them based on the gradient of the Binary Cross Entropy loss.\n*   We store the loss at each iteration and round it to 4 decimal places.\n*   Finally, we return the optimized coefficients and the collected loss values.\n\nThe example usage demonstrates how to use the `train_logreg` function with some sample data."}
{"task_id": 107, "completion_id": 0, "completion": "## Masked Self-Attention Implementation\n\n### Overview\n\nThe following code implements masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. The implementation consists of two main functions: `compute_qkv` and `masked_attention`.\n\n### Code\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n    - X (np.ndarray): Input matrix.\n    - W_q (np.ndarray): Weight matrix for query.\n    - W_k (np.ndarray): Weight matrix for key.\n    - W_v (np.ndarray): Weight matrix for value.\n\n    Returns:\n    - Q (np.ndarray): Query matrix.\n    - K (np.ndarray): Key matrix.\n    - V (np.ndarray): Value matrix.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Args:\n    - Q (np.ndarray): Query matrix.\n    - K (np.ndarray): Key matrix.\n    - V (np.ndarray): Value matrix.\n    - mask (np.ndarray): Attention mask.\n\n    Returns:\n    - output (np.ndarray): Masked self-attention output.\n    \"\"\"\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n\n    # Apply attention mask\n    attention_scores = np.where(mask == 0, -1e9, attention_scores)\n\n    # Compute attention weights\n    attention_weights = np.softmax(attention_scores, axis=-1)\n\n    # Compute output\n    output = np.dot(attention_weights, V)\n\n    return output.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input matrix and weight matrices\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    W_q = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    W_k = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    W_v = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    # Compute Q, K, and V matrices\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    print(\"Q:\")\n    print(Q)\n    print(\"K:\")\n    print(K)\n    print(\"V:\")\n    print(V)\n\n    # Define attention mask\n    mask = np.array([[1, 1], [1, 0]])\n\n    # Compute masked self-attention output\n    output = masked_attention(Q, K, V, mask)\n    print(\"Masked Self-Attention Output:\")\n    print(output)\n```\n\n### Explanation\n\nThe `compute_qkv` function computes the Query (Q), Key (K), and Value (V) matrices by taking the dot product of the input matrix `X` with the weight matrices `W_q`, `W_k`, and `W_v`, respectively.\n\nThe `masked_attention` function computes the masked self-attention output by applying the attention mask to the attention scores. The attention scores are computed by taking the dot product of the query matrix `Q` with the transpose of the key matrix `K`, and then scaling by the square root of the key matrix's last dimension. The attention mask is applied by setting the attention scores to a very low value (-1e9) where the mask is 0. The attention weights are then computed by taking the softmax of the attention scores along the last axis. Finally, the output is computed by taking the dot product of the attention weights with the value matrix `V`.\n\nIn the example usage, we define an input matrix `X` and weight matrices `W_q`, `W_k`, and `W_v`, and compute the Q, K, and V matrices using the `compute_qkv` function. We then define an attention mask `mask` and compute the masked self-attention output using the `masked_attention` function. The output is printed to the console."}
{"task_id": 108, "completion_id": 0, "completion": "# Measuring Disorder in Apple Colors\n\n## Overview\n\nThe disorder in a basket of apples can be measured using the concept of entropy from information theory. Entropy is a measure of the amount of uncertainty or randomness in a system. In this case, we can use it to quantify the disorder in the apple colors.\n\n## Implementation\n\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n\n    Args:\n    apples (list): A list of integers representing apple colors.\n\n    Returns:\n    float: A measure of disorder in the apple colors, rounded to 4 decimal places.\n    \"\"\"\n    # Count the occurrence of each color\n    color_counts = Counter(apples)\n    \n    # Calculate the total number of apples\n    total_apples = len(apples)\n    \n    # Initialize entropy\n    entropy = 0.0\n    \n    # Calculate entropy\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # If there's only one color, entropy will be 0, which is correct\n    # Return the result rounded to 4 decimal places\n    return round(entropy, 4)\n\n# Example usage:\nprint(disorder([0,0,0,0]))  # Output: 0.0\nprint(disorder([1,1,0,0]))  # Output: 1.0\nprint(disorder([0,1,2,3]))  # Output: 2.0\nprint(disorder([0,0,1,1,2,2,3,3]))  # Output: 2.0\nprint(disorder([0,0,0,0,0,1,2,3]))  # Output: 1.584963\n```\n\n## Explanation\n\n*   We use the `Counter` class from the `collections` module to count the occurrence of each color in the list of apples.\n*   We calculate the total number of apples.\n*   We initialize the entropy to 0.0.\n*   We calculate the entropy using the formula for Shannon entropy: `H = - \u2211 p(x) log2 p(x)`, where `p(x)` is the probability of each color.\n*   We return the result rounded to 4 decimal places.\n\nThis implementation satisfies the given properties:\n\n*   A list of identical colors yields 0 disorder.\n*   A list with multiple colors has a higher disorder than a list with a single color.\n*   A list with more varied colors has a higher disorder than a list with less varied colors."}
{"task_id": 109, "completion_id": 0, "completion": "### Layer Normalization Implementation\n#### Overview\n\nLayer normalization is a technique used in deep learning to normalize the inputs of a layer across certain dimensions. Here, we implement layer normalization for sequence data, which normalizes the data across the feature dimension for each sequence.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Applies layer normalization to the input tensor X.\n\n    Args:\n    - X (np.ndarray): A 3D array of shape (batch_size, sequence_length, feature_dim).\n    - gamma (np.ndarray): A 1D array of shape (feature_dim,) for scaling.\n    - beta (np.ndarray): A 1D array of shape (feature_dim,) for shifting.\n    - epsilon (float, optional): A small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    - list: The normalized X as a list, rounded to 5 decimal places.\n    \"\"\"\n\n    # Calculate the mean across the feature dimension for each sequence\n    mean = np.mean(X, axis=-1, keepdims=True)\n\n    # Calculate the variance across the feature dimension for each sequence\n    variance = np.var(X, axis=-1, keepdims=True)\n\n    # Normalize the data\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n\n    # Apply scaling and shifting parameters\n    output = gamma * normalized_X + beta\n\n    # Round to 5 decimal places and convert to a list\n    output = np.round(output, 5).tolist()\n\n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a sample input tensor\n    X = np.array([\n        [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\n        [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\n    ])\n\n    # Define scaling and shifting parameters\n    gamma = np.array([1.0, 1.0, 1.0])\n    beta = np.array([0.0, 0.0, 0.0])\n\n    # Apply layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n\n    # Print the result\n    print(normalized_X)\n```\n\n#### Explanation\n\n*   The `layer_normalization` function takes in a 3D input tensor `X`, scaling parameters `gamma`, shifting parameters `beta`, and an optional `epsilon` value for numerical stability.\n*   It calculates the mean and variance of `X` across the feature dimension (last axis) for each sequence.\n*   The data is then normalized by subtracting the mean and dividing by the square root of the variance plus `epsilon`.\n*   The scaling and shifting parameters are applied to the normalized data.\n*   Finally, the output is rounded to 5 decimal places and converted to a list using the `tolist()` method.\n\nThis implementation provides a clear and efficient way to perform layer normalization on sequence data. The example usage demonstrates how to apply layer normalization to a sample input tensor."}
{"task_id": 110, "completion_id": 0, "completion": "# METEOR Score Calculation\n## Overview\n\nThe METEOR score is a metric used to evaluate the quality of machine translations. It is based on the degree of alignment between the reference translation and the candidate translation. The score takes into account unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation.\n\n## Implementation\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n    - reference (list): The reference translation.\n    - candidate (list): The candidate translation.\n    - alpha (float, optional): The weight for precision. Defaults to 0.9.\n    - beta (float, optional): The weight for recall. Defaults to 3.\n    - gamma (float, optional): The weight for penalty. Defaults to 0.5.\n\n    Returns:\n    - float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    # Calculate unigram matches\n    ref_counts = Counter(reference)\n    cand_counts = Counter(candidate)\n    matches = sum(min(ref_counts[word], cand_counts[word]) for word in ref_counts.keys() & cand_counts.keys())\n\n    # Calculate precision, recall, and F-mean\n    precision = matches / len(candidate) if candidate else 0\n    recall = matches / len(reference) if reference else 0\n    f_mean = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if precision + recall > 0 else 0\n\n    # Calculate penalty for word order fragmentation\n    aligned = []\n    for i, word in enumerate(candidate):\n        if word in ref_counts and ref_counts[word] > 0:\n            aligned.append(1)\n            ref_counts[word] -= 1\n        else:\n            aligned.append(0)\n\n    chunks = []\n    current_chunk = []\n    for is_aligned in aligned:\n        if is_aligned:\n            current_chunk.append(is_aligned)\n        else:\n            if current_chunk:\n                chunks.append(current_chunk)\n                current_chunk = []\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    penalty = gamma * len(chunks) / len(candidate) if candidate else 0\n\n    # Calculate METEOR score\n    meteor = alpha * f_mean + (1 - alpha) * penalty\n    meteor = max(0, meteor)\n\n    return round(meteor, 3)\n\n# Example usage\nreference = [\"This\", \"is\", \"a\", \"reference\", \"translation\"]\ncandidate = [\"This\", \"is\", \"a\", \"candidate\", \"translation\"]\nprint(meteor_score(reference, candidate))\n```\n\n## Explanation\n\nThe provided Python function `meteor_score` calculates the METEOR score for evaluating machine translation quality. It takes into account unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation.\n\n*   The function first calculates the unigram matches between the reference and candidate translations.\n*   Then, it computes precision, recall, and F-mean based on the matches.\n*   A penalty for word order fragmentation is calculated by identifying aligned and non-aligned words in the candidate translation.\n*   Finally, the METEOR score is computed as a weighted combination of F-mean and the penalty.\n\nThe function returns the METEOR score rounded to 3 decimal places.\n\n## Advice\n\n*   The METEOR score is sensitive to the choice of parameters `alpha`, `beta`, and `gamma`. You may need to adjust these parameters based on your specific use case.\n*   This implementation assumes that the input translations are lists of words. If your translations are in a different format (e.g., strings), you may need to preprocess them before passing them to the `meteor_score` function."}
{"task_id": 111, "completion_id": 0, "completion": "## Computing Pointwise Mutual Information (PMI)\n\n### Overview\n\nThe Pointwise Mutual Information (PMI) is a measure used in information theory and statistics to quantify the mutual dependence between two events. It calculates how much the actual joint occurrence of events differs from what we would expect by chance.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total count of event X.\n    total_counts_y (int): The total count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The computed PMI value rounded to 3 decimal places.\n    \"\"\"\n\n    # Check for zero counts to avoid division by zero\n    if joint_counts == 0:\n        return 0.0\n\n    # Calculate probabilities\n    prob_xy = joint_counts / total_samples\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n\n    # Check for zero probabilities to avoid division by zero\n    if prob_x == 0 or prob_y == 0:\n        return 0.0\n\n    # Compute PMI using the formula: PMI(X;Y) = log2(P(X,Y) / (P(X)P(Y)))\n    pmi = np.log2(prob_xy / (prob_x * prob_y))\n\n    # Return PMI value rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    joint_counts = 10\n    total_counts_x = 100\n    total_counts_y = 50\n    total_samples = 1000\n\n    pmi_value = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\n    print(f\"Computed PMI: {pmi_value}\")\n```\n\n### Explanation\n\nThe provided Python function `compute_pmi` calculates the Pointwise Mutual Information between two events given their joint occurrence count, individual counts, and the total number of samples.\n\n1.  It first checks if the joint count is zero to avoid division by zero errors.\n2.  Then, it calculates the probabilities of the joint occurrence and individual events.\n3.  It checks for zero probabilities to avoid division by zero errors.\n4.  The PMI is computed using the formula: `PMI(X;Y) = log2(P(X,Y) / (P(X)P(Y)))`.\n5.  Finally, it returns the computed PMI value rounded to 3 decimal places.\n\nThe example usage demonstrates how to call the `compute_pmi` function with sample values and print the result."}
{"task_id": 112, "completion_id": 0, "completion": "## Min-Max Normalization Implementation\n\n### Description\nThe following Python function performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1]. This helps ensure that all features contribute equally to a model by scaling them to a common range.\n\n### Code\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers.\n    \n    Args:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    \n    # Check if the input list is empty\n    if not x:\n        return []\n    \n    # Find the minimum and maximum values in the list\n    min_val = min(x)\n    max_val = max(x)\n    \n    # If all values in the list are the same, return a list of zeros\n    if min_val == max_val:\n        return [0.0] * len(x)\n    \n    # Perform Min-Max Normalization\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    \n    # Round the normalized values to 4 decimal places\n    normalized = [round(val, 4) for val in normalized]\n    \n    return normalized\n\n# Example usage:\nif __name__ == \"__main__\":\n    values = [10, 20, 30, 40, 50]\n    print(min_max(values))\n```\n\n### Explanation\n\n1. The function `min_max` takes a list of integers `x` as input and returns a list of floats.\n2. It first checks if the input list is empty and returns an empty list if true.\n3. It then finds the minimum and maximum values in the list using the built-in `min` and `max` functions.\n4. If all values in the list are the same (i.e., `min_val == max_val`), it returns a list of zeros, as there's no range to normalize.\n5. The Min-Max Normalization is performed using a list comprehension, where each value `i` in the list is scaled to the range [0, 1] using the formula `(i - min_val) / (max_val - min_val)`.\n6. Finally, the normalized values are rounded to 4 decimal places using another list comprehension and the `round` function.\n7. The example usage demonstrates how to use the `min_max` function with a sample list of integers."}
{"task_id": 113, "completion_id": 0, "completion": "### Implementing a Simple Residual Block with Shortcut Connection\n\nHere's a Python function that creates a simple residual block using NumPy. This function takes a 1D input array, processes it through two weight layers using matrix multiplication, applies ReLU activations, and adds the original input via a shortcut connection before a final ReLU activation.\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Creates a simple residual block with a shortcut connection.\n\n    Args:\n    - x (np.ndarray): 1D input array.\n    - w1 (np.ndarray): Weight matrix for the first layer.\n    - w2 (np.ndarray): Weight matrix for the second layer.\n\n    Returns:\n    - list: The output of the residual block rounded to 4 decimal places.\n    \"\"\"\n\n    # Ensure input and weights are numpy arrays\n    x = np.asarray(x)\n    w1 = np.asarray(w1)\n    w2 = np.asarray(w2)\n\n    # Check if input is 1D\n    assert len(x.shape) == 1, \"Input must be a 1D array\"\n\n    # Check if weights are 2D\n    assert len(w1.shape) == 2 and len(w2.shape) == 2, \"Weights must be 2D arrays\"\n\n    # Check if the number of columns in w1 matches the number of elements in x\n    assert w1.shape[1] == x.shape[0], \"Number of columns in w1 must match the number of elements in x\"\n\n    # Check if the number of rows in w2 matches the number of rows in w1\n    assert w2.shape[0] == w1.shape[0], \"Number of rows in w2 must match the number of rows in w1\"\n\n    # Apply the first weight layer and ReLU activation\n    z1 = np.matmul(w1, x)\n    a1 = np.maximum(z1, 0)\n\n    # Apply the second weight layer and ReLU activation\n    z2 = np.matmul(w2, a1)\n    a2 = np.maximum(z2, 0)\n\n    # Apply the shortcut connection and final ReLU activation\n    output = np.maximum(x + a2, 0)\n\n    # Round the output to 4 decimal places and convert to a list\n    output = np.round(output, 4).tolist()\n\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the input and weights\n    x = np.array([1, 2, 3])\n    w1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    w2 = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])\n\n    # Create the residual block\n    output = residual_block(x, w1, w2)\n    print(output)\n```\n\nIn this code:\n\n*   We first import the necessary NumPy library.\n*   The `residual_block` function takes a 1D input array `x` and two weight matrices `w1` and `w2` as inputs.\n*   It applies the first weight layer using matrix multiplication, followed by ReLU activation.\n*   Then, it applies the second weight layer and ReLU activation.\n*   The function implements a shortcut connection by adding the original input `x` to the output of the second ReLU activation.\n*   Finally, it applies a final ReLU activation and rounds the output to 4 decimal places before converting it to a list and returning it.\n*   In the example usage section, we demonstrate how to use the `residual_block` function with sample inputs and weights."}
{"task_id": 114, "completion_id": 0, "completion": "### Global Average Pooling Implementation\n\nHere's a Python function that performs Global Average Pooling on a 3D NumPy array:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Args:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n\n    Raises:\n    ValueError: If the input array is not of shape (height, width, channels).\n    \"\"\"\n\n    # Check if the input array has the correct shape\n    if len(x.shape) != 3:\n        raise ValueError(\"Input array must be of shape (height, width, channels)\")\n\n    # Calculate the average of all values in each feature map\n    # We use np.mean with axis=(0, 1) to compute the mean along the height and width axes\n    avg_pool = np.mean(x, axis=(0, 1))\n\n    return avg_pool\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample 3D NumPy array\n    np.random.seed(0)\n    height, width, channels = 4, 4, 3\n    x = np.random.rand(height, width, channels)\n\n    # Perform Global Average Pooling\n    result = global_avg_pool(x)\n\n    print(\"Input shape:\", x.shape)\n    print(\"Output shape:\", result.shape)\n    print(\"Result:\", result)\n```\n\n### Explanation\n\n*   The `global_avg_pool` function takes a 3D NumPy array `x` as input and returns a 1D NumPy array.\n*   It first checks if the input array has the correct shape (height, width, channels). If not, it raises a `ValueError`.\n*   The function then uses `np.mean` with `axis=(0, 1)` to compute the mean along the height and width axes, effectively performing Global Average Pooling.\n*   The result is a 1D NumPy array where each element is the average of all values in the corresponding feature map.\n\n### Example Usage\n\nThe example code demonstrates how to use the `global_avg_pool` function. It creates a sample 3D NumPy array with shape (4, 4, 3), performs Global Average Pooling, and prints the input and output shapes along with the result."}
{"task_id": 115, "completion_id": 0, "completion": "### Batch Normalization Implementation\n\nHere's a Python function that performs batch normalization on a 4D NumPy array in the BCHW format.\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Applies batch normalization to a 4D NumPy array in BCHW format.\n\n    Args:\n    - X (np.ndarray): Input array in BCHW format.\n    - gamma (np.ndarray): Scale parameter.\n    - beta (np.ndarray): Shift parameter.\n    - epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    - list: Normalized output.\n    \"\"\"\n\n    # Get the batch size, number of channels, height, and width from the input array\n    batch_size, num_channels, height, width = X.shape\n\n    # Calculate the mean across the batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n\n    # Calculate the variance across the batch and spatial dimensions for each channel\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n\n    # Normalize the input array\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n\n    # Apply scale and shift parameters\n    output = gamma * normalized_X + beta\n\n    # Round the output to 4 decimal places and convert it to a list\n    output = np.round(output, 4).tolist()\n\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample input array in BCHW format\n    X = np.random.rand(2, 3, 4, 5)\n\n    # Create sample scale and shift parameters\n    gamma = np.random.rand(3)\n    beta = np.random.rand(3)\n\n    # Apply batch normalization\n    normalized_output = batch_normalization(X, gamma, beta)\n\n    # Print the normalized output\n    print(normalized_output)\n```\n\nIn this code:\n\n*   We first calculate the mean and variance of the input array across the batch and spatial dimensions for each channel.\n*   We then normalize the input array using these mean and variance values, adding a small epsilon value for numerical stability.\n*   Next, we apply the scale (gamma) and shift (beta) parameters to the normalized array.\n*   Finally, we round the output to 4 decimal places and convert it to a list before returning it.\n\nThe example usage demonstrates how to create a sample input array, scale and shift parameters, and apply batch normalization using the provided function."}
{"task_id": 116, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires implementing a function that computes the derivative of a polynomial term of the form `c * x^n` at a given point `x`, where `c` is a coefficient and `n` is the exponent. The function should return the value of the derivative, accounting for the coefficient in the power rule.\n\nKey constraints and edge cases:\n\n* The function should handle the case where `n` is 0, in which case the derivative is 0.\n* The function should handle the case where `x` is 0, which is a valid input.\n* The function should return a float value rounded to 4 decimal places.\n\nInput:\n\n* `c`: the coefficient of the polynomial term (float)\n* `x`: the point at which to evaluate the derivative (float)\n* `n`: the exponent of the polynomial term (float)\n\nOutput:\n\n* The value of the derivative of the polynomial term at `x` (float)\n\nSubtle requirements or implicit constraints:\n\n* The power rule of differentiation states that if we have a term of the form `x^n`, its derivative is `n * x^(n-1)`.\n* The derivative of a constant term (i.e., `n` is 0) is 0.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n* The power rule of differentiation can be applied to the polynomial term `c * x^n` to obtain its derivative.\n* The derivative of `c * x^n` is `c * n * x^(n-1)`.\n* If `n` is 0, the derivative is 0, regardless of the values of `c` and `x`.\n* The function should handle the case where `x` is 0, which is a valid input.\n\nPatterns or properties being used:\n\n* The power rule of differentiation.\n* The property that the derivative of a constant term is 0.\n\nKey algorithmic insights:\n\n* The function can be implemented by directly applying the power rule of differentiation.\n* The function should handle the edge case where `n` is 0.\n\nImportant realizations:\n\n* The function should return a float value rounded to 4 decimal places.\n* The function should handle the case where `x` is 0.\n\nCritical examples or test cases:\n\n* `c = 2`, `x = 3`, `n = 2`: the derivative is `2 * 2 * 3^(2-1) = 12`.\n* `c = 1`, `x = 0`, `n = 1`: the derivative is `1 * 1 * 0^(1-1) = 1`.\n* `c = 3`, `x = 2`, `n = 0`: the derivative is `0`.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Understand the power rule of differentiation and its application to the polynomial term `c * x^n`.\n2. Handle the edge case where `n` is 0.\n3. Implement the function by directly applying the power rule of differentiation.\n4. Round the result to 4 decimal places.\n5. Verify the implementation with test cases.\n\nCritical details:\n\n* The function should handle the case where `x` is 0.\n* The function should return a float value rounded to 4 decimal places.\n\nImportant optimizations or special cases to handle:\n\n* The edge case where `n` is 0.\n\nHere's the python solution:\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    This function computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is evaluated.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative of the polynomial term at x, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        derivative = 0.0\n    else:\n        derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n```"}
{"task_id": 117, "completion_id": 0, "completion": "### Computing Orthonormal Basis for 2D Vectors using Gram-Schmidt Process\n\n#### Overview\n\nThe Gram-Schmidt process is a method for orthonormalizing a set of vectors in an inner product space, most commonly the Euclidean space R^n. Here, we implement this process to compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n#### Implementation\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n    Args:\n    - vectors (list[list[float]]): A list of 2D vectors.\n    - tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    - list[list[float]]: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n\n    # Convert input vectors to NumPy array for easier manipulation\n    vectors = np.array(vectors)\n\n    # Initialize an empty list to store the orthonormal basis\n    basis = []\n\n    # Iterate over each vector in the input list\n    for v in vectors:\n        # Convert the current vector to a NumPy array\n        v = np.array(v)\n\n        # Check if the current vector is already in the span of the basis\n        if basis:\n            # Project the current vector onto the basis\n            proj = np.sum(np.dot(v, b) * b for b in basis)\n\n            # Subtract the projection from the current vector\n            v = v - proj\n\n            # Check if the resulting vector is linearly independent (not too small)\n            if np.linalg.norm(v) > tol:\n                # Normalize the vector to have unit length\n                v = v / np.linalg.norm(v)\n\n                # Add the orthonormal vector to the basis\n                basis.append(v)\n\n        else:\n            # If the basis is empty, normalize the current vector and add it to the basis\n            if np.linalg.norm(v) > tol:\n                v = v / np.linalg.norm(v)\n                basis.append(v)\n\n    # Round the basis vectors to 4 decimal places and convert to list\n    basis = [b.round(4).tolist() for b in basis]\n\n    return basis\n\n# Example usage\nif __name__ == \"__main__\":\n    vectors = [[1, 0], [3, 4]]\n    tol = 1e-10\n    orthonormal_basis_vectors = orthonormal_basis(vectors, tol)\n    print(\"Orthonormal Basis Vectors:\")\n    for i, vector in enumerate(orthonormal_basis_vectors):\n        print(f\"Vector {i+1}: {vector}\")\n```\n\n#### Explanation\n\n*   The function `orthonormal_basis` takes a list of 2D vectors and an optional tolerance value `tol` as input.\n*   It applies the Gram-Schmidt process to the input vectors to compute an orthonormal basis for the subspace they span.\n*   The function checks for linear independence by ensuring that the norm of each vector is greater than the specified tolerance `tol`.\n*   The resulting orthonormal basis vectors are rounded to 4 decimal places and returned as a list of lists.\n\n#### Advice\n\n*   The Gram-Schmidt process can be sensitive to numerical instability for large inputs or when dealing with nearly linearly dependent vectors. Consider using more robust methods like QR decomposition for larger or more ill-conditioned inputs.\n*   The choice of tolerance `tol` depends on the specific application and the desired level of precision. A smaller tolerance will result in a more accurate orthonormal basis but may also lead to numerical instability."}
{"task_id": 118, "completion_id": 0, "completion": "## Computing the Cross Product of Two 3D Vectors\n\n### Overview\n\nThe cross product of two vectors in 3D space results in a third vector that is perpendicular to both input vectors. This function will take two 3D vectors as input and return their cross product.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or numpy.ndarray): The first 3D vector.\n        b (list or numpy.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product of vectors a and b, rounded to 4 decimal places.\n\n    Raises:\n        ValueError: If either vector a or b is not a 3D vector.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays\n    a = np.asarray(a)\n    b = np.asarray(b)\n\n    # Check if inputs are 3D vectors\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError(\"Both inputs must be 3D vectors\")\n\n    # Compute the cross product using numpy's cross function\n    cross_product = np.cross(a, b)\n\n    # Round the result to 4 decimal places and convert to a list\n    result = (cross_product).round(4).tolist()\n\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    vector_a = [1, 2, 3]\n    vector_b = [4, 5, 6]\n    print(cross_product(vector_a, vector_b))\n```\n\n### Explanation\n\n*   The function `cross_product` takes two 3D vectors `a` and `b` as input.\n*   It first checks if both inputs are 3D vectors. If not, it raises a `ValueError`.\n*   The cross product is computed using `np.cross(a, b)`.\n*   The result is then rounded to 4 decimal places using the `round` method and converted to a list using the `tolist` method.\n*   Finally, the function returns the cross product as a list.\n\n### Notes\n\n*   This implementation uses the `numpy` library for efficient numerical computations.\n*   The example usage demonstrates how to use the `cross_product` function with two sample 3D vectors."}
{"task_id": 119, "completion_id": 0, "completion": "### Cramer's Rule Implementation in Python\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n    A (numpy.ndarray): A square coefficient matrix.\n    b (numpy.ndarray): A constant vector.\n\n    Returns:\n    list: The solution vector x if a unique solution exists, otherwise -1.\n    \"\"\"\n\n    # Check if A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be a square matrix\")\n\n    # Check if the number of equations matches the number of variables\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"The number of equations must match the number of variables\")\n\n    # Calculate the determinant of matrix A\n    det_A = np.linalg.det(A)\n\n    # If the determinant is zero, the system has no unique solution\n    if det_A == 0:\n        return -1\n\n    # Initialize the solution vector\n    x = np.zeros_like(b)\n\n    # Apply Cramer's Rule for each variable\n    for i in range(A.shape[0]):\n        # Create a copy of matrix A\n        A_i = A.copy()\n\n        # Replace the i-th column of A_i with vector b\n        A_i[:, i] = b\n\n        # Calculate the determinant of A_i\n        det_A_i = np.linalg.det(A_i)\n\n        # Calculate the i-th variable using Cramer's Rule\n        x[i] = det_A_i / det_A\n\n    # Round the solution vector to the nearest 4th decimal\n    x = np.round(x, 4)\n\n    # Return the solution vector as a list\n    return x.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[3, 1], [1, 2]])\n    b = np.array([9, 8])\n\n    result = cramers_rule(A, b)\n\n    if result != -1:\n        print(\"Solution vector x:\", result)\n    else:\n        print(\"The system has no unique solution.\")\n```\n\n### Explanation\n\nThe provided Python function `cramers_rule` implements Cramer's Rule to solve a system of linear equations $Ax = b$. Here's a step-by-step breakdown:\n\n1.  **Input Validation**: The function first checks if the input matrix `A` is square and if the number of equations matches the number of variables.\n2.  **Determinant Calculation**: It calculates the determinant of matrix `A` using `np.linalg.det(A)`. If the determinant is zero, the function returns `-1`, indicating that the system has no unique solution.\n3.  **Cramer's Rule Application**: For each variable $x_i$, the function creates a copy of matrix `A`, replaces the $i$-th column with vector `b`, and calculates the determinant of the modified matrix `A_i`. The $i$-th variable is then computed using Cramer's Rule: $x_i = \\frac{\\det(A_i)}{\\det(A)}$.\n4.  **Solution Vector**: The function rounds the solution vector to the nearest 4th decimal using `np.round(x, 4)` and returns it as a list using `x.tolist()`.\n\nThe example usage demonstrates how to use the `cramers_rule` function to solve a system of linear equations. If a unique solution exists, it prints the solution vector; otherwise, it indicates that the system has no unique solution."}
{"task_id": 120, "completion_id": 0, "completion": "# Problem Understanding\n\nThe problem requires calculating the Bhattacharyya distance between two probability distributions. The Bhattacharyya distance is a measure of similarity between two probability distributions.\n\nKey constraints and edge cases:\n\n- The inputs are two lists representing discrete probability distributions `p` and `q`.\n- The function should return the Bhattacharyya distance rounded to 4 decimal places.\n- If the inputs have different lengths or are empty, the function should return 0.0.\n- The inputs are lists of floats, and the output is a float.\n\nInput: Two lists of floats representing discrete probability distributions `p` and `q`.\nOutput: The Bhattacharyya distance between `p` and `q` rounded to 4 decimal places.\n\nSubtle requirements or implicit constraints:\n\n- The inputs must be valid probability distributions, meaning their elements must be non-negative and sum up to 1.\n- The function should handle edge cases where the inputs have different lengths or are empty.\n\n# Key Insights\n\nCrucial observations that lead to the solution:\n\n- The Bhattacharyya distance between two discrete probability distributions `p` and `q` is given by the formula: $D_B(p, q) = -\\ln(\\sum_{i=1}^{n} \\sqrt{p_i q_i})$, where $n$ is the number of elements in the distributions.\n- The formula involves calculating the sum of the square roots of the products of corresponding elements in `p` and `q`.\n- The Bhattacharyya distance is always non-negative.\n- If the inputs have different lengths or are empty, the function should return 0.0.\n\nPatterns or properties being used:\n\n- The properties of probability distributions, specifically that their elements are non-negative and sum up to 1.\n- The formula for the Bhattacharyya distance, which involves calculating the sum of the square roots of the products of corresponding elements.\n\nCritical examples or test cases that led to insights:\n\n- The example with `p = [0.5, 0.5]` and `q = [0.6, 0.4]`, which illustrates how to calculate the Bhattacharyya distance.\n- The edge cases where `p = []` and `q = [0.5, 0.5]`, or `p = [0.5, 0.5]` and `q = [0.4, 0.6, 0]`, which show how to handle invalid inputs.\n\n# Solution Strategy\n\nSteps to solve this problem:\n\n1. Check if the inputs `p` and `q` have the same length and are not empty. If not, return 0.0.\n2. Calculate the sum of the square roots of the products of corresponding elements in `p` and `q`.\n3. If the sum is 0, return 0.0 to avoid mathematical errors.\n4. Calculate the Bhattacharyya distance using the formula: $D_B(p, q) = -\\ln(\\sum_{i=1}^{n} \\sqrt{p_i q_i})$.\n5. Round the result to 4 decimal places.\n\nImportant optimizations or special cases to handle:\n\n- Handling edge cases where the inputs have different lengths or are empty.\n- Avoiding mathematical errors when the sum of the square roots is 0.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    sum_sqrt = 0\n    for i in range(len(p)):\n        if p[i] < 0 or q[i] < 0:\n            return 0.0  # Probability distributions should not have negative values\n        sum_sqrt += np.sqrt(p[i] * q[i])\n    \n    if sum_sqrt == 0:\n        return 0.0  # To avoid math domain error for log(0)\n    else:\n        db = -np.log(sum_sqrt)\n        return round(db, 4)\n```"}
{"task_id": 121, "completion_id": 0, "completion": "## Vector Element-wise Sum Function\n### Description\nThis function calculates the element-wise sum of two input vectors. It checks if the vectors have compatible dimensions (i.e., the same length) and returns a new vector representing the sum if they are compatible. If the vectors have incompatible dimensions, it returns -1.\n\n### Code\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n\n    Returns:\n    list[int|float]: A new vector representing the element-wise sum of a and b if they have the same length, -1 otherwise.\n    \"\"\"\n\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n\n    # Use list comprehension to compute the element-wise sum\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test with vectors of the same length\n    vector1 = [1, 2, 3]\n    vector2 = [4, 5, 6]\n    print(vector_sum(vector1, vector2))  # Output: [5, 7, 9]\n\n    # Test with vectors of different lengths\n    vector3 = [1, 2]\n    vector4 = [3, 4, 5]\n    print(vector_sum(vector3, vector4))  # Output: -1\n```\n\n### Explanation\n\n*   The function `vector_sum` takes two lists `a` and `b` as input, which represent the vectors to be summed.\n*   It first checks if the lengths of `a` and `b` are equal. If not, it immediately returns -1 to indicate that the operation is invalid due to incompatible dimensions.\n*   If the lengths are equal, it uses a list comprehension with the `zip` function to iterate over the elements of `a` and `b` in parallel, adding corresponding elements together.\n*   The resulting list of sums is then returned as the result.\n*   In the example usage section, we demonstrate the function's behavior with both compatible and incompatible vector inputs."}
{"task_id": 122, "completion_id": 0, "completion": "# Policy Gradient with REINFORCE\n## Description\nThis code implements the policy gradient estimator using the REINFORCE algorithm. The policy is parameterized by a 2D NumPy array `theta` of shape `(num_states, num_actions)`. The policy for each state is computed via softmax over `theta[s, :]`. Given a list of episodes (each a list of (state, action, reward) tuples), compute the average gradient of the log-policy multiplied by the return at each time step.\n\n## Implementation\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the policy gradient using REINFORCE algorithm.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape (num_states, num_actions) parameterizing the policy.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list: The policy gradient.\n    \"\"\"\n\n    # Compute the policy for each state\n    def compute_policy(theta):\n        exp_theta = np.exp(theta)\n        policy = exp_theta / np.sum(exp_theta, axis=1, keepdims=True)\n        return policy\n\n    policy = compute_policy(theta)\n\n    # Initialize the policy gradient\n    policy_gradient = np.zeros_like(theta)\n\n    # Iterate over each episode\n    for episode in episodes:\n        # Compute the rewards for the episode\n        rewards = [reward for _, _, reward in episode]\n\n        # Compute the returns for the episode\n        returns = []\n        cumulative_reward = 0\n        for reward in reversed(rewards):\n            cumulative_reward += reward\n            returns.append(cumulative_reward)\n        returns = list(reversed(returns))\n\n        # Iterate over each time step in the episode\n        for t, (state, action, _) in enumerate(episode):\n            # Compute the log-policy\n            log_policy = np.log(policy[state])\n\n            # Compute the gradient of the log-policy\n            gradient = np.zeros_like(theta)\n            gradient[state] = np.exp(log_policy) / policy[state]\n            gradient[state, action] -= 1\n\n            # Multiply the gradient by the return\n            gradient *= returns[t]\n\n            # Accumulate the policy gradient\n            policy_gradient += gradient\n\n    # Compute the average policy gradient\n    policy_gradient /= len(episodes)\n\n    # Round the result to the nearest 4th decimal\n    policy_gradient = np.round(policy_gradient, 4)\n\n    # Return the policy gradient as a list\n    return policy_gradient.tolist()\n```\n\n## Example Usage\n```python\ntheta = np.array([[0.1, 0.2], [0.3, 0.4]])\nepisodes = [\n    [(0, 0, 10), (1, 1, 20)],\n    [(0, 1, 30), (1, 0, 40)]\n]\n\npolicy_gradient = compute_policy_gradient(theta, episodes)\nprint(policy_gradient)\n```"}
{"task_id": 123, "completion_id": 0, "completion": "### Computational Efficiency of MoE Layer\n\nThe following Python function calculates the computational cost savings of a Mixture of Experts (MoE) layer compared to a dense layer.\n\n#### Code\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    - n_experts (int): The total number of experts in the MoE layer.\n    - k_active (int): The number of active experts in the MoE layer.\n    - d_in (int): The input dimension of the layer.\n    - d_out (int): The output dimension of the layer.\n\n    Returns:\n    - flops_dense (float): The floating-point operations for a dense layer.\n    - flops_moe (float): The floating-point operations for an MoE layer.\n    - savings_percentage (float): The computational cost savings percentage of the MoE layer.\n\n    \"\"\"\n    # Calculate FLOPs for dense layer: d_in * d_out\n    flops_dense = d_in * d_out\n\n    # Calculate FLOPs for MoE layer: k_active * (d_in + d_out)\n    # Assuming that the experts' outputs are combined with a learned weight per expert \n    # (so an additional d_out operations per expert)\n    flops_moe = k_active * (d_in * d_out // n_experts + d_out)\n\n    # Calculate savings percentage\n    savings_percentage = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    # Round results to the nearest 1st decimal\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_percentage = round(savings_percentage, 1)\n\n    return flops_dense, flops_moe, savings_percentage\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    n_experts = 10\n    k_active = 2\n    d_in = 1024\n    d_out = 1024\n\n    flops_dense, flops_moe, savings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\n\n    print(f\"Floating-point operations for dense layer: {flops_dense}\")\n    print(f\"Floating-point operations for MoE layer: {flops_moe}\")\n    print(f\"Computational cost savings percentage: {savings_percentage}%\")\n```\n\n#### Explanation\n\nThe provided Python function `compute_efficiency` takes four parameters:\n\n*   `n_experts`: The total number of experts in the MoE layer.\n*   `k_active`: The number of active experts in the MoE layer.\n*   `d_in`: The input dimension of the layer.\n*   `d_out`: The output dimension of the layer.\n\nIt calculates the floating-point operations (FLOPs) for both a dense layer and an MoE layer and determines the computational cost savings percentage of the MoE layer.\n\nThe FLOPs for a dense layer are calculated as `d_in * d_out`, assuming that each input element is connected to every output element.\n\nFor the MoE layer, the FLOPs are calculated as `k_active * (d_in * d_out // n_experts + d_out)`, assuming that:\n\n*   Each expert processes a fraction of the input dimension (`d_in // n_experts`).\n*   Each expert produces `d_out` outputs.\n*   Only `k_active` experts are active.\n\nThe savings percentage is then calculated as `((flops_dense - flops_moe) / flops_dense) * 100`.\n\nThe results are rounded to the nearest 1st decimal using the `round` function.\n\nThe example usage demonstrates how to call the `compute_efficiency` function with specific parameters and print the results."}
{"task_id": 124, "completion_id": 0, "completion": "### Noisy Top-K Gating Function Implementation\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Args:\n    - X (np.ndarray): Input matrix.\n    - W_g (np.ndarray): Weight matrix for gating.\n    - W_noise (np.ndarray): Pre-sampled noise weight matrix.\n    - N (np.ndarray): Pre-sampled noise.\n    - k (int): Sparsity constraint.\n\n    Returns:\n    - A list of gating probabilities for each expert, rounded to 4 decimal places.\n    \"\"\"\n\n    # Compute logits by taking the dot product of input and gating weight\n    logits = np.dot(X, W_g.T)\n\n    # Add noise to logits\n    noisy_logits = logits + np.dot(N, W_noise.T)\n\n    # Get top-k indices for each sample\n    top_k_indices = np.argsort(noisy_logits, axis=1)[:, -k:]\n\n    # Create a mask for top-k indices\n    mask = np.zeros_like(noisy_logits)\n    mask[np.arange(mask.shape[0])[:, None], top_k_indices] = 1\n\n    # Compute gating probabilities by applying softmax to noisy logits after masking\n    gating_probabilities = np.exp(noisy_logits * mask) / np.sum(np.exp(noisy_logits * mask), axis=1, keepdims=True)\n\n    # Round results to 4 decimal places and return as a list\n    return (gating_probabilities.round(4)).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define input matrix\n    X = np.array([[1, 2], [3, 4]])\n\n    # Define gating weight matrix\n    W_g = np.array([[0.1, 0.2], [0.3, 0.4]])\n\n    # Define noise weight matrix\n    W_noise = np.array([[0.01, 0.02], [0.03, 0.04]])\n\n    # Define pre-sampled noise\n    N = np.array([[0.001, 0.002], [0.003, 0.004]])\n\n    # Define sparsity constraint k\n    k = 1\n\n    # Compute noisy top-k gating probabilities\n    gating_probabilities = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(gating_probabilities)\n```\n\nIn this code:\n\n*   We define a function `noisy_topk_gating` that takes an input matrix `X`, gating weight matrix `W_g`, noise weight matrix `W_noise`, pre-sampled noise `N`, and sparsity constraint `k`.\n*   We compute logits by taking the dot product of the input and gating weight.\n*   We add noise to logits using the pre-sampled noise and noise weight matrix.\n*   We get the top-k indices for each sample using `np.argsort`.\n*   We create a mask for top-k indices and apply it to noisy logits.\n*   We compute gating probabilities by applying softmax to noisy logits after masking.\n*   Finally, we round the results to 4 decimal places and return them as a list.\n\nThe example usage demonstrates how to use the `noisy_topk_gating` function with sample inputs."}
{"task_id": 125, "completion_id": 0, "completion": "### Sparse Mixture of Experts Layer Implementation\n#### Overview\n\nThe following code implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing. This layer takes an input tensor `x`, a set of expert weight matrices `We`, a gating weight matrix `Wg`, and parameters specifying the number of experts `n_experts` and the value of `top_k`. It computes the final MoE output by selecting the top-k experts per token, applying their transformations, and aggregating the results weighted by the normalized gating probabilities.\n\n#### Implementation\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Compute the output of a Sparse Mixture of Experts (MoE) layer.\n\n    Parameters:\n    - x (np.ndarray): Input tensor of shape (batch_size, input_dim)\n    - We (np.ndarray): Stack of expert weight matrices of shape (n_experts, output_dim, input_dim)\n    - Wg (np.ndarray): Gating weight matrix of shape (n_experts, input_dim)\n    - n_experts (int): Number of experts\n    - top_k (int): Number of top experts to select per token\n\n    Returns:\n    - list: MoE output reshaped to a list, rounded to the nearest 4th decimal\n    \"\"\"\n\n    # Compute gating probabilities\n    gating_logits = np.matmul(Wg, x.T)  # shape: (n_experts, batch_size)\n    gating_probs = np.softmax(gating_logits, axis=0)  # shape: (n_experts, batch_size)\n\n    # Select top-k experts per token\n    top_k_indices = np.argsort(gating_probs, axis=0)[:-top_k-1:-1]  # shape: (top_k, batch_size)\n    top_k_probs = gating_probs[top_k_indices, np.arange(gating_probs.shape[1])]  # shape: (top_k, batch_size)\n\n    # Compute expert outputs\n    expert_outputs = []\n    for i in range(n_experts):\n        expert_output = np.matmul(We[i], x.T)  # shape: (output_dim, batch_size)\n        expert_outputs.append(expert_output)\n\n    # Aggregate expert outputs weighted by gating probabilities\n    moE_output = np.zeros((We.shape[1], x.shape[0]))\n    for k in range(top_k):\n        expert_output = expert_outputs[top_k_indices[k]]\n        moE_output += expert_output * top_k_probs[k]\n\n    # Round to the nearest 4th decimal and reshape to a list\n    moE_output = np.round(moE_output.T, 4).tolist()\n\n    return moE_output\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(0)\n\n    batch_size = 2\n    input_dim = 3\n    output_dim = 4\n    n_experts = 5\n    top_k = 2\n\n    x = np.random.rand(batch_size, input_dim)\n    We = np.random.rand(n_experts, output_dim, input_dim)\n    Wg = np.random.rand(n_experts, input_dim)\n\n    moE_output = moe(x, We, Wg, n_experts, top_k)\n    print(moE_output)\n```\n\nIn this code:\n\n*   We first compute the gating probabilities by taking the softmax of the gating logits.\n*   Then, we select the top-k experts per token based on the gating probabilities.\n*   Next, we compute the output of each expert by applying their transformations to the input.\n*   After that, we aggregate the expert outputs weighted by the normalized gating probabilities.\n*   Finally, we round the MoE output to the nearest 4th decimal and reshape it to a list using numpy's `tolist()` method.\n\nThe example usage demonstrates how to call the `moe` function with random inputs and print the resulting MoE output."}
{"task_id": 126, "completion_id": 0, "completion": "### Group Normalization Implementation\n#### Overview\n\nGroup normalization is a technique used in deep learning for normalizing inputs across a group of channels. This implementation provides a Python function to perform group normalization on a 4D input tensor.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Applies group normalization to a 4D input tensor.\n\n    Args:\n    - X (np.ndarray): Input tensor with shape (B, C, H, W)\n    - gamma (np.ndarray): Learnable scale parameter\n    - beta (np.ndarray): Learnable shift parameter\n    - num_groups (int): Number of groups for normalization\n    - epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    - list: Normalized tensor as a list, rounded to 4 decimal places\n    \"\"\"\n\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n\n    # Calculate the number of channels per group\n    channels_per_group = C // num_groups\n\n    # Reshape the input tensor to group channels together\n    X = X.reshape((B, num_groups, channels_per_group, H, W))\n\n    # Calculate the mean and variance for each group\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize the input tensor\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n\n    # Reshape the normalized tensor back to its original shape\n    normalized_X = normalized_X.reshape((B, C, H, W))\n\n    # Apply the learned scale (gamma) and shift (beta)\n    output = gamma * normalized_X + beta\n\n    # Round the output to 4 decimal places and return as a list\n    return (np.round(output, 4)).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    np.random.seed(0)\n    X = np.random.rand(1, 16, 32, 32)\n\n    # Initialize gamma and beta\n    gamma = np.random.rand(16)\n    beta = np.random.rand(16)\n\n    num_groups = 2\n    result = group_normalization(X, gamma, beta, num_groups)\n    print(result)\n```\n\n#### Explanation\n\nThe provided Python function `group_normalization` performs group normalization on a 4D input tensor `X`. It takes in the input tensor `X`, learnable parameters `gamma` and `beta`, the number of groups `num_groups`, and an optional `epsilon` value for numerical stability.\n\n1.  The function first calculates the number of channels per group.\n2.  It then reshapes the input tensor to group channels together.\n3.  The mean and variance are computed for each group.\n4.  The input tensor is normalized using the calculated mean and variance.\n5.  Finally, the learnable scale `gamma` and shift `beta` are applied to the normalized tensor.\n\nThe result is rounded to 4 decimal places and returned as a list.\n\nThe example usage demonstrates how to use the `group_normalization` function with a random input tensor, gamma, and beta values."}
{"task_id": 127, "completion_id": 0, "completion": "## Finding Captain Redbeard's Hidden Treasure\n\n### Problem Description\n\nThe problem requires finding the value of $x$ where the function $f(x) = x^4 - 3x^3 + 2$ reaches its minimum, starting from any random initial position.\n\n### Solution\n\nWe will use gradient descent to find the minimum of the function. The gradient of $f(x)$ is given by $f'(x) = 4x^3 - 9x^2$.\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Find the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Args:\n    - start_x (float): The initial position.\n    - learning_rate (float, optional): The learning rate for gradient descent. Defaults to 0.1.\n    - tolerance (float, optional): The tolerance for convergence. Defaults to 1e-6.\n    - max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    - float: The value of x where f(x) reaches its minimum, rounded to 4 decimal places.\n    \"\"\"\n\n    # Define the function and its derivative\n    def f(x: float) -> float:\n        return x**4 - 3*x**3 + 2\n\n    def df(x: float) -> float:\n        return 4*x**3 - 9*x**2\n\n    # Initialize the current position\n    x = start_x\n\n    # Perform gradient descent\n    for _ in range(max_iters):\n        # Compute the gradient\n        gradient = df(x)\n\n        # Update the position\n        x_new = x - learning_rate * gradient\n\n        # Check for convergence\n        if abs(x_new - x) < tolerance:\n            return round(x_new, 4)\n\n        # Update the current position\n        x = x_new\n\n    # If we reach this point, it means we didn't converge within max_iters\n    return round(x, 4)\n\n# Example usage\nstart_x = 1.0\nmin_x = find_treasure(start_x)\nprint(f\"The treasure is hidden at x = {min_x}\")\n```\n\n### Explanation\n\n*   We define the function `f(x)` and its derivative `df(x)`.\n*   We initialize the current position `x` to the start position `start_x`.\n*   We perform gradient descent by iteratively updating the position `x` based on the gradient of the function.\n*   We check for convergence by verifying if the change in the position is less than the specified tolerance.\n*   If we converge within the maximum number of iterations, we return the position `x` where the function reaches its minimum, rounded to 4 decimal places.\n\n### Advice\n\n*   The choice of learning rate and tolerance can significantly affect the convergence of the algorithm. You may need to adjust these parameters depending on the specific problem.\n*   Gradient descent is a simple and efficient optimization algorithm, but it may not always converge to the global minimum, especially for non-convex functions. In such cases, you may need to use more advanced optimization algorithms or techniques, such as stochastic gradient descent or simulated annealing."}
{"task_id": 128, "completion_id": 0, "completion": "### Dynamic Tanh (DyT) Function Implementation\n\nThe Dynamic Tanh (DyT) function is a normalization-free transformation inspired by the Tanh function. It replaces layer normalization in Transformer architectures while preserving squashing behavior and enabling stable training.\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n\n    Args:\n    - x (np.ndarray): Input array.\n    - alpha (float): Scaling factor.\n    - gamma (float): Shift factor.\n    - beta (float): Additional scaling factor.\n\n    Returns:\n    - list[float]: DyT transformed values as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Apply the DyT transformation\n    dyt = np.tanh(alpha * (x + gamma)) * beta\n\n    # Round results to the nearest 4th decimal\n    dyt = np.round(dyt, 4)\n\n    # Reshape and convert to list\n    result = dyt.tolist()\n\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample input array\n    x = np.array([1.2345, 2.3456, 3.4567, 4.5678])\n\n    # Define DyT parameters\n    alpha = 1.0\n    gamma = 0.5\n    beta = 1.2\n\n    # Apply the DyT transformation\n    result = dynamic_tanh(x, alpha, gamma, beta)\n\n    # Print the result\n    print(result)\n```\n\nIn this code:\n\n*   We define the `dynamic_tanh` function, which takes an input array `x` and parameters `alpha`, `gamma`, and `beta`.\n*   The DyT transformation is applied using the formula `tanh(alpha * (x + gamma)) * beta`.\n*   The results are rounded to the nearest 4th decimal using `np.round`.\n*   The transformed values are reshaped and converted to a list using the `tolist` method.\n*   In the example usage section, we demonstrate how to use the `dynamic_tanh` function with a sample input array and DyT parameters."}
