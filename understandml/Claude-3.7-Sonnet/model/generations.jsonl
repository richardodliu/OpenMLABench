{"task_id": 0, "completion_id": 0, "completion": "I'll implement a Ridge Regression solver using the closed-form solution as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Implements Ridge (L2-regularized) Linear Regression using the closed-form solution.\n    \n    Args:\n        data: An ndarray where the last column is the target y and all preceding columns are features X\n        alpha: Regularization strength parameter (default: 0.0)\n    \n    Returns:\n        A list of coefficients (weights) rounded to 4 decimal places, or -1 if the matrix is not invertible\n    \"\"\"\n    # Split data into features (X) and target (y)\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias term (column of ones) to X\n    X_with_bias = np.column_stack((np.ones(X.shape[0]), X))\n    \n    # Calculate X^T * X\n    X_transpose_X = np.dot(X_with_bias.T, X_with_bias)\n    \n    # Add regularization term (alpha * I)\n    n_features = X_with_bias.shape[1]\n    regularization = alpha * np.eye(n_features)\n    \n    # Calculate (X^T * X + alpha * I)\n    matrix_to_invert = X_transpose_X + regularization\n    \n    # Check if the matrix is invertible (determinant not equal to 0)\n    if np.linalg.det(matrix_to_invert) == 0:\n        return -1\n    \n    try:\n        # Calculate (X^T * X + alpha * I)^-1\n        inverted_matrix = np.linalg.inv(matrix_to_invert)\n        \n        # Calculate (X^T * X + alpha * I)^-1 * X^T\n        intermediate = np.dot(inverted_matrix, X_with_bias.T)\n        \n        # Calculate (X^T * X + alpha * I)^-1 * X^T * y\n        weights = np.dot(intermediate, y)\n        \n        # Round to 4 decimal places and convert to list\n        rounded_weights = [round(w, 4) for w in weights]\n        \n        return rounded_weights\n    \n    except np.linalg.LinAlgError:\n        # In case of numerical issues with inversion\n        return -1\n```\n\nThis implementation:\n1. Separates the features and target from the input data\n2. Adds a bias term (column of ones) to the feature matrix\n3. Computes the closed-form solution for ridge regression\n4. Checks if the matrix is invertible before attempting inversion\n5. Returns the weights rounded to 4 decimal places as a Python list\n6. Returns -1 if the matrix is not invertible\n\nThe solution handles the regularization parameter alpha and includes proper error checking for non-invertible matrices."}
{"task_id": 1, "completion_id": 0, "completion": "# Dual-Form Perceptron Learning Algorithm\n\nI'll implement the dual-form perceptron learning algorithm as requested. This approach uses the Gram matrix of inner products between samples and maintains dual parameters \u03b1 instead of directly updating the weight vector.\n\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples = X_train.shape[0]\n    \n    # Initialize dual coefficients and bias\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    \n    # Precompute the Gram matrix (all pairwise inner products)\n    K = np.dot(X_train, X_train.T)  # K[i,j] = x_i \u00b7 x_j\n    \n    # Counter for the number of updates\n    updates = 0\n    \n    while updates < n_iter:\n        misclassified = False\n        \n        # Scan through all training samples\n        for i in range(n_samples):\n            # Compute activation using dual formulation\n            activation = np.sum(alpha * y_train * K[:, i]) + b\n            \n            # Check if the margin is violated (misclassification)\n            if y_train[i] * activation <= 0:\n                # Update alpha and bias\n                alpha[i] += eta\n                b += eta * y_train[i]\n                \n                misclassified = True\n                updates += 1\n                \n                # Restart the scan from the beginning\n                break\n        \n        # If no misclassification in the entire pass, we're done\n        if not misclassified:\n            break\n    \n    # Compute the primal weight vector from dual coefficients\n    w = np.sum(alpha.reshape(-1, 1) * y_train.reshape(-1, 1) * X_train, axis=0)\n    \n    # Round to 4 decimal places\n    w_list = [round(float(val), 4) for val in w]\n    b_rounded = round(float(b), 4)\n    \n    return w_list, b_rounded\n```\n\nThis implementation follows the algorithm steps as described:\n1. It initializes \u03b1 and b to zero\n2. Precomputes the Gram matrix for efficient inner product calculations\n3. Scans through the samples and updates \u03b1 and b when a misclassification is found\n4. Restarts the scan after each update\n5. Stops when either a full pass with no updates occurs or the maximum number of updates is reached\n6. Computes the primal weight vector from the dual coefficients\n7. Returns the weight vector as a list and the bias, both rounded to 4 decimal places"}
{"task_id": 2, "completion_id": 0, "completion": "I'll implement the standard GLM link functions as requested. Here's a complete solution:\n\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {}\n    \n    # 1. Identity link (for Gaussian family)\n    links[\"identity\"] = {\n        \"link\": lambda mu: mu,\n        \"inv_link\": lambda eta: eta,\n        \"derivative\": lambda mu: np.ones_like(mu)\n    }\n    \n    # 2. Log link (for Poisson family)\n    links[\"log\"] = {\n        \"link\": lambda mu: np.log(mu),\n        \"inv_link\": lambda eta: np.exp(eta),\n        \"derivative\": lambda mu: 1.0 / mu\n    }\n    \n    # 3. Logit link (for Bernoulli/Binomial family)\n    links[\"logit\"] = {\n        \"link\": lambda mu: np.log(mu / (1.0 - mu)),\n        \"inv_link\": lambda eta: 1.0 / (1.0 + np.exp(-eta)),\n        \"derivative\": lambda mu: 1.0 / (mu * (1.0 - mu))\n    }\n    \n    return links\n\n\n# Example usage\nif __name__ == \"__main__\":\n    links = glm_links()\n    \n    # Test with scalars\n    print(\"Testing with scalars:\")\n    mu_scalar = 0.2\n    \n    # Identity link\n    eta_identity = links[\"identity\"][\"link\"](mu_scalar)\n    mu_back_identity = links[\"identity\"][\"inv_link\"](eta_identity)\n    deriv_identity = links[\"identity\"][\"derivative\"](mu_scalar)\n    print(f\"Identity: g(\u03bc) = {eta_identity:.4f}, g\u207b\u00b9(\u03b7) = {mu_back_identity:.4f}, g'(\u03bc) = {deriv_identity:.4f}\")\n    \n    # Log link\n    eta_log = links[\"log\"][\"link\"](mu_scalar)\n    mu_back_log = links[\"log\"][\"inv_link\"](eta_log)\n    deriv_log = links[\"log\"][\"derivative\"](mu_scalar)\n    print(f\"Log: g(\u03bc) = {eta_log:.4f}, g\u207b\u00b9(\u03b7) = {mu_back_log:.4f}, g'(\u03bc) = {deriv_log:.4f}\")\n    \n    # Logit link\n    eta_logit = links[\"logit\"][\"link\"](mu_scalar)\n    mu_back_logit = links[\"logit\"][\"inv_link\"](eta_logit)\n    deriv_logit = links[\"logit\"][\"derivative\"](mu_scalar)\n    print(f\"Logit: g(\u03bc) = {eta_logit:.4f}, g\u207b\u00b9(\u03b7) = {mu_back_logit:.4f}, g'(\u03bc) = {deriv_logit:.4f}\")\n    \n    # Test with arrays\n    print(\"\\nTesting with arrays:\")\n    mu_array = np.array([0.2, 0.8])\n    \n    # Identity link\n    eta_identity = links[\"identity\"][\"link\"](mu_array)\n    mu_back_identity = links[\"identity\"][\"inv_link\"](eta_identity)\n    deriv_identity = links[\"identity\"][\"derivative\"](mu_array)\n    print(f\"Identity: g(\u03bc) = [{eta_identity[0]:.4f}, {eta_identity[1]:.4f}], \"\n          f\"g\u207b\u00b9(\u03b7) = [{mu_back_identity[0]:.4f}, {mu_back_identity[1]:.4f}], \"\n          f\"g'(\u03bc) = [{deriv_identity[0]:.4f}, {deriv_identity[1]:.4f}]\")\n    \n    # Log link\n    eta_log = links[\"log\"][\"link\"](mu_array)\n    mu_back_log = links[\"log\"][\"inv_link\"](eta_log)\n    deriv_log = links[\"log\"][\"derivative\"](mu_array)\n    print(f\"Log: g(\u03bc) = [{eta_log[0]:.4f}, {eta_log[1]:.4f}], \"\n          f\"g\u207b\u00b9(\u03b7) = [{mu_back_log[0]:.4f}, {mu_back_log[1]:.4f}], \"\n          f\"g'(\u03bc) = [{deriv_log[0]:.4f}, {deriv_log[1]:.4f}]\")\n    \n    # Logit link\n    eta_logit = links[\"logit\"][\"link\"](mu_array)\n    mu_back_logit = links[\"logit\"][\"inv_link\"](eta_logit)\n    deriv_logit = links[\"logit\"][\"derivative\"](mu_array)\n    print(f\"Logit: g(\u03bc) = [{eta_logit[0]:.4f}, {eta_logit[1]:.4f}], \"\n          f\"g\u207b\u00b9(\u03b7) = [{mu_back_logit[0]:.4f}, {mu_back_logit[1]:.4f}], \"\n          f\"g'(\u03bc) = [{deriv_logit[0]:.4f}, {deriv_logit[1]:.4f}]\")\n```\n\nThis implementation provides the three required link functions (identity, log, and logit) along with their inverses and derivatives. Each function works with both numpy scalars and arrays through element-wise operations. The example code demonstrates the usage with both scalar and array inputs, and rounds the displayed values to four decimal places as requested."}
{"task_id": 3, "completion_id": 0, "completion": "I'll implement the isolation tree path length function according to the specifications. The approach will involve building a recursive tree structure and then tracking the depth of each sample.\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    \n    n_samples = data.shape[0]\n    depths = [0] * n_samples  # Initialize depths for all samples\n    \n    def build_tree(indices, depth=0):\n        \"\"\"Recursively build the isolation tree and record depths.\"\"\"\n        if len(indices) < 3:\n            # Leaf node - record depth for all samples in this leaf\n            for idx in indices:\n                depths[idx] = depth\n            return\n        \n        # Choose a random feature\n        n_features = data.shape[1]\n        feature_idx = np.random.randint(0, n_features)\n        \n        # Find min and max values for the chosen feature\n        feature_values = data[indices, feature_idx]\n        min_val = np.min(feature_values)\n        max_val = np.max(feature_values)\n        \n        # Choose a random split value\n        split_val = np.random.uniform(min_val, max_val)\n        \n        # Split the indices\n        left_indices = [idx for idx in indices if data[idx, feature_idx] <= split_val]\n        right_indices = [idx for idx in indices if data[idx, feature_idx] > split_val]\n        \n        # Continue building the tree recursively\n        if left_indices:\n            build_tree(left_indices, depth + 1)\n        if right_indices:\n            build_tree(right_indices, depth + 1)\n    \n    # Start building the tree with all indices\n    all_indices = list(range(n_samples))\n    build_tree(all_indices)\n    \n    return depths\n```\n\nThis implementation follows the specified algorithm:\n\n1. It sets the random seed to 0 for reproducibility\n2. It recursively builds the isolation tree:\n   - If fewer than 3 samples reach a node, it becomes a leaf\n   - Otherwise, it randomly selects a feature and a split value\n   - It divides the samples based on the split and continues recursively\n3. It tracks the depth of each sample in the `depths` list\n4. It returns the depths as a list where the i-th element corresponds to the i-th row of the input matrix\n\nFor the example input `data = np.arange(5).reshape(-1, 1)`, this should return `[2, 2, 2, 1, 1]` as specified."}
{"task_id": 7, "completion_id": 0, "completion": "# Frequent Itemset Mining Solution\n\nI'll implement the Apriori algorithm, which is a classic approach for finding frequent itemsets. The algorithm works by iteratively generating candidate itemsets and then filtering out those that don't meet the minimum support threshold.\n\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    if not transactions or minsup <= 0 or minsup > 1:\n        return {}\n    \n    # Convert transactions to sets to handle duplicates within transactions\n    transaction_sets = [set(transaction) for transaction in transactions]\n    n_transactions = len(transaction_sets)\n    \n    # Calculate the absolute minimum support count\n    min_support_count = math.ceil(minsup * n_transactions)\n    \n    # Find frequent 1-itemsets\n    item_counts = Counter()\n    for transaction in transaction_sets:\n        for item in transaction:\n            item_counts[item] += 1\n    \n    # Filter items that meet the minimum support\n    frequent_items = {item for item, count in item_counts.items() if count >= min_support_count}\n    \n    # Sort frequent items for consistent ordering\n    frequent_items = sorted(frequent_items)\n    \n    # Initialize result dictionary with frequent 1-itemsets\n    result = {(item,): count for item, count in item_counts.items() \n              if count >= min_support_count}\n    \n    # Generate frequent k-itemsets for k >= 2\n    k = 2\n    prev_frequent_itemsets = [frozenset([item]) for item in frequent_items]\n    \n    while prev_frequent_itemsets:\n        # Generate candidate k-itemsets\n        candidates = []\n        for i in range(len(prev_frequent_itemsets)):\n            for j in range(i + 1, len(prev_frequent_itemsets)):\n                itemset1 = prev_frequent_itemsets[i]\n                itemset2 = prev_frequent_itemsets[j]\n                \n                # If the first k-2 items are the same, we can join them\n                if len(itemset1.union(itemset2)) == k:\n                    candidates.append(itemset1.union(itemset2))\n        \n        # Count support for candidates\n        candidate_counts = Counter()\n        for transaction in transaction_sets:\n            for candidate in candidates:\n                if candidate.issubset(transaction):\n                    candidate_counts[candidate] += 1\n        \n        # Filter candidates that meet the minimum support\n        frequent_k_itemsets = [itemset for itemset, count in candidate_counts.items() \n                              if count >= min_support_count]\n        \n        # Add frequent k-itemsets to result\n        for itemset in frequent_k_itemsets:\n            sorted_itemset = tuple(sorted(itemset))\n            result[sorted_itemset] = candidate_counts[itemset]\n        \n        # Prepare for next iteration\n        prev_frequent_itemsets = frequent_k_itemsets\n        k += 1\n        \n        # If no frequent k-itemsets are found, break the loop\n        if not frequent_k_itemsets:\n            break\n    \n    return result\n```\n\nThis implementation follows the Apriori algorithm's principles:\n\n1. First, it finds all frequent 1-itemsets by counting occurrences of each item.\n2. Then, it iteratively generates candidate k-itemsets from frequent (k-1)-itemsets.\n3. For each candidate, it counts its support across all transactions.\n4. It filters out candidates that don't meet the minimum support threshold.\n5. The process continues until no more frequent itemsets can be found.\n\nThe algorithm returns a dictionary where keys are tuples of sorted items (representing itemsets) and values are their support counts."}
{"task_id": 11, "completion_id": 0, "completion": "# K-Means Clustering Implementation from Scratch\n\nI'll implement the K-Means clustering algorithm without using any external machine learning libraries, following all the specified requirements.\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    # Validate inputs\n    if k < 1 or k > len(data):\n        raise ValueError(\"k must be between 1 and the number of samples\")\n    \n    # Initialize centers with the first k samples\n    centers = data[:k].copy()\n    \n    # Main K-means loop\n    for iteration in range(max_iter):\n        # Store previous centers to check for convergence\n        prev_centers = centers.copy()\n        \n        # Assign each sample to the nearest center\n        distances = np.zeros((len(data), k))\n        for i in range(k):\n            # Calculate squared Euclidean distance for each sample to center i\n            distances[:, i] = np.sum((data - centers[i])**2, axis=1)\n        \n        # Get the index of the closest center for each sample\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centers\n        max_change = 0\n        for i in range(k):\n            # Get samples assigned to this cluster\n            cluster_samples = data[labels == i]\n            \n            # Handle empty clusters\n            if len(cluster_samples) == 0:\n                # Reinitialize with a random sample\n                random_idx = np.random.randint(0, len(data))\n                centers[i] = data[random_idx]\n            else:\n                # Calculate new center as the mean of assigned samples\n                new_center = np.mean(cluster_samples, axis=0)\n                \n                # Calculate change in center position\n                center_change = np.sqrt(np.sum((centers[i] - new_center)**2))\n                max_change = max(max_change, center_change)\n                \n                # Update center\n                centers[i] = new_center\n        \n        # Check for convergence\n        if max_change < epsilon:\n            break\n    \n    # Round centers to 4 decimal places\n    centers_rounded = [[round(value, 4) for value in center] for center in centers]\n    \n    # Convert labels to list\n    labels_list = labels.tolist()\n    \n    return centers_rounded, labels_list\n```\n\nThis implementation follows all the requirements:\n1. It initializes centers with the first k samples\n2. It assigns samples to the nearest center using squared Euclidean distance\n3. It recomputes centers as the mean of assigned samples\n4. It handles empty clusters by reinitializing with a random sample\n5. It stops when the maximum change is less than epsilon or max_iter is reached\n6. It rounds the final centers to 4 decimal places\n7. It returns both the centers and the cluster labels"}
{"task_id": 13, "completion_id": 0, "completion": "I'll implement the item-based k-NN collaborative filtering recommender according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_users, n_items = data.shape\n    \n    # Step 1: Build the item-item similarity matrix\n    S = np.zeros((n_items, n_items))\n    \n    for i in range(n_items):\n        for j in range(i, n_items):  # Only compute upper triangle (symmetric matrix)\n            # Find users who rated both items i and j\n            users_rated_both = np.where((data[:, i] > 0) & (data[:, j] > 0))[0]\n            \n            # If no users rated both items, similarity is 0\n            if len(users_rated_both) == 0:\n                S[i, j] = S[j, i] = 0\n                continue\n            \n            # Get rating vectors for both items from users who rated both\n            v1 = data[users_rated_both, i]\n            v2 = data[users_rated_both, j]\n            \n            if criterion == \"cosine\":\n                # Mean center vectors if std > 1e-3\n                std1 = np.std(v1)\n                std2 = np.std(v2)\n                \n                if std1 > 1e-3:\n                    v1 = v1 - np.mean(v1)\n                if std2 > 1e-3:\n                    v2 = v2 - np.mean(v2)\n                \n                # Compute cosine similarity\n                norm1 = np.linalg.norm(v1)\n                norm2 = np.linalg.norm(v2)\n                \n                if norm1 > 0 and norm2 > 0:\n                    similarity = np.dot(v1, v2) / (norm1 * norm2)\n                else:\n                    similarity = 0\n            \n            elif criterion == \"pearson\":\n                # Compute Pearson correlation\n                if len(v1) > 1:  # Need at least 2 points for correlation\n                    similarity = np.corrcoef(v1, v2)[0, 1]\n                    # Handle NaN (can happen if one vector has zero variance)\n                    if np.isnan(similarity):\n                        similarity = 0\n                else:\n                    similarity = 0\n            \n            # Set the similarity value in the matrix (symmetric)\n            S[i, j] = S[j, i] = similarity\n    \n    # Step 2: Get items rated by the active user\n    user_ratings = data[user_ind]\n    rated_items = np.where(user_ratings > 0)[0]\n    unrated_items = np.where(user_ratings == 0)[0]\n    \n    # Step 5: If user has rated all items, return empty list\n    if len(unrated_items) == 0:\n        return []\n    \n    # Step 3: Compute predicted scores for unrated items\n    scores = {}\n    for t in unrated_items:\n        # Get similarities between unrated item t and all rated items\n        similarities = S[t, rated_items]\n        \n        # Calculate the numerator: sum of (rating * similarity)\n        numerator = np.sum(user_ratings[rated_items] * similarities)\n        \n        # Calculate the denominator: sum of absolute similarities\n        denominator = np.sum(np.abs(similarities))\n        \n        # Calculate the score (if denominator is 0, score is 0)\n        score = numerator / denominator if denominator > 0 else 0\n        \n        # Store the score\n        scores[t] = score\n    \n    # Step 4: Sort items by decreasing score and return top k\n    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    recommended_items = [item for item, score in sorted_items[:k]]\n    \n    return recommended_items\n```\n\nThis implementation follows the algorithm steps exactly as described:\n\n1. It builds an item-item similarity matrix using either cosine similarity (with mean centering when standard deviation > 1e-3) or Pearson correlation.\n2. It identifies the items already rated by the active user.\n3. For each unrated item, it computes a predicted score based on the weighted average of the user's ratings for similar items.\n4. It returns the top k unrated items sorted by their predicted scores.\n5. If the user has already rated all items, it returns an empty list.\n\nThe implementation handles edge cases like zero denominators and ensures that the sorting is stable (items with the same score will be ordered by their index)."}
{"task_id": 19, "completion_id": 0, "completion": "I'll solve this task by implementing the `best_split` function for gradient boosting trees. The function will evaluate all possible splits and return the one with the highest gain.\n\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Calculate the loss of the current node (before splitting)\n    G_total = np.sum(g)\n    H_total = np.sum(h)\n    current_loss = -0.5 * (G_total**2) / (H_total + lam) + gamma\n    \n    best_gain = 0.0\n    best_split = None\n    \n    # Iterate through each feature\n    for feature_idx in range(n_features):\n        # Get the feature values and sort them\n        feature_values = X[:, feature_idx]\n        \n        # Sort indices by feature values\n        sorted_indices = np.argsort(feature_values)\n        sorted_feature_values = feature_values[sorted_indices]\n        sorted_g = g[sorted_indices]\n        sorted_h = h[sorted_indices]\n        \n        # Find unique values to use as potential thresholds\n        # We need to ensure we don't consider duplicate values as thresholds\n        unique_values = np.unique(sorted_feature_values)\n        \n        # If there's only one unique value, we can't split on this feature\n        if len(unique_values) <= 1:\n            continue\n        \n        # Initialize cumulative sums for left side\n        G_left = 0.0\n        H_left = 0.0\n        \n        # Track the last index we processed for each unique value\n        last_idx = 0\n        \n        # Evaluate each potential threshold (except the last unique value)\n        for i in range(len(unique_values) - 1):\n            # Find the last index with this value\n            while last_idx < n_samples and sorted_feature_values[last_idx] <= unique_values[i]:\n                G_left += sorted_g[last_idx]\n                H_left += sorted_h[last_idx]\n                last_idx += 1\n            \n            # Calculate right side statistics\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n            \n            # Check if we have at least 2 samples on each side\n            if last_idx < 2 or last_idx > n_samples - 2:\n                continue\n            \n            # Calculate loss for left and right children\n            loss_left = -0.5 * (G_left**2) / (H_left + lam) + gamma\n            loss_right = -0.5 * (G_right**2) / (H_right + lam) + gamma\n            \n            # Calculate gain\n            gain = current_loss - loss_left - loss_right\n            \n            # Update best split if this one is better\n            if gain > best_gain:\n                best_gain = gain\n                # Use the midpoint between current and next unique value as threshold\n                threshold = (unique_values[i] + unique_values[i+1]) / 2\n                best_split = (feature_idx, threshold)\n    \n    # Return the best split if it has a positive gain, otherwise None\n    return best_split if best_gain > 0 else None\n```\n\nThis implementation:\n\n1. Calculates the current loss before any split\n2. Iterates through each feature\n3. For each feature, sorts the values and finds unique values to use as potential thresholds\n4. For each potential threshold, calculates the gain from splitting\n5. Tracks the best split that maximizes the gain\n6. Returns the best split if it has a positive gain, otherwise None\n\nThe algorithm ensures that each split has at least two samples on each side, as required."}
{"task_id": 20, "completion_id": 0, "completion": "I'll implement the sigmoid activation function and its gradient according to your requirements.\n\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Convert input to numpy array if it's a list\n    is_scalar = isinstance(x, (int, float))\n    if not is_scalar and not isinstance(x, np.ndarray):\n        x = np.array(x)\n    \n    # Compute sigmoid: sigmoid(x) = 1 / (1 + exp(-x))\n    sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n    \n    # Compute gradient: sigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))\n    gradient_x = sigmoid_x * (1.0 - sigmoid_x)\n    \n    # Round to 4 decimal places\n    if is_scalar:\n        return round(float(sigmoid_x), 4), round(float(gradient_x), 4)\n    else:\n        # Convert numpy arrays to Python lists and round each element\n        sigmoid_x_rounded = np.round(sigmoid_x, 4).tolist()\n        gradient_x_rounded = np.round(gradient_x, 4).tolist()\n        return sigmoid_x_rounded, gradient_x_rounded\n```\n\nThis implementation:\n1. Handles scalar inputs (int/float), Python lists, and NumPy arrays\n2. Computes the sigmoid function and its gradient\n3. Rounds all values to 4 decimal places\n4. Returns the appropriate type based on the input (float for scalars, Python lists for array-like inputs)\n5. Uses only the standard library and NumPy as required\n\nThe sigmoid function is calculated as 1/(1+e^(-x)), and its gradient is sigmoid(x)*(1-sigmoid(x))."}
{"task_id": 21, "completion_id": 0, "completion": "# AdaBoost with One-Dimensional Decision Stumps\n\nI'll implement the AdaBoost algorithm with one-dimensional decision stumps as requested. The implementation will follow the steps outlined in the task description.\n\n```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble's training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    n_samples = len(x_train)\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(n_samples) / n_samples\n    \n    # Sort training data for threshold calculation\n    sorted_indices = np.argsort(x_train)\n    sorted_x = x_train[sorted_indices]\n    sorted_y = y_train[sorted_indices]\n    \n    # Initialize ensemble\n    ensemble = []  # Will store (alpha, d, threshold) for each weak classifier\n    \n    # Continue training until error is below epsilon\n    while True:\n        # Find the best stump\n        best_stump = find_best_stump(x_train, y_train, weights)\n        d, threshold, min_error = best_stump\n        \n        # Calculate alpha (classifier weight)\n        alpha = 0.5 * np.log((1 - min_error) / max(min_error, 1e-10))\n        \n        # Add the stump to the ensemble\n        ensemble.append((alpha, d, threshold))\n        \n        # Check if we've reached the desired training error\n        ensemble_predictions = predict_ensemble(x_train, ensemble)\n        training_error = np.sum(ensemble_predictions != y_train) / n_samples\n        \n        if training_error <= epsilon:\n            break\n        \n        # Update weights\n        h_predictions = predict_stump(x_train, d, threshold)\n        weights *= np.exp(-alpha * y_train * h_predictions)\n        weights /= np.sum(weights)  # Normalize weights\n    \n    # Make predictions on test data\n    return predict_ensemble(x_test, ensemble).tolist()\n\ndef find_best_stump(x, y, weights):\n    \"\"\"Find the best decision stump given the current weights.\"\"\"\n    n_samples = len(x)\n    \n    # Sort data for threshold calculation\n    sorted_indices = np.argsort(x)\n    sorted_x = x[sorted_indices]\n    sorted_y = y[sorted_indices]\n    sorted_weights = weights[sorted_indices]\n    \n    # Generate thresholds (midpoints between consecutive sorted values)\n    thresholds = []\n    for i in range(n_samples - 1):\n        if sorted_x[i] != sorted_x[i + 1]:  # Avoid duplicate thresholds\n            thresholds.append((sorted_x[i] + sorted_x[i + 1]) / 2)\n    \n    # If no valid thresholds (all x values are the same), use the single value\n    if not thresholds and n_samples > 0:\n        thresholds = [sorted_x[0]]\n    \n    best_error = float('inf')\n    best_d = 0\n    best_threshold = 0\n    \n    # Try both directions (d=0 and d=1)\n    for d in [0, 1]:\n        for threshold in thresholds:\n            # Make predictions with this stump\n            predictions = predict_stump(x, d, threshold)\n            \n            # Calculate weighted error\n            error = np.sum(weights * (predictions != y))\n            \n            if error < best_error:\n                best_error = error\n                best_d = d\n                best_threshold = threshold\n    \n    return best_d, best_threshold, best_error\n\ndef predict_stump(x, d, threshold):\n    \"\"\"Make predictions using a single decision stump.\"\"\"\n    if d == 0:\n        return np.where(x <= threshold, 1, -1)\n    else:  # d == 1\n        return np.where(x > threshold, 1, -1)\n\ndef predict_ensemble(x, ensemble):\n    \"\"\"Make predictions using the entire ensemble.\"\"\"\n    # Initialize predictions with zeros\n    ensemble_scores = np.zeros(len(x))\n    \n    # Sum up the weighted predictions from each weak classifier\n    for alpha, d, threshold in ensemble:\n        predictions = predict_stump(x, d, threshold)\n        ensemble_scores += alpha * predictions\n    \n    # Return the sign of the ensemble scores\n    return np.sign(ensemble_scores).astype(int)\n```\n\nThis implementation follows the AdaBoost algorithm with one-dimensional decision stumps as specified:\n\n1. It starts with uniform sample weights\n2. It finds the best stump by trying all possible thresholds (midpoints between consecutive sorted training points)\n3. It computes the alpha coefficient for each selected stump\n4. It updates and normalizes the sample weights\n5. It stops when the training error is not larger than the specified epsilon\n6. It returns predictions for the test set using the trained ensemble\n\nThe implementation handles edge cases like when all training points have the same value, and it ensures numerical stability when calculating alpha."}
{"task_id": 25, "completion_id": 0, "completion": "# Gaussian Kernel SVM Prediction Solution\n\nI'll implement a fully vectorized solution for predicting labels using a pre-trained Gaussian kernel SVM.\n\n```python\nimport numpy as np\nimport math\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    m = X_test.shape[0]  # Number of test samples\n    n = X_train.shape[0]  # Number of training samples\n    \n    # Initialize predictions array\n    predictions = np.zeros(m)\n    \n    # Vectorized implementation of the decision function\n    # For each test point z, compute g(z) = \u03a3_{i=1..n} \u03b1_i \u00b7 y_i \u00b7 exp(-\u03b3 \u00b7 ||x_i \u2212 z||\u00b2) + b\n    \n    # Compute the kernel matrix between X_train and X_test\n    # This is a vectorized way to compute the squared Euclidean distance ||x_i - z_j||\u00b2\n    # for all pairs of training samples x_i and test samples z_j\n    \n    # For each test sample\n    for i in range(m):\n        # Compute squared Euclidean distances between the current test sample and all training samples\n        # Using broadcasting to compute ||x_i - z||\u00b2 for all i\n        squared_diffs = np.sum((X_train - X_test[i, :])**2, axis=1)\n        \n        # Compute the kernel values: exp(-\u03b3 \u00b7 ||x_i - z||\u00b2)\n        kernel_values = np.exp(-gamma * squared_diffs)\n        \n        # Compute the decision function value: \u03a3_{i=1..n} \u03b1_i \u00b7 y_i \u00b7 K(x_i, z) + b\n        decision_value = np.sum(alpha * y_train * kernel_values) + b\n        \n        # Determine the predicted class based on the sign of the decision function\n        predictions[i] = 1 if decision_value >= 0 else -1\n    \n    # Convert to list of integers\n    return predictions.astype(int).tolist()\n```\n\nThis implementation:\n1. Computes the decision function for each test sample by calculating the kernel values between the test sample and all training samples\n2. Applies the SVM decision rule by taking the sign of the decision function\n3. Returns the predictions as a list of integers (1 or -1)\n\nThe solution is fully vectorized within each test sample iteration, avoiding explicit loops over training samples."}
{"task_id": 28, "completion_id": 0, "completion": "I'll implement a function that computes the optimal rank-k reconstruction of a matrix using truncated SVD, which is equivalent to the best linear autoencoder with k latent units.\n\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float] | int:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Convert input to numpy array for computation\n    X_array = np.array(X, dtype=float)\n    \n    # Get dimensions\n    m, n = X_array.shape\n    \n    # Check if k is valid\n    if k < 1 or k > min(m, n):\n        return -1\n    \n    # Compute SVD\n    U, S, Vt = np.linalg.svd(X_array, full_matrices=False)\n    \n    # Truncate to rank k\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    \n    # Compute reconstruction\n    X_hat_array = U_k @ np.diag(S_k) @ Vt_k\n    \n    # Compute mean squared error\n    mse = np.mean((X_array - X_hat_array) ** 2)\n    \n    # Round to 4 decimals\n    X_hat_array = np.round(X_hat_array, 4)\n    mse = round(mse, 4)\n    \n    # Convert back to list of lists\n    X_hat = X_hat_array.tolist()\n    \n    return X_hat, mse\n```\n\nThis implementation:\n1. Converts the input list of lists to a NumPy array\n2. Checks if k is valid (between 1 and min(m,n))\n3. Computes the SVD of the input matrix\n4. Truncates the SVD to keep only the first k singular values/vectors\n5. Reconstructs the matrix using the truncated SVD\n6. Computes the mean squared error between the original and reconstructed matrices\n7. Rounds the reconstructed matrix and MSE to 4 decimal places\n8. Returns the reconstructed matrix as a list of lists and the MSE\n\nThe truncated SVD gives us the optimal rank-k approximation of the original matrix, which is equivalent to the best linear autoencoder with k latent dimensions."}
{"task_id": 29, "completion_id": 0, "completion": "I'll implement the gradient boosting regressor with decision stumps for one-dimensional data as described. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    # Sort the training samples by feature value\n    sorted_indices = np.argsort(x_train)\n    x_sorted = x_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n    \n    # Determine candidate split points (midpoints between consecutive feature values)\n    split_points = (x_sorted[:-1] + x_sorted[1:]) / 2\n    \n    # Initialize the ensemble prediction and residuals\n    ensemble_prediction = np.zeros_like(y_sorted)\n    residuals = y_sorted.copy()  # Initial residuals are just the targets\n    \n    # List to store the stumps (split_point, c1, c2)\n    stumps = []\n    \n    # Calculate initial RSS\n    current_rss = np.sum(residuals**2)\n    \n    # Main boosting loop\n    while current_rss > epsilon and len(split_points) > 0:\n        best_split = None\n        best_c1 = None\n        best_c2 = None\n        best_rss = float('inf')\n        \n        # Try each candidate split point\n        for split in split_points:\n            # Determine which samples are on each side of the split\n            left_mask = x_sorted <= split\n            right_mask = ~left_mask\n            \n            # Calculate the mean of residuals on each side\n            c1 = np.mean(residuals[left_mask]) if np.any(left_mask) else 0\n            c2 = np.mean(residuals[right_mask]) if np.any(right_mask) else 0\n            \n            # Calculate new predictions with this stump\n            new_predictions = ensemble_prediction.copy()\n            new_predictions[left_mask] += c1\n            new_predictions[right_mask] += c2\n            \n            # Calculate new RSS\n            new_residuals = y_sorted - new_predictions\n            new_rss = np.sum(new_residuals**2)\n            \n            # Update best stump if this one is better\n            if new_rss < best_rss:\n                best_rss = new_rss\n                best_split = split\n                best_c1 = c1\n                best_c2 = c2\n        \n        # Add the best stump to the ensemble\n        stumps.append((best_split, best_c1, best_c2))\n        \n        # Update ensemble predictions and residuals\n        left_mask = x_sorted <= best_split\n        right_mask = ~left_mask\n        ensemble_prediction[left_mask] += best_c1\n        ensemble_prediction[right_mask] += best_c2\n        residuals = y_sorted - ensemble_prediction\n        \n        # Update current RSS\n        current_rss = np.sum(residuals**2)\n    \n    # Make prediction for the query point\n    prediction = 0.0\n    for split, c1, c2 in stumps:\n        if x_query <= split:\n            prediction += c1\n        else:\n            prediction += c2\n    \n    # Round to 4 decimal places\n    return round(prediction, 4)\n```\n\nThis implementation follows the algorithm described in the task:\n\n1. It sorts the training data by feature value\n2. It computes all candidate split points\n3. It iteratively adds the best stump (the one that minimizes RSS) to the ensemble\n4. It updates residuals after each stump is added\n5. It continues until the RSS is below the tolerance threshold\n6. For prediction, it sums the contributions from all stumps based on which side of each split the query point falls\n\nThe final prediction is rounded to 4 decimal places as required."}
{"task_id": 34, "completion_id": 0, "completion": "I'll implement the random binary tensor generator according to the specifications. Here's the solution:\n\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray | int:\n    \"\"\"\n    Generate a random binary tensor with specified shape and sparsity.\n    \n    Parameters:\n    -----------\n    shape : tuple of ints\n        The shape of the output tensor.\n    sparsity : float, default=0.5\n        The probability of generating a 1.0 value in the tensor.\n        Must be in the range [0, 1].\n    seed : int or None, default=None\n        Random seed for reproducibility. If provided, np.random.seed(seed) is called.\n    \n    Returns:\n    --------\n    np.ndarray or int\n        A NumPy array of the specified shape containing only 0.0 and 1.0 values,\n        where each element has a probability of 'sparsity' to be 1.0.\n        Returns -1 if sparsity is outside the range [0, 1].\n    \"\"\"\n    # Check if sparsity is in the valid range\n    if sparsity < 0 or sparsity > 1:\n        return -1\n    \n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate random values and apply threshold\n    random_values = np.random.rand(*shape)\n    \n    # Values >= (1 - sparsity) become 1.0, others become 0.0\n    # This ensures that the probability of getting 1.0 is equal to sparsity\n    binary_tensor = np.where(random_values >= (1 - sparsity), 1.0, 0.0)\n    \n    return binary_tensor\n```\n\nThis implementation:\n1. Validates that sparsity is in the range [0, 1], returning -1 if it's not\n2. Sets the random seed if provided\n3. Generates random values using `np.random.rand`\n4. Applies a threshold to convert the random values to binary (0.0 or 1.0)\n5. Returns the resulting binary tensor as a NumPy array with floating-point values\n\nThe threshold is set at (1 - sparsity) so that values greater than or equal to this threshold become 1.0, ensuring that the probability of generating a 1.0 is equal to the specified sparsity."}
{"task_id": 39, "completion_id": 0, "completion": "I'll implement the function to perform a median split for KD-Tree construction using `numpy.argpartition` to achieve O(n) time complexity.\n\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    # ===== write your code below =====\n    n_samples = data.shape[0]\n    median_idx = n_samples // 2\n    \n    # Get the indices that would sort the column d\n    # We only need to partition around the median, not fully sort\n    indices = np.arange(n_samples)\n    pivot_idx = np.argpartition(data[:, d], median_idx)[median_idx]\n    pivot_value = data[pivot_idx, d]\n    \n    # Find indices of elements strictly less than the pivot value\n    left_mask = data[:, d] < pivot_value\n    left_indices = indices[left_mask].tolist()\n    left_indices.sort()  # Sort for deterministic output\n    \n    # Find indices of elements strictly greater than the pivot value\n    right_mask = data[:, d] > pivot_value\n    right_indices = indices[right_mask].tolist()\n    right_indices.sort()  # Sort for deterministic output\n    \n    return pivot_idx, left_indices, right_indices\n```\n\nThis implementation:\n1. Uses `np.argpartition` to find the index of the median value in O(n) time\n2. Creates masks for values less than and greater than the median\n3. Extracts and sorts the indices for the left and right partitions\n4. Returns the required tuple with the pivot index and sorted lists of left and right indices"}
{"task_id": 40, "completion_id": 0, "completion": "I'll implement the `to_frames` function that breaks a 1D signal into overlapping frames using NumPy's stride manipulation. Here's the complete solution:\n\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check preconditions\n    assert x.ndim == 1, \"Input signal must be one-dimensional\"\n    assert stride >= 1, \"Stride must be at least 1\"\n    assert len(x) >= frame_width, \"Signal length must be at least frame_width\"\n    \n    # Calculate number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n    \n    # Get the strides of the original array\n    # The first value is the stride in bytes to move one position in memory\n    # For a 1D array, this is the size of the data type\n    item_size = x.itemsize\n    \n    # Create a new view with the desired shape and strides\n    # For the framed view:\n    # - First dimension (frames): stride * item_size bytes to move to the next frame\n    # - Second dimension (samples within a frame): item_size bytes to move to the next sample\n    framed = as_strided(\n        x,\n        shape=(n_frames, frame_width),\n        strides=(stride * item_size, item_size),\n        writeable=writeable\n    )\n    \n    return framed\n```\n\nThis implementation:\n1. Checks all the required preconditions using assertions\n2. Calculates the number of frames based on the formula provided\n3. Uses `as_strided` to create a view on the original array with the appropriate shape and strides\n4. Handles the `writeable` parameter as specified\n5. Returns a view (not a copy) of the original data with the desired shape\n\nThe function correctly handles the stride manipulation to create overlapping frames without making copies of the data."}
{"task_id": 48, "completion_id": 0, "completion": "I'll create a function to validate whether a matrix is row-stochastic according to the given requirements.\n\n```python\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Convert to numpy array if it's a list of lists\n    if isinstance(X, list):\n        try:\n            X = np.array(X, dtype=float)\n        except:\n            return False\n    \n    # Check if X is a 2D array\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        return False\n    \n    # Check if all elements are between 0 and 1 inclusive\n    if not np.all((X >= 0) & (X <= 1)):\n        return False\n    \n    # Check if each row sums to 1 (within tolerance)\n    row_sums = np.sum(X, axis=1)\n    return np.allclose(row_sums, 1.0, rtol=0, atol=1e-8)\n```\n\nThis function:\n1. Converts the input to a NumPy array if it's a list of lists\n2. Checks if the input is a 2D array\n3. Verifies all elements are valid probabilities (between 0 and 1)\n4. Confirms each row sums to 1 within the specified tolerance\n\nThe function handles both list of lists and NumPy array inputs, and returns a boolean result without raising exceptions."}
{"task_id": 55, "completion_id": 0, "completion": "# RMSprop Optimizer for Linear Regression\n\nI'll implement the RMSprop optimization algorithm for ordinary least-squares linear regression as requested. The implementation will handle mini-batch processing and early stopping based on the gradient norm.\n\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n, d = X.shape\n    \n    # Initialize weights and running average\n    w = np.zeros(d)\n    s = np.zeros(d)\n    \n    # Adjust batch size if n < batch_size\n    actual_batch_size = min(n, batch_size)\n    \n    # Number of batches\n    num_batches = int(np.ceil(n / actual_batch_size))\n    \n    for iter_count in range(max_iter):\n        # Shuffle indices for each epoch\n        indices = np.random.permutation(n)\n        \n        # Process mini-batches\n        for batch_idx in range(num_batches):\n            # Get batch indices (with wrap-around if needed)\n            start_idx = (batch_idx * actual_batch_size) % n\n            end_idx = min(start_idx + actual_batch_size, n)\n            batch_indices = indices[start_idx:end_idx]\n            \n            # If we need to wrap around\n            if end_idx - start_idx < actual_batch_size and batch_idx < num_batches - 1:\n                remaining = actual_batch_size - (end_idx - start_idx)\n                batch_indices = np.concatenate([batch_indices, indices[:remaining]])\n            \n            # Get batch data\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            batch_n = len(batch_indices)\n            \n            # Compute prediction and error\n            pred = X_batch @ w\n            error = pred - y_batch\n            \n            # Compute gradient: g = (1/n) * X^T * (X*w - y)\n            g = (1/batch_n) * X_batch.T @ error\n            \n            # Update running average of squared gradients\n            s = rho * s + (1 - rho) * g**2\n            \n            # Update weights\n            w = w - eta * g / (np.sqrt(s) + eps_station)\n        \n        # Check for early stopping after each epoch\n        # Compute full gradient for convergence check\n        full_pred = X @ w\n        full_error = full_pred - y\n        full_gradient = (1/n) * X.T @ full_error\n        \n        # Check if gradient norm is below threshold\n        if np.linalg.norm(full_gradient) < epsilon:\n            break\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(val), 4) for val in w]\n```\n\nThis implementation:\n1. Initializes weights to zeros and sets up the running average for squared gradients\n2. Processes the data in mini-batches, handling the case where n < batch_size\n3. Implements the RMSprop update rule as specified\n4. Checks for early stopping based on the gradient norm\n5. Returns the weight vector rounded to 4 decimal places as a Python list\n\nThe algorithm shuffles the data for each epoch and handles wrap-around for mini-batches when needed."}
{"task_id": 56, "completion_id": 0, "completion": "I'll implement the SoftPlus activation function with gradient support, ensuring numerical stability for large inputs.\n\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Compute the SoftPlus activation function or its derivative.\n    \n    SoftPlus(x) = log(1 + exp(x))\n    Derivative of SoftPlus(x) = sigmoid(x) = 1/(1 + exp(-x))\n    \n    Args:\n        x: Input value(s) - can be a scalar, list, or numpy array\n        deriv: If True, compute the derivative (sigmoid) instead of SoftPlus\n        \n    Returns:\n        The computed values rounded to 4 decimal places as a list or scalar float\n    \"\"\"\n    # Convert input to numpy array for consistent handling\n    is_scalar = np.isscalar(x)\n    x_array = np.atleast_1d(np.array(x, dtype=float))\n    \n    if deriv:\n        # Compute sigmoid (derivative of softplus)\n        # Use a numerically stable implementation to avoid overflow\n        result = np.zeros_like(x_array)\n        \n        # For x >= 0, use 1 / (1 + exp(-x))\n        pos_mask = x_array >= 0\n        result[pos_mask] = 1 / (1 + np.exp(-x_array[pos_mask]))\n        \n        # For x < 0, use exp(x) / (1 + exp(x)) to avoid large negative exponents\n        neg_mask = ~pos_mask\n        result[neg_mask] = np.exp(x_array[neg_mask]) / (1 + np.exp(x_array[neg_mask]))\n    else:\n        # Compute softplus: log(1 + exp(x))\n        # Use a numerically stable implementation to avoid overflow\n        result = np.zeros_like(x_array)\n        \n        # For large positive x, softplus(x) \u2248 x\n        large_x_mask = x_array > 50  # threshold where exp(x) would overflow\n        result[large_x_mask] = x_array[large_x_mask]\n        \n        # For moderate x, use log1p(exp(x))\n        moderate_x_mask = (x_array <= 50) & (x_array > -50)\n        result[moderate_x_mask] = np.log1p(np.exp(x_array[moderate_x_mask]))\n        \n        # For large negative x, softplus(x) \u2248 exp(x)\n        small_x_mask = x_array <= -50\n        result[small_x_mask] = np.exp(x_array[small_x_mask])\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return scalar or list based on input type\n    if is_scalar:\n        return float(result[0])\n    else:\n        return result.tolist()\n```\n\nThis implementation:\n\n1. Handles scalar, list, and NumPy array inputs\n2. Implements both the SoftPlus function and its derivative (sigmoid)\n3. Uses numerically stable formulations to avoid overflow for large positive or negative inputs\n4. Rounds all results to 4 decimal places\n5. Returns a Python list for array inputs or a float for scalar inputs\n\nThe numerical stability is achieved by:\n- For SoftPlus: Using different approximations for different ranges of x\n- For the derivative: Using different formulations of the sigmoid function depending on whether x is positive or negative"}
{"task_id": 58, "completion_id": 0, "completion": "I'll implement the unsmoothed Maximum-Likelihood N-gram log-probability function as requested. Here's the solution:\n\n```python\nfrom collections import Counter\nimport numpy as np\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n    \"\"\"\n    # Handle the case where sequence is shorter than N\n    if len(sequence) < N:\n        return float('-inf')\n    \n    # Build N-gram counts from corpus\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    \n    # For unigrams, we just need the total corpus size for the denominator\n    corpus_size = len(corpus)\n    \n    # Build counts for N-grams and (N-1)-grams (prefixes)\n    for i in range(len(corpus) - N + 1):\n        ngram = tuple(corpus[i:i+N])\n        ngram_counts[ngram] += 1\n        \n        if N > 1:\n            prefix = tuple(corpus[i:i+N-1])\n            prefix_counts[prefix] += 1\n    \n    # Calculate log probability for each N-gram in the sequence\n    total_log_prob = 0.0\n    \n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i:i+N])\n        \n        # If N=1, use unigram probability\n        if N == 1:\n            numerator = ngram_counts[ngram]\n            denominator = corpus_size\n        else:\n            # For N>1, use conditional probability\n            prefix = tuple(sequence[i:i+N-1])\n            numerator = ngram_counts[ngram]\n            denominator = prefix_counts[prefix]\n        \n        # If either count is zero, return negative infinity\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n        \n        # Add log probability of this N-gram\n        log_prob = np.log(numerator / denominator)\n        total_log_prob += log_prob\n    \n    # Round to 4 decimal places\n    return round(total_log_prob, 4)\n```\n\nThis implementation:\n\n1. Builds count tables for N-grams and (N-1)-grams from the corpus\n2. Handles the special case for unigrams (N=1)\n3. Computes the log probability for each N-gram in the sequence\n4. Returns negative infinity if any required count is zero\n5. Rounds the final result to 4 decimal places as required\n\nThe function correctly implements the unsmoothed Maximum-Likelihood Estimation for N-gram language models as specified in the task description."}
{"task_id": 62, "completion_id": 0, "completion": "I'll implement a univariate Lasso regression learner with polynomial feature expansion using coordinate descent as requested.\n\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n    \n    # Number of samples\n    m = len(X)\n    \n    # Build the design matrix with polynomial features\n    # First column is all ones (for bias), followed by X^1, X^2, ..., X^degree\n    X_poly = np.ones((m, degree + 1))\n    for j in range(1, degree + 1):\n        X_poly[:, j] = X ** j\n    \n    # Initialize weights\n    w = np.zeros(degree + 1)\n    \n    # Precompute X_j^T * X_j for each feature\n    x_squared = np.sum(X_poly ** 2, axis=0)\n    \n    # Coordinate descent\n    for iteration in range(n_iterations):\n        w_old = w.copy()\n        \n        # Update bias (w_0) - not regularized\n        residuals = y - np.dot(X_poly[:, 1:], w[1:])\n        w[0] = np.mean(residuals)\n        \n        # Update other coefficients\n        for j in range(1, degree + 1):\n            # Compute the partial residual\n            partial_residual = y - np.dot(X_poly, w) + w[j] * X_poly[:, j]\n            \n            # Compute rho\n            rho = np.dot(X_poly[:, j], partial_residual)\n            \n            # Soft thresholding\n            if rho > reg_factor:\n                w[j] = (rho - reg_factor) / x_squared[j]\n            elif rho < -reg_factor:\n                w[j] = (rho + reg_factor) / x_squared[j]\n            else:\n                w[j] = 0\n        \n        # Check for convergence\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n    \n    # Round coefficients to 4 decimal places and return as a list\n    return [round(coef, 4) for coef in w]\n```\n\nThis implementation:\n1. Converts inputs to numpy arrays\n2. Builds a design matrix with polynomial features up to the specified degree\n3. Implements coordinate descent to optimize the Lasso objective function\n4. Updates the bias (w\u2080) exactly in each iteration without regularization\n5. Updates other coefficients using the soft thresholding operator\n6. Checks for convergence based on the maximum absolute change in weights\n7. Returns the coefficients rounded to 4 decimal places as a Python list\n\nThe algorithm will converge to the ordinary least-squares solution when reg_factor is 0, as the soft thresholding operation becomes a simple assignment in that case."}
{"task_id": 63, "completion_id": 0, "completion": "I'll implement the backward algorithm for Hidden Markov Models to calculate the backward probability vector \u03b2_t for a given time index t.\n\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Calculate the backward probability vector \u03b2_t for a Hidden Markov Model.\n    \n    Parameters:\n    -----------\n    A : list[list[float]]\n        Transition probability matrix of shape (N, N), where A[i][j] is the \n        probability of transitioning from state i to state j.\n    B : list[list[float]]\n        Emission probability matrix of shape (N, M), where B[i][j] is the \n        probability of observing symbol j in state i.\n    obs : list[int]\n        List of observation indices (length T).\n    t : int\n        Time index for which to calculate the backward probabilities (0 \u2264 t < T).\n    \n    Returns:\n    --------\n    list[float]\n        The backward probability vector \u03b2_t containing N probabilities rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    B = np.array(B)\n    \n    # Get dimensions\n    N = A.shape[0]  # Number of states\n    T = len(obs)    # Length of observation sequence\n    \n    # Initialize beta matrix\n    beta = np.zeros((T, N))\n    \n    # Set the base case: \u03b2_{T-1}(i) = 1 for all i\n    beta[T-1, :] = 1.0\n    \n    # Compute \u03b2_t for t = T-2 down to 0\n    for step in range(T-2, -1, -1):\n        for i in range(N):\n            # \u03b2_t(i) = \u03a3_j a_ij * b_j(o_{t+1}) * \u03b2_{t+1}(j)\n            beta[step, i] = sum(A[i, j] * B[j, obs[step+1]] * beta[step+1, j] for j in range(N))\n    \n    # Return the backward probabilities at time t, rounded to 4 decimal places\n    return [round(prob, 4) for prob in beta[t, :]]\n```\n\nThis implementation follows the backward algorithm for HMMs:\n\n1. It initializes the backward probabilities at the final time step T-1 to 1.0 for all states.\n2. It then recursively computes the backward probabilities for earlier time steps using the formula:\n   \u03b2_t(i) = \u03a3_j a_ij * b_j(o_{t+1}) * \u03b2_{t+1}(j)\n3. Finally, it returns the backward probabilities at the requested time index t, rounded to 4 decimal places.\n\nThe algorithm has a time complexity of O(T*N\u00b2) where T is the length of the observation sequence and N is the number of states."}
{"task_id": 65, "completion_id": 0, "completion": "I'll implement the backward algorithm for Hidden Markov Models as requested. This algorithm calculates the probability of an observation sequence given the model parameters.\n\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    B = np.array(B)\n    pi = np.array(pi)\n    \n    T = len(obs)  # Length of observation sequence\n    N = len(pi)   # Number of states\n    \n    # Initialize beta matrix\n    beta = np.zeros((T, N))\n    \n    # Initialize the last time step\n    beta[T-1, :] = 1.0\n    \n    # Recursive calculation of beta values\n    for t in range(T-2, -1, -1):\n        for i in range(N):\n            beta[t, i] = 0\n            for j in range(N):\n                beta[t, i] += A[i][j] * B[j][obs[t+1]] * beta[t+1, j]\n    \n    # Calculate the final probability\n    probability = 0.0\n    for i in range(N):\n        probability += pi[i] * B[i][obs[0]] * beta[0, i]\n    \n    # Round to 6 decimal places\n    return round(probability, 6)\n```\n\nThis implementation follows the backward algorithm as described in the task:\n\n1. It first checks for empty inputs and returns 0.0 if any are found.\n2. It initializes the beta values for the last time step to 1.0.\n3. It recursively calculates the beta values for each time step and state.\n4. Finally, it computes the overall probability using the initial state distribution, emission probabilities for the first observation, and the beta values at time 0.\n\nThe result is rounded to 6 decimal places as required."}
{"task_id": 69, "completion_id": 0, "completion": "I'll implement the forward algorithm for Hidden Markov Models. This algorithm calculates the probability of an observation sequence given the model parameters.\n\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    if not observations:\n        return -1\n    \n    # Convert inputs to numpy arrays for easier manipulation\n    S = np.array(S)\n    A = np.array(A)\n    B = np.array(B)\n    \n    # Get dimensions\n    n = len(S)  # Number of states\n    \n    # Check that S is a valid probability vector\n    if len(S.shape) != 1 or S.shape[0] != n:\n        return -1\n    \n    # Check that A is a valid transition matrix\n    if len(A.shape) != 2 or A.shape[0] != n or A.shape[1] != n:\n        return -1\n    \n    # Check that B is a valid emission matrix\n    if len(B.shape) != 2 or B.shape[0] != n:\n        return -1\n    \n    m = B.shape[1]  # Number of possible observations\n    \n    # Check that all observations are in valid range\n    if any(obs < 0 or obs >= m for obs in observations):\n        return -1\n    \n    # Initialize forward probabilities\n    alpha = np.zeros((len(observations), n))\n    \n    # Base case: initial probabilities * emission probabilities for first observation\n    alpha[0] = S * B[:, observations[0]]\n    \n    # Recursive case: compute forward probabilities for each time step\n    for t in range(1, len(observations)):\n        for j in range(n):\n            # Sum over all possible previous states\n            alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, observations[t]]\n    \n    # Final probability is the sum of the last column of alpha\n    probability = np.sum(alpha[-1])\n    \n    return round(probability, 4)\n```\n\nThis implementation:\n1. Validates all input dimensions and checks for empty observation sequences\n2. Ensures all observations are within the valid range\n3. Returns -1 for any invalid inputs\n4. Implements the forward algorithm using dynamic programming\n5. Returns the final probability rounded to 4 decimal places\n\nThe algorithm works by calculating the probability of being in each state at each time step, considering all possible paths that could lead to that state and the probability of observing the given symbol from that state."}
{"task_id": 70, "completion_id": 0, "completion": "# FP-Growth Algorithm Implementation\n\nI'll implement the FP-Growth algorithm for finding frequent itemsets in transaction data. This algorithm is more efficient than Apriori because it only requires two database scans.\n\n```python\nfrom collections import Counter, defaultdict\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    if not transactions:\n        return []\n    \n    # Step 1: Count item frequencies and filter infrequent items\n    item_counter = Counter()\n    for transaction in transactions:\n        item_counter.update(transaction)\n    \n    # Filter items that meet minimum support\n    frequent_items = {item: count for item, count in item_counter.items() \n                     if count >= min_support}\n    \n    if not frequent_items:\n        return []\n    \n    # Sort frequent items by frequency (descending) for FP-tree construction\n    # This helps in building a more compact tree\n    ordered_items = sorted(frequent_items.keys(), \n                          key=lambda x: (-frequent_items[x], x))\n    \n    # Create a mapping for quick lookup\n    item_order = {item: i for i, item in enumerate(ordered_items)}\n    \n    # Step 2: Build the FP-tree\n    root = FPNode(None, None, None)\n    \n    # Insert each transaction into the tree\n    for transaction in transactions:\n        # Filter and sort items in the transaction\n        filtered_items = [item for item in transaction if item in frequent_items]\n        filtered_items.sort(key=lambda x: item_order[x])\n        \n        if filtered_items:\n            insert_tree(filtered_items, root, 1)\n    \n    # Step 3: Mine the FP-tree recursively\n    frequent_itemsets = []\n    mine_tree(root, {}, frequent_itemsets, min_support)\n    \n    # Sort the result as required\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return frequent_itemsets\n\nclass FPNode:\n    \"\"\"Node in an FP-tree.\"\"\"\n    \n    def __init__(self, item, count, parent):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.next = None  # Link to the next node with the same item\n\ndef insert_tree(items, node, count):\n    \"\"\"Insert a transaction into the FP-tree.\"\"\"\n    if not items:\n        return\n    \n    item = items[0]\n    \n    # If the item is already a child of the current node, increment its count\n    if item in node.children:\n        node.children[item].count += count\n    else:\n        # Create a new node\n        new_node = FPNode(item, count, node)\n        node.children[item] = new_node\n    \n    # Recursively insert the rest of the items\n    insert_tree(items[1:], node.children[item], count)\n\ndef mine_tree(tree, prefix, frequent_itemsets, min_support):\n    \"\"\"Mine the FP-tree recursively to find all frequent itemsets.\"\"\"\n    # Get all paths from root to leaf\n    paths = get_paths(tree)\n    \n    # For each item in the header table\n    for item, nodes in paths.items():\n        # Create a new itemset by adding the current item to the prefix\n        new_itemset = sorted(list(prefix) + [item])\n        \n        # Calculate the support of the new itemset\n        support = sum(node.count for node in nodes)\n        \n        if support >= min_support:\n            # Add the new itemset to the result\n            frequent_itemsets.append(new_itemset)\n            \n            # Build the conditional pattern base\n            conditional_pattern_base = []\n            \n            for node in nodes:\n                # Get the path from the node to the root (excluding the root)\n                path = []\n                current = node.parent\n                \n                while current.item is not None:\n                    path.append(current.item)\n                    current = current.parent\n                \n                if path:\n                    # Add the path to the conditional pattern base with the node's count\n                    conditional_pattern_base.append((path, node.count))\n            \n            if conditional_pattern_base:\n                # Build the conditional FP-tree\n                conditional_tree = build_conditional_tree(conditional_pattern_base, min_support)\n                \n                if conditional_tree.children:\n                    # Mine the conditional FP-tree recursively\n                    new_prefix = dict(prefix)\n                    new_prefix[item] = support\n                    mine_tree(conditional_tree, new_prefix, frequent_itemsets, min_support)\n\ndef get_paths(tree):\n    \"\"\"Get all paths from the root to the leaves.\"\"\"\n    paths = defaultdict(list)\n    \n    def traverse(node):\n        for item, child in node.children.items():\n            paths[item].append(child)\n            traverse(child)\n    \n    traverse(tree)\n    return paths\n\ndef build_conditional_tree(pattern_base, min_support):\n    \"\"\"Build a conditional FP-tree from a conditional pattern base.\"\"\"\n    # Count the frequency of each item in the conditional pattern base\n    item_counter = Counter()\n    \n    for path, count in pattern_base:\n        for item in path:\n            item_counter[item] += count\n    \n    # Filter items that meet minimum support\n    frequent_items = {item: count for item, count in item_counter.items() \n                     if count >= min_support}\n    \n    # Create a new tree\n    root = FPNode(None, None, None)\n    \n    # Insert each path into the tree\n    for path, count in pattern_base:\n        # Filter and sort items in the path\n        filtered_path = [item for item in path if item in frequent_items]\n        \n        if filtered_path:\n            # Sort by frequency (descending)\n            filtered_path.sort(key=lambda x: (-frequent_items[x], x))\n            insert_tree(filtered_path, root, count)\n    \n    return root\n```\n\nThis implementation follows the FP-Growth algorithm as described by Han et al. (2000). It:\n\n1. Counts the support of each item and filters out infrequent ones\n2. Builds an FP-tree with the frequent items\n3. Recursively mines the tree to find all frequent itemsets\n\nThe result is sorted as required - first by itemset length and then lexicographically within each length group. Each itemset has its items sorted alphabetically."}
{"task_id": 75, "completion_id": 0, "completion": "# k-Nearest Neighbour Classifier Implementation\n\nHere's a complete implementation of the k-NN classifier according to the requirements:\n\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return [-1]\n    \n    # Convert inputs to numpy arrays for efficient computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    predictions = []\n    \n    for test_sample in X_test_np:\n        # Calculate squared Euclidean distances (omitting square root as mentioned)\n        # Using broadcasting to compute distances efficiently\n        squared_distances = np.sum((X_train_np - test_sample) ** 2, axis=1)\n        \n        # Get indices of k nearest neighbors\n        nearest_indices = np.argsort(squared_distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        nearest_labels = y_train_np[nearest_indices]\n        \n        # Count occurrences of each label\n        unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n        \n        # Find the label(s) with the maximum count\n        max_count = np.max(counts)\n        max_count_indices = np.where(counts == max_count)[0]\n        \n        # If there's a tie, choose the smallest label\n        if len(max_count_indices) > 1:\n            prediction = min(unique_labels[max_count_indices])\n        else:\n            prediction = unique_labels[max_count_indices[0]]\n        \n        predictions.append(int(prediction))\n    \n    return predictions\n```\n\nThis implementation:\n1. Validates that k is a positive integer not exceeding the number of training samples\n2. Computes squared Euclidean distances between each test sample and all training samples\n3. Finds the k nearest neighbors for each test sample\n4. Determines the majority class among these neighbors\n5. Handles ties by selecting the smallest class label\n6. Returns predictions as a Python list of integers"}
{"task_id": 76, "completion_id": 0, "completion": "I'll implement the categorical cross-entropy loss function as requested.\n\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are lists\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Add epsilon for numerical stability\n    epsilon = np.finfo(float).eps\n    \n    # Clip predictions to avoid log(0)\n    y_pred_safe = np.clip(y_pred, epsilon, 1.0)\n    \n    # Calculate cross-entropy loss\n    # Only the positions where y=1 contribute to the sum (due to one-hot encoding)\n    loss = -np.sum(y * np.log(y_pred_safe))\n    \n    # Round to 4 decimal places\n    return round(float(loss), 4)\n```\n\nThis implementation:\n1. Converts the inputs to NumPy arrays if they're lists\n2. Adds a small epsilon value to prevent log(0)\n3. Calculates the cross-entropy loss using the formula provided\n4. Returns the loss rounded to 4 decimal places as a float\n\nThe function handles both list and NumPy array inputs and correctly implements the categorical cross-entropy loss formula."}
{"task_id": 77, "completion_id": 0, "completion": "I'll implement the forward propagation for an L-layer neural network as requested. Here's the solution:\n\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    # Helper functions for activations\n    def relu(Z):\n        return np.maximum(0, Z)\n    \n    def sigmoid(Z):\n        return 1 / (1 + np.exp(-Z))\n    \n    # Initialize caches list and set A as the input\n    caches = []\n    A = X\n    \n    # Determine the number of layers\n    L = len(parameters) // 2\n    \n    # Forward propagation for layers 1 to L-1 (ReLU)\n    for l in range(1, L):\n        A_prev = A\n        \n        # Linear forward\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        \n        # ReLU activation for hidden layers\n        A = relu(Z)\n        \n        # Store cache for this layer\n        cache = (A_prev, W, b, Z)\n        caches.append(cache)\n    \n    # Forward propagation for the output layer L (Sigmoid)\n    W = parameters[f'W{L}']\n    b = parameters[f'b{L}']\n    Z = np.dot(W, A) + b\n    AL = sigmoid(Z)\n    \n    # Store cache for the output layer\n    cache = (A, W, b, Z)\n    caches.append(cache)\n    \n    # Round to 4 decimal places and convert to list\n    AL_rounded = np.round(AL, 4).tolist()\n    \n    return AL_rounded\n```\n\nThis implementation:\n1. Iterates through all layers, applying the linear step (Z = W\u00b7A_prev + b) followed by the appropriate activation function (ReLU for hidden layers, sigmoid for the output layer)\n2. Collects a cache for each layer containing the inputs, weights, biases, and pre-activation values\n3. Returns the final activation AL rounded to 4 decimal places and converted to a Python list\n\nThe function handles networks of any depth by determining the number of layers from the parameters dictionary."}
{"task_id": 81, "completion_id": 0, "completion": "# Binary Cross-Entropy Cost Computation\n\nHere's a self-contained Python script that implements the binary cross-entropy cost function as specified:\n\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Get the number of samples\n    m = A2.shape[0] if A2.ndim == 1 else A2.shape[1]\n    \n    # Clip values to avoid log(0) or log(1)\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Calculate the binary cross-entropy cost\n    # J = -(1/m) * sum(Y * log(A2) + (1-Y) * log(1-A2))\n    cost = -(1/m) * np.sum(Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped))\n    \n    # Round to 6 decimal places\n    return round(float(cost), 6)\n```\n\nThis implementation:\n1. Determines the number of samples `m` from the shape of the input arrays\n2. Clips the predicted probabilities to avoid numerical issues with log(0) or log(1)\n3. Computes the binary cross-entropy cost using the formula provided\n4. Rounds the result to 6 decimal places as required\n\nThe function handles both 1D arrays and 2D arrays with shape (1, m)."}
{"task_id": 82, "completion_id": 0, "completion": "I'll implement the \u03b5-soft exploration policy function as requested. This policy ensures that every action has at least a small probability of being selected, while still favoring the greedy action.\n\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    # Convert Q to numpy array if it's a list\n    Q_array = np.array(Q)\n    \n    # Number of actions\n    n = len(Q_array)\n    \n    # Find the index of the greedy action (first occurrence in case of ties)\n    greedy_index = np.argmax(Q_array)\n    \n    # Calculate base probability for non-greedy actions\n    base_prob = epsilon / n\n    \n    # Initialize probabilities with the base probability\n    probabilities = np.full(n, base_prob)\n    \n    # Set the probability for the greedy action\n    probabilities[greedy_index] = 1 - epsilon + base_prob\n    \n    # Round to 4 decimal places\n    probabilities = np.round(probabilities, 4)\n    \n    # Ensure probabilities sum to 1 (handle potential rounding issues)\n    if abs(np.sum(probabilities) - 1.0) > 1e-12:\n        # Adjust the greedy action probability to make sum exactly 1\n        probabilities[greedy_index] = round(1.0 - sum(probabilities) + probabilities[greedy_index], 4)\n    \n    return probabilities.tolist()\n```\n\nThis implementation:\n1. Converts the input Q-values to a NumPy array\n2. Finds the index of the first maximum value (greedy action)\n3. Calculates the base probability for all actions (\u03b5/n)\n4. Sets the probability for the greedy action to (1-\u03b5+\u03b5/n)\n5. Rounds all probabilities to 4 decimal places\n6. Ensures the sum of probabilities equals 1 (within the specified tolerance)\n7. Returns the probabilities as a list\n\nThe function handles both list and NumPy array inputs and ensures the output meets all the specified constraints."}
{"task_id": 86, "completion_id": 0, "completion": "# Random Forest Majority Vote Aggregator\n\nI'll implement a function that aggregates individual tree predictions using majority voting, with a tie-breaking mechanism that selects the smallest label.\n\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Check if predictions is empty\n    if not predictions or not predictions[0]:\n        return []\n    \n    # Get the number of samples (length of each inner list)\n    n_samples = len(predictions[0])\n    \n    # Initialize the result list\n    result = []\n    \n    # For each sample\n    for sample_idx in range(n_samples):\n        # Collect votes from all trees for this sample\n        votes = [tree[sample_idx] for tree in predictions]\n        \n        # Count the votes\n        vote_counter = Counter(votes)\n        \n        # Find the maximum vote count\n        max_count = max(vote_counter.values())\n        \n        # Get all labels with the maximum vote count\n        max_labels = [label for label, count in vote_counter.items() if count == max_count]\n        \n        # Break ties by choosing the smallest label\n        final_prediction = min(max_labels)\n        \n        # Add to result\n        result.append(final_prediction)\n    \n    return result\n```\n\nThis implementation:\n\n1. Iterates through each sample position\n2. Collects all tree predictions for that sample\n3. Counts the votes using Counter\n4. Identifies the maximum vote count\n5. Finds all labels that have the maximum vote count\n6. Selects the smallest label among those with the maximum votes\n7. Returns the final aggregated predictions\n\nThe function handles both numeric and string labels, and properly breaks ties by selecting the smallest label according to the requirements."}
{"task_id": 88, "completion_id": 0, "completion": "I'll implement the softplus activation function using the numerically stable approach with `np.logaddexp`. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input's shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Convert input to numpy array if it's a list\n    if isinstance(z, list):\n        z_array = np.array(z)\n    else:\n        z_array = z\n    \n    # Apply the softplus function using logaddexp for numerical stability\n    result = np.logaddexp(0.0, z_array)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return appropriate type based on input\n    if np.isscalar(z):\n        return float(result)\n    else:\n        return result.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test with the example from the problem\n    test_input = np.array([-1000, 0, 3])\n    print(softplus(test_input))  # Should output [0.0, 0.6931, 3.0486]\n    \n    # Test with a scalar\n    print(softplus(0))  # Should output 0.6931\n    \n    # Test with a Python list\n    print(softplus([-1000, 0, 3]))  # Should output [0.0, 0.6931, 3.0486]\n```\n\nThis implementation:\n1. Handles scalar, list, and numpy array inputs\n2. Uses `np.logaddexp(0.0, z)` for numerical stability\n3. Rounds results to 4 decimal places\n4. Returns a Python list for array-like inputs using `tolist()`\n5. Returns a single float for scalar inputs"}
{"task_id": 90, "completion_id": 0, "completion": "I'll solve this task by implementing the `mse` function to calculate the mean-squared error between the true expected payouts and the policy's estimates.\n\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Get the true expected values from the bandit\n    arm_evs = getattr(bandit, 'arm_evs', bandit.get('arm_evs', None))\n    \n    # Get the policy's estimates\n    ev_estimates = None\n    if hasattr(policy, 'ev_estimates'):\n        ev_estimates = policy.ev_estimates\n    elif isinstance(policy, dict) and 'ev_estimates' in policy:\n        ev_estimates = policy['ev_estimates']\n    \n    # Return NaN if policy doesn't have estimates or they're empty\n    if ev_estimates is None or len(ev_estimates) == 0:\n        return np.nan\n    \n    # Sort the arm indices to ensure correct matching\n    sorted_indices = sorted(ev_estimates.keys())\n    \n    # Calculate squared errors for each arm\n    squared_errors = []\n    for i in sorted_indices:\n        true_value = arm_evs[i]\n        estimated_value = ev_estimates[i]\n        squared_error = (estimated_value - true_value) ** 2\n        squared_errors.append(squared_error)\n    \n    # Calculate the mean of squared errors\n    mean_squared_error = sum(squared_errors) / len(squared_errors)\n    \n    # Round to 4 decimal places\n    return round(mean_squared_error, 4)\n```\n\nThis implementation:\n1. Extracts the true expected values and policy estimates, handling both object attributes and dictionary keys\n2. Returns `np.nan` if the policy doesn't have estimates or they're empty\n3. Sorts the arm indices to ensure correct matching between true values and estimates\n4. Calculates the squared error for each arm\n5. Computes the mean of these squared errors\n6. Rounds the result to 4 decimal places as required"}
{"task_id": 96, "completion_id": 0, "completion": "I'll implement the additive-smoothed N-gram log-probability function as requested. Here's the complete solution:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Split the sequence into tokens\n    tokens = sequence.split()\n    \n    # If sequence length is smaller than N, return 0.0\n    if len(tokens) < N:\n        return 0.0\n    \n    # Build vocabulary and n-gram frequency tables from corpus\n    vocab = set()\n    ngram_counts = defaultdict(Counter)\n    \n    # Process corpus to build vocabulary and count n-grams\n    for sentence in corpus:\n        sentence_tokens = sentence.split()\n        \n        # Add all tokens to vocabulary\n        for token in sentence_tokens:\n            vocab.add(token)\n        \n        # Count n-grams of all orders from 1 to N\n        for n in range(1, N + 1):\n            for i in range(len(sentence_tokens) - n + 1):\n                if n == 1:\n                    # Unigram case\n                    ngram = (sentence_tokens[i],)\n                    ngram_counts[n][ngram] += 1\n                else:\n                    # Higher order n-grams\n                    context = tuple(sentence_tokens[i:i+n-1])\n                    word = sentence_tokens[i+n-1]\n                    ngram_counts[n][(context, word)] += 1\n    \n    # Add <UNK> to vocabulary\n    vocab.add(\"<UNK>\")\n    vocab_size = len(vocab)\n    \n    # Replace OOV words in the target sequence with <UNK>\n    processed_tokens = [\"<UNK>\" if token not in vocab else token for token in tokens]\n    \n    # Calculate log probability\n    log_prob = 0.0\n    \n    # Process each n-gram in the target sequence\n    for i in range(len(processed_tokens) - N + 1):\n        context = tuple(processed_tokens[i:i+N-1])\n        word = processed_tokens[i+N-1]\n        \n        # Get counts\n        context_count = sum(ngram_counts[N-1][(context, w)] for w in vocab if (context, w) in ngram_counts[N])\n        ngram_count = ngram_counts[N].get((context, word), 0)\n        \n        # Apply additive smoothing\n        smoothed_prob = (ngram_count + K) / (context_count + K * vocab_size)\n        \n        # Add log probability\n        log_prob += math.log(smoothed_prob)\n    \n    # Round to 4 decimal places\n    return round(log_prob, 4)\n```\n\nWait, I need to fix some issues in my implementation. Let me correct it:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Split the sequence into tokens\n    tokens = sequence.split()\n    \n    # If sequence length is smaller than N, return 0.0\n    if len(tokens) < N:\n        return 0.0\n    \n    # Build vocabulary from corpus\n    vocab = set()\n    for sentence in corpus:\n        for token in sentence.split():\n            vocab.add(token)\n    \n    # Add <UNK> to vocabulary\n    vocab.add(\"<UNK>\")\n    vocab_size = len(vocab)\n    \n    # Replace OOV words in the target sequence with <UNK>\n    processed_tokens = [\"<UNK>\" if token not in vocab else token for token in tokens]\n    \n    # Build n-gram counts for all orders from 1 to N\n    ngram_counts = {}\n    context_counts = {}\n    \n    for n in range(1, N+1):\n        ngram_counts[n] = Counter()\n        context_counts[n-1] = Counter() if n > 1 else None\n        \n        for sentence in corpus:\n            sentence_tokens = sentence.split()\n            for i in range(len(sentence_tokens) - n + 1):\n                ngram = tuple(sentence_tokens[i:i+n])\n                ngram_counts[n][ngram] += 1\n                \n                if n > 1:\n                    context = tuple(sentence_tokens[i:i+n-1])\n                    context_counts[n-1][context] += 1\n    \n    # Calculate log probability\n    log_prob = 0.0\n    \n    # Process each n-gram in the target sequence\n    for i in range(len(processed_tokens) - N + 1):\n        ngram = tuple(processed_tokens[i:i+N])\n        context = tuple(processed_tokens[i:i+N-1])\n        \n        # Get counts\n        ngram_count = ngram_counts[N].get(ngram, 0)\n        context_count = context_counts[N-1].get(context, 0)\n        \n        # Apply additive smoothing\n        smoothed_prob = (ngram_count + K) / (context_count + K * vocab_size)\n        \n        # Add log probability\n        log_prob += math.log(smoothed_prob)\n    \n    # Round to 4 decimal places\n    return round(log_prob, 4)\n```\n\nI still need to refine the implementation. Let me provide the correct solution:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Split the sequence into tokens\n    tokens = sequence.split()\n    \n    # If sequence length is smaller than N, return 0.0\n    if len(tokens) < N:\n        return 0.0\n    \n    # Build vocabulary from corpus\n    vocab = set()\n    for sentence in corpus:\n        for token in sentence.split():\n            vocab.add(token)\n    \n    # Add <UNK> to vocabulary\n    vocab.add(\"<UNK>\")\n    vocab_size = len(vocab)\n    \n    # Replace OOV words in the target sequence with <UNK>\n    processed_tokens = [\"<UNK>\" if token not in vocab else token for token in tokens]\n    \n    # Build n-gram counts for all orders from 1 to N\n    ngram_counts = {}\n    context_counts = {}\n    \n    for n in range(1, N"}
{"task_id": 108, "completion_id": 0, "completion": "I'll implement the regularized Alternating Least Squares (ALS) algorithm for matrix factorization. This algorithm alternates between optimizing W and H while keeping the other fixed, which leads to a series of regularized least squares problems.\n\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    # Get dimensions\n    N, M = X.shape\n    \n    # Initialize W and H with fixed random seed\n    np.random.seed(0)\n    W = np.random.rand(N, K)\n    H = np.random.rand(K, M)\n    \n    # Identity matrix for regularization\n    I_K = np.eye(K)\n    \n    # Initial loss\n    X_hat = W @ H\n    current_loss = np.linalg.norm(X - X_hat, 'fro')**2 + alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n    \n    # ALS iterations\n    for _ in range(max_iter):\n        # Update H (keeping W fixed)\n        for j in range(M):\n            # Solve regularized least squares for each column of H\n            H[:, j] = np.linalg.solve(W.T @ W + alpha * I_K, W.T @ X[:, j])\n        \n        # Update W (keeping H fixed)\n        for i in range(N):\n            # Solve regularized least squares for each row of W\n            W[i, :] = np.linalg.solve(H @ H.T + alpha * I_K, H @ X[i, :].T)\n        \n        # Compute new loss\n        X_hat = W @ H\n        new_loss = np.linalg.norm(X - X_hat, 'fro')**2 + alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n        \n        # Check convergence\n        if new_loss < tol or abs(current_loss - new_loss) < tol:\n            break\n        \n        current_loss = new_loss\n    \n    # Return reconstructed matrix rounded to 4 decimal places as list of lists\n    return (W @ H).round(4).tolist()\n```\n\nThis implementation:\n1. Initializes W and H with random values using a fixed seed (0)\n2. Alternates between updating H and W by solving regularized least squares problems\n3. Checks for convergence based on the loss value or change in loss\n4. Returns the reconstructed matrix rounded to 4 decimal places as a list of lists\n\nThe algorithm uses the closed-form solution for regularized least squares to update each column of H and each row of W efficiently."}
{"task_id": 109, "completion_id": 0, "completion": "I'll implement the K-Means++ clustering algorithm according to your specifications. Here's the complete solution:\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    # Set random seeds if provided\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    \n    m, n = X.shape\n    \n    # K-means++ initialization\n    centroids = kmeans_plus_plus_init(X, K)\n    \n    # Lloyd's algorithm\n    for _ in range(max_iter):\n        # Assign step: assign each point to the nearest centroid\n        labels = assign_clusters(X, centroids)\n        \n        # Update step: update centroids based on assigned points\n        new_centroids = update_centroids(X, labels, K, centroids)\n        \n        # Check for convergence\n        if np.array_equal(centroids, new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round to 4 decimal places and convert to list format\n    centroids_list = [list(np.round(centroid, 4)) for centroid in centroids]\n    \n    # Sort by first coordinate, then by the complete tuple as secondary key\n    centroids_list.sort(key=lambda x: (x[0], tuple(x)))\n    \n    return centroids_list\n\ndef kmeans_plus_plus_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Initialize centroids using K-means++ algorithm.\"\"\"\n    m, n = X.shape\n    centroids = np.zeros((K, n))\n    \n    # Choose the first centroid randomly\n    first_centroid_idx = random.randint(0, m - 1)\n    centroids[0] = X[first_centroid_idx]\n    \n    # Choose the remaining K-1 centroids\n    for k in range(1, K):\n        # Calculate squared distances to the nearest existing centroid for each point\n        min_distances = np.min([np.sum((X - centroid) ** 2, axis=1) for centroid in centroids[:k]], axis=0)\n        \n        # Choose the next centroid with probability proportional to squared distance\n        probabilities = min_distances / np.sum(min_distances)\n        next_centroid_idx = np.random.choice(m, p=probabilities)\n        centroids[k] = X[next_centroid_idx]\n    \n    return centroids\n\ndef assign_clusters(X: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n    \"\"\"Assign each data point to the nearest centroid.\"\"\"\n    distances = np.array([np.sum((X - centroid) ** 2, axis=1) for centroid in centroids])\n    return np.argmin(distances, axis=0)\n\ndef update_centroids(X: np.ndarray, labels: np.ndarray, K: int, old_centroids: np.ndarray) -> np.ndarray:\n    \"\"\"Update centroids based on assigned points.\"\"\"\n    n_features = X.shape[1]\n    new_centroids = np.zeros((K, n_features))\n    \n    for k in range(K):\n        # Get points assigned to cluster k\n        cluster_points = X[labels == k]\n        \n        # If cluster is not empty, update centroid\n        if len(cluster_points) > 0:\n            new_centroids[k] = np.mean(cluster_points, axis=0)\n        else:\n            # If cluster is empty, keep the old centroid\n            new_centroids[k] = old_centroids[k]\n    \n    return new_centroids\n```\n\nThis implementation follows the K-Means++ algorithm for initialization, which selects initial centroids that are far apart from each other. The Lloyd's algorithm then iteratively assigns points to the nearest centroid and updates the centroids until convergence or until the maximum number of iterations is reached.\n\nThe final centroids are rounded to 4 decimal places and sorted by their first coordinate, with the complete centroid tuple used as a secondary key to break ties."}
{"task_id": 111, "completion_id": 0, "completion": "I'll implement these three helper functions for character-level sequence encoding and decoding. Here's the complete solution:\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encodes a string into a fixed-length list of integer token ids.\n    \n    Args:\n        text: The input string to encode\n        time_steps: The fixed length of the output sequence\n        vocabulary: A dictionary mapping characters to integer ids\n        \n    Returns:\n        A list of integer ids with length equal to time_steps\n    \"\"\"\n    # Initialize the result with padding tokens\n    result = [0] * time_steps\n    \n    # Fill in the result with character encodings\n    for i, char in enumerate(text[:time_steps]):  # Truncate if longer than time_steps\n        # Use the padding token (0) for characters not in the vocabulary\n        result[i] = vocabulary.get(char, 0)\n    \n    return result\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Converts a list/array of integer ids back to text.\n    \n    Args:\n        indices: A list or array of integer ids\n        inverse_vocab: A dictionary mapping integer ids to characters\n        \n    Returns:\n        The decoded string with padding tokens removed\n    \"\"\"\n    # Convert each non-padding token to its character representation\n    chars = [inverse_vocab.get(idx, '') for idx in indices if idx != 0]\n    \n    # Join the characters into a string\n    return ''.join(chars)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    Encodes text, runs it through a model, and decodes the prediction.\n    \n    Args:\n        model: A model with a predict method that takes a batch of encoded sequences\n        input_vocabulary: Dictionary mapping input characters to integer ids\n        inv_output_vocabulary: Dictionary mapping integer ids to output characters\n        text: The input text to process\n        \n    Returns:\n        The decoded prediction string\n    \"\"\"\n    # Encode the input text\n    encoded_text = string_to_int(text, TIME_STEPS, input_vocabulary)\n    \n    # Reshape to match the model's expected input shape (batch_size=1, time_steps)\n    model_input = np.array([encoded_text])\n    \n    # Get model predictions\n    predictions = model.predict(model_input)\n    \n    # Apply argmax to get the most likely token at each time step\n    predicted_indices = np.argmax(predictions, axis=-1)[0]  # Take first batch item\n    \n    # Decode the predicted indices back to a string\n    result = int_to_string(predicted_indices, inv_output_vocabulary)\n    \n    return result\n```\n\nThis implementation handles all the requirements:\n\n1. `string_to_int` encodes characters to integers using the provided vocabulary, truncates or pads to the fixed length `time_steps`, and uses padding token 0 for unknown characters.\n\n2. `int_to_string` converts integer IDs back to characters using the inverse vocabulary and ignores padding tokens (0).\n\n3. `run_example` ties everything together by encoding the input text, running it through the model, applying argmax to get the predicted token IDs, and decoding them back to a string."}
{"task_id": 113, "completion_id": 0, "completion": "I'll implement the `run_examples` function according to the specifications. This function will call the `run_example` helper for each input string, collect the results, and return them in the same order.\n\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # If examples is None, use the global EXAMPLES constant\n    if examples is None:\n        examples = EXAMPLES\n    \n    # List to collect all predictions\n    predictions = []\n    \n    # Process each example\n    for example in examples:\n        # Call run_example to get the list of characters\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        \n        # Join the characters into a single string\n        output_string = ''.join(output_chars)\n        \n        # Print the input and output\n        print(f\"input:  {example}\")\n        print(f\"output: {output_string}\")\n        \n        # Add the prediction to our collection\n        predictions.append(output_string)\n    \n    # Return all predictions\n    return predictions\n```\n\nThis implementation:\n1. Uses the global `EXAMPLES` if no examples are provided\n2. Processes each example by calling `run_example`\n3. Joins the returned character list into a string\n4. Prints the input and output in the required format\n5. Collects all predictions in a list and returns it\n\nThe function follows the API requirements and doesn't use the vocabulary dictionaries, which are only passed through to the `run_example` function for compatibility."}
{"task_id": 115, "completion_id": 0, "completion": "I'll implement the logistic loss and gradient function according to the specifications. Here's the solution:\n\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # Ensure proper shapes\n    m = X.shape[0]  # Number of samples\n    \n    # Reshape y to (m,) if it's (m, 1)\n    if len(y.shape) > 1 and y.shape[1] == 1:\n        y = y.flatten()\n    \n    # Reshape w to (n,) if it's (n, 1)\n    if len(w.shape) > 1 and w.shape[1] == 1:\n        w = w.flatten()\n    \n    # Calculate the linear combination z = X \u00b7 w\n    z = X.dot(w)\n    \n    # Calculate the predicted probabilities using the sigmoid function\n    p = 1 / (1 + np.exp(-z))\n    \n    # Avoid numerical issues by clipping probabilities\n    epsilon = 1e-20\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Calculate the binary cross-entropy loss\n    loss = -1/m * np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n    \n    # Calculate the gradient of the loss with respect to w\n    gradient = 1/m * X.T.dot(p - y)\n    \n    # Reshape gradient to (n, 1) for consistent output format\n    gradient = gradient.reshape(-1, 1)\n    \n    # Round the loss and gradient to 4 decimal places\n    loss_rounded = round(float(loss), 4)\n    gradient_rounded = np.round(gradient, 4).tolist()\n    \n    return loss_rounded, gradient_rounded\n```\n\nThis implementation:\n1. Handles different input shapes for X, y, and w\n2. Computes the logistic function (sigmoid) to get probabilities\n3. Clips probabilities to avoid numerical issues with logarithms\n4. Calculates the binary cross-entropy loss\n5. Computes the gradient of the loss with respect to weights\n6. Rounds the loss to 4 decimal places and returns it as a float\n7. Rounds the gradient to 4 decimal places and returns it as a nested list\n\nThe function returns a tuple with the loss and gradient as specified in the requirements."}
{"task_id": 118, "completion_id": 0, "completion": "I'll implement the AdaBoost algorithm with decision stumps from scratch. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n    \n    # Get dimensions\n    m, n_features = X_train.shape\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(m) / m\n    \n    # Lists to store the weak classifiers and their weights\n    alphas = []\n    stumps = []\n    \n    # Train n_clf weak classifiers\n    for _ in range(n_clf):\n        # Find the best stump for the current weights\n        best_stump, min_error = find_best_stump(X_train, y_train, weights)\n        \n        # Calculate stump weight (alpha)\n        # Avoid division by zero or log(0)\n        epsilon = max(min(min_error, 0.9999), 0.0001)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        \n        # Store the stump and its weight\n        stumps.append(best_stump)\n        alphas.append(alpha)\n        \n        # Update weights for the next iteration\n        # Predict with the current stump\n        predictions = np.array([stump_predict(x, best_stump) for x in X_train])\n        \n        # Calculate the new weights\n        # w_i = w_i * exp(-alpha * y_i * h(x_i))\n        weights *= np.exp(-alpha * y_train * predictions)\n        \n        # Normalize weights\n        weights /= np.sum(weights)\n    \n    # Make predictions on the test set\n    test_predictions = []\n    for x in X_test:\n        # Weighted sum of weak classifier predictions\n        weighted_sum = sum(alpha * stump_predict(x, stump) for alpha, stump in zip(alphas, stumps))\n        # Final prediction is the sign of the weighted sum\n        prediction = 1 if weighted_sum >= 0 else -1\n        test_predictions.append(prediction)\n    \n    return test_predictions\n\ndef find_best_stump(X, y, weights):\n    \"\"\"Find the best decision stump for the weighted data.\"\"\"\n    m, n_features = X.shape\n    best_error = float('inf')\n    best_stump = {}\n    \n    for feature_idx in range(n_features):\n        # Get unique values for the feature\n        feature_values = np.unique(X[:, feature_idx])\n        \n        # Try thresholds between each pair of unique values\n        thresholds = [(feature_values[i] + feature_values[i+1]) / 2 for i in range(len(feature_values)-1)]\n        if not thresholds:  # If there's only one unique value\n            thresholds = [feature_values[0]]\n        \n        for threshold in thresholds:\n            # Try both directions: > and <=\n            for polarity in [1, -1]:\n                # Make predictions with this stump\n                predictions = np.ones(m)\n                if polarity == 1:\n                    predictions[X[:, feature_idx] <= threshold] = -1\n                else:\n                    predictions[X[:, feature_idx] > threshold] = -1\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                error = np.sum(weights[misclassified])\n                \n                # Update best stump if this one is better\n                if error < best_error:\n                    best_error = error\n                    best_stump = {\n                        'feature_idx': feature_idx,\n                        'threshold': threshold,\n                        'polarity': polarity\n                    }\n    \n    return best_stump, best_error\n\ndef stump_predict(x, stump):\n    \"\"\"Make a prediction with a single decision stump.\"\"\"\n    feature_idx = stump['feature_idx']\n    threshold = stump['threshold']\n    polarity = stump['polarity']\n    \n    if polarity == 1:\n        return 1 if x[feature_idx] > threshold else -1\n    else:\n        return 1 if x[feature_idx] <= threshold else -1\n```\n\nThis implementation follows the AdaBoost algorithm with decision stumps as weak learners:\n\n1. It initializes sample weights uniformly.\n2. For each iteration:\n   - Finds the best decision stump that minimizes the weighted error\n   - Calculates the stump's weight (alpha) based on its error\n   - Updates the sample weights, giving more weight to misclassified examples\n3. For prediction, it computes a weighted sum of all weak classifier predictions and returns the sign.\n\nThe decision stumps are simple one-level decision trees that make decisions based on a single feature, threshold, and polarity (direction of comparison)."}
{"task_id": 128, "completion_id": 0, "completion": "I'll implement the multi-class Linear Discriminant Analysis (LDA) transformation as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    # If n_components is 0, return an empty list\n    if n_components == 0:\n        return []\n    \n    # Get dimensions\n    n_samples, n_features = X.shape\n    \n    # Ensure n_components doesn't exceed the number of features\n    n_components = min(n_components, n_features)\n    \n    # Get unique classes and their counts\n    classes = np.unique(y)\n    n_classes = len(classes)\n    \n    # Compute global mean\n    global_mean = np.mean(X, axis=0)\n    \n    # Initialize scatter matrices\n    S_W = np.zeros((n_features, n_features))\n    S_B = np.zeros((n_features, n_features))\n    \n    # Compute within-class and between-class scatter matrices\n    for c in classes:\n        # Get samples for this class\n        X_c = X[y == c]\n        n_c = X_c.shape[0]\n        \n        # Compute class mean\n        mean_c = np.mean(X_c, axis=0)\n        \n        # Update within-class scatter matrix\n        X_c_centered = X_c - mean_c\n        S_W += X_c_centered.T @ X_c_centered\n        \n        # Update between-class scatter matrix\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B += n_c * (mean_diff @ mean_diff.T)\n    \n    # Compute S_W^(-1) * S_B using pseudo-inverse for numerical stability\n    S_W_pinv = np.linalg.pinv(S_W)\n    A = S_W_pinv @ S_B\n    \n    # Perform eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Select the first n_components eigenvectors\n    W = eigenvectors[:, :n_components]\n    \n    # Apply deterministic sign: flip sign if first non-zero element is negative\n    for i in range(W.shape[1]):\n        col = W[:, i]\n        non_zero_idx = np.nonzero(col)[0]\n        if len(non_zero_idx) > 0 and col[non_zero_idx[0]] < 0:\n            W[:, i] = -col\n    \n    # Project the data onto the LDA directions\n    X_transformed = X @ W\n    \n    # Round to 4 decimal places\n    X_transformed = np.round(X_transformed, 4)\n    \n    # Convert to list of lists\n    return X_transformed.tolist()\n```\n\nThis implementation follows all the specified steps:\n1. Computes the within-class scatter matrix S_W\n2. Computes the between-class scatter matrix S_B\n3. Forms the matrix A = pinv(S_W) \u00b7 S_B using the Moore-Penrose pseudo-inverse\n4. Performs eigen-decomposition of A and sorts the eigen-pairs\n5. Applies the deterministic sign fix to make results consistent\n6. Projects the data onto the first n_components eigenvectors\n7. Rounds to 4 decimal places and returns as a list of lists\n\nThe implementation handles edge cases like n_components = 0 and ensures n_components doesn't exceed the number of features."}
{"task_id": 140, "completion_id": 0, "completion": "I'll solve this problem by using a breadth-first search (BFS) approach with a twist: we need to simulate both the agent's movement and the fire's spread simultaneously. Let me implement this solution:\n\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    \n    # Edge cases\n    if grid[0][0] == 'F' or grid[n-1][n-1] == 'F':\n        return -1\n    \n    if n == 1 and grid[0][0] == '.':\n        return 0\n    \n    # Convert grid to a 2D array for easier manipulation\n    maze = [list(row) for row in grid]\n    \n    # Initialize fire spread times\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    fire_queue = deque()\n    \n    # Find all initial fire sources\n    for i in range(n):\n        for j in range(n):\n            if maze[i][j] == 'F':\n                fire_time[i][j] = 0\n                fire_queue.append((i, j, 0))  # (row, col, time)\n    \n    # Directions: up, right, down, left\n    directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n    \n    # Simulate fire spread using BFS\n    while fire_queue:\n        r, c, time = fire_queue.popleft()\n        \n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            \n            if 0 <= nr < n and 0 <= nc < n and maze[nr][nc] == '.' and fire_time[nr][nc] == math.inf:\n                fire_time[nr][nc] = time + 1\n                fire_queue.append((nr, nc, time + 1))\n    \n    # BFS for agent's path\n    agent_queue = deque([(0, 0, 0)])  # (row, col, time)\n    visited = set([(0, 0)])\n    \n    while agent_queue:\n        r, c, time = agent_queue.popleft()\n        \n        # Check if reached the goal\n        if r == n - 1 and c == n - 1:\n            return time\n        \n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            \n            if (0 <= nr < n and 0 <= nc < n and \n                maze[nr][nc] == '.' and \n                (nr, nc) not in visited and \n                time + 1 < fire_time[nr][nc]):  # Agent can move before fire reaches\n                \n                visited.add((nr, nc))\n                agent_queue.append((nr, nc, time + 1))\n    \n    # If we can't reach the goal\n    return -1\n```\n\nThis solution works by:\n\n1. First checking edge cases (start or goal already on fire, or single cell maze)\n2. Simulating the fire spread using BFS to calculate when each cell will catch fire\n3. Using another BFS to find the shortest path for the agent, ensuring the agent only moves to cells that won't be on fire when the agent arrives\n4. Returning the minimum time needed or -1 if it's impossible\n\nThe time complexity is O(n\u00b2) where n is the size of the grid, as we potentially visit each cell in both BFS traversals."}
{"task_id": 141, "completion_id": 0, "completion": "# k-Nearest Neighbors Classifier Implementation\n\nI'll implement the k-NN classifier as requested, with support for the three distance metrics and proper handling of ties.\n\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    Implements k-Nearest Neighbors classifier.\n    \n    Parameters:\n    -----------\n    X_train : np.ndarray\n        Training data features of shape (n_samples, n_features)\n    y_train : np.ndarray\n        Training data labels of shape (n_samples,)\n    X_test : np.ndarray\n        Test data features of shape (m_samples, n_features)\n    k : int\n        Number of neighbors to consider\n    metric : str, default='euclidean'\n        Distance metric to use: 'euclidean', 'manhattan', or 'cosine'\n        \n    Returns:\n    --------\n    np.ndarray\n        Predicted labels for X_test of shape (m_samples,)\n    \"\"\"\n    n_test = X_test.shape[0]\n    n_train = X_train.shape[0]\n    predictions = np.zeros(n_test, dtype=y_train.dtype)\n    \n    # Calculate distances based on the specified metric\n    if metric == 'manhattan':\n        # Manhattan distance (L1)\n        distances = np.zeros((n_test, n_train))\n        for i in range(n_test):\n            distances[i] = np.sum(np.abs(X_train - X_test[i]), axis=1)\n    elif metric == 'cosine':\n        # Cosine distance (1 - cosine similarity)\n        # Normalize vectors for cosine similarity\n        X_train_norm = X_train / np.maximum(np.linalg.norm(X_train, axis=1, keepdims=True), 1e-10)\n        X_test_norm = X_test / np.maximum(np.linalg.norm(X_test, axis=1, keepdims=True), 1e-10)\n        \n        # Calculate cosine similarity and convert to distance\n        distances = np.zeros((n_test, n_train))\n        for i in range(n_test):\n            cosine_sim = np.dot(X_train_norm, X_test_norm[i])\n            distances[i] = 1 - cosine_sim\n    else:  # default to euclidean\n        # Euclidean distance (L2)\n        distances = np.zeros((n_test, n_train))\n        for i in range(n_test):\n            distances[i] = np.sqrt(np.sum((X_train - X_test[i])**2, axis=1))\n    \n    # Find k nearest neighbors and predict by majority vote\n    for i in range(n_test):\n        # Get indices of k nearest neighbors\n        nearest_indices = np.argsort(distances[i])[:k]\n        nearest_labels = y_train[nearest_indices]\n        \n        # Count occurrences of each label\n        unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n        \n        # Find the label(s) with the maximum count\n        max_count = np.max(counts)\n        max_labels = unique_labels[counts == max_count]\n        \n        # In case of a tie, return the smallest label\n        predictions[i] = np.min(max_labels)\n    \n    return predictions\n```\n\nThis implementation:\n1. Calculates distances between each test sample and all training samples using the specified metric\n2. For each test sample, finds the k nearest neighbors\n3. Predicts the class by majority vote, with ties broken by selecting the smallest label value\n4. Falls back to Euclidean distance if an unknown metric is provided\n5. Returns a 1D NumPy array of predictions\n\nThe code handles all three distance metrics correctly and follows the specified requirements."}
{"task_id": 146, "completion_id": 0, "completion": "I'll implement the k-Nearest Neighbors classifier from scratch using NumPy. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    # Ensure k doesn't exceed the number of training samples\n    k = min(k, len(X))\n    \n    # Initialize predictions array\n    predictions = np.zeros(len(X_test), dtype=y.dtype)\n    \n    # Calculate distances based on the chosen metric\n    for i, test_sample in enumerate(X_test):\n        distances = np.zeros(len(X))\n        \n        if metric == 'euclidean':\n            # Euclidean distance: sqrt(sum((x_i - x_hat_i)^2))\n            distances = np.sqrt(np.sum((X - test_sample)**2, axis=1))\n            \n        elif metric == 'manhattan':\n            # Manhattan distance: sum(|x_i - x_hat_i|)\n            distances = np.sum(np.abs(X - test_sample), axis=1)\n            \n        elif metric == 'cosine':\n            # Cosine distance: 1 - (x\u00b7x_hat)/(||x||\u00b7||x_hat||)\n            epsilon = 1e-12  # To avoid division by zero\n            \n            # Calculate dot products\n            dot_products = np.sum(X * test_sample, axis=1)\n            \n            # Calculate norms\n            norm_X = np.sqrt(np.sum(X**2, axis=1))\n            norm_test = np.sqrt(np.sum(test_sample**2))\n            \n            # Calculate cosine similarity and convert to distance\n            cosine_sim = dot_products / (norm_X * norm_test + epsilon)\n            distances = 1 - cosine_sim\n        \n        # Get indices of k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get labels of k nearest neighbors\n        nearest_labels = y[nearest_indices]\n        \n        # Count occurrences of each label\n        unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n        \n        # Find the most common label(s)\n        max_count = np.max(counts)\n        most_common_labels = unique_labels[counts == max_count]\n        \n        # In case of a tie, return the smallest label\n        predictions[i] = np.min(most_common_labels)\n    \n    return predictions.tolist()\n```\n\nThis implementation:\n1. Calculates distances between each test sample and all training samples using the specified metric\n2. Finds the k nearest neighbors for each test sample\n3. Performs majority voting on the labels of these neighbors\n4. Handles ties by selecting the smallest label\n5. Returns the predictions as a Python list\n\nThe code handles all three distance metrics correctly and includes the epsilon value for the cosine distance to avoid division by zero."}
{"task_id": 155, "completion_id": 0, "completion": "I'll implement the forward pass of the actor network as described. The function will take a state vector and weights dictionary, then apply the linear transformations and activations to produce an action vector.\n\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to numpy array if it's not already\n    state_np = np.array(state)\n    \n    # First layer: Linear + ReLU\n    x1 = np.dot(state_np, weights['W1']) + weights['b1']\n    x1_relu = np.maximum(0, x1)  # ReLU activation\n    \n    # Second layer: Linear + ReLU\n    x2 = np.dot(x1_relu, weights['W2']) + weights['b2']\n    x2_relu = np.maximum(0, x2)  # ReLU activation\n    \n    # Output layer: Linear + tanh\n    x3 = np.dot(x2_relu, weights['W3']) + weights['b3']\n    action = np.tanh(x3)  # tanh activation to keep values in [-1, 1]\n    \n    # Round to 4 decimal places and convert to list\n    rounded_action = [round(float(a), 4) for a in action]\n    \n    return rounded_action\n```\n\nThis implementation follows the network topology exactly as specified:\n1. Takes a state vector and weights dictionary as input\n2. Applies three linear transformations with their respective weights and biases\n3. Uses ReLU activation after the first two linear layers\n4. Uses tanh activation after the final linear layer\n5. Returns the action vector as a Python list with values rounded to 4 decimal places"}
{"task_id": 160, "completion_id": 0, "completion": "I'll implement Lasso regression using coordinate descent as specified. Let me walk through the solution step by step.\n\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb \u00b7 L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    # Ensure X is a 2D array\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    # Ensure y is a 1D array\n    y = np.asarray(y)\n    if y.ndim > 1:\n        y = y.ravel()\n    \n    m, n = X.shape  # m samples, n features\n    \n    # Step 1: Add a column for the intercept\n    if fit_intercept:\n        X_with_intercept = np.column_stack((np.ones(m), X))\n    else:\n        X_with_intercept = np.column_stack((np.zeros(m), X))\n    \n    # Initialize weights to zeros\n    w = np.zeros(n + 1)\n    \n    # Soft thresholding operator\n    def soft_threshold(a, tau):\n        return np.sign(a) * np.maximum(np.abs(a) - tau, 0)\n    \n    # Coordinate descent\n    for _ in range(max_iters):\n        # Step 2: If fitting intercept, recompute it as mean residual\n        if fit_intercept:\n            # Compute predictions without intercept\n            preds_without_intercept = X_with_intercept[:, 1:] @ w[1:]\n            # Intercept is mean residual\n            w[0] = np.mean(y - preds_without_intercept)\n        \n        # Step 3: Coordinate loop over each weight (excluding intercept)\n        for j in range(1, n + 1):  # Skip the intercept (index 0)\n            # Temporarily set the current weight to 0\n            w_j_old = w[j]\n            w[j] = 0\n            \n            # Compute partial residual r_j = y - X\u00b7w_{\u00acj}\n            r_j = y - X_with_intercept @ w\n            \n            # Compute dot product <x_j, r_j>\n            x_j = X_with_intercept[:, j]\n            dot_product = np.dot(x_j, r_j)\n            \n            # Compute sum of squares of x_j\n            sum_squares = np.sum(x_j**2)\n            \n            if sum_squares > 0:  # Avoid division by zero\n                # Update weight using soft thresholding\n                w[j] = soft_threshold(dot_product, lambda_param * m) / sum_squares\n    \n    # Step 4: Return final intercept and weight vector\n    # Round to 4 decimal places\n    bias = round(float(w[0]), 4)\n    weights = [round(float(w_i), 4) for w_i in w[1:]]\n    \n    return (weights, bias)\n```\n\nThis implementation follows the specified steps:\n\n1. It adds an all-ones column to X if `fit_intercept=True`, otherwise an all-zeros column.\n2. It initializes all parameters to zero and recomputes the intercept in each outer loop if needed.\n3. It performs coordinate descent for `max_iters` iterations, updating each weight using the soft-thresholding operator.\n4. It returns the final intercept and weight vector, rounded to 4 decimal places.\n\nThe algorithm correctly implements the Lasso regression using coordinate descent optimization as specified in the task description."}
{"task_id": 165, "completion_id": 0, "completion": "I'll implement the normalized Hamming distance function according to the requirements:\n\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are lists\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Check if the lengths are equal\n    if len(x) != len(y):\n        return -1\n    \n    # Calculate the normalized Hamming distance\n    # Count positions where elements differ and divide by total length\n    different_positions = np.sum(x != y)\n    total_positions = len(x)\n    \n    # Calculate normalized distance and round to 4 decimal places\n    distance = different_positions / total_positions\n    return round(distance, 4)\n```\n\nThis implementation:\n1. Converts input lists to NumPy arrays if needed\n2. Checks if the vectors have the same length, returning -1 if they don't\n3. Calculates the normalized Hamming distance by counting positions where elements differ\n4. Rounds the result to 4 decimal places as required\n5. Returns 0 when all elements are identical and 1 when all elements differ"}
{"task_id": 169, "completion_id": 0, "completion": "I'll implement the `generate_window` function to generate coefficients for the specified window functions according to the requirements.\n\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Check if N is a positive integer\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer\")\n    \n    # Special case: N = 1\n    if N == 1:\n        return [1.0]\n    \n    # Generate the window based on the specified type\n    if window == \"hamming\":\n        n = np.arange(N)\n        w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"hann\":\n        n = np.arange(N)\n        w = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"blackman_harris\":\n        n = np.arange(N)\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (a0 - a1 * np.cos(2 * np.pi * n / (N - 1)) \n             + a2 * np.cos(4 * np.pi * n / (N - 1)) \n             - a3 * np.cos(6 * np.pi * n / (N - 1)))\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Coefficients must be provided for generalized_cosine window\")\n        \n        n = np.arange(N)\n        w = np.zeros(N)\n        \n        for k, a_k in enumerate(coefficients):\n            if k % 2 == 0:  # Even index (0, 2, 4, ...)\n                w += a_k * np.cos(2 * np.pi * (k // 2) * n / (N - 1))\n            else:  # Odd index (1, 3, 5, ...)\n                w -= a_k * np.cos(2 * np.pi * ((k + 1) // 2) * n / (N - 1))\n    else:\n        raise ValueError(f\"Unknown window type: {window}\")\n    \n    # Round to 4 decimal places and convert to a Python list\n    return [round(float(val), 4) for val in w]\n```\n\nWait, I need to correct the implementation for the generalized cosine window. Let me fix that:\n\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Check if N is a positive integer\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer\")\n    \n    # Special case: N = 1\n    if N == 1:\n        return [1.0]\n    \n    # Generate the window based on the specified type\n    if window == \"hamming\":\n        n = np.arange(N)\n        w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"hann\":\n        n = np.arange(N)\n        w = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"blackman_harris\":\n        n = np.arange(N)\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (a0 - a1 * np.cos(2 * np.pi * n / (N - 1)) \n             + a2 * np.cos(4 * np.pi * n / (N - 1)) \n             - a3 * np.cos(6 * np.pi * n / (N - 1)))\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Coefficients must be provided for generalized_cosine window\")\n        \n        n = np.arange(N)\n        w = np.zeros(N)\n        \n        for k, a_k in enumerate(coefficients):\n            w += a_k * np.cos(2 * np.pi * k * n / (N - 1))\n    else:\n        raise ValueError(f\"Unknown window type: {window}\")\n    \n    # Round to 4 decimal places and convert to a Python list\n    return [round(float(val), 4) for val in w]\n```"}
{"task_id": 171, "completion_id": 0, "completion": "# Binary Logistic Regression with Mini-Batch Gradient Descent\n\nI'll implement a binary logistic regression classifier from scratch using mini-batch gradient descent. The implementation will handle all the requirements specified in the task.\n\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test, dtype=float)\n    \n    # Handle the special case where all training labels are identical\n    unique_labels = np.unique(y_train)\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Map the original labels to 0 and 1 for internal use\n    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n    reverse_map = {0: unique_labels[0], 1: unique_labels[1]}\n    y_mapped = np.array([label_map[y] for y in y_train])\n    \n    # Add intercept term (bias) to the data\n    X_train_with_bias = np.column_stack((np.ones(X_train.shape[0]), X_train))\n    X_test_with_bias = np.column_stack((np.ones(X_test.shape[0]), X_test))\n    \n    # Initialize weights\n    n_features = X_train_with_bias.shape[1]\n    weights = np.zeros(n_features)\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to avoid overflow\n    \n    # Mini-batch gradient descent\n    n_samples = X_train_with_bias.shape[0]\n    n_batches = int(np.ceil(n_samples / batch_size))\n    \n    for epoch in range(epochs):\n        # Shuffle the data for each epoch\n        indices = np.random.permutation(n_samples)\n        X_shuffled = X_train_with_bias[indices]\n        y_shuffled = y_mapped[indices]\n        \n        for batch in range(n_batches):\n            start_idx = batch * batch_size\n            end_idx = min((batch + 1) * batch_size, n_samples)\n            \n            X_batch = X_shuffled[start_idx:end_idx]\n            y_batch = y_shuffled[start_idx:end_idx]\n            \n            # Forward pass\n            z = np.dot(X_batch, weights)\n            predictions = sigmoid(z)\n            \n            # Compute gradient of negative log-likelihood (cross-entropy)\n            gradient = np.dot(X_batch.T, (predictions - y_batch)) / len(X_batch)\n            \n            # Update weights\n            weights -= learning_rate * gradient\n    \n    # Make predictions on test set\n    z_test = np.dot(X_test_with_bias, weights)\n    probabilities = sigmoid(z_test)\n    \n    # Apply decision threshold of 0.5 and map back to original labels\n    binary_predictions = (probabilities >= 0.5).astype(int)\n    final_predictions = [reverse_map[pred] for pred in binary_predictions]\n    \n    return final_predictions\n```\n\nThis implementation:\n\n1. Converts input data to numpy arrays for efficient computation\n2. Handles the special case where all training labels are identical\n3. Maps the original labels to {0, 1} for internal use and maps predictions back to the original label set\n4. Adds an intercept term (bias) to both training and test data\n5. Implements mini-batch gradient descent to minimize the negative log-likelihood loss\n6. Uses the sigmoid function to compute probabilities\n7. Applies a decision threshold of 0.5 to obtain binary predictions\n8. Returns predictions in the same label set as the input\n\nThe implementation also includes a clip in the sigmoid function to prevent numerical overflow issues with large values."}
{"task_id": 176, "completion_id": 0, "completion": "# AdaBoost with Decision Stumps Implementation\n\nI'll implement AdaBoost with decision stumps from scratch according to the specifications. The implementation will follow the standard AdaBoost algorithm with decision stumps as weak learners.\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"\n    Implements AdaBoost algorithm with decision stumps as weak learners.\n    \n    Args:\n        X_train: List of training feature vectors\n        y_train: List of training labels (0 or 1)\n        X_test: List of test feature vectors\n        n_estimators: Number of weak learners to train\n        \n    Returns:\n        List of predicted labels (0 or 1) for X_test\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Convert y_train from {0,1} to {-1,1} for AdaBoost\n    y_train_adaboost = np.where(y_train == 1, 1, -1)\n    \n    n_samples = len(X_train)\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(n_samples) / n_samples\n    \n    # List to store the weak learners and their weights\n    weak_learners = []\n    alphas = []\n    \n    for t in range(n_estimators):\n        # Find the best decision stump\n        best_stump, error = find_best_stump(X_train, y_train_adaboost, weights)\n        \n        # If perfect stump is found, adjust its weight and break\n        if error == 0:\n            alpha = 10  # A large value to give high importance to this perfect classifier\n            weak_learners.append(best_stump)\n            alphas.append(alpha)\n            break\n        \n        # Calculate the weight of this classifier\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        \n        # Store the stump and its weight\n        weak_learners.append(best_stump)\n        alphas.append(alpha)\n        \n        # Update sample weights\n        predictions = predict_stump(X_train, best_stump)\n        weights *= np.exp(-alpha * y_train_adaboost * predictions)\n        \n        # Normalize weights\n        weights /= np.sum(weights)\n    \n    # Make predictions on test data\n    y_pred = predict_adaboost(X_test, weak_learners, alphas)\n    \n    # Convert back to {0,1} and return as a list\n    return [int(pred == 1) for pred in y_pred]\n\ndef find_best_stump(X, y, weights):\n    \"\"\"\n    Find the best decision stump for the weighted dataset.\n    \n    Args:\n        X: Feature matrix\n        y: Labels (-1 or 1)\n        weights: Sample weights\n        \n    Returns:\n        Tuple of (best_stump, error) where best_stump is a dictionary with\n        feature_index, threshold, and polarity\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_error = float('inf')\n    best_stump = {}\n    \n    for feature_idx in range(n_features):\n        # Get unique values for the feature\n        feature_values = np.unique(X[:, feature_idx])\n        \n        # Generate thresholds (midpoints between consecutive unique values)\n        if len(feature_values) == 1:\n            thresholds = [feature_values[0]]\n        else:\n            thresholds = [(feature_values[i] + feature_values[i+1]) / 2 for i in range(len(feature_values)-1)]\n            # Add the smallest value as a threshold too\n            thresholds = [feature_values[0] - 1] + thresholds\n        \n        for threshold in thresholds:\n            for polarity in [1, -1]:\n                # Make predictions with this stump\n                predictions = np.ones(n_samples)\n                if polarity == 1:\n                    predictions[X[:, feature_idx] < threshold] = -1\n                else:\n                    predictions[X[:, feature_idx] >= threshold] = -1\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                error = np.sum(weights[misclassified])\n                \n                # Update best stump if this one is better\n                if error < best_error:\n                    best_error = error\n                    best_stump = {\n                        'feature_index': feature_idx,\n                        'threshold': threshold,\n                        'polarity': polarity\n                    }\n    \n    return best_stump, best_error\n\ndef predict_stump(X, stump):\n    \"\"\"\n    Make predictions using a single decision stump.\n    \n    Args:\n        X: Feature matrix\n        stump: Dictionary with feature_index, threshold, and polarity\n        \n    Returns:\n        Array of predictions (-1 or 1)\n    \"\"\"\n    feature_idx = stump['feature_index']\n    threshold = stump['threshold']\n    polarity = stump['polarity']\n    \n    predictions = np.ones(len(X))\n    if polarity == 1:\n        predictions[X[:, feature_idx] < threshold] = -1\n    else:\n        predictions[X[:, feature_idx] >= threshold] = -1\n    \n    return predictions\n\ndef predict_adaboost(X, weak_learners, alphas):\n    \"\"\"\n    Make predictions using the AdaBoost ensemble.\n    \n    Args:\n        X: Feature matrix\n        weak_learners: List of decision stumps\n        alphas: List of classifier weights\n        \n    Returns:\n        Array of predictions (-1 or 1)\n    \"\"\"\n    n_samples = len(X)\n    scores = np.zeros(n_samples)\n    \n    for stump, alpha in zip(weak_learners, alphas):\n        predictions = predict_stump(X, stump)\n        scores += alpha * predictions\n    \n    # Return sign of the scores\n    return np.sign(scores)\n```\n\nThis implementation follows the AdaBoost algorithm as described in the task:\n\n1. It initializes sample weights uniformly\n2. For each boosting round:\n   - Finds the best decision stump (weak learner) based on weighted error\n   - Calculates the weight (alpha) for this stump\n   - Updates sample weights based on misclassifications\n   - Normalizes the weights\n3. For prediction, it combines the weak learners with their weights\n4. Returns predictions as a list of integers (0 or 1)\n\nThe implementation handles the tie-breaking as specified: smallest feature index, then smallest threshold, then polarity 1 before -1. It also stops early if a perfect stump is found."}
{"task_id": 178, "completion_id": 0, "completion": "I'll implement the Particle Swarm Optimization algorithm to minimize the Sphere function according to your specifications.\n\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for valid inputs\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n    \n    # PSO parameters\n    w = 0.5      # inertia weight\n    c1 = 1.5     # cognitive weight\n    c2 = 1.5     # social weight\n    \n    # Bounds of the search space\n    lower_bound = -1\n    upper_bound = 1\n    \n    # Initialize particles' positions and velocities\n    positions = rng.uniform(lower_bound, upper_bound, (num_particles, n_dims))\n    velocities = rng.uniform(-1, 1, (num_particles, n_dims))\n    \n    # Define the Sphere function\n    def sphere_function(x):\n        return np.sum(x**2, axis=1)\n    \n    # Evaluate initial positions\n    fitness = sphere_function(positions)\n    \n    # Initialize personal best positions and fitness\n    personal_best_positions = positions.copy()\n    personal_best_fitness = fitness.copy()\n    \n    # Initialize global best position and fitness\n    global_best_idx = np.argmin(personal_best_fitness)\n    global_best_position = personal_best_positions[global_best_idx].copy()\n    global_best_fitness = personal_best_fitness[global_best_idx]\n    \n    # Main PSO loop\n    for _ in range(num_iterations):\n        # Generate random coefficients\n        r1 = rng.random((num_particles, n_dims))\n        r2 = rng.random((num_particles, n_dims))\n        \n        # Update velocities\n        cognitive_component = c1 * r1 * (personal_best_positions - positions)\n        social_component = c2 * r2 * (global_best_position - positions)\n        velocities = w * velocities + cognitive_component + social_component\n        \n        # Update positions with clipping to bounds\n        positions = np.clip(positions + velocities, lower_bound, upper_bound)\n        \n        # Evaluate new positions\n        fitness = sphere_function(positions)\n        \n        # Update personal bests\n        improved_indices = fitness < personal_best_fitness\n        personal_best_positions[improved_indices] = positions[improved_indices]\n        personal_best_fitness[improved_indices] = fitness[improved_indices]\n        \n        # Update global best\n        if np.min(personal_best_fitness) < global_best_fitness:\n            global_best_idx = np.argmin(personal_best_fitness)\n            global_best_position = personal_best_positions[global_best_idx].copy()\n            global_best_fitness = personal_best_fitness[global_best_idx]\n    \n    # Return the best fitness value found, rounded to 4 decimals\n    return round(global_best_fitness, 4)\n```\n\nThis implementation follows the PSO algorithm as described in the task. It initializes particles with random positions and velocities, then iteratively updates them according to the specified update rules. The algorithm keeps track of personal best positions for each particle and the global best position across all particles. The Sphere function is minimized within the specified bounds, and the final result is rounded to four decimal places as required."}
{"task_id": 180, "completion_id": 0, "completion": "I'll implement the k-Nearest Neighbors algorithm as requested. Here's the complete solution:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    # Adjust k if it's 0 or larger than the number of training samples\n    n_samples = len(X_train)\n    if k == 0 or k > n_samples:\n        k = n_samples\n    \n    predictions = []\n    \n    for test_point in X_test_np:\n        # Calculate Euclidean distances\n        distances = np.sqrt(np.sum((X_train_np - test_point) ** 2, axis=1))\n        \n        # Get indices of k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get the corresponding labels/values\n        nearest_targets = y_train_np[nearest_indices]\n        \n        if task == \"classification\":\n            # Count occurrences of each label\n            counter = Counter(nearest_targets)\n            \n            # Find the most common label(s)\n            max_count = max(counter.values())\n            most_common = [label for label, count in counter.items() if count == max_count]\n            \n            # In case of a tie, return the smallest label\n            predictions.append(min(most_common))\n        \n        elif task == \"regression\":\n            # Calculate the mean of the target values\n            mean_value = np.mean(nearest_targets)\n            # Round to 4 decimal places\n            predictions.append(round(float(mean_value), 4))\n    \n    return predictions\n```\n\nThis implementation:\n1. Uses Euclidean distance for calculating distances between points\n2. Handles the case where k is 0 or larger than the number of training samples\n3. For classification, returns the most frequent label among neighbors, with ties broken by selecting the smallest label\n4. For regression, returns the mean of neighbor values rounded to 4 decimal places\n5. Preserves the order of test samples in the output\n\nThe solution is efficient by using NumPy for vector operations and follows all the specified requirements."}
{"task_id": 184, "completion_id": 0, "completion": "I'll implement a CART decision tree classifier from scratch. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    # Define the Node class for the decision tree\n    class Node:\n        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n            self.feature = feature      # Index of the feature to split on\n            self.threshold = threshold  # Threshold value for the split\n            self.left = left            # Left child node (samples where feature <= threshold)\n            self.right = right          # Right child node (samples where feature > threshold)\n            self.value = value          # Class label for leaf nodes\n\n    # Calculate Gini impurity\n    def gini_impurity(y):\n        m = len(y)\n        if m == 0:\n            return 0\n        \n        # Count occurrences of each class\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / m\n        \n        # Calculate Gini impurity: 1 - sum(p_i^2)\n        return 1 - np.sum(probabilities**2)\n\n    # Find the majority class in a set of labels\n    def majority_class(y):\n        if len(y) == 0:\n            return 0\n        unique_classes, counts = np.unique(y, return_counts=True)\n        return unique_classes[np.argmax(counts)]\n\n    # Find the best split for a node\n    def find_best_split(X, y):\n        m, n_features = X.shape\n        \n        # If there's only one sample or all labels are the same, no split is needed\n        if m <= 1 or len(np.unique(y)) == 1:\n            return None, None, None, None, None\n        \n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_indices = None\n        best_right_indices = None\n        \n        # Calculate parent Gini impurity\n        parent_gini = gini_impurity(y)\n        \n        # Try each feature\n        for feature_idx in range(n_features):\n            # Get unique values for this feature\n            feature_values = X[:, feature_idx]\n            \n            # Sort the values and get the midpoints\n            sorted_indices = np.argsort(feature_values)\n            sorted_values = feature_values[sorted_indices]\n            sorted_labels = y[sorted_indices]\n            \n            # Find midpoints between different consecutive values\n            midpoints = []\n            for i in range(1, len(sorted_values)):\n                if sorted_values[i] != sorted_values[i-1]:\n                    midpoints.append((sorted_values[i] + sorted_values[i-1]) / 2)\n            \n            # Try each midpoint as a threshold\n            for threshold in midpoints:\n                # Split the data\n                left_indices = np.where(X[:, feature_idx] <= threshold)[0]\n                right_indices = np.where(X[:, feature_idx] > threshold)[0]\n                \n                # Skip if one of the splits is empty\n                if len(left_indices) == 0 or len(right_indices) == 0:\n                    continue\n                \n                # Calculate the weighted Gini impurity of children\n                left_gini = gini_impurity(y[left_indices])\n                right_gini = gini_impurity(y[right_indices])\n                \n                # Calculate the weights (proportion of samples)\n                left_weight = len(left_indices) / m\n                right_weight = len(right_indices) / m\n                \n                # Calculate the weighted Gini impurity\n                weighted_gini = left_weight * left_gini + right_weight * right_gini\n                \n                # Calculate the Gini gain\n                gain = parent_gini - weighted_gini\n                \n                # Update the best split if this one is better\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                    best_left_indices = left_indices\n                    best_right_indices = right_indices\n        \n        return best_gain, best_feature, best_threshold, best_left_indices, best_right_indices\n\n    # Build the decision tree recursively\n    def build_tree(X, y, depth=0):\n        # Check stopping criteria\n        if (max_depth is not None and depth >= max_depth) or len(np.unique(y)) == 1:\n            return Node(value=majority_class(y))\n        \n        # Find the best split\n        gain, feature, threshold, left_indices, right_indices = find_best_split(X, y)\n        \n        # If no improvement or empty split, create a leaf node\n        if gain is None or gain <= 0:\n            return Node(value=majority_class(y))\n        \n        # Create child nodes\n        left_node = build_tree(X[left_indices], y[left_indices], depth + 1)\n        right_node = build_tree(X[right_indices], y[right_indices], depth + 1)\n        \n        # Return a decision node\n        return Node(feature=feature, threshold=threshold, left=left_node, right=right_node)\n\n    # Make predictions using the decision tree\n    def predict_sample(node, sample):\n        # If it's a leaf node, return its value\n        if node.value is not None:\n            return node.value\n        \n        # Otherwise, navigate the tree based on the feature and threshold\n        if sample[node.feature] <= node.threshold:\n            return predict_sample(node.left, sample)\n        else:\n            return predict_sample(node.right, sample)\n\n    # Build the tree\n    root = build_tree(X_train, y_train)\n    \n    # Make predictions for each test sample\n    predictions = []\n    for sample in X_test:\n        predictions.append(predict_sample(root, sample))\n    \n    return predictions\n```\n\nThis implementation follows the CART algorithm with Gini impurity as the splitting criterion. It recursively builds a binary decision tree by finding the best feature and threshold to split on at each node. The tree stops growing when one of the stopping criteria is met: all labels are identical, maximum depth is reached, or a split would produce an empty child node.\n\nFor each test sample, the algorithm traverses the tree from the root to a leaf node based on the feature values and thresholds, and returns the predicted class label."}
{"task_id": 190, "completion_id": 0, "completion": "I'll solve this task by implementing a function to find the best Gini split for a decision tree classifier.\n\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    X = np.array(X)\n    y = np.array(y)\n    \n    n_samples, n_features = X.shape\n    n_total = n_samples\n    \n    # Calculate Gini impurity of the whole dataset\n    unique_classes, counts = np.unique(y, return_counts=True)\n    class_probabilities = counts / n_total\n    gini_whole = 1 - np.sum(class_probabilities ** 2)\n    \n    # If all samples belong to the same class, no valid split exists\n    if len(unique_classes) <= 1:\n        return (-1, None, round(gini_whole, 4))\n    \n    best_gini = float('inf')\n    best_feature = -1\n    best_threshold = None\n    \n    for feature_idx in range(n_features):\n        # Get unique values for this feature\n        feature_values = X[:, feature_idx]\n        \n        # If all values are the same, skip this feature\n        if np.all(feature_values == feature_values[0]):\n            continue\n        \n        # Get unique values to try as thresholds\n        unique_values = np.unique(feature_values)\n        \n        # Try each value as a threshold\n        for i in range(len(unique_values) - 1):\n            threshold = (unique_values[i] + unique_values[i + 1]) / 2\n            \n            # Split the data\n            left_mask = feature_values <= threshold\n            right_mask = ~left_mask\n            \n            n_left = np.sum(left_mask)\n            n_right = np.sum(right_mask)\n            \n            # Skip if either child node is empty\n            if n_left == 0 or n_right == 0:\n                continue\n            \n            # Calculate Gini impurity for left child\n            left_labels = y[left_mask]\n            unique_left, counts_left = np.unique(left_labels, return_counts=True)\n            prob_left = counts_left / n_left\n            gini_left = 1 - np.sum(prob_left ** 2)\n            \n            # Calculate Gini impurity for right child\n            right_labels = y[right_mask]\n            unique_right, counts_right = np.unique(right_labels, return_counts=True)\n            prob_right = counts_right / n_right\n            gini_right = 1 - np.sum(prob_right ** 2)\n            \n            # Calculate weighted Gini impurity\n            gini_split = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n            \n            # Update best split if this one is better\n            if gini_split < best_gini - 1e-12:\n                best_gini = gini_split\n                best_feature = feature_idx\n                best_threshold = threshold\n            # Tie-breaking rules\n            elif abs(gini_split - best_gini) < 1e-12:\n                if feature_idx < best_feature:\n                    best_gini = gini_split\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                elif feature_idx == best_feature and threshold < best_threshold:\n                    best_gini = gini_split\n                    best_threshold = threshold\n    \n    # If no valid split was found\n    if best_feature == -1:\n        return (-1, None, round(gini_whole, 4))\n    \n    return (best_feature, best_threshold, round(best_gini, 4))\n```\n\nThis implementation:\n\n1. Calculates the Gini impurity of the whole dataset\n2. Iterates through each feature and potential threshold value\n3. For each potential split, calculates the weighted Gini impurity\n4. Keeps track of the best split according to the specified tie-breaking rules\n5. Returns the appropriate tuple with the best feature index, threshold value, and rounded Gini impurity\n6. Handles the case where no valid split exists\n\nThe algorithm follows the specified tie-breaking rules by preferring splits with smaller Gini impurity, then smaller feature index, and finally smaller threshold value."}
{"task_id": 191, "completion_id": 0, "completion": "I'll implement the metric factory function that produces validated distance functions according to the requirements.\n\n```python\nimport numpy as np\nfrom typing import Callable, Union, List, Tuple, Iterable, Any\n\ndef metric(name: str) -> Callable[[Any, Any], Union[float, int]]:\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    \n    def validate_vectors(x: Any, y: Any) -> Union[Tuple[np.ndarray, np.ndarray], bool]:\n        \"\"\"Validate input vectors according to requirements.\"\"\"\n        # Check if inputs are lists, tuples or numpy arrays\n        if not (isinstance(x, (list, tuple, np.ndarray)) and isinstance(y, (list, tuple, np.ndarray))):\n            return False\n        \n        try:\n            # Convert to numpy arrays\n            x_arr = np.array(x, dtype=float)\n            y_arr = np.array(y, dtype=float)\n            \n            # Check if one-dimensional\n            if x_arr.ndim != 1 or y_arr.ndim != 1:\n                return False\n            \n            # Check if non-empty\n            if x_arr.size == 0 or y_arr.size == 0:\n                return False\n            \n            # Check if same length\n            if x_arr.size != y_arr.size:\n                return False\n            \n            return x_arr, y_arr\n        except (ValueError, TypeError):\n            return False\n    \n    def euclidean_distance(x: Any, y: Any) -> Union[float, int]:\n        \"\"\"Compute Euclidean distance between two vectors.\"\"\"\n        validated = validate_vectors(x, y)\n        if validated is False:\n            return -1\n        \n        x_arr, y_arr = validated\n        return round(np.sqrt(np.sum((x_arr - y_arr) ** 2)), 4)\n    \n    def manhattan_distance(x: Any, y: Any) -> Union[float, int]:\n        \"\"\"Compute Manhattan distance between two vectors.\"\"\"\n        validated = validate_vectors(x, y)\n        if validated is False:\n            return -1\n        \n        x_arr, y_arr = validated\n        return round(np.sum(np.abs(x_arr - y_arr)), 4)\n    \n    def chebyshev_distance(x: Any, y: Any) -> Union[float, int]:\n        \"\"\"Compute Chebyshev distance between two vectors.\"\"\"\n        validated = validate_vectors(x, y)\n        if validated is False:\n            return -1\n        \n        x_arr, y_arr = validated\n        return round(np.max(np.abs(x_arr - y_arr)), 4)\n    \n    def cosine_distance(x: Any, y: Any) -> Union[float, int]:\n        \"\"\"Compute Cosine distance between two vectors.\"\"\"\n        validated = validate_vectors(x, y)\n        if validated is False:\n            return -1\n        \n        x_arr, y_arr = validated\n        \n        # Calculate norms\n        x_norm = np.linalg.norm(x_arr)\n        y_norm = np.linalg.norm(y_arr)\n        \n        # Check if norms are non-zero\n        if x_norm == 0 or y_norm == 0:\n            return -1\n        \n        # Calculate cosine similarity\n        cosine_sim = np.dot(x_arr, y_arr) / (x_norm * y_norm)\n        \n        # Ensure cosine similarity is within valid range [-1, 1]\n        cosine_sim = max(min(cosine_sim, 1.0), -1.0)\n        \n        # Calculate cosine distance\n        return round(1 - cosine_sim, 4)\n    \n    # Map metric names to their corresponding functions\n    metrics = {\n        'euclidean': euclidean_distance,\n        'manhattan': manhattan_distance,\n        'chebyshev': chebyshev_distance,\n        'cosine': cosine_distance\n    }\n    \n    # Return the appropriate distance function or a function that always returns -1\n    return metrics.get(name, lambda x, y: -1)\n```\n\nThis implementation:\n\n1. Creates a factory function that returns the appropriate distance function based on the provided name\n2. Includes a validation function that checks if inputs are valid according to the requirements\n3. Implements all four distance metrics as specified\n4. Handles edge cases like zero-norm vectors for cosine distance\n5. Returns -1 for invalid inputs or unsupported metric names\n6. Rounds all valid results to 4 decimal places"}
{"task_id": 197, "completion_id": 0, "completion": "I'll implement the `action_stats` function to analyze RL environment action spaces according to the requirements.\n\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    n_actions_per_dim = []\n    action_dim = 0\n    has_continuous = cont_action\n    \n    if md_action:\n        # Multi-dimensional action space\n        action_dim = len(env.action_space.spaces)\n        \n        # Process each dimension\n        for space in env.action_space.spaces:\n            if hasattr(space, 'n'):  # Discrete dimension\n                n_actions_per_dim.append(space.n)\n            else:  # Continuous dimension\n                has_continuous = True\n                n_actions_per_dim.append(np.inf)\n    else:\n        # Single-dimensional action space\n        action_dim = 1\n        \n        if cont_action:  # Continuous\n            n_actions_per_dim.append(np.inf)\n        else:  # Discrete\n            n_actions_per_dim.append(env.action_space.n)\n    \n    # Generate action_ids for fully discrete spaces\n    if has_continuous:\n        action_ids = None\n    else:\n        # Create ranges for each dimension\n        ranges = [range(n) for n in n_actions_per_dim]\n        # Generate cartesian product\n        action_ids = list(product(*ranges))\n    \n    return n_actions_per_dim, action_ids, action_dim\n```\n\nThis implementation:\n1. Handles both single and multi-dimensional action spaces\n2. Distinguishes between continuous and discrete spaces\n3. Returns the required statistics:\n   - `n_actions_per_dim`: List with number of actions per dimension (infinity for continuous dimensions)\n   - `action_ids`: List of all possible discrete actions as tuples, or None if any dimension is continuous\n   - `action_dim`: Total number of action dimensions\n\nThe function properly handles all the specified cases and follows the OpenAI Gym action space conventions."}
{"task_id": 198, "completion_id": 0, "completion": "I'll implement the \u03b2-maximization step for Latent Dirichlet Allocation as requested. Let me break down the approach:\n\n1. Create a matrix to accumulate the counts for each word-topic pair\n2. Iterate through each document and its tokens, updating the counts based on the phi values\n3. Normalize each topic (column) to sum to 1\n4. Round to 4 decimal places\n5. Return the result as a list of lists\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # Get the number of topics T from the first document's phi matrix\n    T = phi[0].shape[1]\n    \n    # Initialize beta matrix with zeros\n    beta = np.zeros((V, T))\n    \n    # Iterate through each document\n    for d in range(len(corpus)):\n        # Iterate through each token in the document\n        for n in range(len(corpus[d])):\n            # Get the word index for this token\n            v = corpus[d][n]\n            # Add the contribution of this token to beta\n            beta[v, :] += phi[d][n, :]\n    \n    # Normalize each column (topic) to sum to 1\n    # Handle the case where a column might be all zeros\n    col_sums = beta.sum(axis=0)\n    for t in range(T):\n        if col_sums[t] > 0:\n            beta[:, t] /= col_sums[t]\n    \n    # Round to 4 decimal places\n    beta = np.round(beta, 4)\n    \n    # Convert to list of lists\n    beta_list = [list(row) for row in beta]\n    \n    return beta_list\n```\n\nThis implementation follows the formula provided in the task description. It accumulates the counts for each word-topic pair based on the phi values and the corpus, then normalizes each topic column to sum to 1. The result is rounded to 4 decimal places and returned as a list of lists."}
{"task_id": 202, "completion_id": 0, "completion": "# Partitioning Around Medoids (PAM) Clustering Implementation\n\nI'll implement the PAM clustering algorithm following the specified deterministic approach.\n\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # 1. Initialize medoids as the first k samples\n    medoid_indices = list(range(k))\n    \n    # Function to calculate total cost given a set of medoid indices\n    def calculate_cost(medoid_idx):\n        # Calculate distances between each sample and each medoid\n        distances = np.zeros((n_samples, len(medoid_idx)))\n        for i, m_idx in enumerate(medoid_idx):\n            # Euclidean distance between each sample and this medoid\n            distances[:, i] = np.sqrt(np.sum((X - X[m_idx])**2, axis=1))\n        \n        # Assign each sample to the closest medoid\n        closest_medoid = np.argmin(distances, axis=1)\n        \n        # Calculate total cost (sum of distances to closest medoid)\n        total_cost = sum(distances[i, closest_medoid[i]] for i in range(n_samples))\n        return total_cost\n    \n    # 2. Iteratively improve the medoids\n    while True:\n        current_cost = calculate_cost(medoid_indices)\n        best_swap = None\n        best_cost_reduction = 0\n        \n        # Try all possible swaps between a medoid and a non-medoid\n        for i, medoid_idx in enumerate(medoid_indices):\n            for non_medoid_idx in range(n_samples):\n                if non_medoid_idx not in medoid_indices:\n                    # Create a new set of medoid indices with the swap\n                    new_medoid_indices = medoid_indices.copy()\n                    new_medoid_indices[i] = non_medoid_idx\n                    \n                    # Calculate the cost with the new medoids\n                    new_cost = calculate_cost(new_medoid_indices)\n                    cost_reduction = current_cost - new_cost\n                    \n                    # If this swap gives a better cost reduction, remember it\n                    if cost_reduction > best_cost_reduction:\n                        best_cost_reduction = cost_reduction\n                        best_swap = (i, non_medoid_idx)\n        \n        # If no swap can further decrease the cost, stop\n        if best_cost_reduction <= 0:\n            break\n        \n        # Perform the best swap\n        medoid_indices[best_swap[0]] = best_swap[1]\n    \n    # 3. Assign each sample to the closest medoid\n    # Calculate distances between each sample and each medoid\n    distances = np.zeros((n_samples, k))\n    for i, m_idx in enumerate(medoid_indices):\n        distances[:, i] = np.sqrt(np.sum((X - X[m_idx])**2, axis=1))\n    \n    # Assign each sample to the closest medoid\n    # In case of ties, choose the medoid that appears first\n    labels = np.argmin(distances, axis=1)\n    \n    # 4. Return the cluster labels\n    return labels.tolist()\n```\n\nThis implementation follows the specified algorithm:\n1. It initializes the medoids as the first k samples\n2. It iteratively tries all possible swaps between a medoid and a non-medoid, accepting the swap that gives the largest cost reduction\n3. It stops when no swap can further decrease the cost\n4. It assigns each sample to the closest medoid, breaking ties by choosing the medoid that appears first in the list\n5. It returns the cluster labels as a Python list of integers\n\nThe algorithm is deterministic as required, and will produce consistent results for the same input data."}
{"task_id": 216, "completion_id": 0, "completion": "I'll implement the Elastic Net regression from scratch as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    x_train = np.array(x_train, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n    x_test = np.array(x_test, dtype=float)\n    \n    # Check for valid inputs\n    if x_train.shape[1] != x_test.shape[1] or n_iterations < 1:\n        return -1\n    \n    # Generate polynomial features\n    x_train_poly = generate_polynomial_features(x_train, degree)\n    x_test_poly = generate_polynomial_features(x_test, degree)\n    \n    # Standardize features (excluding the bias term which will be added later)\n    x_train_std, mean, std = standardize_features(x_train_poly)\n    x_test_std = (x_test_poly - mean) / std\n    \n    # Add bias term (column of ones)\n    x_train_final = np.hstack((np.ones((x_train_std.shape[0], 1)), x_train_std))\n    x_test_final = np.hstack((np.ones((x_test_std.shape[0], 1)), x_test_std))\n    \n    # Initialize weights\n    n_features = x_train_final.shape[1]\n    weights = np.zeros(n_features)\n    \n    # Batch gradient descent\n    n_samples = x_train_final.shape[0]\n    \n    for _ in range(n_iterations):\n        # Compute predictions\n        y_pred = x_train_final.dot(weights)\n        \n        # Compute gradients for MSE\n        mse_gradient = -2/n_samples * x_train_final.T.dot(y_train - y_pred)\n        \n        # Compute regularization gradients (excluding bias term)\n        l1_gradient = np.zeros(n_features)\n        l2_gradient = np.zeros(n_features)\n        \n        # Apply regularization to all weights except bias\n        l1_gradient[1:] = l1_ratio * np.sign(weights[1:])\n        l2_gradient[1:] = (1 - l1_ratio) * 2 * weights[1:]\n        \n        # Combine gradients\n        gradient = mse_gradient + reg_factor * (l1_gradient + l2_gradient)\n        \n        # Update weights\n        weights = weights - learning_rate * gradient\n    \n    # Make predictions\n    predictions = x_test_final.dot(weights)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(predictions, 4).tolist()\n\ndef generate_polynomial_features(X, degree):\n    \"\"\"Generate polynomial features up to the specified degree.\"\"\"\n    if degree == 1:\n        return X\n    \n    n_samples, n_features = X.shape\n    poly_features = np.ones((n_samples, 0))\n    \n    for d in range(1, degree + 1):\n        for i in range(n_features):\n            poly_features = np.hstack((poly_features, X[:, i:i+1] ** d))\n    \n    return poly_features\n\ndef standardize_features(X):\n    \"\"\"Standardize features to have zero mean and unit variance.\"\"\"\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n    \n    # Avoid division by zero\n    std = np.where(std == 0, 1, std)\n    \n    X_std = (X - mean) / std\n    return X_std, mean, std\n```\n\nThis implementation:\n\n1. Converts inputs to numpy arrays and performs validation checks\n2. Generates polynomial features of the specified degree\n3. Standardizes all features (except the bias term)\n4. Adds a bias term (column of ones)\n5. Implements batch gradient descent with Elastic Net regularization\n6. Returns predictions rounded to 4 decimal places\n\nThe regularization combines L1 (lasso) and L2 (ridge) penalties according to the l1_ratio parameter, and applies them to all weights except the bias term."}
{"task_id": 217, "completion_id": 0, "completion": "I'll implement the logistic components function as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Calculate sigmoid function for predicted values\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Calculate probabilities using sigmoid\n    probabilities = sigmoid(predicted_array)\n    \n    # Calculate gradient: actual_i * sigmoid(-actual_i * predicted_i)\n    gradient = actual_array * sigmoid(-actual_array * predicted_array)\n    \n    # Calculate Hessian: sigmoid(predicted_i) * (1 - sigmoid(predicted_i))\n    hessian = probabilities * (1 - probabilities)\n    \n    # Round all values to 6 decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    probabilities_rounded = np.round(probabilities, 6)\n    \n    # Convert numpy arrays back to Python lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = probabilities_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\nThis implementation:\n1. Converts the input containers to NumPy arrays\n2. Defines the sigmoid function\n3. Calculates the probabilities using the sigmoid function\n4. Computes the gradient and Hessian according to the given formulas\n5. Rounds all values to exactly 6 decimal places\n6. Converts the NumPy arrays back to Python lists\n7. Returns the three lists as a tuple\n\nThe function handles all the required input types (lists, tuples, or NumPy arrays) and performs vectorized computation for efficiency."}
{"task_id": 218, "completion_id": 0, "completion": "I'll solve this Blackjack hand outcome evaluation task by implementing the function according to the specified rules.\n\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    # Calculate the score of a hand\n    def calculate_score(hand: list[int]) -> int:\n        # Count aces and sum non-ace cards\n        aces = hand.count(1)\n        score = sum(hand) - aces  # Subtract aces to handle them separately\n        \n        # Add aces as 1 each\n        score += aces\n        \n        # Check if we can use one ace as 11 without busting\n        if aces > 0 and score + 10 <= 21:\n            score += 10\n            \n        return score\n    \n    # Check if a hand is a natural blackjack\n    def is_natural_blackjack(hand: list[int]) -> bool:\n        return len(hand) == 2 and 1 in hand and 10 in hand\n    \n    # Calculate scores\n    player_score = calculate_score(player)\n    dealer_score = calculate_score(dealer)\n    \n    # Rule 1: If player busts, player loses\n    if player_score > 21:\n        return -1.0\n    \n    # Rule 2: If dealer busts (and player didn't), player wins\n    if dealer_score > 21:\n        # Check if player has natural blackjack for 1.5x payout\n        if is_natural_blackjack(player):\n            return 1.5\n        return 1.0\n    \n    # Rule 3: Compare scores\n    if player_score > dealer_score:\n        # Rule 4: Check for natural blackjack\n        if is_natural_blackjack(player):\n            return 1.5\n        return 1.0\n    elif player_score < dealer_score:\n        return -1.0\n    else:  # Equal scores\n        return 0.0\n```\n\nThis implementation follows the rules specified in the task:\n1. It calculates the score for each hand, accounting for usable aces\n2. It checks for player busts first (immediate loss)\n3. It checks for dealer busts (player win)\n4. It compares scores when neither busts\n5. It handles the special case of natural blackjack (1.5x payout)\n\nThe function returns exactly the values required: -1, 0, 1, or 1.5 as floats."}
{"task_id": 221, "completion_id": 0, "completion": "I'll implement the Mean Squared Logarithmic Error (MSLE) function according to the requirements. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Helper function that returns element-wise squared log errors.\n    \n    Computes (log(1 + actual) - log(1 + predicted))^2 for each element.\n    \"\"\"\n    actual_log = np.log1p(actual)  # log(1 + x) is more numerically stable\n    predicted_log = np.log1p(predicted)\n    return np.square(actual_log - predicted_log)\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values. All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values. Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals. If the inputs are invalid the\n        function returns \u20111.\n    \"\"\"\n    # Convert inputs to numpy arrays for consistent handling\n    actual_array = np.asarray(actual)\n    predicted_array = np.asarray(predicted)\n    \n    # Validate inputs\n    # Check if shapes match\n    if actual_array.shape != predicted_array.shape:\n        return -1\n    \n    # Check if all values are non-negative\n    if np.any(actual_array < 0) or np.any(predicted_array < 0):\n        return -1\n    \n    # Calculate MSLE\n    sle = squared_log_error(actual_array, predicted_array)\n    msle = np.mean(sle)\n    \n    # Round to 4 decimal places\n    return round(float(msle), 4)\n```\n\nThis implementation:\n1. Converts inputs to NumPy arrays for consistent handling\n2. Validates that both inputs have the same shape and contain only non-negative values\n3. Uses `np.log1p()` which computes log(1+x) in a numerically stable way\n4. Calculates the element-wise squared logarithmic errors using the helper function\n5. Computes the mean and rounds to 4 decimal places\n6. Returns -1 if any validation check fails"}
{"task_id": 222, "completion_id": 0, "completion": "I'll implement a numerically stable sigmoid activation function that works with scalar values, Python lists, and NumPy arrays.\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Convert input to numpy array for consistent handling\n    is_scalar = np.isscalar(z)\n    z_array = np.array(z, dtype=float)\n    \n    # Numerically stable implementation\n    result = np.zeros_like(z_array, dtype=float)\n    \n    # For z >= 0, compute sigmoid directly\n    pos_mask = z_array >= 0\n    result[pos_mask] = 1.0 / (1.0 + np.exp(-z_array[pos_mask]))\n    \n    # For z < 0, use the mathematically equivalent but more stable form\n    neg_mask = ~pos_mask\n    exp_z = np.exp(z_array[neg_mask])\n    result[neg_mask] = exp_z / (1.0 + exp_z)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return appropriate type based on input\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n```\n\nThis implementation:\n\n1. Handles scalar values, Python lists, and NumPy arrays\n2. Uses a numerically stable approach by handling positive and negative inputs differently\n3. Returns a float for scalar inputs and a Python list for array-like inputs\n4. Rounds all values to 4 decimal places as required\n\nThe numerical stability comes from avoiding computing `exp(-z)` for large positive values of `z`, which could cause underflow, and avoiding computing `1/(1+exp(-z))` for large negative values of `z`, which could cause issues with division by a very small number."}
{"task_id": 224, "completion_id": 0, "completion": "# Leaky ReLU Activation Function Implementation\n\nHere's a complete implementation of the Leaky ReLU activation function that handles scalars, lists, and NumPy arrays:\n\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    # Convert input to numpy array for consistent processing\n    is_scalar = np.isscalar(z)\n    original_shape = None\n    \n    if is_scalar:\n        # Handle scalar input\n        result = z if z >= 0 else a * z\n        return result\n    else:\n        # Convert to numpy array if it's not already\n        if not isinstance(z, np.ndarray):\n            z_array = np.array(z)\n            original_shape = np.array(z).shape\n        else:\n            z_array = z\n            original_shape = z.shape\n        \n        # Apply leaky ReLU element-wise\n        result = np.where(z_array >= 0, z_array, a * z_array)\n        \n        # Convert back to list with the same structure as input\n        if isinstance(z, list) or not isinstance(z, np.ndarray):\n            return result.tolist()\n        else:\n            return result.tolist()\n```\n\nThis implementation:\n1. Handles scalar inputs directly\n2. Converts list inputs to NumPy arrays for processing\n3. Applies the Leaky ReLU formula using NumPy's efficient vectorized operations\n4. Returns the result as a Python list with the same structure as the input\n5. Uses the default slope parameter of 0.01 for negative inputs, but allows customization\n\nThe function correctly preserves the shape and structure of the input data while applying the Leaky ReLU activation to each element."}
{"task_id": 226, "completion_id": 0, "completion": "# AdaBoost with Decision Stumps Implementation\n\nI'll implement the AdaBoost algorithm with decision stumps as weak learners. The implementation will follow the steps outlined in the task description.\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(n_samples) / n_samples\n    \n    # List to store the ensemble of weak learners and their weights\n    ensemble = []\n    \n    for _ in range(n_clf):\n        # Find the best stump for the current weights\n        best_stump, min_error, predictions = get_best_stump(X_train, y_train, weights)\n        \n        # Calculate the weight of this classifier\n        epsilon = 1e-10  # Small value to prevent division by zero\n        error = min_error\n        alpha = 0.5 * np.log((1 - error) / (error + epsilon))\n        \n        # Store the stump and its weight\n        ensemble.append((best_stump, alpha))\n        \n        # Update sample weights\n        weights = update_weights(weights, alpha, y_train, predictions)\n        \n    # Make predictions on the test set\n    return ensemble_predict(X_test, ensemble)\n\ndef get_best_stump(X, y, weights):\n    \"\"\"Find the best decision stump for the weighted data.\"\"\"\n    n_samples, n_features = X.shape\n    min_error = float('inf')\n    best_stump = None\n    best_predictions = None\n    \n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        \n        # Get unique values for thresholds\n        thresholds = np.unique(feature_values)\n        \n        for threshold in thresholds:\n            # Try both polarities\n            for polarity in [1, -1]:\n                # Make predictions with this stump\n                predictions = stump_predict(X, feature_idx, threshold, polarity)\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                error = np.sum(weights[misclassified])\n                \n                if error < min_error:\n                    min_error = error\n                    best_stump = (feature_idx, threshold, polarity)\n                    best_predictions = predictions\n    \n    return best_stump, min_error, best_predictions\n\ndef stump_predict(X, feature_idx, threshold, polarity):\n    \"\"\"Make predictions using a decision stump.\"\"\"\n    n_samples = X.shape[0]\n    predictions = np.ones(n_samples)\n    \n    if polarity == 1:\n        predictions[X[:, feature_idx] < threshold] = -1\n    else:  # polarity == -1\n        predictions[X[:, feature_idx] < threshold] = 1\n    \n    return predictions\n\ndef update_weights(weights, alpha, y_true, y_pred):\n    \"\"\"Update sample weights based on misclassifications.\"\"\"\n    # Calculate the new weights\n    weights = weights * np.exp(-alpha * y_true * y_pred)\n    \n    # Normalize weights to sum to 1\n    return weights / np.sum(weights)\n\ndef ensemble_predict(X, ensemble):\n    \"\"\"Make predictions using the trained ensemble.\"\"\"\n    n_samples = X.shape[0]\n    ensemble_predictions = np.zeros(n_samples)\n    \n    for stump, alpha in ensemble:\n        feature_idx, threshold, polarity = stump\n        predictions = stump_predict(X, feature_idx, threshold, polarity)\n        ensemble_predictions += alpha * predictions\n    \n    # Return the sign of the weighted sum\n    return [int(np.sign(pred)) for pred in ensemble_predictions]\n```\n\nThis implementation follows the AdaBoost algorithm with decision stumps as weak learners. For each boosting round, it:\n\n1. Finds the best decision stump that minimizes the weighted classification error\n2. Computes the weight (alpha) for this stump\n3. Updates the sample weights to focus more on misclassified samples\n4. Finally makes predictions by taking the sign of the weighted sum of all stump decisions\n\nThe output is a list of integers, either -1 or 1, representing the predicted class labels for each sample in X_test."}
{"task_id": 241, "completion_id": 0, "completion": "I'll implement the orthogonal weight initializer as described in the task. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    # Check if shape has at least 2 dimensions\n    if len(shape) < 2:\n        return -1\n    \n    # Calculate the flattened shape for the 2D matrix A\n    d0 = shape[0]\n    d_rest = np.prod(shape[1:])\n    \n    # Create a 2D matrix A filled with samples from standard normal distribution\n    A = np.random.normal(0, 1, size=(d0, d_rest))\n    \n    # Compute the SVD of A\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n    \n    # Choose the SVD factor that has the same size as A\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n    \n    # Reshape Q back to the requested tensor shape and multiply by scale\n    result = scale * Q.reshape(shape)\n    \n    # Convert to Python list and return\n    return result.tolist()\n```\n\nLet's verify this implementation with the example provided:\n\n```python\nnp.random.seed(0)\nresult = orthogonal((2, 2), 0.5)\nprint(np.array(result))\n\n# Verify orthogonality\nflat_result = np.array(result).reshape(2, -1)\nprint(\"Orthogonality check:\", np.allclose(flat_result @ flat_result.T, 0.5**2 * np.eye(2), atol=1e-6))\n```\n\nThe implementation follows the algorithm described in the task:\n1. It checks if the shape has at least 2 dimensions\n2. Creates a 2D matrix A with the appropriate dimensions filled with normal distribution samples\n3. Computes the SVD of A\n4. Selects the appropriate SVD factor (U or Vt) based on which matches A's shape\n5. Reshapes the result back to the requested tensor shape and scales it\n6. Returns the result as a Python list\n\nThe resulting matrix will satisfy the orthogonality condition as required."}
{"task_id": 243, "completion_id": 0, "completion": "I'll implement the forward pass of the Actor-Critic network as described, using only NumPy operations.\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert state to numpy array\n    state = np.array(state)\n    \n    # All weights are 1.0 and all biases are 0.0\n    # Dense-1: input \u2192 4 neurons, ReLU activation\n    # Each neuron receives sum of all inputs\n    z1 = np.maximum(0, np.sum(state) * np.ones(4))\n    \n    # Dense-2: 4 \u2192 4 neurons, ReLU activation\n    # Each neuron receives sum of all previous activations\n    z2 = np.maximum(0, np.sum(z1) * np.ones(4))\n    \n    # Dense-3: 4 \u2192 4 neurons, ReLU activation\n    # Each neuron receives sum of all previous activations\n    z3 = np.maximum(0, np.sum(z2) * np.ones(4))\n    \n    # Actor head: 4 \u2192 3 neurons, Softmax activation\n    # Each neuron receives sum of all previous activations\n    logits = np.sum(z3) * np.ones(3)\n    \n    # Apply softmax to get action probabilities\n    exp_logits = np.exp(logits - np.max(logits))  # Subtract max for numerical stability\n    action_probs = exp_logits / np.sum(exp_logits)\n    \n    # Critic head: 4 \u2192 1 neuron (no activation)\n    state_value = np.sum(z3)\n    \n    # Round to 4 decimal places\n    action_probs = [round(prob, 4) for prob in action_probs]\n    state_value = round(state_value, 4)\n    \n    return action_probs, state_value\n```\n\nThis implementation follows the network architecture described in the task. Since all weights are 1.0 and all biases are 0.0, each neuron in a layer receives the sum of all activations from the previous layer. The ReLU activation function is applied to the first three dense layers, and softmax is applied to the actor head to get action probabilities. The critic head outputs a single scalar value without any activation function."}
{"task_id": 249, "completion_id": 0, "completion": "I'll implement the `actor_critic_forward` function that performs a forward pass through a three-layer Actor-Critic network using NumPy.\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to numpy array if it's not already\n    state = np.array(state, dtype=np.float64)\n    \n    # Convert all parameters from lists to numpy arrays\n    weights = {}\n    for key, value in params.items():\n        weights[key] = np.array(value, dtype=np.float64)\n    \n    # Reshape weights if needed\n    for key in weights:\n        if key.startswith('W'):\n            # For weight matrices, ensure they're 2D\n            if weights[key].ndim == 1:\n                weights[key] = weights[key].reshape(1, -1)\n    \n    # Forward pass through shared layers\n    # Layer 1: Dense + ReLU\n    h1 = np.dot(state, weights['W1'].T) + weights['b1']\n    h1 = np.maximum(0, h1)  # ReLU activation\n    \n    # Layer 2: Dense + ReLU\n    h2 = np.dot(h1, weights['W2'].T) + weights['b2']\n    h2 = np.maximum(0, h2)  # ReLU activation\n    \n    # Layer 3: Dense + ReLU\n    h3 = np.dot(h2, weights['W3'].T) + weights['b3']\n    h3 = np.maximum(0, h3)  # ReLU activation\n    \n    # Actor head: Dense + Softmax\n    actor_output = np.dot(h3, weights['Wa'].T) + weights['ba']\n    \n    # Apply softmax to get action probabilities\n    # Subtract max for numerical stability\n    exp_actor = np.exp(actor_output - np.max(actor_output))\n    probabilities = exp_actor / np.sum(exp_actor)\n    \n    # Critic head: Dense (single output)\n    value = float(np.dot(h3, weights['Wc'].T) + weights['bc'])\n    \n    # Round to 4 decimal places\n    probabilities = [round(float(p), 4) for p in probabilities]\n    value = round(value, 4)\n    \n    return probabilities, value\n```\n\nThis implementation:\n1. Converts the input state and parameters to NumPy arrays\n2. Performs the forward pass through the shared layers with ReLU activations\n3. Computes the actor head output and applies softmax to get action probabilities\n4. Computes the critic head output to get the state value\n5. Rounds all outputs to 4 decimal places as required\n6. Returns the probabilities as a list and the value as a float\n\nThe softmax implementation includes the standard numerical stability trick of subtracting the maximum value before applying the exponential function."}
{"task_id": 253, "completion_id": 0, "completion": "I'll implement the `elastic_net_regularization` function that computes either the Elastic-Net penalty value or its gradient based on the provided formulas.\n\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    w = np.array(w, dtype=float)\n    \n    if gradient:\n        # Compute the gradient: \u03b1 \u00b7 [ \u03bb \u00b7 sign(w) + (1\u2212\u03bb) \u00b7 w ]\n        l1_grad = l1_ratio * np.sign(w)\n        l2_grad = (1 - l1_ratio) * w\n        grad = alpha * (l1_grad + l2_grad)\n        \n        # Round each element to 4 decimal places and convert to list\n        return [round(g, 4) for g in grad]\n    else:\n        # Compute the penalty: \u03b1 \u00b7 [ \u03bb \u00b7 ||w||\u2081 + (1\u2212\u03bb)\u00b70.5\u00b7w\u1d40w ]\n        l1_penalty = l1_ratio * np.sum(np.abs(w))\n        l2_penalty = (1 - l1_ratio) * 0.5 * np.sum(w**2)\n        penalty = alpha * (l1_penalty + l2_penalty)\n        \n        # Round to 4 decimal places\n        return round(penalty, 4)\n```\n\nThis implementation:\n1. Converts the input weight vector to a NumPy array\n2. Computes either the penalty value or the gradient based on the `gradient` flag\n3. Rounds the results to 4 decimal places as required\n4. Returns a scalar for the penalty value or a list for the gradient\n\nThe function follows the mathematical formulas provided in the task description."}
{"task_id": 256, "completion_id": 0, "completion": "I'll implement the numerical gradient check for a vanilla RNN parameter as requested. Here's the solution:\n\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    # Handle non-trainable parameters\n    if param_name in [\"X\", \"y\"]:\n        return None\n    \n    # Handle special cases for parameter names\n    actual_param_name = param_name.lower() if param_name in [\"Ba\", \"Bx\"] else param_name\n    \n    # Check if the parameter exists in the model\n    if actual_param_name not in model.parameters:\n        raise ValueError(f\"Parameter '{actual_param_name}' not found in model parameters\")\n    \n    # Get the parameter to check\n    param = model.parameters[actual_param_name]\n    \n    # Initialize gradient array with the same shape as the parameter\n    grads = np.zeros_like(param, dtype=np.float64)\n    \n    # Iterate over each element of the parameter\n    for idx in np.ndindex(param.shape):\n        # Create copies of the model for positive and negative perturbations\n        model_plus = deepcopy(model)\n        model_minus = deepcopy(model)\n        \n        # Apply positive perturbation\n        model_plus.parameters[actual_param_name] = model_plus.parameters[actual_param_name].astype(np.float64)\n        model_plus.parameters[actual_param_name][idx] += epsilon\n        \n        # Apply negative perturbation\n        model_minus.parameters[actual_param_name] = model_minus.parameters[actual_param_name].astype(np.float64)\n        model_minus.parameters[actual_param_name][idx] -= epsilon\n        \n        # Forward pass with positive perturbation\n        model_plus.flush_gradients()\n        predictions_plus = []\n        for t in range(n_t):\n            pred_plus = model_plus.forward(X[:, :, t])\n            predictions_plus.append(pred_plus)\n        loss_plus = loss_func(predictions_plus)\n        \n        # Forward pass with negative perturbation\n        model_minus.flush_gradients()\n        predictions_minus = []\n        for t in range(n_t):\n            pred_minus = model_minus.forward(X[:, :, t])\n            predictions_minus.append(pred_minus)\n        loss_minus = loss_func(predictions_minus)\n        \n        # Compute numerical gradient using centered finite difference\n        grads[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n    \n    # Return the transposed gradient\n    return grads.T\n```\n\nThis implementation:\n1. Handles special cases for parameter names (\"X\", \"y\", \"Ba\", \"Bx\")\n2. Creates deep copies of the model for positive and negative perturbations\n3. Iterates through each element of the parameter, perturbing it by +\u03b5 and -\u03b5\n4. Runs the forward pass for n_t time steps for each perturbation\n5. Computes the numerical gradient using the centered finite difference formula\n6. Returns the transposed gradient array with dtype float64"}
{"task_id": 257, "completion_id": 0, "completion": "# AdaBoost with Decision Stumps Implementation\n\nI'll implement the AdaBoost algorithm from scratch using decision stumps as weak learners. The implementation follows the classic AdaBoost algorithm with proper weighting of samples and classifiers.\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n    \n    m = X_train.shape[0]  # Number of training samples\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(m) / m\n    \n    # List to store weak classifiers and their weights\n    weak_classifiers = []\n    \n    for _ in range(n_clf):\n        # Find the best decision stump\n        stump, error, predictions = build_decision_stump(X_train, y_train, weights)\n        \n        # Calculate classifier weight (alpha)\n        # Avoid division by zero or log(0)\n        epsilon = max(1e-10, min(error, 1 - 1e-10))\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        \n        # Store the classifier and its weight\n        weak_classifiers.append((stump, alpha))\n        \n        # Update sample weights\n        weights = weights * np.exp(-alpha * y_train * predictions)\n        \n        # Normalize weights\n        weights = weights / np.sum(weights)\n    \n    # Make predictions on test data\n    return predict_with_ensemble(X_test, weak_classifiers)\n\ndef build_decision_stump(X, y, weights):\n    \"\"\"Build a decision stump that minimizes weighted error.\"\"\"\n    m, n = X.shape\n    min_error = float('inf')\n    best_stump = {}\n    best_predictions = None\n    \n    for feature_idx in range(n):\n        # Get unique values for the feature\n        feature_values = np.unique(X[:, feature_idx])\n        \n        # Try thresholds between each pair of values\n        thresholds = [(feature_values[i] + feature_values[i+1]) / 2 \n                      for i in range(len(feature_values) - 1)]\n        \n        # If there's only one unique value, use that value as the threshold\n        if len(thresholds) == 0 and len(feature_values) > 0:\n            thresholds = [feature_values[0]]\n        \n        for threshold in thresholds:\n            for polarity in [-1, 1]:\n                # Make predictions with this stump\n                predictions = stump_predict(X, feature_idx, threshold, polarity)\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                error = np.sum(weights[misclassified])\n                \n                # If this stump is better, save it\n                if error < min_error:\n                    min_error = error\n                    best_stump = {\n                        'feature_idx': feature_idx,\n                        'threshold': threshold,\n                        'polarity': polarity\n                    }\n                    best_predictions = predictions\n    \n    return best_stump, min_error, best_predictions\n\ndef stump_predict(X, feature_idx, threshold, polarity):\n    \"\"\"Make predictions with a decision stump.\"\"\"\n    m = X.shape[0]\n    predictions = np.ones(m)\n    \n    if polarity == 1:\n        predictions[X[:, feature_idx] <= threshold] = -1\n    else:\n        predictions[X[:, feature_idx] > threshold] = -1\n    \n    return predictions\n\ndef predict_with_ensemble(X, weak_classifiers):\n    \"\"\"Make predictions using the ensemble of weak classifiers.\"\"\"\n    m = X.shape[0]\n    ensemble_predictions = np.zeros(m)\n    \n    for stump, alpha in weak_classifiers:\n        feature_idx = stump['feature_idx']\n        threshold = stump['threshold']\n        polarity = stump['polarity']\n        \n        # Get predictions from this stump\n        predictions = stump_predict(X, feature_idx, threshold, polarity)\n        \n        # Add weighted predictions to ensemble\n        ensemble_predictions += alpha * predictions\n    \n    # Return the sign of the ensemble predictions\n    return [1 if pred >= 0 else -1 for pred in ensemble_predictions]\n```\n\nThis implementation:\n1. Initializes sample weights uniformly\n2. For each iteration:\n   - Builds a decision stump that minimizes the weighted error\n   - Calculates the stump's weight (alpha) based on its error\n   - Updates sample weights to focus more on misclassified examples\n3. Makes predictions by taking the sign of the weighted sum of all weak classifier predictions\n\nThe decision stumps are simple one-level decision trees that make predictions based on a single feature, threshold, and polarity. The algorithm finds the best combination of these parameters to minimize the weighted error at each iteration."}
{"task_id": 261, "completion_id": 0, "completion": "I'll implement the Glorot (Xavier) normal weight initialization function according to the specifications.\n\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least 2 dimensions\")\n    \n    # Calculate fan_in and fan_out\n    if len(shape) == 2:\n        # For 2D tensors (e.g., fully connected layers)\n        fan_in, fan_out = shape\n    else:\n        # For tensors with more than 2 dimensions (e.g., convolutional kernels)\n        # Calculate receptive field size (product of all dimensions after the first two)\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    \n    # Calculate standard deviation\n    std_dev = np.sqrt(2.0 / (fan_in + fan_out))\n    \n    # Generate array with normal distribution\n    # Using the current random state without modifying it globally\n    return np.random.normal(0, std_dev, size=shape).astype(np.float64)\n```\n\nThis implementation:\n1. Checks if the shape has at least 2 dimensions\n2. Calculates fan_in and fan_out based on the shape:\n   - For 2D shapes, uses the dimensions directly\n   - For higher-dimensional shapes, computes the receptive field size and applies the formula\n3. Calculates the standard deviation using the formula sqrt(2/(fan_in + fan_out))\n4. Returns a NumPy array with the requested shape, filled with values drawn from a normal distribution with mean 0 and the calculated standard deviation\n\nThe function uses the current NumPy random state without modifying it globally, and returns a float array with the exact requested shape."}
{"task_id": 266, "completion_id": 0, "completion": "I'll solve this task by building an adjacency list representation of an undirected graph.\n\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Create a mapping from vertex to its index in V for quick lookups\n    vertex_to_index = {vertex: idx for idx, vertex in enumerate(V)}\n    \n    # Initialize the adjacency list with empty lists for each vertex\n    adj_list = [[] for _ in range(len(V))]\n    \n    # Process each edge\n    for u, v in E:\n        # Skip if either vertex is not in V\n        if u not in vertex_to_index or v not in vertex_to_index:\n            continue\n            \n        # For undirected graph, add edge in both directions\n        # (if not already present)\n        if v not in adj_list[vertex_to_index[u]]:\n            adj_list[vertex_to_index[u]].append(v)\n        if u not in adj_list[vertex_to_index[v]]:\n            adj_list[vertex_to_index[v]].append(u)\n    \n    # Sort the neighbors according to their order in V\n    for i in range(len(adj_list)):\n        # Create a list of (vertex, index_in_V) pairs\n        neighbors_with_indices = [(neighbor, vertex_to_index[neighbor]) \n                                  for neighbor in adj_list[i]]\n        # Sort by index_in_V and extract just the vertices\n        adj_list[i] = [vertex for vertex, _ in sorted(neighbors_with_indices, \n                                                     key=lambda x: x[1])]\n    \n    return adj_list\n```\n\nThis solution works by:\n1. Creating a mapping from each vertex to its index in V for efficient lookups\n2. Initializing an empty adjacency list for each vertex\n3. Processing each edge and adding connections in both directions (since it's undirected)\n4. Ensuring no duplicates by checking if a neighbor already exists before adding\n5. Finally, sorting each vertex's neighbors according to their order in the original V list\n\nThe time complexity is O(|E| + |V|\u00b2) in the worst case, where |E| is the number of edges and |V| is the number of vertices. The space complexity is O(|V| + |E|)."}
{"task_id": 267, "completion_id": 0, "completion": "# Weighted Decision Stump Learning Solution\n\nI'll implement a function to find the optimal weighted decision stump for binary classification. The approach will be to:\n\n1. Iterate through each feature\n2. For each feature, sort the data points by feature value\n3. Try all possible thresholds (unique feature values)\n4. For each threshold, try both polarities\n5. Calculate the weighted error for each configuration\n6. Return the stump with the minimum weighted error\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Initialize variables to track the best stump\n    min_error = float('inf')\n    best_feature = 0\n    best_threshold = 0.0\n    best_polarity = 1\n    \n    # Iterate through each feature\n    for feature_idx in range(n_features):\n        # Get the feature values\n        feature_values = X[:, feature_idx]\n        \n        # Get unique values as potential thresholds\n        # Sort them to make the threshold search more efficient\n        thresholds = np.unique(feature_values)\n        \n        # Try each threshold\n        for threshold in thresholds:\n            # Try both polarities\n            for polarity in [1, -1]:\n                # Make predictions based on the current stump configuration\n                predictions = np.ones(n_samples)\n                \n                if polarity == 1:\n                    predictions[feature_values >= threshold] = -1\n                else:  # polarity == -1\n                    predictions[feature_values < threshold] = -1\n                    predictions[feature_values >= threshold] = 1\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                weighted_error = np.sum(sample_weights[misclassified])\n                \n                # Update best stump if this one is better\n                if weighted_error < min_error:\n                    min_error = weighted_error\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                    best_polarity = polarity\n    \n    # Round the threshold and error to 4 decimal places\n    best_threshold = round(best_threshold, 4)\n    min_error = round(min_error, 4)\n    \n    return {\n        \"feature_index\": best_feature,\n        \"threshold\": best_threshold,\n        \"polarity\": best_polarity,\n        \"weighted_error\": min_error\n    }\n```\n\nThis implementation:\n1. Examines all features and all unique feature values as candidate thresholds\n2. Tries both polarities for each threshold\n3. Calculates the weighted classification error for each configuration\n4. Returns the stump with the minimum weighted error\n5. Rounds the threshold and error values to 4 decimal places as required\n\nThe time complexity is O(n\u00b7d\u00b7log(n)) where n is the number of samples and d is the number of features. The log(n) factor comes from sorting the feature values to find unique thresholds."}
{"task_id": 273, "completion_id": 0, "completion": "I'll solve this task by implementing the `best_split` function that finds the optimal numerical threshold for a decision tree split using information gain.\n\n```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    n = len(feature)\n    if n == 0:\n        return (None, 0.0)\n    \n    # Calculate entropy of the parent node\n    parent_counter = Counter(target)\n    parent_entropy = entropy(parent_counter, n)\n    \n    # If all labels are the same, no useful split exists\n    if len(parent_counter) <= 1:\n        return (None, 0.0)\n    \n    # Sort feature values and corresponding targets\n    sorted_indices = sorted(range(n), key=lambda i: feature[i])\n    sorted_features = [feature[i] for i in sorted_indices]\n    sorted_targets = [target[i] for i in sorted_indices]\n    \n    # If all feature values are identical, no useful split exists\n    if sorted_features[0] == sorted_features[-1]:\n        return (None, 0.0)\n    \n    # Find all candidate thresholds (midpoints between distinct consecutive values)\n    thresholds = []\n    for i in range(1, n):\n        if sorted_features[i] != sorted_features[i-1]:\n            thresholds.append((sorted_features[i] + sorted_features[i-1]) / 2)\n    \n    # Evaluate each threshold\n    max_gain = 0.0\n    best_threshold = None\n    \n    left_counter = Counter()\n    right_counter = Counter(sorted_targets)\n    \n    for threshold in thresholds:\n        # Reset counters for this threshold\n        left_counter.clear()\n        right_counter = Counter(sorted_targets)\n        \n        # Find split point for this threshold\n        split_idx = 0\n        while split_idx < n and sorted_features[split_idx] < threshold:\n            left_counter[sorted_targets[split_idx]] += 1\n            right_counter[sorted_targets[split_idx]] -= 1\n            if right_counter[sorted_targets[split_idx]] == 0:\n                del right_counter[sorted_targets[split_idx]]\n            split_idx += 1\n        \n        # Calculate information gain\n        left_count = split_idx\n        right_count = n - left_count\n        \n        if left_count > 0 and right_count > 0:\n            left_entropy = entropy(left_counter, left_count)\n            right_entropy = entropy(right_counter, right_count)\n            \n            gain = parent_entropy - (left_count / n) * left_entropy - (right_count / n) * right_entropy\n            \n            # Update best threshold if this one is better\n            if gain > max_gain or (gain == max_gain and threshold < best_threshold):\n                max_gain = gain\n                best_threshold = threshold\n    \n    # If no positive gain was found\n    if max_gain <= 0:\n        return (None, 0.0)\n    \n    return (round(best_threshold, 4), round(max_gain, 4))\n\ndef entropy(counter, total):\n    \"\"\"Calculate Shannon entropy for a distribution of classes.\"\"\"\n    if total == 0:\n        return 0\n    \n    entropy_val = 0\n    for count in counter.values():\n        p = count / total\n        entropy_val -= p * math.log2(p)\n    \n    return entropy_val\n```\n\nThis implementation:\n\n1. Calculates the entropy of the parent node\n2. Sorts the feature values and corresponding targets\n3. Identifies all candidate thresholds (midpoints between distinct consecutive values)\n4. For each threshold:\n   - Divides the data into left and right partitions\n   - Calculates the entropy of each partition\n   - Computes the information gain\n5. Returns the threshold with the highest information gain (or the smallest such threshold if there are ties)\n6. Returns (None, 0.0) if no useful split exists\n\nThe implementation efficiently handles the case where all feature values are identical or all labels belong to the same class."}
{"task_id": 286, "completion_id": 0, "completion": "I'll implement the required functions for the dynamic weight initializer retrieval task. Here's the complete solution:\n\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    rows, cols = shape\n    return [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    global_objects = globals()\n    \n    if name in global_objects and callable(global_objects[name]):\n        return global_objects[name]\n    else:\n        raise ValueError(\"Invalid initialization function.\")\n```\n\nThis implementation:\n1. Completes the three initializer functions to create matrices of the requested shape\n2. Implements the `get_initializer` function that:\n   - Looks up the requested name in the global namespace\n   - Verifies that the found object is callable\n   - Returns the callable if found\n   - Raises a ValueError with the exact required message if not found or not callable"}
{"task_id": 287, "completion_id": 0, "completion": "I'll implement the 2D average pooling forward and backward pass as requested. Here's the solution:\n\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    # Extract dimensions\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n    \n    # Calculate output dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n    \n    # Initialize output for forward pass\n    output = np.zeros((N, C, out_h, out_w))\n    \n    # Forward pass\n    for n in range(N):\n        for c in range(C):\n            for h in range(out_h):\n                for w in range(out_w):\n                    # Extract the window\n                    h_start = h * s_h\n                    h_end = h_start + p_h\n                    w_start = w * s_w\n                    w_end = w_start + p_w\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    \n                    # Compute average\n                    output[n, c, h, w] = np.mean(window)\n    \n    # Initialize gradient input for backward pass\n    grad_input = np.zeros_like(X)\n    \n    # Backward pass\n    for n in range(N):\n        for c in range(C):\n            for h in range(out_h):\n                for w in range(out_w):\n                    # Calculate window boundaries\n                    h_start = h * s_h\n                    h_end = h_start + p_h\n                    w_start = w * s_w\n                    w_end = w_start + p_w\n                    \n                    # Distribute gradient equally to all elements in the window\n                    # Each element in the window contributed equally to the average\n                    window_size = p_h * p_w\n                    grad_value = accum_grad[n, c, h, w] / window_size\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += grad_value\n    \n    # Round to 4 decimal places and convert to list\n    output_rounded = np.round(output, 4).tolist()\n    grad_input_rounded = np.round(grad_input, 4).tolist()\n    \n    return (output_rounded, grad_input_rounded)\n```\n\nThis implementation:\n\n1. Calculates the output dimensions based on input size, pool shape, and stride\n2. Performs the forward pass by sliding the window and computing the average for each position\n3. Performs the backward pass by distributing the gradient equally to all elements that contributed to each average\n4. Rounds both output arrays to 4 decimal places and converts them to Python lists\n\nThe time complexity is O(N\u00d7C\u00d7H\u00d7W) for both forward and backward passes, which is proportional to the size of the input tensor."}
{"task_id": 290, "completion_id": 0, "completion": "I'll solve this task by implementing a recursive function to compare two decision trees. The function will check if both trees have the same structure, feature indices, thresholds, and leaf values.\n\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both are the same type (both Node or both Leaf)\n    if type(tree_a) != type(tree_b):\n        return False\n    \n    # If both are leaves, compare their values\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        # Use numpy.allclose for numerical comparison with tolerance\n        return np.allclose(tree_a.value, tree_b.value, rtol=0, atol=1e-8)\n    \n    # If both are nodes, compare their features and thresholds\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Check if features are the same\n        if tree_a.feature != tree_b.feature:\n            return False\n        \n        # Check if thresholds are close enough\n        if not np.isclose(tree_a.threshold, tree_b.threshold, rtol=0, atol=1e-8):\n            return False\n        \n        # Recursively check left and right subtrees\n        return (compare_trees(tree_a.left, tree_b.left) and \n                compare_trees(tree_a.right, tree_b.right))\n    \n    # This should never happen if the input is valid\n    return False\n```\n\nThis solution recursively compares the two trees by:\n1. Checking if both trees are of the same type (both Node or both Leaf)\n2. If they're both leaves, comparing their values using numpy.allclose with the specified tolerance\n3. If they're both nodes, checking if they have the same feature index and if their thresholds are close enough\n4. Recursively comparing their left and right subtrees\n\nThe function returns True only if all these conditions are met throughout the entire tree structure."}
{"task_id": 292, "completion_id": 0, "completion": "I'll implement the single-point column crossover function for neural network weights as requested.\n\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                          parent2: list[list[int | float]],\n                          cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier manipulation\n    parent1_np = np.array(parent1)\n    parent2_np = np.array(parent2)\n    \n    # Validate that both parents have the same shape\n    if parent1_np.shape != parent2_np.shape:\n        return -1\n    \n    # Create copies to avoid modifying the originals\n    child1 = parent1_np.copy()\n    child2 = parent2_np.copy()\n    \n    # Perform the crossover: swap columns from cutoff (inclusive) to the end\n    child1[:, cutoff:] = parent2_np[:, cutoff:]\n    child2[:, cutoff:] = parent1_np[:, cutoff:]\n    \n    # Convert back to Python lists and return\n    return (child1.tolist(), child2.tolist())\n```\n\nThis implementation:\n1. Converts the input parent matrices to NumPy arrays\n2. Validates that both parents have the same shape, returning -1 if they don't\n3. Creates copies of the parents to avoid modifying them in-place\n4. Performs the column crossover at the specified cutoff point (inclusive)\n5. Converts the resulting NumPy arrays back to Python lists using tolist()\n6. Returns the two children as a tuple\n\nThe function handles all edge cases, including when cutoff is 0 (swap all columns) or equal to the number of columns (swap none)."}
{"task_id": 294, "completion_id": 0, "completion": "I'll solve this task by creating a function that converts the custom graph representation to an adjacency dictionary with the specified requirements.\n\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # Handle empty graph case\n    if not G.get('_V2I', {}):\n        return {}\n    \n    # Initialize the adjacency dictionary with empty lists for all vertices\n    adj = {vertex: [] for vertex in G['_V2I']}\n    \n    # Process all edges from the graph\n    for edges in G['_G']:\n        for source, target, weight in edges:\n            # Add the edge to the adjacency list\n            adj[source].append((target, weight))\n            \n            # If the graph is undirected and this is not a self-loop,\n            # add the reverse edge if it doesn't already exist\n            if not G['is_directed'] and source != target:\n                # For undirected graphs, we'll add all edges in both directions\n                # and then deduplicate later if needed\n                adj[target].append((source, weight))\n    \n    # Sort the adjacency lists by neighbor label\n    for vertex in adj:\n        # Remove duplicates for undirected graphs (keeping the first occurrence)\n        if not G['is_directed']:\n            # Create a dictionary to track seen neighbors and their weights\n            seen = {}\n            unique_edges = []\n            for neighbor, weight in adj[vertex]:\n                if neighbor not in seen:\n                    seen[neighbor] = weight\n                    unique_edges.append((neighbor, weight))\n                # If we've seen this neighbor before but with a different weight, \n                # we keep the original weight (as per the requirement to have exactly one edge in each direction)\n            adj[vertex] = unique_edges\n        \n        # Sort by neighbor label\n        adj[vertex].sort(key=lambda x: x[0])\n    \n    return adj\n```\n\nThis implementation:\n1. Creates an empty adjacency dictionary with all vertices from the graph\n2. Processes all edges from the custom graph format\n3. For undirected graphs, ensures each edge appears exactly once in each direction\n4. Sorts the neighbor lists alphabetically by neighbor label\n5. Handles the empty graph case by returning an empty dictionary\n\nThe function maintains all the required properties for the output adjacency dictionary."}
{"task_id": 296, "completion_id": 0, "completion": "I'll solve this task by creating a function that checks if all elements in a NumPy array are binary (0 or 1).\n\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    arr = np.asarray(x)\n    \n    # For boolean arrays, all values are valid (True maps to 1, False maps to 0)\n    if arr.dtype == bool:\n        return True\n    \n    # For numeric arrays, check if all elements are either 0 or 1\n    # Using isclose for float arrays to handle potential floating-point precision issues\n    zeros = np.isclose(arr, 0)\n    ones = np.isclose(arr, 1)\n    \n    # Return True if every element is either close to 0 or close to 1\n    return np.all(np.logical_or(zeros, ones))\n```\n\nThis solution:\n1. Converts the input to a NumPy array if it's not already\n2. Handles boolean arrays (which are always binary by definition)\n3. Uses `np.isclose()` to handle floating-point precision issues when comparing with 0 or 1\n4. Checks if every element is either close to 0 or close to 1\n5. Returns a boolean result as required"}
{"task_id": 298, "completion_id": 0, "completion": "I'll implement the N-gram log probability calculator as requested. Here's the complete solution:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    # Tokenize the query\n    query_tokens = query.split()\n    \n    # Check if query is too short for the requested N\n    if len(query_tokens) < N:\n        return float('-inf')\n    \n    # Pad the query tokens if N > 1\n    if N > 1:\n        query_tokens = [\"<bol>\"] * (N - 1) + query_tokens + [\"<eol>\"]\n    \n    # Process corpus to count n-grams and (n-1)-grams\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    \n    for sentence in corpus:\n        tokens = sentence.split()\n        \n        # Pad tokens if N > 1\n        if N > 1:\n            tokens = [\"<bol>\"] * (N - 1) + tokens + [\"<eol>\"]\n        \n        # Count n-grams and their prefixes\n        for i in range(len(tokens) - N + 1):\n            ngram = tuple(tokens[i:i+N])\n            ngram_counts[ngram] += 1\n            \n            if N > 1:\n                prefix = tuple(tokens[i:i+N-1])\n                prefix_counts[prefix] += 1\n    \n    # For unigrams, the denominator is the total number of tokens\n    if N == 1:\n        total_tokens = sum(ngram_counts.values())\n        \n        # Calculate log probability for the query\n        log_prob = 0\n        for i in range(len(query_tokens)):\n            token = (query_tokens[i],)  # Make it a tuple for consistency\n            if ngram_counts[token] == 0:\n                return float('-inf')\n            log_prob += math.log(ngram_counts[token] / total_tokens)\n    else:\n        # Calculate log probability for the query\n        log_prob = 0\n        for i in range(len(query_tokens) - N + 1):\n            ngram = tuple(query_tokens[i:i+N])\n            prefix = tuple(query_tokens[i:i+N-1])\n            \n            if ngram_counts[ngram] == 0 or prefix_counts[prefix] == 0:\n                return float('-inf')\n            \n            prob = ngram_counts[ngram] / prefix_counts[prefix]\n            log_prob += math.log(prob)\n    \n    # Round to 4 decimal places\n    return round(log_prob, 4)\n```\n\nThis implementation:\n1. Tokenizes the corpus and query sentences\n2. Pads tokens with `<bol>` and `<eol>` when N > 1\n3. Counts N-grams and (N-1)-gram prefixes in the corpus\n4. Handles unigrams (N=1) separately, using total token count as denominator\n5. Calculates the log probability of the query sentence\n6. Returns negative infinity if any required N-gram has zero probability\n7. Rounds the final result to 4 decimal places"}
{"task_id": 302, "completion_id": 0, "completion": "# Spectral Clustering Implementation\n\nI'll implement the spectral clustering algorithm from scratch using only NumPy for numerical operations. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Step 1: Build weighted adjacency matrix W\n    W = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if i != j:  # Diagonal must be 0\n                # Calculate Euclidean distance\n                dist = np.linalg.norm(X[i] - X[j])\n                # Set weight as 1/(1+dist)\n                W[i, j] = 1 / (1 + dist)\n    \n    # Step 2: Compute the unnormalized graph Laplacian L = D - W\n    # First, compute the diagonal degree matrix D\n    D = np.diag(np.sum(W, axis=1))\n    # Then compute the Laplacian\n    L = D - W\n    \n    # Step 3: Compute the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    # Sort eigenvalues and corresponding eigenvectors\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    # Take the first k eigenvectors (those with smallest eigenvalues)\n    E = eigenvectors[:, :k]\n    \n    # Step 4: Run k-means on the rows of E\n    # Initialize centroids with the first k rows of E\n    centroids = E[:k].copy()\n    \n    # Run k-means\n    labels = np.zeros(n_samples, dtype=int)\n    old_labels = np.ones(n_samples, dtype=int)  # Initialize with different values\n    \n    iteration = 0\n    max_iterations = 100\n    \n    while not np.array_equal(labels, old_labels) and iteration < max_iterations:\n        old_labels = labels.copy()\n        \n        # Assign points to nearest centroids\n        for i in range(n_samples):\n            # Calculate distances to all centroids\n            distances = np.linalg.norm(E[i] - centroids, axis=1)\n            # Assign to closest centroid\n            labels[i] = np.argmin(distances)\n        \n        # Update centroids\n        for j in range(k):\n            cluster_points = E[labels == j]\n            if len(cluster_points) > 0:\n                centroids[j] = np.mean(cluster_points, axis=0)\n            else:\n                # If a cluster becomes empty, reinitialize with a random point\n                random_idx = np.random.randint(0, n_samples)\n                centroids[j] = E[random_idx]\n        \n        iteration += 1\n    \n    # Step 5: Return the resulting cluster labels as a Python list\n    return labels.tolist()\n```\n\nThis implementation follows the steps outlined in the task:\n\n1. It builds the weighted adjacency matrix W with weights defined as 1/(1+\u2016x_i\u2212x_j\u2016_2) and zeros on the diagonal.\n2. It computes the unnormalized graph Laplacian L = D - W.\n3. It finds the k eigenvectors corresponding to the k smallest eigenvalues of L.\n4. It runs k-means on the rows of the eigenvector matrix E, using the first k rows as initial centroids.\n5. It returns the cluster labels as a Python list.\n\nThe implementation handles the case of empty clusters by reinitializing the centroid with a random data point, and it stops when either the assignments stop changing or after 100 iterations."}
{"task_id": 303, "completion_id": 0, "completion": "I'll create a function that formats the error message exactly as specified:\n\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix-1, 0)\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n    \n    debug_str = \"------------------------- DEBUG -------------------------\\n\"\n    debug_str += f\"Mine (prev) [{prev_label}]:\\n{prev_mine}\\n\\n\"\n    debug_str += f\"Theirs (prev) [{prev_label}]:\\n{golds[prev_label]}\\n\\n\"\n    debug_str += f\"Mine [{curr_label}]:\\n{curr_mine}\\n\\n\"\n    debug_str += f\"Theirs [{curr_label}]:\\n{golds[curr_label]}{warn_str}\\n\"\n    debug_str += \"----------------------- END DEBUG -----------------------\"\n    \n    return debug_str\n```\n\nThis function follows the exact format requirements:\n1. It uses 25 dashes on both sides of \"DEBUG\"\n2. It uses 23 dashes on both sides of \"END DEBUG\"\n3. It maintains the exact spacing and line breaks as specified\n4. It appends the `warn_str` exactly as provided\n5. It handles the previous element correctly, using the element at index `max(ix-1, 0)`"}
{"task_id": 304, "completion_id": 0, "completion": "I'll implement the Bayesian posterior mean function for linear regression according to the specifications.\n\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Make a copy of X to avoid modifying the input\n    X_design = X.copy()\n    \n    # Add intercept column if required\n    if fit_intercept:\n        X_design = np.column_stack((np.ones(X.shape[0]), X_design))\n    \n    # Get the number of features (including intercept if fit_intercept=True)\n    n_features = X_design.shape[1]\n    \n    # Process prior mean (mu)\n    if np.isscalar(mu):\n        mu = np.full(n_features, mu)\n    else:\n        mu = np.asarray(mu)\n        if mu.shape[0] != n_features:\n            raise ValueError(f\"Prior mean dimension {mu.shape[0]} does not match feature dimension {n_features}\")\n    \n    # Process prior covariance scale (V)\n    if V is None:\n        # Identity matrix\n        V_inv = np.eye(n_features)\n    elif np.isscalar(V):\n        # Scalar times identity\n        V_inv = np.eye(n_features) / V\n    elif isinstance(V, (list, tuple, np.ndarray)) and np.asarray(V).ndim == 1:\n        # Diagonal matrix specified as a vector\n        V_inv = np.diag(1.0 / np.asarray(V))\n    else:\n        # Full matrix\n        V_inv = np.linalg.inv(V)\n    \n    # Compute X^T X\n    XTX = X_design.T @ X_design\n    \n    # Compute X^T y\n    XTy = X_design.T @ y\n    \n    # Compute posterior covariance (up to \u03c3\u00b2): \u03a3_b = (V^(-1) + X^T X)^(-1)\n    Sigma_b = np.linalg.inv(V_inv + XTX)\n    \n    # Compute posterior mean: \u03bc_b = \u03a3_b (V^(-1) \u03bc + X^T y)\n    mu_b = Sigma_b @ (V_inv @ mu + XTy)\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(coef), 4) for coef in mu_b]\n```\n\nThis implementation:\n1. Handles the optional intercept addition\n2. Processes different forms of prior mean (scalar or vector)\n3. Processes different forms of prior covariance scale (None, scalar, diagonal, or full matrix)\n4. Computes the posterior mean according to the formula\n5. Rounds the results to 4 decimal places as required\n\nThe function follows the Bayesian linear regression formulation with a normal-inverse-gamma prior as specified in the task description."}
{"task_id": 308, "completion_id": 0, "completion": "I'll implement the Mel filterbank matrix construction function according to the specifications. Let me break down the steps:\n\n1. Convert frequency limits to Mel scale\n2. Generate equally-spaced points on the Mel scale\n3. Convert those points back to Hertz\n4. For each filter and DFT bin, compute the triangular filter weights\n5. Normalize if required\n6. Return the result as a Python list with values rounded to 4 decimal places\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    # Set default max_freq to Nyquist frequency if not provided\n    if max_freq is None:\n        max_freq = fs // 2\n    \n    # Convert min and max frequencies to Mel scale\n    def hz_to_mel(f):\n        return 2595 * np.log10(1 + f / 700)\n    \n    def mel_to_hz(mel):\n        return 700 * (10**(mel / 2595) - 1)\n    \n    min_mel = hz_to_mel(min_freq)\n    max_mel = hz_to_mel(max_freq)\n    \n    # Generate n_filters+2 equally-spaced points on the Mel scale\n    mel_points = np.linspace(min_mel, max_mel, n_filters + 2)\n    \n    # Convert Mel points back to Hertz\n    hz_points = mel_to_hz(mel_points)\n    \n    # Calculate the DFT bin frequencies\n    bin_freqs = np.linspace(0, fs // 2, N // 2 + 1)\n    \n    # Initialize the filterbank matrix\n    filterbank = np.zeros((n_filters, N // 2 + 1))\n    \n    # For each filter\n    for i in range(n_filters):\n        # Get the three corner frequencies for this filter\n        f_left = hz_points[i]\n        f_center = hz_points[i + 1]\n        f_right = hz_points[i + 2]\n        \n        # For each DFT bin\n        for k in range(N // 2 + 1):\n            f = bin_freqs[k]\n            \n            # Calculate left and right slopes\n            left_slope = (f - f_left) / (f_center - f_left) if f_center != f_left else 0\n            right_slope = (f_right - f) / (f_right - f_center) if f_right != f_center else 0\n            \n            # Take the positive minimum of both slopes\n            weight = max(0, min(left_slope, right_slope))\n            filterbank[i, k] = weight\n    \n    # Normalize if required\n    if normalize:\n        for i in range(n_filters):\n            # Scale by 2/(f_{i+2} - f_i)\n            scale_factor = 2 / (hz_points[i + 2] - hz_points[i])\n            filterbank[i, :] *= scale_factor\n    \n    # Convert to Python list and round to 4 decimal places\n    result = []\n    for row in filterbank:\n        result.append([round(float(val), 4) for val in row])\n    \n    return result\n```\n\nThis implementation follows the specified algorithm exactly:\n1. It converts frequency limits to the Mel scale\n2. Generates equally-spaced points on the Mel scale\n3. Converts those points back to Hertz to get the corner frequencies\n4. For each filter and DFT bin, computes the triangular filter weights\n5. Normalizes the filters if required\n6. Returns the result as a Python list with values rounded to 4 decimal places\n\nThe function should produce results consistent with libraries like LibROSA."}
{"task_id": 312, "completion_id": 0, "completion": "I'll implement the Blackman-Harris window generator function according to the specifications.\n\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # Handle special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    # Blackman-Harris coefficients\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    # For periodic window, we need to generate window_len+1 points and discard the last one\n    if not symmetric:\n        N = window_len\n        n = np.arange(window_len)\n    else:\n        # For symmetric window, N = L-1\n        N = window_len - 1\n        n = np.arange(window_len)\n    \n    # Calculate the window values\n    window = (a0 \n              - a1 * np.cos(2 * np.pi * n / N) \n              + a2 * np.cos(4 * np.pi * n / N) \n              - a3 * np.cos(6 * np.pi * n / N))\n    \n    # Round to 4 decimal places and convert to list\n    return [round(x, 4) for x in window]\n```\n\nThis implementation handles both the symmetric and periodic variants of the Blackman-Harris window. For the periodic form, it uses N = window_len, while for the symmetric form it uses N = window_len - 1 as specified in the formula. The function also properly handles the special cases where window_len is less than or equal to 0 (returning an empty list) and where window_len is 1 (returning [1.0])."}
{"task_id": 313, "completion_id": 0, "completion": "I'll implement the expected logarithm of Dirichlet components function as requested, including a custom implementation of the digamma function.\n\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    # =====  Write your code below this line  =====\n    \n    # Convert gamma to numpy array if it's a list\n    if isinstance(gamma, list):\n        gamma = np.array(gamma)\n    \n    # Compute the digamma values needed for the formula\n    gamma_dt = gamma[d, t]\n    gamma_d_sum = np.sum(gamma[d, :])\n    \n    # Calculate the expectation using the formula\n    expectation = digamma(gamma_dt) - digamma(gamma_d_sum)\n    \n    # Round to 4 decimal places\n    return round(expectation, 4)\n\ndef digamma(x):\n    \"\"\"\n    Compute the digamma function \u03c8(x) for x > 0.\n    \n    Uses recursion to handle small values and asymptotic expansion for large values.\n    \"\"\"\n    # Threshold for switching to asymptotic expansion\n    threshold = 6.0\n    \n    # For small values, use the recursion formula\n    result = 0.0\n    while x < threshold:\n        result -= 1.0 / x\n        x += 1.0\n    \n    # For large values, use the asymptotic expansion\n    # \u03c8(x) \u2248 ln(x) - 1/(2x) - 1/(12x\u00b2) + 1/(120x\u2074) - 1/(252x\u2076)\n    x_squared = x * x\n    x_fourth = x_squared * x_squared\n    x_sixth = x_fourth * x_squared\n    \n    result += math.log(x) - 1.0 / (2.0 * x) - 1.0 / (12.0 * x_squared) + 1.0 / (120.0 * x_fourth) - 1.0 / (252.0 * x_sixth)\n    \n    return result\n```\n\nThis implementation:\n1. Converts the input to a NumPy array if it's a list\n2. Implements the digamma function using the suggested approach:\n   - Recursively shifts small values up to a threshold (6.0)\n   - Uses the asymptotic expansion for larger values\n3. Computes the expectation using the formula: \u03c8(gamma[d,t]) - \u03c8(sum(gamma[d,:]))\n4. Rounds the result to 4 decimal places as required\n\nThe function handles both list and NumPy array inputs and returns a single float value rounded to 4 decimal places."}
{"task_id": 317, "completion_id": 0, "completion": "I'll implement the `err_fmt` function according to the specifications. Here's the complete solution:\n\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # Get the previous index (max of 0 and ix-1)\n    prev_ix = max(0, ix - 1)\n    \n    # Get the previous and current predictions and labels\n    prev_prediction, prev_label = params[prev_ix]\n    curr_prediction, curr_label = params[ix]\n    \n    # Get the corresponding gold/expected texts\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    # Construct the formatted debug string\n    debug_str = \"------------------------- DEBUG -------------------------\\n\"\n    debug_str += f\"Mine (prev) [{prev_label}]:\\n\"\n    debug_str += f\"{prev_prediction}\\n\\n\"\n    debug_str += f\"Theirs (prev) [{prev_label}]:\\n\"\n    debug_str += f\"{prev_gold}\\n\\n\\n\"\n    debug_str += f\"Mine [{curr_label}]:\\n\"\n    debug_str += f\"{curr_prediction}\\n\\n\"\n    debug_str += f\"Theirs [{curr_label}]:\\n\"\n    debug_str += f\"{curr_gold}{warn_str}\\n\"\n    debug_str += \"----------------------- END DEBUG -----------------------\"\n    \n    return debug_str\n```\n\nThis implementation follows the exact format specified in the task description:\n1. It creates the header with 25 dashes on each side of \"DEBUG\"\n2. It shows the previous prediction and gold text (using index 0 if ix is already 0)\n3. It adds the proper spacing between sections\n4. It shows the current prediction and gold text\n5. It appends the optional warning string\n6. It adds the footer with 23 dashes on each side of \"END DEBUG\"\n\nThe function handles all the formatting requirements precisely, including all newlines and dashes as specified."}
{"task_id": 318, "completion_id": 0, "completion": "I'll implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak learners. Here's the complete solution:\n\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    \n    # Initialize sample weights uniformly\n    weights = np.ones(n_samples) / n_samples\n    \n    # List to store the weak learners and their weights\n    weak_learners = []\n    alphas = []\n    \n    for m in range(n_estimators):\n        # Train a decision stump\n        stump, error = train_decision_stump(X_train, y_train, weights)\n        \n        # If the error is 0 or 0.5, we need to handle it specially\n        if error == 0:\n            alpha = 50  # A large value\n        elif error >= 0.5:\n            # If error is 0.5 or greater, we'll skip this iteration\n            continue\n        else:\n            # Calculate the weight of this classifier\n            alpha = 0.5 * math.log((1 - error) / error)\n        \n        # Store the weak learner and its weight\n        weak_learners.append(stump)\n        alphas.append(alpha)\n        \n        # Update sample weights\n        predictions = predict_with_stump(X_train, stump)\n        weights = update_weights(weights, alpha, y_train, predictions)\n        \n        # Normalize weights\n        weights = weights / np.sum(weights)\n    \n    # Make predictions on the test set\n    test_predictions = predict_ensemble(X_test, weak_learners, alphas)\n    \n    # Convert to list of integers\n    return [int(pred) for pred in test_predictions]\n\ndef train_decision_stump(X, y, weights):\n    \"\"\"Train a decision stump on the weighted data.\n    \n    Returns:\n        stump: A tuple (feature_idx, threshold, polarity)\n        error: The weighted error rate\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_error = float('inf')\n    best_stump = None\n    \n    for feature_idx in range(n_features):\n        # Get all unique values for this feature\n        feature_values = X[:, feature_idx]\n        thresholds = np.unique(feature_values)\n        \n        # If there are too many unique values, sample some thresholds\n        if len(thresholds) > 100:\n            thresholds = np.percentile(feature_values, np.linspace(0, 100, 100))\n        \n        for threshold in thresholds:\n            # Try both polarities\n            for polarity in [-1, 1]:\n                # Predict with this stump\n                predictions = np.ones(n_samples)\n                predictions[polarity * (X[:, feature_idx] - threshold) < 0] = -1\n                \n                # Calculate weighted error\n                misclassified = predictions != y\n                error = np.sum(weights[misclassified])\n                \n                # Update if this is the best stump so far\n                if error < best_error:\n                    best_error = error\n                    best_stump = (feature_idx, threshold, polarity)\n    \n    return best_stump, best_error\n\ndef predict_with_stump(X, stump):\n    \"\"\"Make predictions using a decision stump.\"\"\"\n    feature_idx, threshold, polarity = stump\n    predictions = np.ones(X.shape[0])\n    predictions[polarity * (X[:, feature_idx] - threshold) < 0] = -1\n    return predictions\n\ndef update_weights(weights, alpha, y_true, y_pred):\n    \"\"\"Update sample weights based on prediction errors.\"\"\"\n    # Calculate the exponent term\n    exponent = -alpha * y_true * y_pred\n    # Update weights\n    return weights * np.exp(exponent)\n\ndef predict_ensemble(X, weak_learners, alphas):\n    \"\"\"Make predictions using the ensemble of weak learners.\"\"\"\n    n_samples = X.shape[0]\n    ensemble_predictions = np.zeros(n_samples)\n    \n    for stump, alpha in zip(weak_learners, alphas):\n        predictions = predict_with_stump(X, stump)\n        ensemble_predictions += alpha * predictions\n    \n    # Return the sign of the ensemble predictions\n    return np.sign(ensemble_predictions)\n```\n\nThis implementation follows the AdaBoost.M1 algorithm as specified:\n\n1. It initializes sample weights uniformly\n2. For each boosting round:\n   - Trains a decision stump that minimizes the weighted error\n   - Calculates the weight (alpha) for this weak learner\n   - Updates the sample weights based on the errors\n   - Normalizes the weights\n3. Makes predictions using the weighted ensemble of weak learners\n4. Returns the predictions as a list of -1 and 1 integers\n\nThe decision stump implementation finds the best feature, threshold, and polarity to split the data based on the weighted error. The final ensemble prediction is the sign of the weighted sum of all weak learner predictions."}
{"task_id": 329, "completion_id": 0, "completion": "I'll solve this task by implementing the `dft_bins` function to calculate the center frequencies of DFT bins according to the specifications.\n\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # Check if N or fs is non-positive\n    if N <= 0 or fs <= 0:\n        return []\n    \n    if positive_only:\n        # For positive_only=True, return frequencies from 0 to \u230aN/2\u230b\n        # This gives us the non-negative part of the spectrum\n        num_bins = N // 2 + 1\n        frequencies = np.arange(num_bins) * (fs / N)\n    else:\n        # For positive_only=False, use NumPy's fftfreq which arranges frequencies as:\n        # [0, 1\u00b7\u0394f, \u2026, (\u2308N/2\u2309\u22121)\u00b7\u0394f, \u2212\u230aN/2\u230b\u00b7\u0394f, \u2026, \u2212\u0394f]\n        frequencies = np.fft.fftfreq(N, 1/fs)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(frequencies, 4).tolist()\n```\n\nThis implementation:\n1. Checks if either `N` or `fs` is non-positive, returning an empty list in that case\n2. For `positive_only=True`, calculates only the non-negative frequencies (0 to \u230aN/2\u230b)\n3. For `positive_only=False`, uses NumPy's `fftfreq` function to get the full spectrum with the correct arrangement\n4. Rounds all frequencies to 4 decimal places and returns them as a Python list\n\nThe function handles both even and odd values of `N` correctly, following the formula f\u2096 = k \u00b7 (f\u209b / N)."}
{"task_id": 331, "completion_id": 0, "completion": "# Out-of-Bag MSE for Random Forest Regression\n\nI'll implement a function to compute the out-of-bag mean squared error for a Random Forest regressor. This is a useful metric for evaluating the model's performance without needing a separate validation set.\n\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask, dtype=bool)\n    \n    # Check if any sample has OOB predictions\n    has_oob_predictions = np.any(oob_mask, axis=1)\n    \n    if not np.any(has_oob_predictions):\n        return -1.0\n    \n    # Initialize array to store OOB predictions for each sample\n    oob_predictions = np.zeros(len(y_true))\n    \n    # For each sample, compute the average of its OOB predictions\n    for i in range(len(y_true)):\n        if has_oob_predictions[i]:\n            # Get the OOB predictions for this sample\n            sample_oob_preds = predictions[i, oob_mask[i]]\n            # Compute the average OOB prediction\n            oob_predictions[i] = np.mean(sample_oob_preds)\n    \n    # Compute squared errors only for samples with OOB predictions\n    squared_errors = (y_true[has_oob_predictions] - oob_predictions[has_oob_predictions]) ** 2\n    \n    # Compute mean squared error\n    mse = np.mean(squared_errors)\n    \n    # Round to 4 decimal places\n    return round(float(mse), 4)\n```\n\nThis implementation:\n1. Converts inputs to numpy arrays for efficient processing\n2. Checks if any samples have OOB predictions\n3. For each sample with OOB predictions, computes the average of those predictions\n4. Calculates the squared error between the average OOB prediction and true value\n5. Returns the mean of those squared errors, rounded to 4 decimal places"}
{"task_id": 332, "completion_id": 0, "completion": "I'll implement the FP-Tree construction algorithm using only nested dictionaries as requested. Here's the complete solution:\n\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Count the frequency of each item\n    item_counter = Counter()\n    for transaction in transactions:\n        # Count each unique item in the transaction\n        item_counter.update(set(transaction))\n    \n    # Step 2: Filter items by minimum support\n    frequent_items = {item: count for item, count in item_counter.items() if count >= min_support}\n    \n    # Step 3: Create global ordering of items\n    # First by decreasing support, then alphabetically to break ties\n    item_order = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    \n    # Create a mapping from item to its position in the global order\n    item_position = {item: i for i, item in enumerate(item_order)}\n    \n    # Initialize the root node of the FP-Tree\n    root = {\"support\": 0, \"children\": {}}\n    \n    # Step 4: Process each transaction and build the tree\n    for transaction in transactions:\n        # Filter out infrequent items and remove duplicates\n        filtered_transaction = list(set(item for item in transaction if item in frequent_items))\n        \n        # Sort items according to the global ordering\n        filtered_transaction.sort(key=lambda x: item_position[x])\n        \n        # Skip empty transactions\n        if not filtered_transaction:\n            continue\n        \n        # Insert the transaction into the tree\n        current_node = root\n        for item in filtered_transaction:\n            # Increase the support of the current node\n            current_node[\"support\"] += 1\n            \n            # If the item is not a child of the current node, add it\n            if item not in current_node[\"children\"]:\n                current_node[\"children\"][item] = {\"support\": 0, \"children\": {}}\n            \n            # Move to the child node\n            current_node = current_node[\"children\"][item]\n        \n        # Increase the support of the last node\n        current_node[\"support\"] += 1\n    \n    return root\n```\n\nThis implementation follows the specified building rules:\n1. It computes the support for each item across all transactions\n2. It discards items with support less than the minimum threshold\n3. It creates a global ordering of items by decreasing support and then alphabetically\n4. For each transaction, it:\n   - Removes duplicates and keeps only frequent items\n   - Reorders items according to the global ordering\n   - Walks the tree from the root, updating/extending paths and increasing support counts\n\nThe resulting tree is represented as a nested dictionary with the specified structure."}
{"task_id": 336, "completion_id": 0, "completion": "I'll implement the linear kernel function according to the requirements. Here's the solution:\n\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # Convert inputs to numpy arrays for consistent handling\n    X_array = np.array(X, dtype=float)\n    \n    # Handle empty X case\n    if X_array.size == 0:\n        return []\n    \n    # Set Y = X if Y is None\n    if Y is None:\n        Y_array = X_array\n    else:\n        Y_array = np.array(Y, dtype=float)\n    \n    # Check if X and Y have the same number of columns (features)\n    if X_array.ndim < 2 or Y_array.ndim < 2 or X_array.shape[1] != Y_array.shape[1]:\n        return -1\n    \n    # Compute the kernel matrix\n    # For each pair of vectors (x_i, y_j), compute x_i \u00b7 y_j + c0\n    kernel_matrix = np.dot(X_array, Y_array.T) + c0\n    \n    # Round to 4 decimal places and convert to list of lists\n    rounded_matrix = [[round(val, 4) for val in row] for row in kernel_matrix.tolist()]\n    \n    return rounded_matrix\n```\n\nThis implementation:\n1. Converts both X and Y to NumPy arrays for consistent handling\n2. Handles the case where X is empty\n3. Sets Y = X when Y is None\n4. Checks if X and Y have compatible dimensions\n5. Computes the kernel matrix using matrix multiplication\n6. Rounds all values to 4 decimal places\n7. Returns the result as a list of lists\n\nThe function will work with both Python lists and NumPy arrays as input, and will handle all the validation rules correctly."}
{"task_id": 340, "completion_id": 0, "completion": "I'll implement the Adam optimization step according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n    \n    # Update biased first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_t / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v_t / (1 - beta2 ** t)\n    \n    # Update parameters\n    updated_param = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n    \n    # Round to 8 decimal places and convert to Python lists\n    updated_param_list = np.round(updated_param, 8).tolist()\n    m_t_list = np.round(m_t, 8).tolist()\n    v_t_list = np.round(v_t, 8).tolist()\n    \n    return (updated_param_list, m_t_list, v_t_list)\n```\n\nThis implementation follows the Adam update rules exactly as specified:\n1. It updates the first and second moment estimates\n2. It applies bias correction\n3. It updates the parameter using the corrected moments\n4. It works with multi-dimensional parameters through NumPy's vectorized operations\n5. It handles zero gradients correctly (the parameter stays unchanged when gradient is zero)\n6. It returns the results as Python lists rounded to 8 decimal places"}
{"task_id": 343, "completion_id": 0, "completion": "I'll implement the cross-entropy update function according to the requirements. This function will select the top-performing parameter vectors based on rewards, compute their mean and variance, and return these statistics rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to numpy array if it's not already\n    rewards = np.array(rewards)\n    \n    # Calculate how many samples to retain\n    n_retain = int(retain_prcnt * len(rewards))\n    \n    # Get indices of top n_retain rewards\n    elite_indices = np.argsort(rewards)[-n_retain:]\n    \n    # Select the elite samples\n    elite_samples = theta_samples[elite_indices]\n    \n    # Calculate mean and variance along each dimension\n    means = np.mean(elite_samples, axis=0)\n    variances = np.var(elite_samples, axis=0)\n    \n    # Round to 4 decimal places and convert to Python lists\n    means_rounded = [round(float(m), 4) for m in means]\n    variances_rounded = [round(float(v), 4) for v in variances]\n    \n    return means_rounded, variances_rounded\n```\n\nThis implementation:\n1. Determines how many samples to retain based on the `retain_prcnt`\n2. Sorts the rewards and selects the indices of the top-performing samples\n3. Extracts the elite samples using these indices\n4. Computes the mean and variance of the elite samples along each dimension\n5. Rounds the results to four decimal places and converts them to Python lists\n6. Returns the mean and variance as a tuple of lists"}
{"task_id": 353, "completion_id": 0, "completion": "I'll implement the Shannon entropy function according to the requirements. Here's the solution:\n\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Handle empty input\n    if len(labels) == 0:\n        return 0.0\n    \n    # Count occurrences of each label\n    counts = Counter(labels)\n    \n    # Calculate total number of labels\n    total = sum(counts.values())\n    \n    # Calculate entropy\n    entropy_value = 0.0\n    for count in counts.values():\n        probability = count / total\n        # By convention, 0 * log(0) = 0\n        if probability > 0:\n            entropy_value -= probability * math.log2(probability)\n    \n    # Round to 5 decimal places\n    return round(entropy_value, 5)\n```\n\nThis implementation:\n1. Handles the empty input case by returning 0.0\n2. Uses `Counter` to efficiently count the occurrences of each label\n3. Calculates the probability of each label\n4. Computes the entropy using the formula H = -\u2211(p_i * log2(p_i))\n5. Rounds the result to 5 decimal places as required\n\nThe function works with any iterable of hashable labels, including Python lists and NumPy arrays."}
{"task_id": 354, "completion_id": 0, "completion": "I'll implement the 2D convolution using the im2col + GEMM strategy as requested. This approach reshapes the input data to enable efficient matrix multiplication, which is much faster than naive convolution implementations.\n\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    # Extract dimensions\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kernel_rows, kernel_cols, _, out_ch = W.shape\n    \n    # Adjust dilation (0 means normal convolution, which is dilation=1)\n    dilation = dilation + 1\n    \n    # Calculate effective kernel dimensions with dilation\n    eff_kernel_rows = kernel_rows + (kernel_rows - 1) * (dilation - 1)\n    eff_kernel_cols = kernel_cols + (kernel_cols - 1) * (dilation - 1)\n    \n    # Handle padding\n    if pad == 'same':\n        # Calculate padding needed for 'same' output size\n        pad_rows = max(0, (eff_kernel_rows - stride + (in_rows - 1) % stride) // 2)\n        pad_cols = max(0, (eff_kernel_cols - stride + (in_cols - 1) % stride) // 2)\n        pad = (pad_rows, pad_rows, pad_cols, pad_cols)\n    \n    # Convert pad to 4-tuple format (top, bottom, left, right)\n    if isinstance(pad, int):\n        pad = (pad, pad, pad, pad)\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        pad = (pad[0], pad[0], pad[1], pad[1])\n    \n    # Apply padding to input\n    X_padded = np.pad(X, ((0, 0), (pad[0], pad[1]), (pad[2], pad[3]), (0, 0)), mode='constant')\n    \n    # Calculate output dimensions\n    out_rows = (X_padded.shape[1] - eff_kernel_rows) // stride + 1\n    out_cols = (X_padded.shape[2] - eff_kernel_cols) // stride + 1\n    \n    # Initialize output array\n    output = np.zeros((n_ex, out_rows, out_cols, out_ch))\n    \n    # Reshape kernel for matrix multiplication\n    # Original shape: (kernel_rows, kernel_cols, in_ch, out_ch)\n    # Target shape: (kernel_rows * kernel_cols * in_ch, out_ch)\n    W_reshaped = W.reshape(-1, out_ch)\n    \n    # Implement im2col\n    for i in range(n_ex):\n        # Extract columns for this example\n        cols = np.zeros((kernel_rows * kernel_cols * in_ch, out_rows * out_cols))\n        col_idx = 0\n        \n        for r in range(0, out_rows):\n            for c in range(0, out_cols):\n                # Extract the patch\n                r_start = r * stride\n                c_start = c * stride\n                \n                # Handle dilation by creating a dilated patch\n                if dilation > 1:\n                    patch = np.zeros((kernel_rows, kernel_cols, in_ch))\n                    for kr in range(kernel_rows):\n                        for kc in range(kernel_cols):\n                            r_pos = r_start + kr * dilation\n                            c_pos = c_start + kc * dilation\n                            patch[kr, kc, :] = X_padded[i, r_pos, c_pos, :]\n                else:\n                    # For normal convolution, extract the patch directly\n                    patch = X_padded[i, \n                                    r_start:r_start + eff_kernel_rows:dilation, \n                                    c_start:c_start + eff_kernel_cols:dilation, \n                                    :]\n                \n                # Flatten the patch and add it as a column\n                cols[:, col_idx] = patch.reshape(-1)\n                col_idx += 1\n        \n        # Perform matrix multiplication\n        result = W_reshaped.T @ cols\n        \n        # Reshape the result back to spatial dimensions\n        output[i] = result.reshape(out_ch, out_rows, out_cols).transpose(1, 2, 0)\n    \n    # Return as Python list\n    return output.tolist()\n```\n\nThis implementation:\n\n1. Handles all the required padding specifications\n2. Supports dilation\n3. Uses the im2col + GEMM strategy for efficient computation\n4. Returns the result as a Python list via tolist()\n\nThe core of the algorithm is reshaping the input patches into columns (im2col), then performing matrix multiplication with the reshaped kernels, which is much more efficient than direct convolution, especially for larger inputs."}
{"task_id": 355, "completion_id": 0, "completion": "I'll implement the PCA function with deterministic sign fixing as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # 1. Mean centering\n    data_centered = data - np.mean(data, axis=0)\n    \n    # 2. Compute principal directions based on the chosen solver\n    if solver == \"svd\":\n        # SVD on centered data\n        U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)\n        # Principal directions are in Vt.T (or V)\n        directions = Vt.T\n        # Variance explained by each component\n        explained_variance = (S ** 2) / (data.shape[0] - 1)\n    elif solver == \"eigen\":\n        # Compute covariance matrix\n        cov_matrix = np.cov(data_centered, rowvar=False, ddof=1)\n        # Eigen-decomposition of covariance matrix\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        # Sort in descending order\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        # Principal directions are eigenvectors\n        directions = eigenvectors\n        # Variance explained by each component\n        explained_variance = eigenvalues\n    else:\n        raise ValueError(\"Solver must be either 'svd' or 'eigen'\")\n    \n    # 3. Limit to n_components (or all available if n_components is too large)\n    max_components = min(n_components, directions.shape[1])\n    directions = directions[:, :max_components]\n    explained_variance = explained_variance[:max_components]\n    \n    # 4. Fix sign ambiguity\n    for i in range(directions.shape[1]):\n        # Find the first non-zero element in the direction\n        non_zero_idx = np.nonzero(directions[:, i])[0]\n        if len(non_zero_idx) > 0 and directions[non_zero_idx[0], i] < 0:\n            # If the first non-zero loading is negative, flip the sign\n            directions[:, i] = -directions[:, i]\n    \n    # 5. Project data onto principal directions\n    scores = data_centered @ directions\n    \n    # 6. Calculate explained variance ratio\n    total_variance = np.sum(np.var(data_centered, axis=0, ddof=1))\n    explained_variance_ratio = explained_variance / total_variance\n    \n    # Round to 4 decimals and convert to Python built-ins\n    scores_rounded = np.round(scores, 4).tolist()\n    explained_variance_ratio_rounded = np.round(explained_variance_ratio, 4).tolist()\n    \n    return scores_rounded, explained_variance_ratio_rounded\n```\n\nThis implementation follows all the requirements:\n1. It standardizes the data by mean centering\n2. It implements both SVD and eigen-decomposition solvers\n3. It sorts the directions by explained variance\n4. It fixes the sign ambiguity by ensuring the first non-zero loading is positive\n5. It projects the data onto the principal directions\n6. It returns the scores and explained variance ratios as requested, rounded to 4 decimals\n\nThe code handles the case where n_components is larger than the original feature dimension by keeping all available components."}
{"task_id": 356, "completion_id": 0, "completion": "I'll implement the `leaf_predict` function to extract the correct prediction from a `Leaf` instance based on whether it's a classification or regression tree.\n\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # For classification, return the index of the highest probability\n        # numpy.argmax behavior: returns the index of the first occurrence of the maximum\n        return max(range(len(leaf.value)), key=lambda i: leaf.value[i])\n    else:\n        # For regression, return the scalar value directly\n        return leaf.value\n```\n\nThis implementation handles both cases:\n1. For classification (`classifier=True`), it finds the index of the maximum probability in the leaf's value array, resolving ties in favor of the smaller index.\n2. For regression (`classifier=False`), it simply returns the scalar value stored in the leaf."}
{"task_id": 357, "completion_id": 0, "completion": "I'll implement the K-means clustering algorithm from scratch according to the specifications. Here's the solution:\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Handle the special case of K=1\n    if K == 1:\n        centroid = np.mean(data, axis=0)\n        return [[round(float(coord), 4) for coord in centroid]]\n    \n    # Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n    \n    # Main K-means loop\n    for _ in range(max_iters):\n        # Initialize cluster assignments and a flag to check for convergence\n        clusters = [[] for _ in range(K)]\n        old_centroids = centroids.copy()\n        \n        # Assign each sample to the closest centroid\n        for i in range(n_samples):\n            sample = data[i]\n            # Calculate distances to all centroids\n            distances = np.sqrt(np.sum((centroids - sample) ** 2, axis=1))\n            # Find the closest centroid\n            closest_centroid = np.argmin(distances)\n            # Add this sample to the corresponding cluster\n            clusters[closest_centroid].append(i)\n        \n        # Update centroids\n        for i in range(K):\n            if clusters[i]:  # If the cluster is not empty\n                centroids[i] = np.mean(data[clusters[i]], axis=0)\n            # If a cluster is empty, keep its centroid unchanged\n        \n        # Check for convergence (if centroids don't move significantly)\n        if np.allclose(old_centroids, centroids, rtol=0, atol=1e-6):\n            break\n    \n    # Sort centroids lexicographically\n    centroids = sorted(centroids, key=lambda x: tuple(x))\n    \n    # Round to 4 decimal places and convert to list of lists\n    return [[round(float(coord), 4) for coord in centroid] for centroid in centroids]\n```\n\nThis implementation follows the specified algorithm:\n1. It initializes centroids with the first K samples\n2. It iteratively assigns samples to the nearest centroid and updates centroids\n3. It stops early if centroids don't change significantly\n4. It handles the special case of K=1\n5. It sorts the final centroids lexicographically\n6. It rounds all coordinates to 4 decimal places\n7. It returns the result as a list of lists\n\nThe algorithm works for any dimensionality \u2265 1 as specified in the requirements."}
{"task_id": 362, "completion_id": 0, "completion": "I'll solve this task by implementing the row-wise stochastic matrix normalization function according to the requirements.\n\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    matrix = np.array(data, dtype=float)\n    \n    # Compute the sum of each row\n    row_sums = np.sum(matrix, axis=1)\n    \n    # Check if any row sums to 0\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row by dividing by its sum\n    # Using broadcasting to divide each row by its sum\n    normalized_matrix = matrix / row_sums[:, np.newaxis]\n    \n    # Round to 4 decimal places\n    rounded_matrix = np.round(normalized_matrix, 4)\n    \n    # Convert back to a Python list\n    return rounded_matrix.tolist()\n```\n\nThis implementation:\n1. Converts the input to a NumPy array with float data type\n2. Calculates the sum of each row using `np.sum` with `axis=1`\n3. Checks if any row sums to zero using `np.any`\n4. If no zero sums, normalizes each row by dividing by its sum using broadcasting\n5. Rounds all values to 4 decimal places\n6. Returns the result as a Python list using `tolist()`\n\nThe function handles both list of lists and NumPy arrays as input, and returns either a normalized matrix as a list of lists or -1 if normalization is impossible."}
{"task_id": 363, "completion_id": 0, "completion": "I'll implement the `is_continuous` function to determine whether the action and observation spaces of an environment are continuous based on the given criteria.\n\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def check_space_continuity(space, is_composite):\n        if not is_composite:\n            # If not composite, simply check if it's a Box\n            return isinstance(space, Box)\n        \n        # For composite spaces (Tuple or Dict)\n        if isinstance(space, Tuple):\n            # All sub-spaces must be Box for the space to be continuous\n            return all(isinstance(subspace, Box) for subspace in space.spaces)\n        elif isinstance(space, Dict):\n            # All values in the dict must be Box for the space to be continuous\n            return all(isinstance(subspace, Box) for subspace in space.spaces.values())\n        else:\n            # If marked as composite but isn't actually Tuple or Dict, \n            # we'll consider it non-continuous\n            return False\n    \n    # Check continuity for action and observation spaces\n    cont_action = check_space_continuity(env.action_space, tuple_action)\n    cont_obs = check_space_continuity(env.observation_space, tuple_obs)\n    \n    return (cont_action, cont_obs)\n```\n\nThis implementation:\n1. Defines a helper function `check_space_continuity` that determines if a space is continuous\n2. For non-composite spaces, it simply checks if the space is an instance of `Box`\n3. For composite spaces (when the corresponding flag is `True`), it checks if all sub-spaces are instances of `Box`\n4. Returns a tuple of two booleans indicating whether the action and observation spaces are continuous"}
{"task_id": 369, "completion_id": 0, "completion": "# Gradient Boosting Prediction Aggregation Solution\n\nI'll implement the function to aggregate outputs from gradient boosting trees according to the specifications.\n\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree's\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    # Initialize running prediction with zeros matching the shape of a single update\n    running_pred = np.zeros_like(updates[0])\n    \n    # Aggregate all tree predictions\n    for update in updates:\n        running_pred -= learning_rate * update\n    \n    if regression:\n        # For regression, return the final predictions rounded to 4 decimal places\n        return np.round(running_pred, 4)\n    else:\n        # For classification, apply softmax to get probabilities and return class indices\n        # Softmax is applied row-wise (for each sample)\n        \n        # Numerical stability: subtract max value before exponentiating\n        exp_pred = np.exp(running_pred - np.max(running_pred, axis=1, keepdims=True))\n        probabilities = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n        \n        # Return the predicted class indices (argmax of probabilities)\n        return np.argmax(probabilities, axis=1)\n```\n\nThis implementation follows the requirements:\n1. It initializes a running prediction with zeros\n2. For each tree update, it subtracts the scaled update from the running prediction\n3. For regression, it returns the final prediction rounded to 4 decimal places\n4. For classification, it applies softmax to convert logits to probabilities and returns the predicted class indices\n5. The implementation is fully vectorized (no Python loops over individual samples)\n6. It only relies on NumPy"}
{"task_id": 371, "completion_id": 0, "completion": "I'll implement a 1D convolution function that supports all the required features: batches, multiple channels, stride, padding, and dilation. Here's the complete solution:\n\n```python\nimport numpy as np\nimport math\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Get dimensions\n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, in_channels_kernel, out_channels = W.shape\n    \n    # Check that input channels match\n    assert in_channels == in_channels_kernel, \"Input channels in X and W must match\"\n    \n    # Calculate effective kernel width with dilation\n    effective_kernel_width = kernel_width + (kernel_width - 1) * dilation\n    \n    # Handle padding\n    if pad == \"same\":\n        # Calculate output length based on SAME padding rule\n        output_length = math.ceil(signal_length / stride)\n        # Calculate total padding needed\n        total_pad = max(0, (output_length - 1) * stride + effective_kernel_width - signal_length)\n        # Split padding between left and right\n        pad_left = total_pad // 2\n        pad_right = total_pad - pad_left\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        pad_left, pad_right = pad\n    elif isinstance(pad, int):\n        pad_left = pad_right = pad\n    else:\n        raise ValueError(\"Padding must be an integer, a 2-tuple, or 'same'\")\n    \n    # Calculate output dimensions\n    padded_length = signal_length + pad_left + pad_right\n    output_length = (padded_length - effective_kernel_width) // stride + 1\n    \n    # Initialize output array\n    output = np.zeros((n_examples, output_length, out_channels))\n    \n    # Pad the input\n    X_padded = np.pad(X, ((0, 0), (pad_left, pad_right), (0, 0)), mode='constant')\n    \n    # Perform convolution\n    for b in range(n_examples):  # For each example in the batch\n        for i in range(output_length):  # For each position in the output\n            # Calculate the start position in the padded input\n            pos = i * stride\n            \n            for k in range(kernel_width):  # For each element in the kernel\n                # Calculate the position in the padded input with dilation\n                input_pos = pos + k * (dilation + 1)\n                \n                if input_pos < padded_length:\n                    # For all input and output channels\n                    for c_in in range(in_channels):\n                        for c_out in range(out_channels):\n                            output[b, i, c_out] += X_padded[b, input_pos, c_in] * W[k, c_in, c_out]\n    \n    return output.tolist()\n```\n\nThis implementation:\n\n1. Handles all required features: batches, multiple channels, stride, padding, and dilation\n2. Supports three types of padding: integer (same on both sides), tuple (different left/right padding), and \"same\" padding following TensorFlow's rule\n3. Calculates the effective kernel width accounting for dilation\n4. Performs the cross-correlation operation (not reversing the kernel)\n5. Returns the result as a Python list via ndarray.tolist()\n\nThe algorithm first pads the input signal, then iterates through each example in the batch, each position in the output, and each element in the kernel, applying the convolution operation while accounting for stride and dilation."}
{"task_id": 373, "completion_id": 0, "completion": "I'll implement a function to calculate the Gini impurity of a discrete label sequence according to the formula provided.\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Handle empty input\n    if len(y) == 0:\n        return 0.0\n    \n    # Convert to numpy array if it's a list\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Get the total number of samples\n    N = len(y)\n    \n    # Count occurrences of each class\n    class_counts = Counter(y)\n    \n    # Calculate the sum of squared probabilities\n    sum_squared_probs = sum((count / N) ** 2 for count in class_counts.values())\n    \n    # Calculate Gini impurity\n    gini_impurity = 1 - sum_squared_probs\n    \n    # Round to 4 decimal places\n    return round(gini_impurity, 4)\n```\n\nThis implementation:\n1. Handles empty input by returning 0.0\n2. Converts the input to a numpy array if it's a list\n3. Uses Counter to efficiently count occurrences of each class\n4. Calculates the Gini impurity using the formula: 1 - sum(p_c^2)\n5. Rounds the result to 4 decimal places as required\n\nThe function works with both list and numpy array inputs containing integer class labels."}
{"task_id": 374, "completion_id": 0, "completion": "I'll implement a linear regression model with batch gradient descent as specified. Here's the solution:\n\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n    \n    Args:\n        X: Feature matrix of shape (m, n) where m is the number of samples and n is the number of features\n        y: Target vector of shape (m,)\n        learning_rate: Learning rate for gradient descent\n        n_iterations: Number of iterations for gradient descent\n        \n    Returns:\n        tuple: (weights as list of floats, bias as float) rounded to 4 decimal places\n              or -1 if input data are inconsistent\n    \"\"\"\n    # Check if input data are consistent\n    if X.shape[0] != len(y):\n        return -1\n    \n    # Get dimensions\n    m, n = X.shape\n    \n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n    \n    # Gradient descent\n    for _ in range(n_iterations):\n        # Compute predictions\n        y_pred = X @ theta + b\n        \n        # Compute gradients\n        error = y_pred - y\n        dw = (2/m) * (X.T @ error)\n        db = (2/m) * np.sum(error)\n        \n        # Update parameters\n        theta = theta - learning_rate * dw\n        b = b - learning_rate * db\n    \n    # Round to 4 decimal places\n    theta_list = [round(float(w), 4) for w in theta]\n    b_rounded = round(float(b), 4)\n    \n    return (theta_list, b_rounded)\n```\n\nThis implementation:\n1. Checks if the input data are consistent (same number of samples in X and y)\n2. Initializes the weight vector \u03b8 with zeros and bias b with 0\n3. Performs batch gradient descent for the specified number of iterations\n4. Computes the gradients using the formulas provided\n5. Updates the parameters using the learning rate\n6. Returns the learned parameters rounded to 4 decimal places as specified\n\nThe implementation uses NumPy for efficient matrix operations, which makes the code both concise and performant."}
{"task_id": 376, "completion_id": 0, "completion": "# Symmetry Check for Square Matrices Solution\n\nHere's a solution that checks if a given 2D array is symmetric with respect to its main diagonal:\n\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert input to numpy array for easier manipulation\n    X_array = np.array(X, dtype=float)\n    \n    # Check if the matrix is square\n    if X_array.shape[0] != X_array.shape[1]:\n        return False\n    \n    # Check if the matrix equals its transpose\n    # Using allclose with default atol=1e-8 to handle floating point precision\n    return np.allclose(X_array, X_array.T)\n```\n\nThis solution:\n1. Converts the input list of lists to a NumPy array\n2. Checks if the array is square (same number of rows and columns)\n3. Uses NumPy's `allclose` function to compare the array with its transpose, which handles floating-point comparison with the required tolerance of 1\u00d710\u207b\u2078\n\nThe function will return `True` only if the matrix is square and symmetric, and `False` otherwise."}
{"task_id": 377, "completion_id": 0, "completion": "I'll implement the negative gradient function for logistic loss in gradient boosting. Here's the solution:\n\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculate the negative gradient for logistic loss in gradient boosting.\n    \n    Parameters:\n    -----------\n    y : list\n        Binary class labels (0 or 1)\n    f : list\n        Current prediction scores (floats)\n        \n    Returns:\n    --------\n    list\n        Negative gradients rounded to 4 decimal places\n        Returns -1 if y contains values other than 0 or 1\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    y_array = np.array(y)\n    f_array = np.array(f)\n    \n    # Check if y contains only 0 and 1\n    if not np.all(np.isin(y_array, [0, 1])):\n        return -1\n    \n    # Transform y from {0, 1} to {-1, 1} using y' = 2y - 1\n    y_transformed = 2 * y_array - 1\n    \n    # Calculate negative gradient: g_i = y'_i / (1 + exp(y'_i * f_i))\n    negative_gradient = y_transformed / (1 + np.exp(y_transformed * f_array))\n    \n    # Round to 4 decimal places and convert back to list\n    rounded_gradient = [round(g, 4) for g in negative_gradient]\n    \n    return rounded_gradient\n```\n\nThis implementation:\n1. Converts the inputs to numpy arrays for efficient computation\n2. Validates that the labels are only 0 or 1\n3. Transforms the labels from {0, 1} to {-1, 1} using the formula y' = 2y - 1\n4. Calculates the negative gradient using the formula g_i = y'_i / (1 + exp(y'_i * f_i))\n5. Rounds each value to 4 decimal places and returns the result as a list"}
{"task_id": 380, "completion_id": 0, "completion": "I'll implement the SELU activation function and its derivatives as requested.\n\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Convert input to numpy array for consistent handling\n    x_array = np.asarray(x, dtype=float)\n    \n    # SELU constants from the paper\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    \n    # Create a mask for positive and non-positive values\n    pos_mask = x_array > 0\n    \n    # Calculate the result based on the order\n    if order == 0:\n        # SELU(x) = scale * (x if x > 0 else alpha * (exp(x) - 1))\n        result = np.zeros_like(x_array)\n        result[pos_mask] = x_array[pos_mask]\n        result[~pos_mask] = alpha * (np.exp(x_array[~pos_mask]) - 1)\n        result = scale * result\n    \n    elif order == 1:\n        # First derivative: scale if x > 0, scale * alpha * exp(x) otherwise\n        result = np.zeros_like(x_array)\n        result[pos_mask] = scale\n        result[~pos_mask] = scale * alpha * np.exp(x_array[~pos_mask])\n    \n    elif order == 2:\n        # Second derivative: 0 if x > 0, scale * alpha * exp(x) otherwise\n        result = np.zeros_like(x_array)\n        result[~pos_mask] = scale * alpha * np.exp(x_array[~pos_mask])\n    \n    else:\n        raise ValueError(\"Order must be 0, 1, or 2\")\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Convert to Python list or scalar\n    if np.isscalar(x):\n        return float(result)\n    else:\n        return result.tolist()\n```\n\nThis implementation:\n1. Converts the input to a NumPy array for consistent handling\n2. Defines the SELU constants as specified in the paper\n3. Creates masks for positive and non-positive values\n4. Calculates the appropriate function based on the order parameter:\n   - For order=0: The SELU function itself\n   - For order=1: The first derivative\n   - For order=2: The second derivative\n5. Rounds the results to 4 decimal places\n6. Returns a scalar for scalar inputs or a Python list via tolist() for array inputs\n\nThe implementation handles scalar inputs, 1D arrays, and multi-dimensional arrays while preserving the input shape."}
{"task_id": 387, "completion_id": 0, "completion": "I'll implement a Gradient Boosting Regressor that works on one numerical feature using decision stumps. Let me break down the approach:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Initialize predictions with the mean of targets\n    y_pred = np.full_like(y, np.mean(y))\n    \n    # Perform boosting iterations\n    for _ in range(n_estimators):\n        # Compute residuals (negative gradients)\n        residuals = y - y_pred\n        \n        # Find the best split point\n        best_split = None\n        best_error = float('inf')\n        \n        # Sort data by feature values\n        sorted_indices = np.argsort(x)\n        sorted_x = x[sorted_indices]\n        sorted_residuals = residuals[sorted_indices]\n        \n        # Check if all feature values are identical\n        if len(np.unique(sorted_x)) == 1:\n            # No valid split, use global mean residual\n            mean_residual = np.mean(residuals)\n            y_pred += learning_rate * mean_residual\n            continue\n        \n        # Evaluate all possible split points\n        for i in range(len(sorted_x) - 1):\n            # Only consider splits between different x values\n            if sorted_x[i] == sorted_x[i + 1]:\n                continue\n                \n            # Calculate potential split point\n            split_point = (sorted_x[i] + sorted_x[i + 1]) / 2\n            \n            # Calculate mean residuals for left and right sides\n            left_indices = sorted_indices[:i + 1]\n            right_indices = sorted_indices[i + 1:]\n            \n            left_mean = np.mean(residuals[left_indices])\n            right_mean = np.mean(residuals[right_indices])\n            \n            # Calculate predictions for this split\n            split_preds = np.zeros_like(residuals)\n            split_preds[x <= split_point] = left_mean\n            split_preds[x > split_point] = right_mean\n            \n            # Calculate error for this split\n            error = np.sum((residuals - split_preds) ** 2)\n            \n            # Update best split if this one is better\n            if error < best_error:\n                best_error = error\n                best_split = (split_point, left_mean, right_mean)\n        \n        # Update predictions using the best split\n        if best_split:\n            split_point, left_mean, right_mean = best_split\n            stump_preds = np.zeros_like(residuals)\n            stump_preds[x <= split_point] = left_mean\n            stump_preds[x > split_point] = right_mean\n            y_pred += learning_rate * stump_preds\n    \n    # Round predictions to 4 decimal places and convert to list\n    return np.round(y_pred, 4).tolist()\n```\n\nThis implementation follows the specified algorithm:\n1. It initializes predictions with the mean of target values\n2. For each boosting round:\n   - Computes residuals (negative gradients)\n   - Finds the best split point by evaluating all possible splits between different consecutive x values\n   - If all feature values are identical, it uses the global mean residual\n   - Updates predictions using the learning rate and the stump predictions\n3. Returns the final predictions rounded to 4 decimal places as a Python list\n\nThe algorithm handles edge cases like when all feature values are identical, and it only considers splits between different consecutive x values as required."}
{"task_id": 394, "completion_id": 0, "completion": "I'll implement the RMSprop optimizer update step according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # Initialize Eg with zeros if it's None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    # Small constant for numerical stability\n    epsilon = 1e-8\n    \n    # Update the running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * np.square(grad)\n    \n    # Update the parameters\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + epsilon)\n    \n    # Round to 4 decimal places and convert to regular Python lists\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    \n    return w_next_rounded, Eg_next_rounded\n```\n\nThis implementation follows the RMSprop update rule exactly as specified:\n1. It initializes `Eg` with zeros if it's None\n2. It computes the running average of squared gradients using the decay rate `rho`\n3. It updates the parameters using the learning rate scaled by the inverse square root of the running average\n4. It rounds both the updated parameters and running average to 4 decimal places and converts them to regular Python lists before returning\n\nThe implementation handles both scalar and multi-dimensional arrays correctly, as NumPy operations are applied element-wise."}
{"task_id": 398, "completion_id": 0, "completion": "I'll implement the ELU activation function and its gradient according to the specifications.\n\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to numpy array if it's a list\n    x_array = np.array(x, dtype=float)\n    \n    if not derivative:\n        # Compute ELU activation\n        result = np.where(x_array >= 0, \n                          x_array, \n                          alpha * (np.exp(x_array) - 1))\n    else:\n        # Compute ELU gradient\n        result = np.where(x_array >= 0, \n                          1.0, \n                          alpha * np.exp(x_array))\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(val), 4) for val in result]\n```\n\nThis implementation:\n1. Converts the input to a NumPy array for efficient computation\n2. Uses `np.where` to apply the appropriate formula based on whether x is positive or negative\n3. For the activation function (when `derivative=False`), it returns x for positive values and \u03b1(e^x - 1) for negative values\n4. For the gradient (when `derivative=True`), it returns 1 for positive values and \u03b1*e^x for negative values (which is equivalent to ELU(x, \u03b1) + \u03b1 for negative x)\n5. Rounds each value to 4 decimal places and returns the result as a Python list"}
{"task_id": 411, "completion_id": 0, "completion": "I'll implement the `env_stats` function to analyze actions and observations according to the requirements. Here's the solution:\n\n```python\nimport numpy as np\nfrom typing import List, Dict, Any, Union, Iterable\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    result = {}\n    \n    # Process actions\n    result.update(process_collection(actions, prefix=\"action\"))\n    \n    # Process observations\n    result.update(process_collection(observations, prefix=\"obs\"))\n    \n    return result\n\ndef process_collection(collection: list, prefix: str) -> dict:\n    \"\"\"Process a collection (actions or observations) and extract statistics.\n    \n    Parameters\n    ----------\n    collection : list\n        List of actions or observations\n    prefix : str\n        Prefix for the dictionary keys (\"action\" or \"obs\")\n    \n    Returns\n    -------\n    dict\n        Dictionary with statistics for the collection\n    \"\"\"\n    stats = {}\n    \n    # Check if elements are tuples/iterables\n    is_tuple = False\n    for item in collection:\n        if isinstance(item, (list, tuple, np.ndarray)) and not isinstance(item, str):\n            is_tuple = True\n            break\n    \n    stats[f'tuple_{prefix}s'] = is_tuple\n    \n    # Determine dimensionality and prepare for further analysis\n    if is_tuple:\n        # For tuple/iterable elements, get the first item to determine dimensionality\n        first_item = next(item for item in collection if isinstance(item, (list, tuple, np.ndarray)))\n        dim = len(first_item)\n        \n        # Convert all items to lists for consistent processing\n        processed_collection = []\n        for item in collection:\n            if isinstance(item, (list, tuple, np.ndarray)):\n                processed_collection.append(list(item))\n            else:\n                # Handle scalar values in a collection that has some tuples\n                processed_collection.append([item])\n    else:\n        # For scalar elements\n        dim = 1\n        processed_collection = [[item] for item in collection]\n    \n    stats[f'{prefix}_dim'] = dim\n    stats[f'multidim_{prefix}s'] = dim > 1\n    \n    # Check if values are continuous (non-integer floats)\n    is_continuous = False\n    for item in processed_collection:\n        for value in item:\n            if isinstance(value, float) and not value.is_integer():\n                is_continuous = True\n                break\n        if is_continuous:\n            break\n    \n    stats[f'continuous_{prefix}s'] = is_continuous\n    \n    # Count unique values per dimension and collect unique IDs\n    unique_per_dim = []\n    ids_per_dim = []\n    \n    # Transpose the collection to get values by dimension\n    transposed = [[] for _ in range(dim)]\n    for item in processed_collection:\n        for i, val in enumerate(item[:dim]):  # Ensure we don't exceed dimensions\n            transposed[i].append(val)\n    \n    for dim_values in transposed:\n        unique_values = sorted(set(dim_values))\n        unique_per_dim.append(len(unique_values))\n        ids_per_dim.append(unique_values)\n    \n    stats[f'n_{prefix}s_per_dim'] = unique_per_dim\n    stats[f'{prefix}_ids'] = ids_per_dim\n    \n    return stats\n```\n\nThis implementation:\n\n1. Processes actions and observations separately using a helper function\n2. Determines if elements are tuples/iterables\n3. Calculates dimensionality\n4. Checks if values are continuous (non-integer floats)\n5. Counts unique values per dimension and collects unique IDs\n6. Returns a dictionary with all required statistics\n\nThe function handles both scalar and multi-dimensional inputs correctly and preserves the order of dimensions as required."}
{"task_id": 413, "completion_id": 0, "completion": "I'll implement a simplified Gradient Boosting Decision Tree regressor from scratch using only NumPy. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree's prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Initial prediction is the mean of the target values\n    initial_prediction = np.mean(y_train)\n    \n    # Initialize predictions for training and test sets\n    train_predictions = np.full(len(X_train), initial_prediction)\n    test_predictions = np.full(len(X_test), initial_prediction)\n    \n    # Boosting iterations\n    for _ in range(n_estimators):\n        # Calculate residuals (negative gradients for MSE loss)\n        residuals = y_train - train_predictions\n        \n        # Fit a regression tree to the residuals\n        tree = RegressionTree(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        \n        # Update predictions\n        train_predictions += learning_rate * tree.predict(X_train)\n        test_predictions += learning_rate * tree.predict(X_test)\n    \n    # Round to 4 decimal places and convert to Python list\n    return [round(float(pred), 4) for pred in test_predictions]\n\n\nclass Node:\n    \"\"\"Node class for the regression tree.\"\"\"\n    def __init__(self):\n        self.feature_idx = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.value = None\n        self.is_leaf = False\n\n\nclass RegressionTree:\n    \"\"\"Simple CART regression tree implementation.\"\"\"\n    def __init__(self, max_depth=3):\n        self.max_depth = max_depth\n        self.root = None\n    \n    def fit(self, X, y):\n        \"\"\"Fit the regression tree to the data.\"\"\"\n        self.root = self._grow_tree(X, y, depth=0)\n    \n    def _grow_tree(self, X, y, depth):\n        \"\"\"Recursively grow the tree.\"\"\"\n        node = Node()\n        \n        # Check stopping criteria\n        if depth >= self.max_depth or len(np.unique(y)) == 1 or len(y) <= 1:\n            node.is_leaf = True\n            node.value = np.mean(y)\n            return node\n        \n        # Find the best split\n        best_feature_idx, best_threshold = self._find_best_split(X, y)\n        \n        # If no good split is found, make this a leaf node\n        if best_feature_idx is None:\n            node.is_leaf = True\n            node.value = np.mean(y)\n            return node\n        \n        # Split the data\n        left_indices = X[:, best_feature_idx] <= best_threshold\n        right_indices = ~left_indices\n        \n        # If either split is empty, make this a leaf node\n        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n            node.is_leaf = True\n            node.value = np.mean(y)\n            return node\n        \n        # Set node attributes\n        node.feature_idx = best_feature_idx\n        node.threshold = best_threshold\n        \n        # Recursively grow left and right subtrees\n        node.left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n        node.right = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n        \n        return node\n    \n    def _find_best_split(self, X, y):\n        \"\"\"Find the best feature and threshold to split on.\"\"\"\n        n_samples, n_features = X.shape\n        \n        if n_samples <= 1:\n            return None, None\n        \n        # Calculate current MSE (before any split)\n        current_mse = self._calculate_mse(y)\n        \n        best_feature_idx = None\n        best_threshold = None\n        best_mse_reduction = 0\n        \n        # Try each feature\n        for feature_idx in range(n_features):\n            # Get unique values for the feature\n            thresholds = np.unique(X[:, feature_idx])\n            \n            # Try each threshold\n            for threshold in thresholds:\n                # Split the data\n                left_indices = X[:, feature_idx] <= threshold\n                right_indices = ~left_indices\n                \n                # Skip if either split is empty\n                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n                    continue\n                \n                # Calculate MSE for left and right splits\n                left_mse = self._calculate_mse(y[left_indices])\n                right_mse = self._calculate_mse(y[right_indices])\n                \n                # Calculate weighted MSE\n                n_left = np.sum(left_indices)\n                n_right = np.sum(right_indices)\n                weighted_mse = (n_left * left_mse + n_right * right_mse) / n_samples\n                \n                # Calculate MSE reduction\n                mse_reduction = current_mse - weighted_mse\n                \n                # Update best split if this one is better\n                if mse_reduction > best_mse_reduction:\n                    best_mse_reduction = mse_reduction\n                    best_feature_idx = feature_idx\n                    best_threshold = threshold\n        \n        return best_feature_idx, best_threshold\n    \n    def _calculate_mse(self, y):\n        \"\"\"Calculate mean squared error for a set of values.\"\"\"\n        if len(y) == 0:\n            return 0\n        return np.mean((y - np.mean(y)) ** 2)\n    \n    def predict(self, X):\n        \"\"\"Predict using the regression tree.\"\"\"\n        return np.array([self._predict_sample(x, self.root) for x in X])\n    \n    def _predict_sample(self, x, node):\n        \"\"\"Predict for a single sample by traversing the tree.\"\"\"\n        if node.is_leaf:\n            return node.value\n        \n        if x[node.feature_idx] <= node.threshold:\n            return self._predict_sample(x, node.left)\n        else:\n            return self._predict_sample(x, node.right)\n```\n\nThis implementation includes:\n\n1. A `gbdt_regression` function that:\n   - Initializes predictions with the mean of training targets\n   - Iteratively fits regression trees to the residuals\n   - Updates predictions using the learning rate\n   - Returns rounded predictions for the test set\n\n2. A `RegressionTree` class that:\n   - Implements a CART regression tree with binary splits\n   - Uses MSE as the split criterion\n   - Stops growing when max_depth is reached or no further error reduction is possible\n   - Handles edge cases like empty splits or homogeneous data\n\n3. A `Node` class to represent the tree structure\n\nThe implementation follows all the requirements and constraints specified in the task."}
{"task_id": 416, "completion_id": 0, "completion": "# Multivariate Gaussian PDF Implementation\n\nI'll implement the multivariate Gaussian probability density function according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculate the probability density function of a multivariate Gaussian distribution.\n    \n    Parameters:\n    -----------\n    X : numpy.ndarray\n        Array of shape (n_samples, n_features) containing the data points.\n        If X is 1D, it will be treated as (n_samples, 1).\n    mean : numpy.ndarray\n        Mean vector of the distribution with length n_features.\n    cov : numpy.ndarray\n        Covariance matrix of shape (n_features, n_features).\n        Must be positive-definite (invertible).\n    \n    Returns:\n    --------\n    list\n        A list where the i-th element is the PDF value for X[i] rounded to 4 decimal places.\n    \"\"\"\n    # Ensure X is 2D\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    n_samples, n_features = X.shape\n    \n    # Ensure mean is the right shape\n    mean = np.asarray(mean)\n    if mean.ndim == 0:\n        mean = np.array([mean])\n    \n    # Calculate determinant and inverse of covariance matrix\n    det_cov = np.linalg.det(cov)\n    inv_cov = np.linalg.inv(cov)\n    \n    # Calculate normalization constant\n    norm_const = 1.0 / (math.sqrt((2 * math.pi) ** n_features * det_cov))\n    \n    # Initialize result list\n    pdf_values = []\n    \n    # Calculate PDF for each sample\n    for i in range(n_samples):\n        x = X[i]\n        # Calculate (x - mean)\n        x_minus_mean = x - mean\n        # Calculate Mahalanobis distance: (x - mean)^T * inv_cov * (x - mean)\n        mahalanobis = np.dot(np.dot(x_minus_mean, inv_cov), x_minus_mean)\n        # Calculate PDF value\n        pdf = norm_const * math.exp(-0.5 * mahalanobis)\n        # Round to 4 decimal places and add to result\n        pdf_values.append(round(pdf, 4))\n    \n    return pdf_values\n```\n\nThis implementation:\n1. Handles 1D input arrays by reshaping them to (n_samples, 1)\n2. Calculates the determinant and inverse of the covariance matrix\n3. Computes the normalization constant\n4. For each sample, calculates the Mahalanobis distance and the PDF value\n5. Rounds each PDF value to 4 decimal places\n6. Returns the results as a Python list\n\nThe implementation follows the mathematical formula provided in the task description without using any third-party libraries beyond NumPy for basic array operations."}
{"task_id": 419, "completion_id": 0, "completion": "I'll implement the Bayesian linear regression with MAP prediction as requested. Here's the solution:\n\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # Make copies to avoid modifying the original arrays\n    X = X.copy()\n    X_new = X_new.copy()\n    \n    # 1. Add intercept column if required\n    if fit_intercept:\n        X = np.column_stack([np.ones(X.shape[0]), X])\n        X_new = np.column_stack([np.ones(X_new.shape[0]), X_new])\n    \n    # Get dimensions\n    N, M = X.shape  # N samples, M features (including intercept if fit_intercept=True)\n    \n    # 2. Process the prior parameters\n    # Handle mu: either scalar (replicate to all dimensions) or vector\n    if np.isscalar(mu):\n        mu = np.full(M, mu)\n    else:\n        # Ensure mu has the right shape\n        if len(mu) != M:\n            raise ValueError(f\"mu must have length {M} to match the number of features\")\n    \n    # Handle V: scalar, 1-D array, or full matrix\n    if V is None:\n        V_inv = np.eye(M)  # Identity matrix if V is None\n    elif np.isscalar(V):\n        V_inv = np.eye(M) / V  # Scalar multiple of identity\n    elif V.ndim == 1:\n        # V is a 1-D array representing the diagonal\n        if len(V) != M:\n            raise ValueError(f\"Diagonal V must have length {M} to match the number of features\")\n        V_inv = np.diag(1.0 / V)\n    else:\n        # V is a full matrix\n        if V.shape != (M, M):\n            raise ValueError(f\"V must be a {M}x{M} matrix to match the number of features\")\n        V_inv = np.linalg.inv(V)\n    \n    # 3. Compute the posterior distribution\n    # \u03a3\u209a = (V\u207b\u00b9 + X\u1d40 X)\u207b\u00b9\n    # \u03bc\u209a = \u03a3\u209a(V\u207b\u00b9\u03bc + X\u1d40y)\n    \n    # Calculate V\u207b\u00b9\u03bc\n    V_inv_mu = V_inv @ mu\n    \n    # Calculate X\u1d40y\n    X_T_y = X.T @ y\n    \n    # Calculate X\u1d40 X\n    X_T_X = X.T @ X\n    \n    # Calculate posterior precision matrix: V\u207b\u00b9 + X\u1d40 X\n    posterior_precision = V_inv + X_T_X\n    \n    # Calculate posterior covariance: \u03a3\u209a = (V\u207b\u00b9 + X\u1d40 X)\u207b\u00b9\n    posterior_cov = np.linalg.inv(posterior_precision)\n    \n    # Calculate posterior mean: \u03bc\u209a = \u03a3\u209a(V\u207b\u00b9\u03bc + X\u1d40y)\n    posterior_mean = posterior_cov @ (V_inv_mu + X_T_y)\n    \n    # 4. Compute MAP predictive mean: \u0177 = X_new \u03bc\u209a\n    y_pred = X_new @ posterior_mean\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(y_val), 4) for y_val in y_pred]\n```\n\nThis implementation follows the steps outlined in the task:\n\n1. It adds an intercept column when `fit_intercept=True`\n2. It handles the prior parameters correctly, supporting scalar, vector, and matrix forms of `V`\n3. It computes the posterior distribution using the formulas provided\n4. It calculates the MAP predictive mean for new data points\n5. It rounds the predictions to 4 decimal places and returns them as a Python list\n\nThe implementation also includes proper error handling for cases where the dimensions of the inputs don't match."}
{"task_id": 423, "completion_id": 0, "completion": "# Root Mean Squared Logarithmic Error Calculator\n\nHere's a complete implementation of the RMSLE calculator function:\n\n```python\nimport numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"\n    Calculate the Root Mean Squared Logarithmic Error between two lists.\n    \n    Args:\n        actual: List of ground-truth values (non-negative)\n        predicted: List of predicted values (non-negative)\n        \n    Returns:\n        RMSLE value rounded to 4 decimal places, or -1 if validation fails\n    \"\"\"\n    # Validate inputs\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Check for non-negative values\n    if any(val < 0 for val in actual) or any(val < 0 for val in predicted):\n        return -1\n    \n    # Convert to numpy arrays for vectorized operations\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Calculate RMSLE\n    log_diff = np.log1p(predicted_array) - np.log1p(actual_array)\n    squared_log_diff = np.square(log_diff)\n    mean_squared_log_diff = np.mean(squared_log_diff)\n    rmsle = np.sqrt(mean_squared_log_diff)\n    \n    # Round to 4 decimal places\n    return round(rmsle, 4)\n```\n\nThis implementation:\n1. Validates that both inputs have the same length\n2. Checks that all values in both lists are non-negative\n3. Uses NumPy's vectorized operations for efficient calculation\n4. Uses `np.log1p()` which computes log(1+x) with better numerical stability\n5. Returns the result rounded to 4 decimal places as required"}
{"task_id": 428, "completion_id": 0, "completion": "I'll create a function to generate a Hann window according to the specifications. Here's the implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    # Handle special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    # For periodic window, we need to compute window_len+1 points and discard the last one\n    if not symmetric:\n        # Create a symmetric window of length window_len+1 and drop the last point\n        n = np.arange(window_len + 1)\n        denominator = window_len\n        window = 0.5 - 0.5 * np.cos(2 * np.pi * n / denominator)\n        window = window[:-1]  # Drop the last point\n    else:\n        # Create a symmetric window of length window_len\n        n = np.arange(window_len)\n        denominator = window_len - 1\n        window = 0.5 - 0.5 * np.cos(2 * np.pi * n / denominator)\n    \n    # Round to 4 decimal places and convert to list\n    window = [round(x, 4) for x in window]\n    \n    return window\n```\n\nThis implementation handles all the specified cases:\n1. Returns an empty list when window_len \u2264 0\n2. Returns [1.0] when window_len = 1\n3. Creates a symmetric window when symmetric=True\n4. Creates a periodic window when symmetric=False by generating a symmetric window of length window_len+1 and dropping the last point\n5. Rounds all values to 4 decimal places\n6. Returns a regular Python list"}
{"task_id": 433, "completion_id": 0, "completion": "I'll implement the `glorot_fan` function according to the specifications. Here's the solution:\n\n```python\nimport numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    # Check if the shape has exactly 4 dimensions (convolutional kernel)\n    if len(shape) == 4:\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = float(in_channels * receptive_field_size)\n        fan_out = float(out_channels * receptive_field_size)\n    else:\n        # For all other tensors, take the first two dimensions directly\n        fan_in = float(shape[1])\n        fan_out = float(shape[0])\n    \n    return (fan_in, fan_out)\n```\n\nThis implementation follows the rules specified in the task:\n1. For 4D tensors (convolutional kernels), it calculates fan_in and fan_out using the receptive field size.\n2. For all other tensors, it takes the first two dimensions directly.\n3. The function returns both values as floats in a tuple."}
{"task_id": 435, "completion_id": 0, "completion": "I'll implement a tiny Gradient Boosting Regressor as specified. Here's the solution:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert inputs to NumPy arrays of type float\n    X_np = np.array(X, dtype=float)\n    y_np = np.array(y, dtype=float)\n    \n    # Initialize prediction with the mean of y\n    y_mean = np.mean(y_np)\n    \n    # Handle special cases\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(y_mean, 4)] * len(y_np)\n    \n    # Initialize current prediction\n    y_pred = np.full(len(y_np), y_mean)\n    \n    # Boosting iterations\n    for t in range(1, n_estimators + 1):\n        # Compute residuals\n        residuals = y_np - y_pred\n        \n        # Fit OLS linear model to predict residuals from X\n        # Add a column of ones to X for the intercept\n        X_with_intercept = np.column_stack((np.ones(X_np.shape[0]), X_np))\n        \n        # Solve the normal equations: (X^T X)^(-1) X^T r\n        beta = np.linalg.lstsq(X_with_intercept, residuals, rcond=None)[0]\n        \n        # Get predictions from the weak learner\n        h_t = X_with_intercept @ beta\n        \n        # Update the overall prediction\n        y_pred = y_pred + learning_rate * h_t\n    \n    # Round to 4 decimal places and convert to Python list\n    return [round(pred, 4) for pred in y_pred]\n```\n\nThis implementation follows the specified algorithm:\n1. Converts inputs to NumPy arrays\n2. Initializes predictions with the mean of y\n3. Handles special cases as required\n4. For each iteration:\n   - Computes residuals\n   - Fits an OLS linear model to predict residuals from X (including intercept)\n   - Updates predictions using the learning rate\n5. Returns the final predictions rounded to 4 decimal places as a Python list\n\nThe implementation uses NumPy's `linalg.lstsq` to solve the normal equations for OLS regression, which is a numerically stable approach."}
{"task_id": 437, "completion_id": 0, "completion": "I'll implement the logistic sigmoid function and its derivatives as requested.\n\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    # Check if order is valid\n    if order not in [0, 1, 2]:\n        return -1\n    \n    # Convert input to numpy array for consistent handling\n    is_scalar = np.isscalar(x)\n    x_array = np.array(x, dtype=float)\n    \n    # Compute sigmoid\n    sigmoid_x = 1.0 / (1.0 + np.exp(-x_array))\n    \n    # Compute result based on order\n    if order == 0:\n        result = sigmoid_x\n    elif order == 1:\n        result = sigmoid_x * (1 - sigmoid_x)\n    else:  # order == 2\n        result = sigmoid_x * (1 - sigmoid_x) * (1 - 2 * sigmoid_x)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return appropriate type based on input\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n```\n\nThis implementation:\n1. Checks if the order is valid (0, 1, or 2) and returns -1 if not\n2. Converts the input to a NumPy array while preserving its shape\n3. Computes the sigmoid function or its derivatives based on the order parameter\n4. Rounds the result to 4 decimal places\n5. Returns a float for scalar inputs or a Python list for list/array inputs\n\nThe function handles all the required input types (scalar, list, and NumPy array) and preserves the original shape of the input."}
{"task_id": 438, "completion_id": 0, "completion": "I'll implement the `to_categorical` function as requested. This function will convert class labels to one-hot encoding, handling the various requirements specified in the task.\n\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Convert input to numpy array if it's a list\n    y_array = np.asarray(y)\n    \n    # Determine the number of classes\n    required_classes = np.max(y_array) + 1\n    \n    # If num_classes is None, use the required number\n    if num_classes is None:\n        num_classes = required_classes\n    \n    # Check if num_classes is sufficient\n    if num_classes < required_classes:\n        return -1\n    \n    # Create a zero matrix of shape (len(y), num_classes)\n    result = np.zeros((len(y_array), num_classes), dtype=int)\n    \n    # Set the appropriate positions to 1\n    for i, class_idx in enumerate(y_array):\n        result[i, class_idx] = 1\n    \n    # Convert to list of lists and return\n    return result.tolist()\n```\n\nThis implementation:\n1. Converts the input to a NumPy array\n2. Determines the required number of classes based on the maximum value in the input\n3. Checks if the provided `num_classes` is sufficient\n4. Creates a zero matrix and sets the appropriate positions to 1\n5. Converts the result to a list of lists before returning\n\nThe function handles all the specified requirements and edge cases."}
{"task_id": 439, "completion_id": 0, "completion": "# Apriori Frequent Itemset Mining Implementation\n\nI'll implement the Apriori algorithm to find frequent itemsets in a transaction database. The algorithm follows the classic breadth-first approach, starting with single items and progressively building larger itemsets.\n\n```python\nimport itertools\nfrom collections import Counter\n\ndef apriori_frequent_itemsets(transactions: list[list], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    if not transactions or min_sup <= 0 or min_sup > 1:\n        return []\n    \n    num_transactions = len(transactions)\n    min_count = min_sup * num_transactions\n    \n    # Find all unique items in the transactions\n    all_items = set()\n    for transaction in transactions:\n        all_items.update(transaction)\n    \n    # Count occurrences of each item\n    item_counts = Counter()\n    for transaction in transactions:\n        for item in set(transaction):  # Count each item once per transaction\n            item_counts[item] += 1\n    \n    # Find frequent 1-itemsets\n    L1 = {(item,) for item in all_items if item_counts[item] >= min_count}\n    \n    # Sort L1 lexicographically\n    L1 = sorted(L1)\n    \n    # Initialize result with L1\n    result = list(L1)\n    \n    # Current set of frequent itemsets\n    Lk = L1\n    \n    # Generate frequent itemsets of size k > 1\n    k = 2\n    while Lk:\n        # Generate candidate itemsets of size k\n        Ck = self_join(Lk, k)\n        \n        # Prune candidates that contain infrequent subsets\n        Ck = prune(Ck, Lk, k)\n        \n        # Count support for each candidate\n        itemset_counts = Counter()\n        for transaction in transactions:\n            transaction_set = set(transaction)\n            for candidate in Ck:\n                if all(item in transaction_set for item in candidate):\n                    itemset_counts[candidate] += 1\n        \n        # Filter candidates by minimum support\n        Lk = [candidate for candidate in Ck if itemset_counts[candidate] >= min_count]\n        \n        # Sort Lk lexicographically\n        Lk = sorted(Lk)\n        \n        # Add frequent k-itemsets to result\n        result.extend(Lk)\n        \n        k += 1\n    \n    return result\n\ndef self_join(Lk, k):\n    \"\"\"Join Lk with itself to generate candidates of size k.\"\"\"\n    candidates = []\n    \n    for i in range(len(Lk)):\n        for j in range(i+1, len(Lk)):\n            # Check if first k-2 items are the same\n            if k == 2 or Lk[i][:-1] == Lk[j][:-1]:\n                # Create a new candidate by combining the two itemsets\n                # and ensure the items are in lexicographic order\n                new_candidate = tuple(sorted(set(Lk[i]) | set(Lk[j])))\n                if len(new_candidate) == k:\n                    candidates.append(new_candidate)\n    \n    return candidates\n\ndef prune(candidates, prev_frequent, k):\n    \"\"\"Prune candidates that contain infrequent subsets.\"\"\"\n    pruned_candidates = []\n    \n    for candidate in candidates:\n        is_valid = True\n        # Generate all k-1 subsets of the candidate\n        for subset in itertools.combinations(candidate, k-1):\n            if subset not in prev_frequent:\n                is_valid = False\n                break\n        \n        if is_valid:\n            pruned_candidates.append(candidate)\n    \n    return pruned_candidates\n```\n\nThis implementation follows the classic Apriori algorithm:\n\n1. First, it identifies all frequent 1-itemsets by counting item occurrences and filtering by the minimum support threshold.\n2. Then it iteratively generates candidate k-itemsets by joining frequent (k-1)-itemsets with themselves.\n3. It prunes candidates that contain infrequent subsets.\n4. It counts the support for each candidate and keeps only those that meet the minimum support threshold.\n5. The process continues until no new frequent itemsets are found.\n\nThe result is a list of tuples representing frequent itemsets, ordered first by length and then lexicographically within each length group. Each tuple contains items in ascending lexicographic order."}
{"task_id": 440, "completion_id": 0, "completion": "I'll implement a function to aggregate predictions from multiple estimators by averaging their probability distributions.\n\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert to numpy array for easier manipulation\n    predictions_array = np.array(predictions)\n    \n    # Get dimensions\n    n_estimators, n_samples, n_classes = predictions_array.shape\n    \n    # Average probabilities across estimators\n    avg_probabilities = np.mean(predictions_array, axis=0)\n    \n    # Round to 4 decimal places\n    avg_probabilities_rounded = np.round(avg_probabilities, 4)\n    \n    # Get predicted class indices (argmax for each sample)\n    predicted_classes = np.argmax(avg_probabilities_rounded, axis=1)\n    \n    # Convert back to Python lists\n    avg_probabilities_list = avg_probabilities_rounded.tolist()\n    predicted_classes_list = predicted_classes.tolist()\n    \n    return avg_probabilities_list, predicted_classes_list\n```\n\nThis implementation:\n1. Converts the input list to a NumPy array for efficient computation\n2. Calculates the mean probability across all estimators for each sample and class\n3. Rounds the averaged probabilities to 4 decimal places\n4. Finds the class index with the highest probability for each sample\n5. Converts the results back to Python lists before returning them\n\nThe function handles ties correctly by using NumPy's `argmax`, which returns the index of the first occurrence of the maximum value, effectively choosing the smallest index in case of a tie."}
{"task_id": 444, "completion_id": 0, "completion": "I'll implement the RBF kernel function according to the specifications. Here's the solution:\n\n```python\nimport numpy as np\nfrom typing import Union, List, Optional\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    X_np = np.array(X, dtype=float)\n    \n    # If Y is None, use X\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y, dtype=float)\n    \n    # Get dimensions\n    N, C_X = X_np.shape\n    M, C_Y = Y_np.shape\n    \n    # Validate feature dimensions\n    if C_X != C_Y:\n        return -1\n    \n    # Process sigma parameter\n    if sigma is None:\n        # Default value: sqrt(C/2)\n        sigma_values = np.sqrt(C_X / 2) * np.ones(C_X)\n    elif isinstance(sigma, (int, float)):\n        # Single value: use for all features\n        if sigma <= 0:\n            return -1\n        sigma_values = sigma * np.ones(C_X)\n    else:\n        # List/array: one value per feature\n        sigma_values = np.array(sigma, dtype=float)\n        if len(sigma_values) != C_X:\n            return -1\n        if np.any(sigma_values <= 0):\n            return -1\n    \n    # Compute the kernel matrix\n    kernel_matrix = np.zeros((N, M))\n    \n    for i in range(N):\n        for j in range(M):\n            # Calculate squared differences scaled by sigma\n            squared_diff = ((X_np[i] - Y_np[j]) / sigma_values) ** 2\n            # Sum and apply exponential\n            kernel_matrix[i, j] = np.exp(-0.5 * np.sum(squared_diff))\n    \n    # Round to 4 decimal places and convert to nested list\n    rounded_matrix = np.round(kernel_matrix, 4).tolist()\n    \n    return rounded_matrix\n```\n\nThis implementation:\n1. Validates all inputs as required\n2. Handles the three cases for sigma (None, scalar, vector)\n3. Computes the RBF kernel matrix according to the formula\n4. Rounds all values to 4 decimal places\n5. Returns the result as a nested Python list\n\nThe function will return -1 in all the specified error cases: when sigma is non-positive, when sigma's length doesn't match the feature dimension, or when the feature dimensions of X and Y don't match."}
{"task_id": 446, "completion_id": 0, "completion": "I'll implement the `decision_stump` function that finds the best single-level split according to information gain. Let me break down the approach:\n\n1. Calculate the entropy of the parent node\n2. For each feature:\n   - Find all possible thresholds (midpoints between distinct sorted values)\n   - For each threshold, calculate the information gain\n   - Keep track of the best split\n3. Return the appropriate tuple based on whether we found a good split\n\nHere's the implementation:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n    \n    # Calculate the entropy of the parent node\n    parent_entropy = calculate_entropy(y)\n    \n    # Find the majority label in the entire dataset\n    majority_label = get_majority_label(y)\n    \n    # If all samples have the same label, no split can improve information gain\n    if parent_entropy == 0:\n        return (-1, None, majority_label, majority_label)\n    \n    best_gain = 0\n    best_feature = -1\n    best_threshold = None\n    best_left_majority = None\n    best_right_majority = None\n    \n    # For each feature\n    for j in range(n_features):\n        # Get unique sorted values for this feature\n        feature_values = np.sort(np.unique(X[:, j]))\n        \n        # If there's only one unique value, we can't split on this feature\n        if len(feature_values) <= 1:\n            continue\n        \n        # Calculate thresholds as midpoints between consecutive values\n        thresholds = [(feature_values[i] + feature_values[i+1]) / 2 \n                      for i in range(len(feature_values) - 1)]\n        \n        # For each threshold\n        for threshold in thresholds:\n            # Split the dataset\n            left_mask = X[:, j] <= threshold\n            right_mask = ~left_mask\n            \n            # Skip if the split doesn't divide the dataset\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n            \n            # Calculate proportions\n            p_left = np.sum(left_mask) / n_samples\n            p_right = np.sum(right_mask) / n_samples\n            \n            # Calculate entropies of children\n            left_entropy = calculate_entropy(y[left_mask])\n            right_entropy = calculate_entropy(y[right_mask])\n            \n            # Calculate information gain\n            gain = parent_entropy - (p_left * left_entropy + p_right * right_entropy)\n            \n            # Get majority labels for left and right children\n            left_majority = get_majority_label(y[left_mask])\n            right_majority = get_majority_label(y[right_mask])\n            \n            # Update best split if this one is better\n            # (or equal but with tie-breaking rules applied)\n            if (gain > best_gain or \n                (gain == best_gain and j < best_feature) or\n                (gain == best_gain and j == best_feature and threshold < best_threshold)):\n                best_gain = gain\n                best_feature = j\n                best_threshold = threshold\n                best_left_majority = left_majority\n                best_right_majority = right_majority\n    \n    # If no split improves information gain\n    if best_gain <= 0:\n        return (-1, None, majority_label, majority_label)\n    \n    # Round the threshold to 4 decimal places\n    best_threshold = round(best_threshold, 4)\n    \n    return (best_feature, best_threshold, best_left_majority, best_right_majority)\n\ndef calculate_entropy(labels):\n    \"\"\"Calculate the Shannon entropy of a set of labels.\"\"\"\n    if len(labels) == 0:\n        return 0\n    \n    # Count occurrences of each label\n    counts = Counter(labels)\n    probabilities = [count / len(labels) for count in counts.values()]\n    \n    # Calculate entropy\n    entropy = -sum(p * np.log2(p) for p in probabilities)\n    return entropy\n\ndef get_majority_label(labels):\n    \"\"\"Get the most frequent label (with tie-breaking).\"\"\"\n    if len(labels) == 0:\n        return None\n    \n    # Count occurrences of each label\n    counter = Counter(labels)\n    \n    # Find the maximum count\n    max_count = max(counter.values())\n    \n    # Get all labels with the maximum count\n    max_labels = [label for label, count in counter.items() if count == max_count]\n    \n    # Return the smallest label among those with maximum count\n    return min(max_labels)\n```\n\nThis implementation:\n1. Calculates the entropy of the parent node\n2. Iterates through each feature and possible threshold\n3. Computes information gain for each split\n4. Applies the specified tie-breaking rules\n5. Returns the appropriate tuple with the best split information\n6. Handles the case where no split improves information gain\n7. Rounds the threshold to 4 decimal places as required\n\nThe helper functions `calculate_entropy` and `get_majority_label` handle the entropy calculation and finding the majority label with proper tie-breaking, respectively."}
{"task_id": 452, "completion_id": 0, "completion": "I'll implement the `divide_on_feature` function to split a dataset based on a feature threshold. Here's the complete solution:\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Convert X to numpy array if it's not already\n    X_array = np.array(X)\n    \n    # Initialize empty lists for the two subsets\n    first_subset = []\n    second_subset = []\n    \n    # Check if threshold is numeric (int or float)\n    if isinstance(threshold, (int, float)):\n        # For numeric threshold: first subset contains samples with feature value >= threshold\n        first_subset_indices = [i for i, sample in enumerate(X_array) if sample[feature_i] >= threshold]\n        second_subset_indices = [i for i, sample in enumerate(X_array) if sample[feature_i] < threshold]\n    else:\n        # For non-numeric threshold: first subset contains samples with feature value == threshold\n        first_subset_indices = [i for i, sample in enumerate(X_array) if sample[feature_i] == threshold]\n        second_subset_indices = [i for i, sample in enumerate(X_array) if sample[feature_i] != threshold]\n    \n    # Extract the samples for each subset while preserving the original order\n    first_subset = X_array[first_subset_indices].tolist() if first_subset_indices else []\n    second_subset = X_array[second_subset_indices].tolist() if second_subset_indices else []\n    \n    return [first_subset, second_subset]\n```\n\nThis implementation:\n1. Converts the input to a NumPy array if it's not already\n2. Determines whether the threshold is numeric (int or float)\n3. For numeric thresholds, splits based on whether values are >= threshold\n4. For non-numeric thresholds, splits based on exact equality\n5. Preserves the original order of samples in both subsets\n6. Converts the results to Python lists using tolist()\n7. Returns a list containing the two subsets"}
{"task_id": 453, "completion_id": 0, "completion": "I'll solve this task by implementing the DQN target Q-value update function according to the specifications.\n\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if gamma is in the valid range\n    if gamma < 0 or gamma > 1:\n        return -1\n    \n    # Check if all input arrays have consistent lengths\n    batch_size = Q.shape[0]\n    if (Q_next.shape[0] != batch_size or \n        len(actions) != batch_size or \n        len(rewards) != batch_size or \n        len(dones) != batch_size):\n        return -1\n    \n    # Create a copy of Q to modify\n    Q_targets = Q.copy()\n    \n    # Update the Q-values for the actions that were taken\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        \n        # Update only the Q-value for the action that was taken\n        Q_targets[i, actions[i]] = target\n    \n    # Round to 4 decimal places and convert to Python list\n    Q_targets = np.round(Q_targets, 4).tolist()\n    \n    return Q_targets\n```\n\nThis implementation:\n1. Validates the inputs (gamma range and consistent array lengths)\n2. Creates a copy of the original Q-values to avoid modifying the input\n3. Updates only the Q-values for the actions that were taken in each state\n4. Applies the DQN update rule: rewards[i] for terminal states, or rewards[i] + gamma * max(Q_next[i]) for non-terminal states\n5. Rounds all values to 4 decimal places and returns the result as a nested Python list"}
{"task_id": 458, "completion_id": 0, "completion": "I'll implement the `to_categorical` function that converts integer labels to one-hot encoded format according to the requirements.\n\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Convert labels to a list if it's not already\n    labels_list = list(labels)\n    \n    # Check if any label is negative\n    if any(label < 0 for label in labels_list):\n        return -1\n    \n    # Determine the number of classes\n    max_label = max(labels_list) if labels_list else -1\n    required_classes = max_label + 1\n    \n    # If num_classes is provided, check if it's sufficient\n    if num_classes is not None and num_classes < required_classes:\n        return -1\n    \n    # Use the provided num_classes or infer it\n    actual_num_classes = num_classes if num_classes is not None else required_classes\n    \n    # Create the one-hot encoded matrix\n    result = []\n    for label in labels_list:\n        one_hot = [0] * actual_num_classes\n        one_hot[label] = 1\n        result.append(one_hot)\n    \n    return result\n```\n\nThis implementation:\n1. Converts the input to a list for consistent processing\n2. Checks for negative labels and returns -1 if any are found\n3. Determines the required number of classes based on the maximum label\n4. Validates the provided num_classes if it's specified\n5. Creates a one-hot encoded matrix as a list of lists with integers (0s and 1s)\n6. Returns the result or -1 if the input is invalid"}
{"task_id": 461, "completion_id": 0, "completion": "I'll implement a lightweight gradient boosting regressor for 1D regression data using decision stumps as the weak learners. This implementation will follow the algorithm you've described, using only functions (no classes).\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    # Convert inputs to numpy arrays for easier manipulation\n    x_array = np.array(x)\n    y_array = np.array(y)\n    \n    # Initialize predictions with the mean of targets\n    y_pred = np.full_like(y_array, np.mean(y_array))\n    \n    # If n_estimators is 0 or negative, return the mean prediction\n    if n_estimators <= 0:\n        return np.round(y_pred, 4).tolist()\n    \n    # Main boosting loop\n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_array - y_pred\n        \n        # If all residuals are zero, stop early\n        if np.all(np.abs(residuals) < 1e-10):\n            break\n        \n        # Find the best split\n        best_split = find_best_split(x_array, residuals)\n        \n        # Update predictions using the stump and learning rate\n        y_pred += learning_rate * apply_stump(x_array, best_split)\n    \n    # Return final predictions rounded to 4 decimal places\n    return np.round(y_pred, 4).tolist()\n\ndef find_best_split(x: np.ndarray, residuals: np.ndarray) -> dict:\n    \"\"\"Find the best split for a decision stump.\n    \n    Parameters\n    ----------\n    x : np.ndarray\n        Feature values.\n    residuals : np.ndarray\n        Current residuals.\n        \n    Returns\n    -------\n    dict\n        Dictionary containing the split threshold and mean values for left and right sides.\n    \"\"\"\n    # Get unique x values except the greatest one\n    unique_x = np.sort(np.unique(x))\n    if len(unique_x) <= 1:\n        # If no valid split exists, return the mean residual for all data\n        return {\n            'threshold': None,\n            'left_value': np.mean(residuals),\n            'right_value': np.mean(residuals)\n        }\n    \n    split_candidates = unique_x[:-1]\n    \n    best_sse = float('inf')\n    best_threshold = None\n    best_left_value = None\n    best_right_value = None\n    \n    # Try each possible split\n    for threshold in split_candidates:\n        # Split the data\n        left_mask = x <= threshold\n        right_mask = ~left_mask\n        \n        # Skip if either side is empty\n        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n            continue\n        \n        # Calculate mean residuals for each side\n        left_value = np.mean(residuals[left_mask])\n        right_value = np.mean(residuals[right_mask])\n        \n        # Calculate SSE\n        sse = np.sum((residuals[left_mask] - left_value) ** 2) + \\\n              np.sum((residuals[right_mask] - right_value) ** 2)\n        \n        # Update best split if this one is better\n        if sse < best_sse:\n            best_sse = sse\n            best_threshold = threshold\n            best_left_value = left_value\n            best_right_value = right_value\n    \n    # If no valid split was found, use the mean residual for all data\n    if best_threshold is None:\n        mean_residual = np.mean(residuals)\n        return {\n            'threshold': None,\n            'left_value': mean_residual,\n            'right_value': mean_residual\n        }\n    \n    return {\n        'threshold': best_threshold,\n        'left_value': best_left_value,\n        'right_value': best_right_value\n    }\n\ndef apply_stump(x: np.ndarray, split: dict) -> np.ndarray:\n    \"\"\"Apply a decision stump to the data.\n    \n    Parameters\n    ----------\n    x : np.ndarray\n        Feature values.\n    split : dict\n        Dictionary containing the split threshold and mean values for left and right sides.\n        \n    Returns\n    -------\n    np.ndarray\n        Predictions from the stump.\n    \"\"\"\n    if split['threshold'] is None:\n        # If no valid split, return the same value for all points\n        return np.full_like(x, split['left_value'], dtype=float)\n    \n    # Apply the stump rule\n    predictions = np.where(x <= split['threshold'], \n                          split['left_value'], \n                          split['right_value'])\n    \n    return predictions\n```\n\nThis implementation follows the algorithm you described:\n1. It initializes predictions with the mean of the target values\n2. For each boosting iteration:\n   - Computes residuals\n   - Finds the best split by evaluating all possible thresholds\n   - Applies the stump to get predictions\n   - Updates the ensemble predictions using the learning rate\n3. It handles all the special cases:\n   - Returns mean prediction if n_estimators \u2264 0\n   - Stops early if residuals become all zeros\n   - Handles the case when no valid split exists\n4. Finally, it returns predictions rounded to 4 decimal places as a list\n\nThe implementation uses only numpy and the Python standard library as required."}
{"task_id": 471, "completion_id": 0, "completion": "I'll implement the Expected SARSA TD(0) Q-Table update algorithm as requested. This algorithm calculates the expected future value based on the \u03b5-soft policy derived from the current Q-table.\n\n```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # Create a copy of the Q-table to avoid modifying the original\n    q_table_copy = [row[:] for row in q_table]\n    \n    # Get the number of actions\n    num_actions = len(q_table[0])\n    \n    # Calculate the target value\n    if next_state is None:\n        # Terminal state, expected future value is 0\n        expected_value = 0\n    else:\n        # Find the greedy action in the next state\n        next_state_values = q_table[next_state]\n        greedy_action = np.argmax(next_state_values)\n        \n        # Calculate the expected value using the \u03b5-soft policy\n        expected_value = 0\n        for a in range(num_actions):\n            # Probability of taking action a according to \u03b5-soft policy\n            if a == greedy_action:\n                # Greedy action: \u03c0(a|s') = 1 \u2212 \u03b5 + \u03b5/|A|\n                prob = 1 - epsilon + epsilon / num_actions\n            else:\n                # Non-greedy action: \u03c0(a|s') = \u03b5/|A|\n                prob = epsilon / num_actions\n            \n            expected_value += prob * next_state_values[a]\n    \n    # Calculate the TD target\n    target = reward + gamma * expected_value\n    \n    # Update the Q-value for the current state-action pair\n    q_table_copy[state][action] += lr * (target - q_table_copy[state][action])\n    \n    # Round all entries to 4 decimal places\n    for i in range(len(q_table_copy)):\n        for j in range(len(q_table_copy[i])):\n            q_table_copy[i][j] = round(q_table_copy[i][j], 4)\n    \n    return q_table_copy\n```\n\nThis implementation:\n1. Creates a copy of the Q-table to avoid modifying the original\n2. Calculates the expected future value based on the \u03b5-soft policy:\n   - For terminal states, the expected value is 0\n   - For non-terminal states, it calculates the weighted sum of Q-values in the next state\n3. Computes the TD target using the reward and discounted expected future value\n4. Updates the Q-value for the current state-action pair\n5. Rounds all entries in the Q-table to 4 decimal places before returning"}
{"task_id": 474, "completion_id": 0, "completion": "I'll implement the `generate_data` function according to the requirements. Here's the solution:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    Y_train = []\n    X_val = []\n    Y_val = []\n    \n    for i in range(m):\n        # Calculate the horizontal position based on floor division\n        x_min = (i // 2 + 0.1) * 10\n        x_max = (i // 2 + 0.9) * 10\n        \n        # Calculate the vertical position based on modulo\n        y_min = ((i % 2) * interval + 0.1) * 10\n        y_max = ((i % 2) * interval + 0.9) * 10\n        \n        # Calculate the label for this class\n        label = (i - 0.5) * 2\n        \n        # Generate training samples for this class\n        for _ in range(n_train):\n            x = random.uniform(x_min, x_max)\n            y = random.uniform(y_min, y_max)\n            X_train.append([round(x, 4), round(y, 4)])\n            Y_train.append(label)\n        \n        # Generate validation samples for this class\n        for _ in range(n_val):\n            x = random.uniform(x_min, x_max)\n            y = random.uniform(y_min, y_max)\n            X_val.append([round(x, 4), round(y, 4)])\n            Y_val.append(label)\n    \n    # Convert to numpy arrays\n    X_train = np.array(X_train)\n    Y_train = np.array(Y_train)\n    X_val = np.array(X_val)\n    Y_val = np.array(Y_val)\n    \n    return X_train, X_val, Y_train, Y_val\n```\n\nThis implementation:\n1. Takes all the required parameters and an optional seed\n2. Sets the random seed if provided\n3. Generates data for each class according to the specified rules\n4. Rounds all coordinates to 4 decimal places\n5. Returns the four NumPy arrays with the correct shapes\n\nThe function correctly handles multiple classes (m \u2265 2) and arranges them in a pattern where classes alternate between two rows, with the vertical distance between rows controlled by the interval parameter."}
{"task_id": 475, "completion_id": 0, "completion": "# Adam Optimizer Implementation\n\nHere's a complete implementation of a single Adam optimization step:\n\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Initialize moments to zero if None\n    if m_prev is None:\n        m_prev = np.zeros_like(grad, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad, dtype=float)\n    \n    # Update biased first moment estimate\n    m_new = b1 * m_prev + (1 - b1) * grad\n    \n    # Update biased second raw moment estimate\n    v_new = b2 * v_prev + (1 - b2) * np.square(grad)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_new / (1 - b1**t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v_new / (1 - b2**t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_new, v_new\n```\n\nThis implementation:\n1. Handles both scalar and array inputs\n2. Initializes moments to zeros if they're None\n3. Computes the first and second moments\n4. Applies bias correction\n5. Updates the parameters according to the Adam formula\n6. Returns the updated parameters and moments"}
{"task_id": 479, "completion_id": 0, "completion": "I'll implement the epsilon-greedy algorithm for the multi-armed bandit problem as described. Here's the complete solution:\n\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Number of arms is the length of the first inner list\n    n_arms = len(rewards[0])\n    \n    # Initialize estimated values and pull counters\n    arm_values = [ev_prior] * n_arms\n    arm_pulls = [0] * n_arms\n    \n    # For each time step\n    for t in range(len(rewards)):\n        # Choose an arm\n        if np.random.random() < epsilon:\n            # Explore: choose a random arm\n            chosen_arm = np.random.randint(0, n_arms)\n        else:\n            # Exploit: choose the arm with the highest estimated value\n            # In case of ties, select the arm with the smallest index\n            max_value = max(arm_values)\n            chosen_arm = arm_values.index(max_value)\n        \n        # Get the reward for the chosen arm at this time step\n        reward = rewards[t][chosen_arm]\n        \n        # Update the pull counter for the chosen arm\n        arm_pulls[chosen_arm] += 1\n        \n        # Update the estimated value using incremental sample mean\n        arm_values[chosen_arm] += (reward - arm_values[chosen_arm]) / arm_pulls[chosen_arm]\n    \n    # Round the final estimates to 4 decimal places\n    return [round(value, 4) for value in arm_values]\n```\n\nThis implementation follows the algorithm steps as described:\n1. It initializes the estimated values for all arms with the provided prior value\n2. For each time step, it either explores (with probability epsilon) or exploits (choosing the arm with highest estimated value)\n3. It updates the chosen arm's estimated value using the incremental sample mean formula\n4. Finally, it returns the list of arm value estimates rounded to 4 decimal places\n\nThe code handles the random seed for reproducibility and properly breaks ties by selecting the arm with the smallest index when multiple arms have the same estimated value."}
{"task_id": 481, "completion_id": 0, "completion": "# DBSCAN Implementation from Scratch\n\nI'll implement the DBSCAN algorithm from scratch using only NumPy. The algorithm follows these steps:\n1. Find all neighbors for each point\n2. Identify core points (those with at least min_samples neighbors)\n3. Expand clusters from core points\n4. Label border points and noise points\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if len(data) == 0:\n        return []\n    \n    n_samples = data.shape[0]\n    \n    # Initialize all points as unvisited and unlabeled (noise)\n    visited = [False] * n_samples\n    labels = [-1] * n_samples\n    \n    # Find neighbors for each point\n    def find_neighbors(point_idx):\n        neighbors = []\n        for i in range(n_samples):\n            # Calculate Euclidean distance\n            distance = np.linalg.norm(data[point_idx] - data[i])\n            if distance <= eps:\n                neighbors.append(i)\n        return neighbors\n    \n    # Current cluster ID\n    cluster_id = 0\n    \n    # Process each point\n    for point_idx in range(n_samples):\n        # Skip if already visited\n        if visited[point_idx]:\n            continue\n        \n        # Mark as visited\n        visited[point_idx] = True\n        \n        # Find neighbors\n        neighbors = find_neighbors(point_idx)\n        \n        # If not enough neighbors, mark as noise (already -1)\n        if len(neighbors) < min_samples:\n            continue\n        \n        # Start a new cluster\n        labels[point_idx] = cluster_id\n        \n        # Process neighbors (expand cluster)\n        i = 0\n        while i < len(neighbors):\n            neighbor = neighbors[i]\n            \n            # If not visited, mark as visited and check if it's a core point\n            if not visited[neighbor]:\n                visited[neighbor] = True\n                \n                # Find neighbors of this neighbor\n                neighbor_neighbors = find_neighbors(neighbor)\n                \n                # If this is a core point, add its neighbors to the processing queue\n                if len(neighbor_neighbors) >= min_samples:\n                    neighbors.extend([nn for nn in neighbor_neighbors if nn not in neighbors])\n            \n            # If not yet part of a cluster, add to current cluster\n            if labels[neighbor] == -1:\n                labels[neighbor] = cluster_id\n                \n            i += 1\n        \n        # Move to next cluster\n        cluster_id += 1\n    \n    return labels\n```\n\nThis implementation follows the DBSCAN algorithm exactly as described:\n1. It identifies core points (those with at least min_samples points within eps distance)\n2. It expands clusters from core points\n3. Border points (reachable from core points but not core points themselves) get the cluster ID of the core point\n4. Points that aren't reachable from any core point remain labeled as -1 (noise)\n5. Clusters are numbered 0, 1, 2, ... in discovery order\n\nThe algorithm uses a breadth-first search approach to expand clusters from core points, ensuring that all reachable points are included in the same cluster."}
{"task_id": 482, "completion_id": 0, "completion": "# One-Hot Encoding Implementation\n\nHere's a complete implementation of the `to_categorical` function that converts integer labels to one-hot encoded format:\n\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    y = np.array(y, dtype=int)\n    \n    # Ensure y is 1D\n    if y.ndim > 1:\n        y = y.flatten()\n    \n    # Determine number of classes if not provided\n    if num_classes is None:\n        if len(y) == 0:\n            num_classes = 0\n        else:\n            num_classes = np.max(y) + 1\n    \n    # Validate that all labels are within range\n    if np.any(y < 0) or np.any(y >= num_classes):\n        raise ValueError(f\"Labels must be in range [0, {num_classes-1}]\")\n    \n    # Create output array filled with zeros\n    output = np.zeros((len(y), num_classes), dtype=int)\n    \n    # Set the appropriate positions to 1\n    for i, label in enumerate(y):\n        output[i, label] = 1\n    \n    return output\n```\n\nThis implementation:\n1. Converts the input to a NumPy array\n2. Ensures the input is 1-dimensional\n3. Determines the number of classes if not provided\n4. Validates that all labels are within the valid range\n5. Creates a zero-filled array of the appropriate shape\n6. Sets the appropriate positions to 1 based on the labels\n7. Returns the one-hot encoded matrix with integer dtype"}
{"task_id": 485, "completion_id": 0, "completion": "I'll implement a univariate regression tree that follows the CART algorithm for a single numerical feature. The implementation will recursively build a binary tree by finding the optimal split point that minimizes the sum of squared errors.\n\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    if not X_train:\n        return []\n    \n    # Convert inputs to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Define the recursive tree building function\n    def build_tree(X, y, depth):\n        # If we've reached max depth or have fewer than 2 samples, create a leaf\n        if depth >= max_depth or len(X) < 2:\n            return {\"is_leaf\": True, \"value\": np.mean(y)}\n        \n        best_sse = float('inf')\n        best_split = None\n        \n        # Sort X and y by X values\n        sorted_indices = np.argsort(X)\n        X_sorted = X[sorted_indices]\n        y_sorted = y[sorted_indices]\n        \n        # Try all possible split points\n        for i in range(1, len(X_sorted)):\n            # Only consider unique values as split points\n            if X_sorted[i] == X_sorted[i-1]:\n                continue\n                \n            split_point = (X_sorted[i-1] + X_sorted[i]) / 2\n            \n            # Split the data\n            left_mask = X <= split_point\n            right_mask = ~left_mask\n            \n            # Skip if either side is empty\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n            \n            # Calculate the mean for each side\n            left_mean = np.mean(y[left_mask])\n            right_mean = np.mean(y[right_mask])\n            \n            # Calculate SSE for this split\n            left_sse = np.sum((y[left_mask] - left_mean) ** 2)\n            right_sse = np.sum((y[right_mask] - right_mean) ** 2)\n            total_sse = left_sse + right_sse\n            \n            # Update best split if this one is better\n            if total_sse < best_sse:\n                best_sse = total_sse\n                best_split = {\n                    \"is_leaf\": False,\n                    \"split_point\": split_point,\n                    \"left\": {\"is_leaf\": True, \"value\": left_mean},\n                    \"right\": {\"is_leaf\": True, \"value\": right_mean}\n                }\n        \n        # If no valid split was found, return a leaf\n        if best_split is None:\n            return {\"is_leaf\": True, \"value\": np.mean(y)}\n        \n        # Recursively build subtrees\n        left_mask = X <= best_split[\"split_point\"]\n        right_mask = ~left_mask\n        \n        best_split[\"left\"] = build_tree(X[left_mask], y[left_mask], depth + 1)\n        best_split[\"right\"] = build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return best_split\n    \n    # Build the tree\n    tree = build_tree(X_train, y_train, 0)\n    \n    # Define prediction function\n    def predict(tree, x):\n        if tree[\"is_leaf\"]:\n            return tree[\"value\"]\n        \n        if x <= tree[\"split_point\"]:\n            return predict(tree[\"left\"], x)\n        else:\n            return predict(tree[\"right\"], x)\n    \n    # Make predictions for each test point\n    predictions = [predict(tree, x) for x in X_test]\n    \n    return predictions\n```\n\nThis implementation:\n1. Builds a binary tree using recursive greedy splitting\n2. Finds the optimal split point that minimizes the sum of squared errors\n3. Stops recursion when max_depth is reached or when a node has fewer than 2 samples\n4. Labels each leaf with the mean of target values in that leaf\n5. Returns predictions for test points by traversing the tree\n6. Returns an empty list if the training set is empty\n\nThe algorithm considers all possible split points between adjacent sorted feature values and selects the one that minimizes the sum of squared errors in the resulting child nodes."}
{"task_id": 490, "completion_id": 0, "completion": "# Vector to Diagonal Matrix Converter Solution\n\nHere's a complete solution for converting a one-dimensional vector into a square diagonal matrix:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Handle empty input case\n    if len(x) == 0:\n        return []\n    \n    # Convert input to numpy array if it's not already\n    if not isinstance(x, np.ndarray):\n        x = np.array(x)\n    \n    # Create diagonal matrix using numpy's diag function\n    diagonal_matrix = np.diag(x)\n    \n    # Convert numpy array to list of lists and return\n    return diagonal_matrix.tolist()\n```\n\nThis solution:\n1. Handles the empty input case by returning an empty list\n2. Converts the input to a NumPy array if it's not already one\n3. Uses NumPy's `diag` function to create a diagonal matrix\n4. Converts the resulting NumPy array to a list of lists using `tolist()`\n\nThe function works with lists, tuples, and NumPy arrays containing integers or floats."}
{"task_id": 491, "completion_id": 0, "completion": "# FP-Growth Algorithm Implementation\n\nI'll implement the FP-Growth algorithm for frequent itemset mining. This algorithm is more efficient than Apriori because it only requires two database scans.\n\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # Count item frequencies\n    item_counter = Counter()\n    for transaction in transactions:\n        item_counter.update(transaction)\n    \n    # Filter items that don't meet minimum support\n    frequent_items = {item: count for item, count in item_counter.items() if count >= min_sup}\n    \n    # If no items meet minimum support, return empty list\n    if not frequent_items:\n        return []\n    \n    # Sort frequent items by frequency (descending)\n    header_table = {item: [count, None] for item, count in \n                    sorted(frequent_items.items(), key=lambda x: (-x[1], x[0]))}\n    \n    # Create FP-tree root\n    fp_tree_root = FPNode(None, None, None)\n    \n    # Insert transactions into FP-tree\n    for transaction in transactions:\n        # Filter and sort items in transaction\n        filtered_items = [item for item in transaction if item in frequent_items]\n        if filtered_items:\n            filtered_items.sort(key=lambda x: (-frequent_items[x], x))\n            insert_tree(filtered_items, fp_tree_root, header_table, 1)\n    \n    # Mine the FP-tree recursively\n    frequent_itemsets = []\n    mine_tree(fp_tree_root, header_table, set(), frequent_itemsets, min_sup)\n    \n    # Sort the frequent itemsets by length and then lexicographically\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return frequent_itemsets\n\nclass FPNode:\n    \"\"\"A node in the FP tree.\"\"\"\n    \n    def __init__(self, item, count, parent):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.next = None\n\ndef insert_tree(items, node, header_table, count):\n    \"\"\"Insert a transaction into the FP tree.\"\"\"\n    if items:\n        item = items[0]\n        \n        # If the item is already a child of the current node, increment its count\n        if item in node.children:\n            node.children[item].count += count\n        else:\n            # Create a new node\n            new_node = FPNode(item, count, node)\n            node.children[item] = new_node\n            \n            # Link the new node to the header table\n            if header_table[item][1] is None:\n                header_table[item][1] = new_node\n            else:\n                current = header_table[item][1]\n                while current.next is not None:\n                    current = current.next\n                current.next = new_node\n        \n        # Recursively insert the rest of the items\n        insert_tree(items[1:], node.children[item], header_table, count)\n\ndef mine_tree(tree, header_table, prefix, frequent_itemsets, min_sup):\n    \"\"\"Mine the FP tree recursively.\"\"\"\n    # Process the header table in ascending order of item name\n    items = sorted(header_table.keys())\n    \n    for item in items:\n        # Current prefix is the combination of previous prefix and current item\n        new_prefix = prefix.union({item})\n        \n        # Add the new prefix to the list of frequent itemsets\n        frequent_itemsets.append(tuple(sorted(new_prefix)))\n        \n        # Construct conditional pattern base\n        conditional_pattern_base = []\n        node = header_table[item][1]\n        \n        while node is not None:\n            # Get the path from root to this node\n            path = []\n            count = node.count\n            parent = node.parent\n            \n            # Don't include the root node\n            while parent is not None and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            \n            if path:\n                # Add the path to the conditional pattern base with the count\n                conditional_pattern_base.append((path, count))\n            \n            node = node.next\n        \n        # Construct conditional FP tree\n        conditional_tree_root, conditional_header = build_conditional_tree(conditional_pattern_base, min_sup)\n        \n        # If the conditional tree has at least one path\n        if conditional_header:\n            # Check if the conditional tree consists of a single path\n            if is_single_path(conditional_tree_root):\n                # Generate all combinations of items in the path\n                path_items = list(conditional_header.keys())\n                for i in range(1, len(path_items) + 1):\n                    for comb in itertools.combinations(path_items, i):\n                        # Add the combination with the current prefix\n                        new_itemset = tuple(sorted(new_prefix.union(comb)))\n                        frequent_itemsets.append(new_itemset)\n            else:\n                # Recursively mine the conditional tree\n                mine_tree(conditional_tree_root, conditional_header, new_prefix, frequent_itemsets, min_sup)\n\ndef build_conditional_tree(conditional_pattern_base, min_sup):\n    \"\"\"Build a conditional FP tree from a conditional pattern base.\"\"\"\n    # Count item frequencies in the conditional pattern base\n    item_counter = Counter()\n    for path, count in conditional_pattern_base:\n        for item in path:\n            item_counter[item] += count\n    \n    # Filter items that don't meet minimum support\n    frequent_items = {item: count for item, count in item_counter.items() if count >= min_sup}\n    \n    # If no items meet minimum support, return empty tree\n    if not frequent_items:\n        return FPNode(None, None, None), {}\n    \n    # Create header table for the conditional tree\n    header_table = {item: [count, None] for item, count in \n                    sorted(frequent_items.items(), key=lambda x: (-x[1], x[0]))}\n    \n    # Create the root of the conditional tree\n    conditional_tree_root = FPNode(None, None, None)\n    \n    # Insert paths into the conditional tree\n    for path, count in conditional_pattern_base:\n        # Filter and sort items in the path\n        filtered_path = [item for item in path if item in frequent_items]\n        if filtered_path:\n            filtered_path.sort(key=lambda x: (-frequent_items[x], x))\n            insert_tree(filtered_path, conditional_tree_root, header_table, count)\n    \n    return conditional_tree_root, header_table\n\ndef is_single_path(node):\n    \"\"\"Check if the tree consists of a single path.\"\"\"\n    if not node.children:\n        return True\n    if len(node.children) > 1:\n        return False\n    return is_single_path(list(node.children.values())[0])\n```\n\nThis implementation follows the FP-Growth algorithm as described in the task:\n\n1. First, it counts item frequencies and removes infrequent items\n2. It builds the FP-tree by inserting each transaction\n3. It recursively mines the FP-tree to find all frequent itemsets\n4. It handles the special case of single-path conditional trees by generating all combinations directly\n5. The output is sorted first by length and then lexicographically, with items within each tuple also sorted lexicographically\n\nThe algorithm is efficient because it only scans the database twice - once to count item frequencies and once to build the FP-tree."}
{"task_id": 492, "completion_id": 0, "completion": "I'll implement the polynomial feature expansion function as requested. Here's the solution:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    if degree < 0:\n        return -1\n    \n    # Convert input to numpy array for easier manipulation\n    X_array = np.array(X)\n    n_samples, n_features = X_array.shape\n    \n    # Initialize result matrix with ones (for the constant term)\n    result = np.ones((n_samples, 1))\n    \n    # For each degree from 1 to the specified degree\n    for d in range(1, degree + 1):\n        # Generate all combinations of feature indices for the current degree\n        for combo in combinations_with_replacement(range(n_features), d):\n            # Create a new column for this combination\n            new_column = np.ones(n_samples)\n            \n            # For each feature index in the combination, multiply by the corresponding feature\n            for feature_idx in combo:\n                new_column *= X_array[:, feature_idx]\n            \n            # Add the new column to the result\n            result = np.column_stack((result, new_column))\n    \n    # Convert the result back to a list of lists\n    return result.tolist()\n```\n\nThis implementation:\n1. Handles the negative degree case by returning -1\n2. Converts the input to a numpy array for efficient operations\n3. Starts with a column of ones (the degree-0 term)\n4. For each degree from 1 to the specified degree:\n   - Generates all combinations of features using `combinations_with_replacement`\n   - Creates a new column for each combination by multiplying the appropriate features\n   - Adds the new column to the result matrix\n5. Converts the result back to a list of lists before returning\n\nThe implementation follows the specified ordering:\n- First by ascending total degree (0, then 1, ..., d)\n- Within the same degree, by lexicographic order of indices as produced by `itertools.combinations_with_replacement`"}
{"task_id": 493, "completion_id": 0, "completion": "I'll implement the mean-squared error function according to the specifications.\n\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    # Convert input to numpy array for consistent handling\n    y_array = np.array(y, dtype=float)\n    \n    # Check if the array is empty\n    if len(y_array) == 0:\n        return -1\n    \n    # Calculate the mean of the sample\n    mean = np.mean(y_array)\n    \n    # Calculate the squared differences from the mean\n    squared_diffs = (y_array - mean) ** 2\n    \n    # Calculate the MSE (average of squared differences)\n    mse_value = np.mean(squared_diffs)\n    \n    # Round to 4 decimal places\n    return round(mse_value, 4)\n```\n\nThis implementation:\n1. Converts the input to a NumPy array for consistent handling\n2. Checks if the array is empty and returns -1 if it is\n3. Calculates the mean of the sample\n4. Computes the squared differences between each element and the mean\n5. Calculates the MSE by taking the average of these squared differences\n6. Rounds the result to 4 decimal places as required"}
{"task_id": 496, "completion_id": 0, "completion": "I'll implement the `apply_affine` function according to the requirements. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    x_array = np.atleast_1d(np.array(x, dtype=float))\n    \n    # Calculate the affine function: f(x) = slope * x + intercept\n    y = slope * x_array + intercept\n    \n    # First derivative is constant: f'(x) = slope\n    grad = np.full_like(x_array, slope)\n    \n    # Second derivative is zero: f''(x) = 0\n    grad2 = np.zeros_like(x_array)\n    \n    # Round all values to 4 decimal places\n    y_rounded = [round(val, 4) for val in y]\n    grad_rounded = [round(val, 4) for val in grad]\n    grad2_rounded = [round(val, 4) for val in grad2]\n    \n    return y_rounded, grad_rounded, grad2_rounded\n```\n\nThis implementation:\n1. Converts the input to a numpy array and ensures it's at least 1D (handles scalar inputs)\n2. Calculates the affine function value for each element\n3. Creates arrays for the first derivative (constant slope) and second derivative (zeros)\n4. Rounds all values to 4 decimal places\n5. Returns the three lists as a tuple\n\nThe function handles both Python lists and NumPy arrays as input, and properly treats scalar inputs as length-one vectors."}
{"task_id": 499, "completion_id": 0, "completion": "I'll implement the PCA dimensionality reduction routine from scratch according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Check if data is 2D\n    if len(data.shape) != 2:\n        raise ValueError(\"Input data must be a 2D array\")\n    \n    n_samples, n_features = data.shape\n    \n    # Check if k is in the valid range\n    if k < 1 or k > n_features:\n        return -1\n    \n    # Step 1: Standardize each feature (zero mean, unit population variance)\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0, ddof=0)  # population standard deviation\n    \n    # Handle zero variance features\n    stds[stds == 0] = 1.0\n    \n    standardized_data = (data - means) / stds\n    \n    # Step 2: Compute the sample covariance matrix with Bessel's correction\n    cov_matrix = np.cov(standardized_data, rowvar=False)  # rowvar=False means each column is a variable\n    \n    # Step 3: Perform eigen-decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Step 4: Sort eigenvalues in descending order and arrange eigenvectors accordingly\n    idx = np.argsort(eigenvalues)[::-1]  # indices for descending order\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Step 5: Fix the sign of eigenvectors so that the entry with largest absolute value is positive\n    for i in range(eigenvectors.shape[1]):\n        max_abs_idx = np.argmax(np.abs(eigenvectors[:, i]))\n        if eigenvectors[max_abs_idx, i] < 0:\n            eigenvectors[:, i] = -eigenvectors[:, i]\n    \n    # Step 6: Project the standardized data onto the first k eigenvectors\n    principal_components = eigenvectors[:, :k]\n    projected_data = np.dot(standardized_data, principal_components)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(projected_data, 4).tolist()\n```\n\nThis implementation follows all the specified steps:\n1. Standardizes the data (zero mean, unit population variance)\n2. Computes the sample covariance matrix with Bessel's correction\n3. Performs eigen-decomposition of the covariance matrix\n4. Sorts eigenvalues in descending order and arranges eigenvectors accordingly\n5. Fixes the sign of eigenvectors so that the entry with largest absolute value is positive\n6. Projects the standardized data onto the first k eigenvectors\n7. Returns the result rounded to 4 decimal places as a Python list\n\nThe implementation also handles the edge case where a feature has zero variance and returns -1 if k is outside the valid range."}
{"task_id": 500, "completion_id": 0, "completion": "# Gradient Boosting Binary Classifier Implementation\n\nI'll implement a tiny gradient boosting binary classifier using decision stumps as weak learners. The implementation will follow the squared-error loss approach and build an additive model as specified.\n\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    n_samples, n_features = X_train.shape\n    \n    # Initialize the model with the average of training labels\n    c = np.mean(y_train)\n    F_train = np.full(n_samples, c)\n    \n    # List to store the weak learners\n    weak_learners = []\n    \n    # Boosting iterations\n    for _ in range(n_estimators):\n        # Calculate residuals\n        residuals = y_train - F_train\n        \n        # Find the best decision stump for the current residuals\n        best_stump = find_best_decision_stump(X_train, residuals)\n        weak_learners.append(best_stump)\n        \n        # Update the model predictions\n        predictions = apply_decision_stump(X_train, best_stump)\n        F_train += learning_rate * predictions\n    \n    # Make predictions on the test set\n    F_test = np.full(len(X_test), c)\n    for stump in weak_learners:\n        predictions = apply_decision_stump(X_test, stump)\n        F_test += learning_rate * predictions\n    \n    # Convert regression scores to binary predictions\n    y_pred = (F_test >= 0.5).astype(int)\n    \n    return y_pred.tolist()\n\ndef find_best_decision_stump(X, residuals):\n    \"\"\"Find the best decision stump to fit the residuals.\"\"\"\n    n_samples, n_features = X.shape\n    best_error = float('inf')\n    best_stump = {'feature': 0, 'threshold': 0, 'left_value': 0, 'right_value': 0}\n    \n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        # Use unique values as potential thresholds\n        thresholds = np.unique(feature_values)\n        \n        for threshold in thresholds:\n            # Split samples based on the threshold\n            left_mask = feature_values <= threshold\n            right_mask = ~left_mask\n            \n            # Skip if all samples fall on one side\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n            \n            # Calculate the optimal prediction values for each side\n            left_value = np.mean(residuals[left_mask])\n            right_value = np.mean(residuals[right_mask])\n            \n            # Calculate the squared error\n            predictions = np.zeros(n_samples)\n            predictions[left_mask] = left_value\n            predictions[right_mask] = right_value\n            error = np.sum((residuals - predictions) ** 2)\n            \n            if error < best_error:\n                best_error = error\n                best_stump = {\n                    'feature': feature_idx,\n                    'threshold': threshold,\n                    'left_value': left_value,\n                    'right_value': right_value\n                }\n    \n    return best_stump\n\ndef apply_decision_stump(X, stump):\n    \"\"\"Apply a decision stump to make predictions.\"\"\"\n    feature_idx = stump['feature']\n    threshold = stump['threshold']\n    left_value = stump['left_value']\n    right_value = stump['right_value']\n    \n    feature_values = X[:, feature_idx]\n    predictions = np.zeros(len(X))\n    predictions[feature_values <= threshold] = left_value\n    predictions[feature_values > threshold] = right_value\n    \n    return predictions\n```\n\nThis implementation:\n1. Initializes the model with the average of training labels\n2. For each boosting iteration:\n   - Calculates residuals\n   - Finds the best decision stump to fit these residuals\n   - Updates the model predictions\n3. Makes predictions on the test set by applying all weak learners\n4. Converts regression scores to binary predictions by thresholding at 0.5\n\nThe decision stump finding algorithm:\n1. For each feature and potential threshold:\n   - Splits the samples into left and right groups\n   - Calculates the optimal prediction values for each group (mean of residuals)\n   - Evaluates the squared error\n   - Keeps track of the best stump"}
{"task_id": 505, "completion_id": 0, "completion": "I'll implement the contrastive divergence algorithm for a Restricted Boltzmann Machine (RBM) as requested. This algorithm is used to train RBMs by approximating the gradient of the log-likelihood.\n\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    # Handle empty mini-batch case\n    if X.shape[0] == 0:\n        return []\n    \n    m = X.shape[0]  # batch size\n    \n    # Compute positive phase hidden probabilities\n    h0_prob = sigmoid(np.dot(X, W) + hbias)\n    \n    # Initialize the chain with the positive phase hidden probabilities\n    h_k = h0_prob\n    \n    # Perform k steps of Gibbs sampling\n    for _ in range(k):\n        # Compute visible probabilities given hidden\n        v_k_prob = sigmoid(np.dot(h_k, W.T) + vbias)\n        \n        # Compute hidden probabilities given visible\n        h_k = sigmoid(np.dot(v_k_prob, W) + hbias)\n    \n    # Compute positive gradient (from data)\n    pos_grad = np.dot(X.T, h0_prob)\n    \n    # Compute negative gradient (from model)\n    neg_grad = np.dot(v_k_prob.T, h_k)\n    \n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    \n    # Round to 4 decimal places and convert to list of lists\n    W_rounded = np.round(W_new, 4).tolist()\n    \n    return W_rounded\n\ndef sigmoid(x):\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-x))\n```\n\nThis implementation follows the contrastive divergence algorithm for RBMs:\n\n1. It computes the positive phase by calculating hidden unit probabilities given the input data\n2. It performs k steps of Gibbs sampling, alternating between visible and hidden units\n3. It computes the positive and negative gradients\n4. It updates the weights using the learning rate and batch size\n5. It rounds the results to 4 decimal places and returns them as a list of lists\n\nThe sigmoid function is used to compute the activation probabilities for both visible and hidden units."}
{"task_id": 509, "completion_id": 0, "completion": "# Chebyshev Distance Calculator Solution\n\nI'll implement a function to calculate the Chebyshev distance between two vectors, following all the requirements.\n\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    x_array = np.asarray(x)\n    y_array = np.asarray(y)\n    \n    # Check if the vectors have the same length\n    if len(x_array) != len(y_array):\n        return -1.0\n    \n    # Calculate the component-wise absolute differences\n    abs_diff = np.abs(x_array - y_array)\n    \n    # Find the maximum difference (Chebyshev distance)\n    max_diff = np.max(abs_diff)\n    \n    # Round to 4 decimal places and return as a standard Python float\n    return round(float(max_diff), 4)\n```\n\nThis implementation:\n1. Converts the inputs to NumPy arrays for efficient computation\n2. Checks if the vectors have the same length, returning -1 if they don't\n3. Calculates the absolute differences between corresponding elements\n4. Finds the maximum of these differences (the Chebyshev distance)\n5. Rounds the result to 4 decimal places and returns it as a standard Python float"}
{"task_id": 510, "completion_id": 0, "completion": "I'll implement the VAE loss function for Bernoulli visible units according to the specifications.\n\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # Get batch size\n    batch_size = y.shape[0]\n    \n    # Clip y_pred for numerical stability\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1.0 - epsilon)\n    \n    # Compute reconstruction loss (binary cross-entropy)\n    reconstruction_loss = -np.sum(\n        y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    )\n    \n    # Compute KL divergence\n    kl_divergence = -0.5 * np.sum(\n        1 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    )\n    \n    # Compute total loss (average per sample)\n    total_loss = (reconstruction_loss + kl_divergence) / batch_size\n    \n    # Round to 6 decimal places\n    return round(float(total_loss), 6)\n```\n\nThis implementation:\n1. Clips the predicted values to avoid numerical issues when taking logarithms\n2. Computes the binary cross-entropy reconstruction loss\n3. Computes the KL divergence between the approximate posterior and the unit Gaussian prior\n4. Averages the total loss over the mini-batch\n5. Rounds the result to 6 decimal places as required"}
{"task_id": 513, "completion_id": 0, "completion": "I'll implement the Factorization Machine prediction function according to the formula and requirements.\n\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X_array = np.array(X)\n    w_array = np.array(w)\n    V_array = np.array(V)\n    \n    # Get dimensions\n    n_samples = X_array.shape[0]\n    n_features = X_array.shape[1]\n    k = V_array.shape[1]\n    \n    # Initialize predictions with bias term\n    predictions = np.full(n_samples, w0)\n    \n    # Add linear terms: \u03a3\u2c7c w\u2c7c x\u2c7c\n    predictions += X_array.dot(w_array)\n    \n    # Compute interaction terms: \u00bd \u03a3_{f=1}^{k} [ (\u03a3\u2c7c V_{j,f} x\u2c7c)\u00b2 \u2212 \u03a3\u2c7c V_{j,f}\u00b2 x\u2c7c\u00b2 ]\n    for f in range(k):\n        # Get the f-th latent factor for all features\n        v_f = V_array[:, f]\n        \n        # Compute (\u03a3\u2c7c V_{j,f} x\u2c7c)\u00b2\n        sum_vx_squared = np.square(X_array.dot(v_f))\n        \n        # Compute \u03a3\u2c7c V_{j,f}\u00b2 x\u2c7c\u00b2\n        v_squared_x_squared = np.sum(np.square(v_f) * np.square(X_array), axis=1)\n        \n        # Add the difference to predictions\n        predictions += 0.5 * (sum_vx_squared - v_squared_x_squared)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_predictions = np.round(predictions, 4).tolist()\n    \n    return rounded_predictions\n```\n\nThis implementation:\n1. Converts the input lists to NumPy arrays for efficient computation\n2. Calculates the linear terms using matrix multiplication\n3. Computes the interaction terms for each latent factor dimension\n4. Rounds the results to 4 decimal places as required\n5. Returns the predictions as a list of floats\n\nThe function handles arbitrary numbers of samples, features, and latent factors as specified in the requirements."}
{"task_id": 517, "completion_id": 0, "completion": "# Sigmoid Activation Function Implementation\n\nI'll implement a numerically stable sigmoid function that handles various input types and rounds to 4 decimal places.\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Convert input to numpy array for consistent handling\n    is_scalar = np.isscalar(z)\n    z_array = np.array(z)\n    \n    # Numerically stable sigmoid implementation\n    # For large negative values, sigmoid approaches 0\n    # For large positive values, sigmoid approaches 1\n    # For values in between, compute the standard formula\n    result = np.zeros_like(z_array, dtype=float)\n    \n    # Handle large positive values\n    pos_mask = z_array >= 0\n    neg_mask = ~pos_mask\n    \n    # For positive values: 1 / (1 + exp(-z))\n    result[pos_mask] = 1.0 / (1.0 + np.exp(-z_array[pos_mask]))\n    \n    # For negative values: exp(z) / (1 + exp(z))\n    # This avoids overflow for large negative values\n    exp_z = np.exp(z_array[neg_mask])\n    result[neg_mask] = exp_z / (1.0 + exp_z)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return in the appropriate format\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n```\n\nThis implementation:\n1. Handles scalar, list/tuple, and NumPy array inputs\n2. Uses a numerically stable approach by handling positive and negative values separately\n3. Rounds results to 4 decimal places\n4. Returns the appropriate type based on the input (scalar or list)\n5. Avoids overflow for large values of |z|\n\nFor example, `sigmoid([-1, 0, 1])` will return `[0.2689, 0.5, 0.7311]` as required."}
{"task_id": 518, "completion_id": 0, "completion": "I'll create a decorator that automatically converts one-hot encoded NumPy arrays to their integer label representation before passing them to the metric function.\n\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    # =======  Write your code below  =======\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Process actual array\n        if len(actual.shape) == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        \n        # Process predicted array\n        if len(predicted.shape) == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        \n        # Call the original function with processed arrays\n        return function(actual, predicted)\n    # =======  End of your code  =======\n    \n    return wrapper\n\n\n# Example usage\n@unhot\ndef accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Simple accuracy rounded to 4 decimals.\"\"\"\n    return float(np.round(np.mean(actual == predicted), 4))\n\n# Test with one-hot encoded arrays\nactual = np.array([[0,1,0], [1,0,0], [0,0,1]])  # one-hot\npredicted = np.array([[0,1,0], [0,1,0], [0,0,1]])  # one-hot\nprint(accuracy(actual, predicted))  # Should print 0.6667\n\n# Test with already 1D arrays\nactual_1d = np.array([1, 0, 2])\npredicted_1d = np.array([1, 1, 2])\nprint(accuracy(actual_1d, predicted_1d))  # Should also print 0.6667\n\n# Test with (n,1) shaped arrays\nactual_n1 = np.array([[1], [0], [2]])\npredicted_n1 = np.array([[1], [1], [2]])\nprint(accuracy(actual_n1, predicted_n1))  # Should also print 0.6667\n```\n\nThis implementation:\n1. Checks if each input array is 2D with a second dimension larger than 1\n2. If so, converts it to a 1D array using argmax along axis 1\n3. Leaves 1D arrays and arrays with shape (n,1) unchanged\n4. Calls the original function with the processed arrays\n5. Returns the result unmodified\n\nThe decorator works with any metric function that takes two NumPy arrays as positional arguments."}
{"task_id": 520, "completion_id": 0, "completion": "I'll implement the sigmoid gradient function according to the requirements. The function needs to handle different input types (scalar, list, or NumPy array) and return the same type with the gradient values.\n\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Handle scalar input\n    if isinstance(z, (int, float)):\n        return float(np.round(z * (1 - z), 4))\n    \n    # Handle empty list\n    if isinstance(z, list) and len(z) == 0:\n        return []\n    \n    # Handle NumPy array\n    if isinstance(z, np.ndarray):\n        return np.round(z * (1 - z), 4)\n    \n    # Handle list (potentially nested)\n    if isinstance(z, list):\n        # Check if it's a flat list of numbers\n        if all(isinstance(item, (int, float)) for item in z):\n            return [float(np.round(item * (1 - item), 4)) for item in z]\n        # Handle nested list\n        else:\n            return [sigmoid_grad(item) for item in z]\n    \n    # If we get here, the input type is not supported\n    raise TypeError(\"Input must be a float, list, or NumPy array\")\n```\n\nThis implementation:\n1. Handles scalar inputs by returning a rounded float\n2. Returns an empty list for empty list inputs\n3. Processes NumPy arrays using vectorized operations\n4. Handles both flat and nested lists recursively\n5. Rounds all results to 4 decimal places as required\n6. Returns the same type/structure as the input\n\nThe function computes the sigmoid gradient formula z*(1-z) for each element in the input."}
{"task_id": 528, "completion_id": 0, "completion": "I'll implement the `decision_boundary_grid` function according to the requirements. This function will create a grid spanning the data points in `X`, and then classify each point in the grid using the linear classifier defined by weights `W` and bias `b`.\n\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Convert X to numpy array for easier manipulation\n    X_array = np.array(X)\n    \n    # Find min and max values for each dimension\n    x1_min, x1_max = X_array[:, 0].min(), X_array[:, 0].max()\n    x2_min, x2_max = X_array[:, 1].min(), X_array[:, 1].max()\n    \n    # Create equally spaced arrays for each dimension\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create mesh grid\n    X1_grid, X2_grid = np.meshgrid(x1_plot, x2_plot)\n    \n    # Initialize the result grid\n    result_grid = []\n    \n    # Classify each point in the grid\n    for i in range(grid_n):\n        row = []\n        for j in range(grid_n):\n            # Get the current grid point\n            x1 = X1_grid[i, j]\n            x2 = X2_grid[i, j]\n            \n            # Apply the classification rule\n            # sign(w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b)\n            z = W[0] * x1 + W[1] * x2 + b\n            \n            # Determine the label: 1 if z \u2265 0, -1 otherwise\n            label = 1 if z >= 0 else -1\n            \n            row.append(label)\n        \n        result_grid.append(row)\n    \n    return result_grid\n```\n\nThis implementation:\n1. Converts the input data to a NumPy array for easier manipulation\n2. Finds the min and max values for each dimension to determine the grid boundaries\n3. Creates equally spaced arrays for each dimension using `np.linspace`\n4. Creates a mesh grid using `np.meshgrid`\n5. Iterates through each point in the grid, applies the classification rule, and stores the result\n6. Returns a nested Python list containing the classification results (1 or -1) for each grid point\n\nThe function follows the specified classification rule: a point (x\u2081, x\u2082) is labeled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and with -1 otherwise."}
{"task_id": 537, "completion_id": 0, "completion": "# Gaussian Naive Bayes Classifier Implementation\n\nI'll implement a Gaussian Naive Bayes classifier from scratch following the requirements. This implementation will:\n1. Calculate class priors\n2. Compute mean and standard deviation for each feature per class\n3. Calculate posterior probabilities using Gaussian likelihood\n4. Predict the class with the highest posterior probability\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    # Get unique classes and number of features\n    classes = np.unique(y_train)\n    n_features = X_train.shape[1]\n    epsilon = 1e-9  # Small constant to avoid division by zero\n    \n    # Calculate class priors\n    class_priors = {}\n    for c in classes:\n        class_priors[c] = np.sum(y_train == c) / len(y_train)\n    \n    # Calculate mean and standard deviation for each feature per class\n    class_stats = {}\n    for c in classes:\n        # Get samples belonging to this class\n        X_c = X_train[y_train == c]\n        \n        # Calculate mean and std for each feature\n        means = np.mean(X_c, axis=0)\n        stds = np.std(X_c, axis=0) + epsilon  # Add epsilon to avoid division by zero\n        \n        class_stats[c] = {'mean': means, 'std': stds}\n    \n    # Prediction phase\n    predictions = []\n    \n    for x in X_test:\n        # Calculate posterior probability for each class\n        posteriors = {}\n        \n        for c in classes:\n            # Start with log of prior probability\n            log_posterior = np.log(class_priors[c])\n            \n            # Add log of conditional probabilities for each feature\n            means = class_stats[c]['mean']\n            stds = class_stats[c]['std']\n            \n            # Calculate log of Gaussian PDF for each feature\n            for i in range(n_features):\n                # Log of Gaussian PDF: -log(sqrt(2\u03c0)\u03c3) - (x-\u03bc)\u00b2/(2\u03c3\u00b2)\n                log_likelihood = -np.log(np.sqrt(2 * np.pi) * stds[i]) - ((x[i] - means[i]) ** 2) / (2 * stds[i] ** 2)\n                log_posterior += log_likelihood\n            \n            posteriors[c] = log_posterior\n        \n        # Predict the class with the highest posterior probability\n        predicted_class = max(posteriors, key=posteriors.get)\n        predictions.append(predicted_class)\n    \n    return predictions\n```\n\nThis implementation:\n- Calculates class priors as the relative frequency of each class\n- Computes the mean and standard deviation for each feature per class\n- Uses log probabilities to avoid numerical underflow\n- Adds a small epsilon (1e-9) to standard deviations to prevent division by zero\n- Predicts the class with the highest posterior probability for each test sample\n\nThe function returns a list of integers (0 or 1) with the same length as X_test."}
{"task_id": 539, "completion_id": 0, "completion": "I'll implement the binary cross-entropy cost function according to the specifications.\n\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Compute the binary cross-entropy cost function.\n    \n    Parameters:\n    AL -- probability vector of shape (1, m) or (m,) corresponding to the predicted labels\n    Y -- true label vector of shape (1, m) or (m,) containing 0 or 1\n    \n    Returns:\n    cost -- binary cross-entropy cost as a Python float\n    \"\"\"\n    # Get the number of examples\n    m = Y.shape[0] if Y.ndim == 1 else Y.shape[1]\n    \n    # Clip probabilities to avoid numerical issues\n    epsilon = 1e-15\n    AL_clipped = np.clip(AL, epsilon, 1 - epsilon)\n    \n    # Compute the binary cross-entropy cost\n    # J = \u2212 1\u2044m \u00b7 \u03a3 [ y\u00b7ln(a) + (1\u2212y)\u00b7ln(1\u2212a) ]\n    logprobs = np.multiply(Y, np.log(AL_clipped)) + np.multiply(1 - Y, np.log(1 - AL_clipped))\n    cost = -np.sum(logprobs) / m\n    \n    # Return as a Python float (not a numpy array)\n    return float(cost)\n```\n\nThis implementation:\n1. Accepts two NumPy arrays `AL` and `Y` of identical shape (1,m) or (m,)\n2. Clips the probabilities to avoid numerical issues when taking logarithms\n3. Computes the binary cross-entropy cost according to the formula\n4. Returns the cost as a Python float, not a NumPy array\n\nThe function handles both (1,m) and (m,) shaped arrays correctly by determining the number of examples based on the shape of the input arrays."}
{"task_id": 552, "completion_id": 0, "completion": "I'll implement the function to compute the posterior state distribution (gamma) for a Hidden Markov Model using the forward-backward algorithm.\n\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    # Check if t is within valid range\n    if t < 0 or t >= len(observations):\n        return []\n    \n    # Extract HMM parameters\n    pi = np.array(hmm['pi'])  # Initial state distribution\n    A = np.array(hmm['A'])    # Transition matrix\n    B = np.array(hmm['B'])    # Emission matrix\n    \n    N = len(pi)  # Number of states\n    T = len(observations)  # Length of observation sequence\n    \n    # Initialize alpha (forward) and beta (backward) matrices\n    alpha = np.zeros((T, N))\n    beta = np.zeros((T, N))\n    \n    # Forward algorithm (compute alpha)\n    # Initialization\n    alpha[0, :] = pi * B[:, observations[0]]\n    \n    # Induction\n    for t_idx in range(1, T):\n        for j in range(N):\n            alpha[t_idx, j] = np.sum(alpha[t_idx-1, :] * A[:, j]) * B[j, observations[t_idx]]\n    \n    # Backward algorithm (compute beta)\n    # Initialization\n    beta[T-1, :] = 1.0\n    \n    # Induction\n    for t_idx in range(T-2, -1, -1):\n        for i in range(N):\n            beta[t_idx, i] = np.sum(A[i, :] * B[:, observations[t_idx+1]] * beta[t_idx+1, :])\n    \n    # Compute gamma at time t\n    gamma_t = alpha[t, :] * beta[t, :]\n    \n    # Normalize gamma\n    gamma_t = gamma_t / np.sum(gamma_t)\n    \n    # Round to 4 decimal places and convert to list\n    gamma_t_rounded = [round(prob, 4) for prob in gamma_t]\n    \n    return gamma_t_rounded\n```\n\nThis implementation:\n1. Checks if the time index t is valid\n2. Computes the forward probabilities (alpha) using the forward algorithm\n3. Computes the backward probabilities (beta) using the backward algorithm\n4. Combines alpha and beta to compute gamma at time t\n5. Normalizes gamma to ensure probabilities sum to 1\n6. Rounds each probability to 4 decimal places and returns the result as a list\n\nThe function handles all the required steps and follows the mathematical formulation provided in the task description."}
{"task_id": 555, "completion_id": 0, "completion": "# DBSCAN Implementation\n\nI'll implement the DBSCAN algorithm from scratch following the specified requirements. The implementation will include computing pairwise distances, identifying core points, and expanding clusters using breadth-first search.\n\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample. Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    \n    # Step 1: Compute pairwise Euclidean distances\n    # We'll use a more memory-efficient approach by computing distances on-demand\n    def euclidean_distance(point1, point2):\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n    \n    # Step 2: Build neighborhood lists for each sample\n    neighborhoods = []\n    for i in range(n_samples):\n        neighbors = []\n        for j in range(n_samples):\n            if euclidean_distance(data[i], data[j]) <= eps:\n                neighbors.append(j)\n        neighborhoods.append(neighbors)\n    \n    # Step 3: Identify core points\n    is_core = [len(neighbors) >= min_samples for neighbors in neighborhoods]\n    \n    # Initialize labels: -2 = unvisited, -1 = noise, 1+ = cluster labels\n    labels = [-2] * n_samples\n    current_cluster = 0\n    \n    # Step 4: Expand clusters with BFS\n    for i in range(n_samples):\n        # Skip if the point is already processed\n        if labels[i] != -2:\n            continue\n        \n        # If not a core point, mark as noise for now\n        if not is_core[i]:\n            labels[i] = -1\n            continue\n        \n        # Start a new cluster\n        current_cluster += 1\n        labels[i] = current_cluster\n        \n        # Initialize queue for BFS\n        queue = deque([i])\n        \n        # Process the queue\n        while queue:\n            current_point = queue.popleft()\n            \n            # Check neighbors\n            for neighbor in neighborhoods[current_point]:\n                # If neighbor is unvisited\n                if labels[neighbor] == -2:\n                    # Add to current cluster\n                    labels[neighbor] = current_cluster\n                    \n                    # If neighbor is a core point, add to queue for expansion\n                    if is_core[neighbor]:\n                        queue.append(neighbor)\n    \n    # Step 5: Ensure all unvisited points are marked as noise\n    for i in range(n_samples):\n        if labels[i] == -2:\n            labels[i] = -1\n    \n    return labels\n```\n\nThis implementation follows the DBSCAN algorithm as specified:\n1. It computes Euclidean distances between points\n2. Builds neighborhood lists for each sample\n3. Identifies core points based on the minimum samples threshold\n4. Uses BFS to expand clusters from core points\n5. Marks remaining unvisited points as noise\n\nThe algorithm returns a list of cluster labels where clusters are numbered starting from 1, and noise points are labeled as -1."}
{"task_id": 561, "completion_id": 0, "completion": "I'll implement the Gaussian Mixture Model using the Expectation-Maximization algorithm as requested. Here's the complete solution:\n\n```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Implements the Expectation-Maximization algorithm for a Gaussian Mixture Model.\n    \n    Args:\n        data: A 2-D NumPy array of shape (N, D) containing N samples with D features.\n        k: The number of Gaussian components.\n        max_iter: Maximum number of iterations for the EM algorithm.\n        epsilon: Convergence threshold for the change in mixture weights.\n        \n    Returns:\n        A list of length N containing the final cluster label of each sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Handle the special case of k=1\n    if k == 1:\n        return [0] * n_samples\n    \n    # Initialize parameters\n    # Mixture weights\n    weights = np.ones(k) / k\n    \n    # Means - randomly select k data points as initial means\n    indices = np.random.choice(n_samples, k, replace=False)\n    means = data[indices].copy()\n    \n    # Covariance matrices - initialize with identity matrices scaled by data variance\n    covariances = np.array([np.eye(n_features) * np.var(data, axis=0).mean() for _ in range(k)])\n    \n    # Initialize log-likelihood\n    prev_log_likelihood = -np.inf\n    \n    # EM algorithm\n    for iteration in range(max_iter):\n        # E-step: Calculate responsibilities\n        responsibilities = np.zeros((n_samples, k))\n        \n        for j in range(k):\n            # Calculate multivariate normal density for each data point\n            diff = data - means[j]\n            inv_cov = np.linalg.inv(covariances[j])\n            det_cov = np.linalg.det(covariances[j])\n            \n            # Handle numerical issues with determinant\n            if det_cov < 1e-10:\n                det_cov = 1e-10\n                \n            exponent = -0.5 * np.sum(np.dot(diff, inv_cov) * diff, axis=1)\n            norm_const = 1.0 / (np.power(2 * np.pi, n_features / 2) * np.sqrt(det_cov))\n            responsibilities[:, j] = weights[j] * norm_const * np.exp(exponent)\n        \n        # Normalize responsibilities\n        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n        # Handle numerical issues\n        responsibilities_sum[responsibilities_sum < 1e-10] = 1e-10\n        responsibilities /= responsibilities_sum\n        \n        # Calculate log-likelihood\n        log_likelihood = np.sum(np.log(np.sum(responsibilities * responsibilities_sum, axis=1)))\n        \n        # Check for convergence\n        if abs(log_likelihood - prev_log_likelihood) < epsilon:\n            break\n        \n        prev_log_likelihood = log_likelihood\n        \n        # M-step: Update parameters\n        # Calculate effective number of points assigned to each component\n        N_k = np.sum(responsibilities, axis=0)\n        \n        # Update weights\n        old_weights = weights.copy()\n        weights = N_k / n_samples\n        \n        # Update means\n        for j in range(k):\n            means[j] = np.sum(responsibilities[:, j, np.newaxis] * data, axis=0) / N_k[j]\n        \n        # Update covariances\n        for j in range(k):\n            diff = data - means[j]\n            covariances[j] = np.dot((responsibilities[:, j, np.newaxis] * diff).T, diff) / N_k[j]\n            \n            # Add a small regularization to avoid singular matrices\n            covariances[j] += 1e-6 * np.eye(n_features)\n        \n        # Check for convergence based on weight changes\n        if np.max(np.abs(weights - old_weights)) < epsilon:\n            break\n    \n    # Assign each sample to the component with highest responsibility\n    cluster_labels = np.argmax(responsibilities, axis=1)\n    \n    # Sort components by the first coordinate of their mean in ascending order\n    sorted_indices = np.argsort(means[:, 0])\n    \n    # Create a mapping from old to new labels\n    label_map = {old_label: new_label for new_label, old_label in enumerate(sorted_indices)}\n    \n    # Apply the mapping to get the final labels\n    final_labels = [label_map[label] for label in cluster_labels]\n    \n    return final_labels\n```\n\nThis implementation:\n\n1. Initializes the GMM parameters (weights, means, covariances) randomly\n2. Implements the E-step by calculating the responsibilities (posterior probabilities)\n3. Implements the M-step by updating the model parameters\n4. Checks for convergence based on log-likelihood or weight changes\n5. Assigns each sample to the component with the highest responsibility\n6. Makes the output deterministic by sorting components by the first coordinate of their means\n7. Handles the special case of k=1 as required\n\nThe code includes safeguards against numerical issues like singular covariance matrices and zero probabilities. The final cluster labels are returned as a list of integers from 0 to K-1 with no gaps, as required."}
{"task_id": 562, "completion_id": 0, "completion": "I'll implement the spectral clustering algorithm as requested. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    N = data.shape[0]\n    \n    # Handle the special case of n_cluster = 1\n    if n_cluster == 1:\n        return [0] * N\n    \n    # Step 1: Build similarity graph using Gaussian kernel\n    # Compute pairwise squared distances\n    squared_dists = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            diff = data[i] - data[j]\n            squared_dists[i, j] = np.sum(diff * diff)\n    \n    # Apply Gaussian kernel to get similarity matrix W\n    W = np.exp(-gamma * squared_dists)\n    \n    # Step 2: Construct the Laplacian matrix\n    # Compute degree matrix D (diagonal)\n    D = np.diag(np.sum(W, axis=1))\n    \n    # Compute Laplacian L = D - W\n    L = D - W\n    \n    # If normalized method is chosen, compute normalized Laplacian\n    if method == 'normalized':\n        # Compute D^(-1/2)\n        D_sqrt_inv = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))  # Add small constant to avoid division by zero\n        # Compute normalized Laplacian: L_sym = D^(-1/2) * L * D^(-1/2)\n        L = D_sqrt_inv @ L @ D_sqrt_inv\n    \n    # Step 3: Compute eigenvectors corresponding to the n_cluster smallest eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Select the eigenvectors corresponding to the n_cluster smallest eigenvalues\n    selected_eigenvectors = eigenvectors[:, :n_cluster]\n    \n    # If normalized method, row-normalize the eigenvector matrix\n    if method == 'normalized':\n        row_norms = np.sqrt(np.sum(selected_eigenvectors * selected_eigenvectors, axis=1))\n        selected_eigenvectors = selected_eigenvectors / row_norms[:, np.newaxis]\n    \n    # Step 4: Run k-means in the eigenvector space\n    # Initialize centroids as the first n_cluster samples\n    centroids = selected_eigenvectors[:n_cluster].copy()\n    \n    # Run k-means\n    labels = np.zeros(N, dtype=int)\n    old_labels = np.ones(N, dtype=int) * -1\n    \n    # Iterate until convergence\n    while not np.array_equal(labels, old_labels):\n        old_labels = labels.copy()\n        \n        # Assign points to nearest centroid\n        for i in range(N):\n            min_dist = float('inf')\n            for j in range(n_cluster):\n                dist = np.sum((selected_eigenvectors[i] - centroids[j]) ** 2)\n                if dist < min_dist:\n                    min_dist = dist\n                    labels[i] = j\n        \n        # Update centroids\n        for j in range(n_cluster):\n            points_in_cluster = selected_eigenvectors[labels == j]\n            if len(points_in_cluster) > 0:\n                centroids[j] = np.mean(points_in_cluster, axis=0)\n    \n    # Relabel clusters so that the cluster containing the smallest original index gets label 0, etc.\n    new_labels = np.zeros(N, dtype=int)\n    cluster_min_indices = {}\n    \n    for j in range(n_cluster):\n        cluster_indices = np.where(labels == j)[0]\n        if len(cluster_indices) > 0:\n            cluster_min_indices[j] = np.min(cluster_indices)\n    \n    # Sort clusters by their minimum index\n    sorted_clusters = sorted(cluster_min_indices.items(), key=lambda x: x[1])\n    \n    # Create mapping from old labels to new labels\n    label_mapping = {old_label: new_label for new_label, (old_label, _) in enumerate(sorted_clusters)}\n    \n    # Apply mapping\n    for i in range(N):\n        new_labels[i] = label_mapping[labels[i]]\n    \n    return new_labels.tolist()\n```\n\nThis implementation follows the steps outlined in the task:\n1. Builds a similarity graph using the Gaussian kernel\n2. Constructs the Laplacian matrix (either unnormalized or normalized)\n3. Computes eigenvectors for the smallest eigenvalues\n4. Runs k-means on the eigenvector space with deterministic initialization\n5. Relabels clusters based on the smallest original index\n6. Returns the cluster labels as a Python list\n\nThe code handles the special case of n_cluster=1 and includes safeguards against numerical issues like division by zero."}
{"task_id": 563, "completion_id": 0, "completion": "I'll implement the dual-form Perceptron learning algorithm as described in the task. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N = X_data.shape[0]  # Number of samples\n    \n    # Initialize alpha and bias\n    alpha = np.zeros(N)\n    b = 0.0\n    \n    # Build the Gram matrix\n    G = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            G[i, j] = np.dot(X_data[i], X_data[j])\n    \n    # Training loop\n    for iteration in range(max_iter):\n        updates = 0\n        \n        # Loop through all samples\n        for i in range(N):\n            # Compute g = \u03a3\u2c7c \u03b1\u2c7c y\u2c7c G[j, i]\n            g = 0.0\n            for j in range(N):\n                g += alpha[j] * y_data[j] * G[j, i]\n            \n            # Check if an update is needed\n            if y_data[i] * (g + b) <= 0:\n                # Update alpha and bias\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updates += 1\n        \n        # If no updates were made in this pass, we've converged\n        if updates == 0:\n            break\n    \n    # Recover the weight vector: w = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62\n    w = np.zeros(X_data.shape[1])\n    for i in range(N):\n        w += alpha[i] * y_data[i] * X_data[i]\n    \n    # Round to 4 decimal places\n    w = np.round(w, 4)\n    b = round(b, 4)\n    \n    return (w.tolist(), b)\n```\n\nThis implementation follows the algorithm exactly as specified:\n1. It initializes alpha and bias to zeros\n2. Builds the Gram matrix with dot products between all pairs of samples\n3. Iterates through the training data, updating alpha and bias when a misclassification occurs\n4. Stops when either no updates are made in a complete pass or max_iter is reached\n5. Recovers the weight vector from the dual coefficients\n6. Rounds all values to 4 decimal places and returns the result in the required format"}
{"task_id": 564, "completion_id": 0, "completion": "I'll implement the `predict_sample` function according to the requirements. The function will traverse the decision tree based on the given rules for classification and regression trees.\n\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    # If we've reached a leaf node, return its value\n    if hasattr(root, 'y'):\n        return root.y\n    \n    # Get the feature value to inspect\n    feature_value = features[root.label]\n    \n    if task_type == 'classification':\n        # For classification trees, find the child with matching x value\n        for child in root.child:\n            if hasattr(child, 'x') and child.x == feature_value:\n                return predict_sample(child, features, task_type)\n        \n        # Default to the second child if no match is found\n        return predict_sample(root.child[1], features, task_type)\n    \n    elif task_type == 'regression':\n        # For regression trees, compare feature value with split point\n        if feature_value <= root.s:\n            return predict_sample(root.child[0], features, task_type)\n        else:\n            return predict_sample(root.child[1], features, task_type)\n```\n\nThis implementation follows the traversal rules exactly as specified:\n\n1. For both tree types, it first checks if the current node is a leaf (has a 'y' attribute) and returns that value if true.\n2. For classification trees:\n   - It looks for a child whose 'x' attribute matches the feature value\n   - If no match is found, it defaults to the second child (index 1)\n3. For regression trees:\n   - If the feature value is less than or equal to the split point 's', it goes to the first child\n   - Otherwise, it goes to the second child\n\nThe function recursively traverses the tree until it reaches a leaf node and returns the prediction value unchanged."}
{"task_id": 565, "completion_id": 0, "completion": "I'll implement the ID3 feature selection function to find the feature that maximizes information gain. Here's the complete solution:\n\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Calculate entropy of the original dataset\n    original_entropy = calculate_entropy(y)\n    \n    max_info_gain = -float('inf')\n    best_feature_idx = -1\n    \n    # Iterate through each feature\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        \n        # Calculate conditional entropy after splitting on this feature\n        conditional_entropy = 0\n        unique_values = np.unique(feature_values)\n        \n        for value in unique_values:\n            # Get indices where feature has this value\n            indices = np.where(feature_values == value)[0]\n            \n            # Get corresponding labels\n            subset_labels = y[indices]\n            \n            # Calculate weight (proportion of samples with this value)\n            weight = len(indices) / n_samples\n            \n            # Calculate entropy for this subset\n            subset_entropy = calculate_entropy(subset_labels)\n            \n            # Add weighted entropy to conditional entropy\n            conditional_entropy += weight * subset_entropy\n        \n        # Calculate information gain\n        info_gain = original_entropy - conditional_entropy\n        \n        # Update best feature if this one has higher gain\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_feature_idx = feature_idx\n    \n    # Round the information gain to 6 decimal places\n    max_info_gain = round(max_info_gain, 6)\n    \n    return (best_feature_idx, max_info_gain)\n\ndef calculate_entropy(labels: np.ndarray) -> float:\n    \"\"\"Calculate the entropy of a label distribution.\n    \n    Parameters\n    ----------\n    labels : np.ndarray\n        1-D array of class labels.\n        \n    Returns\n    -------\n    float\n        The entropy value.\n    \"\"\"\n    n_samples = len(labels)\n    if n_samples == 0:\n        return 0\n    \n    # Count occurrences of each label\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    \n    # Calculate probabilities\n    probabilities = counts / n_samples\n    \n    # Calculate entropy\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    \n    return entropy\n```\n\nThis implementation:\n1. Calculates the original entropy of the label distribution\n2. For each feature:\n   - Splits the data based on the feature values\n   - Calculates the conditional entropy after the split\n   - Computes the information gain\n3. Returns the feature index with the highest information gain and the gain value rounded to 6 decimal places\n\nThe algorithm correctly handles the case where multiple features have the same information gain by returning the one with the smallest index, as it iterates through features in order and only updates when a strictly higher gain is found."}
{"task_id": 566, "completion_id": 0, "completion": "I'll implement a mini Isolation Forest algorithm for outlier detection according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)\n    \n    n_samples = data.shape[0]\n    \n    # If epsilon is 0, no outliers are allowed\n    if epsilon == 0:\n        return []\n    \n    # Adjust sample_size if it's larger than n_samples\n    actual_sample_size = min(sample_size, n_samples)\n    \n    # Calculate height limit\n    height_limit = math.ceil(math.log2(actual_sample_size))\n    \n    # Calculate phi for the anomaly score formula\n    phi = 2 * math.log(n_samples - 1) - 2 * (n_samples - 1) / n_samples if n_samples > 1 else 1\n    \n    # Function to calculate c(n)\n    def c(n):\n        if n <= 1:\n            return 0\n        return 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n    \n    # Function to build an isolation tree\n    def build_isolation_tree(X, current_height=0):\n        n = X.shape[0]\n        \n        # If we've reached the height limit or have 1 or 0 samples, create a leaf\n        if current_height >= height_limit or n <= 1:\n            return {\"type\": \"leaf\", \"size\": n}\n        \n        # Choose a random feature\n        feature_idx = np.random.randint(0, X.shape[1])\n        \n        # Find min and max values for the chosen feature\n        min_val = np.min(X[:, feature_idx])\n        max_val = np.max(X[:, feature_idx])\n        \n        # If min and max are the same, create a leaf\n        if min_val == max_val:\n            return {\"type\": \"leaf\", \"size\": n}\n        \n        # Choose a random threshold\n        threshold = min_val + np.random.random() * (max_val - min_val)\n        \n        # Split the data\n        left_indices = X[:, feature_idx] < threshold\n        right_indices = ~left_indices\n        \n        # If either split is empty, create a leaf\n        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n            return {\"type\": \"leaf\", \"size\": n}\n        \n        # Create a node\n        return {\n            \"type\": \"node\",\n            \"feature_idx\": feature_idx,\n            \"threshold\": threshold,\n            \"left\": build_isolation_tree(X[left_indices], current_height + 1),\n            \"right\": build_isolation_tree(X[right_indices], current_height + 1)\n        }\n    \n    # Function to compute the path length for a sample\n    def compute_path_length(x, tree, current_height=0):\n        if tree[\"type\"] == \"leaf\":\n            return current_height + c(tree[\"size\"])\n        \n        if x[tree[\"feature_idx\"]] < tree[\"threshold\"]:\n            return compute_path_length(x, tree[\"left\"], current_height + 1)\n        else:\n            return compute_path_length(x, tree[\"right\"], current_height + 1)\n    \n    # Build n_trees isolation trees\n    trees = []\n    for _ in range(n_trees):\n        # Sample data without replacement\n        sample_indices = np.random.choice(n_samples, actual_sample_size, replace=False)\n        sample_data = data[sample_indices]\n        \n        # Build a tree\n        tree = build_isolation_tree(sample_data)\n        trees.append(tree)\n    \n    # Compute average path length for each sample\n    avg_path_lengths = np.zeros(n_samples)\n    for i in range(n_samples):\n        path_lengths = [compute_path_length(data[i], tree) for tree in trees]\n        avg_path_lengths[i] = np.mean(path_lengths)\n    \n    # Compute anomaly scores\n    scores = 2 ** (-avg_path_lengths / phi)\n    \n    # Determine the number of outliers\n    n_outliers = int(epsilon * n_samples)\n    \n    # If n_outliers is 0, return an empty list\n    if n_outliers == 0:\n        return []\n    \n    # Get the indices of the top n_outliers scores\n    outlier_indices = np.argsort(scores)[-n_outliers:]\n    \n    # Sort the indices in ascending order\n    outlier_indices = sorted(outlier_indices)\n    \n    return outlier_indices.tolist()\n```\n\nThis implementation follows the specifications exactly:\n\n1. It builds `n_trees` random isolation trees using the specified algorithm\n2. For each tree, it samples `sample_size` distinct rows (or all rows if `sample_size` \u2265 `n_samples`)\n3. It recursively splits the data until reaching the height limit or having at most one sample\n4. Splits are performed by choosing a random feature and a random threshold\n5. It computes the average path length for each observation and converts it to an anomaly score\n6. It returns the indices of the observations with the largest scores, sorted in ascending order\n7. It handles the special case where `epsilon = 0` by returning an empty list\n8. It sets the random seed to 42 for reproducibility\n\nThe algorithm correctly implements the path length correction using the c(n) function and calculates the anomaly score using the specified formula."}
{"task_id": 568, "completion_id": 0, "completion": "I'll implement a Maximum Entropy classifier with Generalized Iterative Scaling (GIS) algorithm as requested. This approach models the conditional probability distribution of labels given features by finding the distribution with maximum entropy that satisfies the empirical constraints.\n\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Maximum Entropy classifier with Generalized Iterative Scaling.\n    \n    Args:\n        X_train: Training features, shape (n_samples, n_features)\n        y_train: Training labels\n        X_test: Test features to predict\n        epsilon: Convergence threshold for weight updates\n        n_iter: Maximum number of iterations\n        \n    Returns:\n        Predicted labels for X_test\n    \"\"\"\n    # Get unique labels and their count\n    unique_labels = np.unique(y_train)\n    n_labels = len(unique_labels)\n    \n    # Build feature-value pairs from training data\n    feature_values = []\n    for j in range(X_train.shape[1]):\n        feature_values.append(np.unique(X_train[:, j]))\n    \n    # Create feature functions mapping (feature_idx, feature_value, label) -> function_idx\n    feature_functions = {}\n    function_idx = 0\n    for j in range(X_train.shape[1]):\n        for val in feature_values[j]:\n            for label in unique_labels:\n                feature_functions[(j, val, label)] = function_idx\n                function_idx += 1\n    \n    n_functions = len(feature_functions)\n    \n    # Initialize weights\n    weights = np.zeros(n_functions)\n    \n    # Calculate empirical counts (expected values of feature functions in training data)\n    empirical_counts = np.zeros(n_functions)\n    for i in range(len(X_train)):\n        x = X_train[i]\n        y = y_train[i]\n        for j in range(len(x)):\n            key = (j, x[j], y)\n            if key in feature_functions:\n                function_idx = feature_functions[key]\n                empirical_counts[function_idx] += 1\n    \n    # Normalize empirical counts\n    empirical_counts /= len(X_train)\n    \n    # Calculate the correction constant C (sum of active features for any instance)\n    # In this case, for each instance, we have exactly one active feature per feature dimension\n    C = X_train.shape[1]\n    \n    # GIS algorithm\n    for iteration in range(n_iter):\n        # Calculate model expectations\n        model_expectations = np.zeros(n_functions)\n        \n        for i in range(len(X_train)):\n            x = X_train[i]\n            \n            # Calculate normalization factor Z(x)\n            z_x = 0\n            label_probs = {}\n            \n            for label in unique_labels:\n                # Calculate sum of weights for active features\n                sum_weights = 0\n                for j in range(len(x)):\n                    key = (j, x[j], label)\n                    if key in feature_functions:\n                        function_idx = feature_functions[key]\n                        sum_weights += weights[function_idx]\n                \n                # Calculate unnormalized probability\n                prob = math.exp(sum_weights)\n                label_probs[label] = prob\n                z_x += prob\n            \n            # Normalize probabilities\n            for label in unique_labels:\n                p_y_given_x = label_probs[label] / z_x\n                \n                # Update model expectations\n                for j in range(len(x)):\n                    key = (j, x[j], label)\n                    if key in feature_functions:\n                        function_idx = feature_functions[key]\n                        model_expectations[function_idx] += p_y_given_x\n        \n        # Normalize model expectations\n        model_expectations /= len(X_train)\n        \n        # Update weights using GIS update rule\n        max_update = 0\n        for k in range(n_functions):\n            if empirical_counts[k] > 0:\n                update = (1/C) * math.log(empirical_counts[k] / model_expectations[k]) if model_expectations[k] > 0 else 0\n                weights[k] += update\n                max_update = max(max_update, abs(update))\n        \n        # Check convergence\n        if max_update < epsilon:\n            break\n    \n    # Predict labels for test data\n    predictions = []\n    for i in range(len(X_test)):\n        x = X_test[i]\n        \n        # Calculate probabilities for each label\n        label_probs = {}\n        for label in unique_labels:\n            # Calculate sum of weights for active features\n            sum_weights = 0\n            for j in range(len(x)):\n                key = (j, x[j], label)\n                if key in feature_functions:\n                    function_idx = feature_functions[key]\n                    sum_weights += weights[function_idx]\n            \n            # Store unnormalized probability\n            label_probs[label] = math.exp(sum_weights)\n        \n        # Find label with highest probability\n        best_label = max(label_probs.items(), key=lambda item: item[1])[0]\n        predictions.append(best_label)\n    \n    return predictions\n```\n\nThis implementation:\n\n1. Builds feature functions for each (feature_index, feature_value, label) combination\n2. Calculates empirical distributions from the training data\n3. Uses the GIS algorithm to iteratively update weights until convergence\n4. Predicts labels for test data by finding the label with the highest conditional probability\n\nThe algorithm handles categorical features by treating each feature-value pair as a binary indicator. The implementation is efficient and follows the mathematical formulation of the Maximum Entropy model with GIS optimization."}
{"task_id": 569, "completion_id": 0, "completion": "I'll implement the function to analyze document-frequency statistics for keywords. Here's the complete solution:\n\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    # Handle empty data case\n    if not data:\n        return [], set()\n    \n    # Count in how many documents each word appears\n    word_doc_count = Counter()\n    total_docs = len(data)\n    \n    # For each document, add each unique word to the counter\n    for _, words in data:\n        # Convert words to a set to count each word only once per document\n        unique_words = set(words)\n        for word in unique_words:\n            word_doc_count[word] += 1\n    \n    # Calculate document frequency ratio for each word and round to 4 decimal places\n    word_freq_ratio = [(word, round(count / total_docs, 4)) \n                       for word, count in word_doc_count.items()]\n    \n    # Sort by frequency ratio in descending order\n    word_freq_ratio.sort(key=lambda x: x[1], reverse=True)\n    \n    # Determine which words to include in the set\n    if cut_off is None:\n        # Include all words\n        top_words = {word for word, _ in word_freq_ratio}\n    elif cut_off <= 0:\n        # Empty set for cut_off of 0 or negative\n        top_words = set()\n    else:\n        # Include only the top k words (or all if k > number of words)\n        top_words = {word for word, _ in word_freq_ratio[:cut_off]}\n    \n    return word_freq_ratio, top_words\n```\n\nThis implementation:\n\n1. Counts how many documents each word appears in (counting each word only once per document)\n2. Calculates the document frequency ratio by dividing by the total number of documents\n3. Rounds each ratio to 4 decimal places\n4. Sorts the words by their frequency ratio in descending order\n5. Creates a set of the top k words based on the cut_off parameter\n6. Handles edge cases like empty data, cut_off=None, cut_off=0, etc."}
{"task_id": 571, "completion_id": 0, "completion": "I'll implement the function to compute the SVM parameters from the Lagrange multipliers according to the given formulas.\n\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute weight vector w using the formula w = X^T(alpha \u2299 y)\n    # where \u2299 is element-wise multiplication\n    alpha_y = alpha * y  # Element-wise multiplication of alpha and y\n    w = X.T @ alpha_y    # Matrix multiplication of X^T and (alpha \u2299 y)\n    \n    # Compute bias term b using the formula b = (1/n) * sum(y_i - w^T * x_i)\n    n = len(y)\n    wx = X @ w  # Matrix multiplication of X and w to get w^T * x_i for all i\n    b = np.sum(y - wx) / n\n    \n    # Round to 4 decimal places\n    w_rounded = [round(val, 4) for val in w.tolist()]\n    b_rounded = round(b, 4)\n    \n    return (w_rounded, b_rounded)\n```\n\nThis implementation follows the formulas provided in the task:\n1. First, it computes the weight vector `w` by calculating `X.T @ (alpha * y)`, which is equivalent to the formula `w = sum(alpha_i * y_i * x_i)`.\n2. Then, it computes the bias term `b` by calculating the average of `y_i - w^T * x_i` for all samples.\n3. Finally, it rounds all values to 4 decimal places and returns the weight vector as a Python list and the bias as a float."}
{"task_id": 572, "completion_id": 0, "completion": "I'll implement the Local Outlier Factor (LOF) algorithm from scratch according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Implements the Local Outlier Factor algorithm from scratch.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        The input data matrix of shape (n_samples, n_features)\n    k : int\n        The number of neighbors to consider\n    epsilon : float, default=1.5\n        The threshold for determining outliers\n        \n    Returns:\n    --------\n    tuple\n        (list_of_scores, list_of_outlier_indices)\n        - list_of_scores: List of LOF scores rounded to 4 decimals\n        - list_of_outlier_indices: Sorted list of indices of samples with LOF > epsilon\n    \"\"\"\n    n_samples = data.shape[0]\n    \n    # 1. Compute the full pair-wise Euclidean distance matrix\n    dist_matrix = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            dist_matrix[i, j] = np.sqrt(np.sum((data[i] - data[j])**2))\n    \n    # 2. For each sample, find its k-distance and k-nearest neighbors\n    k_distances = np.zeros(n_samples)\n    k_neighbors = np.zeros((n_samples, k), dtype=int)\n    \n    for i in range(n_samples):\n        # Sort distances and get indices (excluding self)\n        sorted_indices = np.argsort(dist_matrix[i])\n        # Skip the first index (which is the point itself with distance 0)\n        sorted_indices = sorted_indices[1:k+1]\n        k_neighbors[i] = sorted_indices\n        # k-distance is the distance to the k-th nearest neighbor\n        k_distances[i] = dist_matrix[i, sorted_indices[-1]]\n    \n    # 3 & 4. Compute reachability distances and local reachability densities\n    lrd = np.zeros(n_samples)\n    \n    for i in range(n_samples):\n        reach_dists = np.zeros(k)\n        neighbors = k_neighbors[i]\n        \n        for j in range(k):\n            neighbor = neighbors[j]\n            # reach-dist_k(p,q) = max(k-distance(q), dist(p,q))\n            reach_dists[j] = max(k_distances[neighbor], dist_matrix[i, neighbor])\n        \n        # LRD_k(p) = k / sum(reach-dist_k(p,q))\n        lrd[i] = k / np.sum(reach_dists)\n    \n    # 5. Compute Local Outlier Factor\n    lof = np.zeros(n_samples)\n    \n    for i in range(n_samples):\n        neighbors = k_neighbors[i]\n        lrd_sum = 0\n        \n        for j in range(k):\n            neighbor = neighbors[j]\n            lrd_sum += lrd[neighbor]\n        \n        # LOF_k(p) = (sum(LRD_k(q)) / k) / LRD_k(p)\n        lof[i] = (lrd_sum / k) / lrd[i]\n    \n    # Round LOF scores to 4 decimal places\n    lof_rounded = np.round(lof, 4)\n    \n    # Find outliers (samples with LOF > epsilon)\n    outlier_indices = np.where(lof > epsilon)[0]\n    outlier_indices = sorted(outlier_indices)\n    \n    return lof_rounded.tolist(), outlier_indices.tolist()\n```\n\nThis implementation follows the steps outlined in the task:\n\n1. Computes the full pairwise Euclidean distance matrix\n2. Finds the k-distance and k-nearest neighbors for each sample\n3. Calculates the reachability distance between points\n4. Computes the Local Reachability Density (LRD) for each point\n5. Calculates the Local Outlier Factor (LOF) for each point\n\nThe function returns a tuple containing the list of LOF scores rounded to 4 decimal places and a sorted list of indices of samples whose LOF score is greater than the specified epsilon threshold."}
{"task_id": 574, "completion_id": 0, "completion": "I'll implement a function that generates synthetic 2D data according to the specifications. Here's the complete solution:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Initialize arrays\n    X_train = np.zeros((m * n_train, 2))\n    X_val = np.zeros((m * n_val, 2))\n    y_train = np.zeros(m * n_train, dtype=int)\n    y_val = np.zeros(m * n_val, dtype=int)\n    \n    for c in range(m):\n        # Calculate base position for this class\n        base = 10 * (c // 2)\n        \n        # Generate training data for this class\n        start_idx = c * n_train\n        end_idx = (c + 1) * n_train\n        \n        # Sample from uniform distribution and round to 4 decimal places\n        X_train[start_idx:end_idx, 0] = np.round(np.random.uniform(base + 1, base + 9, n_train), 4)\n        X_train[start_idx:end_idx, 1] = np.round(np.random.uniform(base + 1, base + 9, n_train), 4)\n        y_train[start_idx:end_idx] = c\n        \n        # Generate validation data for this class\n        start_idx = c * n_val\n        end_idx = (c + 1) * n_val\n        \n        # Sample from uniform distribution and round to 4 decimal places\n        X_val[start_idx:end_idx, 0] = np.round(np.random.uniform(base + 1, base + 9, n_val), 4)\n        X_val[start_idx:end_idx, 1] = np.round(np.random.uniform(base + 1, base + 9, n_val), 4)\n        y_val[start_idx:end_idx] = c\n    \n    return X_train, X_val, y_train, y_val\n```\n\nThis implementation:\n1. Sets the random seed if provided\n2. Initializes the output arrays\n3. For each class, calculates the base position using the formula `10 * (c // 2)`\n4. Generates training and validation points for each class by sampling from a uniform distribution in the range `[base+1, base+9]` for both x and y coordinates\n5. Rounds all coordinates to 4 decimal places as required\n6. Returns the four arrays in the specified format"}
{"task_id": 579, "completion_id": 0, "completion": "# K-Means Clustering Implementation\n\nI'll implement the K-Means clustering algorithm from scratch according to the specifications. The key points are:\n- Use the first k points as initial centers (deterministic initialization)\n- Assign points to the nearest center (with tie-breaking by index)\n- Update centers as the mean of assigned points\n- Repeat until convergence or max iterations\n- Round center coordinates to 4 decimal places\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n    \n    # 1. Initialization: take the first k points as initial centers\n    centers = X[:k].copy()\n    \n    # Initialize labels\n    labels = np.zeros(m, dtype=int)\n    \n    # Main loop\n    for iteration in range(max_iters):\n        # Store previous labels to check for convergence\n        prev_labels = labels.copy()\n        \n        # 2. Assignment step: assign each point to the nearest center\n        for i in range(m):\n            # Calculate distances to all centers\n            distances = np.array([np.sqrt(np.sum((X[i] - centers[j])**2)) for j in range(k)])\n            \n            # Find the index of the minimum distance\n            # In case of a tie, the smaller index will be chosen automatically\n            labels[i] = np.argmin(distances)\n        \n        # Check for convergence\n        if np.array_equal(labels, prev_labels):\n            break\n        \n        # 3. Update step: recalculate centers\n        for j in range(k):\n            # Get all points assigned to this cluster\n            cluster_points = X[labels == j]\n            \n            # If the cluster is not empty, update its center\n            if len(cluster_points) > 0:\n                centers[j] = np.mean(cluster_points, axis=0)\n    \n    # Round centers to 4 decimal places\n    rounded_centers = [[round(coord, 4) for coord in center] for center in centers]\n    \n    return labels.tolist(), rounded_centers\n```\n\nThis implementation follows the specified algorithm exactly:\n1. It initializes centers with the first k points\n2. It assigns points to the nearest center (with automatic tie-breaking by index)\n3. It updates centers as the mean of assigned points, keeping centers unchanged if they lose all points\n4. It repeats until convergence or max iterations\n5. It rounds all center coordinates to 4 decimal places before returning\n\nThe function returns a tuple containing the list of labels and the list of rounded center coordinates as required."}
{"task_id": 581, "completion_id": 0, "completion": "I'll implement the Elastic-Net regression algorithm from scratch using batch gradient descent as specified. Here's the complete solution:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to 4 decimal places.\n    \"\"\"\n    # Early return if learning_rate or iterations is 0\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * X.shape[1], 0.0\n    \n    # Initialize parameters\n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0.0\n    \n    # Define sign function with sign(0) = -1\n    def custom_sign(x):\n        return np.where(x != 0, np.sign(x), -1)\n    \n    # Perform batch gradient descent\n    for _ in range(iterations):\n        # Compute predictions\n        y_pred = X.dot(w) + b\n        \n        # Compute residuals\n        residuals = y - y_pred\n        \n        # Compute gradients\n        dw = -2 * X.T.dot(residuals) + l1_penalty * custom_sign(w) + 2 * l2_penalty * w\n        db = -2 * np.sum(residuals)\n        \n        # Update parameters\n        w = w - learning_rate * dw / m\n        b = b - learning_rate * db / m\n    \n    # Round to 4 decimal places\n    w_rounded = [round(float(val), 4) for val in w]\n    b_rounded = round(float(b), 4)\n    \n    return w_rounded, b_rounded\n```\n\nThis implementation follows the specified algorithm:\n1. It initializes weights and bias to zero\n2. For each iteration, it computes predictions, residuals, and gradients\n3. It updates the parameters using the learning rate and mean gradients\n4. It handles the special case where sign(0) = -1 as specified\n5. It returns the weights as a list and bias as a float, both rounded to 4 decimal places\n6. It handles the early return case when learning_rate or iterations is 0"}
{"task_id": 591, "completion_id": 0, "completion": "I'll implement a function to perform a forward pass through a fully-connected neural network with ReLU activations in the hidden layers and a linear activation in the output layer.\n\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network's weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Convert input to numpy array\n    current_activation = np.array(x, dtype=float)\n    \n    # Process each layer\n    for layer_idx, layer_weights in enumerate(weights):\n        # Convert layer weights to numpy array\n        weight_matrix = np.array(layer_weights, dtype=float)\n        \n        # Add bias term (1) to the input\n        current_activation_with_bias = np.insert(current_activation, 0, 1.0)\n        \n        # Compute the dot product for each neuron in the layer\n        z = np.array([np.dot(neuron_weights, current_activation_with_bias) \n                      for neuron_weights in weight_matrix])\n        \n        # Apply ReLU activation to all layers except the last one\n        if layer_idx < len(weights) - 1:\n            current_activation = np.maximum(0, z)  # ReLU activation\n        else:\n            current_activation = z  # Linear activation for the last layer\n    \n    # Round the results to 4 decimal places\n    current_activation = np.round(current_activation, 4)\n    \n    # Return a single float if there's only one output neuron, otherwise return a list\n    if len(current_activation) == 1:\n        return float(current_activation[0])\n    else:\n        return current_activation.tolist()\n```\n\nThis implementation follows these steps:\n1. Converts the input to a numpy array for efficient computation\n2. For each layer:\n   - Adds the bias term (1.0) to the current activation\n   - Computes the dot product between each neuron's weights and the input (with bias)\n   - Applies ReLU activation for hidden layers, or no activation (linear) for the output layer\n3. Rounds the final output to 4 decimal places\n4. Returns a single float if there's only one output neuron, or a list of floats otherwise\n\nThe function handles networks of any depth and width, as specified by the weights parameter."}
{"task_id": 595, "completion_id": 0, "completion": "I'll implement the K-means clustering algorithm from scratch according to your requirements. Here's the solution:\n\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"\n    Implements K-means clustering algorithm from scratch.\n    \n    Args:\n        X: Input data of shape (n_samples, n_features)\n        k: Number of clusters\n        max_iters: Maximum number of iterations\n        \n    Returns:\n        List of centroids sorted in lexicographical order and rounded to 4 decimal places\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Initialize centroids with the first k samples\n    centroids = X[:k].copy()\n    \n    for iteration in range(max_iters):\n        # Store previous centroids to check for convergence\n        prev_centroids = centroids.copy()\n        \n        # Step 1: Assign samples to nearest centroids\n        # Initialize array to store cluster assignments\n        cluster_assignments = np.zeros(n_samples, dtype=int)\n        \n        # Compute distances and assign each sample to the nearest centroid\n        for i in range(n_samples):\n            # Calculate squared Euclidean distances to all centroids\n            distances = np.sum((centroids - X[i])**2, axis=1)\n            # Assign to the closest centroid\n            cluster_assignments[i] = np.argmin(distances)\n        \n        # Step 2: Update centroids\n        for j in range(k):\n            # Get samples assigned to this cluster\n            cluster_samples = X[cluster_assignments == j]\n            \n            # If the cluster is not empty, update its centroid\n            if len(cluster_samples) > 0:\n                centroids[j] = np.mean(cluster_samples, axis=0)\n            # If empty, keep the previous centroid (requirement 4)\n        \n        # Check for convergence: if all centroids moved less than 1e-4\n        if np.all(np.sqrt(np.sum((centroids - prev_centroids)**2, axis=1)) < 1e-4):\n            break\n    \n    # Sort centroids lexicographically\n    centroids = centroids[np.lexsort([centroids[:, i] for i in range(n_features-1, -1, -1)])]\n    \n    # Round to 4 decimal places and convert to list of lists\n    rounded_centroids = [[round(val, 4) for val in centroid] for centroid in centroids]\n    \n    return rounded_centroids\n```\n\nThis implementation follows all the requirements:\n1. Initializes centroids with the first k samples\n2. Iterates until convergence or max_iters\n3. Assigns samples to the nearest centroid using squared Euclidean distance\n4. Updates centroids to the mean of assigned samples\n5. Stops early if centroids move less than 1e-4\n6. Keeps centroids unchanged for empty clusters\n7. Sorts centroids lexicographically and rounds to 4 decimal places\n8. Returns a list of lists as required"}
