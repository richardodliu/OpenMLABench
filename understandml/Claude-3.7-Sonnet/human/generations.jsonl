{"task_id": 1, "completion_id": 0, "completion": "# Matrix-Vector Dot Product Implementation\n\nI'll implement a function that computes the dot product of a matrix and a vector, with proper dimension validation.\n\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Compute the dot product of a matrix and a vector.\n    \n    Args:\n        a: A matrix represented as a list of lists\n        b: A vector represented as a list\n        \n    Returns:\n        A list representing the resulting vector if dimensions are compatible,\n        or -1 if dimensions are incompatible\n    \"\"\"\n    # Check if matrix is empty\n    if not a:\n        return -1\n    \n    # Check if vector is empty\n    if not b:\n        return -1\n    \n    # Get number of columns in matrix\n    num_cols = len(a[0])\n    \n    # Check if all rows have the same number of columns\n    if any(len(row) != num_cols for row in a):\n        return -1\n    \n    # Check if matrix columns match vector length\n    if num_cols != len(b):\n        return -1\n    \n    # Compute dot product for each row\n    result = []\n    for row in a:\n        dot_product = sum(row[i] * b[i] for i in range(num_cols))\n        result.append(dot_product)\n    \n    return result\n```\n\nThis implementation:\n1. Validates that neither the matrix nor vector is empty\n2. Ensures the matrix is well-formed (all rows have the same number of columns)\n3. Checks that the number of columns in the matrix equals the length of the vector\n4. Computes the dot product for each row of the matrix with the vector\n5. Returns -1 if any validation fails\n\nThe time complexity is O(n*m) where n is the number of rows in the matrix and m is the number of columns."}
{"task_id": 2, "completion_id": 0, "completion": "# Transpose Matrix Function\n\nHere's a Python function that computes the transpose of a matrix:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Returns the transpose of the input matrix.\n    \n    Args:\n        a: A matrix represented as a list of lists where each inner list is a row\n        \n    Returns:\n        The transposed matrix where rows become columns and columns become rows\n        \n    Examples:\n        >>> transpose_matrix([[1, 2, 3], [4, 5, 6]])\n        [[1, 4], [2, 5], [3, 6]]\n        \n        >>> transpose_matrix([[1, 2], [3, 4], [5, 6]])\n        [[1, 3, 5], [2, 4, 6]]\n    \"\"\"\n    # Handle empty matrix\n    if not a:\n        return []\n        \n    # Use the zip function to elegantly transpose the matrix\n    # The * operator unpacks the list of rows, and zip groups elements by position\n    return [list(row) for row in zip(*a)]\n```\n\nThis solution uses Python's `zip` function which is both concise and efficient for transposing matrices. The function handles:\n\n- Regular matrices of any dimension\n- Empty matrices\n- Row vectors (1\u00d7n matrices)\n- Column vectors (m\u00d71 matrices)\n\nThe time complexity is O(m\u00d7n) where m is the number of rows and n is the number of columns in the input matrix."}
{"task_id": 3, "completion_id": 0, "completion": "# Matrix Reshaping Solution\n\nI'll implement a function that reshapes a matrix to a new shape, using NumPy for efficient handling of the operation.\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape a matrix to a new shape if possible.\n    \n    Args:\n        a: Input matrix as a list of lists containing integers or floats\n        new_shape: Target shape as a tuple of (rows, columns)\n        \n    Returns:\n        Reshaped matrix as a list of lists, or empty list if reshaping is not possible\n    \"\"\"\n    try:\n        # Convert input list to numpy array\n        arr = np.array(a)\n        \n        # Get the total number of elements in the original array\n        total_elements = arr.size\n        \n        # Check if reshaping is possible\n        target_elements = new_shape[0] * new_shape[1]\n        if total_elements != target_elements:\n            return []\n        \n        # Reshape the array and convert back to list\n        reshaped = arr.reshape(new_shape)\n        return reshaped.tolist()\n    \n    except (ValueError, TypeError):\n        # Handle cases where the input is not a valid matrix or cannot be reshaped\n        return []\n```\n\nThis solution:\n1. Converts the input list to a NumPy array\n2. Checks if reshaping is possible by comparing the total number of elements\n3. Uses NumPy's reshape function to perform the reshaping\n4. Converts the result back to a Python list using tolist()\n5. Handles potential errors with try/except\n\nThe function will return an empty list if:\n- The input cannot be reshaped to the target shape\n- The input is not a valid matrix\n- Any other error occurs during the process"}
{"task_id": 4, "completion_id": 0, "completion": "# Matrix Mean Calculator\n\nI'll create a function that calculates the mean of a matrix by row or column as requested.\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    Args:\n        matrix: A 2D list (matrix) of floating-point numbers\n        mode: Either 'row' or 'column' to specify calculation direction\n    \n    Returns:\n        A list of mean values according to the specified mode\n    \n    Raises:\n        ValueError: If the mode is not 'row' or 'column' or if the matrix is empty\n    \"\"\"\n    # Check if matrix is empty\n    if not matrix or not matrix[0]:\n        raise ValueError(\"Matrix cannot be empty\")\n    \n    # Check if mode is valid\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        return [sum(row) / len(row) for row in matrix]\n    else:  # mode == 'column'\n        # Get number of columns\n        num_cols = len(matrix[0])\n        \n        # Check if all rows have the same length\n        if not all(len(row) == num_cols for row in matrix):\n            raise ValueError(\"All rows must have the same length\")\n        \n        # Calculate mean for each column\n        means = []\n        for col_idx in range(num_cols):\n            col_sum = sum(row[col_idx] for row in matrix)\n            means.append(col_sum / len(matrix))\n        \n        return means\n```\n\nThis function:\n1. Validates the input matrix and mode\n2. For row mode, calculates the mean of each row using a list comprehension\n3. For column mode, iterates through each column index and calculates the mean of values at that index across all rows\n4. Handles edge cases like empty matrices or inconsistent row lengths\n\nThe function returns a list of means as specified by the mode parameter."}
{"task_id": 5, "completion_id": 0, "completion": "# Scalar Multiplication of a Matrix\n\nI'll implement a function that multiplies each element of a matrix by a scalar value.\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiply each element of a matrix by a scalar value.\n    \n    Args:\n        matrix: A matrix represented as a list of lists\n        scalar: The scalar value to multiply by\n        \n    Returns:\n        A new matrix with each element multiplied by the scalar\n        \n    Example:\n        >>> scalar_multiply([[1, 2], [3, 4]], 2)\n        [[2, 4], [6, 8]]\n    \"\"\"\n    # Check if matrix is empty\n    if not matrix:\n        return []\n    \n    # Create a new matrix with the same dimensions\n    result = []\n    \n    # Multiply each element by the scalar\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    \n    return result\n```\n\nThis function:\n1. Takes a matrix (list of lists) and a scalar value as input\n2. Creates a new matrix rather than modifying the original\n3. Multiplies each element in the matrix by the scalar value\n4. Returns the resulting matrix\n\nThe implementation uses a list comprehension inside a loop to efficiently create each new row of the result matrix."}
{"task_id": 6, "completion_id": 0, "completion": "# Calculating Eigenvalues of a 2x2 Matrix\n\nI'll write a function to calculate the eigenvalues of a 2x2 matrix. For a 2x2 matrix, we can find the eigenvalues using the characteristic equation: \u03bb\u00b2 - (trace)\u03bb + (determinant) = 0.\n\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n        matrix: A 2x2 matrix represented as a list of lists.\n        \n    Returns:\n        A list of eigenvalues sorted from highest to lowest.\n    \"\"\"\n    # Extract elements from the 2x2 matrix\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    # Calculate trace and determinant\n    trace = a + d\n    determinant = a * d - b * c\n    \n    # Calculate discriminant\n    discriminant = trace**2 - 4 * determinant\n    \n    # Calculate eigenvalues\n    if discriminant >= 0:\n        # Real eigenvalues\n        eigenvalue1 = (trace + math.sqrt(discriminant)) / 2\n        eigenvalue2 = (trace - math.sqrt(discriminant)) / 2\n    else:\n        # Complex eigenvalues - return the real parts\n        eigenvalue1 = trace / 2\n        eigenvalue2 = trace / 2\n    \n    # Sort eigenvalues from highest to lowest\n    eigenvalues = [eigenvalue1, eigenvalue2]\n    eigenvalues.sort(reverse=True)\n    \n    return eigenvalues\n```\n\n## Explanation:\n\n1. The function takes a 2x2 matrix as input and extracts its elements.\n2. It calculates the trace (sum of diagonal elements) and determinant.\n3. It computes the discriminant to determine if the eigenvalues are real or complex.\n4. For real eigenvalues (discriminant \u2265 0), it applies the quadratic formula.\n5. For complex eigenvalues (discriminant < 0), it returns the real parts, which are identical.\n6. Finally, it sorts the eigenvalues from highest to lowest and returns them.\n\nThis solution handles both real and complex eigenvalues, returning a list of float values as required."}
{"task_id": 7, "completion_id": 0, "completion": "# Matrix Transformation Solution\n\nI'll implement a function that performs the matrix transformation $T^{-1}AS$ after validating that both T and S are invertible matrices.\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transform matrix A using the operation T^(-1)AS.\n    \n    Args:\n        A: Input matrix to transform\n        T: First transformation matrix (must be invertible)\n        S: Second transformation matrix (must be invertible)\n        \n    Returns:\n        Transformed matrix as a list of lists, or -1 if T or S is not invertible\n    \"\"\"\n    # Convert input lists to numpy arrays\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by calculating their determinants\n    if abs(np.linalg.det(T_np)) < 1e-10 or abs(np.linalg.det(S_np)) < 1e-10:\n        return -1\n    \n    try:\n        # Calculate T^(-1)\n        T_inv = np.linalg.inv(T_np)\n        \n        # Perform the transformation T^(-1)AS\n        result = T_inv @ A_np @ S_np\n        \n        # Round to 4 decimal places\n        result = np.round(result, 4)\n        \n        # Convert back to list of lists\n        return result.tolist()\n    except np.linalg.LinAlgError:\n        # This catches any linear algebra errors that might occur\n        return -1\n```\n\nThis implementation:\n1. Converts the input lists to NumPy arrays\n2. Checks if T and S are square matrices (a requirement for invertibility)\n3. Verifies that T and S are invertible by checking if their determinants are non-zero\n4. Calculates the inverse of T\n5. Performs the matrix multiplication T^(-1) \u00d7 A \u00d7 S\n6. Rounds the result to 4 decimal places\n7. Converts the result back to a list of lists\n\nThe function returns -1 if either T or S is not invertible or if any linear algebra errors occur during the computation."}
{"task_id": 8, "completion_id": 0, "completion": "# Matrix Inverse Calculator Function\n\nHere's a solution to calculate the inverse of a 2x2 matrix:\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n        matrix: A 2x2 matrix represented as a list of lists\n        \n    Returns:\n        The inverse matrix as a list of lists, or None if the matrix is not invertible\n    \"\"\"\n    # Check if input is a valid 2x2 matrix\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2x2 matrix\")\n    \n    # Extract matrix elements\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate determinant\n    determinant = a * d - b * c\n    \n    # Check if matrix is invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse using the formula:\n    # 1/det * [d, -b; -c, a]\n    inverse_determinant = 1 / determinant\n    \n    return [\n        [d * inverse_determinant, -b * inverse_determinant],\n        [-c * inverse_determinant, a * inverse_determinant]\n    ]\n```\n\n## How It Works\n\n1. The function first validates that the input is indeed a 2\u00d72 matrix\n2. It extracts the individual elements (a, b, c, d) from the matrix\n3. Calculates the determinant using the formula: a\u00d7d - b\u00d7c\n4. If the determinant is zero, the matrix is not invertible, so it returns None\n5. Otherwise, it calculates the inverse using the formula for a 2\u00d72 matrix:\n   1/det \u00d7 [[d, -b], [-c, a]]\n\nThis implementation handles all the requirements and will correctly compute the inverse of any valid 2\u00d72 matrix or return None when appropriate."}
{"task_id": 9, "completion_id": 0, "completion": "# Matrix Multiplication in Python\n\nHere's a solution that multiplies two matrices and returns -1 if they cannot be multiplied:\n\n```python\ndef matrixmul(a: list[list[int|float]], \n              b: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if matrices are empty\n    if not a or not b:\n        return -1\n        \n    # Check if matrices have consistent dimensions\n    rows_a = len(a)\n    if not all(len(row) == len(a[0]) for row in a) or not all(len(row) == len(b[0]) for row in b):\n        return -1\n        \n    cols_a = len(a[0])\n    rows_b = len(b)\n    \n    # Check if matrices can be multiplied (cols of A must equal rows of B)\n    if cols_a != rows_b:\n        return -1\n    \n    # Get dimensions of result matrix\n    cols_b = len(b[0])\n    \n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n```\n\n## Explanation:\n\n1. First, I check if either matrix is empty, returning -1 if so.\n\n2. Then I validate that both matrices have consistent dimensions (every row has the same number of elements).\n\n3. Next, I check the compatibility condition for matrix multiplication: the number of columns in matrix A must equal the number of rows in matrix B.\n\n4. If the matrices can be multiplied, I create a result matrix of the appropriate size (rows_a \u00d7 cols_b).\n\n5. The triple nested loop implements the standard matrix multiplication algorithm:\n   - For each element in the result matrix at position (i,j)\n   - Calculate the dot product of row i from matrix A and column j from matrix B\n\nTime complexity: O(n\u00d7m\u00d7p) where n,m,p are the dimensions involved in the multiplication."}
{"task_id": 10, "completion_id": 0, "completion": "# Covariance Matrix Calculation in Python\n\nI'll implement a function to calculate the covariance matrix from a list of feature vectors, along with test cases to verify correctness.\n\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n        vectors: A list of lists, where each inner list represents a feature with its observations.\n        \n    Returns:\n        A covariance matrix as a list of lists.\n    \"\"\"\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    \n    # Calculate means for each feature\n    means = [sum(feature) / n_observations for feature in vectors]\n    \n    # Initialize covariance matrix\n    covariance_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    \n    # Calculate covariance for each pair of features\n    for i in range(n_features):\n        for j in range(n_features):\n            covariance = 0.0\n            for k in range(n_observations):\n                deviation_i = vectors[i][k] - means[i]\n                deviation_j = vectors[j][k] - means[j]\n                covariance += deviation_i * deviation_j\n            covariance_matrix[i][j] = covariance / (n_observations - 1)\n    \n    return covariance_matrix\n\n\n# Utility function to check if two matrices are approximately equal\ndef are_matrices_approximately_equal(matrix1, matrix2, epsilon=1e-10):\n    if len(matrix1) != len(matrix2) or len(matrix1[0]) != len(matrix2[0]):\n        return False\n    for i in range(len(matrix1)):\n        for j in range(len(matrix1[0])):\n            if abs(matrix1[i][j] - matrix2[i][j]) > epsilon:\n                return False\n    return True\n\n\n# Test cases\ndef test_calculate_covariance_matrix():\n    # Test Case 1: Simple 2x2 covariance matrix\n    features_1 = [\n        [1, 2, 3],  # Feature 1 with 3 observations\n        [4, 5, 6]   # Feature 2 with 3 observations\n    ]\n    expected_1 = [\n        [1.0, 1.0],\n        [1.0, 1.0]\n    ]\n    result_1 = calculate_covariance_matrix(features_1)\n    assert are_matrices_approximately_equal(result_1, expected_1), \\\n        f\"Test Case 1 failed: Expected {expected_1}, but got {result_1}\"\n    \n    # Test Case 2: More complex 3x3 covariance matrix\n    features_2 = [\n        [1, 2, 3, 4],  # Feature 1 with 4 observations\n        [2, 4, 6, 8],  # Feature 2 with 4 observations\n        [0, 1, 2, 3]   # Feature 3 with 4 observations\n    ]\n    expected_2 = [\n        [1.6666666666666667, 3.3333333333333335, 1.6666666666666667],\n        [3.3333333333333335, 6.666666666666667, 3.3333333333333335],\n        [1.6666666666666667, 3.3333333333333335, 1.6666666666666667]\n    ]\n    result_2 = calculate_covariance_matrix(features_2)\n    assert are_matrices_approximately_equal(result_2, expected_2), \\\n        f\"Test Case 2 failed: Expected {expected_2}, but got {result_2}\"\n    \n    # Test Case 3: Edge case - single observation (should raise error)\n    features_3 = [\n        [1],\n        [2]\n    ]\n    try:\n        calculate_covariance_matrix(features_3)\n        assert False, \"Test Case 3 failed: Should have raised ZeroDivisionError\"\n    except ZeroDivisionError:\n        pass  # Expected behavior\n    \n    print(\"All test cases passed!\")\n\n\nif __name__ == \"__main__\":\n    test_calculate_covariance_matrix()\n```\n\n## Explanation\n\nThe `calculate_covariance_matrix` function works as follows:\n\n1. It takes a list of feature vectors, where each vector contains observations for a specific feature.\n2. It calculates the mean of each feature.\n3. It computes the covariance between each pair of features using the formula:\n   ```\n   cov(X, Y) = \u03a3[(Xi - X\u0304)(Yi - \u0232)] / (n-1)\n   ```\n   where X\u0304 and \u0232 are the means of features X and Y, and n is the number of observations.\n4. It returns the resulting covariance matrix.\n\nThe test cases verify the function's correctness with:\n1. A simple 2\u00d72 case with perfectly correlated features\n2. A more complex 3\u00d73 case with linearly related features\n3. An edge case with only one observation, which should cause division by zero\n\nThe covariance matrix will always be symmetric, and the diagonal elements represent the variance of each feature."}
{"task_id": 11, "completion_id": 0, "completion": "# Jacobi Method Implementation\n\nI'll solve this by implementing the Jacobi iterative method for linear equations. This method works by isolating each variable on the left-hand side and using previous iteration values for the other variables.\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n    \n    Args:\n        A: Coefficient matrix\n        b: Right-hand side vector\n        n: Number of iterations\n        \n    Returns:\n        Approximate solution as a Python list\n    \"\"\"\n    # Get the size of the system\n    size = len(b)\n    \n    # Initialize solution vector with zeros\n    x = np.zeros_like(b, dtype=float)\n    \n    # Extract diagonal elements\n    D = np.diag(A)\n    \n    # Create a copy of A with zeros on the diagonal\n    A_off_diag = A.copy()\n    np.fill_diagonal(A_off_diag, 0)\n    \n    # Perform n iterations\n    for _ in range(n):\n        # Jacobi update formula: x = D^-1 * (b - (A-D) * x)\n        x_new = (b - np.dot(A_off_diag, x)) / D\n        \n        # Round to 4 decimal places\n        x_new = np.round(x_new, 4)\n        \n        # Update solution for next iteration\n        x = x_new\n    \n    # Return the result as a Python list\n    return x.tolist()\n```\n\nThis implementation:\n1. Extracts the diagonal elements of A\n2. Creates a version of A with zeros on the diagonal\n3. For each iteration:\n   - Computes the new solution vector using the Jacobi update formula\n   - Rounds to 4 decimal places as required\n   - Updates the solution for the next iteration\n4. Returns the solution as a Python list using tolist()\n\nThe method will work for both 1D arrays and column vectors, maintaining the original shape when returning the list."}
{"task_id": 12, "completion_id": 0, "completion": "# Singular Value Decomposition (SVD) Implementation\n\nI'll implement a function to compute the SVD of a 2x2 matrix. The SVD decomposes a matrix A into the product U * \u03a3 * V^T, where U and V are orthogonal matrices and \u03a3 is a diagonal matrix containing the singular values.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using the Jacobi method.\n    \n    Args:\n        A: Input 2x2 matrix\n        \n    Returns:\n        tuple: (U, Sigma, V_T) where:\n            - U is a 2x2 orthogonal matrix\n            - Sigma is a 2x2 diagonal matrix with singular values\n            - V_T is the transpose of a 2x2 orthogonal matrix V\n    \"\"\"\n    # Compute A^T * A\n    ATA = A.T @ A\n    \n    # Calculate eigenvalues of A^T * A analytically\n    a, b = ATA[0, 0], ATA[0, 1]\n    c, d = ATA[1, 0], ATA[1, 1]\n    \n    trace = a + d\n    det = a * d - b * c\n    \n    discriminant = trace**2 - 4 * det\n    \n    # Compute eigenvalues (squares of singular values)\n    lambda1 = (trace + np.sqrt(discriminant)) / 2\n    lambda2 = (trace - np.sqrt(discriminant)) / 2\n    \n    # Compute singular values\n    sigma1 = np.sqrt(max(0, lambda1))  # Ensure non-negative\n    sigma2 = np.sqrt(max(0, lambda2))  # Ensure non-negative\n    \n    # Ensure sigma1 >= sigma2\n    if sigma2 > sigma1:\n        sigma1, sigma2 = sigma2, sigma1\n    \n    # Compute eigenvectors for V\n    if abs(b) < 1e-10 and abs(c) < 1e-10:\n        # B is diagonal\n        V = np.eye(2)\n    else:\n        v1 = np.array([b, lambda1 - a])\n        if np.linalg.norm(v1) < 1e-10:\n            v1 = np.array([lambda1 - d, c])\n        v1 = v1 / np.linalg.norm(v1)\n        \n        # v2 should be orthogonal to v1\n        v2 = np.array([-v1[1], v1[0]])\n        \n        V = np.column_stack([v1, v2])\n    \n    # Compute U\n    U = np.zeros((2, 2))\n    \n    # First column of U\n    if sigma1 > 1e-10:\n        U[:, 0] = (A @ V[:, 0]) / sigma1\n    else:\n        # If sigma1 is zero, choose any unit vector\n        U[:, 0] = np.array([1, 0])\n    \n    # Second column of U (orthogonal to first column)\n    U[:, 1] = np.array([-U[1, 0], U[0, 0]])\n    \n    # Create the diagonal matrix of singular values\n    Sigma = np.zeros((2, 2))\n    Sigma[0, 0] = sigma1\n    Sigma[1, 1] = sigma2\n    \n    # Round all results to 4 decimal places\n    U = np.round(U, 4)\n    Sigma = np.round(Sigma, 4)\n    V_T = np.round(V.T, 4)\n    \n    return U, Sigma, V_T\n```\n\nThis implementation:\n1. Computes the eigenvalues of A^T*A analytically\n2. Derives the singular values by taking square roots of these eigenvalues\n3. Calculates the eigenvectors to form matrix V\n4. Computes U from A, V, and the singular values\n5. Ensures orthogonality in both U and V\n6. Rounds all values to 4 decimal places as required\n\nThe function returns a tuple containing the three components of the SVD: (U, \u03a3, V^T)."}
{"task_id": 13, "completion_id": 0, "completion": "# Determinant of a 4x4 Matrix using Laplace's Expansion\n\nI'll implement a recursive solution to calculate the determinant of a 4x4 matrix using Laplace's expansion method.\n\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's expansion.\n    \n    Args:\n        matrix: A 4x4 matrix represented as a list of lists\n        \n    Returns:\n        The determinant value as a float\n    \"\"\"\n    # Validate that the matrix is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Matrix must be 4x4\")\n    \n    # Helper function to get the minor matrix (submatrix after removing row i and column j)\n    def get_minor(matrix, i, j):\n        return [[matrix[row][col] for col in range(len(matrix)) if col != j] \n                for row in range(len(matrix)) if row != i]\n    \n    # Recursive function to calculate determinant of matrices of any size\n    def calculate_determinant(matrix):\n        n = len(matrix)\n        \n        # Base case: 2x2 matrix\n        if n == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        \n        # Recursive case using Laplace's expansion along the first row\n        det = 0\n        for j in range(n):\n            # Get minor matrix by removing first row and column j\n            minor = get_minor(matrix, 0, j)\n            # Add to determinant: element * cofactor\n            # Cofactor = (-1)^(i+j) * minor determinant (i=0 in this case)\n            det += matrix[0][j] * calculate_determinant(minor) * ((-1) ** j)\n        \n        return det\n    \n    return calculate_determinant(matrix)\n```\n\nThis implementation:\n1. Validates that the input is a 4x4 matrix\n2. Defines a helper function to extract minor matrices\n3. Uses a recursive approach to calculate determinants of any size matrix\n4. For a 4x4 matrix, it applies Laplace's expansion along the first row:\n   - For each element in the first row, remove its row and column to get a 3x3 minor\n   - Calculate the determinant of that 3x3 minor recursively\n   - Multiply by the sign factor (-1)^(i+j) to get the cofactor\n   - Sum the products of elements and their cofactors\n\nThe recursion continues until we reach 2x2 matrices, which are calculated directly using the formula ad-bc."}
{"task_id": 14, "completion_id": 0, "completion": "# Linear Regression Using Normal Equation\n\nI'll implement a linear regression function using the normal equation. This approach directly computes the optimal parameters without iterative optimization.\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays\n    X_array = np.array(X)\n    y_array = np.array(y)\n    \n    # Compute the normal equation: \u03b8 = (X^T X)^(-1) X^T y\n    # First calculate X^T X\n    X_transpose_X = X_array.T @ X_array\n    \n    # Then calculate X^T y\n    X_transpose_y = X_array.T @ y_array\n    \n    # Solve for the coefficients using the normal equation\n    # Using pinv instead of inv handles cases where X^T X might not be invertible\n    coefficients = np.linalg.pinv(X_transpose_X) @ X_transpose_y\n    \n    # Round to 4 decimal places\n    coefficients = np.round(coefficients, 4)\n    \n    # Convert numpy array to list for return\n    return coefficients.tolist()\n```\n\nThis implementation:\n1. Converts the input lists to numpy arrays for efficient matrix operations\n2. Calculates the terms in the normal equation: (X^T X)^(-1) X^T y\n3. Uses the pseudoinverse (pinv) which is more stable than a regular inverse\n4. Rounds all results to 4 decimal places as required\n5. Returns the coefficients as a list\n\nThe function will work correctly even with potential numerical issues like multicollinearity in the input features."}
{"task_id": 15, "completion_id": 0, "completion": "# Linear Regression Using Gradient Descent\n\nI'll implement a gradient descent algorithm for linear regression. This algorithm works by iteratively updating the model parameters (coefficients) in the direction that minimizes the cost function.\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    Args:\n        X: Feature matrix with a column of ones for the intercept\n        y: Target values\n        alpha: Learning rate\n        iterations: Number of iterations\n        \n    Returns:\n        List of coefficients (weights) for the linear regression model\n    \"\"\"\n    # Get number of samples and features\n    m, n = X.shape\n    \n    # Initialize coefficients with zeros\n    theta = np.zeros((n, 1))\n    \n    # Reshape y to ensure it's a column vector\n    y = y.reshape(-1, 1)\n    \n    # Gradient descent\n    for _ in range(iterations):\n        # Compute predictions\n        predictions = X @ theta\n        \n        # Compute errors\n        errors = predictions - y\n        \n        # Update parameters using gradient descent\n        gradient = (1/m) * (X.T @ errors)\n        theta = theta - alpha * gradient\n    \n    # Round to 4 decimal places and convert to list\n    rounded_theta = np.round(theta, 4)\n    \n    # Convert to list and handle negative zeros\n    result = rounded_theta.flatten().tolist()\n    \n    # Replace -0.0 with 0.0 for cleaner output\n    for i in range(len(result)):\n        if result[i] == -0.0:\n            result[i] = 0.0\n            \n    return result\n```\n\nThis implementation:\n\n1. Initializes the coefficients (theta) with zeros\n2. Iteratively updates the coefficients using the gradient descent formula\n3. The gradient is calculated as (1/m) * X^T * (X * theta - y)\n4. After the specified number of iterations, the coefficients are rounded to 4 decimal places\n5. The function handles the case of negative zeros (-0.0) by converting them to positive zeros (0.0)\n6. Returns the coefficients as a Python list\n\nThe algorithm minimizes the mean squared error cost function by taking steps proportional to the negative of the gradient."}
{"task_id": 16, "completion_id": 0, "completion": "# Feature Scaling Implementation\n\nI'll create a Python function that performs both standardization and min-max normalization on a dataset.\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Performs feature scaling on a dataset using standardization and min-max normalization.\n    \n    Args:\n        data: 2D NumPy array where rows are samples and columns are features.\n        \n    Returns:\n        Tuple of two 2D lists:\n        - First list: data scaled using standardization\n        - Second list: data scaled using min-max normalization\n    \"\"\"\n    # Standardization (z-score normalization)\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    \n    # Create a safe version of std for division\n    std_safe = np.copy(std)\n    std_safe[std_safe == 0] = 1  # Avoid division by zero\n    \n    standardized = (data - mean) / std_safe\n    \n    # For features with zero std, set standardized values to 0\n    zero_std_mask = (std == 0)\n    if np.any(zero_std_mask):\n        standardized[:, zero_std_mask] = 0\n    \n    # Min-max normalization\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    range_vals = max_vals - min_vals\n    \n    # Create a safe version of range for division\n    range_safe = np.copy(range_vals)\n    range_safe[range_safe == 0] = 1  # Avoid division by zero\n    \n    min_max_normalized = (data - min_vals) / range_safe\n    \n    # For features with zero range, set normalized values to 0.5\n    zero_range_mask = (range_vals == 0)\n    if np.any(zero_range_mask):\n        min_max_normalized[:, zero_range_mask] = 0.5\n    \n    # Round to 4 decimal places\n    standardized = np.round(standardized, 4)\n    min_max_normalized = np.round(min_max_normalized, 4)\n    \n    # Convert NumPy arrays to Python lists\n    standardized_list = standardized.tolist()\n    min_max_list = min_max_normalized.tolist()\n    \n    return standardized_list, min_max_list\n```\n\nThis implementation:\n\n1. Performs standardization by subtracting the mean and dividing by the standard deviation\n2. Performs min-max normalization by scaling features to the range [0,1]\n3. Handles edge cases:\n   - Features with zero standard deviation (constant features)\n   - Features with zero range (constant features)\n4. Rounds all values to 4 decimal places\n5. Converts NumPy arrays to Python lists as required\n\nThe function returns two lists containing the standardized and min-max normalized data respectively."}
{"task_id": 17, "completion_id": 0, "completion": "# K-Means Clustering Algorithm Implementation\n\nI'll implement a function that performs k-means clustering on a set of points and returns the final centroids.\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-means clustering algorithm.\n    \n    Args:\n        points: A list of points, where each point is a tuple of coordinates.\n        k: The number of clusters to form.\n        initial_centroids: A list of initial centroid points.\n        max_iterations: The maximum number of iterations to perform.\n        \n    Returns:\n        A list of the final centroids rounded to 4 decimal places.\n    \"\"\"\n    # Convert points and centroids to numpy arrays for easier computation\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        clusters = [[] for _ in range(k)]\n        for point in points_array:\n            # Compute squared distances from the point to each centroid\n            distances = np.sum((point - centroids) ** 2, axis=1)\n            # Find the index of the nearest centroid\n            closest_centroid_idx = np.argmin(distances)\n            # Assign the point to the cluster of the nearest centroid\n            clusters[closest_centroid_idx].append(point)\n        \n        # Store the previous centroids to check for convergence\n        previous_centroids = centroids.copy()\n        \n        # Update centroids\n        for i in range(k):\n            if clusters[i]:  # Ensure the cluster is not empty\n                centroids[i] = np.mean(clusters[i], axis=0)\n        \n        # Check for convergence (if centroids don't change much)\n        if np.allclose(previous_centroids, centroids, atol=1e-8):\n            break\n    \n    # Round the centroids to the nearest 4th decimal\n    centroids = np.round(centroids, 4)\n    \n    # Convert the centroids back to a list of tuples\n    return [tuple(float(coord) for coord in centroid) for centroid in centroids]\n```\n\nThis implementation follows the standard k-means algorithm:\n\n1. Start with the provided initial centroids\n2. Repeatedly:\n   - Assign each point to its nearest centroid (forming clusters)\n   - Recalculate centroids as the mean of all points in each cluster\n   - Check for convergence\n3. Round final centroids to 4 decimal places as required\n\nThe algorithm terminates when either:\n- It reaches the maximum number of iterations, or\n- The centroids stop changing significantly between iterations (convergence)"}
{"task_id": 18, "completion_id": 0, "completion": "# K-Fold Cross-Validation Implementation\n\nI'll implement a function that generates train and test splits for K-fold cross-validation. This technique is widely used in machine learning to evaluate model performance.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for k-fold cross-validation.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Features matrix.\n    y : np.ndarray\n        Target vector.\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int or None, default=None\n        Random seed for reproducibility.\n        \n    Returns:\n    --------\n    folds : list of tuples\n        List of (train_indices, test_indices) tuples for each fold.\n    \"\"\"\n    # Validate input\n    if len(X) != len(y):\n        raise ValueError(\"X and y must have the same number of samples\")\n    \n    if k <= 1:\n        raise ValueError(\"k must be greater than 1\")\n    \n    # Get the number of samples\n    n_samples = len(X)\n    \n    # Create array of indices\n    indices = np.arange(n_samples)\n    \n    # Shuffle indices if requested\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    # Split the indices into k roughly equal folds\n    fold_indices = np.array_split(indices, k)\n    \n    # Generate the train-test splits for each fold\n    folds = []\n    for i in range(k):\n        # Use fold i as the test set\n        test_indices = fold_indices[i]\n        \n        # Use all other folds as the training set\n        train_indices = np.concatenate([fold_indices[j] for j in range(k) if j != i])\n        \n        # Add the train-test split to the list of folds\n        folds.append((train_indices, test_indices))\n    \n    return folds\n```\n\n## Example Usage:\n\n```python\n# Sample data\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\ny = np.array([0, 1, 0, 1, 0, 1])\n\n# Generate 3-fold cross-validation splits\nfolds = k_fold_cross_validation(X, y, k=3, random_seed=42)\n\n# Print the train-test splits\nfor i, (train_idx, test_idx) in enumerate(folds):\n    print(f\"Fold {i+1}:\")\n    print(f\"  Train indices: {train_idx}\")\n    print(f\"  Test indices: {test_idx}\")\n    print(f\"  Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n```\n\nThis implementation:\n1. Validates the input data\n2. Creates an array of indices and optionally shuffles them\n3. Splits the indices into k roughly equal folds using `np.array_split`\n4. For each fold i, uses fold i as the test set and all other folds as the training set\n5. Returns a list of tuples, where each tuple contains the train and test indices for a fold"}
{"task_id": 19, "completion_id": 0, "completion": "# Principal Component Analysis (PCA) Implementation\n\nI'll implement a PCA function from scratch that follows the required specifications. Here's my solution:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis on the input data.\n    \n    Args:\n        data: Input data matrix where rows are samples and columns are features\n        k: Number of principal components to return\n        \n    Returns:\n        A list of lists representing the top k principal components (eigenvectors)\n    \"\"\"\n    # Step 1: Standardize the dataset\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    # Handle any features with zero standard deviation\n    std[std == 0] = 1.0\n    standardized_data = (data - mean) / std\n    \n    # Step 2: Compute the covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Ensure values are real (in case of numerical precision issues)\n    eigenvalues = np.real(eigenvalues)\n    eigenvectors = np.real(eigenvectors)\n    \n    # Step 4: Sort eigenvectors by decreasing eigenvalues\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Step 5: Take the top k eigenvectors as principal components\n    principal_components = eigenvectors[:, :k]\n    \n    # Round to 4 decimal places and convert to list format\n    principal_components = np.round(principal_components, 4).tolist()\n    \n    return principal_components\n```\n\nThis implementation:\n1. Standardizes the data by subtracting the mean and dividing by the standard deviation\n2. Calculates the covariance matrix of the standardized data\n3. Computes the eigenvalues and eigenvectors of the covariance matrix\n4. Sorts the eigenvectors by their corresponding eigenvalues in descending order\n5. Selects the top k eigenvectors as the principal components\n6. Rounds all values to 4 decimal places as required\n\nThe returned result is a list of lists where each inner list represents the coefficients of a feature in the principal components."}
{"task_id": 20, "completion_id": 0, "completion": "# Decision Tree Learning Implementation\n\nI'll implement a decision tree learning algorithm based on entropy and information gain for classification. Let me walk through how this algorithm works:\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Build a decision tree using recursive binary splitting based on entropy and information gain.\n    \n    Args:\n        examples: List of dictionaries where each dictionary represents an example with attribute-value pairs\n        attributes: List of attribute names to consider for splitting\n        target_attr: The name of the target attribute to predict\n    \n    Returns:\n        A nested dictionary representing the decision tree\n    \"\"\"\n    # Base case 1: No examples left\n    if not examples:\n        return {\"leaf\": None}  # This should be handled by the caller\n    \n    # Base case 2: All examples have the same target value\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return {\"leaf\": target_values[0]}\n    \n    # Base case 3: No attributes left to split on\n    if not attributes:\n        counter = Counter(target_values)\n        most_common_target = counter.most_common(1)[0][0]\n        return {\"leaf\": most_common_target}\n    \n    # Find the best attribute to split on based on information gain\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    \n    # Create a new decision tree with the best attribute as the root\n    tree = {\"attribute\": best_attr, \"branches\": {}}\n    \n    # Create a new list of attributes (excluding the best one)\n    remaining_attrs = [attr for attr in attributes if attr != best_attr]\n    \n    # Get all possible values of the best attribute\n    attr_values = set(example[best_attr] for example in examples)\n    \n    # Create branches for each value of the best attribute\n    for value in attr_values:\n        # Create a subset of examples where best_attr has the current value\n        subset = [example for example in examples if example[best_attr] == value]\n        \n        # If subset is empty, use the most common target value from parent\n        if not subset:\n            counter = Counter(target_values)\n            most_common_target = counter.most_common(1)[0][0]\n            tree[\"branches\"][value] = {\"leaf\": most_common_target}\n        else:\n            # Recursive call to build subtree\n            tree[\"branches\"][value] = learn_decision_tree(subset, remaining_attrs, target_attr)\n    \n    return tree\n\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Choose the best attribute to split on based on information gain.\n    \n    Args:\n        examples: List of example dictionaries\n        attributes: List of candidate attributes\n        target_attr: Target attribute name\n    \n    Returns:\n        The name of the attribute with the highest information gain\n    \"\"\"\n    # Calculate entropy of the entire dataset\n    target_values = [example[target_attr] for example in examples]\n    total_entropy = entropy(target_values)\n    \n    best_gain = -1\n    best_attr = None\n    \n    # Calculate information gain for each attribute\n    for attr in attributes:\n        # Group examples by attribute value\n        value_subsets = {}\n        for example in examples:\n            value = example[attr]\n            if value not in value_subsets:\n                value_subsets[value] = []\n            value_subsets[value].append(example[target_attr])\n        \n        # Calculate weighted entropy for this attribute\n        attr_entropy = 0\n        for value_targets in value_subsets.values():\n            weight = len(value_targets) / len(examples)\n            attr_entropy += weight * entropy(value_targets)\n        \n        # Calculate information gain\n        gain = total_entropy - attr_entropy\n        \n        # Update best attribute if this one has higher gain\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    \n    return best_attr\n\ndef entropy(values: list) -> float:\n    \"\"\"\n    Calculate the entropy of a list of values.\n    \n    Args:\n        values: List of categorical values\n        \n    Returns:\n        The entropy of the value distribution\n    \"\"\"\n    if not values:\n        return 0\n    \n    # Count occurrences of each value\n    counter = Counter(values)\n    total = len(values)\n    \n    # Calculate entropy using the formula: -\u03a3(p_i * log2(p_i))\n    result = 0\n    for count in counter.values():\n        probability = count / total\n        result -= probability * math.log2(probability)\n    \n    return result\n```\n\nThis implementation:\n1. Recursively builds a decision tree by selecting the attribute with the highest information gain at each step\n2. Handles base cases (all examples have same class, no attributes left, etc.)\n3. Creates branches for each value of the selected attribute\n4. Returns a nested dictionary that represents the decision tree structure\n\nThe algorithm will continue splitting until it reaches a pure node (all examples have the same target value) or runs out of attributes to split on."}
{"task_id": 21, "completion_id": 0, "completion": "# Pegasos Kernel SVM Implementation\n\nI'll implement a deterministic version of the Pegasos algorithm for training a kernel SVM classifier from scratch. This implementation will use all samples in every iteration (unlike the stochastic version that selects random samples).\n\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm for kernel SVM.\n    \n    Parameters:\n    -----------\n    data: np.ndarray\n        2D array where each row is a data sample\n    labels: np.ndarray\n        1D array of binary labels (should be +1 or -1)\n    kernel: str\n        The kernel to use ('linear' or 'rbf')\n    lambda_val: float\n        Regularization parameter\n    iterations: int\n        Number of iterations for training\n    sigma: float\n        Parameter for RBF kernel\n        \n    Returns:\n    --------\n    alphas: list\n        Alpha coefficients for each training sample\n    bias: float\n        The bias term for the SVM\n    \"\"\"\n    num_samples = data.shape[0]\n    \n    # Initialize alpha coefficients and bias\n    alphas = np.zeros(num_samples)\n    bias = 0.0\n    \n    # Define kernel functions\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n    \n    def rbf_kernel(x1, x2):\n        return np.exp(-np.sum((x1 - x2) ** 2) / (2 * sigma ** 2))\n    \n    # Choose kernel function\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = rbf_kernel\n    else:\n        raise ValueError(\"Kernel must be 'linear' or 'rbf'\")\n    \n    # Precompute kernel matrix\n    K = np.zeros((num_samples, num_samples))\n    for i in range(num_samples):\n        for j in range(num_samples):\n            K[i, j] = kernel_func(data[i], data[j])\n    \n    # Training loop\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)  # Learning rate\n        \n        # Calculate predictions for all samples\n        predictions = np.zeros(num_samples)\n        for i in range(num_samples):\n            for j in range(num_samples):\n                predictions[i] += alphas[j] * labels[j] * K[i, j]\n            predictions[i] += bias\n        \n        # Identify samples that are incorrectly classified or within the margin\n        margin_violations = (labels * predictions) < 1\n        \n        # Update alphas with regularization decay\n        alphas = (1 - eta * lambda_val) * alphas\n        \n        # Add gradient update term for margin-violating samples\n        for i in range(num_samples):\n            if margin_violations[i]:\n                alphas[i] += eta * labels[i]\n        \n        # Update bias (no regularization for bias)\n        if np.sum(margin_violations) > 0:\n            bias += eta * np.mean(labels[margin_violations])\n    \n    # Round to 4 decimal places\n    alphas = np.round(alphas, 4)\n    bias = np.round(bias, 4)\n    \n    return alphas.tolist(), bias\n```\n\nThis implementation follows the deterministic Pegasos algorithm with the following key components:\n\n1. Kernel computation - supports both linear and RBF kernels\n2. Alpha coefficient updates - applies regularization decay to all coefficients and adds the gradient update for margin violations\n3. Bias updates - adjusts based on the average error of margin-violating samples\n4. Round results to 4 decimal places as required\n\nThe algorithm deterministically updates all samples in each iteration and returns the model parameters (alpha coefficients and bias) that can be used for prediction."}
{"task_id": 22, "completion_id": 0, "completion": "# Sigmoid Activation Function Solution\n\nThe sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve, commonly used in machine learning as an activation function. It maps any input value to a value between 0 and 1.\n\nHere's the implementation:\n\n```python\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function value for input z.\n    \n    Args:\n        z (float): The input value\n    \n    Returns:\n        float: The sigmoid value of z, rounded to 4 decimal places\n    \"\"\"\n    # Calculate sigmoid: 1 / (1 + e^(-z))\n    sig_value = 1 / (1 + math.exp(-z))\n    \n    # Round to 4 decimal places and return\n    return round(sig_value, 4)\n```\n\nThis implementation:\n1. Takes a floating-point input `z`\n2. Calculates the sigmoid using the formula: 1 / (1 + e^(-z))\n3. Rounds the result to 4 decimal places\n4. Returns the final value\n\nExample values:\n- sigmoid(0) = 0.5\n- sigmoid(2) = 0.8808\n- sigmoid(-2) = 0.1192"}
{"task_id": 23, "completion_id": 0, "completion": "# Softmax Activation Function Implementation\n\nHere's the implementation of the softmax function with the specified requirements:\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a list of scores.\n    \n    Args:\n        scores: A list of floating-point numbers\n        \n    Returns:\n        A list of softmax values rounded to 4 decimal places\n    \"\"\"\n    # Subtract max score for numerical stability\n    max_score = max(scores)\n    shifted_scores = [score - max_score for score in scores]\n    \n    # Compute exponentials of shifted scores\n    exp_scores = [math.exp(score) for score in shifted_scores]\n    \n    # Compute sum of exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute softmax values and round to 4 decimal places\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_scores\n```\n\nThe implementation includes a numerical stability optimization by subtracting the maximum score before computing exponentials, which prevents potential overflow issues with large values while preserving the mathematical properties of the softmax function."}
{"task_id": 24, "completion_id": 0, "completion": "# Single Neuron Implementation\n\nI'll implement a function that simulates a single neuron with sigmoid activation for binary classification:\n\n```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Convert inputs to NumPy arrays for vector operations\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    weights_array = np.array(weights)\n    \n    # Calculate weighted sum (z = w\u00b7x + b)\n    z = np.dot(features_array, weights_array) + bias\n    \n    # Apply sigmoid activation function: \u03c3(z) = 1/(1 + e^(-z))\n    predictions = 1 / (1 + np.exp(-z))\n    \n    # Calculate mean squared error: MSE = (1/n)\u2211(y_pred - y_true)\u00b2\n    mse = np.mean(np.square(predictions - labels_array))\n    \n    # Round results to 4 decimal places\n    rounded_predictions = np.round(predictions, 4).tolist()\n    rounded_mse = round(mse, 4)\n    \n    return rounded_predictions, rounded_mse\n```\n\n## How It Works\n\nThis function:\n\n1. Converts input data to NumPy arrays for efficient computation\n2. Calculates the weighted sum of inputs (z = w\u00b7x + b)\n3. Applies the sigmoid activation function to get probabilities\n4. Computes the mean squared error between predictions and true labels\n5. Rounds all results to 4 decimal places as required\n\nThe sigmoid function squeezes the neuron's output between 0 and 1, making it suitable for binary classification where the output can be interpreted as a probability."}
{"task_id": 25, "completion_id": 0, "completion": "# Single Neuron with Backpropagation\n\nI'll implement a function that simulates a single neuron with sigmoid activation and updates weights and bias using backpropagation.\n\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    # Initialize weights, bias, and MSE history\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    \n    # Define sigmoid function\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Forward pass\n        z = np.dot(features, weights) + bias\n        y_pred = sigmoid(z)\n        \n        # Compute MSE loss\n        mse = np.mean((y_pred - labels) ** 2)\n        mse_history.append(round(mse, 4))\n        \n        # Compute gradients for backpropagation\n        # dL/dy_pred = 2(y_pred - y_true)/n\n        # dy_pred/dz = y_pred * (1 - y_pred)\n        gradient_factor = 2 * (y_pred - labels) * y_pred * (1 - y_pred) / len(labels)\n        \n        # Gradient for weights: dL/dw = dL/dz * dz/dw = gradient_factor * x\n        dL_dw = np.dot(features.T, gradient_factor)\n        \n        # Gradient for bias: dL/db = dL/dz * dz/db = gradient_factor * 1\n        dL_db = np.sum(gradient_factor)\n        \n        # Update weights and bias using gradient descent\n        weights = weights - learning_rate * dL_dw\n        bias = bias - learning_rate * dL_db\n    \n    # Round weights and bias to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    \n    return weights.tolist(), bias, mse_history\n```\n\nThis implementation:\n1. Performs forward propagation using the sigmoid activation function\n2. Calculates the MSE loss\n3. Computes gradients for the weights and bias using the chain rule\n4. Updates the parameters using gradient descent\n5. Tracks the MSE for each epoch\n6. Returns the updated weights (as a Python list), bias, and MSE history rounded to 4 decimal places\n\nThe backpropagation process follows the chain rule to compute the gradient of the loss with respect to the weights and bias, which allows us to update the parameters in a direction that minimizes the loss."}
{"task_id": 26, "completion_id": 0, "completion": "# Implementing Basic Autograd Operations\n\nLet me implement the `Value` class with basic autograd operations for automatic differentiation: addition, multiplication, and ReLU activation. This implementation is inspired by Andrej Karpathy's micrograd.\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n    \n    def __add__(self, other):\n        # Convert other to Value if it's a scalar\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        \n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def __mul__(self, other):\n        # Convert other to Value if it's a scalar\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        \n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n        \n        def _backward():\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def backward(self):\n        # Topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        # Go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n    \n    # Helper methods for convenience\n    def __radd__(self, other):  # other + self\n        return self + other\n    \n    def __rmul__(self, other):  # other * self\n        return self * other\n    \n    def __neg__(self):  # -self\n        return self * -1\n    \n    def __sub__(self, other):  # self - other\n        return self + (-other)\n    \n    def __rsub__(self, other):  # other - self\n        return other + (-self)\n        \n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n```\n\n## How the Implementation Works\n\n1. **Value Class**: Holds a scalar value and its gradient, plus a computational graph structure.\n\n2. **Operations**:\n   - **Addition**: Creates a new Value with the sum of inputs. In backprop, gradients flow directly to both inputs.\n   - **Multiplication**: Creates a new Value with the product. In backprop, gradients are scaled by the other input value.\n   - **ReLU**: Implements max(0,x). In backprop, gradients only flow if the input was positive.\n\n3. **Backward Method**: Traverses the computational graph in reverse topological order, applying the chain rule to propagate gradients from outputs to inputs.\n\n4. **Helper Methods**: Support for operations like subtraction and negation, plus right-hand versions of addition and multiplication for scalar operations.\n\n## Usage Example\n\n```python\n# Create inputs\na = Value(2.0)\nb = Value(3.0)\n\n# Create a simple computation\nc = a * b\nd = c + Value(1.0)\ne = d.relu()\n\n# Run backpropagation\ne.backward()\n\n# Check gradients\nprint(a)  # Value(data=2.0, grad=3.0)\nprint(b)  # Value(data=3.0, grad=2.0)\n```\n\nThis implementation provides a simple yet powerful way to build computational graphs that can automatically compute gradients through backpropagation."}
{"task_id": 27, "completion_id": 0, "completion": "# Understanding the Transformation Matrix Between Bases\n\nTo find the transformation matrix P from basis B to basis C, I need to build a matrix that converts the coordinates of any vector expressed in basis B to its coordinates in basis C.\n\nThe mathematical relationship is:\n- If [v]_B represents coordinates in basis B\n- And [v]_C represents the same vector in basis C\n- Then [v]_C = P \u00b7 [v]_B\n\nHere's my implementation:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert basis vectors to matrices where columns are basis vectors\n    B_matrix = np.array(B).T  \n    C_matrix = np.array(C).T\n    \n    # The transformation matrix is C^(-1) \u00b7 B\n    P = np.linalg.inv(C_matrix) @ B_matrix\n    \n    # Round to 4 decimal places\n    P = np.round(P, 4)\n    \n    return P.tolist()\n```\n\n## Explanation:\n1. I create matrices whose columns are the basis vectors (hence the transpose)\n2. The transformation matrix P = C^(-1) \u00b7 B converts coordinates from basis B to basis C\n3. This works because:\n   - B transforms coordinates in basis B to coordinates in the standard basis\n   - C^(-1) transforms coordinates in the standard basis to coordinates in basis C\n   - Therefore, P = C^(-1) \u00b7 B transforms coordinates from basis B to basis C\n\nThe result is rounded to 4 decimal places as required."}
{"task_id": 28, "completion_id": 0, "completion": "# SVD of a 2x2 Matrix using Eigen Values & Vectors\n\nI'll implement a function to compute the SVD of a 2x2 matrix following the approach described in the blog post.\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix.\n    \n    Parameters:\n    A (np.ndarray): 2x2 matrix\n    \n    Returns:\n    tuple: (U, S, V) where A = U * S * V^T, with results rounded to 4 decimal places\n    \"\"\"\n    # Compute A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Compute eigenvalues and eigenvectors of A^T * A\n    eigenvalues, eigenvectors = np.linalg.eig(ATA)\n    \n    # Ensure eigenvalues are real and non-negative\n    eigenvalues = np.real(eigenvalues)\n    eigenvalues[eigenvalues < 0] = 0\n    \n    # Sort eigenvalues in descending order and adjust eigenvectors accordingly\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Compute V (matrix of eigenvectors)\n    V = np.real(eigenvectors)\n    \n    # Compute singular values (square roots of eigenvalues)\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Compute U using the formula: u_i = (A * v_i) / \u03c3_i\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 1e-10:  # Avoid division by very small numbers\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n        else:\n            # Handle the case when singular value is zero\n            if i == 0:\n                U[:, i] = np.array([1, 0])\n            else:\n                # Create a vector orthogonal to the first column\n                U[:, i] = np.array([-U[1, 0], U[0, 0]])\n                norm = np.sqrt(U[0, i]**2 + U[1, i]**2)\n                if norm > 0:\n                    U[:, i] /= norm\n    \n    # Create diagonal matrix S with singular values\n    S = np.zeros((2, 2))\n    S[0, 0] = singular_values[0]\n    S[1, 1] = singular_values[1]\n    \n    # Round results to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    return U.tolist(), S.tolist(), V.tolist()\n```\n\nThis implementation:\n\n1. Computes A^T \u00d7 A and finds its eigenvalues and eigenvectors\n2. Sorts the eigenvalues in descending order (and eigenvectors accordingly)\n3. Sets V as the matrix of eigenvectors\n4. Computes the singular values as the square roots of eigenvalues\n5. Constructs U by calculating A \u00d7 v_i / \u03c3_i for each column\n6. Handles the special case of zero singular values\n7. Creates the diagonal matrix S\n8. Rounds all results to 4 decimal places\n9. Returns the matrices as Python lists using the tolist() method\n\nThe result gives us U, S, and V such that A = U \u00d7 S \u00d7 V^T."}
{"task_id": 29, "completion_id": 0, "completion": "# Python Function for Random Data Shuffling\n\nHere's my implementation of a function that randomly shuffles two numpy arrays while maintaining their correspondence:\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two numpy arrays in the same order.\n    \n    Parameters:\n    -----------\n    X : numpy.ndarray\n        First array to shuffle\n    y : numpy.ndarray\n        Second array to shuffle\n    seed : int, optional\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    list, list\n        Shuffled X and y as Python lists\n    \"\"\"\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Get the number of samples\n    n_samples = len(X)\n    \n    # Generate a random permutation of indices\n    indices = np.random.permutation(n_samples)\n    \n    # Shuffle X and y using the same permutation\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert to Python lists\n    X_list = X_shuffled.tolist()\n    y_list = y_shuffled.tolist()\n    \n    return X_list, y_list\n```\n\nThis function:\n1. Takes numpy arrays X and y as input\n2. Accepts an optional seed parameter for reproducible shuffling\n3. Uses the same random permutation for both arrays to maintain correspondence\n4. Returns the shuffled data as Python lists using numpy's tolist() method\n\nThe implementation ensures that the relationship between elements in X and y is preserved after shuffling."}
{"task_id": 30, "completion_id": 0, "completion": "# Batch Iterator for Dataset Implementation\n\nLet me implement a batch iterator function that processes numpy arrays efficiently according to your requirements:\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from X and optionally y.\n    \n    Parameters:\n    -----------\n    X : numpy.ndarray\n        Input features\n    y : numpy.ndarray, optional\n        Target values\n    batch_size : int, default=64\n        Size of each batch\n        \n    Yields:\n    -------\n    If y is None:\n        list: A batch of X samples as a Python list\n    If y is not None:\n        tuple: (X_batch, y_batch) where both are Python lists\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        X_batch = X[batch_indices].tolist()\n        \n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n```\n\nThis implementation:\n1. Takes a numpy array `X` and an optional numpy array `y`\n2. Creates batches of size `batch_size` by slicing the arrays\n3. Converts numpy arrays to Python lists using the `.tolist()` method as required\n4. Yields either X batches alone or (X, y) batch pairs depending on whether y is provided\n5. Handles the last batch correctly even if it's smaller than the specified batch size\n\nThe iterator will process all samples in the original order, delivering them in batches of the specified size."}
{"task_id": 31, "completion_id": 0, "completion": "# Python Function to Divide Dataset Based on Feature Threshold\n\nHere's my implementation of the requested function. This will split a dataset into two subsets based on whether values in a specified feature column are greater than or equal to a threshold value:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset based on if sample value of feature_i is >= threshold\n    \n    Parameters:\n    -----------\n    X : numpy array\n        Dataset where rows are samples and columns are features\n    feature_i : int\n        The index of the feature to divide on\n    threshold : float\n        The threshold value to divide the dataset on\n        \n    Returns:\n    --------\n    tuple (list, list)\n        Two lists containing samples where the feature value is greater than\n        or equal to the threshold and samples where it's less than the threshold\n    \"\"\"\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Create mask for samples where feature value >= threshold\n    mask = X[:, feature_i] >= threshold\n    \n    # Divide dataset based on the mask\n    X_1 = X[mask]\n    X_2 = X[~mask]\n    \n    # Return as Python lists using tolist() method\n    return X_1.tolist(), X_2.tolist()\n```\n\nThis function:\n1. Takes a dataset X, feature index, and threshold value as inputs\n2. Creates a boolean mask identifying samples where the feature value meets the condition\n3. Applies the mask to split the dataset into two parts\n4. Converts both resulting numpy arrays to Python lists before returning them\n\nThe implementation handles the division efficiently using numpy's boolean indexing, resulting in clean, readable code."}
{"task_id": 32, "completion_id": 0, "completion": "# Polynomial Feature Generation Function\n\nI'll create a function that generates polynomial features from a dataset up to a specified degree.\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree.\n    \n    Parameters:\n    X (numpy.ndarray): 2D array where rows are samples and columns are features\n    degree (int): The maximum polynomial degree to generate\n    \n    Returns:\n    list: A 2D list containing the original data with polynomial features\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Generate all combinations of features for each degree\n    combinations = []\n    for d in range(0, degree + 1):\n        combinations.extend(combinations_with_replacement(range(n_features), d))\n    \n    # Initialize output matrix\n    n_output_features = len(combinations)\n    X_poly = np.ones((n_samples, n_output_features))\n    \n    # For each combination of features\n    for i, combo in enumerate(combinations):\n        if len(combo) == 0:  # Constant term (already ones)\n            continue\n        else:\n            # Multiply the features according to the combination\n            X_poly[:, i] = np.prod(X[:, combo], axis=1)\n    \n    return X_poly.tolist()\n```\n\n## How It Works\n\n1. The function takes a 2D numpy array `X` and an integer `degree` as input.\n2. It generates all possible combinations of features (with replacement) up to the specified degree.\n3. A new matrix is initialized with ones (for the constant term).\n4. For each combination of features, it computes the product of those features for each sample.\n5. The result is returned as a Python list using the `tolist()` method.\n\nFor example, if we have features [a, b] and degree=2, the function will generate:\n- Degree 0: 1 (constant term)\n- Degree 1: a, b\n- Degree 2: a\u00b2, ab, b\u00b2"}
{"task_id": 33, "completion_id": 0, "completion": "# Understanding the Task\nI need to create a function to generate random subsets of a given dataset. This function should:\n- Take a 2D array X (features), a 1D array y (labels), number of subsets to generate, and a parameter to decide sampling method\n- Return a list of tuples, each containing a random subset of X and its corresponding y values\n- Convert numpy arrays to Python lists in the returned result\n\n# Solution\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n    \n    Parameters:\n        X (numpy.ndarray): 2D array of features\n        y (numpy.ndarray): 1D array of labels\n        n_subsets (int): Number of random subsets to generate\n        replacements (bool): Whether to sample with replacements\n        seed (int): Random seed for reproducibility\n    \n    Returns:\n        list: List of tuples where each tuple contains (X_subset, y_subset) as Python lists\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Get number of samples in dataset\n    n_samples = X.shape[0]\n    \n    # List to store random subsets\n    random_subsets = []\n    \n    # Generate n_subsets random subsets\n    for _ in range(n_subsets):\n        # Generate random indices based on replacement parameter\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        \n        # Extract subset using these indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Convert to Python lists and append to result\n        random_subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return random_subsets\n```\n\n# Explanation\n1. The function takes in the required parameters and sets the random seed for reproducibility.\n2. For each of the `n_subsets` iterations:\n   - It generates random indices using `np.random.choice`, where:\n     - The sampling is done from indices 0 to n_samples-1\n     - The size of each subset is the same as the original dataset\n     - The `replace` parameter controls whether an index can be selected multiple times\n   - It selects the corresponding rows from X and y using these indices\n   - It converts the numpy arrays to Python lists using `tolist()` method\n   - It adds the tuple (X_subset, y_subset) to the result list\n\nThis implementation ensures sampling with or without replacement as specified and returns the data in the required format."}
{"task_id": 34, "completion_id": 0, "completion": "# One-Hot Encoding Implementation\n\nI'll implement the one-hot encoding function as requested. This function will convert integer categorical values into one-hot encoded vectors.\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D array of integer values to a one-hot encoded matrix.\n    \n    Parameters:\n    -----------\n    x : array-like\n        1D array of integer values to be encoded.\n    n_col : int, optional\n        Number of columns for the one-hot encoded array.\n        If None, determined automatically from the maximum value in x.\n        \n    Returns:\n    --------\n    list\n        One-hot encoded matrix as a Python list.\n    \"\"\"\n    # Convert input to numpy array\n    x = np.array(x, dtype=int)\n    \n    # Determine number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Create a matrix of zeros with shape (len(x), n_col)\n    one_hot = np.zeros((len(x), n_col), dtype=int)\n    \n    # Set the appropriate positions to 1\n    for i, val in enumerate(x):\n        one_hot[i, val] = 1\n    \n    # Convert to Python list and return\n    return one_hot.tolist()\n```\n\nThis implementation:\n1. Converts the input to a numpy array\n2. Determines the number of columns automatically if not provided\n3. Creates a zero matrix with the appropriate dimensions\n4. Sets the corresponding positions to 1 based on the input values\n5. Returns the result as a Python list using tolist()\n\nThe function handles any 1D array of integer values and correctly produces the one-hot encoded representation."}
{"task_id": 35, "completion_id": 0, "completion": "# Solution: Convert Vector to Diagonal Matrix\n\nHere's a function that converts a 1D numpy array into a diagonal matrix and returns it as a Python list:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of values to place on the diagonal\n    \n    Returns:\n    list: A 2D list representation of the diagonal matrix\n    \"\"\"\n    # Create a diagonal matrix using numpy's diag function\n    diagonal_matrix = np.diag(x)\n    \n    # Convert to Python list as required by the constraints\n    return diagonal_matrix.tolist()\n```\n\nThis solution uses NumPy's `diag()` function, which creates a diagonal matrix with the input array as its diagonal elements. The function then converts the NumPy array to a Python list using the `tolist()` method as specified in the constraints.\n\nExample usage:\n```python\n# Example\nvector = np.array([1, 2, 3, 4])\nresult = make_diagonal(vector)\nprint(result)\n# Output: [[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "# Solution: Calculate Accuracy Score\n\nI'll implement a function to calculate the accuracy score between true labels and predicted labels with proper rounding.\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    -----------\n    y_true : numpy.ndarray\n        The true labels.\n    y_pred : numpy.ndarray\n        The predicted labels.\n        \n    Returns:\n    --------\n    float\n        The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    # Verify that the input arrays have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input arrays must have the same length\")\n    \n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    \n    # Calculate accuracy as the ratio of correct predictions to total predictions\n    accuracy = correct_predictions / len(y_true)\n    \n    # Round to 4 decimal places\n    return round(accuracy, 4)\n```\n\nThis function:\n1. Takes two 1D numpy arrays as input\n2. Checks if they have the same length\n3. Computes the number of matching elements (correct predictions)\n4. Divides by the total number of elements to get the accuracy\n5. Rounds the result to 4 decimal places\n\nThe implementation is efficient, using numpy's vectorized operations for the comparison."}
{"task_id": 37, "completion_id": 0, "completion": "# Solution: Calculate Correlation Matrix\n\nI'll implement a function that calculates the correlation matrix for a dataset X and optionally finds the cross-correlation between X and another dataset Y.\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        # Calculate correlation matrix of X with itself\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        # Ensure X and Y have the same number of samples (rows)\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X and Y must have the same number of samples (rows)\")\n            \n        # Calculate cross-correlation between X and Y\n        # First concatenate X and Y horizontally\n        Z = np.concatenate([X, Y], axis=1)\n        \n        # Calculate the correlation matrix for the combined data\n        full_corr = np.corrcoef(Z, rowvar=False)\n        \n        # Extract the cross-correlation part\n        n_x = X.shape[1]\n        corr_matrix = full_corr[:n_x, n_x:]\n    \n    # Round to 4 decimal places and convert to Python list\n    return np.round(corr_matrix, 4).tolist()\n```\n\n## Explanation:\n\n1. When only X is provided, the function calculates the correlation matrix among all variables in X.\n2. When both X and Y are provided, it calculates the cross-correlation matrix showing how each feature in X correlates with each feature in Y.\n3. The parameter `rowvar=False` tells numpy that variables are stored as columns (not rows).\n4. For the cross-correlation case, we:\n   - Concatenate X and Y horizontally\n   - Calculate the full correlation matrix\n   - Extract only the section that represents correlations between X variables and Y variables\n5. Finally, we round all values to 4 decimal places and convert to a Python list as required.\n\nThis function handles both scenarios efficiently and returns results in the requested format."}
{"task_id": 38, "completion_id": 0, "completion": "# AdaBoost Fit Method Implementation\n\nI'll implement the AdaBoost algorithm's fit method, which will build multiple weak classifiers (decision stumps) and combine them into a strong classifier.\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    \n    # Ensure labels are -1 and 1 for AdaBoost\n    y = np.where(y <= 0, -1, 1)\n    \n    # Initialize weights uniformly\n    weights = np.ones(n_samples) / n_samples\n    \n    # List to store all classifiers\n    classifiers = []\n    \n    # Train n_clf weak classifiers\n    for _ in range(n_clf):\n        # Find best threshold and feature that minimizes weighted error\n        min_error = float('inf')\n        best_clf = {}\n        \n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            # Use unique values as potential thresholds\n            thresholds = np.unique(feature_values)\n            \n            for threshold in thresholds:\n                # Try both polarities\n                for polarity in [1, -1]:\n                    # Make predictions with current stump\n                    predictions = np.ones(n_samples)\n                    if polarity == 1:\n                        predictions[feature_values < threshold] = -1\n                    else:\n                        predictions[feature_values >= threshold] = -1\n                    \n                    # Calculate weighted error\n                    misclassified = predictions != y\n                    error = np.sum(weights[misclassified])\n                    \n                    # Update best classifier if this one is better\n                    if error < min_error:\n                        min_error = error\n                        best_clf = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'polarity': polarity\n                        }\n        \n        # Avoid division by zero\n        epsilon = 1e-10\n        \n        # Calculate classifier weight (alpha)\n        alpha = 0.5 * math.log((1 - min_error) / (min_error + epsilon))\n        \n        # Get predictions from best classifier\n        predictions = np.ones(n_samples)\n        if best_clf['polarity'] == 1:\n            predictions[X[:, best_clf['feature_idx']] < best_clf['threshold']] = -1\n        else:\n            predictions[X[:, best_clf['feature_idx']] >= best_clf['threshold']] = -1\n        \n        # Update sample weights: increase weights of misclassified samples\n        weights = weights * np.exp(-alpha * y * predictions)\n        # Normalize weights\n        weights = weights / np.sum(weights)\n        \n        # Round values to 4 decimal places\n        best_clf['alpha'] = round(alpha, 4)\n        best_clf['threshold'] = round(best_clf['threshold'], 4)\n        \n        # Add classifier to ensemble\n        classifiers.append(best_clf)\n    \n    return classifiers\n```\n\nThis implementation:\n1. Initializes sample weights uniformly\n2. For each weak classifier:\n   - Finds the best feature and threshold combination by trying all possibilities\n   - Calculates the weighted error of the best stump\n   - Computes the classifier weight (alpha) based on the error\n   - Updates sample weights to focus more on misclassified samples\n   - Rounds values to 4 decimal places as required\n3. Returns a list of classifier dictionaries with their parameters\n\nEach classifier in the returned list contains the feature index, threshold value, polarity (direction of the prediction), and alpha (classifier weight)."}
{"task_id": 39, "completion_id": 0, "completion": "# Implementation of Log Softmax Function\n\nTo implement the log softmax function, I need to address numerical stability, handle the computation properly, and follow the output constraints.\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert list to numpy array\n    scores_array = np.array(scores, dtype=float)\n    \n    # Subtract max value for numerical stability\n    # (prevents overflow when calculating exp of large values)\n    shifted_scores = scores_array - np.max(scores_array)\n    \n    # Log softmax formula: x_i - log(sum(exp(x_j)))\n    # Here we use shifted scores for numerical stability\n    log_sum_exp = np.log(np.sum(np.exp(shifted_scores)))\n    log_softmax_values = shifted_scores - log_sum_exp\n    \n    # Round to 4 decimal places\n    log_softmax_values = np.round(log_softmax_values, 4)\n    \n    # Convert back to list as required\n    return log_softmax_values.tolist()\n```\n\nThe function works by:\n1. Converting the input list to a numpy array\n2. Subtracting the maximum value for numerical stability (this prevents potential overflow)\n3. Computing the log softmax values using the formula `x_i - log(sum(exp(x_j)))`\n4. Rounding the values to 4 decimal places\n5. Converting the numpy array back to a Python list\n\nThis implementation is numerically stable and handles edge cases like very large input values that might cause overflow in a naive implementation."}
{"task_id": 40, "completion_id": 0, "completion": "# Custom Dense Layer Implementation\n\nLooking at the task, I need to implement a fully connected neural network layer as a subclass of the given `Layer` class. Let me implement the `Dense` class with all the required functionality:\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n        \n    def initialize(self, optimizer):\n        # Calculate limit for weights initialization\n        limit = 1 / math.sqrt(self.input_shape[0])\n        \n        # Initialize weights using uniform distribution\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        \n        # Initialize bias as zeros\n        self.w0 = np.zeros((1, self.n_units))\n        \n        # Initialize optimizers for weights and bias\n        self.W_optimizer = copy.copy(optimizer)\n        self.w0_optimizer = copy.copy(optimizer)\n        \n    def parameters(self):\n        # Return total number of parameters (weights + biases)\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n        \n    def forward_pass(self, X, training=True):\n        # Store input for use in backward pass\n        self.layer_input = X\n        \n        # Compute output: X * W + w0\n        output = X.dot(self.W) + self.w0\n        \n        # Round to 4 decimal places\n        return np.round(output, 4)\n        \n    def backward_pass(self, accum_grad):\n        # Calculate gradient with respect to layer input (for backpropagation)\n        grad_X = accum_grad.dot(self.W.T)\n        \n        # Update parameters if layer is trainable\n        if self.trainable:\n            # Calculate gradients for weights and bias\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            \n            # Update weights and bias using optimizers\n            self.W = self.W_optimizer.update(self.W, grad_w)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_w0)\n        \n        # Round to 4 decimal places\n        return np.round(grad_X, 4)\n        \n    def output_shape(self):\n        # Return shape of the output\n        return (self.n_units,)\n```\n\n## Explanation\n\n1. **Initialization** (`__init__`): Sets up the layer properties including neuron count and placeholders for weights and biases.\n\n2. **Weight Initialization** (`initialize`): \n   - Initializes weights using uniform distribution with limit 1/sqrt(input_shape[0])\n   - Sets biases to zero\n   - Creates copies of the optimizer for both parameters\n\n3. **Parameter Count** (`parameters`): \n   - Returns the total number of trainable parameters (weights + biases)\n\n4. **Forward Pass** (`forward_pass`):\n   - Stores the input for later use in backward pass\n   - Computes X\u00b7W + w0\n   - Rounds result to 4 decimal places\n\n5. **Backward Pass** (`backward_pass`):\n   - Computes gradient with respect to input\n   - If trainable, calculates gradients for weights and bias\n   - Updates parameters using the optimizers\n   - Rounds result to 4 decimal places\n\n6. **Output Shape** (`output_shape`):\n   - Returns the shape of the layer's output as a tuple\n\nThis implementation ensures the Dense layer works correctly within a neural network framework, handling both forward and backward passes while maintaining the expected parameter shapes."}
{"task_id": 41, "completion_id": 0, "completion": "# Simple Convolutional 2D Layer Implementation\n\nI'll implement a 2D convolutional layer as requested. This function will apply a convolution operation with the given kernel, padding, and stride to the input matrix.\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Get dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Apply padding\n    if padding > 0:\n        padded_matrix = np.zeros((input_height + 2 * padding, input_width + 2 * padding))\n        padded_matrix[padding:padding + input_height, padding:padding + input_width] = input_matrix\n    else:\n        padded_matrix = input_matrix.copy()\n    \n    # Calculate output dimensions\n    output_height = (padded_matrix.shape[0] - kernel_height) // stride + 1\n    output_width = (padded_matrix.shape[1] - kernel_width) // stride + 1\n    \n    # Initialize output matrix\n    output = np.zeros((output_height, output_width))\n    \n    # Perform convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Extract the region from the padded matrix\n            i_start = i * stride\n            j_start = j * stride\n            region = padded_matrix[i_start:i_start + kernel_height, j_start:j_start + kernel_width]\n            \n            # Element-wise multiplication and sum\n            output[i, j] = np.sum(region * kernel)\n    \n    # Round to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Convert to list\n    return output.tolist()\n```\n\nThis implementation:\n1. Pads the input matrix with zeros if padding > 0\n2. Calculates the output dimensions based on input size, kernel size, and stride\n3. Slides the kernel across the padded input with the specified stride\n4. At each position, computes the sum of element-wise products\n5. Rounds all results to 4 decimal places\n6. Returns the result as a Python list using tolist()"}
{"task_id": 42, "completion_id": 0, "completion": "# ReLU Activation Function Implementation\n\nHere's the implementation of the Rectified Linear Unit (ReLU) activation function:\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Args:\n        z (float): The input value\n        \n    Returns:\n        float: The output of the ReLU function (max(0, z))\n    \"\"\"\n    return max(0, z)\n```\n\nThis implementation follows the ReLU definition: it returns the input value if it's positive, otherwise it returns 0. The `max(0, z)` is an elegant way to express this logic in a single line.\n\nThe function is properly type-hinted to indicate it takes a float as input and returns a float as output."}
{"task_id": 43, "completion_id": 0, "completion": "# Ridge Regression Loss Function Implementation\n\nHere's the implementation of the Ridge Regression loss function as requested:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate predictions\n    y_pred = X.dot(w)\n    \n    # Calculate mean squared error (MSE)\n    n_samples = X.shape[0]\n    mse = np.sum((y_pred - y_true) ** 2) / n_samples\n    \n    # Calculate L2 regularization term\n    l2_term = alpha * np.sum(w ** 2)\n    \n    # Compute the total ridge loss\n    loss = mse + l2_term\n    \n    # Round to 4 decimal places as required\n    return round(loss, 4)\n```\n\n## Explanation:\n\n1. **Predictions**: We first compute the predicted values by multiplying the feature matrix `X` with the coefficient vector `w`.\n\n2. **Mean Squared Error**: The MSE is calculated as the average of the squared differences between the predicted values and the true values.\n\n3. **L2 Regularization**: Ridge regression adds an L2 penalty term (alpha \u00d7 sum of squared weights) to the loss function.\n\n4. **Total Loss**: The ridge loss combines the MSE and the regularization term.\n\n5. **Rounding**: As requested, the final result is rounded to 4 decimal places.\n\nThe implementation follows the standard ridge regression loss formula:\nRidge Loss = MSE + alpha \u00d7 L2 penalty"}
{"task_id": 44, "completion_id": 0, "completion": "# Leaky ReLU Activation Function Implementation\n\nThe Leaky ReLU function is a variant of the ReLU activation function that allows a small gradient when the input is negative, preventing the \"dying ReLU\" problem. Here's the implementation:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    \"\"\"\n    Implements the Leaky ReLU activation function.\n    \n    Args:\n        z (float): The input value\n        alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n        float|int: The output after applying Leaky ReLU\n    \"\"\"\n    return z if z > 0 else alpha * z\n```\n\n## Explanation:\n- If `z` is positive (>0), the function simply returns `z` (like standard ReLU)\n- If `z` is negative or zero (\u22640), the function returns `alpha * z`, allowing a small gradient\n- The default value for `alpha` is 0.01, which is a commonly used value\n\nThis implementation is clean and efficient, using Python's conditional expression (ternary operator) for conciseness."}
{"task_id": 45, "completion_id": 0, "completion": "# Linear Kernel Function Implementation\n\nHere's a clean implementation of the linear kernel function:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    The linear kernel is defined as the dot product of the two input vectors.\n    \n    Parameters:\n    -----------\n    x1 : array-like\n        First input vector\n    x2 : array-like\n        Second input vector\n        \n    Returns:\n    --------\n    float\n        The dot product between x1 and x2\n    \n    Examples:\n    ---------\n    >>> kernel_function(np.array([1, 2]), np.array([3, 4]))\n    11.0\n    \"\"\"\n    return np.dot(x1, x2)\n```\n\nThis implementation:\n- Uses NumPy's `dot` function for efficient vector inner product computation\n- Works with any array-like inputs of compatible shapes\n- Returns a scalar value representing the inner product\n\nThe linear kernel is the simplest kernel function used in machine learning algorithms like Support Vector Machines (SVMs) and is equivalent to the standard dot product between two vectors."}
{"task_id": 46, "completion_id": 0, "completion": "# Precision Metric Implementation\n\nLet me write a function that calculates the precision metric based on true and predicted labels.\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Precision = TP / (TP + FP) = TP / (all predicted positives)\n    \n    Args:\n        y_true (numpy.ndarray): Array of true binary labels (0 or 1).\n        y_pred (numpy.ndarray): Array of predicted binary labels (0 or 1).\n        \n    Returns:\n        float: Precision score. Returns 0.0 if there are no predicted positives.\n    \"\"\"\n    # Calculate true positives (predicted 1 and actually 1)\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    \n    # Calculate false positives (predicted 1 but actually 0)\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    \n    # Total predicted positives\n    predicted_positives = true_positives + false_positives\n    \n    # Handle division by zero (when there are no predicted positives)\n    if predicted_positives == 0:\n        return 0.0\n    \n    # Calculate precision\n    return true_positives / predicted_positives\n```\n\nThis implementation:\n1. Counts true positives (TP) - cases where both prediction and actual value are 1\n2. Counts false positives (FP) - cases where prediction is 1 but actual value is 0\n3. Handles the edge case where there are no predicted positives (returns 0)\n4. Calculates precision as TP / (TP + FP)\n\nThe function uses NumPy's efficient boolean operations to perform these calculations on arrays of any size."}
{"task_id": 47, "completion_id": 0, "completion": "# Understanding the Task\n\nI need to implement a function that performs three gradient descent variants (SGD, Batch, and Mini-Batch) using Mean Squared Error (MSE) as the loss function. Let me code this solution systematically.\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements three variants of gradient descent using MSE loss function.\n    \n    Parameters:\n    X: numpy array - Feature matrix of shape (m, n)\n    y: numpy array - Target values of shape (m,)\n    weights: list/array - Initial weights\n    learning_rate: float - Step size for gradient updates\n    n_iterations: int - Number of iterations to run\n    batch_size: int - Size of mini-batches (default=1 for SGD)\n    method: str - One of 'batch', 'stochastic', or 'mini-batch'\n    \n    Returns:\n    list - Optimized weights rounded to 4 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays with proper shapes\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float).reshape(-1, 1)\n    weights = np.array(weights, dtype=float).reshape(-1, 1)\n    \n    m = X.shape[0]  # Number of training examples\n    \n    if method == 'batch':\n        # Batch Gradient Descent: use all training examples\n        for _ in range(n_iterations):\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = (2/m) * np.dot(X.T, errors)\n            weights = weights - learning_rate * gradient\n            \n    elif method == 'stochastic':\n        # Stochastic Gradient Descent: use one example at a time\n        for _ in range(n_iterations):\n            for i in range(m):\n                xi = X[i:i+1]\n                yi = y[i:i+1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * np.dot(xi.T, error)  # No need to divide by 1\n                weights = weights - learning_rate * gradient\n                \n    elif method == 'mini-batch':\n        # Mini-Batch Gradient Descent: use a batch of examples\n        for _ in range(n_iterations):\n            # Shuffle the data\n            indices = np.random.permutation(m)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            \n            # Process each mini-batch\n            for i in range(0, m, batch_size):\n                batch_end = min(i + batch_size, m)\n                X_batch = X_shuffled[i:batch_end]\n                y_batch = y_shuffled[i:batch_end]\n                \n                batch_size_actual = X_batch.shape[0]\n                predictions = np.dot(X_batch, weights)\n                errors = predictions - y_batch\n                gradient = (2/batch_size_actual) * np.dot(X_batch.T, errors)\n                weights = weights - learning_rate * gradient\n    \n    # Round to 4 decimal places and convert to list\n    weights = np.round(weights, 4)\n    return weights.reshape(-1).tolist()\n```\n\nThis implementation:\n1. Handles all three gradient descent variants\n2. Uses MSE loss function with the proper gradient calculation (2/m) * X^T * (X*w - y)\n3. Properly reshapes inputs and outputs\n4. Rounds results to 4 decimal places\n5. Returns a Python list using tolist() after reshaping\n\nThe key differences between the three methods:\n- Batch: Updates weights once per iteration using all examples\n- Stochastic: Updates weights for each individual example\n- Mini-batch: Updates weights using small random batches of examples"}
{"task_id": 48, "completion_id": 0, "completion": "# Reduced Row Echelon Form (RREF) Implementation\n\nI'll implement a function to convert a matrix to its Reduced Row Echelon Form. The key aspects of RREF are:\n1. Each leading non-zero entry in a row is 1\n2. Each column containing a leading 1 has zeros in all other positions\n3. Each leading 1 appears to the right of the leading 1 in the row above it\n4. All rows of all zeros are at the bottom\n\nHere's my implementation:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts a matrix to its Reduced Row Echelon Form (RREF).\n    \n    Args:\n        matrix: A 2D list or numpy array representing a matrix\n        \n    Returns:\n        The RREF of the input matrix as a list of lists\n    \"\"\"\n    # Convert to numpy array for easier manipulation\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    \n    # Current row being processed\n    r = 0\n    \n    # Process each column\n    for c in range(cols):\n        # Find the pivot row for the current column\n        pivot_row = None\n        for i in range(r, rows):\n            if abs(A[i, c]) > 1e-10:  # Use tolerance for floating-point comparison\n                pivot_row = i\n                break\n        \n        # If a pivot is found in this column\n        if pivot_row is not None:\n            # Swap the pivot row with the current row\n            if pivot_row != r:\n                A[[r, pivot_row]] = A[[pivot_row, r]]\n            \n            # Scale the pivot row to make the pivot element 1\n            pivot_value = A[r, c]\n            A[r] = A[r] / pivot_value\n            \n            # Eliminate all other entries in the pivot column\n            for i in range(rows):\n                if i != r:\n                    factor = A[i, c]\n                    A[i] = A[i] - factor * A[r]\n            \n            # Move to the next row\n            r += 1\n            \n            # If we've processed all rows, stop\n            if r == rows:\n                break\n    \n    # Clean up numerical errors (values very close to 0)\n    A[abs(A) < 1e-10] = 0\n    \n    return A.tolist()\n```\n\nThis implementation follows the Gauss-Jordan elimination process:\n1. Find the leftmost column with a non-zero element\n2. Place the row with the non-zero element at the top\n3. Scale that row to have a leading 1\n4. Eliminate all other entries in that column\n5. Repeat with the submatrix, excluding the current row\n\nThe function handles all required cases including reducible matrices and ensures proper floating-point handling to avoid numerical errors."}
{"task_id": 49, "completion_id": 0, "completion": "# Adam Optimization Algorithm Implementation\n\nHere's my implementation of the Adam optimizer algorithm in Python:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n    \n    Args:\n        f (function): The objective function to minimize\n        grad (function): A function that computes the gradient of f\n        x0 (numpy.ndarray): Initial parameter values\n        learning_rate (float): Step size parameter (default: 0.001)\n        beta1 (float): Exponential decay rate for first moment estimates (default: 0.9)\n        beta2 (float): Exponential decay rate for second moment estimates (default: 0.999)\n        epsilon (float): Small constant for numerical stability (default: 1e-8)\n        num_iterations (int): Number of iterations to run the optimizer (default: 10)\n    \n    Returns:\n        list: Optimized parameters as a Python list\n    \"\"\"\n    # Initialize parameters\n    x = np.copy(x0)\n    m = np.zeros_like(x)  # First moment estimate\n    v = np.zeros_like(x)  # Second moment estimate\n    \n    for t in range(1, num_iterations + 1):\n        # Compute gradient\n        g = grad(x)\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update biased second moment estimate\n        v = beta2 * v + (1 - beta2) * np.square(g)\n        \n        # Bias correction\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the optimized parameters as a Python list\n    return x.tolist()\n```\n\n## Explanation\n\nThis implementation of the Adam optimizer follows these steps:\n\n1. **Initialization**:\n   - Copy the initial parameters to avoid modifying the input\n   - Initialize first moment vector (m) and second moment vector (v) as zeros\n\n2. **Optimization Loop**:\n   - Calculate the gradient at the current point\n   - Update the biased first moment estimate using exponential moving average\n   - Update the biased second moment estimate (squared gradients)\n   - Apply bias correction to both moment estimates\n   - Update parameters using the Adam update rule\n\n3. **Output**:\n   - Round the optimized parameters to 4 decimal places\n   - Convert the numpy array to a Python list using tolist()\n\nThe algorithm adaptively adjusts the learning rate for each parameter based on the first and second moments of the gradients, which helps with convergence in many optimization problems."}
{"task_id": 50, "completion_id": 0, "completion": "# Lasso Regression Implementation with Gradient Descent\n\nI'll implement Lasso Regression using gradient descent optimization. The implementation will handle the L1 regularization term that helps with feature selection by shrinking some coefficients to zero.\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent\n    \n    Parameters:\n    X (np.array): Feature matrix of shape (n_samples, n_features)\n    y (np.array): Target vector of shape (n_samples,)\n    alpha (float): Regularization parameter\n    learning_rate (float): Learning rate for gradient descent\n    max_iter (int): Maximum number of iterations\n    tol (float): Tolerance for convergence\n    \n    Returns:\n    tuple: (weights, bias) where weights is a list and bias is a float\n    \"\"\"\n    # Get dimensions\n    n_samples, n_features = X.shape\n    \n    # Initialize parameters\n    weights = np.zeros(n_features)\n    bias = 0\n    \n    # Track cost history for convergence check\n    cost_history = []\n    \n    # Gradient Descent iterations\n    for iteration in range(max_iter):\n        # Calculate predictions\n        y_pred = np.dot(X, weights) + bias\n        \n        # Calculate errors\n        errors = y - y_pred\n        \n        # Calculate gradients\n        # For weights: gradient of MSE + gradient of L1 penalty\n        dw = -(1/n_samples) * np.dot(X.T, errors) + alpha * np.sign(weights)\n        # For bias: only gradient of MSE (no regularization on bias)\n        db = -(1/n_samples) * np.sum(errors)\n        \n        # Update parameters\n        weights = weights - learning_rate * dw\n        bias = bias - learning_rate * db\n        \n        # Calculate cost function: MSE + L1 penalty\n        mse = (1/(2*n_samples)) * np.sum(errors**2)\n        l1_penalty = alpha * np.sum(np.abs(weights))\n        cost = mse + l1_penalty\n        \n        # Store cost for convergence check\n        cost_history.append(cost)\n        \n        # Check for convergence\n        if iteration > 0 and abs(cost_history[-1] - cost_history[-2]) < tol:\n            break\n    \n    # Round to 4 decimal places as required\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    \n    # Return as specified in the requirements\n    return weights.tolist(), bias\n```\n\nThis implementation:\n\n1. Initializes weights to zeros and bias to 0\n2. Performs gradient descent by:\n   - Computing predictions and errors\n   - Computing gradients for both the MSE loss and L1 penalty term\n   - Updating weights and bias using learning rate\n   - Tracking the cost function for convergence checking\n3. Rounds the final weights and bias to 4 decimal places\n4. Returns the weights as a Python list and the bias as a float\n\nThe L1 regularization encourages sparsity in the model parameters by penalizing the absolute values of the coefficients, which leads to feature selection as some coefficients are reduced to exactly zero."}
{"task_id": 51, "completion_id": 0, "completion": "# Optimal String Alignment Distance Implementation\n\nI'll implement the Optimal String Alignment (OSA) distance algorithm using dynamic programming. This algorithm measures the minimum number of operations (insertions, deletions, substitutions, and transpositions) needed to transform one string into another.\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment distance between two strings.\n    \n    Args:\n        source (str): The source string\n        target (str): The target string\n        \n    Returns:\n        int: The minimum number of operations required to transform source into target\n    \"\"\"\n    m, n = len(source), len(target)\n    \n    # Create a distance matrix\n    dp = np.zeros((m+1, n+1), dtype=int)\n    \n    # Initialize first row and column (base cases)\n    for i in range(m+1):\n        dp[i, 0] = i  # i deletions to convert source[:i] to empty string\n    for j in range(n+1):\n        dp[0, j] = j  # j insertions to convert empty string to target[:j]\n    \n    # Fill the DP matrix\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            # If characters match, no operation needed for these positions\n            if source[i-1] == target[j-1]:\n                dp[i, j] = dp[i-1, j-1]\n            else:\n                # Calculate costs for different operations\n                deletion = dp[i-1, j] + 1\n                insertion = dp[i, j-1] + 1\n                substitution = dp[i-1, j-1] + 1\n                \n                # Take the minimum cost operation\n                dp[i, j] = min(deletion, insertion, substitution)\n                \n                # Check if transposition is possible (when two adjacent characters are swapped)\n                if (i > 1 and j > 1 and \n                    source[i-1] == target[j-2] and \n                    source[i-2] == target[j-1]):\n                    dp[i, j] = min(dp[i, j], dp[i-2, j-2] + 1)\n    \n    return dp[m, n]\n```\n\n## Explanation:\n\n1. We create a DP matrix where `dp[i][j]` represents the minimum number of operations to transform `source[0:i]` into `target[0:j]`.\n\n2. Base cases:\n   - Converting a string to an empty string requires deleting all characters\n   - Converting an empty string to a target string requires inserting all characters\n\n3. For each position, we calculate the cost of:\n   - Deletion: `dp[i-1][j] + 1`\n   - Insertion: `dp[i][j-1] + 1`\n   - Substitution: `dp[i-1][j-1] + 1` (only if characters don't match)\n   - Transposition: `dp[i-2][j-2] + 1` (when two adjacent characters are swapped)\n\n4. We take the minimum of these operations at each step.\n\n5. Finally, `dp[m][n]` gives us the minimum number of operations to transform the entire source string into the target string.\n\nFor the example \"caper\" and \"acer\", the algorithm will correctly return 2 (delete 'p' and transpose 'c' and 'a')."}
{"task_id": 52, "completion_id": 0, "completion": "# Recall Metric Implementation for Binary Classification\n\nTo implement the recall metric, I'll calculate the ratio of true positives to the total actual positives (true positives + false negatives). This measures how well the model identifies all positive instances.\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate recall metric for binary classification.\n    \n    Parameters:\n        y_true (list): List of true binary labels (0 or 1)\n        y_pred (list): List of predicted binary labels (0 or 1)\n        \n    Returns:\n        float: Recall score rounded to 3 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (model predicted 1, and it was actually 1)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false negatives (model predicted 0, but it was actually 1)\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate the denominator (total actual positives)\n    denominator = true_positives + false_negatives\n    \n    # Handle the case where there are no positive examples\n    if denominator == 0:\n        return 0.0\n    \n    # Calculate and return recall rounded to 3 decimal places\n    return round(true_positives / denominator, 3)\n```\n\nThis implementation:\n- Converts inputs to numpy arrays for efficient calculation\n- Computes true positives and false negatives using logical operations\n- Handles the division by zero case by returning 0.0\n- Rounds the result to three decimal places as specified\n\nThe recall metric helps us understand how well our model captures all positive instances, which is especially important in situations where missing a positive case is costly (like disease detection)."}
{"task_id": 53, "completion_id": 0, "completion": "# Implementing Self-Attention Mechanism\n\nI'll implement the self-attention mechanism, which is a key component of transformer architectures. Self-attention allows the model to weigh the importance of different parts of the input sequence when forming contextual representations.\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes self-attention over the input sequence.\n    \n    Args:\n        X: Input tensor of shape (batch_size, seq_len, d_model)\n        W_q: Query weight matrix of shape (d_model, d_k)\n        W_k: Key weight matrix of shape (d_model, d_k)\n        W_v: Value weight matrix of shape (d_model, d_v)\n        \n    Returns:\n        attention_output: Self-attention output as a Python list\n    \"\"\"\n    # Step 1: Project input to query, key, and value representations\n    Q = np.dot(X, W_q)  # (batch_size, seq_len, d_k)\n    K = np.dot(X, W_k)  # (batch_size, seq_len, d_k)\n    V = np.dot(X, W_v)  # (batch_size, seq_len, d_v)\n    \n    # Step 2: Compute attention scores\n    # Transpose K for matrix multiplication: (batch_size, d_k, seq_len)\n    K_T = np.transpose(K, (0, 2, 1))\n    \n    # Compute scaled dot-product attention scores\n    # (batch_size, seq_len, seq_len)\n    attention_scores = np.matmul(Q, K_T)\n    \n    # Step 3: Scale the scores by the square root of dimension\n    d_k = K.shape[-1]\n    attention_scores = attention_scores / np.sqrt(d_k)\n    \n    # Step 4: Apply softmax to get attention weights\n    # For numerical stability, subtract max before exponentiating\n    scores_max = np.max(attention_scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(attention_scores - scores_max)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Step 5: Compute weighted sum of values\n    attention_output = np.matmul(attention_weights, V)\n    \n    # Round to 4 decimal places and convert to list\n    attention_output = np.round(attention_output, 4).tolist()\n    \n    return attention_output\n```\n\nThis implementation follows the standard self-attention algorithm:\n1. Project input sequence into query, key, and value representations\n2. Calculate attention scores between all pairs of positions\n3. Scale the scores by \u221ad_k to prevent exploding gradients in softmax\n4. Apply softmax to get attention weights (with numerical stability fix)\n5. Compute the weighted sum of values based on attention weights\n6. Round the results to 4 decimal places and return as a Python list\n\nThe output maintains the same batch size and sequence length as the input, with each token representation now incorporating contextual information from the entire sequence."}
{"task_id": 54, "completion_id": 0, "completion": "# Implementing a Simple RNN\n\nI'll write a function that processes a sequence through a simple RNN cell. Here's my implementation:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert all inputs to numpy arrays for efficient computation\n    input_sequence = np.array(input_sequence)\n    hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Process each input vector in the sequence\n    for x_t in input_sequence:\n        # RNN update formula: h_t = tanh(W_x * x_t + W_h * h_{t-1} + b)\n        hidden_state = np.tanh(np.dot(x_t, Wx) + np.dot(hidden_state, Wh) + b)\n    \n    # Round to 4 decimal places and convert back to list\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    \n    return final_hidden_state\n```\n\n## Explanation:\n\n1. First, I convert all inputs to NumPy arrays for efficient matrix operations.\n\n2. Then, I iterate through each input vector in the sequence, updating the hidden state using the RNN formula:\n   - h_t = tanh(W_x * x_t + W_h * h_{t-1} + b)\n   - where x_t is the current input, h_{t-1} is the previous hidden state\n\n3. For each step, I:\n   - Multiply the input with its weight matrix (x_t * Wx)\n   - Multiply the previous hidden state with its weight matrix (h_{t-1} * Wh)\n   - Add the bias vector\n   - Apply the tanh activation function\n\n4. Finally, I round the results to 4 decimal places and convert back to a Python list.\n\nThe function returns the final hidden state after processing the entire sequence."}
{"task_id": 55, "completion_id": 0, "completion": "# Python 2D Translation Matrix Implementation\n\nI'll implement a function that applies a 2D translation matrix to a set of points. This will move each point by the specified distances in the x and y directions.\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2D translation matrix to a set of points.\n    \n    Parameters:\n    points (list): List of [x, y] coordinates\n    tx (float): Translation distance in x direction\n    ty (float): Translation distance in y direction\n    \n    Returns:\n    list: New list of translated points\n    \"\"\"\n    # Convert input points to numpy array\n    points_np = np.array(points)\n    \n    # Create the translation matrix (3x3 for homogeneous coordinates)\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Convert points to homogeneous coordinates by adding a column of ones\n    homogeneous_points = np.hstack((points_np, np.ones((len(points_np), 1))))\n    \n    # Apply the translation using matrix multiplication\n    translated_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    \n    # Convert back to 2D coordinates (remove the homogeneous component)\n    translated_points = translated_homogeneous_points[:, :2]\n    \n    # Return as a Python list\n    return translated_points.tolist()\n```\n\nThis implementation:\n1. Converts the input points to a numpy array\n2. Creates a proper 2D translation matrix using homogeneous coordinates\n3. Converts the points to homogeneous coordinates (adds a third column of 1s)\n4. Applies the translation using matrix multiplication\n5. Converts back to 2D coordinates\n6. Returns the result as a Python list using numpy's tolist() method\n\nThe function works with any number of input points, as specified in the requirements."}
{"task_id": 56, "completion_id": 0, "completion": "# KL Divergence Between Two Normal Distributions\n\nThe Kullback-Leibler (KL) divergence between two normal distributions has a closed-form solution. For normal distributions P ~ N(\u03bc_P, \u03c3_P\u00b2) and Q ~ N(\u03bc_Q, \u03c3_Q\u00b2), the formula is:\n\n```\nKL(P||Q) = log(\u03c3_Q/\u03c3_P) + (\u03c3_P\u00b2 + (\u03bc_P - \u03bc_Q)\u00b2)/(2\u03c3_Q\u00b2) - 1/2\n```\n\nHere's my implementation:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n    \n    Returns:\n    float: KL divergence from P to Q\n    \"\"\"\n    # Check for valid inputs\n    if sigma_p <= 0 or sigma_q <= 0:\n        return float('inf')\n    \n    # Calculate each term in the KL divergence formula\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    term3 = -0.5\n    \n    # Sum the terms to get the KL divergence\n    kl_div = term1 + term2 + term3\n    \n    return kl_div\n```\n\nThis function:\n1. Takes the means and standard deviations of both distributions as inputs\n2. Performs validation to prevent invalid inputs (negative standard deviations)\n3. Calculates the KL divergence using the closed-form formula\n4. Returns the result as a floating-point number\n\nNote that KL divergence is asymmetric, meaning KL(P||Q) \u2260 KL(Q||P), so the order of the arguments matters."}
{"task_id": 57, "completion_id": 0, "completion": "# Gauss-Seidel Method Implementation\n\nI'll implement the Gauss-Seidel method for solving linear systems according to your requirements.\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: Square matrix of coefficients\n    - b: Right-hand side vector\n    - n: Number of iterations\n    - x_ini: Optional initial guess for the solution vector\n    \n    Returns:\n    - Approximated solution vector after n iterations\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient operations\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the size of the system\n    size = len(b)\n    \n    # Initialize the solution vector\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform n iterations\n    for _ in range(n):\n        for i in range(size):\n            # Calculate the sum of a_ij * x_j for j < i (using newest values)\n            sum1 = np.sum(A[i][:i] * x[:i])\n            \n            # Calculate the sum of a_ij * x_j for j > i (using previous values)\n            sum2 = np.sum(A[i][i+1:] * x[i+1:])\n            \n            # Update x[i] according to Gauss-Seidel formula\n            x[i] = (b[i] - sum1 - sum2) / A[i][i]\n    \n    # Round to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Convert to a Python list and return\n    return x.tolist()\n```\n\n## How It Works:\n\n1. **Input Processing**: Converts inputs to numpy arrays for efficient calculations\n2. **Initialization**: Sets initial solution vector to zeros if not provided\n3. **Iteration Process**: For each iteration and each variable:\n   - Uses the most recently computed values for variables 0 to i-1\n   - Uses values from the previous iteration for variables i+1 to n-1\n   - Updates the current variable using the Gauss-Seidel formula\n4. **Rounding**: Rounds all values to 4 decimal places as required\n5. **Output**: Returns the result as a Python list using tolist()\n\nThis implementation follows the Gauss-Seidel iterative method exactly, using the most up-to-date values available during each calculation step."}
{"task_id": 58, "completion_id": 0, "completion": "# Gaussian Elimination Implementation\n\nI'll implement a function for solving linear systems using Gaussian elimination with partial pivoting.\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n    A (array-like): Coefficient matrix\n    b (array-like): Right-hand side vector\n    \n    Returns:\n    list: Solution vector rounded to 4 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Ensure b is a column vector if it's 1D\n    if b.ndim == 1:\n        b = b.reshape(-1, 1)\n    \n    n = len(A)\n    \n    # Create augmented matrix [A|b]\n    aug = np.column_stack((A, b))\n    \n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Find pivot row (row with largest absolute value in current column)\n        pivot_row = i\n        for j in range(i+1, n):\n            if abs(aug[j, i]) > abs(aug[pivot_row, i]):\n                pivot_row = j\n        \n        # Swap rows if necessary\n        if pivot_row != i:\n            aug[[i, pivot_row]] = aug[[pivot_row, i]]\n        \n        # Check for singular matrix\n        if abs(aug[i, i]) < 1e-10:\n            raise ValueError(\"Matrix is singular or nearly singular\")\n        \n        # Eliminate elements below the pivot\n        for j in range(i+1, n):\n            factor = aug[j, i] / aug[i, i]\n            aug[j, i:] = aug[j, i:] - factor * aug[i, i:]\n    \n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        x[i] = aug[i, -1]\n        for j in range(i+1, n):\n            x[i] = x[i] - aug[i, j] * x[j]\n        x[i] = x[i] / aug[i, i]\n    \n    # Round to 4 decimal places\n    x = np.round(x, 4)\n    \n    return x.tolist()\n```\n\nThis implementation:\n1. Takes a coefficient matrix A and a right-hand side vector b\n2. Performs forward elimination with partial pivoting to transform A into an upper triangular matrix\n3. Uses back substitution to solve for the variables\n4. Rounds results to 4 decimal places as required\n5. Returns the solution as a Python list\n\nThe partial pivoting technique (selecting the largest element in each column as the pivot) helps improve numerical stability by reducing the effects of round-off errors during elimination."}
{"task_id": 59, "completion_id": 0, "completion": "# LSTM Implementation in Python\n\nHere's an implementation of a Long Short-Term Memory (LSTM) network in Python:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Args:\n            x: Input sequence of shape (seq_len, input_size)\n            initial_hidden_state: Initial hidden state of shape (hidden_size, 1)\n            initial_cell_state: Initial cell state of shape (hidden_size, 1)\n            \n        Returns:\n            hidden_states: Hidden states at each time step\n            final_hidden_state: Final hidden state\n            final_cell_state: Final cell state\n        \"\"\"\n        # Define sigmoid activation function\n        def sigmoid(x):\n            return 1 / (1 + np.exp(-x))\n        \n        # Get sequence length\n        seq_len = x.shape[0]\n        \n        # Initialize hidden state and cell state\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        \n        # Initialize list to store hidden states at each time step\n        hidden_states = []\n        \n        # Process each input in the sequence\n        for t in range(seq_len):\n            # Reshape input at time t to be a column vector\n            x_t = x[t].reshape(-1, 1)\n            \n            # Concatenate input at time t with previous hidden state\n            concat = np.vstack((x_t, h_t))\n            \n            # Forget gate\n            f_t = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            \n            # Input gate\n            i_t = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            \n            # Candidate cell state\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            \n            # Update cell state\n            c_t = f_t * c_t + i_t * c_tilde\n            \n            # Output gate\n            o_t = sigmoid(np.dot(self.Wo, concat) + self.bo)\n            \n            # Update hidden state\n            h_t = o_t * np.tanh(c_t)\n            \n            # Store hidden state for this time step\n            hidden_states.append(h_t)\n        \n        # Round to 4 decimal places\n        hidden_states = np.round(np.array(hidden_states), 4)\n        final_h = np.round(h_t, 4)\n        final_c = np.round(c_t, 4)\n        \n        # Convert numpy arrays to Python lists\n        hidden_states = hidden_states.tolist()\n        final_h = final_h.tolist()\n        final_c = final_c.tolist()\n        \n        return hidden_states, final_h, final_c\n```\n\nThis implementation:\n\n1. Initializes LSTM weights and biases in the constructor\n2. Processes a sequence of inputs in the forward method\n3. Computes the four gates at each time step:\n   - Forget gate to control what information to discard\n   - Input gate to control what new information to store\n   - Candidate cell state for new values\n   - Output gate to control what to output\n4. Updates the cell state and hidden state at each step\n5. Returns all hidden states and the final cell/hidden states\n6. Rounds values to 4 decimal places and converts to Python lists as required\n\nThe implementation handles sequences of any length and maintains the proper dimensionality throughout the calculation."}
{"task_id": 60, "completion_id": 0, "completion": "# TF-IDF Implementation\n\nI'll implement a function to compute TF-IDF scores for query terms against a corpus of documents. This will help evaluate the importance of query terms in each document.\n\n```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for query terms against a corpus of documents.\n    \n    Args:\n        corpus: List of documents, where each document is a list of words\n        query: List of words to compute TF-IDF scores for\n        \n    Returns:\n        List of lists containing TF-IDF scores for each query term in each document\n    \"\"\"\n    # Handle empty corpus case\n    if not corpus:\n        return []\n    \n    # Initialize TF-IDF matrix (documents \u00d7 query terms)\n    tf_idf_matrix = np.zeros((len(corpus), len(query)))\n    \n    # Calculate term frequency (TF) for each document and query term\n    for doc_idx, document in enumerate(corpus):\n        # Handle empty document case\n        if not document:\n            continue\n            \n        doc_length = len(document)\n        \n        # Calculate frequency of each query term in this document\n        for term_idx, term in enumerate(query):\n            term_count = document.count(term)\n            tf = term_count / doc_length\n            tf_idf_matrix[doc_idx, term_idx] = tf\n    \n    # Calculate inverse document frequency (IDF) for each query term\n    for term_idx, term in enumerate(query):\n        # Count documents containing this term\n        doc_count = sum(1 for doc in corpus if term in doc)\n        \n        # Apply smoothing to avoid division by zero\n        idf = math.log((len(corpus) + 1) / (doc_count + 1)) + 1\n        \n        # Multiply TF values by IDF for this term\n        tf_idf_matrix[:, term_idx] *= idf\n    \n    # Round to 4 decimal places as specified\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    \n    # Return as list of lists\n    return tf_idf_matrix.tolist()\n```\n\nThis implementation:\n1. Computes TF (term frequency) as the count of a term in a document divided by the document length\n2. Computes IDF (inverse document frequency) with smoothing to avoid division by zero\n3. Handles all the specified edge cases:\n   - Empty corpus\n   - Empty documents\n   - Query terms not in corpus\n4. Rounds results to 4 decimal places as required\n5. Returns the results as a Python list via NumPy's tolist() method\n\nThe smoothing approach used in the IDF calculation ensures the function remains stable even when terms don't appear in any document."}
{"task_id": 61, "completion_id": 0, "completion": "# F-Score Calculation for Binary Classification\n\nI'll implement the f_score function as requested. This function calculates the F-Score for binary classification, which is a weighted harmonic mean of precision and recall.\n\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n    false_negative = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate precision and recall\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n    \n    # Handle the edge case where both precision and recall are 0\n    if precision == 0 and recall == 0:\n        return 0.0\n    \n    # Calculate F-score using the formula: (1 + beta^2) * (precision * recall) / ((beta^2 * precision) + recall)\n    f_score_value = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_score_value, 3)\n```\n\n## Explanation:\n\n1. **Calculation of Classification Metrics**:\n   - True Positives (TP): Instances correctly predicted as positive\n   - False Positives (FP): Instances incorrectly predicted as positive\n   - False Negatives (FN): Instances incorrectly predicted as negative\n\n2. **Precision and Recall**:\n   - Precision = TP / (TP + FP) - measures how many of the predicted positives are actually positive\n   - Recall = TP / (TP + FN) - measures how many of the actual positives were correctly identified\n   - Zero division is handled to avoid errors\n\n3. **F-Score Calculation**:\n   - Combines precision and recall into a single metric\n   - Beta parameter controls the weight given to precision vs. recall\n   - When beta=1, precision and recall are weighted equally (F1-score)\n   - When beta<1, precision is given more weight\n   - When beta>1, recall is given more weight\n\n4. **Edge Case Handling**:\n   - Returns 0 when both precision and recall are 0\n\nThe result is rounded to three decimal places as required."}
{"task_id": 62, "completion_id": 0, "completion": "# Simple RNN with Backpropagation Through Time (BPTT) Implementation\n\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            x: sequence of inputs, each with shape (input_size, 1)\n            \n        Returns:\n            outputs: list of outputs at each time step\n            last_inputs: list of inputs at each time step\n            last_hiddens: list of hidden states at each time step\n        \"\"\"\n        outputs, last_inputs, last_hiddens = self.rnn_forward(\n            self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y, \n            self.hidden_size, x\n        )\n        return outputs\n    \n    def backward(self, x, expected_output, learning_rate=0.01):\n        \"\"\"\n        Backward pass through the RNN to update weights using BPTT.\n        \n        Args:\n            x: sequence of inputs\n            expected_output: expected outputs\n            learning_rate: learning rate for weight updates\n            \n        Returns:\n            loss: total loss across all time steps\n        \"\"\"\n        # Forward pass to get necessary values\n        outputs, last_inputs, last_hiddens = self.rnn_forward(\n            self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y, \n            self.hidden_size, x\n        )\n        \n        # Backward pass to get updated weights and loss\n        self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y, loss = self.rnn_backward(\n            self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y,\n            self.hidden_size, x, expected_output, outputs, \n            last_inputs, last_hiddens, learning_rate\n        )\n        \n        return loss\n    \n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns outputs and internal states.\n        \n        Args:\n            W_xh: Weight matrix for input to hidden connections\n            W_hh: Weight matrix for hidden to hidden connections\n            W_hy: Weight matrix for hidden to output connections\n            b_h: Bias for hidden layer\n            b_y: Bias for output layer\n            hidden_size: Size of the hidden layer\n            input_sequence: Sequence of inputs, each with shape (input_size, 1)\n            \n        Returns:\n            outputs: List of outputs at each time step\n            last_inputs: List of inputs at each time step\n            last_hiddens: List of hidden states at each time step\n        \"\"\"\n        T = len(input_sequence)\n        \n        # Initialize states\n        last_inputs = []  # Store inputs at each time step\n        last_hiddens = [np.zeros((hidden_size, 1))]  # Initial hidden state is zero\n        outputs = []  # Store outputs at each time step\n        \n        # Forward pass for each time step\n        for t in range(T):\n            # Current input\n            xt = np.array(input_sequence[t]).reshape(-1, 1)\n            last_inputs.append(xt)\n            \n            # Calculate hidden state\n            hidden = np.tanh(np.dot(W_xh, xt) + np.dot(W_hh, last_hiddens[t]) + b_h)\n            last_hiddens.append(hidden)\n            \n            # Calculate output\n            yt = np.dot(W_hy, hidden) + b_y\n            outputs.append(yt)\n        \n        return outputs, last_inputs, last_hiddens\n    \n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, \n                      expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs BPTT to adjust weights based on the loss.\n        \n        Args:\n            W_xh, W_hh, W_hy, b_h, b_y: RNN parameters\n            hidden_size: Size of hidden layer\n            input_sequence: Input sequence\n            expected_output: Expected output sequence\n            outputs: Outputs from forward pass\n            last_inputs: Inputs from forward pass\n            last_hiddens: Hidden states from forward pass\n            learning_rate: Learning rate for weight updates\n            \n        Returns:\n            Updated W_xh, W_hh, W_hy, b_h, b_y and the loss\n        \"\"\"\n        T = len(input_sequence)\n        \n        # Initialize gradients\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        \n        # Initialize hidden gradient for the last time step\n        dhnext = np.zeros((hidden_size, 1))\n        \n        # Calculate loss (1/2 * MSE)\n        loss = 0\n        for t in range(T):\n            loss += 0.5 * np.sum((outputs[t] - expected_output[t]) ** 2)\n        \n        # Backward pass (BPTT)\n        for t in reversed(range(T)):\n            # Gradient of the output\n            dy = outputs[t] - expected_output[t]\n            \n            # Gradient of weights from hidden to output\n            dW_hy += np.dot(dy, last_hiddens[t+1].T)\n            db_y += dy\n            \n            # Gradient of hidden state\n            dhidden = np.dot(W_hy.T, dy) + dhnext\n            \n            # Gradient through tanh\n            dtanh = (1 - last_hiddens[t+1] ** 2) * dhidden\n            \n            # Accumulate gradients\n            db_h += dtanh\n            dW_xh += np.dot(dtanh, last_inputs[t].T)\n            dW_hh += np.dot(dtanh, last_hiddens[t].T)\n            \n            # Gradient for next iteration (previous time step)\n            dhnext = np.dot(W_hh.T, dtanh)\n        \n        # Clip gradients to prevent explosion\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        \n        # Update weights\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        \n        return W_xh, W_hh, W_hy, b_h, b_y, loss\n```\n\nThis implementation provides a complete SimpleRNN class with:\n\n1. Initialization of weights and biases\n2. Forward pass method that processes sequences\n3. Backward pass method that implements Backpropagation Through Time (BPTT)\n\nThe RNN works by:\n- Maintaining hidden state across time steps\n- Using tanh activation for the hidden layer\n- Computing 1/2 * MSE loss across all time steps\n- Backpropagating the error through time to update weights\n\nYou can use this RNN for sequence prediction tasks by training it on input-output sequence pairs."}
{"task_id": 63, "completion_id": 0, "completion": "# Conjugate Gradient Method Implementation\n\nHere's my implementation of the Conjugate Gradient method for solving linear systems:\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Get the size of the system\n    m = len(b)\n    \n    # Initialize x0 if not provided\n    if x0 is None:\n        x = np.zeros(m)\n    else:\n        x = x0.copy()\n    \n    # Initialize residual and direction\n    r = b - np.dot(A, x)\n    p = r.copy()\n    \n    # Initialize residual norm squared\n    r_norm_squared = np.dot(r, r)\n    \n    # Main iteration loop\n    for k in range(n):\n        # Check convergence\n        if np.sqrt(r_norm_squared) < tol:\n            break\n        \n        # Compute A*p\n        Ap = np.dot(A, p)\n        \n        # Compute step size\n        alpha = r_norm_squared / np.dot(p, Ap)\n        \n        # Update solution vector\n        x = x + alpha * p\n        \n        # Update residual\n        r = r - alpha * Ap\n        \n        # Compute new residual norm squared\n        r_norm_squared_new = np.dot(r, r)\n        \n        # Compute update factor for direction\n        beta = r_norm_squared_new / r_norm_squared\n        \n        # Update residual norm\n        r_norm_squared = r_norm_squared_new\n        \n        # Update search direction\n        p = r + beta * p\n    \n    # Round to 8 decimal places and convert to list\n    return np.round(x, 8).tolist()\n```\n\n## Explanation:\n\nThis implementation follows the Conjugate Gradient algorithm, which efficiently solves linear systems where A is symmetric and positive-definite:\n\n1. **Initialization**: We start with an initial guess (or zeros if none provided) and compute the initial residual and search direction.\n\n2. **Iteration**: For each iteration, we:\n   - Check if the residual is small enough to stop\n   - Calculate the optimal step size alpha\n   - Update the solution vector x\n   - Update the residual r\n   - Calculate beta to update the search direction\n   - Generate a new search direction that's conjugate to previous directions\n\n3. **Output**: The solution is rounded to 8 decimal places and returned as a list.\n\nThe beauty of this method is that it theoretically converges in at most n steps (where n is the size of the matrix), though in practice we often achieve good approximations much earlier, especially for well-conditioned systems."}
{"task_id": 64, "completion_id": 0, "completion": "# Gini Impurity Implementation\n\nI'll implement the Gini impurity calculation function. The Gini impurity measures how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \n    Formula: Gini = 1 - \u03a3(p_i^2) where p_i is the probability of class i\n    \n    Examples:\n    >>> gini_impurity([0, 0, 0, 0])  # Pure node\n    0.0\n    >>> gini_impurity([0, 0, 1, 1])  # Equal split\n    0.5\n    >>> gini_impurity([0, 0, 0, 1])  # 75%/25% split\n    0.375\n    \"\"\"\n    # Handle empty list case\n    if not y:\n        return 0.0\n    \n    # Count occurrences of each class\n    total_samples = len(y)\n    \n    # Get unique values and their counts\n    unique_values, counts = np.unique(y, return_counts=True)\n    \n    # Calculate probabilities for each class\n    probabilities = counts / total_samples\n    \n    # Calculate Gini impurity: 1 - sum(p_i^2)\n    gini = 1 - np.sum(probabilities**2)\n    \n    # Round to three decimal places\n    return round(gini, 3)\n```\n\nThis implementation:\n1. Handles the edge case of an empty list\n2. Uses NumPy's `unique` function to efficiently count class occurrences\n3. Calculates the probabilities for each class\n4. Applies the Gini impurity formula: 1 - sum(p_i^2)\n5. Rounds the result to three decimal places as required\n\nThe function works for any number of classes, not just binary classification problems."}
{"task_id": 65, "completion_id": 0, "completion": "# Compressed Row Sparse Matrix Implementation\n\nI'll implement a function to convert a dense matrix to Compressed Row Sparse (CSR) format. The CSR format is an efficient way to represent sparse matrices using three arrays:\n\n1. Values array - contains all non-zero elements\n2. Column indices array - stores the column index of each non-zero element\n3. Row pointer array - indicates where each row starts in the values array\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return [], [], [0]  # Handle empty matrix case\n    \n    values = []          # Will store non-zero values\n    col_indices = []     # Will store column indices of non-zero values\n    row_pointers = [0]   # Start with 0 as the first row pointer\n    \n    # Iterate through each row of the matrix\n    for row in dense_matrix:\n        non_zeros_in_current_row = 0\n        \n        # Process each element in the current row\n        for col_idx, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_idx)\n                non_zeros_in_current_row += 1\n        \n        # Update row pointer for the next row\n        row_pointers.append(row_pointers[-1] + non_zeros_in_current_row)\n    \n    return values, col_indices, row_pointers\n```\n\n## Example Usage:\n\n```python\n# Example 1: Simple sparse matrix\nmatrix1 = [\n    [0, 0, 3, 0, 4],\n    [0, 0, 5, 7, 0],\n    [0, 0, 0, 0, 0],\n    [0, 2, 6, 0, 0]\n]\nvalues, col_indices, row_pointers = compressed_row_sparse_matrix(matrix1)\nprint(\"Values:\", values)\nprint(\"Column indices:\", col_indices)\nprint(\"Row pointers:\", row_pointers)\n# Expected output:\n# Values: [3, 4, 5, 7, 2, 6]\n# Column indices: [2, 4, 2, 3, 1, 2]\n# Row pointers: [0, 2, 4, 4, 6]\n\n# Example 2: Dense matrix\nmatrix2 = [\n    [1, 2],\n    [3, 4]\n]\nvalues, col_indices, row_pointers = compressed_row_sparse_matrix(matrix2)\nprint(\"Values:\", values)\nprint(\"Column indices:\", col_indices)\nprint(\"Row pointers:\", row_pointers)\n# Expected output:\n# Values: [1, 2, 3, 4]\n# Column indices: [0, 1, 0, 1]\n# Row pointers: [0, 2, 4]\n```\n\nThis implementation correctly handles all cases, including empty matrices, completely sparse matrices (all zeros), and dense matrices. The time complexity is O(m\u00d7n) where m is the number of rows and n is the number of columns, as we need to examine each element once."}
{"task_id": 66, "completion_id": 0, "completion": "# Orthogonal Projection Implementation\n\nLet me implement a function that calculates the orthogonal projection of a vector onto a line.\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Check that the dimensions match\n    if len(v) != len(L):\n        raise ValueError(\"Vectors v and L must have the same dimension.\")\n    \n    # Compute the dot products\n    dot_v_L = sum(v_i * L_i for v_i, L_i in zip(v, L))\n    dot_L_L = sum(L_i * L_i for L_i in L)\n    \n    # Handle the case where L is the zero vector\n    if dot_L_L == 0:\n        raise ValueError(\"Cannot project onto the zero vector.\")\n    \n    # Compute the scalar projection coefficient\n    scalar = dot_v_L / dot_L_L\n    \n    # Compute the projection vector\n    projection = [scalar * L_i for L_i in L]\n    \n    # Round to three decimal places\n    projection = [round(p, 3) for p in projection]\n    \n    return projection\n```\n\n## Explanation:\n\n1. The orthogonal projection of vector v onto vector L is given by the formula:\n   \n   proj_L(v) = (dot(v, L) / dot(L, L)) \u00d7 L\n   \n   where dot(a, b) represents the dot product of vectors a and b.\n\n2. I first validate that both vectors have the same dimension and that L is not a zero vector.\n\n3. Then I compute the dot products needed for the projection formula.\n\n4. The scalar coefficient is calculated as the ratio of dot(v, L) to dot(L, L).\n\n5. The projection vector is obtained by multiplying each component of L by this scalar.\n\n6. Finally, I round each component to three decimal places as required.\n\nThis implementation handles n-dimensional vectors and includes error handling for invalid inputs."}
{"task_id": 67, "completion_id": 0, "completion": "# Compressed Column Sparse Matrix Implementation\n\nI'll implement a function that converts a dense matrix to the Compressed Column Sparse (CSC) format. The CSC format is efficient for sparse matrices and consists of three components:\n\n1. `values`: Non-zero elements in column-major order\n2. `row_indices`: Row index for each non-zero element\n3. `column_pointer`: Indices in the values array where each column starts\n\nHere's the implementation:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Handle empty matrices\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    \n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    column_pointer = [0]  # Start with 0 as the first column starts at index 0\n    \n    # Process each column\n    for col in range(num_cols):\n        # Process each row in the current column\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        \n        # After processing a column, record the current length of values\n        # This indicates where the next column's values will start\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n```\n\n## Explanation\n\n1. First, we handle edge cases like empty matrices.\n2. We initialize our three output lists: `values`, `row_indices`, and `column_pointer`.\n3. We traverse the matrix in column-major order (by column first, then by row).\n4. For each non-zero element:\n   - We add its value to the `values` list\n   - We add its row index to the `row_indices` list\n5. After processing each column, we record the current size of the `values` list in `column_pointer`.\n6. The `column_pointer` array's last element will be equal to the total number of non-zero elements.\n\nThis implementation correctly produces the CSC representation, which can then be used for efficient sparse matrix operations."}
{"task_id": 68, "completion_id": 0, "completion": "# Finding the Column Space (Image) of a Matrix\n\nI'll implement a function to find the column space (image) of a matrix using row reduction to identify the linearly independent columns.\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space (image) of matrix A.\n    \n    Args:\n        A: Input matrix as a 2D array or list\n        \n    Returns:\n        A list of vectors that form a basis for the column space\n    \"\"\"\n    # Convert to numpy array with float type for numerical stability\n    A = np.array(A, dtype=float)\n    m, n = A.shape\n    \n    # Create an augmented matrix with identity matrix for tracking column operations\n    aug = np.hstack((A.copy(), np.eye(m)))\n    \n    # Compute the row echelon form\n    r = 0  # current row index\n    pivot_cols = []\n    \n    for j in range(n):\n        # Find pivot in current column\n        pivot_found = False\n        for i in range(r, m):\n            if abs(aug[i, j]) > 1e-10:  # numerical tolerance for zero\n                # Found pivot element\n                if i != r:\n                    # Swap rows\n                    aug[[i, r]] = aug[[r, i]]\n                pivot_found = True\n                break\n        \n        if pivot_found:\n            # Scale pivot row to have leading 1\n            aug[r] = aug[r] / aug[r, j]\n            \n            # Eliminate entries in current column\n            for i in range(m):\n                if i != r:\n                    aug[i] = aug[i] - aug[i, j] * aug[r]\n            \n            pivot_cols.append(j)\n            r += 1\n            \n            if r == m:  # No more rows to process\n                break\n    \n    # Extract the columns of the original matrix that correspond to pivot columns\n    basis = A[:, pivot_cols]\n    \n    # Round to 8 decimal places and convert to list\n    return np.round(basis, 8).tolist()\n```\n\n## How This Works:\n\n1. **Gaussian Elimination**: The function implements Gaussian elimination to transform the matrix into row echelon form.\n\n2. **Pivot Column Identification**: During the elimination process, it identifies the pivot columns, which correspond to the linearly independent columns of the original matrix.\n\n3. **Basis Extraction**: The function extracts the columns from the original matrix that correspond to the pivot positions in the row echelon form.\n\n4. **Rounding and Formatting**: The resulting basis vectors are rounded to 8 decimal places and returned as a list of lists.\n\nThis implementation efficiently computes a basis for the column space (image) of the given matrix."}
{"task_id": 69, "completion_id": 0, "completion": "# R-squared Function Implementation for Regression Analysis\n\nI'll create a function to calculate the R-squared value (coefficient of determination), which measures how well the model explains the variance in the dependent variable.\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n    \n    Parameters:\n    y_true (array-like): The true values of the dependent variable\n    y_pred (array-like): The predicted values from the regression model\n    \n    Returns:\n    float: R-squared value rounded to three decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (SST)\n    ss_total = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (SSR)\n    ss_residual = np.sum((y_true - y_pred) ** 2)\n    \n    # Handle the case where there's no variance in the true values\n    if ss_total == 0:\n        return round(1.0 if ss_residual == 0 else 0.0, 3)\n    \n    # Calculate R-squared\n    r2 = 1 - (ss_residual / ss_total)\n    \n    # Round to three decimal places\n    return round(r2, 3)\n```\n\n## Explanation:\n1. The function accepts two arrays: the true values and the predicted values\n2. It calculates the mean of the true values\n3. It computes the total sum of squares (SS_total), which measures the total variance in the dependent variable\n4. It calculates the residual sum of squares (SS_residual), which measures the unexplained variance\n5. R-squared is computed as 1 - (SS_residual / SS_total)\n6. The function handles the edge case where all true values are identical (no variance)\n7. The final result is rounded to three decimal places as required\n\nThis implementation correctly handles all possible input scenarios, including the edge case where there's no variance in the true values."}
{"task_id": 70, "completion_id": 0, "completion": "# Solution: Image Brightness Calculator\n\nI'll implement a function that calculates the average brightness of a grayscale image, handling all the specified edge cases.\n\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if len(img) == 0:\n        return -1\n    \n    # Check if all rows have the same length\n    row_length = len(img[0])\n    if row_length == 0:\n        return -1\n    \n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    # Check if all pixel values are within the valid range [0, 255]\n    for row in img:\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                return -1\n    \n    # Calculate the average brightness\n    total_brightness = sum(sum(row) for row in img)\n    total_pixels = len(img) * row_length\n    average_brightness = total_brightness / total_pixels\n    \n    # Round to two decimal places and return\n    return round(average_brightness, 2)\n```\n\n## Explanation:\n\n1. **Empty image check**: Returns -1 if the image has no rows\n2. **Row consistency check**: \n   - Ensures the first row isn't empty\n   - Verifies all rows have the same length as the first row\n3. **Valid pixel range check**: Confirms all pixels are between 0 and 255\n4. **Brightness calculation**:\n   - Sums all pixel values\n   - Divides by the total number of pixels\n   - Rounds to two decimal places\n\nThis solution handles all specified edge cases while efficiently calculating the average brightness for valid images."}
{"task_id": 71, "completion_id": 0, "completion": "# Root Mean Square Error (RMSE) Implementation\n\nHere's a robust implementation of the RMSE function that handles all the specified requirements:\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error between two arrays.\n    \n    Parameters:\n    y_true (array-like): Array of actual values\n    y_pred (array-like): Array of predicted values\n    \n    Returns:\n    float: RMSE value rounded to three decimal places\n    \n    Raises:\n    ValueError: If arrays have different shapes or are empty\n    TypeError: If inputs are not array-like or contain non-numeric values\n    \"\"\"\n    # Check if inputs are valid\n    try:\n        y_true = np.array(y_true, dtype=float)\n        y_pred = np.array(y_pred, dtype=float)\n    except (ValueError, TypeError):\n        raise TypeError(\"Inputs must be array-like objects containing numeric values\")\n    \n    # Check if arrays are empty\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"Input arrays cannot be empty\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f\"Input shapes don't match: y_true.shape={y_true.shape}, y_pred.shape={y_pred.shape}\")\n    \n    # Calculate RMSE\n    rmse_value = np.sqrt(np.mean(np.square(y_true - y_pred)))\n    \n    # Round to three decimal places\n    return round(rmse_value, 3)\n```\n\n## Explanation:\n\n1. **Input Validation**:\n   - Converts inputs to NumPy arrays with float dtype\n   - Catches non-numeric values and non-array-like objects\n   - Ensures arrays are not empty\n   - Verifies that both arrays have the same shape\n\n2. **RMSE Calculation**:\n   - Computes the square of differences between actual and predicted values\n   - Takes the mean of these squared differences\n   - Calculates the square root of the mean\n   - Rounds the result to three decimal places\n\n3. **Error Handling**:\n   - Provides clear error messages for common issues\n   - Raises appropriate exceptions for different error conditions\n\nThis implementation efficiently handles all the requirements using NumPy's vectorized operations for performance."}
{"task_id": 72, "completion_id": 0, "completion": "# Implementing the Jaccard Index for Binary Classification\n\nFor binary classification, the Jaccard Index measures the similarity between predicted and true labels by calculating the ratio of their intersection to their union.\n\nHere's my implementation:\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between binary arrays y_true and y_pred.\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        Binary array of true labels\n    y_pred : array-like\n        Binary array of predicted labels\n        \n    Returns:\n    --------\n    float\n        Jaccard Index rounded to 3 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection (elements that are 1 in both arrays)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    \n    # Calculate union (elements that are 1 in either array)\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where both arrays contain only zeros\n    if union == 0:\n        return 1.0\n    \n    # Calculate and return Jaccard Index\n    return round(intersection / union, 3)\n```\n\n## Explanation:\n\n1. The function first converts inputs to NumPy arrays for efficient operations\n2. It calculates the intersection using logical AND (elements that are 1 in both arrays)\n3. It calculates the union using logical OR (elements that are 1 in either array)\n4. It handles the edge case where both arrays contain only zeros (returning 1.0 since they're identical)\n5. Finally, it returns the Jaccard Index (intersection/union) rounded to 3 decimal places\n\nThis implementation correctly handles all the required cases, including no overlap between sets and the edge case where both arrays contain only zeros."}
{"task_id": 73, "completion_id": 0, "completion": "# Dice Score Implementation\n\nI'll implement a function to calculate the Dice Score (also known as F1-score) for binary classification.\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between true and predicted binary labels.\n    \n    Args:\n        y_true: Array of true binary labels (0s and 1s)\n        y_pred: Array of predicted binary labels (0s and 1s)\n        \n    Returns:\n        Dice Score as a float rounded to 3 decimal places (0.0 to 1.0)\n    \"\"\"\n    # Convert inputs to numpy arrays for consistency\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection (elements that are 1 in both arrays)\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate total number of 1s in each array\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Handle edge cases\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0  # Perfect agreement when both arrays are all zeros\n    \n    if sum_true == 0 or sum_pred == 0:\n        return 0.0  # No overlap when one array has no positive elements\n    \n    # Calculate Dice Score\n    dice = (2 * intersection) / (sum_true + sum_pred)\n    \n    # Round to 3 decimal places\n    return round(float(dice), 3)\n```\n\n## Explanation:\n\n1. The function converts inputs to numpy arrays to ensure consistent behavior.\n2. It calculates the intersection by multiplying the arrays element-wise and summing the result.\n3. It handles important edge cases:\n   - Returns 1.0 when both arrays contain no positive elements (perfect agreement)\n   - Returns 0.0 when one array has positive elements but the other doesn't (no overlap)\n4. The Dice Score is calculated using the formula: 2 \u00d7 intersection / (sum of true positives + sum of predicted positives)\n5. The result is rounded to 3 decimal places as required.\n\nThis implementation efficiently computes the Dice Score for binary classification tasks."}
{"task_id": 74, "completion_id": 0, "completion": "# Hyperdimensional Computing for Dataset Row Representation\n\nI'll write a function to create a composite hypervector for a dataset row using Hyperdimensional Computing principles. The solution involves binding feature name hypervectors with their corresponding value hypervectors, then bundling them together.\n\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a dataset row.\n    \n    Args:\n        row: Dictionary representing a dataset row with feature names as keys and values\n        dim: Dimensionality of the hypervectors\n        random_seeds: Dictionary mapping feature names to random seeds\n        \n    Returns:\n        A list representing the composite hypervector for the row\n    \"\"\"\n    # Initialize the composite hypervector (bundle)\n    composite_hv = np.zeros(dim)\n    \n    # Process each feature in the row\n    for feature_name, feature_value in row.items():\n        # Generate hypervector for feature name (using hash of feature name as seed)\n        np.random.seed(hash(feature_name))\n        feature_name_hv = np.random.choice([-1, 1], dim)\n        \n        # Generate hypervector for feature value (using provided seed for reproducibility)\n        if feature_name in random_seeds:\n            seed = random_seeds[feature_name]\n            \n            # Create a unique seed based on the feature value\n            if isinstance(feature_value, (int, float)):\n                value_seed = seed + int(feature_value)\n            else:\n                value_seed = seed + hash(str(feature_value))\n                \n            np.random.seed(value_seed)\n            feature_value_hv = np.random.choice([-1, 1], dim)\n        else:\n            # Default case if feature doesn't have a specified seed\n            np.random.seed(0)\n            feature_value_hv = np.random.choice([-1, 1], dim)\n        \n        # Bind feature name and value hypervectors (elementwise multiplication)\n        bound_hv = feature_name_hv * feature_value_hv\n        \n        # Bundle (add) the bound hypervector to the composite hypervector\n        composite_hv += bound_hv\n    \n    # Return as a list\n    return composite_hv.tolist()\n```\n\nThis implementation:\n1. Creates a unique hypervector for each feature name using the hash of the name as a seed\n2. Creates hypervectors for feature values using the provided random seeds to ensure reproducibility\n3. Binds each feature name and value using element-wise multiplication (standard binding in HDC)\n4. Bundles all bound hypervectors using element-wise addition\n5. Returns the composite hypervector as a list\n\nThe function handles both numeric and non-numeric feature values by creating appropriate unique seeds in each case."}
{"task_id": 75, "completion_id": 0, "completion": "# Binary Classification Confusion Matrix Generator\n\nI'll implement a function that creates a confusion matrix for binary classification data. The matrix will follow the standard format:\n\n```\n               | Predicted Negative | Predicted Positive\n---------------|-------------------|-------------------\nActual Negative |        TN         |        FP\nActual Positive |        FN         |        TP\n```\n\nHere's my implementation:\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification.\n    \n    Args:\n        data: List of [y_true, y_pred] pairs, where each value is 0 or 1\n            y_true: actual label (0 or 1)\n            y_pred: predicted label (0 or 1)\n    \n    Returns:\n        A 2x2 confusion matrix as [[TN, FP], [FN, TP]]\n    \"\"\"\n    # Create a counter for the (y_true, y_pred) pairs\n    counter = Counter((y_true, y_pred) for y_true, y_pred in data)\n    \n    # Extract the counts for each category\n    TN = counter.get((0, 0), 0)  # True Negatives: actual=0, predicted=0\n    FP = counter.get((0, 1), 0)  # False Positives: actual=0, predicted=1\n    FN = counter.get((1, 0), 0)  # False Negatives: actual=1, predicted=0\n    TP = counter.get((1, 1), 0)  # True Positives: actual=1, predicted=1\n    \n    # Return the confusion matrix in standard format\n    return [[TN, FP], [FN, TP]]\n```\n\nThis implementation:\n1. Uses Counter to efficiently count all combinations of true/predicted values\n2. Handles the case where some combinations might be missing\n3. Returns the matrix in the standard format used in machine learning libraries\n4. Works correctly for binary classification (where labels are 0 and 1)"}
{"task_id": 76, "completion_id": 0, "completion": "# Cosine Similarity Implementation\n\nHere's a clean implementation of the cosine similarity function using NumPy:\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Args:\n        v1 (numpy.ndarray): First input vector\n        v2 (numpy.ndarray): Second input vector\n        \n    Returns:\n        float: Cosine similarity between v1 and v2, rounded to three decimal places\n    \n    Raises:\n        ValueError: If vectors don't have the same shape or have zero magnitude\n    \"\"\"\n    # Check if vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape\")\n    \n    # Calculate dot product\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate magnitudes\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Check for zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude\")\n    \n    # Calculate and round cosine similarity\n    return round(dot_product / (magnitude_v1 * magnitude_v2), 3)\n```\n\n## How It Works:\n\n1. The function first checks if both vectors have the same shape\n2. It calculates the dot product of the two vectors using `np.dot()`\n3. It computes the magnitude (Euclidean norm) of each vector using `np.linalg.norm()`\n4. It verifies that neither vector has zero magnitude\n5. It calculates the cosine similarity as the dot product divided by the product of magnitudes\n6. Finally, it rounds the result to three decimal places as required\n\nThis implementation efficiently handles all the requirements and constraints specified in the task."}
{"task_id": 77, "completion_id": 0, "completion": "I'll implement a function to calculate various performance metrics for a binary classification model. Here's the solution:\n\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n    \n    Args:\n        actual: The actual class labels (1 for positive, 0 for negative).\n        predicted: The predicted class labels from the model.\n        \n    Returns:\n        A tuple containing confusion_matrix, accuracy, f1_score, specificity, and\n        negative_predictive_value.\n    \"\"\"\n    # Validate input\n    if len(actual) != len(predicted):\n        raise ValueError(\"Both lists must have the same length.\")\n    \n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in the lists must be either 0 or 1.\")\n    \n    # Calculate components of confusion matrix\n    true_positive = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n    true_negative = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n    false_positive = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n    false_negative = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n    \n    # Create confusion matrix\n    confusion_matrix = [[true_positive, false_negative], \n                         [false_positive, true_negative]]\n    \n    # Calculate accuracy\n    total = len(actual)\n    accuracy = round((true_positive + true_negative) / total, 3)\n    \n    # Calculate precision and recall for F1 score\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n    \n    # Calculate F1 score\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    \n    # Calculate specificity\n    specificity = round(true_negative / (true_negative + false_positive), 3) if (true_negative + false_positive) > 0 else 0\n    \n    # Calculate negative predictive value\n    npv = round(true_negative / (true_negative + false_negative), 3) if (true_negative + false_negative) > 0 else 0\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, npv)\n```\n\nThis implementation:\n\n1. Validates the input to ensure both lists have the same length and contain only 0 and 1\n2. Calculates the confusion matrix elements (TP, TN, FP, FN)\n3. Constructs the confusion matrix with the format [[TP, FN], [FP, TN]]\n4. Calculates accuracy, F1 score, specificity, and negative predictive value\n5. Handles edge cases where denominators might be zero\n6. Rounds the final metric values to three decimal places as required\n\nThe function returns a tuple with all the requested metrics in the specified format."}
{"task_id": 78, "completion_id": 0, "completion": "# Descriptive Statistics Calculator\n\nI'll implement a function that calculates various descriptive statistics for a dataset using NumPy.\n\n```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a dataset.\n    \n    Args:\n        data: List or NumPy array of numerical values\n        \n    Returns:\n        Dictionary containing descriptive statistics\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    data_array = np.array(data, dtype=float)\n    \n    # Calculate basic statistics\n    mean = round(float(np.mean(data_array)), 4)\n    median = round(float(np.median(data_array)), 4)\n    \n    # Calculate mode\n    unique_values, counts = np.unique(data_array, return_counts=True)\n    max_count = np.max(counts)\n    \n    # If the maximum count is 1, there's no mode (all values appear only once)\n    if max_count == 1:\n        mode = None\n    else:\n        mode_indices = np.where(counts == max_count)[0]\n        modes = unique_values[mode_indices].tolist()\n        \n        # If there's only one mode, return it directly\n        if len(modes) == 1:\n            mode = float(modes[0])\n        else:\n            # Return all modes\n            mode = [float(m) for m in modes]\n    \n    # Calculate variance and standard deviation\n    variance = round(float(np.var(data_array)), 4)\n    std_dev = round(float(np.std(data_array)), 4)\n    \n    # Calculate percentiles\n    percentile_25 = round(float(np.percentile(data_array, 25)), 4)\n    percentile_50 = round(float(np.percentile(data_array, 50)), 4)\n    percentile_75 = round(float(np.percentile(data_array, 75)), 4)\n    \n    # Calculate interquartile range (IQR)\n    iqr = round(float(percentile_75 - percentile_25), 4)\n    \n    # Return results as a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': std_dev,\n        '25th_percentile': percentile_25,\n        '50th_percentile': percentile_50,\n        '75th_percentile': percentile_75,\n        'interquartile_range': iqr\n    }\n```\n\nThis function:\n1. Takes a list or NumPy array of numerical values\n2. Calculates all required statistics (mean, median, mode, etc.)\n3. Handles edge cases like no mode (when no value appears more than once)\n4. Returns multiple modes if they exist\n5. Rounds all calculated values to 4 decimal places as required\n6. Returns all the statistics in a dictionary with the specified keys\n\nYou can use it with any numerical dataset, like:\n```python\nstats = descriptive_statistics([1, 2, 2, 3, 4, 5, 5, 6])\nprint(stats)\n```"}
{"task_id": 79, "completion_id": 0, "completion": "# Binomial Distribution Probability Solution\n\nHere's a Python function to calculate the binomial probability as requested:\n\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Check if k is within valid range\n    if k < 0 or k > n:\n        return 0.0\n    \n    # Calculate combinations C(n, k)\n    combinations = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n    \n    # Calculate probability using binomial formula: C(n,k) * p^k * (1-p)^(n-k)\n    probability = combinations * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Round to five decimal places\n    return round(probability, 5)\n```\n\n## Explanation:\n\n1. This function implements the binomial probability formula: P(X = k) = C(n,k) \u00d7 p^k \u00d7 (1-p)^(n-k)\n\n2. First, we handle the edge case where k is not in the valid range (0 to n), returning 0 probability.\n\n3. We calculate the binomial coefficient C(n,k) using factorial calculations.\n\n4. Then we apply the complete formula, multiplying the combinations by p^k and (1-p)^(n-k).\n\n5. Finally, we round the result to five decimal places as required.\n\n## Example Usage:\n```python\n# Probability of getting exactly 3 heads in 10 fair coin tosses\nprint(binomial_probability(10, 3, 0.5))  # Output: 0.11719\n```"}
{"task_id": 80, "completion_id": 0, "completion": "# Normal Distribution PDF Calculator Solution\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    # Check if standard deviation is positive\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be positive\")\n        \n    # Calculate the denominator part: \u03c3 * sqrt(2\u03c0)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    \n    # Calculate the exponent part: -(x - \u03bc)\u00b2/(2\u03c3\u00b2)\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    \n    # Calculate the PDF value using the formula: (1 / (\u03c3 * sqrt(2\u03c0))) * e^(-(x - \u03bc)\u00b2/(2\u03c3\u00b2))\n    pdf_value = (1 / denominator) * math.exp(exponent)\n    \n    # Round the result to 5 decimal places\n    return round(pdf_value, 5)\n```\n\n## Explanation:\n\nThis function implements the probability density function for the normal distribution using the mathematical formula:\n\nf(x) = (1 / (\u03c3 * \u221a(2\u03c0))) * e^(-(x - \u03bc)\u00b2/(2\u03c3\u00b2))\n\nWhere:\n- x is the input value\n- \u03bc (mu) is the mean\n- \u03c3 (sigma) is the standard deviation\n\nThe implementation:\n1. Validates that the standard deviation is positive\n2. Computes the denominator part of the formula\n3. Calculates the exponent term\n4. Combines these to get the final PDF value\n5. Returns the result rounded to 5 decimal places as required\n\nTo use this function, simply call it with your desired parameters:\n```\n# Example: Standard normal distribution at x=0\nprobability = normal_pdf(0, 0, 1)  # Returns 0.39894\n```"}
{"task_id": 81, "completion_id": 0, "completion": "# Poisson Distribution Probability Calculator\n\nI'll implement a function to calculate the probability using the Poisson distribution formula:\n\nP(X = k) = (e^(-\u03bb) * \u03bb^k) / k!\n\nHere's the complete implementation:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability rounded to 5 decimal places\n    \"\"\"\n    # Check if k is a non-negative integer\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    \n    # Check if lambda is positive\n    if lam <= 0:\n        raise ValueError(\"lambda must be positive\")\n    \n    # Calculate using the Poisson formula: P(X = k) = (e^(-\u03bb) * \u03bb^k) / k!\n    numerator = math.exp(-lam) * (lam ** k)\n    denominator = math.factorial(k)\n    \n    probability = numerator / denominator\n    \n    # Round to 5 decimal places\n    return round(probability, 5)\n```\n\nThis function:\n1. Validates the inputs (k must be a non-negative integer, lambda must be positive)\n2. Calculates the Poisson probability using the formula (e^(-\u03bb) * \u03bb^k) / k!\n3. Returns the result rounded to 5 decimal places\n\nThe implementation uses Python's built-in math module for the exponential function and factorial calculation."}
{"task_id": 82, "completion_id": 0, "completion": "# Grayscale Image Contrast Calculator\n\nHere's my solution for calculating the contrast of a grayscale image:\n\n```python\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    # Find the maximum and minimum pixel values\n    max_val = np.max(img)\n    min_val = np.min(img)\n    \n    # Calculate the contrast as the difference between max and min values\n    contrast = max_val - min_val\n    \n    return contrast\n```\n\n## Explanation:\n\n1. The function takes a 2D numpy array representing a grayscale image as input.\n2. It finds the maximum pixel value in the image using `np.max()`.\n3. It finds the minimum pixel value in the image using `np.min()`.\n4. It calculates the contrast as the difference between these two values.\n5. The function returns this contrast value.\n\nThis implementation is simple yet effective for calculating the contrast of grayscale images based on the max-min definition of contrast."}
{"task_id": 83, "completion_id": 0, "completion": "# Dot Product Calculator\n\nHere's a function to calculate the dot product of two vectors using NumPy:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the vectors are not 1D or have different lengths.\n    \"\"\"\n    # Check if inputs are 1D arrays\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays\")\n    \n    # Check if vectors have the same length\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Vectors must have the same length\")\n    \n    # Calculate dot product using NumPy's dot function\n    return np.dot(vec1, vec2)\n```\n\nThis function:\n1. Validates that both inputs are 1D arrays\n2. Checks that both vectors have the same length\n3. Uses NumPy's optimized `dot()` function to calculate the dot product\n4. Returns the result as a single number\n\nThe function will work with any valid 1D NumPy arrays, including integer and floating-point vectors."}
{"task_id": 84, "completion_id": 0, "completion": "I'll implement the phi transformation function for polynomial features. This function will take a list of numerical values and transform them into polynomial features up to the specified degree.\n\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features\n                          of the corresponding data point, rounded to 8 decimal places.\n    \"\"\"\n    # Check if degree is less than 0\n    if degree < 0:\n        return []\n    \n    # Convert input data to numpy array for easier manipulation\n    data_array = np.array(data, dtype=float)\n    \n    # Initialize the result list\n    result = []\n    \n    # For each data point, generate polynomial features\n    for x in data_array:\n        features = []\n        # Add polynomial features from degree 0 (constant) to the specified degree\n        for d in range(degree + 1):\n            # Calculate x^d and round to 8 decimal places\n            feature_value = round(x ** d, 8)\n            features.append(feature_value)\n        result.append(features)\n    \n    return result\n```\n\nThis implementation:\n1. Checks if the degree is negative, returning an empty list if it is\n2. Converts the input data to a numpy array for easier manipulation\n3. For each data point, generates polynomial features from x^0 (constant 1) to x^degree\n4. Rounds each feature value to 8 decimal places\n5. Returns the list of transformed feature lists\n\nThe function handles the transformation of each input value into a set of polynomial features, allowing linear models to capture nonlinear relationships in the data."}
{"task_id": 85, "completion_id": 0, "completion": "# Positional Encoding Calculator Solution\n\nI'll implement the positional encoding function for Transformers as specified in the \"Attention is All You Need\" paper.\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    # Check for invalid inputs\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Create an array for the positional encodings\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Calculate positional encodings\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                # Even indices: apply sine function\n                pe[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n            else:\n                # Odd indices: apply cosine function\n                pe[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n    \n    return pe.tolist()\n```\n\nThis implementation:\n1. Returns -1 for invalid inputs (`position` = 0 or `d_model` \u2264 0)\n2. Creates a position encoding matrix of shape (position, d_model)\n3. Applies the standard Transformer positional encoding formula:\n   - For even dimensions (0, 2, 4...): sin(pos / 10000^(i/d_model))\n   - For odd dimensions (1, 3, 5...): cos(pos / 10000^((i-1)/d_model))\n4. Returns the result as a list after converting to float16 datatype\n\nThe positional encoding helps Transformer models understand the order of tokens in a sequence, as the basic attention mechanism has no notion of sequence order."}
{"task_id": 86, "completion_id": 0, "completion": "# Model Fit Quality Detector\n\nI'll implement a function to determine whether a model is overfitting, underfitting, or performing well based on the given accuracy criteria.\n\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting: Training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    \n    # Check for underfitting: Both accuracies are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    \n    # If neither overfitting nor underfitting, then it's a good fit\n    return 0\n```\n\n## Explanation:\n\n1. **Overfitting (return 1)**: \n   - The model performs much better on training data than test data (difference > 0.2)\n   - This indicates the model has memorized the training data but fails to generalize\n\n2. **Underfitting (return -1)**:\n   - Both training and test accuracies are below 0.7\n   - This suggests the model is too simple to capture the underlying patterns\n\n3. **Good fit (return 0)**:\n   - The model shows similar performance on both training and test data\n   - Both accuracies are reasonably high\n\nNote that if both conditions are true (e.g., training_accuracy = 0.65, test_accuracy = 0.4), the function prioritizes detecting overfitting first, since that check comes first in the evaluation sequence."}
{"task_id": 87, "completion_id": 0, "completion": "# Adam Optimizer Implementation\n\nThe Adam optimizer is a popular method for stochastic optimization that combines the benefits of AdaGrad and RMSProp. Here's an implementation of the update step for the Adam optimizer:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m_new = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v_new = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_new / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v_new / (1 - beta2 ** t)\n    \n    # Update parameters\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 5 decimal places\n    updated_parameter = np.round(updated_parameter, 5)\n    m_new = np.round(m_new, 5)\n    v_new = np.round(v_new, 5)\n    \n    # Convert to list if arrays\n    if isinstance(updated_parameter, np.ndarray):\n        updated_parameter = updated_parameter.tolist()\n    if isinstance(m_new, np.ndarray):\n        m_new = m_new.tolist()\n    if isinstance(v_new, np.ndarray):\n        v_new = v_new.tolist()\n    \n    return updated_parameter, m_new, v_new\n```\n\nThis implementation:\n\n1. Updates the first moment (mean) and second moment (uncentered variance) estimates\n2. Applies bias correction to counteract the initialization bias\n3. Computes the parameter update using the Adam formula\n4. Rounds all values to 5 decimal places as required\n5. Converts numpy arrays to Python lists when necessary\n6. Works with both scalar values and multi-dimensional arrays\n\nThe function returns a tuple with the updated parameter value and the new moving averages."}
{"task_id": 88, "completion_id": 0, "completion": "# GPT-2 Text Generation Implementation\n\nI'll implement a simplified GPT-2-like text generation function that includes the core components you described. Let me build this step by step:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    \n    # Load encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Tokenize the prompt\n    token_ids = encoder.encode(prompt)\n    \n    # Generate tokens one by one in an autoregressive manner\n    for _ in range(n_tokens_to_generate):\n        # Limit context to model's context window size\n        context = token_ids[-hparams[\"n_ctx\"]:]\n        \n        # Forward pass through model to get next token predictions\n        next_token_logits = gpt2_forward(context, hparams, params)\n        \n        # Sample next token from the logits\n        next_token_probs = softmax(next_token_logits)\n        next_token = np.random.choice(len(next_token_probs), p=next_token_probs)\n        \n        # Add the new token to our sequence\n        token_ids.append(next_token)\n    \n    # Decode the full sequence back to text\n    generated_text = encoder.decode(token_ids)\n    return generated_text\n\ndef gpt2_forward(token_ids, hparams, params):\n    \"\"\"Forward pass through the GPT-2 model to get logits for next token prediction\"\"\"\n    # Get token embeddings for the sequence\n    token_embeddings = np.array([params[\"wte\"][token_id] for token_id in token_ids])\n    \n    # Get position embeddings\n    positions = np.arange(len(token_ids))\n    position_embeddings = params[\"wpe\"][positions]\n    \n    # Combine token and positional embeddings\n    hidden_states = token_embeddings + position_embeddings\n    \n    # Process through transformer blocks\n    # Note: Since params[\"blocks\"] is empty in our simplified setup,\n    # we'll implement a basic simulation of transformer blocks\n    hidden_states = apply_transformer_blocks(hidden_states, hparams)\n    \n    # Apply final layer normalization\n    hidden_states = layer_norm(hidden_states, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n    \n    # Get logits for next token prediction (only from the last position)\n    logits = hidden_states[-1] @ params[\"wte\"].T\n    \n    return logits\n\ndef apply_transformer_blocks(hidden_states, hparams):\n    \"\"\"Apply a sequence of transformer blocks to the hidden states\"\"\"\n    # This is a simplified version since we don't have actual transformer block parameters\n    \n    # In each transformer block, we would normally:\n    # 1. Apply self-attention\n    # 2. Apply feed-forward network\n    # with residual connections and layer normalization\n    \n    # Multi-head attention\n    attention_output = multi_head_attention(hidden_states, hparams[\"n_head\"])\n    hidden_states = hidden_states + attention_output  # Residual connection\n    \n    # Feed-forward network\n    ffn_output = feed_forward_network(hidden_states)\n    hidden_states = hidden_states + ffn_output  # Residual connection\n    \n    return hidden_states\n\ndef multi_head_attention(hidden_states, n_heads):\n    \"\"\"Apply multi-head self-attention mechanism\"\"\"\n    seq_length = hidden_states.shape[0]\n    \n    # In a full implementation, we would:\n    # 1. Project hidden states to query, key, value tensors\n    # 2. Split into multiple attention heads\n    # 3. Compute scaled dot-product attention\n    # 4. Combine attention outputs\n    # 5. Project back to original dimension\n    \n    # Since we don't have the actual weights, we'll create a simplified attention pattern\n    # that respects causal masking (only attending to previous tokens)\n    \n    # Create causal attention mask (lower triangular)\n    mask = np.tril(np.ones((seq_length, seq_length)))\n    \n    # Simple attention implementation\n    attention_output = np.zeros_like(hidden_states)\n    for i in range(seq_length):\n        # For each position, create a weighted sum of previous positions\n        weights = mask[i][:seq_length] / (np.sum(mask[i][:seq_length]) + 1e-10)\n        attention_output[i] = weights.reshape(-1, 1) * hidden_states\n        attention_output[i] = np.sum(attention_output[i], axis=0)\n    \n    return attention_output * 0.1  # Scale down the output to prevent instability\n\ndef feed_forward_network(hidden_states):\n    \"\"\"Apply a feed-forward network to each position\"\"\"\n    # In a full implementation, this would be:\n    # FFN(x) = max(0, x\u00b7W\u2081 + b\u2081)\u00b7W\u2082 + b\u2082\n    \n    # Simplified version\n    return hidden_states * 0.1  # Scale to prevent instability\n\ndef layer_norm(x, g, b, eps=1e-5):\n    \"\"\"Apply layer normalization\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(var + eps) + b\n\ndef softmax(x):\n    \"\"\"Compute softmax values for the vector x\"\"\"\n    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n    return exp_x / np.sum(exp_x)\n```\n\nThis implementation provides a simplified version of GPT-2 text generation with all the key components:\n\n1. **Token Embeddings**: We convert input tokens to embeddings using the `wte` parameter.\n2. **Positional Embeddings**: We add positional information using the `wpe` parameter.\n3. **Multi-head Attention**: We implemented a basic causal self-attention mechanism.\n4. **Feed-Forward Network**: We applied a simplified feed-forward transformation.\n5. **Layer Normalization**: We implemented proper layer normalization.\n\nThe function follows an autoregressive generation process, predicting one token at a time based on the previously generated tokens. Since we're using a very simplified model with random parameters, the actual output won't be meaningful text, but the structure demonstrates the core concepts of GPT-2 text generation."}
{"task_id": 89, "completion_id": 0, "completion": "# Understanding the Pattern Weaver Task\n\nThis task is asking me to implement a simplified self-attention mechanism for a set of crystals. Self-attention is a powerful concept where each element in a sequence interacts with every other element, weighing their importance to produce enhanced representations.\n\nLet me implement this step by step:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implementing softmax with numerical stability\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    \n    # Reshape crystal values into an n\u00d7dimension matrix if needed\n    crystal_matrix = np.array(crystal_values)\n    if len(crystal_matrix.shape) == 1:\n        crystal_matrix = crystal_matrix.reshape(n, dimension)\n    \n    # Initialize the result matrix to hold the enhanced patterns\n    enhanced_patterns = []\n    \n    # Calculate self-attention for each crystal\n    for i in range(n):\n        # Calculate attention scores with all crystals\n        attention_scores = np.zeros(n)\n        for j in range(n):\n            # Using dot product as the attention mechanism\n            attention_scores[j] = np.dot(crystal_matrix[i], crystal_matrix[j])\n        \n        # Scale by sqrt(dimension) for stability\n        attention_scores = attention_scores / np.sqrt(dimension)\n        \n        # Apply softmax to get attention weights\n        attention_weights = softmax(attention_scores)\n        \n        # Compute the weighted sum to get the enhanced pattern\n        enhanced_pattern = np.zeros(dimension)\n        for j in range(n):\n            enhanced_pattern += attention_weights[j] * crystal_matrix[j]\n        \n        enhanced_patterns.append(enhanced_pattern)\n    \n    # Flatten and round to 4 decimal places\n    result = np.round(np.array(enhanced_patterns).flatten(), 4).tolist()\n    return result\n```\n\nThis implementation:\n1. Takes the crystal values and organizes them into a matrix\n2. For each crystal, calculates its attention scores with every other crystal\n3. Applies softmax to convert these scores into weights\n4. Computes a weighted sum to get the enhanced pattern for each crystal\n5. Returns the flattened results rounded to 4 decimal places\n\nThe key insight is that each crystal's enhanced pattern depends on all crystals, with the strength of influence determined by their relationship through the attention mechanism."}
{"task_id": 90, "completion_id": 0, "completion": "# BM25 Ranking Implementation\n\nLet me implement the BM25 ranking function, which is an advanced variant of TF-IDF that handles term frequency saturation and document length normalization.\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus with respect to the query.\n    \n    Args:\n        corpus (list): List of documents, where each document is a list of terms.\n        query (list): List of terms in the query.\n        k1 (float): Parameter to control term frequency saturation.\n        b (float): Parameter to control document length normalization.\n    \n    Returns:\n        list: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    # Calculate document statistics\n    N = len(corpus)  # Number of documents in corpus\n    doc_lengths = [len(doc) for doc in corpus]  # Length of each document\n    avgdl = sum(doc_lengths) / N  # Average document length\n    \n    # Calculate document frequency for each term in the query\n    df = {}\n    for term in query:\n        df[term] = sum(1 for doc in corpus if term in doc)\n    \n    # Calculate IDF for each term in the query\n    idf = {}\n    for term, count in df.items():\n        idf[term] = np.log((N - count + 0.5) / (count + 0.5) + 1)\n    \n    # Calculate BM25 scores for each document\n    scores = []\n    for i, doc in enumerate(corpus):\n        score = 0\n        doc_counter = Counter(doc)\n        doc_len = doc_lengths[i]\n        \n        for term in query:\n            if term in doc_counter:\n                term_freq = doc_counter[term]\n                numerator = term_freq * (k1 + 1)\n                denominator = term_freq + k1 * (1 - b + b * doc_len / avgdl)\n                term_score = idf[term] * (numerator / denominator)\n                score += term_score\n        \n        scores.append(round(score, 3))\n    \n    return scores\n```\n\n### Explanation:\n\n1. **Document Statistics**: I first calculate the total number of documents, their individual lengths, and the average document length.\n\n2. **Document Frequency**: For each query term, I count how many documents contain that term.\n\n3. **Inverse Document Frequency (IDF)**: I calculate the IDF for each query term using the BM25 formula: log((N - df + 0.5) / (df + 0.5) + 1).\n\n4. **BM25 Scoring**:\n   - For each document, I iterate through query terms\n   - For each term that appears in the document, I apply the BM25 formula with term frequency saturation (k1) and document length normalization (b)\n   - The final document score is the sum of all term scores\n   - Scores are rounded to three decimal places as required\n\nThis implementation balances both efficiency and readability while correctly implementing the BM25 ranking algorithm."}
{"task_id": 91, "completion_id": 0, "completion": "# F1 Score Calculator in Python\n\nHere's a solution to calculate the F1 score from predicted and true labels:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Ensure the inputs have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input lists must have the same length\")\n    \n    # Calculate true positives, false positives, and false negatives\n    tp = fp = fn = 0\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    \n    # Calculate precision\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    \n    # Calculate recall\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    \n    # Calculate F1 score\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    \n    # Round to 3 decimal places\n    return round(f1_score, 3)\n```\n\n## Explanation:\n\n1. First, I check if the input lists have the same length.\n2. I initialize counters for true positives (tp), false positives (fp), and false negatives (fn).\n3. I iterate through both lists simultaneously to count:\n   - True positives: both true and predicted labels are 1\n   - False positives: true label is 0 but predicted is 1\n   - False negatives: true label is 1 but predicted is 0\n4. I calculate precision as tp/(tp+fp), handling the case where tp+fp=0\n5. I calculate recall as tp/(tp+fn), handling the case where tp+fn=0\n6. I calculate the F1 score using the formula: 2*(precision*recall)/(precision+recall)\n7. Finally, I round the result to 3 decimal places and return it\n\nThis implementation correctly handles edge cases where precision or recall might be undefined."}
{"task_id": 92, "completion_id": 0, "completion": "# Power Grid Optimization Solution\n\nI'll build a solution to forecast Mars colony's power consumption for day 15, with all the required steps.\n\n```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    days = list(range(1, 11))\n    detrended_data = []\n    \n    for i, consumption in zip(days, consumption_data):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    \n    # 2) Perform linear regression on the detrended data.\n    mean_days = sum(days) / len(days)\n    mean_consumption = sum(detrended_data) / len(detrended_data)\n    \n    # Calculate slope using least squares method\n    numerator = sum((x - mean_days) * (y - mean_consumption) for x, y in zip(days, detrended_data))\n    denominator = sum((x - mean_days) ** 2 for x in days)\n    slope = numerator / denominator\n    \n    # Calculate y-intercept\n    intercept = mean_consumption - slope * mean_days\n    \n    # 3) Predict day 15's base consumption.\n    day_15_base = slope * 15 + intercept\n    \n    # 4) Add the day 15 fluctuation back.\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_with_fluctuation = day_15_base + fluctuation_15\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    day_15_rounded = round(day_15_with_fluctuation)\n    day_15_with_safety = math.ceil(day_15_rounded * 1.05)\n    \n    # 6) Return the final integer.\n    return day_15_with_safety\n```\n\n## Explanation\n\nThis solution:\n\n1. Removes the known daily fluctuation pattern from each data point by calculating the fluctuation value for each day and subtracting it from the consumption data\n2. Implements a linear regression model from scratch using the ordinary least squares method\n3. Uses the regression model to predict the base consumption for day 15\n4. Adds back the daily fluctuation for day 15 using the same formula\n5. Applies a 5% safety margin as specified:\n   - First rounds the prediction to the nearest integer\n   - Then adds 5% to this rounded value\n   - Finally rounds up to ensure sufficient capacity\n6. Returns the final integer value\n\nThis approach ensures the Mars colony has adequate power capacity, accounting for both the underlying consumption trend and the cyclical fluctuations, with a safety buffer for unexpected needs."}
{"task_id": 93, "completion_id": 0, "completion": "# Mean Absolute Error (MAE) Implementation\n\nHere's my implementation of the function to calculate Mean Absolute Error between two arrays:\n\n```python\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape\")\n    \n    # Calculate absolute differences between true and predicted values\n    abs_diff = np.abs(y_true - y_pred)\n    \n    # Calculate mean of absolute differences\n    mae_value = np.mean(abs_diff)\n    \n    # Round to 3 decimal places\n    return round(mae_value, 3)\n```\n\n## Explanation:\n\n1. First, I verify that both input arrays have the same shape - this is necessary for element-wise comparison\n2. I calculate the absolute differences between corresponding elements using NumPy's `abs()` function\n3. Then I compute the mean of these absolute differences using NumPy's `mean()` function\n4. Finally, I round the result to 3 decimal places as specified in the requirements\n\nThis implementation is efficient as it leverages NumPy's vectorized operations rather than using loops to process the array elements."}
{"task_id": 94, "completion_id": 0, "completion": "# Multi-Head Attention Implementation\n\nI'll implement the multi-head attention mechanism which is a key component in Transformer models. The implementation will have three main functions: computing QKV matrices, self-attention, and multi-head attention.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, and Value matrices by multiplying input with weight matrices\n    \n    Args:\n        X: Input matrix of shape (batch_size, seq_len, d_model)\n        W_q, W_k, W_v: Weight matrices for Q, K, V projections\n        \n    Returns:\n        Q, K, V matrices\n    \"\"\"\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    \n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute self-attention mechanism\n    \n    Args:\n        Q: Query matrix\n        K: Key matrix\n        V: Value matrix\n        \n    Returns:\n        Attention output\n    \"\"\"\n    # Get dimensions\n    d_k = K.shape[-1]\n    \n    # Compute attention scores (Q * K^T / sqrt(d_k))\n    scores = np.matmul(Q, np.transpose(K, (0, 2, 1))) / np.sqrt(d_k)\n    \n    # Apply softmax to get attention weights\n    exp_scores = np.exp(scores)\n    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Apply attention weights to values\n    output = np.matmul(weights, V)\n    \n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention\n    \n    Args:\n        X: Input tensor of shape (batch_size, seq_len, d_model)\n        W_q, W_k, W_v: Weight matrices for Q, K, V\n        n_heads: Number of attention heads\n        \n    Returns:\n        Multi-head attention output as a Python list\n    \"\"\"\n    # Compute the Query (Q), Key (K), and Value (V) matrices.\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Get dimensions\n    batch_size, seq_len = X.shape[0], X.shape[1]\n    d_k = W_q.shape[1] // n_heads\n    d_v = W_v.shape[1] // n_heads\n    \n    # Reshape for multi-head attention\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_k)\n    K = K.reshape(batch_size, seq_len, n_heads, d_k)\n    V = V.reshape(batch_size, seq_len, n_heads, d_v)\n    \n    # Transpose to get shapes (batch_size, n_heads, seq_len, d_k/d_v)\n    Q = Q.transpose(0, 2, 1, 3)\n    K = K.transpose(0, 2, 1, 3)\n    V = V.transpose(0, 2, 1, 3)\n    \n    # Compute attention for each head\n    heads_output = np.zeros((batch_size, n_heads, seq_len, d_v))\n    for i in range(n_heads):\n        heads_output[:, i] = self_attention(Q[:, i:i+1], K[:, i:i+1], V[:, i:i+1])\n    \n    # Transpose and reshape back: (batch_size, seq_len, n_heads*d_v)\n    attention_output = heads_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, n_heads * d_v)\n    \n    # Round to 4 decimal places as required\n    attention_output = np.round(attention_output, 4)\n    \n    # Convert to Python list\n    return attention_output.tolist()\n```\n\nThis implementation follows the standard multi-head attention mechanism:\n\n1. First, it projects the input data into Query, Key, and Value representations\n2. Then, it splits these projections into multiple heads\n3. Self-attention is computed separately for each head\n4. Finally, results from all heads are concatenated and returned as a Python list\n\nAll results are rounded to 4 decimal places as specified in the requirements."}
{"task_id": 95, "completion_id": 0, "completion": "# Solution: Calculate the Phi Coefficient\n\nHere's my implementation of the phi_corr function:\n\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    import math\n    \n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length\")\n    \n    # Initialize counters for the 2x2 contingency table\n    a = b = c = d = 0\n    \n    # Count occurrences\n    for i in range(len(x)):\n        if x[i] == 1 and y[i] == 1:\n            a += 1        # Both x and y are 1\n        elif x[i] == 1 and y[i] == 0:\n            b += 1        # x is 1, y is 0\n        elif x[i] == 0 and y[i] == 1:\n            c += 1        # x is 0, y is 1\n        else:             # x is 0, y is 0\n            d += 1\n    \n    # Calculate Phi coefficient\n    numerator = a * d - b * c\n    denominator = math.sqrt((a + b) * (c + d) * (a + c) * (b + d))\n    \n    # Handle division by zero (when one variable has no variation)\n    if denominator == 0:\n        return 0.0\n    \n    phi = numerator / denominator\n    \n    # Round to 4 decimal places\n    return round(phi, 4)\n```\n\n## Explanation\n\nThe Phi coefficient measures correlation between two binary variables, similar to the Pearson correlation coefficient. The implementation works by:\n\n1. Validating input lists have the same length\n2. Constructing a 2\u00d72 contingency table by counting the four possible combinations:\n   - a: Both variables are 1\n   - b: First variable is 1, second is 0\n   - c: First variable is 0, second is 1\n   - d: Both variables are 0\n3. Calculating the coefficient using the formula: \u03c6 = (ad - bc)/\u221a((a+b)(c+d)(a+c)(b+d))\n4. Handling edge cases where the denominator might be zero\n5. Rounding the result to 4 decimal places\n\nThe function properly handles binary inputs (0s and 1s) and returns a correlation value between -1 and 1."}
{"task_id": 96, "completion_id": 0, "completion": "# Hard Sigmoid Activation Function Implementation\n\nThe Hard Sigmoid function is a computationally efficient approximation of the standard sigmoid function. It's often used in deep learning models where performance is critical.\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Hard Sigmoid piecewise function:\n    # 0             if x < -2.5\n    # 0.2 * x + 0.5 if -2.5 <= x <= 2.5\n    # 1             if x > 2.5\n    \n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n```\n\nThis implementation follows the standard definition used in many deep learning frameworks like Keras. The function:\n- Returns 0 for inputs less than -2.5\n- Returns 1 for inputs greater than 2.5\n- Returns a linear interpolation (0.2 * x + 0.5) for inputs between -2.5 and 2.5\n\nThe advantage of Hard Sigmoid over regular Sigmoid is that it avoids expensive exponential operations, making it faster to compute while maintaining a reasonable approximation."}
{"task_id": 97, "completion_id": 0, "completion": "# ELU Activation Function Implementation\n\nI'll implement the ELU activation function according to its mathematical definition:\n\n```python\nimport math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```\n\nThis implementation follows the ELU formula:\n- For positive inputs (x > 0): f(x) = x\n- For negative inputs (x \u2264 0): f(x) = \u03b1(e^x - 1)\n\nThe function returns values rounded to 4 decimal places as required.\n\n### Examples:\n- For x = 2.0: Returns 2.0 (unchanged positive value)\n- For x = 0.0: Returns 0.0 (at the boundary)\n- For x = -1.0 with \u03b1 = 1.0: Returns -0.6321 (saturates toward -\u03b1)\n- For x = -3.0 with \u03b1 = 0.5: Returns -0.4751 (saturates toward -\u03b1)\n\nThe ELU activation provides smooth gradients near zero and allows negative values, addressing the \"dying ReLU\" problem while maintaining ReLU's benefits for positive values."}
{"task_id": 98, "completion_id": 0, "completion": "# PReLU Activation Function Implementation\n\nThe PReLU function is a variant of ReLU that allows for a learned parameter to control the slope in the negative region, making it more flexible than standard ReLU.\n\nHere's the implementation:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n```\n\nThe function works by:\n- Returning the input value (`x`) unchanged if it's positive\n- Multiplying the input by the alpha parameter when the input is negative\n\nThis allows the network to learn the optimal slope for negative inputs during training, potentially addressing the \"dying ReLU\" problem where neurons can get stuck during gradient-based learning.\n\nYou can also write this more concisely using Python's built-in functions:\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    return max(0, x) + alpha * min(0, x)\n```\n\nBoth implementations are equivalent."}
{"task_id": 99, "completion_id": 0, "completion": "# Softplus Activation Function Implementation\n\nHere's my implementation of the softplus function with proper handling of edge cases:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Using a numerically stable approach to avoid overflow/underflow\n    if x >= 0:\n        # For positive inputs, use the identity: log(1 + e^x) = x + log(1 + e^(-x))\n        # This prevents overflow for large positive values\n        result = x + math.log(1 + math.exp(-x))\n    else:\n        # For negative inputs, compute directly\n        # This is numerically stable because e^x is small for negative x\n        result = math.log(1 + math.exp(x))\n    \n    # Round to the 4th decimal place as required\n    return round(result, 4)\n```\n\n## Explanation:\n\n1. **Numerical Stability**: The implementation uses a mathematical identity to avoid overflow for large positive inputs.\n   \n2. **Edge Cases**:\n   - For large positive values, the function will approach x (which is correct)\n   - For large negative values, the function will approach 0 (which is correct)\n   \n3. **Rounding**: The result is rounded to 4 decimal places as specified.\n\nThis implementation properly handles all inputs without numerical issues."}
{"task_id": 100, "completion_id": 0, "completion": "# Solution: Softsign Activation Function\n\nThe Softsign activation function is defined as `f(x) = x / (1 + |x|)`, which creates a smooth curve bounded between -1 and 1. Here's my implementation:\n\n```python\nimport math\n\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # Handle infinity cases\n    if math.isinf(x):\n        return 1.0 if x > 0 else -1.0\n    \n    # Calculate softsign: x / (1 + |x|)\n    result = x / (1 + abs(x))\n    \n    # Round to 4 decimal places\n    return round(result, 4)\n```\n\n## Key features of this implementation:\n\n1. Handles the core formula: `x / (1 + |x|)`\n2. Properly handles edge cases like infinity, which would otherwise result in NaN\n3. Rounds the output to 4 decimal places as required\n4. Returns values always bounded between -1 and 1\n\nThe Softsign function has similar properties to tanh but approaches its asymptotes more slowly, which can be beneficial in some neural network architectures."}
{"task_id": 101, "completion_id": 0, "completion": "# GRPO Objective Function Implementation\n\nI'll implement the GRPO objective function as requested, which combines a clipped surrogate objective with a KL divergence penalty term.\n\n```python\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Calculate current policy probabilities from likelihood ratios\n    pi_theta = rhos * pi_theta_old\n    \n    # Compute clipped surrogate objective (similar to PPO)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate1 = rhos * A\n    surrogate2 = clipped_rhos * A\n    clipped_objective = np.minimum(surrogate1, surrogate2).mean()\n    \n    # Compute KL divergence between current policy and reference policy\n    # Add a small epsilon to avoid numerical issues\n    eps = 1e-10\n    ratio = np.maximum(pi_theta / np.maximum(pi_theta_ref, eps), eps)\n    kl_div = np.sum(pi_theta * np.log(ratio))\n    \n    # Combine the clipped objective and KL penalty\n    objective = clipped_objective - beta * kl_div\n    \n    # Round to 6 decimal places as required\n    return round(float(objective), 6)\n```\n\nThis implementation:\n1. Calculates the current policy probabilities from the likelihood ratios\n2. Computes the clipped surrogate objective (like in PPO)\n3. Adds a KL divergence penalty between the current policy and the reference policy\n4. Combines both terms and returns the final objective value rounded to 6 decimal places\n\nThe epsilon value in the clipping operation helps maintain stability by preventing large policy updates, while the beta parameter controls how much we penalize divergence from the reference policy."}
{"task_id": 102, "completion_id": 0, "completion": "# Swish Activation Function Implementation\n\nThe Swish activation function is defined as f(x) = x * sigmoid(x), where sigmoid(x) = 1/(1+e^(-x)). It's a smooth, non-monotonic function that has shown better performance than ReLU in many deep learning applications.\n\nHere's my implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # Calculate sigmoid of x\n    sigmoid = 1 / (1 + math.exp(-x))\n    \n    # Apply swish formula: x * sigmoid(x)\n    result = x * sigmoid\n    \n    # Round to 4 decimal places\n    return round(result, 4)\n```\n\nThis implementation:\n1. Calculates the sigmoid function value for the input x\n2. Multiplies x by its sigmoid value (the core Swish operation)\n3. Rounds the result to 4 decimal places as required\n\nThe function handles both positive and negative inputs correctly, producing the smooth activation curve that gives Swish its advantages over simpler activation functions."}
{"task_id": 103, "completion_id": 0, "completion": "# SELU Activation Function Implementation\n\nAfter analyzing the SELU activation function, I've implemented it with numerical stability considerations for extreme negative inputs:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        result = scale * x\n    else:\n        # For numerical stability with very negative inputs\n        if x < -100:\n            # As x approaches negative infinity, exp(x) approaches 0\n            # So the function approaches -scale * alpha\n            result = -scale * alpha\n        else:\n            result = scale * alpha * (math.exp(x) - 1)\n    \n    # Round to 4 decimal places as required\n    return round(result, 4)\n```\n\n## Explanation:\n\n1. The SELU function has two components:\n   - For positive inputs: `scale * x`\n   - For negative inputs: `scale * alpha * (exp(x) - 1)`\n\n2. The numerical stability check prevents potential underflow issues when x is extremely negative, as exp(-100) is approximately 3.7e-44, which is essentially zero for our purposes.\n\n3. The function rounds the result to 4 decimal places as specified in the requirements.\n\nThis implementation correctly handles all input ranges while maintaining numerical stability."}
{"task_id": 104, "completion_id": 0, "completion": "# Binary Classification with Logistic Regression Implementation\n\nI'll implement a prediction function for binary classification using logistic regression, which consists of computing linear combinations, applying the sigmoid function, and making predictions based on a threshold.\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute linear combination of features and weights\n    z = np.dot(X, weights) + bias\n    \n    # Apply sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    \n    # Convert to binary predictions using 0.5 threshold\n    predictions = (probabilities > 0.5).astype(int)\n    \n    # Convert to Python list as required\n    return predictions.tolist()\n```\n\nThis implementation:\n1. Calculates the linear decision boundary (z = X\u00b7weights + bias)\n2. Transforms these values to probabilities using the sigmoid function\n3. Applies a threshold of 0.5 to make binary predictions\n4. Converts the numpy array to a Python list as required\n\nThe function handles both single samples and batches of samples correctly, and returns predictions as a list of 0s and 1s."}
{"task_id": 105, "completion_id": 0, "completion": "# Softmax Regression with Gradient Descent Implementation\n\nI'll implement the Softmax regression training algorithm using gradient descent optimization. This will involve computing softmax probabilities, calculating the cross-entropy loss, and updating the parameters using gradients.\n\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Get dimensions\n    N, M = X.shape  # N samples, M features\n    C = len(np.unique(y))  # Number of classes\n    \n    # One-hot encode target variable\n    y_one_hot = np.zeros((N, C))\n    y_one_hot[np.arange(N), y] = 1\n    \n    # Initialize parameters with zeros for consistency\n    B = np.zeros((C, M))\n    \n    # Store loss values during training\n    losses = []\n    \n    for _ in range(iterations):\n        # Forward pass: compute predictions\n        scores = X.dot(B.T)  # Shape: (N, C)\n        \n        # For numerical stability, subtract max score\n        scores -= np.max(scores, axis=1, keepdims=True)\n        \n        # Softmax function\n        exp_scores = np.exp(scores)\n        softmax_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        \n        # Compute cross-entropy loss\n        epsilon = 1e-15  # To prevent log(0)\n        softmax_probs = np.clip(softmax_probs, epsilon, 1 - epsilon)\n        loss = -np.sum(y_one_hot * np.log(softmax_probs)) / N\n        losses.append(round(loss, 4))\n        \n        # Backward pass: compute gradients\n        gradient = softmax_probs - y_one_hot  # Shape: (N, C)\n        dB = gradient.T.dot(X) / N  # Shape: (C, M)\n        \n        # Update parameters\n        B -= learning_rate * dB\n    \n    # Round parameters to 4 decimal places\n    B = np.round(B, 4).tolist()\n    \n    return B, losses\n```\n\n## Key Components Explained:\n\n1. **One-hot encoding**: Converts class labels into a binary matrix\n2. **Softmax function**: Transforms raw scores into probabilities\n3. **Cross-entropy loss**: Measures the difference between predicted and actual distributions\n4. **Gradient calculation**: Computing how to adjust parameters to minimize the loss\n5. **Parameter update**: Using gradient descent to update B in the direction that reduces loss\n\nThe implementation includes numerical stability considerations like subtracting the max score before applying exp() and adding a small epsilon to prevent log(0)."}
{"task_id": 106, "completion_id": 0, "completion": "# Logistic Regression with Gradient Descent Implementation\n\nI'll implement a gradient descent algorithm for logistic regression using Binary Cross Entropy loss. Let me break down the approach:\n\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Get dimensions\n    n_samples, n_features = X.shape\n    \n    # Initialize weights and bias\n    weights = np.zeros(n_features)\n    bias = 0\n    \n    # Track loss history\n    loss_history = []\n    \n    # Sigmoid activation function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n    \n    # Gradient descent loop\n    for i in range(iterations):\n        # Forward pass\n        linear_output = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_output)\n        \n        # Calculate binary cross entropy loss\n        epsilon = 1e-15  # Small value to prevent log(0)\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_history.append(round(loss, 4))\n        \n        # Calculate gradients\n        dz = y_pred - y\n        dw = np.dot(X.T, dz) / n_samples\n        db = np.sum(dz) / n_samples\n        \n        # Update parameters\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    \n    # Round parameters to 4 decimal places\n    weights_rounded = np.round(weights, 4).tolist()\n    bias_rounded = [round(bias, 4)]\n    \n    return (weights_rounded, bias_rounded, loss_history)\n```\n\nThis implementation:\n1. Initializes model parameters (weights and bias)\n2. For each iteration:\n   - Computes the predicted probabilities using the sigmoid function\n   - Calculates the Binary Cross Entropy loss\n   - Computes gradients for weights and bias\n   - Updates parameters using gradient descent\n3. Returns a tuple containing:\n   - Weights (as a list, rounded to 4 decimals)\n   - Bias (as a single-element list, rounded to 4 decimals)\n   - Loss history (each value rounded to 4 decimals)\n\nThe algorithm handles numerical stability issues by clipping values in the sigmoid function and when calculating the loss to avoid overflow/underflow problems."}
{"task_id": 107, "completion_id": 0, "completion": "# Implementing Masked Self-Attention in Python\n\nLet me implement the masked self-attention function for you. This is a key component in transformer architectures, particularly for autoregressive models where each token should only attend to previous tokens in the sequence.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Args:\n        Q: Query matrix of shape [batch_size, seq_len, d_k]\n        K: Key matrix of shape [batch_size, seq_len, d_k]\n        V: Value matrix of shape [batch_size, seq_len, d_v]\n        mask: Attention mask of shape [batch_size, seq_len, seq_len]\n             with large negative values in positions to be masked\n    \n    Returns:\n        Output as a Python list after applying attention\n    \"\"\"\n    # Compute attention scores (scaled dot-product attention)\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, np.transpose(K, (0, 2, 1))) / np.sqrt(d_k)\n    \n    # Apply mask (adding the mask will set masked positions to negative infinity)\n    scores = scores + mask\n    \n    # Apply softmax to get attention weights\n    # Subtract max for numerical stability\n    scores_max = np.max(scores, axis=-1, keepdims=True)\n    scores_exp = np.exp(scores - scores_max)\n    attention_weights = scores_exp / (np.sum(scores_exp, axis=-1, keepdims=True) + 1e-9)\n    \n    # Apply attention weights to values\n    output = np.matmul(attention_weights, V)\n    \n    # Convert to Python list as required\n    return output.tolist()\n```\n\nThis implementation follows the standard formula for masked self-attention:\n1. Calculate attention scores as scaled dot-product of Q and K\n2. Apply the mask to prevent attending to specific positions\n3. Normalize with softmax to get attention weights\n4. Apply these weights to the values (V)\n5. Convert the result to a Python list as required\n\nThe mask typically contains 0s for positions that can be attended to and large negative values (like -1e9) for positions that should be masked out, which become ~0 after the softmax operation."}
{"task_id": 108, "completion_id": 0, "completion": "# Measuring Apple Color Disorder with Entropy\n\nFor this task, I'll implement a measure of disorder based on Shannon entropy from information theory. This is perfect for our needs because:\n\n1. It gives 0 when all elements are the same (complete order)\n2. It increases with more variety and even distribution of colors\n3. It's mathematically sound and widely used for measuring diversity/disorder\n\nHere's my implementation:\n\n```python\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The function uses Shannon entropy to quantify color diversity:\n    - Returns 0 when all apples are the same color\n    - Increases as the variety and distribution of colors increases\n    \n    Args:\n        apples: List of integers representing apple colors\n        \n    Returns:\n        A float value representing the disorder, rounded to 4 decimal places\n    \"\"\"\n    # If basket is empty or all apples are the same color, disorder is 0\n    if not apples or len(set(apples)) <= 1:\n        return 0.0\n    \n    # Count frequency of each color\n    from collections import Counter\n    color_counts = Counter(apples)\n    \n    # Calculate probabilities for each color\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate Shannon entropy as our disorder measure\n    import math\n    entropy = -sum(p * math.log2(p) for p in probabilities)\n    \n    # Round to 4 decimal places\n    return round(entropy, 4)\n```\n\n## Let's verify with the test cases:\n\n1. `[0,0,0,0]` \u2192 0.0 (all same color)\n2. `[1,1,0,0]` \u2192 1.0 (two colors evenly distributed)\n3. `[0,1,2,3]` \u2192 2.0 (four different colors)\n4. `[0,0,1,1,2,2,3,3]` \u2192 2.0 (four colors, evenly distributed)\n5. `[0,0,0,0,0,1,2,3]` \u2192 1.5488 (one dominant color with others less frequent)\n\nThis satisfies all requirements - more diverse color distributions have higher disorder values."}
{"task_id": 109, "completion_id": 0, "completion": "# Layer Normalization Implementation\n\nI'll implement a layer normalization function that normalizes across the feature dimension for sequence data.\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform layer normalization on a 3D input tensor.\n    \n    Args:\n        X: Input tensor of shape (batch_size, sequence_length, features)\n        gamma: Scale parameter of shape (features,)\n        beta: Shift parameter of shape (features,)\n        epsilon: Small constant for numerical stability\n        \n    Returns:\n        Normalized tensor with the same shape as X, rounded to 5 decimal places and converted to list\n    \"\"\"\n    # Get the shape of the input tensor\n    batch_size, seq_length, features = X.shape\n    \n    # Reshape gamma and beta if needed to ensure proper broadcasting\n    gamma = gamma.reshape(1, 1, -1)\n    beta = beta.reshape(1, 1, -1)\n    \n    # Calculate mean and variance along the feature dimension (last dimension)\n    # Keep dimensions for proper broadcasting\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    out = gamma * X_norm + beta\n    \n    # Round to 5 decimal places and convert to list\n    return np.round(out, 5).tolist()\n```\n\nThis implementation:\n1. Calculates the mean and variance across the feature dimension for each position in each sequence\n2. Normalizes the input using these statistics\n3. Applies the scale (gamma) and shift (beta) parameters\n4. Rounds the result to 5 decimal places and converts to a Python list\n\nThe function handles batches of sequences properly and maintains the original dimensions of the input."}
{"task_id": 110, "completion_id": 0, "completion": "# METEOR Score Implementation for Translation Quality Evaluation\n\nI'll implement the METEOR (Metric for Evaluation of Translation with Explicit ORdering) score calculation, which is a metric designed to evaluate machine translation quality by considering precision, recall, and word order.\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate METEOR score between reference and candidate translations.\n    \n    Args:\n        reference (str): Reference translation\n        candidate (str): Candidate translation to evaluate\n        alpha (float): Parameter controlling relative weight of precision and recall\n        beta (float): Parameter for penalty calculation\n        gamma (float): Parameter controlling penalty impact\n    \n    Returns:\n        float: METEOR score rounded to 3 decimal places\n    \"\"\"\n    # Tokenize the sentences\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    \n    # Count tokens\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    \n    # Calculate matches\n    matches = sum((ref_counts & cand_counts).values())\n    \n    # If no matches, return 0\n    if matches == 0:\n        return 0.0\n    \n    # Calculate precision and recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    \n    # Calculate F-mean (harmonic mean weighted by alpha)\n    if precision + recall == 0:\n        return 0.0\n    \n    fmean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    \n    # Calculate chunks (contiguous sequences of matched unigrams)\n    ref_matched = []\n    cand_matched = []\n    \n    for i, token in enumerate(ref_tokens):\n        if token in cand_tokens:\n            ref_matched.append((i, token))\n    \n    for i, token in enumerate(cand_tokens):\n        if token in ref_tokens:\n            cand_matched.append((i, token))\n    \n    # Sort by token to align matches\n    ref_matched.sort(key=lambda x: x[1])\n    cand_matched.sort(key=lambda x: x[1])\n    \n    # Count number of chunks\n    chunks = 1\n    for i in range(1, len(ref_matched)):\n        if ref_matched[i][0] != ref_matched[i-1][0] + 1 or cand_matched[i][0] != cand_matched[i-1][0] + 1:\n            chunks += 1\n    \n    # Calculate fragmentation fraction\n    frag_frac = chunks / matches if matches > 0 else 0\n    \n    # Calculate penalty\n    penalty = gamma * (frag_frac ** beta)\n    \n    # Calculate final METEOR score\n    score = fmean * (1 - penalty)\n    \n    return round(score, 3)\n```\n\nThis implementation follows the METEOR algorithm by:\n\n1. Tokenizing and normalizing both reference and candidate translations\n2. Calculating unigram matches between the two texts\n3. Computing precision and recall based on these matches\n4. Calculating a weighted F-mean using the alpha parameter\n5. Determining the fragmentation by identifying \"chunks\" of consecutive matches\n6. Applying a penalty based on fragmentation\n7. Computing the final score as F-mean * (1 - penalty)\n8. Rounding to 3 decimal places as required\n\nThe function handles edge cases like empty strings or no matches between translations."}
{"task_id": 111, "completion_id": 0, "completion": "# Solution: Compute Pointwise Mutual Information\n\nThe Pointwise Mutual Information (PMI) measures how much the actual joint probability of two events differs from what would be expected if they were independent. Here's the implementation:\n\n```python\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information between two events.\n    \n    Parameters:\n    joint_counts (float): Number of times events x and y occurred together\n    total_counts_x (float): Total occurrences of event x\n    total_counts_y (float): Total occurrences of event y\n    total_samples (float): Total number of samples\n    \n    Returns:\n    float: The PMI value rounded to 3 decimal places\n    \"\"\"\n    # Handle edge case: when joint probability is zero\n    if joint_counts == 0:\n        return float('-inf')\n        \n    # Calculate PMI using the formula: log(P(x,y) / (P(x) * P(y)))\n    # Which simplifies to: log((joint_counts * total_samples) / (total_counts_x * total_counts_y))\n    pmi = np.log((joint_counts * total_samples) / (total_counts_x * total_counts_y))\n    \n    return round(pmi, 3)\n```\n\nThis function:\n1. Takes the joint occurrence count and individual counts as inputs\n2. Handles the special case where the joint count is zero (returning -infinity)\n3. Computes PMI using the formula log(P(x,y) / (P(x) * P(y)))\n4. Returns the result rounded to 3 decimal places as requested\n\nThe implementation uses the natural logarithm (base e) through NumPy's log function, which is common in information theory applications."}
{"task_id": 112, "completion_id": 0, "completion": "# Min-Max Normalization Implementation\n\nI'll implement a function that scales a list of integers to the range [0, 1] using Min-Max normalization.\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers,\n    scaling all values to the range [0, 1].\n    \n    Args:\n        x: A list of integers to normalize\n        \n    Returns:\n        A list of floats normalized to the range [0, 1],\n        rounded to 4 decimal places\n    \"\"\"\n    # Handle empty list case\n    if not x:\n        return []\n    \n    # Get minimum and maximum values\n    min_val = min(x)\n    max_val = max(x)\n    \n    # Handle case where all values are the same\n    if min_val == max_val:\n        return [0.0] * len(x)\n    \n    # Apply min-max normalization formula and round to 4 decimal places\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    \n    # Round each value to 4 decimal places\n    return [round(val, 4) for val in normalized]\n```\n\n## How It Works\n\n1. First, we handle the edge case of an empty list by returning an empty list.\n2. We find the minimum and maximum values in the input list.\n3. If all values are the same (min equals max), we return a list of zeros with the same length as the input list, since the denominator would be zero in the normalization formula.\n4. For the normal case, we apply the min-max formula: (value - min) / (max - min)\n5. Finally, we round each value to 4 decimal places as required.\n\nThe function correctly normalizes any list of integers to the range [0, 1]."}
{"task_id": 113, "completion_id": 0, "completion": "# Residual Block Implementation with NumPy\n\nI'll implement a residual block function that processes a 1D input array through two weight layers with ReLU activations and a shortcut connection.\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Args:\n        x: Input 1D array\n        w1: First weight matrix\n        w2: Second weight matrix\n        \n    Returns:\n        Output of the residual block rounded to 4 decimal places as a list\n    \"\"\"\n    # First transformation\n    h1 = np.dot(x, w1)\n    \n    # Apply ReLU activation after first layer\n    h1_relu = np.maximum(0, h1)\n    \n    # Second transformation\n    h2 = np.dot(h1_relu, w2)\n    \n    # Add shortcut connection (skip connection)\n    output = h2 + x\n    \n    # Apply final ReLU activation\n    final_output = np.maximum(0, output)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(final_output, 4).tolist()\n```\n\nThis implementation:\n1. Takes the input `x` and processes it through the first weight matrix `w1`\n2. Applies ReLU activation (max(0, value)) to the result\n3. Processes the activated values through the second weight matrix `w2`\n4. Adds the original input `x` as a shortcut connection\n5. Applies another ReLU activation\n6. Rounds the final result to 4 decimal places and converts it to a Python list\n\nThe function assumes that the dimensions are compatible for matrix multiplication and addition (if x is of shape (n,), then w1 should be (n, k) and w2 should be (k, n))."}
{"task_id": 114, "completion_id": 0, "completion": "# Global Average Pooling Implementation\n\nGlobal Average Pooling (GAP) is a technique commonly used in convolutional neural networks to reduce spatial dimensions by taking the average of each feature map. Let me implement this function:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Args:\n        x: Input array of shape (height, width, channels)\n        \n    Returns:\n        1D array of shape (channels,) where each element is the average of all values\n        in the corresponding feature map\n    \"\"\"\n    # Check input dimensions\n    if len(x.shape) != 3:\n        raise ValueError(f\"Expected 3D input with shape (height, width, channels), got shape {x.shape}\")\n    \n    # Calculate the mean across spatial dimensions (height and width)\n    # This will reduce the first two dimensions, leaving only the channels dimension\n    return np.mean(x, axis=(0, 1))\n```\n\nThis implementation:\n1. Validates that the input is a 3D array\n2. Uses NumPy's `mean` function with `axis=(0, 1)` to average across the height and width dimensions\n3. Returns a 1D array of shape (channels,) containing the average value for each channel\n\nThe function efficiently handles inputs of any size and preserves the numerical precision of the input data."}
{"task_id": 115, "completion_id": 0, "completion": "# Batch Normalization for BCHW Input Implementation\n\nHere's my implementation of the batch normalization function that works with 4D tensors in BCHW format:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Applies batch normalization to a 4D input tensor in BCHW format\n    \n    Args:\n        X: Input tensor of shape (batch_size, channels, height, width)\n        gamma: Scale parameter of shape (channels,)\n        beta: Shift parameter of shape (channels,)\n        epsilon: Small constant for numerical stability\n        \n    Returns:\n        Normalized tensor with same shape as input, rounded to 4 decimal places\n    \"\"\"\n    # Calculate mean and variance across batch, height, and width dimensions\n    # Keeping dimensions for proper broadcasting\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape gamma and beta for broadcasting\n    gamma_reshaped = gamma.reshape(1, -1, 1, 1)\n    beta_reshaped = beta.reshape(1, -1, 1, 1)\n    \n    # Apply scale and shift\n    out = gamma_reshaped * X_norm + beta_reshaped\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()\n```\n\nThis implementation:\n1. Calculates mean and variance across the batch, height, and width dimensions for each channel\n2. Normalizes the input using these statistics\n3. Applies the scale (gamma) and shift (beta) parameters\n4. Returns the result rounded to 4 decimal places as a Python list\n\nThe function properly handles the 4D BCHW tensor format and ensures correct broadcasting of all operations."}
{"task_id": 116, "completion_id": 0, "completion": "# Polynomial Term Derivative Function\n\nI'll implement a function to compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`.\n\nFor a polynomial term `c * x^n`, the derivative is `c * n * x^(n-1)` according to the power rule of differentiation.\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Args:\n        c (float): The coefficient of the term\n        x (float): The point at which to evaluate the derivative\n        n (float): The exponent of the term\n        \n    Returns:\n        float: The value of the derivative at point x, rounded to 4 decimal places\n    \"\"\"\n    # Apply the power rule for derivatives: d/dx(c * x^n) = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1))\n    \n    # Round the result to 4 decimal places\n    return round(derivative, 4)\n```\n\nThis function correctly handles:\n- Terms with coefficients (multiplying by c)\n- The power rule (multiplying by n and decreasing the exponent by 1)\n- Evaluation at the specific point x\n- Rounding to 4 decimal places as required\n\nNote that this implementation will raise appropriate errors in edge cases like evaluating at x=0 when n<1 (which would result in division by zero)."}
{"task_id": 117, "completion_id": 0, "completion": "# Orthonormal Basis for 2D Vectors\n\nI'll implement the Gram-Schmidt process to compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n    \n    Args:\n        vectors: List of 2D vectors represented as lists of floats\n        tol: Tolerance value to determine linear independence\n        \n    Returns:\n        List of orthonormal vectors (as lists) that form a basis for the subspace\n    \"\"\"\n    # Convert input vectors to numpy arrays for easier computation\n    vectors = [np.array(v) for v in vectors]\n    \n    # Initialize the list to store orthogonal vectors\n    orthogonal_vectors = []\n    \n    # Gram-Schmidt process\n    for v in vectors:\n        # Start with the original vector\n        u = v.copy()\n        \n        # Subtract projections onto previous orthogonal vectors\n        for w in orthogonal_vectors:\n            projection = np.dot(v, w) / np.dot(w, w) * w\n            u = u - projection\n        \n        # Check if the resulting vector is linearly independent\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            orthogonal_vectors.append(u)\n    \n    # Normalize the orthogonal vectors to get orthonormal basis\n    orthonormal_vectors = []\n    for u in orthogonal_vectors:\n        unit_vector = u / np.linalg.norm(u)\n        # Round to 4 decimal places\n        unit_vector = np.round(unit_vector, 4)\n        orthonormal_vectors.append(unit_vector.tolist())\n    \n    return orthonormal_vectors\n```\n\nThis implementation:\n\n1. Converts the input vectors to numpy arrays for easier computation\n2. Applies the Gram-Schmidt process to create orthogonal vectors\n3. Checks for linear independence using the provided tolerance\n4. Normalizes the orthogonal vectors to create an orthonormal basis\n5. Rounds the results to 4 decimal places and converts them back to lists\n\nThe function will return a list of orthonormal vectors that span the same subspace as the input vectors, with each vector represented as a list of floats rounded to 4 decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "# Solution: Computing Cross Product of Two 3D Vectors\n\nTo compute the cross product of two 3D vectors, I'll implement a solution using NumPy's built-in cross product function and handle the required formatting.\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Convert inputs to numpy arrays to ensure compatibility\n    a_array = np.array(a)\n    b_array = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a_array, b_array)\n    \n    # Round to 4 decimal places and convert to list\n    return result.round(4).tolist()\n```\n\n## Explanation:\n\n1. First, I convert the input vectors to NumPy arrays to ensure we can use NumPy's functionality, regardless of how the vectors are provided (lists, tuples, etc.).\n\n2. I use NumPy's `np.cross()` function, which efficiently calculates the cross product of two vectors.\n\n3. Finally, I round the result to 4 decimal places using the `round()` method and convert it back to a Python list with `tolist()` as required.\n\nThis solution handles the cross product calculation in a clean and efficient way, leveraging NumPy's optimized functions while meeting all the output constraints."}
{"task_id": 119, "completion_id": 0, "completion": "# Solving Systems of Linear Equations with Cramer's Rule\n\nI'll implement a function that uses Cramer's Rule to solve a system of linear equations Ax = b. Cramer's Rule works by calculating ratios of determinants.\n\nHere's my implementation:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Args:\n        A: Square coefficient matrix\n        b: Constant vector\n        \n    Returns:\n        Solution vector as a list (rounded to 4 decimal places)\n        -1 if there's no unique solution (determinant of A is zero)\n    \"\"\"\n    # Convert inputs to numpy arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Ensure b is a 1D array\n    if b.ndim > 1:\n        b = b.flatten()\n    \n    # Calculate the determinant of the coefficient matrix\n    det_A = np.linalg.det(A)\n    \n    # Check if the system has a unique solution\n    if abs(det_A) < 1e-10:  # Using small threshold to handle numerical precision\n        return -1\n    \n    # Get the size of the matrix\n    n = A.shape[0]\n    \n    # Initialize the solution vector\n    x = np.zeros(n)\n    \n    # Apply Cramer's Rule for each unknown\n    for i in range(n):\n        # Create a copy of A and replace the i-th column with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the modified matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the i-th component of the solution\n        x[i] = det_A_i / det_A\n    \n    # Round the results to 4 decimal places\n    x = np.round(x, 4)\n    \n    return x.tolist()\n```\n\nThis implementation follows these steps:\n1. Calculate the determinant of A\n2. Check if det(A) is close to zero - if yes, return -1 (no unique solution)\n3. For each variable x_i:\n   - Create a new matrix by replacing the i-th column of A with b\n   - Calculate this new determinant\n   - Divide by the original determinant to find x_i\n4. Round all values to 4 decimal places\n5. Convert the numpy array to a regular Python list using tolist()"}
{"task_id": 120, "completion_id": 0, "completion": "# Bhattacharyya Distance Implementation\n\nI'll implement the function to calculate the Bhattacharyya distance between two probability distributions as per the requirements.\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if inputs are valid (non-empty and same length)\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = sum(np.sqrt(p_i * q_i) for p_i, q_i in zip(p, q))\n    \n    # Calculate the Bhattacharyya distance\n    # If bc is 0, the distributions are completely disjoint\n    if bc > 0:\n        distance = -np.log(bc)\n    else:\n        # Using a very large but finite value for infinity\n        return 9999.9999\n    \n    # Round to 4 decimal places\n    return round(distance, 4)\n```\n\nThis function:\n1. Validates that both input lists are non-empty and of the same length\n2. Calculates the Bhattacharyya coefficient by summing the square roots of the corresponding probability products\n3. Computes the distance as the negative natural logarithm of the coefficient\n4. Handles the edge case when the coefficient is 0 (completely disjoint distributions)\n5. Returns the result rounded to 4 decimal places as required\n\nThe implementation follows mathematical correctness while also handling edge cases according to the specified constraints."}
{"task_id": 121, "completion_id": 0, "completion": "# Vector Element-wise Sum Solution\n\nI'll implement a function that computes the element-wise sum of two vectors, checking for compatibility first.\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Compute the element-wise sum of two vectors.\n    \n    Args:\n        a: First vector (list of numbers)\n        b: Second vector (list of numbers)\n        \n    Returns:\n        A new vector representing the element-wise sum if vectors have the same length,\n        or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    # Check if vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute element-wise sum\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    \n    return result\n```\n\nThis function:\n1. First checks if the two input vectors have the same length\n2. If they don't, returns -1 as specified\n3. If they do have the same length, creates a new list with the element-wise sums\n4. Returns the resulting vector\n\nThe function handles both integer and floating-point values as specified in the type hints."}
{"task_id": 122, "completion_id": 0, "completion": "# Policy Gradient with REINFORCE Solution\n\nI'll implement the REINFORCE algorithm, which uses the policy gradient method to optimize a policy parameterized by theta. The key insight is that we compute the gradient of the log-policy multiplied by the return at each time step.\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    # Initialize gradient with the same shape as theta\n    gradient = np.zeros_like(theta)\n    num_episodes = len(episodes)\n    \n    for episode in episodes:\n        # Calculate returns for each timestep\n        returns = []\n        G = 0\n        for _, _, reward in reversed(episode):\n            G += reward\n            returns.insert(0, G)\n        \n        # For each timestep, compute gradient of log-policy * return\n        for (state, action, _), G in zip(episode, returns):\n            # Compute policy using softmax\n            logits = theta[state, :]\n            # For numerical stability, subtract max\n            shifted_logits = logits - np.max(logits)\n            exp_logits = np.exp(shifted_logits)\n            policy = exp_logits / np.sum(exp_logits)\n            \n            # Gradient of log-policy for all actions in this state\n            gradient[state, :] -= policy * G\n            # Add G for the selected action (completing the gradient formula)\n            gradient[state, action] += G\n    \n    # Average gradient across episodes and round to 4 decimals\n    gradient = np.round(gradient / num_episodes, 4)\n    \n    return gradient.tolist()\n```\n\nThis implementation:\n1. Computes the return G for each timestep by going backwards through the episode\n2. For each state-action pair:\n   - Calculates the softmax policy from theta\n   - Computes the gradient of log-policy (which is 1-policy[action] for the chosen action and -policy[a] for other actions)\n   - Multiplies by the return\n3. Averages across all episodes\n4. Rounds to 4 decimal places and returns as a Python list\n\nThe algorithm follows the REINFORCE update rule: \u03b8 \u2190 \u03b8 + \u03b1\u2207J(\u03b8), where \u2207J(\u03b8) is the policy gradient we're calculating."}
{"task_id": 123, "completion_id": 0, "completion": "# Understanding the Task\n\nI need to calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer. In an MoE architecture, only a subset of experts is activated for each input, which can lead to computational savings.\n\nLet's break down the necessary calculations:\n\n1. Calculate FLOPs for a standard dense layer\n2. Calculate FLOPs for an MoE layer with sparse activation\n3. Determine the percentage savings\n\n# Implementation\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate computational efficiency of MoE compared to dense layer.\n    \n    Args:\n        n_experts: Number of experts in the MoE layer\n        k_active: Number of active experts per forward pass\n        d_in: Input dimension\n        d_out: Output dimension\n        \n    Returns:\n        Dictionary with dense_flops, moe_flops, and savings_percentage\n    \"\"\"\n    # Calculate FLOPs for a dense layer (multiply-add operations)\n    dense_flops = 2 * d_in * d_out\n    \n    # Calculate FLOPs for MoE layer\n    # 1. Gating network FLOPs (to decide which experts to use)\n    gating_flops = 2 * d_in * n_experts\n    \n    # 2. FLOPs for the activated experts\n    expert_flops = k_active * 2 * d_in * d_out\n    \n    # 3. FLOPs for combining the outputs of activated experts\n    combination_flops = k_active * d_out\n    \n    # Total MoE FLOPs\n    moe_flops = gating_flops + expert_flops + combination_flops\n    \n    # Calculate computational savings percentage\n    savings_percentage = (dense_flops - moe_flops) / dense_flops * 100\n    \n    # Round to the nearest 0.1\n    savings_percentage = round(savings_percentage, 1)\n    \n    return {\n        \"dense_flops\": dense_flops,\n        \"moe_flops\": moe_flops,\n        \"savings_percentage\": savings_percentage\n    }\n```\n\n# Example Usage\n\nLet's test this with a few examples:\n\n```python\n# Example 1: 8 experts with 2 active experts\nprint(compute_efficiency(8, 2, 1024, 1024))\n\n# Example 2: 16 experts with 1 active expert\nprint(compute_efficiency(16, 1, 1024, 4096))\n\n# Example 3: 4 experts with 4 active experts (no sparsity)\nprint(compute_efficiency(4, 4, 512, 512))\n```\n\nThe key insight here is that computational efficiency largely depends on the ratio of k_active to n_experts. When k_active is much smaller than n_experts, we see significant computational savings. However, the gating network does add some overhead, which can reduce efficiency if d_in and d_out are relatively small compared to n_experts."}
{"task_id": 124, "completion_id": 0, "completion": "# Implementing Noisy Top-K Gating Function\n\nI'll implement the noisy top-k gating mechanism used in Mixture of Experts models. This function computes the gating probabilities after applying noise and keeping only the top-k values.\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Step 1: Calculate the gating logits\n    logits = np.dot(X, W_g)\n    \n    # Step 2: Apply noise\n    noise = np.dot(N, W_noise)\n    noisy_logits = logits + noise\n    \n    # Step 3: Apply Top-K constraint\n    # Initialize with -inf (will be masked out in softmax)\n    masked_logits = np.full_like(noisy_logits, -np.inf)\n    \n    # For each input example, find the indices of top k values\n    top_k_indices = np.argpartition(noisy_logits, -k, axis=1)[:, -k:]\n    \n    # Use fancy indexing to set only the top-k values\n    batch_indices = np.arange(noisy_logits.shape[0])[:, None]\n    masked_logits[batch_indices, top_k_indices] = noisy_logits[batch_indices, top_k_indices]\n    \n    # Step 4: Apply softmax to get probabilities\n    # Subtract max for numerical stability\n    max_logits = np.max(masked_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(masked_logits - max_logits)\n    probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Step 5: Round to 4 decimal places\n    rounded_probabilities = np.round(probabilities, 4)\n    \n    # Return as Python list\n    return rounded_probabilities.tolist()\n```\n\nThis implementation:\n1. Computes the raw logits by multiplying input `X` with weights `W_g`\n2. Adds noise by multiplying the pre-sampled noise `N` with noise weights `W_noise`\n3. For each input, keeps only the top-k logit values and sets the rest to negative infinity\n4. Applies softmax to convert the masked logits to probabilities\n5. Rounds results to 4 decimal places and returns them as a Python list\n\nThe implementation handles numerical stability in the softmax calculation by subtracting the maximum value before applying exp."}
{"task_id": 125, "completion_id": 0, "completion": "# Mixture of Experts Layer Implementation\n\nI'll implement a sparse MoE layer that routes each input token through its top-k experts, as specified in the task.\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute gating logits by applying gating weights to input\n    logits = np.matmul(x, Wg)  # [batch_size, n_experts]\n    \n    # Apply softmax to get routing probabilities\n    exp_logits = np.exp(logits)\n    routing_probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n    \n    # Get indices of top-k experts for each token\n    top_k_indices = np.argsort(routing_probs, axis=-1)[:, -top_k:]  # Highest probabilities last\n    \n    # Extract the corresponding probabilities for the top-k experts\n    top_k_probs = np.take_along_axis(routing_probs, top_k_indices, axis=1)  # [batch_size, top_k]\n    \n    # Renormalize the top-k probabilities to sum to 1\n    normalized_probs = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    \n    # Initialize output tensor\n    batch_size = x.shape[0]\n    output_dim = We.shape[2]\n    output = np.zeros((batch_size, output_dim))\n    \n    # Apply expert transformations and aggregate weighted results\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            # Apply the expert's transformation to the input token\n            expert_output = np.matmul(x[i], We[expert_idx])\n            # Weight the expert's output by its normalized probability\n            output[i] += normalized_probs[i, j] * expert_output\n    \n    # Round to 4 decimal places\n    output = np.round(output, 4)\n    \n    return output.tolist()\n```\n\nThis implementation:\n1. Calculates routing probabilities using softmax on the gating logits\n2. Selects the top-k experts for each token based on these probabilities\n3. Normalizes the selected probabilities to sum to 1\n4. Applies each selected expert's transformation to the input\n5. Combines the expert outputs using the normalized probabilities as weights\n6. Rounds to 4 decimal places and returns as a Python list"}
{"task_id": 126, "completion_id": 0, "completion": "# Group Normalization Implementation\n\nI'll implement the Group Normalization function, which divides channels into groups and normalizes the features within each group.\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Get dimensions\n    batch_size, num_channels, height, width = X.shape\n    \n    # Ensure num_channels is divisible by num_groups\n    assert num_channels % num_groups == 0, \"Number of channels must be divisible by number of groups\"\n    \n    # Channels per group\n    channels_per_group = num_channels // num_groups\n    \n    # Reshape input to separate out groups\n    # New shape: (batch_size, num_groups, channels_per_group, height, width)\n    X_reshaped = X.reshape(batch_size, num_groups, channels_per_group, height, width)\n    \n    # Compute mean and variance for each group\n    # Computing across dimensions (2,3,4) = (channels_per_group, height, width)\n    mean = np.mean(X_reshaped, axis=(2,3,4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2,3,4), keepdims=True)\n    \n    # Normalize\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original format\n    X_normalized = X_normalized.reshape(batch_size, num_channels, height, width)\n    \n    # Apply scale and shift parameters\n    gamma_reshaped = gamma.reshape(1, num_channels, 1, 1)\n    beta_reshaped = beta.reshape(1, num_channels, 1, 1)\n    out = gamma_reshaped * X_normalized + beta_reshaped\n    \n    # Round to 4 decimal places as required\n    out = np.around(out, decimals=4)\n    \n    return out.tolist()\n```\n\nThis implementation:\n1. Reshapes the input tensor to group channels\n2. Calculates mean and variance within each group\n3. Normalizes the values using these statistics\n4. Applies the learnable scale (gamma) and shift (beta) parameters\n5. Rounds to 4 decimal places and returns the result as a Python list"}
{"task_id": 127, "completion_id": 0, "completion": "# Captain Redbeard's Treasure Finder\n\nLet me implement a gradient descent algorithm to find the minimum point of the function f(x) = x^4 - 3x^3 + 2. This is where Captain Redbeard's treasure is hidden!\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Find the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    \n    Args:\n        start_x: Initial x value to start the search\n        learning_rate: Step size for gradient descent\n        tolerance: Stop when the gradient's magnitude is below this value\n        max_iters: Maximum number of iterations to perform\n        \n    Returns:\n        The x-value corresponding to the minimum of f(x), rounded to 4 decimal places\n    \"\"\"\n    x = start_x\n    \n    for i in range(max_iters):\n        # Calculate the gradient (derivative) of f(x) = x^4 - 3x^3 + 2\n        # f'(x) = 4x^3 - 9x^2\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Check if we've converged\n        if abs(gradient) < tolerance:\n            break\n            \n        # Update x by moving in the opposite direction of the gradient\n        x = x - learning_rate * gradient\n    \n    # Captain Redbeard wants precise coordinates, round to 4 decimal places\n    return round(x, 4)\n```\n\n## How It Works\n\n1. The function starts at a given position `start_x`\n2. It calculates the gradient (derivative) of our function at the current x value\n3. It moves downhill by taking a step in the opposite direction of the gradient\n4. It repeats until it finds the minimum (where the gradient is nearly zero)\n5. The result is rounded to 4 decimal places\n\nThe algorithm will find the global minimum at approximately x = 2.25, which is where Captain Redbeard's treasure is buried! This is the lowest point of the curve f(x) = x^4 - 3x^3 + 2."}
{"task_id": 128, "completion_id": 0, "completion": "# Dynamic Tanh Implementation\n\nI'll implement the Dynamic Tanh (DyT) function as a normalization-free alternative to layer normalization in Transformer architectures.\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function as a normalization-free transformation.\n    \n    Args:\n        x: Input numpy array\n        alpha: Controls the steepness of the tanh function\n        gamma: Scales the output\n        beta: Bias term\n        \n    Returns:\n        List of transformed values rounded to 4 decimal places\n    \"\"\"\n    # Apply the dynamic tanh transformation: gamma * tanh(alpha * x) + beta\n    result = gamma * np.tanh(alpha * x) + beta\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Convert to Python list\n    return result.tolist()\n```\n\nThis implementation:\n\n1. Takes the input array `x` and applies the transformation with parameters `alpha`, `gamma`, and `beta`\n2. The transformation preserves the squashing behavior of tanh while allowing for learnable scaling and shifting\n3. Rounds all values to 4 decimal places as required\n4. Returns the result as a Python list using NumPy's tolist() method\n\nThe DyT function allows Transformers to retain the benefits of normalization through its parameterized form without explicitly computing means and variances, making it a \"normalization-free\" alternative."}
