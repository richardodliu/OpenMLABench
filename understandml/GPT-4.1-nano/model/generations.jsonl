{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Performs Ridge (L2-regularized) linear regression using the closed-form solution.\n    \n    Parameters:\n    - data: numpy.ndarray\n        The dataset where the last column is the target variable y, and all preceding columns are features.\n    - alpha: float\n        The regularization strength (non-negative).\n        \n    Returns:\n    - list of floats: The weight vector (including bias term), rounded to 4 decimal places.\n                       Returns -1 if the matrix to invert is singular.\n    \"\"\"\n    # Separate features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias (intercept) term as a column of ones\n    X_b = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Compute X^T X\n    XtX = X_b.T @ X_b\n    \n    # Add regularization term alpha * I\n    reg_matrix = alpha * np.eye(XtX.shape[0])\n    matrix_to_invert = XtX + reg_matrix\n    \n    # Check if the matrix is invertible\n    det = np.linalg.det(matrix_to_invert)\n    if np.isclose(det, 0):\n        return -1\n    \n    # Compute the inverse\n    try:\n        inv_matrix = np.linalg.inv(matrix_to_invert)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Compute weights\n    w = inv_matrix @ (X_b.T @ y)\n    \n    # Round to 4 decimal places\n    w_rounded = np.round(w, 4)\n    \n    # Convert to list and return\n    return w_rounded.tolist()\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or -1) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    updates = 0\n\n    # Precompute the Gram matrix for efficiency\n    K = X_train @ X_train.T  # shape: (n_samples, n_samples)\n\n    while updates < n_iter:\n        error_found = False\n        for i in range(n_samples):\n            # Compute activation: sum_j alpha_j * y_j * K(x_j, x_i)\n            activation = np.sum(alpha * y_train * K[:, i])\n            margin = y_train[i] * (activation + b)\n            if margin <= 0:\n                # Misclassified, update alpha and b\n                alpha[i] += eta\n                b += eta * y_train[i]\n                updates += 1\n                error_found = True\n                break  # Restart scan from the beginning\n        if not error_found:\n            # No misclassification in this pass, training converged\n            break\n\n    # Compute weight vector w from alpha\n    w = np.sum((alpha * y_train)[:, np.newaxis] * X_train, axis=0)\n\n    # Round w and b to 4 decimals\n    w_rounded = [round(coord, 4) for coord in w]\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n\n    # Identity link: g(\u03bc) = \u03bc\n    identity = {\n        \"link\": lambda mu: mu,\n        \"inv_link\": lambda eta: eta,\n        \"deriv\": lambda mu: np.ones_like(mu)\n    }\n\n    # Log link: g(\u03bc) = log(\u03bc)\n    # Note: \u03bc > 0\n    log_link = {\n        \"link\": lambda mu: np.log(mu),\n        \"inv_link\": lambda eta: np.exp(eta),\n        \"deriv\": lambda mu: 1.0 / mu\n    }\n\n    # Logit link: g(\u03bc) = log(\u03bc / (1 - \u03bc))\n    # Note: 0 < \u03bc < 1\n    logit_link = {\n        \"link\": lambda mu: np.log(mu / (1 - mu)),\n        \"inv_link\": lambda eta: 1 / (1 + np.exp(-eta)),\n        \"deriv\": lambda mu: 1.0 / (mu * (1 - mu))\n    }\n\n    return {\n        \"identity\": identity,\n        \"log\": log_link,\n        \"logit\": logit_link\n    }\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(0)\n\n    n_samples, n_features = data.shape\n\n    # Initialize a list to store the depth for each sample\n    depths = [0] * n_samples\n\n    # Each node will be represented as a dictionary with keys:\n    # 'indices': indices of samples reaching this node\n    # 'depth': depth of this node\n    # 'left': left child node (or None)\n    # 'right': right child node (or None)\n    # 'is_leaf': boolean indicating if node is a leaf\n\n    def build_tree(indices, current_depth):\n        # If fewer than 3 samples, create a leaf node\n        if len(indices) < 3:\n            # Assign depth to all samples in this leaf\n            for idx in indices:\n                depths[idx] = current_depth\n            return\n\n        # Choose a feature at random\n        f = np.random.randint(0, n_features)\n\n        # Get the feature values for current samples\n        feature_values = data[indices, f]\n        down, up = feature_values.min(), feature_values.max()\n\n        # If all feature values are the same, create a leaf\n        if down == up:\n            for idx in indices:\n                depths[idx] = current_depth\n            return\n\n        # Choose a split value uniformly between down and up\n        v = np.random.uniform(down, up)\n\n        # Partition the indices into left and right\n        left_indices = [idx for idx in indices if data[idx, f] <= v]\n        right_indices = [idx for idx in indices if data[idx, f] > v]\n\n        # Recursively build left and right subtrees\n        build_tree(left_indices, current_depth + 1)\n        build_tree(right_indices, current_depth + 1)\n\n    # Start building the tree from all samples at depth 0\n    build_tree(list(range(n_samples)), 0)\n\n    return depths\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Total number of transactions\n    N = len(transactions)\n    # Minimum support count\n    min_support = math.ceil(minsup * N)\n\n    # Convert each transaction to a set for faster subset checks\n    transaction_sets = [set(t) for t in transactions]\n\n    # Dictionary to hold all frequent itemsets with their support counts\n    frequent_itemsets = dict()\n\n    # Step 1: Find frequent 1-itemsets\n    item_counter = Counter()\n    for t in transaction_sets:\n        for item in t:\n            item_counter[item] += 1\n\n    # Filter 1-itemsets by support threshold\n    current_frequent_itemsets = {\n        (item,): count for item, count in item_counter.items() if count >= min_support\n    }\n    # Add to the global result\n    frequent_itemsets.update(current_frequent_itemsets)\n\n    # Initialize variables for iterative process\n    k = 2\n    prev_frequent_itemsets = list(current_frequent_itemsets.keys())\n\n    # Loop to find larger itemsets\n    while prev_frequent_itemsets:\n        candidate_counts = Counter()\n\n        # Generate candidate itemsets of size k\n        # To do this efficiently, we can generate candidates by joining previous frequent itemsets\n        # with each other, ensuring lex order and avoiding duplicates\n        candidates = set()\n        for i in range(len(prev_frequent_itemsets)):\n            for j in range(i + 1, len(prev_frequent_itemsets)):\n                # Join only if first k-2 items are the same (to avoid duplicates)\n                itemset1 = prev_frequent_itemsets[i]\n                itemset2 = prev_frequent_itemsets[j]\n                if itemset1[:-1] == itemset2[:-1]:\n                    candidate = tuple(sorted(set(itemset1) | set(itemset2)))\n                    if len(candidate) == k:\n                        candidates.add(candidate)\n\n        # Count support for each candidate\n        for candidate in candidates:\n            # For each transaction, check if candidate is subset\n            count = 0\n            for t in transaction_sets:\n                if set(candidate).issubset(t):\n                    count += 1\n            if count >= min_support:\n                candidate_counts[candidate] = count\n\n        # Update the list of frequent itemsets\n        current_frequent_itemsets = {\n            itemset: count for itemset, count in candidate_counts.items()\n        }\n\n        # Add new frequent itemsets to the global result\n        frequent_itemsets.update(current_frequent_itemsets)\n\n        # Prepare for next iteration\n        prev_frequent_itemsets = list(current_frequent_itemsets.keys())\n        k += 1\n\n    return frequent_itemsets\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import list\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n\n    n_samples, n_features = data.shape\n\n    # Initialize centres with the first k samples\n    centres = data[:k].copy()\n\n    # Initialize labels for each sample\n    labels = np.zeros(n_samples, dtype=int)\n\n    for iteration in range(max_iter):\n        # Step 1: Assign each sample to the nearest centre\n        for i in range(n_samples):\n            # Compute squared Euclidean distances to each centre\n            distances = np.sum((centres - data[i])**2, axis=1)\n            labels[i] = np.argmin(distances)\n\n        new_centres = np.zeros_like(centres)\n        centres_movement = np.zeros(k)\n\n        # Step 2: Recompute centres\n        for cluster_idx in range(k):\n            # Get all samples assigned to this cluster\n            cluster_points = data[labels == cluster_idx]\n            if len(cluster_points) == 0:\n                # Empty cluster: reinitialize with a random sample\n                rand_idx = np.random.randint(0, n_samples)\n                new_centres[cluster_idx] = data[rand_idx]\n            else:\n                # Compute mean of assigned points\n                new_centres[cluster_idx] = np.mean(cluster_points, axis=0)\n\n        # Step 3: Check for convergence\n        centre_shifts = np.linalg.norm(centres - new_centres, axis=1)\n        max_shift = np.max(centre_shifts)\n\n        # Update centres\n        centres = new_centres\n\n        if max_shift < epsilon:\n            break\n\n    # Round centres to 4 decimal places\n    centres = np.round(centres, 4).tolist()\n\n    # Convert labels to list of ints\n    labels = labels.tolist()\n\n    return centres, labels\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_users, n_items = data.shape\n\n    # 1. Build item\u2013item similarity matrix S of shape (n_items, n_items)\n    S = np.zeros((n_items, n_items))\n    \n    # For each pair of items (i, j), compute similarity\n    for i in range(n_items):\n        for j in range(i, n_items):\n            # Find users who rated both items\n            users_rated_i = data[:, i] > 0\n            users_rated_j = data[:, j] > 0\n            common_users = users_rated_i & users_rated_j\n            common_indices = np.where(common_users)[0]\n            \n            if len(common_indices) == 0:\n                # No common raters\n                S[i, j] = 0\n                S[j, i] = 0\n            else:\n                v1 = data[common_indices, i]\n                v2 = data[common_indices, j]\n                \n                if criterion == 'cosine':\n                    # Mean-centering only if std > 1e-3\n                    def mean_center(vec):\n                        std = np.std(vec)\n                        if std > 1e-3:\n                            return vec - np.mean(vec)\n                        else:\n                            return vec\n                    v1_centered = mean_center(v1)\n                    v2_centered = mean_center(v2)\n                    \n                    numerator = np.dot(v1_centered, v2_centered)\n                    denom = np.linalg.norm(v1_centered) * np.linalg.norm(v2_centered)\n                    if denom > 1e-12:\n                        sim = numerator / denom\n                    else:\n                        sim = 0.0\n                elif criterion == 'pearson':\n                    # Use np.corrcoef\n                    if len(common_indices) > 1:\n                        corr = np.corrcoef(v1, v2)[0,1]\n                        # np.corrcoef can produce nan if variance is zero\n                        if np.isnan(corr):\n                            sim = 0.0\n                        else:\n                            sim = corr\n                    else:\n                        # With only one common user, correlation is undefined\n                        sim = 0.0\n                else:\n                    raise ValueError(\"Invalid criterion. Must be 'cosine' or 'pearson'.\")\n                \n                S[i, j] = sim\n                S[j, i] = sim\n\n    # 2. Collect items rated by the active user\n    user_ratings = data[user_ind]\n    rated_items = np.where(user_ratings > 0)[0]\n    unrated_items = np.where(user_ratings == 0)[0]\n\n    # If user has rated all items, return empty list\n    if len(rated_items) == n_items:\n        return []\n\n    # 3. For each unrated item, compute predicted attractiveness\n    scores = {}\n    for t in unrated_items:\n        # Similarities between item t and rated items\n        sim_t = S[t, rated_items]\n        ratings_rated = user_ratings[rated_items]\n        \n        numerator = np.sum(ratings_rated * sim_t)\n        denominator = np.sum(np.abs(sim_t))\n        if denominator == 0:\n            score = 0.0\n        else:\n            score = numerator / denominator\n        scores[t] = score\n\n    # 4. Sort items by decreasing score, break ties by smaller index\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n\n    # Return the top k items\n    recommended_items = [item for item, score in sorted_items[:k]]\n\n    return recommended_items\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    n_samples, n_features = X.shape\n    total_g = np.sum(g)\n    total_h = np.sum(h)\n\n    # Compute the total leaf score (without split)\n    def leaf_score(G, H):\n        return -0.5 * (G ** 2) / (H + lam) + gamma\n\n    total_score = leaf_score(total_g, total_h)\n\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        # Get unique sorted thresholds\n        thresholds = np.unique(feature_values)\n\n        # To consider splits between unique values, we can consider midpoints\n        # between consecutive unique values\n        if len(thresholds) == 1:\n            # No split possible on a feature with only one unique value\n            continue\n\n        # Generate candidate thresholds as midpoints between consecutive unique values\n        candidate_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n\n        for threshold in candidate_thresholds:\n            # Create boolean mask for left and right splits\n            left_mask = feature_values <= threshold\n            right_mask = ~left_mask\n\n            # Check if both sides have at least 2 instances\n            if np.sum(left_mask) < 2 or np.sum(right_mask) < 2:\n                continue\n\n            # Compute sums for left and right\n            g_left = np.sum(g[left_mask])\n            h_left = np.sum(h[left_mask])\n            g_right = np.sum(g[right_mask])\n            h_right = np.sum(h[right_mask])\n\n            # Compute leaf scores\n            score_left = leaf_score(g_left, h_left)\n            score_right = leaf_score(g_right, h_right)\n\n            # Compute gain\n            gain = total_score - score_left - score_right\n\n            # Update best split if gain is better\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature_idx\n                best_threshold = threshold\n\n    # Return the best split if gain is positive\n    if best_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Convert input to numpy array for uniform processing\n    if isinstance(x, (list, np.ndarray)):\n        arr = np.array(x)\n        is_scalar_input = False\n    elif isinstance(x, (int, float)):\n        arr = np.array([x])\n        is_scalar_input = True\n    else:\n        raise TypeError(\"Input must be a scalar, list, or numpy.ndarray.\")\n\n    # Compute sigmoid\n    sigmoid = 1 / (1 + np.exp(-arr))\n    # Compute gradient: sigmoid * (1 - sigmoid)\n    gradient = sigmoid * (1 - sigmoid)\n\n    # Round results to 4 decimal places\n    sigmoid_rounded = np.round(sigmoid, 4)\n    gradient_rounded = np.round(gradient, 4)\n\n    if is_scalar_input:\n        # For scalar input, return float\n        return float(sigmoid_rounded[0]), float(gradient_rounded[0])\n    else:\n        # For array/list input, convert to list\n        return sigmoid_rounded.tolist(), gradient_rounded.tolist()\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Convert inputs to numpy arrays for convenience\n    X = np.array(x_train)\n    Y = np.array(y_train)\n    n_samples = len(X)\n\n    # Initialize sample weights uniformly\n    weights = np.ones(n_samples) / n_samples\n\n    # Store the ensemble classifiers: list of (d, theta, alpha)\n    classifiers = []\n\n    # Function to compute the ensemble prediction\n    def ensemble_predict(x):\n        total = 0.0\n        for d, theta, alpha in classifiers:\n            if d == 0:\n                pred = 1 if x <= theta else -1\n            else:\n                pred = 1 if x > theta else -1\n            total += alpha * pred\n        return 1 if total >= 0 else -1\n\n    # Function to compute training error\n    def compute_training_error():\n        preds = np.array([ensemble_predict(x) for x in X])\n        return np.mean(preds != Y)\n\n    # Main AdaBoost loop\n    while True:\n        # Sort data by x to consider thresholds between points\n        sorted_indices = np.argsort(X)\n        X_sorted = X[sorted_indices]\n        Y_sorted = Y[sorted_indices]\n        weights_sorted = weights[sorted_indices]\n\n        # Generate candidate thresholds: midpoints between consecutive points\n        thresholds = []\n        for i in range(n_samples - 1):\n            thresholds.append((X_sorted[i] + X_sorted[i+1]) / 2.0)\n        # Also consider thresholds outside the data range\n        min_x, max_x = X_sorted[0], X_sorted[-1]\n        thresholds = [min_x - 1] + thresholds + [max_x + 1]\n\n        best_error = float('inf')\n        best_stump = None\n\n        # For each threshold, evaluate both d=0 and d=1\n        for theta in thresholds:\n            for d in [0, 1]:\n                # Predict for all training points\n                preds = np.empty(n_samples, dtype=int)\n                if d == 0:\n                    preds = np.where(X_sorted <= theta, 1, -1)\n                else:\n                    preds = np.where(X_sorted > theta, 1, -1)\n\n                # Compute weighted error\n                err = np.sum(weights_sorted * (preds != Y_sorted))\n                if err < best_error:\n                    best_error = err\n                    best_stump = (d, theta)\n\n        # Avoid division by zero in alpha calculation\n        err = best_error\n        # To prevent numerical issues, clip error\n        err = min(max(err, 1e-10), 1 - 1e-10)\n\n        # Compute alpha\n        alpha = 0.5 * math.log((1 - err) / err)\n\n        # Add the selected stump to the ensemble\n        classifiers.append((best_stump[0], best_stump[1], alpha))\n\n        # Update weights\n        for i in range(n_samples):\n            d, theta = best_stump\n            x_i = X[i]\n            y_i = Y[i]\n            if d == 0:\n                pred = 1 if x_i <= theta else -1\n            else:\n                pred = 1 if x_i > theta else -1\n            # Update weight\n            weights[i] *= math.exp(-alpha * y_i * pred)\n\n        # Normalize weights\n        weights /= np.sum(weights)\n\n        # Compute training error of current ensemble\n        train_preds = np.array([ensemble_predict(x) for x in X])\n        train_error = np.mean(train_preds != Y)\n\n        # Check stopping condition\n        if train_error <= epsilon:\n            break\n\n        # Optional: stop if error is zero to prevent infinite loop\n        if train_error == 0:\n            break\n\n    # Final predictions on test set\n    predictions = [ensemble_predict(x) for x in x_test]\n    return predictions\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Compute the squared Euclidean distances between each training sample and each test sample\n    # Using broadcasting for efficient computation\n    # X_train shape: (n, d)\n    # X_test shape: (m, d)\n    # Resulting distance matrix shape: (n, m)\n    diff = X_train[:, np.newaxis, :] - X_test[np.newaxis, :, :]  # shape: (n, m, d)\n    sq_distances = np.sum(diff ** 2, axis=2)  # shape: (n, m)\n\n    # Compute the kernel matrix between training samples and test samples\n    # K_{i,j} = exp(-gamma * ||x_i - z_j||^2)\n    K = np.exp(-gamma * sq_distances)  # shape: (n, m)\n\n    # Compute the decision function g(z) for each test sample\n    # g(z_j) = sum_i alpha_i * y_i * K_{i,j} + b\n    # We can vectorize this as:\n    # (alpha * y_train) shape: (n,)\n    # Multiply with K: broadcasting over columns\n    decision_values = np.dot((alpha * y_train), K) + b  # shape: (m,)\n\n    # Predict labels based on the sign of decision values\n    predictions = np.sign(decision_values)\n\n    # Convert predictions from float to int (ensure 1 or -1)\n    # sign(0) returns 0, but in SVM, decision boundary points can be assigned to 1 or -1\n    # Here, we can treat 0 as 1 (or -1), but typically, sign(0) returns 0.\n    # To be safe, assign 1 to zero decision values.\n    predictions = np.where(predictions == 0, 1, predictions).astype(int)\n\n    return predictions.tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Convert input list to numpy array\n    X_np = np.array(X, dtype=float)\n    m, n = X_np.shape\n\n    # Validate k\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # Compute the SVD\n    U, S, VT = np.linalg.svd(X_np, full_matrices=False)\n\n    # Keep only the top k singular values/vectors\n    S_k = np.diag(S[:k])\n    U_k = U[:, :k]\n    VT_k = VT[:k, :]\n\n    # Reconstruct the matrix using truncated SVD\n    X_hat_np = U_k @ S_k @ VT_k\n\n    # Calculate the mean squared error\n    mse = np.mean((X_np - X_hat_np) ** 2)\n\n    # Round the reconstructed matrix and mse to 4 decimal places\n    X_hat_rounded = np.round(X_hat_np, 4).tolist()\n    mse_rounded = round(mse, 4)\n\n    return (X_hat_rounded, mse_rounded)\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n\n    # Convert inputs to numpy arrays for convenience\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n\n    # Sort training data by feature values\n    sorted_indices = np.argsort(x_train)\n    x_sorted = x_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n\n    # Initialize residuals as the original targets\n    residuals = y_sorted.copy()\n\n    # List to store the learned stumps: each as (split_point, c_left, c_right)\n    stumps = []\n\n    # Compute initial residual sum of squares\n    rss = np.sum(residuals ** 2)\n\n    # Generate candidate split points: midpoints between consecutive x values\n    candidate_splits = (x_sorted[:-1] + x_sorted[1:]) / 2\n\n    # Iterative boosting process\n    while rss > epsilon:\n        best_rss = None\n        best_stump = None\n\n        # For each candidate split, compute the residuals if we add that stump\n        for s in candidate_splits:\n            # Find indices where x <= s and x > s\n            left_mask = x_sorted <= s\n            right_mask = x_sorted > s\n\n            # Handle case where no points on one side\n            if np.any(left_mask):\n                c1 = np.mean(residuals[left_mask])\n            else:\n                c1 = 0.0\n            if np.any(right_mask):\n                c2 = np.mean(residuals[right_mask])\n            else:\n                c2 = 0.0\n\n            # Compute new residuals after adding this stump\n            residuals_new = residuals.copy()\n            residuals_new[left_mask] = residuals[left_mask] - c1\n            residuals_new[right_mask] = residuals[right_mask] - c2\n\n            # Compute RSS after adding this stump\n            rss_candidate = np.sum(residuals_new ** 2)\n\n            # Check if this is the best so far\n            if (best_rss is None) or (rss_candidate < best_rss):\n                best_rss = rss_candidate\n                best_stump = (s, c1, c2)\n\n        # If the best RSS is not better than current, break\n        if best_rss >= rss:\n            break\n\n        # Add the best stump to the ensemble\n        s, c_left, c_right = best_stump\n        stumps.append((s, c_left, c_right))\n\n        # Update residuals\n        left_mask = x_sorted <= s\n        right_mask = x_sorted > s\n        residuals[left_mask] -= c_left\n        residuals[right_mask] -= c_right\n\n        # Update current RSS\n        rss = np.sum(residuals ** 2)\n\n    # Make prediction for x_query\n    prediction = 0.0\n    for (s, c_left, c_right) in stumps:\n        if x_query <= s:\n            prediction += c_left\n        else:\n            prediction += c_right\n\n    return round(prediction, 4)\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"\n    Generate a NumPy tensor of the specified shape filled with 0.0s and 1.0s.\n    \n    Each element is 1.0 with probability equal to 'sparsity' and 0.0 otherwise.\n    If 'seed' is provided, the random number generator is seeded for reproducibility.\n    \n    Parameters:\n    - shape (tuple of ints): The shape of the output tensor.\n    - sparsity (float): Probability of each element being 1.0. Must be in [0, 1].\n    - seed (int or None): Optional seed for reproducibility.\n    \n    Returns:\n    - numpy.ndarray: Array of shape 'shape' with dtype float, containing only 0.0 and 1.0.\n    - Returns -1 if 'sparsity' is outside [0, 1].\n    \"\"\"\n    # Validate sparsity\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n    \n    # Seed the random number generator if seed is provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate uniform random numbers in [0, 1)\n    rand_vals = np.random.rand(*shape)\n    \n    # Threshold at 1 - sparsity: values >= 1 - sparsity become 1.0\n    threshold = 1 - sparsity\n    binary_array = (rand_vals >= threshold).astype(float)\n    \n    return binary_array\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    n_samples = data.shape[0]\n    # Find the index of the median element in the column d\n    median_idx = n_samples // 2\n\n    # Use argpartition to find the index of the median value\n    partition_indices = np.argpartition(data[:, d], median_idx)\n    median_row_idx = partition_indices[median_idx]\n\n    # Get the median value\n    median_value = data[median_row_idx, d]\n\n    # Find all indices with values less than the median\n    left_mask = data[:, d] < median_value\n    left_indices = np.where(left_mask)[0]\n    # Find all indices with values greater than the median\n    right_mask = data[:, d] > median_value\n    right_indices = np.where(right_mask)[0]\n\n    # The median index itself\n    pivot = median_row_idx\n\n    # Return sorted lists for deterministic output\n    return pivot, sorted(left_indices.tolist()), sorted(right_indices.tolist())\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Preconditions\n    assert isinstance(x, np.ndarray), \"Input x must be a numpy array.\"\n    assert x.ndim == 1, \"Input x must be one-dimensional.\"\n    assert stride >= 1, \"Stride must be at least 1.\"\n    assert len(x) >= frame_width, \"Input length must be at least as large as frame_width.\"\n\n    N = len(x)\n    n_frames = (N - frame_width) // stride + 1\n\n    # Calculate the new shape\n    shape = (n_frames, frame_width)\n\n    # Calculate the new strides\n    # The stride of the original array in bytes\n    stride_bytes = x.strides[0]\n    # The first dimension (number of frames) advances by stride_bytes\n    # The second dimension (within each frame) advances by stride_bytes\n    strides = (stride * stride_bytes, stride_bytes)\n\n    # Create the view using as_strided\n    frames = as_strided(x, shape=shape, strides=strides)\n\n    # Set writability\n    frames.flags.writeable = writeable\n\n    return frames\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Convert input to numpy array for easier processing\n    X_array = np.array(X)\n\n    # Check if the matrix is two-dimensional\n    if X_array.ndim != 2:\n        return False\n\n    # Check if all elements are within [0, 1]\n    if not np.all((X_array >= 0) & (X_array <= 1)):\n        return False\n\n    # Check if each row sums to 1 within a tolerance\n    row_sums = X_array.sum(axis=1)\n    if not np.allclose(row_sums, np.ones_like(row_sums), atol=1e-8):\n        return False\n\n    return True\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    s = np.zeros(n_features)  # Initialize running average of squared gradients\n\n    # Determine number of batches\n    num_batches = int(np.ceil(n_samples / batch_size))\n    # For cycling through data\n    indices = np.arange(n_samples)\n\n    for iteration in range(max_iter):\n        # Shuffle data at each iteration for stochasticity (optional)\n        # But since not specified, we can process in order or shuffle\n        # Here, process in order for simplicity\n        for batch_idx in range(num_batches):\n            start_idx = (batch_idx * batch_size) % n_samples\n            end_idx = start_idx + batch_size\n            if end_idx > n_samples:\n                # Wrap around\n                batch_indices = np.concatenate((indices[start_idx:], indices[:end_idx - n_samples]))\n            else:\n                batch_indices = indices[start_idx:end_idx]\n\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n\n            # Compute prediction\n            pred = X_batch @ w\n            # Compute gradient g = (1/n) * X^T (Xw - y)\n            residual = pred - y_batch\n            g = (X_batch.T @ residual) / len(y_batch)\n\n            # Update running average of squared gradients\n            s = rho * s + (1 - rho) * (g ** 2)\n\n            # Update weights\n            w -= eta * g / (np.sqrt(s) + eps_station)\n\n        # Check convergence: norm of gradient\n        # For convergence, compute gradient over full data\n        full_pred = X @ w\n        full_residual = full_pred - y\n        full_g = (X.T @ full_residual) / n_samples\n        grad_norm = np.linalg.norm(full_g)\n\n        if grad_norm < epsilon:\n            break\n\n    # Round weights to 4 decimal places and convert to list\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Compute the SoftPlus activation function or its gradient with numerical stability.\n    \n    Parameters:\n    x : scalar, list, or np.ndarray\n        Input value(s).\n    deriv : bool, default=False\n        If False, compute SoftPlus(x).\n        If True, compute the gradient (sigmoid(x)).\n        \n    Returns:\n    list of floats or float\n        Rounded results to 4 decimal places.\n        Single float if input is scalar.\n    \"\"\"\n    # Convert input to numpy array for vectorized operations\n    x_arr = np.array(x, dtype=np.float64)\n    \n    # Initialize result array\n    result = np.empty_like(x_arr)\n    \n    # For large positive x, SoftPlus ~ x\n    mask_pos = x_arr > 20\n    # For large negative x, SoftPlus ~ exp(x)\n    mask_neg = x_arr < -20\n    # For intermediate x, compute normally\n    mask_mid = ~(mask_pos | mask_neg)\n    \n    # Compute for large positive x\n    result[mask_pos] = x_arr[mask_pos]\n    # Compute for large negative x\n    result[mask_neg] = np.exp(x_arr[mask_neg])\n    # Compute for intermediate x using a stable formulation\n    result[mask_mid] = np.log1p(np.exp(x_arr[mask_mid]))\n    \n    if deriv:\n        # Gradient is sigmoid(x)\n        grad = 1 / (1 + np.exp(-x_arr))\n        # Round to 4 decimal places\n        rounded = np.round(grad, 4)\n    else:\n        # Round to 4 decimal places\n        rounded = np.round(result, 4)\n    \n    # Return as list\n    if np.isscalar(x):\n        return float(rounded)\n    else:\n        return rounded.tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n    \"\"\"\n    # Handle edge case: if sequence length < N, no N-grams can be formed\n    if len(sequence) < N:\n        return float('-inf')\n    \n    # Build N-gram counts\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    \n    # Generate N-grams and their counts\n    for i in range(len(corpus) - N + 1):\n        ngram = tuple(corpus[i:i+N])\n        ngram_counts[ngram] += 1\n    \n    # For N > 1, also count (N-1)-grams (prefixes)\n    if N > 1:\n        for i in range(len(corpus) - N + 1):\n            prefix = tuple(corpus[i:i+N-1])\n            prefix_counts[prefix] += 1\n    else:\n        # For unigram, prefix counts are just total tokens\n        total_tokens = len(corpus)\n        prefix_counts[()] = total_tokens  # empty tuple as key\n    \n    total_log_prob = 0.0\n    \n    # Iterate over the sequence to compute the total log probability\n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i:i+N])\n        if N == 1:\n            # Unigram probability: count(w) / total tokens\n            count_ngram = ngram_counts.get(ngram, 0)\n            total_tokens = len(corpus)\n            if count_ngram == 0:\n                return float('-inf')\n            prob = count_ngram / total_tokens\n        else:\n            # N-gram probability: count(w1..wN) / count(w1..wN-1)\n            count_ngram = ngram_counts.get(ngram, 0)\n            prefix = ngram[:-1]\n            count_prefix = prefix_counts.get(prefix, 0)\n            if count_ngram == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n        \n        # Add log probability\n        total_log_prob += np.log(prob)\n    \n    return round(total_log_prob, 4)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n\n    # Convert inputs to numpy arrays\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n\n    m = len(X)\n\n    # Build the design matrix with polynomial features: column of ones + x^1 ... x^degree\n    X_poly = np.vstack([X**j for j in range(degree + 1)]).T  # shape: (m, degree+1)\n\n    # Initialize weights to zeros\n    w = np.zeros(degree + 1)\n\n    # Precompute x_j^T x_j for j >= 1 for efficiency\n    x_j_squared = np.sum(X_poly**2, axis=0)\n\n    for iteration in range(n_iterations):\n        w_old = w.copy()\n\n        # Update bias term w0 exactly\n        residual = y - X_poly[:, 1:] @ w[1:]\n        w0_new = np.mean(residual)\n        delta_w0 = abs(w[0] - w0_new)\n        w[0] = w0_new\n\n        # Update other coefficients\n        for j in range(1, degree + 1):\n            # Compute residual excluding the j-th feature\n            residual = y - (X_poly @ w) + w[j] * X_poly[:, j]\n            rho = X_poly[:, j].T @ residual\n\n            # Soft thresholding\n            if rho > reg_factor:\n                w_j_new = (rho - reg_factor) / x_j_squared[j]\n            elif rho < -reg_factor:\n                w_j_new = (rho + reg_factor) / x_j_squared[j]\n            else:\n                w_j_new = 0.0\n\n            delta_wj = abs(w[j] - w_j_new)\n            w[j] = w_j_new\n\n        # Check for convergence\n        max_delta = max(abs(w - w_old))\n        if max_delta < tol:\n            break\n\n    # Round coefficients to 4 decimal places\n    w_rounded = [round(weight, 4) for weight in w]\n    return w_rounded\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Compute the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n\n    Parameters:\n    A (list[list[float]]): Transition probability matrix of shape (N, N).\n    B (list[list[float]]): Emission probability matrix of shape (N, M).\n    obs (list[int]): Observation sequence as a list of indices (length T).\n    t (int): Time index (0 \u2264 t < T).\n\n    Returns:\n    list[float]: The backward probability vector \u03b2_t, with each value rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A)\n    B = np.array(B)\n    T = len(obs)\n    N = A.shape[0]\n\n    # Initialize beta_t\n    beta_t = np.zeros(N)\n\n    if t == T - 1:\n        # At the last time step, \u03b2_T-1(i) = 1 for all i\n        beta_t = np.ones(N)\n    else:\n        # For t < T-1, compute \u03b2_t(i)\n        # \u03b2_t(i) = sum_j a_ij * b_j(o_{t+1}) * \u03b2_{t+1}(j)\n        # First, get \u03b2_{t+1}\n        beta_next = backward_beta(A.tolist(), B.tolist(), obs, t + 1)\n        beta_next = np.array(beta_next)\n\n        # For each state i, compute the sum over j\n        for i in range(N):\n            sum_j = 0.0\n            for j in range(N):\n                sum_j += A[i, j] * B[j, obs[t + 1]] * beta_next[j]\n            beta_t[i] = sum_j\n\n    # Round to 4 decimal places\n    return [round(prob, 4) for prob in beta_t]\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    N = len(A)  # Number of states\n    T = len(obs)  # Length of observation sequence\n\n    # Convert lists to numpy arrays for easier calculations\n    A = np.array(A)\n    B = np.array(B)\n    pi = np.array(pi)\n    obs = np.array(obs)\n\n    # Initialize beta matrix: shape (T, N)\n    beta = np.zeros((T, N))\n\n    # Base case: at time T-1, beta_{T-1}(i) = 1\n    beta[T - 1, :] = 1.0\n\n    # Recursive computation: t = T-2 down to 0\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            # sum over j: A[i,j] * B[j, obs[t+1]] * beta[t+1, j]\n            beta[t, i] = np.sum(A[i, :] * B[:, obs[t + 1]] * beta[t + 1, :])\n\n    # Compute the total probability\n    prob = np.sum(pi * B[:, obs[0]] * beta[0, :])\n\n    # Round to six decimal places\n    return round(prob, 6)\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier handling\n    try:\n        S = np.array(S, dtype=float)\n        A = np.array(A, dtype=float)\n        B = np.array(B, dtype=float)\n    except Exception:\n        return -1\n\n    n = len(S)  # Number of states\n    if n == 0:\n        return -1\n\n    # Validate dimensions\n    if A.shape != (n, n):\n        return -1\n    if B.shape[0] != n:\n        return -1\n\n    m = B.shape[1]  # Number of observation symbols\n\n    # Validate observations\n    if not observations:\n        return -1\n    for obs in observations:\n        if not isinstance(obs, int) or obs < 0 or obs >= m:\n            return -1\n\n    # Initialize alpha matrix\n    T = len(observations)\n    alpha = np.zeros((T, n))\n\n    # Initialization step\n    alpha[0, :] = S * B[:, observations[0]]\n\n    # Recursive step\n    for t in range(1, T):\n        for j in range(n):\n            alpha[t, j] = np.sum(alpha[t - 1, :] * A[:, j]) * B[j, observations[t]]\n\n    # Total probability of the observation sequence\n    prob = np.sum(alpha[T - 1, :])\n\n    return round(prob, 4)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n\n    # 1. Count support of individual items\n    item_support = Counter()\n    for transaction in transactions:\n        for item in transaction:\n            item_support[item] += 1\n\n    # Filter items below min_support\n    frequent_items = {item for item, count in item_support.items() if count >= min_support}\n\n    # If no items are frequent, return empty list\n    if not frequent_items:\n        return []\n\n    # 2. Build the initial FP-tree\n    # Each node: { 'item': str, 'count': int, 'parent': node, 'children': dict }\n    class FPNode:\n        def __init__(self, item, count, parent):\n            self.item = item\n            self.count = count\n            self.parent = parent\n            self.children = {}\n            self.link = None  # link to next node with same item\n\n        def increment(self, count):\n            self.count += count\n\n    # Build header table: item -> first node in linked list\n    header_table = {}\n    # Initialize header table with nodes\n    for item in frequent_items:\n        header_table[item] = None\n\n    # Function to insert a transaction into the FP-tree\n    def insert_transaction(tree_root, transaction, count=1):\n        current_node = tree_root\n        for item in transaction:\n            if item in current_node.children:\n                current_node.children[item].increment(count)\n            else:\n                new_node = FPNode(item, count, current_node)\n                current_node.children[item] = new_node\n                # Update header table link\n                if header_table[item] is None:\n                    header_table[item] = new_node\n                else:\n                    # Follow link to last node and append\n                    node = header_table[item]\n                    while node.link is not None:\n                        node = node.link\n                    node.link = new_node\n            current_node = current_node.children[item]\n\n    # Build the FP-tree\n    tree_root = FPNode(None, 0, None)\n    # Preprocess transactions: keep only frequent items and sort\n    for transaction in transactions:\n        filtered = [item for item in transaction if item in frequent_items]\n        sorted_items = sorted(filtered)\n        insert_transaction(tree_root, sorted_items)\n\n    # 3. Recursive mining function\n    def mine_fp_tree(tree_root, header_table, prefix):\n        # Collect all items in header table sorted by support ascending\n        items = sorted(header_table.items(), key=lambda x: x[1].count if x[1] else 0)\n        frequent_itemsets = []\n\n        for item, node in items:\n            # Build new prefix with current item\n            new_prefix = prefix + [item]\n            new_prefix_sorted = sorted(new_prefix)\n            frequent_itemsets.append(new_prefix_sorted)\n\n            # Build conditional pattern base\n            conditional_patterns = []\n            node_link = node\n            while node_link is not None:\n                path = []\n                parent = node_link.parent\n                while parent is not None and parent.item is not None:\n                    path.append(parent.item)\n                    parent = parent.parent\n                path.reverse()\n                if path:\n                    for _ in range(node_link.count):\n                        conditional_patterns.append(path)\n                node_link = node_link.link\n\n            # Count support in conditional pattern base\n            cond_support = Counter()\n            for pattern in conditional_patterns:\n                for item_in_pattern in pattern:\n                    cond_support[item_in_pattern] += 1\n\n            # Filter items in conditional pattern base\n            cond_frequent_items = {item for item, count in cond_support.items() if count >= min_support}\n            if not cond_frequent_items:\n                continue\n\n            # Build conditional FP-tree\n            def build_conditional_tree():\n                cond_tree_root = FPNode(None, 0, None)\n                cond_header_table = {}\n                for item in cond_frequent_items:\n                    cond_header_table[item] = None\n\n                for pattern in conditional_patterns:\n                    filtered_pattern = [i for i in pattern if i in cond_frequent_items]\n                    sorted_pattern = sorted(filtered_pattern)\n                    insert_transaction(cond_tree_root, sorted_pattern)\n\n                return cond_tree_root, cond_header_table\n\n            cond_tree_root, cond_header_table = build_conditional_tree()\n\n            # Recursively mine the conditional FP-tree\n            if any(cond_header_table.values()):\n                frequent_itemsets.extend(\n                    mine_fp_tree(cond_tree_root, cond_header_table, new_prefix)\n                )\n\n        return frequent_itemsets\n\n    # Start mining from the main FP-tree\n    result = mine_fp_tree(tree_root, header_table, [])\n\n    # Remove duplicates if any (shouldn't be, but just in case)\n    unique_result = []\n    seen = set()\n    for itemset in result:\n        t = tuple(itemset)\n        if t not in seen:\n            seen.add(t)\n            unique_result.append(itemset)\n\n    # Sort the result as specified\n    unique_result.sort(key=lambda x: (len(x), x))\n    return unique_result\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Validate k\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n\n    # Convert training data to numpy array for efficient computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n\n    predictions = []\n\n    for test_point in X_test:\n        test_point_np = np.array(test_point)\n\n        # Compute squared Euclidean distances (without sqrt for efficiency)\n        distances = np.sum((X_train_np - test_point_np) ** 2, axis=1)\n\n        # Get indices of the k smallest distances\n        neighbor_indices = np.argpartition(distances, k - 1)[:k]\n\n        # Count the class labels among the neighbors\n        neighbor_labels = y_train_np[neighbor_indices]\n        # Count occurrences of each class\n        class_counts = {}\n        for label in neighbor_labels:\n            class_counts[label] = class_counts.get(label, 0) + 1\n\n        # Find the class with the highest count\n        max_count = max(class_counts.values())\n        # Among classes with max count, pick the smallest label\n        candidate_classes = [cls for cls, count in class_counts.items() if count == max_count]\n        predicted_class = min(candidate_classes)\n\n        predictions.append(predicted_class)\n\n    return predictions\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Add epsilon for numerical stability\n    epsilon = np.finfo(float).eps\n    \n    # Compute the element-wise log of y_pred with epsilon\n    log_preds = np.log(y_pred + epsilon)\n    \n    # Element-wise multiplication with y (which is one-hot)\n    # Since y is one-hot, only the true class contributes\n    # Sum over classes for each sample\n    sample_losses = -np.sum(y * log_preds, axis=1)\n    \n    # Sum over all samples to get total loss\n    total_loss = np.sum(sample_losses)\n    \n    # Round to 4 decimal places\n    return round(total_loss, 4)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n\n    def relu(Z):\n        return np.maximum(0, Z)\n\n    def sigmoid(Z):\n        return 1 / (1 + np.exp(-Z))\n\n    caches = []\n    A = X\n    L = len([key for key in parameters if key.startswith('W')])  # Number of layers\n\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A) + b\n        cache = (A, W, b, Z)  # Store cache for potential backward pass\n\n        if l < L:\n            A = relu(Z)\n        else:\n            A = sigmoid(Z)\n\n        caches.append(cache)\n\n    AL = A\n    # Round to 4 decimal places\n    AL_rounded = np.round(AL, 4)\n    return AL_rounded.tolist()\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    # Ensure A2 is a numpy array\n    A2 = np.array(A2)\n    Y = np.array(Y)\n    \n    # Clip A2 to avoid log(0)\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Compute the cost\n    m = Y.shape[0]\n    cost = - (1 / m) * np.sum(\n        Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)\n    )\n    \n    # Return rounded to 6 decimal places\n    return round(cost, 6)\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    # Convert Q to numpy array for easier processing\n    Q = np.array(Q)\n    n = len(Q)\n    \n    # Find the index of the first occurrence of the maximum Q-value\n    greedy_index = np.argmax(Q)\n    \n    # Calculate the probability for the greedy action\n    p_greedy = 1 - epsilon + (epsilon / n)\n    \n    # Probability for non-greedy actions\n    p_other = epsilon / n\n    \n    # Initialize probability array\n    probabilities = np.full(n, p_other)\n    probabilities[greedy_index] = p_greedy\n    \n    # Round to 4 decimal places\n    probabilities = np.round(probabilities, 4)\n    \n    # Adjust for any rounding errors to ensure sum is exactly 1\n    total = np.sum(probabilities)\n    diff = 1.0 - total\n    # Correct the first element to compensate for rounding error\n    probabilities[0] += diff\n    \n    # Final rounding to ensure within tolerance\n    probabilities = np.round(probabilities, 4)\n    \n    return probabilities.tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n\n    num_trees = len(predictions)\n    num_samples = len(predictions[0])\n\n    # Initialize the list to hold final predictions\n    final_predictions = []\n\n    for sample_idx in range(num_samples):\n        # Collect votes for the current sample from all trees\n        votes = [predictions[tree_idx][sample_idx] for tree_idx in range(num_trees)]\n        # Count the frequency of each class label\n        counter = Counter(votes)\n        # Find the maximum vote count\n        max_votes = max(counter.values())\n        # Extract all labels with the maximum votes\n        candidates = [label for label, count in counter.items() if count == max_votes]\n        # Select the smallest label among candidates\n        # For numeric labels, comparison is straightforward\n        # For strings, lex order applies\n        # The min() function works for both types if they are comparable\n        # Assuming all labels are of the same type per sample, as per problem statement\n        selected_label = min(candidates)\n        final_predictions.append(selected_label)\n\n    return final_predictions\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Convert input to numpy array for uniform processing\n    arr = np.array(z, dtype=float)\n\n    # Compute softplus using the numerically stable identity\n    result = np.logaddexp(0.0, arr)\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # Check if input was scalar\n    if np.isscalar(z):\n        return float(result)\n    else:\n        # Convert to list\n        return result.tolist()\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Check if policy has 'ev_estimates' attribute or key\n    if not hasattr(policy, 'ev_estimates') and not isinstance(policy, dict):\n        return np.nan\n    if isinstance(policy, dict):\n        estimates = policy.get('ev_estimates', {})\n    else:\n        estimates = getattr(policy, 'ev_estimates', {})\n\n    # If estimates are missing or empty, return nan\n    if not estimates:\n        return np.nan\n\n    # Extract true expected values\n    true_evs = getattr(bandit, 'arm_evs', None)\n    if true_evs is None:\n        return np.nan\n\n    # Sort the arms to align estimates with true values\n    sorted_arms = sorted(estimates.keys())\n\n    # Compute squared errors\n    squared_errors = []\n    for arm in sorted_arms:\n        true_value = true_evs[arm]\n        estimate = estimates[arm]\n        squared_errors.append((estimate - true_value) ** 2)\n\n    # Calculate mean squared error\n    mse_value = np.mean(squared_errors)\n\n    # Return rounded to 4 decimal places\n    return round(mse_value, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # 1. Build all N-gram frequency tables from the corpus\n    # Tokenize corpus and build vocabulary\n    tokenized_corpus = []\n    for sentence in corpus:\n        tokens = sentence.strip().split()\n        tokenized_corpus.extend(tokens)\n    # Count words in corpus\n    word_counts = Counter(tokenized_corpus)\n    # Add <UNK> token to vocabulary\n    vocab = set(word_counts.keys())\n    vocab.add(\"<UNK>\")\n    V = len(vocab)\n\n    # Replace unseen words in corpus with <UNK>\n    # For building N-grams, we need to process each sentence separately\n    # and replace words not in vocab with <UNK>\n    processed_corpus = []\n    for sentence in corpus:\n        tokens = sentence.strip().split()\n        tokens = [w if w in vocab else \"<UNK>\" for w in tokens]\n        processed_corpus.append(tokens)\n\n    # Build N-gram counts\n    ngram_counts = [Counter() for _ in range(1, N+1)]\n    # For context counts (N-1 grams)\n    context_counts = Counter()\n\n    for tokens in processed_corpus:\n        # Pad the sentence with (N-1) start tokens if needed\n        # Not specified explicitly, but common practice is to pad with <s>\n        # Since not specified, we can proceed without padding\n        for n in range(1, N+1):\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i+n])\n                ngram_counts[n-1][ngram] += 1\n                if n > 1:\n                    context = ngram[:-1]\n                    context_counts[context] += 1\n\n    # 2. Process the target sequence\n    seq_tokens = sequence.strip().split()\n    # Replace unseen words with <UNK>\n    seq_tokens = [w if w in vocab else \"<UNK>\" for w in seq_tokens]\n\n    # If sequence length < N, no N-grams exist\n    if len(seq_tokens) < N:\n        return 0.0\n\n    # Calculate the log probability\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i:i+N])\n        context = ngram[:-1]\n        word = ngram[-1]\n\n        # Count of the context\n        context_count = context_counts[context] if N > 1 else sum(ngram_counts[0].values())\n\n        # Count of the n-gram\n        ngram_count = ngram_counts[N-1][ngram]\n\n        # Apply additive smoothing\n        numerator = ngram_count + K\n        denominator = context_count + K * V\n\n        prob = numerator / denominator\n        log_prob += math.log(prob)\n\n    # Round to 4 decimals\n    return round(log_prob, 4)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)  # For reproducibility\n\n    N, M = X.shape\n\n    # Initialize W and H with fixed seed\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n\n    def compute_loss(X, W, H, alpha):\n        reconstruction = W @ H\n        frob_loss = np.linalg.norm(X - reconstruction, 'fro') ** 2\n        reg_loss = alpha * (np.linalg.norm(W, 'fro') ** 2 + np.linalg.norm(H, 'fro') ** 2)\n        return frob_loss + reg_loss\n\n    prev_loss = compute_loss(X, W, H)\n    for iteration in range(max_iter):\n        # Update W: solve (H H^T + alpha * I) W^T = H X^T\n        HHT = H @ H.T\n        reg_W = alpha * np.eye(K)\n        for i in range(N):\n            # Solve for each row of W\n            W[i, :] = np.linalg.solve(HHT + reg_W, H @ X[i, :].T)\n\n        # Update H: solve (W^T W + alpha * I) H = W^T X\n        WTW = W.T @ W\n        reg_H = alpha * np.eye(K)\n        for j in range(M):\n            # Solve for each column of H\n            H[:, j] = np.linalg.solve(WTW + reg_H, W.T @ X[:, j])\n\n        # Compute current loss\n        current_loss = compute_loss(X, W, H, alpha)\n\n        # Check for convergence\n        if current_loss < tol:\n            break\n\n        # Optional: early stopping if loss does not improve significantly\n        # (not required, but can be added)\n        # if abs(prev_loss - current_loss) < 1e-8:\n        #     break\n        prev_loss = current_loss\n\n    # Reconstruct and round the matrix\n    X_hat = (W @ H).round(4)\n    return X_hat.tolist()\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n\n    # Set seeds for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    m, n = X.shape\n\n    # Handle edge cases\n    if K <= 0:\n        return []\n    if K >= m:\n        # Each point as a centroid if K >= number of points\n        centroids = [X[i].copy() for i in range(min(K, m))]\n        # Pad with duplicates if K > m\n        while len(centroids) < K:\n            centroids.append(centroids[-1].copy())\n        # Round and sort\n        centroids = [list(np.round(c, 4)) for c in centroids]\n        centroids.sort(key=lambda c: (c[0], c))\n        return centroids\n\n    # 1. Initialize centroids with K-Means++ method\n    centroids = []\n\n    # Choose the first centroid randomly\n    first_idx = random.randint(0, m - 1)\n    centroids.append(X[first_idx])\n\n    # Choose remaining K-1 centroids\n    for _ in range(1, K):\n        # Compute squared distances from each point to the nearest centroid\n        distances = np.array([\n            min(np.sum((X[i] - c) ** 2) for c in centroids)\n            for i in range(m)\n        ])\n\n        # Handle the case where all distances are zero (all points are identical)\n        total_distance = distances.sum()\n        if total_distance == 0:\n            # All points are identical; pick random points\n            remaining_indices = list(set(range(m)) - {np.where((X == c).all(axis=1))[0][0] for c in centroids})\n            if remaining_indices:\n                next_idx = random.choice(remaining_indices)\n                centroids.append(X[next_idx])\n            else:\n                # All points are already chosen\n                break\n        else:\n            # Select next centroid with probability proportional to squared distance\n            probabilities = distances / total_distance\n            next_idx = np.random.choice(m, p=probabilities)\n            centroids.append(X[next_idx])\n\n    # Convert centroids to numpy array for easier computation\n    centroids = np.array(centroids)\n\n    # 2. Lloyd's iterations\n    for iteration in range(max_iter):\n        # Assign points to nearest centroid\n        distances = np.array([\n            np.sum((X - c) ** 2, axis=1) for c in centroids\n        ])  # shape: (K, m)\n        cluster_assignments = np.argmin(distances, axis=0)\n\n        new_centroids = np.zeros_like(centroids)\n        for k in range(K):\n            points_in_cluster = X[cluster_assignments == k]\n            if len(points_in_cluster) == 0:\n                # Keep centroid unchanged if cluster is empty\n                new_centroids[k] = centroids[k]\n            else:\n                new_centroids[k] = points_in_cluster.mean(axis=0)\n\n        # Check for convergence\n        if np.allclose(centroids, new_centroids, rtol=1e-8, atol=1e-8):\n            break\n\n        centroids = new_centroids\n\n    # 3. Round to 4 decimal places\n    centroids = np.round(centroids, 4)\n\n    # 4. Sort centroids by first coordinate, then by full tuple\n    sorted_centroids = sorted(centroids.tolist(), key=lambda c: (c[0], c))\n\n    return sorted_centroids\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encodes the input text into a fixed-length list of integer token IDs.\n    Characters not in vocabulary are encoded as 0 (padding).\n    If shorter than time_steps, pad with 0s on the right.\n    If longer, truncate to time_steps.\n    \"\"\"\n    encoded = []\n    for ch in text:\n        encoded.append(vocabulary.get(ch, 0))\n    # Truncate if longer\n    encoded = encoded[:time_steps]\n    # Pad if shorter\n    if len(encoded) < time_steps:\n        encoded.extend([0] * (time_steps - len(encoded)))\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decodes a list/array of integer IDs back to a string.\n    Ignores padding tokens (0).\n    \"\"\"\n    chars = []\n    for idx in indices:\n        if idx == 0:\n            continue  # skip padding\n        chars.append(inverse_vocab.get(idx, ''))\n    return ''.join(chars)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    Encodes input text, runs model prediction, and decodes the output.\n    Assumes model has a .predict method that accepts a batch of shape (batch_size, time_steps).\n    \"\"\"\n    # Encode input text\n    input_encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to numpy array and add batch dimension\n    input_array = np.array([input_encoded], dtype=np.int32)  # shape: (1, TIME_STEPS)\n    \n    # Get model prediction: shape (batch_size, time_steps, vocab_size)\n    prediction = model.predict(input_array)\n    \n    # For each time step, pick the index with the highest probability\n    predicted_ids = np.argmax(prediction, axis=-1)[0]  # shape: (time_steps,)\n    \n    # Decode predicted IDs to string\n    decoded_text = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded_text\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *examples* is None, the function\n        should use the global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Use global EXAMPLES if examples is None\n    if examples is None:\n        examples = EXAMPLES\n\n    predictions = []\n\n    for example in examples:\n        # Call the helper run_example\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        # Concatenate output characters into a string\n        predicted_str = ''.join(output_chars)\n        # Print input and output\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n        # Collect the prediction\n        predictions.append(predicted_str)\n\n    return predictions\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    epsilon = 1e-20  # small constant to avoid log(0)\n    \n    # Ensure y and w are 1D arrays for consistency\n    y = y.reshape(-1)\n    w = w.reshape(-1)\n    \n    # Compute z = X \u00b7 w\n    z = X @ w  # shape (m,)\n    \n    # Compute predicted probabilities p = \u03c3(z)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip p to avoid log(0)\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute the loss\n    loss_terms = y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped)\n    loss = -np.mean(loss_terms)\n    \n    # Compute the gradient\n    diff = p - y  # shape (m,)\n    grad = (X.T @ diff) / X.shape[0]  # shape (n,)\n    \n    # Round loss and gradient\n    loss_rounded = round(loss, 4)\n    grad_rounded = np.round(grad, 4).tolist()\n    \n    return loss_rounded, grad_rounded\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n\n    m, n = X_train.shape\n    # Initialize sample weights uniformly\n    sample_weights = np.ones(m) / m\n\n    # List to store the weak classifiers' parameters\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize variables to store the best stump\n        min_error = float('inf')\n        best_feature_idx = None\n        best_threshold = None\n        best_polarity = 1  # Direction of inequality\n\n        # For each feature, find the best threshold\n        for feature_idx in range(n):\n            feature_values = X_train[:, feature_idx]\n            # Get sorted unique values for potential thresholds\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions based on current threshold and polarity\n                    predictions = np.ones(m)\n                    predictions[polarity * feature_values < polarity * threshold] = -1\n\n                    # Compute weighted error\n                    misclassified = predictions != y_train\n                    error = np.sum(sample_weights * misclassified)\n\n                    if error < min_error:\n                        min_error = error\n                        best_feature_idx = feature_idx\n                        best_threshold = threshold\n                        best_polarity = polarity\n\n        # Compute alpha (classifier weight)\n        # To avoid division by zero, clip error\n        epsilon = 1e-10\n        min_error = max(min_error, epsilon)\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n\n        # Store the classifier parameters\n        classifiers.append({\n            'feature_idx': best_feature_idx,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'alpha': alpha\n        })\n\n        # Make predictions on training data with the selected stump\n        feature_values = X_train[:, best_feature_idx]\n        predictions = np.ones(m)\n        predictions[best_polarity * feature_values < best_polarity * best_threshold] = -1\n\n        # Update sample weights\n        # Increase weights for misclassified, decrease for correct\n        sample_weights *= np.exp(-alpha * y_train * predictions)\n        # Normalize weights\n        sample_weights /= np.sum(sample_weights)\n\n    # Now, predict on X_test\n    final_predictions = np.zeros(X_test.shape[0])\n    for clf in classifiers:\n        feature_idx = clf['feature_idx']\n        threshold = clf['threshold']\n        polarity = clf['polarity']\n        alpha = clf['alpha']\n\n        feature_values = X_test[:, feature_idx]\n        preds = np.ones(X_test.shape[0])\n        preds[polarity * feature_values < polarity * threshold] = -1\n\n        final_predictions += alpha * preds\n\n    # Final prediction is sign of weighted sum\n    return [1 if pred >= 0 else -1 for pred in final_predictions]\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    # Check for edge case: if n_components == 0, return empty list\n    if n_components == 0:\n        return []\n\n    # Unique classes\n    classes = np.unique(y)\n    n_classes = len(classes)\n    n_samples, n_features = X.shape\n\n    # Compute global mean\n    mu = np.mean(X, axis=0)\n\n    # Initialize scatter matrices\n    S_W = np.zeros((n_features, n_features))\n    S_B = np.zeros((n_features, n_features))\n\n    # For each class, compute class mean and scatter matrices\n    for c in classes:\n        X_c = X[y == c]\n        N_c = X_c.shape[0]\n        mu_c = np.mean(X_c, axis=0)\n        # Within-class scatter\n        X_centered = X_c - mu_c\n        S_W += X_centered.T @ X_centered\n        # Between-class scatter\n        mean_diff = (mu_c - mu).reshape(-1, 1)\n        S_B += N_c * (mean_diff @ mean_diff.T)\n\n    # Compute the pseudo-inverse of S_W\n    S_W_inv = np.linalg.pinv(S_W)\n\n    # Form the matrix A = S_W^{-1} * S_B\n    A = S_W_inv @ S_B\n\n    # Eigen-decomposition (A is symmetric)\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n\n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_idx]\n    eigenvectors = eigenvectors[:, sorted_idx]\n\n    # Select the top n_components eigenvectors\n    W = eigenvectors[:, :n_components]\n\n    # Deterministic sign correction\n    for i in range(W.shape[1]):\n        first_nonzero_idx = np.nonzero(W[:, i])[0]\n        if first_nonzero_idx.size > 0:\n            first_nonzero_element = W[first_nonzero_idx[0], i]\n            if first_nonzero_element < 0:\n                W[:, i] = -W[:, i]\n\n    # Project data\n    projected = X @ W\n\n    # Round to four decimal places\n    projected_rounded = np.round(projected, 4)\n\n    # Convert to list of lists\n    return projected_rounded.tolist()\n```"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n\n    # Special case: 1x1 maze\n    if n == 1:\n        if grid[0][0] == '.':\n            return 0\n        else:\n            return -1\n\n    # Directions for movement: up, down, left, right\n    directions = [(-1,0), (1,0), (0,-1), (0,1)]\n\n    # Initialize fire_time grid with infinity\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    fire_queue = deque()\n\n    # Find initial fire sources\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                fire_queue.append((i,j))\n\n    # BFS for fire spread\n    while fire_queue:\n        x, y = fire_queue.popleft()\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] != '#' and fire_time[nx][ny] == math.inf:\n                    fire_time[nx][ny] = fire_time[x][y] + 1\n                    fire_queue.append((nx, ny))\n\n    # Check if start or goal is on fire at time 0\n    if fire_time[0][0] == 0 or fire_time[n-1][n-1] == 0:\n        return -1\n\n    # BFS for agent movement\n    agent_queue = deque()\n    visited = [[math.inf for _ in range(n)] for _ in range(n)]\n    # Start at (0,0) at time 0\n    agent_queue.append((0,0,0))\n    visited[0][0] = 0\n\n    while agent_queue:\n        x, y, time = agent_queue.popleft()\n\n        # If reached goal\n        if x == n-1 and y == n-1:\n            return time\n\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            nt = time + 1\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] != '#' and visited[nx][ny] > nt:\n                    # Check if fire arrives after agent\n                    if fire_time[nx][ny] > nt:\n                        visited[nx][ny] = nt\n                        agent_queue.append((nx, ny, nt))\n    # If no path found\n    return -1\n```"}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement your code here.\"\"\"\n    # Define a function to compute distances based on the selected metric\n    def compute_distance(a, b, metric_type):\n        if metric_type == 'euclidean':\n            # \u2113\u2082 distance\n            return np.linalg.norm(a - b)\n        elif metric_type == 'manhattan':\n            # \u2113\u2081 distance\n            return np.sum(np.abs(a - b))\n        elif metric_type == 'cosine':\n            # Cosine distance: 1 - cosine similarity\n            # To avoid division by zero, add a small epsilon\n            epsilon = 1e-10\n            a_norm = np.linalg.norm(a) + epsilon\n            b_norm = np.linalg.norm(b) + epsilon\n            cosine_similarity = np.dot(a, b) / (a_norm * b_norm)\n            return 1 - cosine_similarity\n        else:\n            # Fallback to Euclidean if unknown metric\n            return np.linalg.norm(a - b)\n\n    # Prepare an array to hold predictions\n    predictions = np.empty(X_test.shape[0], dtype=y_train.dtype)\n\n    # For each test sample, compute distances to all training samples\n    for i, test_sample in enumerate(X_test):\n        distances = np.array([compute_distance(test_sample, train_sample, metric) for train_sample in X_train])\n        # Find indices of the k smallest distances\n        neighbor_indices = np.argpartition(distances, k)[:k]\n        # Get the labels of the k nearest neighbors\n        neighbor_labels = y_train[neighbor_indices]\n        # Count the occurrences of each label\n        labels, counts = np.unique(neighbor_labels, return_counts=True)\n        # Find the label(s) with the maximum count\n        max_count = np.max(counts)\n        candidate_labels = labels[counts == max_count]\n        # In case of tie, select the smallest label\n        predicted_label = np.min(candidate_labels)\n        predictions[i] = predicted_label\n\n    return predictions\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n\n    # Define a small epsilon to avoid division by zero in cosine\n    epsilon = 1e-12\n\n    # Number of training samples\n    n_train = X.shape[0]\n\n    # Adjust k if it exceeds number of training samples\n    k = min(k, n_train)\n\n    predictions = []\n\n    for test_sample in X_test:\n        # Compute distances based on the selected metric\n        if metric == 'euclidean':\n            # Euclidean distance: sqrt(sum((x - y)^2))\n            distances = np.sqrt(np.sum((X - test_sample) ** 2, axis=1))\n        elif metric == 'manhattan':\n            # Manhattan distance: sum of absolute differences\n            distances = np.sum(np.abs(X - test_sample), axis=1)\n        elif metric == 'cosine':\n            # Cosine distance: 1 - (x \u00b7 y) / (||x|| * ||y||)\n            # Compute dot product\n            dot_products = np.dot(X, test_sample)\n            # Compute norms\n            X_norms = np.linalg.norm(X, axis=1)\n            test_norm = np.linalg.norm(test_sample)\n            denom = (X_norms * test_norm) + epsilon\n            cosine_sim = dot_products / denom\n            # Convert similarity to distance\n            distances = 1 - cosine_sim\n        else:\n            raise ValueError(\"Unsupported metric. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n\n        # Get indices of the k closest neighbors\n        neighbor_indices = np.argsort(distances)[:k]\n\n        # Get the labels of the neighbors\n        neighbor_labels = y[neighbor_indices]\n\n        # Count the occurrences of each label\n        labels, counts = np.unique(neighbor_labels, return_counts=True)\n\n        # Find the label(s) with the maximum count\n        max_count = np.max(counts)\n        candidate_labels = labels[counts == max_count]\n\n        # In case of tie, select the smallest label\n        predicted_label = np.min(candidate_labels)\n\n        predictions.append(predicted_label)\n\n    return predictions\n```"}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to numpy array if it's a list\n    x = np.array(state)\n\n    # First layer affine transformation + ReLU\n    z1 = np.dot(x, weights['W1']) + weights['b1']\n    a1 = np.maximum(z1, 0)  # ReLU\n\n    # Second layer affine transformation + ReLU\n    z2 = np.dot(a1, weights['W2']) + weights['b2']\n    a2 = np.maximum(z2, 0)  # ReLU\n\n    # Output layer affine transformation + tanh\n    z3 = np.dot(a2, weights['W3']) + weights['b3']\n    action = np.tanh(z3)\n\n    # Round each component to 4 decimal places and convert to list\n    return [round(val, 4) for val in action]\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n\n    m, n = X.shape\n\n    # Add intercept column\n    if fit_intercept:\n        X_intercept = np.ones((m, 1))\n        X = np.hstack((X_intercept, X))\n        # The first column is the intercept; do not penalize it\n        penalized_indices = np.arange(1, n + 1)\n    else:\n        # Add a column of zeros for intercept (not penalized)\n        X_intercept = np.zeros((m, 1))\n        X = np.hstack((X_intercept, X))\n        penalized_indices = np.arange(1, n + 1)  # same as above, but no penalty on first\n\n    n += 1  # Updated number of features including intercept\n\n    # Initialize weights\n    w = np.zeros(n)\n    b = 0.0  # intercept\n\n    # Precompute sum of squares for each feature (column)\n    X_squared_sum = np.sum(X ** 2, axis=0)\n\n    for iteration in range(max_iters):\n        # Compute residuals\n        residual = y - X @ w\n\n        # If fitting intercept, update it as mean residual\n        if fit_intercept:\n            b = np.mean(residual)\n            residual = residual - b  # adjust residuals after updating intercept\n\n        # Coordinate descent over each feature (excluding intercept if fit_intercept)\n        for j in range(n):\n            if fit_intercept and j == 0:\n                # Do not penalize intercept\n                continue\n\n            # Temporarily set weight to zero\n            w_j_old = w[j]\n            w[j] = 0.0\n\n            # Compute partial residual for feature j\n            r_j = residual + X[:, j] * w_j_old\n\n            # Compute numerator: inner product of feature j and residual\n            a_j = np.dot(X[:, j], r_j)\n\n            # Soft-thresholding operator\n            tau = lambda_param * m\n            if a_j > tau:\n                w_j_new = (a_j - tau) / X_squared_sum[j]\n            elif a_j < -tau:\n                w_j_new = (a_j + tau) / X_squared_sum[j]\n            else:\n                w_j_new = 0.0\n\n            # Update residuals after changing w_j\n            delta_w = w_j_new - w[j]\n            residual -= X[:, j] * delta_w\n            w[j] = w_j_new\n\n    # Extract weights (excluding intercept)\n    weights = w[1:] if fit_intercept else w\n    bias = w[0] if fit_intercept else 0.0\n\n    # Round to 4 decimal places\n    weights_list = [round(wi, 4) for wi in weights]\n    bias = round(bias, 4)\n\n    return weights_list, bias\n```"}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n\n    # Check if lengths are equal\n    if x_arr.shape != y_arr.shape:\n        return -1\n\n    # Compute the number of differing elements\n    differences = np.sum(x_arr != y_arr)\n\n    # Calculate normalized Hamming distance\n    N = x_arr.size\n    distance = differences / N\n\n    # Return rounded to 4 decimal places\n    return round(distance, 4)\n```"}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Validate N\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    \n    # Handle the special case N=1\n    if N == 1:\n        return [1.0]\n    \n    # Generate array of n values\n    n = np.arange(N)\n    # Compute the common factor\n    factor = 2 * np.pi / (N - 1)\n    \n    # Define window calculations\n    if window.lower() == \"hamming\":\n        # w[n] = 0.54 - 0.46 * cos(2\u03c0n/(N\u22121))\n        w = 0.54 - 0.46 * np.cos(factor * n)\n    elif window.lower() == \"hann\":\n        # w[n] = 0.5 - 0.5 * cos(2\u03c0n/(N\u22121))\n        w = 0.5 - 0.5 * np.cos(factor * n)\n    elif window.lower() == \"blackman_harris\":\n        # w[n] = a0 - a1 * cos(2\u03c0n/(N\u22121)) + a2 * cos(4\u03c0n/(N\u22121)) - a3 * cos(6\u03c0n/(N\u22121))\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        w = (a0\n             - a1 * np.cos(factor * n)\n             + a2 * np.cos(2 * factor * n)\n             - a3 * np.cos(3 * factor * n))\n    elif window.lower() == \"generalized_cosine\":\n        # Use provided coefficients\n        if coefficients is None:\n            raise ValueError(\"coefficients must be provided for generalized_cosine window.\")\n        # w[n] = sum_k a_k * cos(2\u03c0k n / (N-1))\n        w = np.zeros(N)\n        for k, a_k in enumerate(coefficients):\n            w += a_k * np.cos(2 * np.pi * k * n / (N - 1))\n    else:\n        raise ValueError(f\"Unknown window type: {window}\")\n    \n    # Round to 4 decimal places\n    w_rounded = [round(val, 4) for val in w]\n    return w_rounded\n```"}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n\n    # Convert lists to numpy arrays for easier computation\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train)\n\n    X_test = np.array(X_test, dtype=float)\n\n    # Handle the case where all training labels are identical\n    unique_labels = np.unique(y_train)\n    if unique_labels.size == 1:\n        # All labels are the same; predict the same label for all test samples\n        return [int(unique_labels[0])] * len(X_test)\n\n    # Map original labels to {0,1}\n    label_mapping = {}\n    label_mapping[unique_labels[0]] = 0\n    label_mapping[unique_labels[1]] = 1\n\n    y_mapped = np.array([label_mapping[label] for label in y_train])\n\n    # Store the inverse mapping for final predictions\n    inverse_mapping = {v: k for k, v in label_mapping.items()}\n\n    # Add intercept term (bias) to training and test data\n    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # Initialize weights\n    weights = np.zeros(X_train.shape[1])\n\n    # Number of training samples\n    n_samples = X_train.shape[0]\n\n    # Number of batches per epoch\n    n_batches = int(np.ceil(n_samples / batch_size))\n\n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    # Training loop\n    for epoch in range(epochs):\n        # Shuffle data at each epoch for better convergence\n        perm = np.random.permutation(n_samples)\n        X_shuffled = X_train[perm]\n        y_shuffled = y_mapped[perm]\n\n        for batch_idx in range(n_batches):\n            start_idx = batch_idx * batch_size\n            end_idx = min(start_idx + batch_size, n_samples)\n            X_batch = X_shuffled[start_idx:end_idx]\n            y_batch = y_shuffled[start_idx:end_idx]\n\n            # Compute predictions\n            z = np.dot(X_batch, weights)\n            preds = sigmoid(z)\n\n            # Compute gradient\n            error = preds - y_batch\n            gradient = np.dot(X_batch.T, error) / len(y_batch)\n\n            # Update weights\n            weights -= learning_rate * gradient\n\n    # Make predictions on test set\n    probs = sigmoid(np.dot(X_test, weights))\n    predicted_labels = (probs >= 0.5).astype(int)\n\n    # Map predictions back to original labels\n    inverse_map = {v: k for k, v in label_mapping.items()}\n    final_predictions = [inverse_map[pred] for pred in predicted_labels]\n\n    return final_predictions\n```"}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"\n    Implements AdaBoost with decision stumps from scratch.\n    \n    Args:\n        X_train: List of feature vectors for training data.\n        y_train: List of labels (0 or 1) for training data.\n        X_test: List of feature vectors for test data.\n        n_estimators: Number of weak learners to use.\n        \n    Returns:\n        List of predicted labels (0 or 1) for X_test.\n    \"\"\"\n    # Convert labels from {0,1} to {-1,1} for convenience\n    y_train_signed = np.array([1 if y == 1 else -1 for y in y_train])\n    X_train_np = np.array(X_train)\n    X_test_np = np.array(X_test)\n    n_samples, n_features = X_train_np.shape\n    \n    # Initialize weights uniformly\n    w = np.ones(n_samples) / n_samples\n    \n    # Store the weak learners\n    learners = []\n    # Store the alpha (vote weight) for each learner\n    alphas = []\n    \n    for _ in range(n_estimators):\n        # Initialize variables to store the best stump\n        min_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_predictions = None\n        \n        # Iterate over all features\n        for feature_idx in range(n_features):\n            feature_values = X_train_np[:, feature_idx]\n            # Get all unique thresholds (candidate thresholds)\n            thresholds = np.unique(feature_values)\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions based on the current stump\n                    preds = np.ones(n_samples)\n                    if polarity == 1:\n                        preds[feature_values < threshold] = 1\n                        preds[feature_values >= threshold] = -1\n                    else:\n                        preds[feature_values >= threshold] = 1\n                        preds[feature_values < threshold] = -1\n                        \n                    # Compute weighted error\n                    misclassified = preds != y_train_signed\n                    error = np.sum(w * misclassified)\n                    \n                    # Check if this is the best stump so far\n                    if error < min_error:\n                        min_error = error\n                        best_feature = feature_idx\n                        best_threshold = threshold\n                        best_polarity = polarity\n                        best_predictions = preds\n                        \n        # If error is zero, perfect stump found, can stop early\n        if min_error == 0:\n            # Store the stump\n            learners.append({\n                'feature_index': best_feature,\n                'threshold': best_threshold,\n                'polarity': best_polarity\n            })\n            # Compute alpha\n            alpha = 0.5 * np.log((1 - min_error + 1e-10) / (min_error + 1e-10))\n            alphas.append(alpha)\n            break\n        \n        # Compute alpha for the selected stump\n        error_t = min_error\n        alpha_t = 0.5 * np.log((1 - error_t + 1e-10) / (error_t + 1e-10))\n        alphas.append(alpha_t)\n        # Store the stump parameters\n        learners.append({\n            'feature_index': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity\n        })\n        \n        # Update weights\n        # h_t(x_i) in {-1,1}\n        h_x = best_predictions\n        # Update rule\n        w = w * np.exp(-alpha_t * y_train_signed * h_x)\n        # Normalize weights\n        w /= np.sum(w)\n    \n    # Now, make predictions on X_test\n    final_scores = np.zeros(len(X_test))\n    for alpha, learner in zip(alphas, learners):\n        feature_idx = learner['feature_index']\n        threshold = learner['threshold']\n        polarity = learner['polarity']\n        feature_values = X_test_np[:, feature_idx]\n        preds = np.ones(len(X_test))\n        if polarity == 1:\n            preds[feature_values < threshold] = 1\n            preds[feature_values >= threshold] = -1\n        else:\n            preds[feature_values >= threshold] = 1\n            preds[feature_values < threshold] = -1\n        # Aggregate weighted votes\n        final_scores += alpha * preds\n    \n    # Convert final scores to {0,1}\n    y_pred = [1 if score >= 0 else 0 for score in final_scores]\n    return y_pred\n```"}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Validate inputs\n    if not (isinstance(n_dims, int) and n_dims > 0):\n        return -1\n    if not (isinstance(num_particles, int) and num_particles > 0):\n        return -1\n    if not (isinstance(num_iterations, int) and num_iterations > 0):\n        return -1\n\n    # Initialize RNG\n    rng = np.random.default_rng(seed)\n\n    # Define bounds\n    lower_bound = -1.0\n    upper_bound = 1.0\n\n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n\n    # Initialize particles' positions and velocities\n    positions = rng.uniform(low=lower_bound, high=upper_bound, size=(num_particles, n_dims))\n    velocities = rng.uniform(low=-abs(upper_bound - lower_bound), high=abs(upper_bound - lower_bound), size=(num_particles, n_dims))\n\n    # Initialize personal bests and their scores\n    p_best_positions = np.copy(positions)\n    p_best_scores = np.sum(p_best_positions**2, axis=1)\n\n    # Initialize global best\n    g_best_idx = np.argmin(p_best_scores)\n    g_best_position = p_best_positions[g_best_idx]\n    g_best_score = p_best_scores[g_best_idx]\n\n    for _ in range(num_iterations):\n        # Generate random coefficients\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n\n        # Update velocities\n        velocities = (\n            w * velocities\n            + c1 * r1 * (p_best_positions - positions)\n            + c2 * r2 * (g_best_position - positions)\n        )\n\n        # Update positions\n        positions = positions + velocities\n        # Clip positions to bounds\n        positions = np.clip(positions, lower_bound, upper_bound)\n\n        # Evaluate current positions\n        scores = np.sum(positions**2, axis=1)\n\n        # Update personal bests\n        better_mask = scores < p_best_scores\n        p_best_positions[better_mask] = positions[better_mask]\n        p_best_scores[better_mask] = scores[better_mask]\n\n        # Update global best\n        current_g_best_idx = np.argmin(p_best_scores)\n        if p_best_scores[current_g_best_idx] < g_best_score:\n            g_best_score = p_best_scores[current_g_best_idx]\n            g_best_position = p_best_positions[current_g_best_idx]\n\n    # Return the best score found, rounded to 4 decimals\n    return round(g_best_score, 4)\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n\n    n_train = len(X_train)\n    # Determine the number of neighbors to consider\n    if k <= 0 or k > n_train:\n        k = n_train\n\n    predictions = []\n\n    for test_point in X_test_np:\n        # Compute Euclidean distances to all training points\n        distances = np.linalg.norm(X_train_np - test_point, axis=1)\n        # Get indices of the k nearest neighbors\n        neighbor_indices = np.argpartition(distances, k - 1)[:k]\n        # Retrieve the neighbor targets\n        neighbor_targets = y_train_np[neighbor_indices]\n\n        if task == \"classification\":\n            # Count the frequency of each label\n            counter = Counter(neighbor_targets)\n            max_count = max(counter.values())\n            # Find all labels with the highest count (to handle ties)\n            candidates = [label for label, count in counter.items() if count == max_count]\n            # Return the smallest label among candidates\n            prediction = min(candidates)\n        elif task == \"regression\":\n            # Compute the mean of neighbor targets, rounded to 4 decimal places\n            mean_value = round(np.mean(neighbor_targets), 4)\n            prediction = mean_value\n        else:\n            raise ValueError(\"Invalid task type. Must be 'classification' or 'regression'.\")\n\n        predictions.append(prediction)\n\n    return predictions\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n\n    class Node:\n        def __init__(self, indices, depth):\n            self.indices = indices  # indices of samples at this node\n            self.depth = depth\n            self.is_leaf = False\n            self.prediction = None\n            self.feature_index = None\n            self.threshold = None\n            self.left = None\n            self.right = None\n\n    def gini_impurity(y):\n        \"\"\"Compute Gini impurity for labels y.\"\"\"\n        if len(y) == 0:\n            return 0.0\n        classes, counts = np.unique(y, return_counts=True)\n        probs = counts / counts.sum()\n        return 1.0 - np.sum(probs ** 2)\n\n    def best_split(X, y, indices):\n        \"\"\"Find the best split for data at current node.\"\"\"\n        n_samples, n_features = X.shape\n        parent_gini = gini_impurity(y[indices])\n        best_gain = 0.0\n        best_feature = None\n        best_threshold = None\n\n        for feature_idx in range(n_features):\n            # Extract feature values for current samples\n            feature_values = X[indices, feature_idx]\n            # Sort samples by feature\n            sorted_idx = np.argsort(feature_values)\n            sorted_values = feature_values[sorted_idx]\n            sorted_y = y[indices][sorted_idx]\n\n            # Evaluate midpoints between consecutive different feature values\n            for i in range(1, len(sorted_values)):\n                if sorted_values[i] == sorted_values[i - 1]:\n                    continue  # skip identical values\n                threshold = (sorted_values[i] + sorted_values[i - 1]) / 2\n\n                # Split samples\n                left_mask = sorted_values <= threshold\n                right_mask = ~left_mask\n\n                left_indices = indices[sorted_idx][left_mask]\n                right_indices = indices[sorted_idx][right_mask]\n\n                # Skip if one child is empty\n                if len(left_indices) == 0 or len(right_indices) == 0:\n                    continue\n\n                # Compute Gini for children\n                gini_left = gini_impurity(y[left_indices])\n                gini_right = gini_impurity(y[right_indices])\n\n                # Compute weighted Gini\n                n_left = len(left_indices)\n                n_right = len(right_indices)\n                weighted_gini = (n_left * gini_left + n_right * gini_right) / (n_left + n_right)\n\n                # Compute Gini gain\n                gain = parent_gini - weighted_gini\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        return best_feature, best_threshold, best_gain\n\n    def build_tree(node, X, y, depth):\n        \"\"\"Recursively build the decision tree.\"\"\"\n        # Check stopping conditions\n        node_labels = y[node.indices]\n        unique_labels = np.unique(node_labels)\n\n        # If all labels are the same, create a leaf\n        if len(unique_labels) == 1:\n            node.is_leaf = True\n            node.prediction = unique_labels[0]\n            return\n\n        # If max_depth is reached\n        if max_depth is not None and depth >= max_depth:\n            node.is_leaf = True\n            # Majority class\n            counts = np.bincount(node_labels)\n            node.prediction = np.argmax(counts)\n            return\n\n        # Find the best split\n        feature_idx, threshold, gain = best_split(X, y, node.indices)\n\n        # If no gain, create a leaf with majority class\n        if gain == 0 or feature_idx is None:\n            node.is_leaf = True\n            counts = np.bincount(node_labels)\n            node.prediction = np.argmax(counts)\n            return\n\n        # Perform split\n        feature_values = X[node.indices, feature_idx]\n        left_mask = feature_values <= threshold\n        right_mask = ~left_mask\n\n        left_indices = node.indices[left_mask]\n        right_indices = node.indices[right_mask]\n\n        # If any child is empty, create a leaf with majority class\n        if len(left_indices) == 0 or len(right_indices) == 0:\n            node.is_leaf = True\n            counts = np.bincount(node_labels)\n            node.prediction = np.argmax(counts)\n            return\n\n        # Set current node's split info\n        node.feature_index = feature_idx\n        node.threshold = threshold\n\n        # Create child nodes\n        node.left = Node(left_indices, depth + 1)\n        node.right = Node(right_indices, depth + 1)\n\n        # Recursively build children\n        build_tree(node.left, X, y, depth + 1)\n        build_tree(node.right, X, y, depth + 1)\n\n    def predict_sample(node, x):\n        \"\"\"Predict label for a single sample.\"\"\"\n        while not node.is_leaf:\n            if x[node.feature_index] <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n        return node.prediction\n\n    # Build the tree\n    n_samples = X_train.shape[0]\n    root = Node(np.arange(n_samples), depth=0)\n    build_tree(root, X_train, y_train, depth=0)\n\n    # Predict for each sample in X_test\n    predictions = [predict_sample(root, x) for x in X_test]\n    return predictions\n```"}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n\n    X = np.array(X)\n    y = np.array(y)\n\n    n_samples, n_features = X.shape\n\n    # Compute the Gini impurity of the whole dataset\n    def gini_impurity(labels):\n        _, counts = np.unique(labels, return_counts=True)\n        proportions = counts / counts.sum()\n        return 1 - np.sum(proportions ** 2)\n\n    G_whole = gini_impurity(y)\n\n    # If all labels are the same, no split is possible\n    if len(np.unique(y)) == 1:\n        return (-1, None, round(G_whole, 4))\n\n    best_feature = -1\n    best_threshold = None\n    best_gini = float('inf')\n\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        unique_values = np.unique(feature_values)\n\n        # If all feature values are the same, skip\n        if len(unique_values) == 1:\n            continue\n\n        # Generate candidate thresholds: midpoints between sorted unique values\n        sorted_values = np.sort(unique_values)\n        candidate_thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n\n        for threshold in candidate_thresholds:\n            # Split data\n            left_mask = feature_values <= threshold\n            right_mask = feature_values > threshold\n\n            # Skip if either child is empty\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n\n            y_left = y[left_mask]\n            y_right = y[right_mask]\n\n            n_left = len(y_left)\n            n_right = len(y_right)\n            n_total = n_left + n_right\n\n            # Compute Gini for left and right nodes\n            def gini(node_labels):\n                _, counts = np.unique(node_labels, return_counts=True)\n                proportions = counts / counts.sum()\n                return 1 - np.sum(proportions ** 2)\n\n            G_left = gini(y_left)\n            G_right = gini(y_right)\n\n            # Compute weighted Gini\n            G_split = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n\n            # Tie-breaking rules\n            if G_split + 1e-12 < best_gini:\n                best_gini = G_split\n                best_feature = feature_idx\n                best_threshold = threshold\n            elif abs(G_split - best_gini) < 1e-12:\n                # Same impurity, tie-break by feature index\n                if feature_idx < best_feature:\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                elif feature_idx == best_feature:\n                    # Same feature, tie-break by threshold\n                    if threshold < best_threshold:\n                        best_threshold = threshold\n\n    if best_feature == -1:\n        # No valid split found\n        return (-1, None, round(G_whole, 4))\n    else:\n        return (best_feature, best_threshold, round(best_gini, 4))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n\n    # Normalize the metric name to lowercase for case-insensitive matching\n    name = name.lower()\n\n    def validate_vectors(x, y):\n        # Check if both are list, tuple, or np.ndarray\n        if not (isinstance(x, (list, tuple, np.ndarray)) and isinstance(y, (list, tuple, np.ndarray))):\n            return False\n        # Convert to numpy arrays for easier processing\n        try:\n            x_arr = np.array(x, dtype=float)\n            y_arr = np.array(y, dtype=float)\n        except (ValueError, TypeError):\n            return False\n        # Check if 1D\n        if x_arr.ndim != 1 or y_arr.ndim != 1:\n            return False\n        # Check non-empty\n        if x_arr.size == 0 or y_arr.size == 0:\n            return False\n        # Check same length\n        if x_arr.shape != y_arr.shape:\n            return False\n        return True\n\n    def euclidean(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        x_arr = np.array(x, dtype=float)\n        y_arr = np.array(y, dtype=float)\n        dist = np.sqrt(np.sum((x_arr - y_arr) ** 2))\n        return round(dist, 4)\n\n    def manhattan(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        x_arr = np.array(x, dtype=float)\n        y_arr = np.array(y, dtype=float)\n        dist = np.sum(np.abs(x_arr - y_arr))\n        return round(dist, 4)\n\n    def chebyshev(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        x_arr = np.array(x, dtype=float)\n        y_arr = np.array(y, dtype=float)\n        dist = np.max(np.abs(x_arr - y_arr))\n        return round(dist, 4)\n\n    def cosine(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        x_arr = np.array(x, dtype=float)\n        y_arr = np.array(y, dtype=float)\n        norm_x = np.linalg.norm(x_arr)\n        norm_y = np.linalg.norm(y_arr)\n        # For cosine distance, norms must be non-zero\n        if norm_x == 0 or norm_y == 0:\n            return -1\n        dot_product = np.dot(x_arr, y_arr)\n        cosine_similarity = dot_product / (norm_x * norm_y)\n        # Cosine distance\n        dist = 1 - cosine_similarity\n        return round(dist, 4)\n\n    # Map metric names to functions\n    metrics_map = {\n        'euclidean': euclidean,\n        'manhattan': manhattan,\n        'chebyshev': chebyshev,\n        'cosine': cosine\n    }\n\n    def distance_func(x, y):\n        if name not in metrics_map:\n            return -1\n        return metrics_map[name](x, y)\n\n    return distance_func\n```"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n\n    # Initialize variables\n    n_actions_per_dim: List[Union[int, float]] = []\n    action_ids: Union[List[Tuple[int, ...]], None] = None\n    action_dim: int = 0\n\n    # Helper function to process a single sub-space\n    def process_subspace(space):\n        # Discrete space\n        if hasattr(space, 'n'):\n            return space.n, False  # number of actions, not continuous\n        # Continuous space\n        elif hasattr(space, 'shape'):\n            return np.inf, True\n        else:\n            raise ValueError(\"Unknown sub-space type.\")\n\n    # Determine the structure of the action space\n    if hasattr(env.action_space, 'spaces'):\n        # Multi-dimensional space with sub-spaces\n        sub_spaces = env.action_space.spaces\n        action_dim = len(sub_spaces)\n        for sub_space in sub_spaces:\n            n_actions, is_continuous = process_subspace(sub_space)\n            n_actions_per_dim.append(n_actions)\n            if is_continuous:\n                # If any dimension is continuous, set action_ids to None\n                action_ids = None\n        # If no continuous dimension, generate all possible actions\n        if action_ids is None:\n            # At least one dimension is continuous\n            pass\n        else:\n            # All dimensions are discrete, generate cartesian product\n            ranges = []\n            for n in n_actions_per_dim:\n                # For discrete, range(n)\n                ranges.append(range(n))\n            # Generate all combinations\n            action_ids = list(product(*ranges))\n    else:\n        # Single space\n        n_actions, is_continuous = process_subspace(env.action_space)\n        if md_action:\n            # Multi-dimensional space: treat as a list with one sub-space\n            action_dim = 1\n            n_actions_per_dim = [n_actions]\n            if is_continuous:\n                action_ids = None\n            else:\n                # Discrete, generate all actions\n                ranges = [range(n_actions)]\n                action_ids = list(product(*ranges))\n        else:\n            # Single-dimensional space\n            action_dim = 1\n            n_actions_per_dim = [n_actions]\n            if is_continuous:\n                action_ids = None\n            else:\n                # Discrete, generate all actions\n                action_ids = list(product(range(n_actions)))\n\n    # If the environment is multi-dimensional and not using env.action_space.spaces,\n    # but md_action is True, we assume the shape attribute\n    if hasattr(env.action_space, 'shape') and md_action:\n        shape = env.action_space.shape\n        action_dim = len(shape)\n        n_actions_per_dim = []\n        for dim_size in shape:\n            if cont_action:\n                n_actions_per_dim.append(np.inf)\n            else:\n                n_actions_per_dim.append(int(dim_size))\n        if cont_action:\n            action_ids = None\n        else:\n            ranges = [range(int(size)) for size in shape]\n            action_ids = list(product(*ranges))\n    elif hasattr(env.action_space, 'shape') and not md_action:\n        # Single shape, treat as one-dimensional\n        shape = env.action_space.shape\n        action_dim = len(shape)\n        if cont_action:\n            n_actions_per_dim = [np.inf] * len(shape)\n            action_ids = None\n        else:\n            n_actions_per_dim = [int(s) for s in shape]\n            ranges = [range(int(s)) for s in shape]\n            action_ids = list(product(*ranges))\n    # Final check: if any dimension is continuous, ensure action_ids is None\n    if any(isinstance(n, float) and np.isinf(n) for n in n_actions_per_dim):\n        action_ids = None\n\n    return n_actions_per_dim, action_ids, action_dim\n```"}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # Determine number of topics T from the shape of phi[0]\n    T = phi[0].shape[1]\n    \n    # Initialize the count matrix for \u03b2: shape (V, T)\n    beta_counts = np.zeros((V, T))\n    \n    # Accumulate counts over all documents and tokens\n    for d, doc in enumerate(corpus):\n        phi_d = phi[d]  # shape (N_d, T)\n        for n, w in enumerate(doc):\n            # Add the variational probabilities for token n in document d\n            beta_counts[w] += phi_d[n]\n    \n    # To avoid division by zero, handle columns with all zeros\n    # For columns with sum zero, leave the row as zeros\n    col_sums = beta_counts.sum(axis=0)\n    \n    # Normalize each column to sum to 1\n    # For columns with sum zero, keep zeros\n    for t in range(T):\n        if col_sums[t] > 0:\n            beta_counts[:, t] /= col_sums[t]\n        # else leave as zeros\n    \n    # Round to 4 decimal places\n    beta_rounded = np.round(beta_counts, 4)\n    \n    # Convert to list of lists\n    beta_list = beta_rounded.tolist()\n    \n    return beta_list\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n\n    n_samples = X.shape[0]\n    # Initialize medoids as the first k samples\n    medoid_indices = list(range(k))\n    medoids = X[medoid_indices]\n\n    # Function to compute total cost given current medoids\n    def total_cost(medoid_idxs):\n        cost = 0.0\n        for i in range(n_samples):\n            # Compute distances to current medoids\n            distances = np.linalg.norm(X[i] - X[medoid_idxs], axis=1)\n            cost += np.min(distances)\n        return cost\n\n    # Initial total cost\n    current_cost = total_cost(medoid_indices)\n\n    # Iterative improvement\n    while True:\n        best_swap = None\n        best_cost_reduction = 0.0\n\n        # For each current medoid\n        for m_idx in medoid_indices:\n            # For each non-medoid sample\n            non_meds = [idx for idx in range(n_samples) if idx not in medoid_indices]\n            for candidate_idx in non_meds:\n                # Propose swap: replace m_idx with candidate_idx\n                new_medoids = medoid_indices.copy()\n                new_medoids[new_medoids.index(m_idx)] = candidate_idx\n                new_cost = total_cost(new_medoids)\n                cost_reduction = current_cost - new_cost\n                if cost_reduction > best_cost_reduction:\n                    best_cost_reduction = cost_reduction\n                    best_swap = (m_idx, candidate_idx, new_medoids, new_cost)\n\n        # If no improvement, break\n        if best_swap is None:\n            break\n        else:\n            # Perform the best swap\n            m_idx, candidate_idx, medoid_indices, current_cost = best_swap\n\n    # Assign each sample to the closest medoid\n    labels = []\n    for i in range(n_samples):\n        distances = np.linalg.norm(X[i] - X[medoid_indices], axis=1)\n        min_dist_idx = np.argmin(distances)\n        labels.append(min_dist_idx)\n\n    return labels\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n\n    # Convert inputs to numpy arrays\n    X_train = np.array(x_train)\n    y_train = np.array(y_train)\n    X_test = np.array(x_test)\n\n    # Validate n_iterations\n    if n_iterations < 1:\n        return -1\n\n    # Check shape compatibility\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        return -1\n    if X_train.shape[0] != y_train.shape[0]:\n        return -1\n    if X_train.shape[1] != X_test.shape[1]:\n        return -1\n\n    # Polynomial feature expansion\n    def polynomial_expansion(X, degree):\n        # For each feature, generate polynomial features up to 'degree'\n        # Concatenate all polynomial features\n        poly_features = [np.ones((X.shape[0], 1))]  # bias term\n        for d in range(1, degree + 1):\n            poly_features.append(X ** d)\n        return np.hstack(poly_features)\n\n    X_train_poly = polynomial_expansion(X_train, degree)\n    X_test_poly = polynomial_expansion(X_test, degree)\n\n    # Standardize features (excluding bias term)\n    # Bias term is the first column (all ones), so skip it\n    feature_means = np.mean(X_train_poly[:, 1:], axis=0)\n    feature_stds = np.std(X_train_poly[:, 1:], axis=0)\n    # To avoid division by zero\n    feature_stds[feature_stds == 0] = 1\n\n    X_train_scaled = X_train_poly.copy()\n    X_test_scaled = X_test_poly.copy()\n\n    X_train_scaled[:, 1:] = (X_train_poly[:, 1:] - feature_means) / feature_stds\n    X_test_scaled[:, 1:] = (X_test_poly[:, 1:] - feature_means) / feature_stds\n\n    # Initialize weights (including bias)\n    n_features = X_train_scaled.shape[1]\n    w = np.zeros(n_features)\n\n    # Gradient descent\n    for _ in range(n_iterations):\n        # Predictions\n        y_pred = X_train_scaled @ w\n        # Error\n        error = y_pred - y_train\n        # Gradient of MSE\n        grad = (2 / X_train_scaled.shape[0]) * (X_train_scaled.T @ error)\n\n        # Regularization (excluding bias term)\n        # L2 gradient\n        l2_grad = 2 * reg_factor * l1_ratio * w\n        l2_grad[0] = 0  # no regularization for bias\n\n        # L1 sub-gradient\n        l1_subgrad = np.sign(w)\n        l1_subgrad[0] = 0  # no regularization for bias\n\n        # Total gradient\n        total_grad = grad + reg_factor * ((1 - l1_ratio) * 2 * w + l1_ratio * l1_subgrad)\n\n        # Update weights\n        w -= learning_rate * total_grad\n\n    # Make predictions on test set\n    y_test_pred = X_test_scaled @ w\n\n    # Round predictions to 4 decimals\n    rounded_preds = np.round(y_test_pred, 4).tolist()\n\n    return rounded_preds\n```"}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n\n    # Compute sigmoid of predicted scores\n    sigmoid_pred = 1 / (1 + np.exp(-predicted))\n    # Compute sigmoid of negative actual * predicted\n    sigmoid_neg_actual_pred = 1 / (1 + np.exp(-(-actual * predicted)))\n\n    # Gradient: actual * sigmoid(-actual * predicted)\n    gradient = actual * sigmoid_neg_actual_pred\n\n    # Hessian: sigmoid(predicted) * (1 - sigmoid(predicted))\n    hessian = sigmoid_pred * (1 - sigmoid_pred)\n\n    # Probabilities: sigmoid(predicted)\n    prob = sigmoid_pred\n\n    # Round all results to 6 decimal places\n    gradient = np.round(gradient, 6)\n    hessian = np.round(hessian, 6)\n    prob = np.round(prob, 6)\n\n    # Convert back to lists\n    return (gradient.tolist(), hessian.tolist(), prob.tolist())\n```"}
{"task_id": 218, "completion_id": 0, "completion": "Let's carefully implement the `blackjack_outcome` function following the rules described.\n\n### Approach:\n\n1. **Calculate hand value**:\n   - Sum the card values.\n   - Check for the presence of an Ace (1).\n   - If there's an Ace and adding 10 doesn't bust, count the Ace as 11 (usable ace).\n   - The final score is the best possible value \u2264 21.\n\n2. **Determine if a hand is a blackjack**:\n   - Exactly two cards.\n   - One Ace and one 10-value card.\n\n3. **Apply the rules**:\n   - If the player busts, return -1.\n   - If the dealer busts (and player hasn't), return +1.\n   - Otherwise, compare scores:\n     - Higher score: +1 for player, -1 for dealer.\n     - Tie: 0.\n   - If the player wins with a natural blackjack, return 1.5.\n\nLet's implement helper functions for clarity:\n- `hand_value(hand: list[int]) -> int`: computes the best score.\n- `is_blackjack(hand: list[int]) -> bool`: checks if the hand is a blackjack.\n\nNow, the code:\n\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\"\"\"\n    \n    def hand_value(hand: list[int]) -> int:\n        total = sum(hand)\n        # Check for usable ace\n        if 1 in hand:\n            # If adding 10 doesn't bust, count Ace as 11\n            if total + 10 <= 21:\n                return total + 10\n        return total\n\n    def is_blackjack(hand: list[int]) -> bool:\n        return len(hand) == 2 and 1 in hand and any(card == 10 for card in hand)\n\n    # Calculate scores\n    player_score = hand_value(player)\n    dealer_score = hand_value(dealer)\n\n    # Check for busts\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n\n    # Check for blackjacks\n    player_blackjack = is_blackjack(player)\n    dealer_blackjack = is_blackjack(dealer)\n\n    # Apply rules\n    if player_bust:\n        return -1\n    if dealer_bust:\n        return 1\n    # Both not bust\n    if player_score > dealer_score:\n        # Check for player's natural blackjack\n        if player_blackjack:\n            return 1.5\n        else:\n            return 1\n    elif player_score < dealer_score:\n        return -1\n    else:\n        # Tie\n        return 0\n```\n\n### Explanation:\n- The `hand_value` function correctly accounts for the usable ace.\n- The `is_blackjack` function checks for the special two-card blackjack.\n- The main logic applies the rules in order, considering busts, blackjacks, and score comparisons.\n\nThis implementation should correctly evaluate the outcome according to the specified rules."}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Return element-wise squared logarithmic errors.\"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    # Compute log(1 + actual) and log(1 + predicted)\n    log_actual = np.log1p(actual)\n    log_predicted = np.log1p(predicted)\n    # Calculate squared differences\n    squared_errors = (log_actual - log_predicted) ** 2\n    return squared_errors\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate MSLE, with input validation.\"\"\"\n    # Convert inputs to numpy arrays\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Validate that inputs have the same shape\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Validate that all values are non-negative\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    \n    # Compute element-wise squared log errors\n    errors = squared_log_error(actual, predicted)\n    \n    # Calculate mean and round to 4 decimal places\n    msle = np.mean(errors)\n    return round(float(msle), 4)\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Convert input to numpy array for vectorized operations\n    z_array = np.array(z, dtype=float)\n\n    # Initialize an array to hold the sigmoid values\n    sigmoid_vals = np.empty_like(z_array)\n\n    # For z >= 0: use the standard form for numerical stability\n    mask_pos = z_array >= 0\n    sigmoid_vals[mask_pos] = 1 / (1 + np.exp(-z_array[mask_pos]))\n\n    # For z < 0: use an alternative form to avoid overflow\n    mask_neg = ~mask_pos\n    exp_z = np.exp(z_array[mask_neg])\n    sigmoid_vals[mask_neg] = exp_z / (1 + exp_z)\n\n    # Determine if input was scalar\n    if np.isscalar(z):\n        # Return a single float rounded to 4 decimal places\n        return round(float(sigmoid_vals), 4)\n    else:\n        # Return a list with elements rounded to 4 decimal places\n        return sigmoid_vals.tolist()\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    # Convert input to numpy array for easy processing\n    arr = np.array(z)\n    # Apply Leaky ReLU: x if x >= 0 else a * x\n    activated = np.where(arr >= 0, arr, a * arr)\n    # Convert back to list with the same nested structure\n    return activated.tolist()\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n\n    n_samples, n_features = X_train.shape\n\n    # Initialize sample weights uniformly\n    sample_weights = np.ones(n_samples) / n_samples\n\n    # Store the weak learners and their weights\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize variables to store the best stump\n        min_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n\n        # Loop over all features to find the best decision stump\n        for feature_i in range(n_features):\n            feature_values = X_train[:, feature_i]\n            # Get all possible thresholds (midpoints between sorted unique values)\n            thresholds = np.unique(feature_values)\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions based on current threshold and polarity\n                    predictions = np.ones(n_samples)\n                    predictions[polarity * feature_values < polarity * threshold] = -1\n\n                    # Calculate weighted error\n                    misclassified = predictions != y_train\n                    error = np.sum(sample_weights * misclassified)\n\n                    if error < min_error:\n                        min_error = error\n                        best_feature = feature_i\n                        best_threshold = threshold\n                        best_polarity = polarity\n\n        # Compute alpha (learner weight)\n        # Add a small epsilon to avoid division by zero\n        epsilon = 1e-10\n        alpha = 0.5 * np.log((1 - min_error + epsilon) / (min_error + epsilon))\n\n        # Store the current stump parameters and alpha\n        classifiers.append({\n            'feature_index': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'alpha': alpha\n        })\n\n        # Make predictions with the current stump\n        feature_values = X_train[:, best_feature]\n        predictions = np.ones(n_samples)\n        predictions[best_polarity * feature_values < best_polarity * best_threshold] = -1\n\n        # Update sample weights\n        # Increase weights for misclassified samples\n        sample_weights *= np.exp(-alpha * y_train * predictions)\n        # Normalize weights\n        sample_weights /= np.sum(sample_weights)\n\n    # After training, make predictions on X_test\n    final_predictions = np.zeros(X_test.shape[0])\n    for clf in classifiers:\n        feature_values = X_test[:, clf['feature_index']]\n        preds = np.ones(X_test.shape[0])\n        preds[clf['polarity'] * feature_values < clf['polarity'] * clf['threshold']] = -1\n        final_predictions += clf['alpha'] * preds\n\n    # Final prediction is the sign of the weighted sum\n    return [1 if pred >= 0 else -1 for pred in final_predictions]\n```"}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    # Check for shape length\n    if len(shape) < 2:\n        return -1\n\n    # Flatten the shape to a 2D matrix\n    d0 = shape[0]\n    rest = np.prod(shape[1:])  # product of remaining dimensions\n    A = np.random.randn(d0, rest)\n\n    # Compute SVD\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # Choose the orthogonal matrix Q\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n\n    # Reshape Q back to the original shape\n    Q_reshaped = Q.reshape(shape)\n\n    # Scale the matrix\n    Q_scaled = Q_reshaped * scale\n\n    # Convert to list of lists\n    return Q_scaled.tolist()\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert input state to numpy array for convenience\n    s = np.array(state)\n\n    # Since all weights are 1.0 and biases are 0.0, we can compute directly:\n    # Step 1: Dense-1 with ReLU\n    z1 = max(0, s.sum())  # sum of s, then ReLU\n    # Step 2: Dense-2 with ReLU\n    z2 = 4 * z1\n    # Step 3: Dense-3 with ReLU\n    z3 = 4 * z2  # equals 16 * z1\n    # Step 4: Actor head logits\n    logits = 4 * z3  # equals 64 * z1\n\n    # Since all logits are identical, softmax over identical values yields equal probabilities\n    action_probs = [1/3, 1/3, 1/3]\n\n    # State value\n    state_value = 4 * z3  # equals 64 * z1\n\n    # Round to 4 decimal places\n    action_probs_rounded = [round(prob, 4) for prob in action_probs]\n    state_value_rounded = round(state_value, 4)\n\n    return action_probs_rounded, state_value_rounded\n```"}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert input lists to numpy arrays\n    x = np.array(state)\n\n    # Extract parameters\n    W1, b1 = np.array(params['W1']), np.array(params['b1'])\n    W2, b2 = np.array(params['W2']), np.array(params['b2'])\n    W3, b3 = np.array(params['W3']), np.array(params['b3'])\n    Wa, ba = np.array(params['Wa']), np.array(params['ba'])\n    Wc, bc = np.array(params['Wc']), np.array(params['bc'])\n\n    # Forward pass through shared layers\n    h1 = np.maximum(0, W1 @ x + b1)  # ReLU\n    h2 = np.maximum(0, W2 @ h1 + b2) # ReLU\n    h3 = np.maximum(0, W3 @ h2 + b3) # ReLU\n\n    # Actor head: affine + softmax\n    actor_logits = Wa @ h3 + ba\n    # To ensure numerical stability in softmax, subtract max\n    max_logit = np.max(actor_logits)\n    exp_logits = np.exp(actor_logits - max_logit)\n    probs = exp_logits / np.sum(exp_logits)\n\n    # Critic head: affine\n    value = Wc @ h3 + bc\n    value_scalar = float(value)\n\n    # Round probabilities and value to 4 decimal places\n    probs_rounded = [float(np.round(p, 4)) for p in probs]\n    # If any probability is exactly 0 or 1, keep one decimal place\n    probs_final = [p if p not in [0.0, 1.0] else float(np.round(p, 1)) for p in probs_rounded]\n    value_final = float(np.round(value_scalar, 4))\n    if value_final in [0.0, 1.0]:\n        value_final = float(np.round(value_final, 1))\n\n    return probs_final, value_final\n```"}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.array(w, dtype=float)\n    \n    if not gradient:\n        # Compute L2 norm\n        l2_norm = np.linalg.norm(w, ord=2)\n        penalty = alpha * (l1_ratio * l2_norm + (1 - l1_ratio) * 0.5 * np.dot(w, w))\n        return round(penalty, 4)\n    else:\n        # Compute gradient\n        sign_w = np.sign(w)\n        grad = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        # Round each element to 4 decimals\n        grad_rounded = [round(elem, 4) for elem in grad]\n        return grad_rounded\n```"}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n\n    # Handle special parameter names\n    if param_name in [\"X\", \"y\"]:\n        return None\n\n    # Map \"Ba\" and \"Bx\" to lowercase keys\n    param_key = param_name.lower()\n\n    # Access the parameter\n    param = model.parameters[param_key]\n\n    # Initialize gradient array with same shape as param\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    # Save original parameter for restoration\n    original_param = param.astype(np.float64)\n\n    # Iterate over every element in the parameter\n    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        idx = it.multi_index\n\n        # Save original value at index\n        original_value = param[idx]\n\n        # Perturb +epsilon\n        param[idx] = original_value + epsilon\n        model.flush_gradients()\n        predictions_plus = []\n\n        for t in range(n_t):\n            # Forward pass for each time step\n            pred = model.forward(X[:, :, t])\n            predictions_plus.append(pred)\n        loss_plus = loss_func(predictions_plus)\n\n        # Perturb -epsilon\n        param[idx] = original_value - epsilon\n        model.flush_gradients()\n        predictions_minus = []\n\n        for t in range(n_t):\n            pred = model.forward(X[:, :, t])\n            predictions_minus.append(pred)\n        loss_minus = loss_func(predictions_minus)\n\n        # Numerical gradient approximation\n        grad_estimate = (loss_plus - loss_minus) / (2 * epsilon)\n        grads[idx] = grad_estimate\n\n        # Restore original value\n        param[idx] = original_value\n\n        it.iternext()\n\n    # Return the transpose of grads\n    return grads.T\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n\n    m, n = X_train.shape\n    # Initialize sample weights uniformly\n    D = np.ones(m) / m\n\n    # Store the weak classifiers: each as a dict with 'feature_index', 'threshold', 'polarity', 'alpha'\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize variables to store the best stump\n        min_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = 1\n\n        # For each feature, find the best threshold\n        for feature_i in range(n):\n            feature_values = X_train[:, feature_i]\n            # Get unique values to consider as thresholds\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions based on current threshold and polarity\n                    predictions = np.ones(m)\n                    predictions[polarity * feature_values < polarity * threshold] = -1\n\n                    # Calculate weighted error\n                    misclassified = predictions != y_train\n                    error = np.sum(D * misclassified)\n\n                    if error < min_error:\n                        min_error = error\n                        best_feature = feature_i\n                        best_threshold = threshold\n                        best_polarity = polarity\n\n        # Compute alpha (classifier weight)\n        # To avoid division by zero, clip error\n        epsilon = 1e-10\n        min_error = max(min_error, epsilon)\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n\n        # Store the classifier parameters\n        classifiers.append({\n            'feature_index': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'alpha': alpha\n        })\n\n        # Make predictions on training data with the selected stump\n        feature_values = X_train[:, best_feature]\n        predictions = np.ones(m)\n        predictions[best_polarity * feature_values < best_polarity * best_threshold] = -1\n\n        # Update sample weights\n        D *= np.exp(-alpha * y_train * predictions)\n        D /= np.sum(D)  # Normalize\n\n    # Now, predict on X_test\n    final_predictions = np.zeros(X_test.shape[0])\n    for clf in classifiers:\n        feature_values = X_test[:, clf['feature_index']]\n        preds = np.ones(X_test.shape[0])\n        preds[clf['polarity'] * feature_values < clf['polarity'] * clf['threshold']] = -1\n        final_predictions += clf['alpha'] * preds\n\n    # Final prediction is sign of weighted sum\n    return [1 if pred >= 0 else -1 for pred in final_predictions]\n```"}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    # Ensure shape has at least two dimensions\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least two dimensions.\")\n\n    # Compute fan_in and fan_out\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    else:\n        # For shapes with more than 2 dimensions\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n\n    # Calculate standard deviation\n    s = np.sqrt(2.0 / (fan_in + fan_out))\n\n    # Generate the array with normal distribution\n    return np.random.normal(loc=0.0, scale=s, size=shape)\n```"}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Create a mapping from vertex to its index for quick lookup\n    index_map = {vertex: idx for idx, vertex in enumerate(V)}\n    \n    # Initialize adjacency list with empty lists\n    G = [[] for _ in V]\n    \n    # For each edge, add the connected vertices to each other's adjacency list\n    for u, v in E:\n        # Check if both vertices are in V\n        if u in index_map and v in index_map:\n            u_idx = index_map[u]\n            v_idx = index_map[v]\n            # Add v to u's adjacency list if not already present\n            if v not in G[u_idx]:\n                G[u_idx].append(v)\n            # Add u to v's adjacency list if not already present\n            if u not in G[v_idx]:\n                G[v_idx].append(u)\n    \n    # Ensure the neighbors are ordered according to their appearance in V\n    # Since we added neighbors in the order of edges, and V order is preserved,\n    # the order inside each adjacency list is already consistent.\n    # But to be safe, we can filter neighbors to only those in V and in the correct order.\n    # (This is optional if edges are consistent, but good for robustness.)\n    for i, neighbors in enumerate(G):\n        # Filter neighbors to only those in V (should always be true)\n        neighbors_in_order = [n for n in V if n in neighbors]\n        G[i] = neighbors_in_order\n    \n    return G\n```"}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n    min_error = float('inf')\n    best_stump = {\n        \"feature_index\": None,\n        \"threshold\": None,\n        \"polarity\": None,\n        \"weighted_error\": None\n    }\n\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        # Get all unique feature values to consider as thresholds\n        thresholds = np.unique(feature_values)\n\n        for threshold in thresholds:\n            for polarity in [1, -1]:\n                # Make predictions based on current threshold and polarity\n                # For polarity=1:\n                #   predict 1 if x < threshold, else -1\n                # For polarity=-1:\n                #   predict -1 if x < threshold, else 1\n                predictions = np.ones(n_samples)\n                if polarity == 1:\n                    predictions[feature_values >= threshold] = -1\n                else:\n                    predictions[feature_values < threshold] = -1\n\n                # Compute weighted error\n                misclassified = predictions != y\n                weighted_error = np.sum(sample_weights[misclassified])\n\n                # Check if this is the best so far\n                if weighted_error < min_error:\n                    min_error = weighted_error\n                    best_stump[\"feature_index\"] = feature_idx\n                    best_stump[\"threshold\"] = round(threshold, 4)\n                    best_stump[\"polarity\"] = polarity\n                    best_stump[\"weighted_error\"] = round(weighted_error, 4)\n\n    return best_stump\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier processing\n    feature = np.array(feature)\n    target = np.array(target)\n\n    # If all feature values are identical, no split is possible\n    unique_values = np.unique(feature)\n    if len(unique_values) == 1:\n        return (None, 0.0)\n\n    # Compute the entropy of the parent node\n    def entropy(labels):\n        total = len(labels)\n        counts = Counter(labels)\n        ent = 0.0\n        for count in counts.values():\n            p = count / total\n            ent -= p * math.log2(p)\n        return ent\n\n    parent_entropy = entropy(target)\n    n = len(target)\n\n    # Generate candidate thresholds: midpoints between consecutive distinct feature values\n    sorted_indices = np.argsort(feature)\n    sorted_feature = feature[sorted_indices]\n    sorted_target = target[sorted_indices]\n\n    thresholds = []\n    for i in range(len(sorted_feature) - 1):\n        if sorted_feature[i] != sorted_feature[i + 1]:\n            t = (sorted_feature[i] + sorted_feature[i + 1]) / 2\n            thresholds.append(t)\n\n    # Initialize variables to track the best split\n    max_info_gain = -1\n    best_threshold = None\n\n    for t in thresholds:\n        # Split data\n        left_mask = feature < t\n        right_mask = ~left_mask\n\n        # If either side is empty, skip\n        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n            continue\n\n        # Compute entropies for left and right splits\n        left_labels = target[left_mask]\n        right_labels = target[right_mask]\n\n        H_left = entropy(left_labels)\n        H_right = entropy(right_labels)\n\n        # Compute weighted average entropy\n        weight_left = len(left_labels) / n\n        weight_right = len(right_labels) / n\n        info_loss = weight_left * H_left + weight_right * H_right\n\n        # Compute information gain\n        info_gain = parent_entropy - info_loss\n\n        # Update best split if needed\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_threshold = t\n        elif math.isclose(info_gain, max_info_gain, rel_tol=1e-9):\n            # Tie: choose the smaller threshold\n            if best_threshold is None or t < best_threshold:\n                best_threshold = t\n\n    # If no positive information gain, return (None, 0.0)\n    if max_info_gain <= 0:\n        return (None, 0.0)\n\n    # Round results to 4 decimal places\n    threshold_rounded = round(best_threshold, 4)\n    info_gain_rounded = round(max_info_gain, 4)\n\n    return (threshold_rounded, info_gain_rounded)\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    rows, cols = shape\n    rng = random.Random(seed)\n    return [[rng.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    # Look for the object in the global namespace\n    obj = globals().get(name)\n    # Check if the object exists and is callable\n    if callable(obj):\n        return obj\n    else:\n        raise ValueError(\"Invalid initialization function.\")\n```"}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    # Calculate output dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n\n    # Initialize pooled output\n    pooled = np.zeros((N, C, out_h, out_w))\n    # Initialize gradient input\n    grad_input = np.zeros_like(X)\n\n    # Forward pass: compute pooled output\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    h_start = i * s_h\n                    w_start = j * s_w\n                    window = X[n, c, h_start:h_start + p_h, w_start:w_start + p_w]\n                    pooled[n, c, i, j] = np.mean(window)\n\n    # Backward pass: distribute gradients\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    h_start = i * s_h\n                    w_start = j * s_w\n                    window_grad = accum_grad[n, c, i, j] / (p_h * p_w)\n                    grad_input[n, c, h_start:h_start + p_h, w_start:w_start + p_w] += window_grad\n\n    # Round to 4 decimal places\n    pooled_rounded = np.round(pooled, 4)\n    grad_input_rounded = np.round(grad_input, 4)\n\n    # Convert to lists\n    return pooled_rounded.tolist(), grad_input_rounded.tolist()\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Both are leaves\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value)\n    \n    # Both are internal nodes\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Check feature indices\n        if tree_a.feature != tree_b.feature:\n            return False\n        # Check thresholds with tolerance\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        # Recursively compare left and right subtrees\n        left_equal = compare_trees(tree_a.left, tree_b.left)\n        right_equal = compare_trees(tree_a.right, tree_b.right)\n        return left_equal and right_equal\n    \n    # One is leaf, the other is node - not equivalent\n    return False\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert input lists to numpy arrays for easy slicing\n    p1 = np.array(parent1)\n    p2 = np.array(parent2)\n    \n    # Validate shape\n    if p1.shape != p2.shape:\n        return -1\n    \n    # Determine the number of columns\n    num_cols = p1.shape[1]\n    \n    # Validate cutoff\n    if not (0 <= cutoff <= num_cols):\n        # If cutoff is out of bounds, treat as invalid input\n        return -1\n    \n    # Create children matrices\n    # Columns before cutoff are from respective parents\n    child1 = np.hstack((p1[:, :cutoff], p2[:, cutoff:]))\n    child2 = np.hstack((p2[:, :cutoff], p1[:, cutoff:]))\n    \n    # Convert back to lists of lists\n    return (child1.tolist(), child2.tolist())\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    is_directed = G.get('is_directed', False)\n    V2I = G.get('_V2I', {})\n    G_list = G.get('_G', [])\n\n    # Initialize adjacency dictionary with all vertices, even isolated ones\n    adj: Dict[Any, List[Tuple[Any, float]]] = {label: [] for label in V2I}\n\n    # To avoid duplicate edges in undirected graphs, keep track of processed edges\n    processed_edges = set()\n\n    for edges in G_list:\n        for source_label, target_label, weight in edges:\n            # Add edge from source to target\n            adj[source_label].append((target_label, weight))\n            if not is_directed:\n                # For undirected graphs, add the reverse edge if not already added\n                edge_id = tuple(sorted([source_label, target_label]))\n                if edge_id not in processed_edges:\n                    adj[target_label].append((source_label, weight))\n                    processed_edges.add(edge_id)\n\n    # Sort neighbor lists alphabetically by neighbor label\n    for vertex in adj:\n        adj[vertex].sort(key=lambda x: x[0])\n\n    return adj\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Convert input to a NumPy array if it isn't already\n    arr = np.asarray(x)\n    \n    # For boolean arrays, True and False are acceptable (equivalent to 1 and 0)\n    # For other types, check if all elements are either 0 or 1\n    # Use np.isin to check if elements are in [0, 1]\n    return np.all(np.isin(arr, [0, 1]))\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n\n    # Special tokens\n    BOL = \"<bol>\"\n    EOL = \"<eol>\"\n\n    # Tokenize corpus and query\n    corpus_tokens = []\n    for sentence in corpus:\n        tokens = sentence.strip().split()\n        corpus_tokens.append(tokens)\n\n    query_tokens = query.strip().split()\n\n    # Handle edge case: N=1 (unigrams)\n    # For N=1, no padding is needed\n    # For N>1, pad each sentence with N-1 <bol> tokens at start and one <eol> at end\n    if N == 1:\n        # For unigrams, just count tokens\n        total_tokens = 0\n        unigram_counts = Counter()\n        for tokens in corpus_tokens:\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n    else:\n        # For N>1, pad sentences\n        padded_corpus = []\n        for tokens in corpus_tokens:\n            padded_tokens = [BOL] * (N - 1) + tokens + [EOL]\n            padded_corpus.append(padded_tokens)\n\n        # Count N-grams and (N-1)-grams\n        ngram_counts = Counter()\n        prefix_counts = Counter()\n\n        for tokens in padded_corpus:\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i:i+N])\n                prefix = tuple(tokens[i:i+N-1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n\n        # For query, pad similarly\n        padded_query = [BOL] * (N - 1) + query_tokens + [EOL]\n        # Generate N-grams from query\n        query_ngrams = []\n        for i in range(len(padded_query) - N + 1):\n            ngram = tuple(padded_query[i:i+N])\n            query_ngrams.append(ngram)\n\n        # Compute total count for unigrams (for N=1)\n        total_tokens = sum(ngram_counts.values())\n\n    # For N=1, counts are stored differently\n    if N == 1:\n        # For unigrams, total tokens and counts are available\n        # Compute probabilities for each token in query\n        total_count = total_tokens\n        # For each token in query, get count\n        log_prob_sum = 0.0\n        for token in query_tokens:\n            count = unigram_counts.get(token, 0)\n            if count == 0:\n                return float('-inf')\n            prob = count / total_count\n            log_prob_sum += math.log(prob)\n        return round(log_prob_sum, 4)\n\n    else:\n        # For N>1, compute probabilities for each N-gram in query\n        log_prob_sum = 0.0\n        for ngram in query_ngrams:\n            prefix = ngram[:-1]\n            count_ngram = ngram_counts.get(ngram, 0)\n            count_prefix = prefix_counts.get(prefix, 0)\n            if count_ngram == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n            log_prob_sum += math.log(prob)\n\n        return round(log_prob_sum, 4)\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Step 1: Build the weighted adjacency matrix W\n    W = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(i+1, n_samples):\n            dist = np.linalg.norm(X[i] - X[j])\n            weight = 1.0 / (1.0 + dist)\n            W[i, j] = weight\n            W[j, i] = weight\n    # Diagonal remains zero\n    \n    # Step 2: Compute the unnormalized graph Laplacian L = D - W\n    D = np.diag(np.sum(W, axis=1))\n    L = D - W\n    \n    # Step 3: Compute the first k eigenvectors of L\n    # Since L is symmetric, eigenvalues are real\n    # We'll implement a simple eigen decomposition using numpy's eigvals and eigvecs\n    # For small datasets, this is acceptable\n    eigvals, eigvecs = np.linalg.eigh(L)\n    # eigvals are sorted in ascending order\n    # Select the first k eigenvectors\n    E = eigvecs[:, :k]\n    \n    # Step 4: Run k-means on rows of E\n    # Initialize centroids with the first k rows of E\n    centroids = E[:k, :].copy()\n    labels = np.zeros(n_samples, dtype=int)\n    max_iterations = 100\n    \n    for iteration in range(max_iterations):\n        # Assign each point to the nearest centroid\n        new_labels = np.zeros(n_samples, dtype=int)\n        for i in range(n_samples):\n            distances = np.linalg.norm(E[i] - centroids, axis=1)\n            new_labels[i] = np.argmin(distances)\n        \n        # Check for convergence\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n        \n        # Update centroids\n        for cluster_idx in range(k):\n            cluster_points = E[labels == cluster_idx]\n            if len(cluster_points) == 0:\n                # Re-initialize centroid with a random row of E\n                rand_idx = np.random.randint(0, n_samples)\n                centroids[cluster_idx] = E[rand_idx]\n            else:\n                centroids[cluster_idx] = np.mean(cluster_points, axis=0)\n    \n    # Convert labels to list\n    return list(labels)\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Determine previous index (clamped at 0)\n    prev_idx = max(ix - 1, 0)\n\n    # Extract previous and current info\n    prev_mine, prev_label = params[prev_idx]\n    curr_mine, curr_label = params[ix]\n\n    # Build the formatted string\n    debug_str = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n\"\n        f\"{prev_mine}\\n\\n\"\n        f\"Theirs (prev) [{prev_label}]:\\n\"\n        f\"{golds[prev_label]}\\n\\n\"\n        f\"Mine [{curr_label}]:\\n\"\n        f\"{curr_mine}\\n\\n\"\n        f\"Theirs [{curr_label}]:\\n\"\n        f\"{golds[curr_label]}{warn_str}\\n\"\n        \"----------------------- END DEBUG -----------------------\"\n    )\n\n    return debug_str\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (unused here).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused here).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar, a sequence (diagonal),\n           or a 2D array.\n        fit_intercept: If True, prepend a column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Add intercept if needed\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n    \n    N, M = X.shape\n\n    # Handle prior mean mu\n    mu = np.array(mu)\n    if mu.ndim == 0:\n        mu = np.full(M, mu)\n    elif mu.ndim == 1:\n        if mu.shape[0] != M:\n            raise ValueError(\"Prior mean mu has incompatible shape.\")\n    else:\n        raise ValueError(\"Prior mean mu must be scalar or 1D array.\")\n\n    # Handle prior scale V\n    if V is None:\n        V_mat = np.eye(M)\n    elif np.isscalar(V):\n        V_mat = V * np.eye(M)\n    else:\n        V = np.array(V)\n        if V.ndim == 1:\n            V_mat = np.diag(V)\n        elif V.ndim == 2:\n            V_mat = V\n        else:\n            raise ValueError(\"V must be None, scalar, 1D, or 2D array.\")\n\n    # Compute V_inv\n    V_inv = np.linalg.inv(V_mat)\n\n    # Compute posterior covariance matrix (up to \u03c3\u00b2)\n    # \u03a3_b = (V^{-1} + X^T X)^{-1}\n    Sigma_b = np.linalg.inv(V_inv + X.T @ X)\n\n    # Compute posterior mean\n    mu_b = Sigma_b @ (V_inv @ mu + X.T @ y)\n\n    # Round to 4 decimal places\n    mu_b_rounded = np.round(mu_b, 4)\n\n    return mu_b_rounded.tolist()\n```"}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    # Set default max_freq if not provided\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # Helper functions for conversions\n    def hz_to_mel(f):\n        return 2595 * np.log10(1 + f / 700)\n\n    def mel_to_hz(m):\n        return 700 * (10**(m / 2595) - 1)\n\n    # Convert frequency limits to Mel scale\n    min_mel = hz_to_mel(min_freq)\n    max_mel = hz_to_mel(max_freq)\n\n    # Generate n_filters + 2 points evenly spaced in Mel scale\n    mel_points = np.linspace(min_mel, max_mel, n_filters + 2)\n\n    # Convert Mel points back to Hz to get the filter edges\n    freq_points = mel_to_hz(mel_points)\n\n    # Compute the bin frequencies for the DFT\n    # The DFT bins are at k * fs / N for k in [0, N//2]\n    bin_freqs = np.linspace(0, fs / 2, N // 2 + 1)\n\n    # Initialize the filterbank matrix\n    filterbank = []\n\n    for i in range(n_filters):\n        # Initialize filter row with zeros\n        filter_row = np.zeros(N // 2 + 1)\n\n        # Define the left, center, and right frequencies of the triangle\n        f_left = freq_points[i]\n        f_center = freq_points[i + 1]\n        f_right = freq_points[i + 2]\n\n        # For each bin, compute the weight\n        for k, freq in enumerate(bin_freqs):\n            # Compute the slopes\n            left_slope = (freq - f_left) / (f_center - f_left) if f_center != f_left else 0\n            right_slope = (f_right - freq) / (f_right - f_center) if f_right != f_center else 0\n\n            # The filter value is the positive minimum of the two slopes\n            weight = max(0, min(left_slope, right_slope))\n            filter_row[k] = weight\n\n        # Normalize if required\n        if normalize:\n            # Compute the scale factor\n            scale = 2 / (f_center - f_left) if (f_center - f_left) != 0 else 0\n            filter_row = filter_row * scale\n\n        # Round to 4 decimal places\n        filterbank.append([round(w, 4) for w in filter_row])\n\n    return filterbank\n```"}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # Coefficients for the Blackman-Harris window\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n\n    # Handle special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Determine N based on symmetric or periodic\n    if symmetric:\n        N = window_len - 1\n    else:\n        N = window_len\n\n    # Generate n array\n    n = np.arange(0, window_len)\n\n    # For periodic, generate with length N+1 and discard last\n    if not symmetric:\n        n_periodic = np.arange(0, window_len + 1)\n        # Compute window with length N+1\n        window = (a0\n                  - a1 * np.cos(2 * np.pi * n_periodic / N)\n                  + a2 * np.cos(4 * np.pi * n_periodic / N)\n                  - a3 * np.cos(6 * np.pi * n_periodic / N))\n        # Discard last element to make it periodic\n        window = window[:-1]\n    else:\n        # Symmetric case\n        window = (a0\n                  - a1 * np.cos(2 * np.pi * n / N)\n                  + a2 * np.cos(4 * np.pi * n / N)\n                  - a3 * np.cos(6 * np.pi * n / N))\n    \n    # Round to 4 decimal places\n    window_rounded = np.round(window, 4).tolist()\n    return window_rounded\n```"}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef digamma(x):\n    \"\"\"Compute the digamma function \u03c8(x) using recursion and asymptotic expansion.\"\"\"\n    # Convert input to float for safety\n    x = float(x)\n    \n    # Use recursion to shift x to a value >= 6\n    result = 0.0\n    while x < 6:\n        result -= 1.0 / x\n        x += 1.0\n    \n    # Apply asymptotic expansion for large x\n    # \u03c8(x) \u2248 ln x - 1/(2x) - 1/(12x^2) + 1/(120x^4) - 1/(252x^6)\n    inv_x = 1.0 / x\n    inv_x2 = inv_x * inv_x\n    inv_x4 = inv_x2 * inv_x2\n    inv_x6 = inv_x4 * inv_x2\n    \n    psi_approx = (math.log(x)\n                  - 0.5 * inv_x\n                  - (1.0 / 12.0) * inv_x2\n                  + (1.0 / 120.0) * inv_x4\n                  - (1.0 / 252.0) * inv_x6)\n    \n    return result + psi_approx\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\"\"\"\n    # Convert gamma to numpy array if it's a list\n    gamma_array = np.array(gamma)\n    \n    # Extract the relevant row\n    gamma_d = gamma_array[d, :]\n    \n    # Compute sum of gamma parameters for the d-th row\n    gamma_sum = np.sum(gamma_d)\n    \n    # Compute \u03c8(gamma[d,t]) and \u03c8(sum(gamma[d,:]))\n    psi_gamma_t = digamma(gamma_d[t])\n    psi_sum = digamma(gamma_sum)\n    \n    # Calculate the expectation\n    expectation = psi_gamma_t - psi_sum\n    \n    # Round to 4 decimal places\n    return round(expectation, 4)\n```"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # Determine previous index, ensuring it doesn't go below 0\n    prev_idx = max(ix - 1, 0)\n\n    # Extract previous prediction and label\n    prev_prediction, prev_label = params[prev_idx]\n    # Extract current prediction and label\n    curr_prediction, curr_label = params[ix]\n\n    # Get the gold texts for previous and current labels\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    # Build the string with the exact format\n    debug_str = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n\"\n        f\"{prev_prediction}\\n\\n\"\n        f\"Theirs (prev) [{prev_label}]:\\n\"\n        f\"{prev_gold}\\n\\n\"\n        f\"Mine [{curr_label}]:\\n\"\n        f\"{curr_prediction}\\n\\n\"\n        f\"Theirs [{curr_label}]:\\n\"\n        f\"{curr_gold}{warn_str}\\n\"\n        \"----------------------- END DEBUG -----------------------\\n\"\n    )\n\n    return debug_str\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n\n    # Initialize weights uniformly\n    weights = np.ones(n_samples) / n_samples\n\n    # Store the parameters of each weak learner\n    classifiers = []\n\n    for m in range(n_estimators):\n        # Initialize variables to store the best stump\n        min_error = float('inf')\n        best_stump = {}\n        # Loop over all features\n        for feature_idx in range(n_features):\n            feature_values = X_train[:, feature_idx]\n            # Get unique values to consider as thresholds\n            thresholds = np.unique(feature_values)\n            # To consider thresholds between unique values, we can take midpoints\n            candidate_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n            # Also consider thresholds below min and above max\n            candidate_thresholds = np.concatenate((\n                [thresholds[0] - 1],\n                candidate_thresholds,\n                [thresholds[-1] + 1]\n            ))\n\n            for threshold in candidate_thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions\n                    predictions = np.ones(n_samples)\n                    predictions[feature_values * polarity < threshold * polarity] = -1\n\n                    # Compute weighted error\n                    misclassified = predictions != y_train\n                    error = np.sum(weights * misclassified)\n\n                    if error < min_error:\n                        min_error = error\n                        best_stump = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'polarity': polarity,\n                            'predictions': predictions\n                        }\n\n        # Compute alpha\n        epsilon = min_error\n        # To avoid division by zero or log(0), clip epsilon\n        epsilon = max(epsilon, 1e-10)\n        alpha = 0.5 * math.log((1 - epsilon) / epsilon)\n\n        # Save the weak learner parameters\n        classifiers.append({\n            'feature_idx': best_stump['feature_idx'],\n            'threshold': best_stump['threshold'],\n            'polarity': best_stump['polarity'],\n            'alpha': alpha\n        })\n\n        # Update weights\n        # Predictions of the current weak learner\n        predictions = np.ones(n_samples)\n        feature_values = X_train[:, best_stump['feature_idx']]\n        predictions[feature_values * best_stump['polarity'] < best_stump['threshold'] * best_stump['polarity']] = -1\n\n        # Update weights\n        weights *= np.exp(-alpha * y_train * predictions)\n        # Normalize weights\n        weights /= np.sum(weights)\n\n    # Make final predictions on X_test\n    final_scores = np.zeros(X_test.shape[0])\n    for clf in classifiers:\n        feature_values = X_test[:, clf['feature_idx']]\n        predictions = np.ones(X_test.shape[0])\n        predictions[feature_values * clf['polarity'] < clf['threshold'] * clf['polarity']] = -1\n        final_scores += clf['alpha'] * predictions\n\n    # Final prediction is sign of the weighted sum\n    final_predictions = [1 if score >= 0 else -1 for score in final_scores]\n\n    return final_predictions\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # Validate inputs\n    if N <= 0 or fs <= 0:\n        return []\n\n    if positive_only:\n        # For positive frequencies, indices 0 to N//2\n        k = np.arange(0, N//2 + 1)\n        delta_f = fs / N\n        freqs = k * delta_f\n    else:\n        # For full spectrum, use np.fft.fftfreq\n        freqs = np.fft.fftfreq(N, d=1/fs)\n\n    # Round to 4 decimal places and convert to list\n    return np.round(freqs, 4).tolist()\n```"}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier processing\n    y_true = np.array(y_true)\n    predictions = np.array(predictions)\n    oob_mask = np.array(oob_mask)\n\n    n_samples, n_estimators = predictions.shape\n\n    # Initialize list to hold squared errors for samples with at least one OOB prediction\n    squared_errors = []\n\n    for i in range(n_samples):\n        # Find indices of trees where sample i is out-of-bag\n        oob_indices = np.where(oob_mask[i])[0]\n        if len(oob_indices) > 0:\n            # Average the OOB predictions for sample i\n            oob_preds = predictions[i, oob_indices]\n            mean_pred = np.mean(oob_preds)\n            # Compute squared error\n            se = (y_true[i] - mean_pred) ** 2\n            squared_errors.append(se)\n\n    if len(squared_errors) == 0:\n        return -1\n\n    mse = np.mean(squared_errors)\n    return round(mse, 4)\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n\n    # Step 1: Count support for each item across all transactions\n    item_counts = Counter()\n    for transaction in transactions:\n        # Count each item only once per transaction\n        unique_items = set(transaction)\n        item_counts.update(unique_items)\n\n    # Step 2: Filter items below min_support\n    frequent_items = {item for item, count in item_counts.items() if count >= min_support}\n\n    # If no items meet min_support, return just the root\n    if not frequent_items:\n        return {\"support\": 0, \"children\": {}}\n\n    # Step 3: Determine global ordering of items\n    # Sort by decreasing support, then alphabetically\n    sorted_items = sorted(\n        [item for item in item_counts if item in frequent_items],\n        key=lambda x: (-item_counts[x], x)\n    )\n\n    # Create a mapping from item to its rank in the ordering for quick lookup\n    item_order = {item: index for index, item in enumerate(sorted_items)}\n\n    # Initialize the root node\n    root = {\"support\": 0, \"children\": {}}\n\n    # Step 4: Build the FP-Tree\n    for transaction in transactions:\n        # Remove duplicates\n        unique_items = set(transaction)\n        # Keep only frequent items\n        filtered_items = [item for item in unique_items if item in frequent_items]\n        # Reorder according to global ordering\n        ordered_items = sorted(filtered_items, key=lambda x: item_order[x])\n\n        # Walk from root, updating/adding nodes\n        current_node = root\n        current_node[\"support\"] += 1  # Increment support for root\n        for item in ordered_items:\n            children = current_node[\"children\"]\n            if item not in children:\n                # Create new node\n                children[item] = {\"support\": 0, \"children\": {}}\n            # Move down the tree\n            current_node = children[item]\n            # Increment support for this node\n            current_node[\"support\"] += 1\n\n    return root\n```"}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(\n    X: list[list[int | float]] | \"np.ndarray\",\n    Y: list[list[int | float]] | \"np.ndarray\" | None = None,\n    c0: int | float = 0\n) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n\n    # Convert inputs to numpy arrays for uniform processing\n    X_np = np.array(X, dtype=float)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y, dtype=float)\n\n    # Check shape compatibility\n    if X_np.ndim != 2 or Y_np.ndim != 2:\n        return -1\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n\n    # Compute the dot product matrix\n    K = np.dot(X_np, Y_np.T) + c0\n\n    # Round to 4 decimal places and convert to list of lists\n    K_rounded = np.round(K, 4).tolist()\n\n    return K_rounded\n```"}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n\n    # Update biased first moment estimate\n    m_new = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v_new = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected first moment estimate\n    m_hat = m_new / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v_new / (1 - beta2 ** t)\n\n    # Compute the parameter update\n    # If gradient is exactly zero, param stays the same, but moments still update\n    # The update formula applies regardless\n    denom = np.sqrt(v_hat) + eps\n    param_new = param - lr * m_hat / denom\n\n    # Round results to 8 decimal places\n    param_new_list = np.round(param_new.flatten().tolist(), 8)\n    m_new_list = np.round(m_new.flatten().tolist(), 8)\n    v_new_list = np.round(v_new.flatten().tolist(), 8)\n\n    return param_new_list, m_new_list, v_new_list\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to numpy array if not already\n    rewards = np.array(rewards)\n\n    N = theta_samples.shape[0]\n    # Determine number of samples to retain\n    retain_count = int(np.floor(retain_prcnt * N))\n    # Get indices of samples sorted by rewards in descending order\n    sorted_indices = np.argsort(rewards)[::-1]\n    # Select top retain_count samples\n    elite_indices = sorted_indices[:retain_count]\n    elite_samples = theta_samples[elite_indices]\n\n    # Compute mean and variance per dimension\n    mean = np.mean(elite_samples, axis=0)\n    var = np.var(elite_samples, axis=0)\n\n    # Round to four decimal places\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n\n    return mean_rounded, var_rounded\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Convert labels to a list if it's a NumPy array or other iterable\n    labels = list(labels)\n    \n    # Handle empty input\n    if len(labels) == 0:\n        return 0.0\n    \n    # Count the frequency of each label\n    label_counts = Counter(labels)\n    total_count = len(labels)\n    \n    # If all labels are identical, entropy is 0\n    if len(label_counts) == 1:\n        return 0.0\n    \n    # Calculate the entropy\n    entropy_value = 0.0\n    for count in label_counts.values():\n        p = count / total_count\n        if p > 0:\n            entropy_value -= p * math.log2(p)\n    \n    # Round to 5 decimal places\n    return round(entropy_value, 5)\n```"}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    k_rows, k_cols, in_ch_W, out_ch = W.shape\n\n    # Check for input consistency\n    assert in_ch == in_ch_W, \"Input channels of X and W must match.\"\n\n    # Handle dilation\n    d = dilation + 1  # dilation=0 means no dilation, so effective dilation is 1\n    # Effective kernel size after dilation\n    k_rows_eff = k_rows + (k_rows - 1) * (d - 1)\n    k_cols_eff = k_cols + (k_cols - 1) * (d - 1)\n\n    # Function to compute padding sizes\n    def get_padding(pad, in_size, kernel_size, stride, dilation):\n        if isinstance(pad, str) and pad.lower() == 'same':\n            # Compute \"same\" padding\n            out_size = int(np.ceil(float(in_size) / float(stride)))\n            pad_total = max((out_size - 1) * stride + kernel_size - in_size, 0)\n            pad_before = pad_total // 2\n            pad_after = pad_total - pad_before\n            return pad_before, pad_after\n        elif isinstance(pad, int):\n            return pad, pad\n        elif isinstance(pad, tuple) and len(pad) == 2:\n            return pad[0], pad[1]\n        elif isinstance(pad, tuple) and len(pad) == 4:\n            pr1, pr2, pc1, pc2 = pad\n            return pr1, pr2, pc1, pc2\n        else:\n            # Invalid padding specification (behavior undefined as per instructions)\n            return 0, 0\n\n    # Compute padding for rows and columns\n    if isinstance(pad, str) and pad.lower() == 'same':\n        pad_top, pad_bottom = get_padding(pad, in_rows, k_rows, stride, d)\n        pad_left, pad_right = get_padding(pad, in_cols, k_cols, stride, d)\n    elif isinstance(pad, int):\n        pad_top = pad_bottom = pad_left = pad_right = pad\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            pad_top, pad_bottom = get_padding(pad[0], in_rows, k_rows, stride, d)\n            pad_left, pad_right = get_padding(pad[1], in_cols, k_cols, stride, d)\n        elif len(pad) == 4:\n            pr1, pr2, pc1, pc2 = pad\n            pad_top, pad_bottom = pr1, pr2\n            pad_left, pad_right = pc1, pc2\n        else:\n            # Invalid, default to zero\n            pad_top = pad_bottom = pad_left = pad_right = 0\n    else:\n        # Default to zero padding\n        pad_top = pad_bottom = pad_left = pad_right = 0\n\n    # Pad the input\n    X_padded = np.pad(\n        X,\n        ((0, 0),\n         (pad_top, pad_bottom),\n         (pad_left, pad_right),\n         (0, 0)),\n        mode='constant',\n        constant_values=0\n    )\n\n    # Compute output spatial dimensions\n    out_rows = ((in_rows + pad_top + pad_bottom - d * (k_rows - 1) - 1) // stride) + 1\n    out_cols = ((in_cols + pad_left + pad_right - d * (k_cols - 1) - 1) // stride) + 1\n\n    # Prepare im2col matrix\n    # For each patch, extract the receptive field considering dilation\n    # The shape of the im2col matrix: (n_ex * out_rows * out_cols, k_rows * k_cols * in_ch)\n    patches = []\n\n    for i in range(out_rows):\n        for j in range(out_cols):\n            # Calculate the start indices for the current patch\n            row_start = i * stride\n            col_start = j * stride\n\n            # Extract the patch for all examples\n            # For each example, extract the patch\n            patch = X_padded[\n                :,\n                row_start:row_start + d * (k_rows - 1) + 1: d,\n                col_start:col_start + d * (k_cols - 1) + 1: d,\n                :\n            ]  # shape: (n_ex, k_rows, k_cols, in_ch)\n\n            # Reshape to (n_ex, k_rows * k_cols * in_ch)\n            patch_reshaped = patch.reshape(n_ex, -1)\n            patches.append(patch_reshaped)\n\n    # Stack patches: shape (n_ex * out_rows * out_cols, k_rows * k_cols * in_ch)\n    im2col = np.vstack(patches)\n\n    # Reshape W to (k_rows * k_cols * in_ch, out_ch)\n    W_reshaped = W.reshape(-1, out_ch)\n\n    # Perform matrix multiplication (GEMM)\n    # For each example, compute the dot product\n    # Since im2col is (n_ex*out_rows*out_cols, kernel_size), we can do:\n    # result shape: (n_ex*out_rows*out_cols, out_ch)\n    conv_out = im2col @ W_reshaped  # shape: (n_ex*out_rows*out_cols, out_ch)\n\n    # Reshape to (n_ex, out_rows, out_cols, out_ch)\n    conv_out = conv_out.reshape(n_ex, out_rows, out_cols, out_ch)\n\n    # Convert to list of lists\n    return conv_out.tolist()\n```"}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n\n    # 1. Standardise the data by subtracting the feature-wise mean\n    mean_centered = data - np.mean(data, axis=0)\n\n    # Determine the number of features\n    n_samples, n_features = mean_centered.shape\n\n    # 2. Obtain principal directions based on the solver\n    if solver == \"svd\":\n        # Use SVD on the mean-centered data\n        # Note: np.linalg.svd returns U, S, Vh\n        U, S, Vh = np.linalg.svd(mean_centered, full_matrices=False)\n        # Directions are rows of Vh\n        directions = Vh\n        # Variance explained by each component is proportional to the squared singular values\n        # but for PCA, the variance explained is (S^2) / (n_samples - 1)\n        # We'll compute explained variance later\n    elif solver == \"eigen\":\n        # Compute covariance matrix\n        cov_matrix = np.cov(mean_centered, rowvar=False, ddof=1)\n        # Eigen-decomposition\n        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n        # Eigenvectors are columns of eigvecs\n        directions = eigvecs.T\n        # Eigenvalues correspond to variance explained\n    else:\n        raise ValueError(\"Invalid solver. Choose 'svd' or 'eigen'.\")\n\n    # 3. Sort directions by importance (descending order of variance)\n    if solver == \"svd\":\n        # Variance explained by each component is (S^2) / (n_samples - 1)\n        variances = (S ** 2) / (n_samples - 1)\n        # Get indices for sorting in descending order\n        sorted_idx = np.argsort(variances)[::-1]\n        sorted_variances = variances[sorted_idx]\n        directions = directions[sorted_idx]\n    else:\n        # For eigen, eigenvalues are variances\n        sorted_idx = np.argsort(eigvals)[::-1]\n        sorted_variances = eigvals[sorted_idx]\n        directions = directions[sorted_idx]\n\n    # Keep only the first n_components (or all if n_components > features)\n    n_components = min(n_components, n_features)\n    directions = directions[:n_components]\n    variances = sorted_variances[:n_components]\n\n    # 4. Make the sign of each direction deterministic\n    for i in range(n_components):\n        direction = directions[i]\n        # Find the first non-zero loading\n        non_zero_idx = np.argmax(np.abs(direction) > 1e-12)\n        if direction[non_zero_idx] < 0:\n            directions[i] = -direction\n\n    # 5. Project the data onto the retained directions\n    scores = np.dot(mean_centered, directions.T)\n\n    # Round scores to 4 decimals\n    scores = np.round(scores, 4)\n\n    # 6. Calculate explained variance ratio\n    total_variance = np.sum(variances)\n    explained_variance_ratio = (variances / total_variance) if total_variance != 0 else np.zeros_like(variances)\n    explained_variance_ratio = np.round(explained_variance_ratio, 4).tolist()\n\n    # Convert scores to list of lists\n    scores_list = scores.tolist()\n\n    return scores_list, explained_variance_ratio\n```"}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # For classification, find the index of the maximum probability\n        return int(leaf.value.index(max(leaf.value)))\n    else:\n        # For regression, return the scalar value directly\n        return leaf.value\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    n_samples, n_features = data.shape\n\n    # Handle the case when K=1: centroid is the mean of all data\n    if K == 1:\n        centroid = np.mean(data, axis=0)\n        # Sort the centroid lex order (only one centroid)\n        sorted_centroid = np.round(centroid, 4).tolist()\n        return [sorted_centroid]\n\n    # Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n\n    for iteration in range(max_iters):\n        # Compute distances from each point to each centroid\n        # Using broadcasting for efficiency\n        # distances shape: (n_samples, K)\n        distances = np.linalg.norm(data[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n\n        # Assign each sample to the closest centroid\n        labels = np.argmin(distances, axis=1)\n\n        new_centroids = np.empty_like(centroids)\n        centroids_changed = False\n\n        for k in range(K):\n            # Get all samples assigned to cluster k\n            cluster_points = data[labels == k]\n            if len(cluster_points) == 0:\n                # Keep centroid unchanged if cluster is empty\n                new_centroids[k] = centroids[k]\n            else:\n                # Compute mean of assigned points\n                new_centroids[k] = np.mean(cluster_points, axis=0)\n\n        # Check for convergence: if centroids do not change more than tolerance\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n\n        # Check if any centroid has changed\n        if not np.allclose(centroids, new_centroids, atol=1e-6):\n            centroids_changed = True\n\n        centroids = new_centroids\n\n        if not centroids_changed:\n            # No change in centroids, early stop\n            break\n\n    # Sort the final centroids lexicographically\n    sorted_centroids = sorted(centroids.tolist())\n\n    # Round each coordinate to four decimals\n    rounded_centroids = [list(map(lambda x: round(x, 4), centroid)) for centroid in sorted_centroids]\n\n    return rounded_centroids\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert input to a numpy array of floats\n    arr = np.array(data, dtype=float)\n    \n    # Compute the sum of each row\n    row_sums = arr.sum(axis=1)\n    \n    # Check if any row sums to zero\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row by its sum\n    normalized = arr / row_sums[:, np.newaxis]\n    \n    # Round to 4 decimal places\n    rounded = np.round(normalized, 4)\n    \n    # Convert to list of lists\n    return rounded.tolist()\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n\n    def check_space(space: Space, is_tuple: bool) -> bool:\n        if is_tuple:\n            # For tuple/dict spaces, check all sub-spaces\n            if isinstance(space, Tuple):\n                return all(check_space(sub_space, False) for sub_space in space.spaces)\n            elif isinstance(space, Dict):\n                return all(check_space(sub_space, False) for sub_space in space.spaces.values())\n            else:\n                # If space is not a Tuple or Dict but is expected to be composite, treat as non-continuous\n                return False\n        else:\n            # For non-tuple spaces, check if it's a Box\n            return isinstance(space, Box)\n\n    cont_action = check_space(env.action_space, tuple_action)\n    cont_obs = check_space(env.observation_space, tuple_obs)\n\n    return (cont_action, cont_obs)\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    # Initialize running prediction with zeros, shape matches the first update\n    running_pred = np.zeros_like(updates[0])\n\n    # Aggregate updates: subtract scaled tree predictions\n    for u in updates:\n        running_pred -= learning_rate * u\n\n    if regression:\n        # For regression, return the predictions rounded to 4 decimals\n        return np.round(running_pred, 4)\n    else:\n        # For classification, convert logits to probabilities using softmax\n        # To ensure numerical stability, subtract max logits per sample\n        max_logits = np.max(running_pred, axis=1, keepdims=True)\n        exp_logits = np.exp(running_pred - max_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        # Predicted class is the argmax of probabilities\n        return np.argmax(probs, axis=1)\n```"}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or the string \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n\n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, kernel_in_channels, out_channels = W.shape\n\n    # Check input consistency\n    assert in_channels == kernel_in_channels, \"Input channels must match between X and W.\"\n    assert stride > 0, \"Stride must be positive.\"\n    assert dilation >= 0, \"Dilation must be non-negative.\"\n\n    # Handle padding\n    if isinstance(pad, str):\n        if pad.lower() == \"same\":\n            # Calculate output length\n            effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n            out_length = int(np.ceil(signal_length / stride))\n            total_pad = max(0, (out_length - 1) * stride + effective_kernel - signal_length)\n            p_left = total_pad // 2\n            p_right = total_pad - p_left\n        else:\n            raise ValueError(\"Invalid padding string. Use 'same' or an integer or 2-tuple.\")\n    elif isinstance(pad, int):\n        p_left = p_right = pad\n    elif isinstance(pad, tuple) or isinstance(pad, list):\n        if len(pad) != 2:\n            raise ValueError(\"Pad tuple/list must have length 2.\")\n        p_left, p_right = pad\n    else:\n        raise TypeError(\"Pad must be an int, a tuple/list of two ints, or 'same'.\")\n\n    # Compute the effective kernel size considering dilation\n    effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n\n    # Pad the input signal\n    X_padded = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant', constant_values=0)\n\n    # Compute output length\n    signal_length_padded = X_padded.shape[1]\n    out_length = ((signal_length_padded - effective_kernel) // stride) + 1\n\n    # Prepare output array\n    output = np.zeros((n_examples, out_length, out_channels))\n\n    # Perform the convolution (cross-correlation)\n    for n in range(n_examples):\n        for out_ch in range(out_channels):\n            for i in range(out_length):\n                start = i * stride\n                # Extract the window with dilation\n                window_indices = start + np.arange(0, kernel_width * (dilation + 1), dilation + 1)\n                # Check bounds\n                if window_indices[-1] >= signal_length_padded:\n                    break\n                # Extract the window for all input channels\n                window = X_padded[n, window_indices, :]  # shape: (kernel_width, in_channels)\n                # Get the kernel for this output channel\n                kernel = W[:, :, out_ch]  # shape: (kernel_width, in_channels)\n                # Element-wise multiply and sum over kernel width and input channels\n                # Cross-correlation: no kernel reversal\n                conv_sum = np.sum(window * kernel)\n                output[n, i, out_ch] = conv_sum\n\n    return output.tolist()\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert input to numpy array for easier processing\n    y = np.array(y)\n    \n    # Handle empty input\n    if y.size == 0:\n        return 0.0\n    \n    # Count occurrences of each class\n    class_counts = np.bincount(y)\n    total = y.size\n    \n    # Calculate class probabilities\n    p_c = class_counts / total\n    \n    # Compute Gini impurity\n    gini_value = 1 - np.sum(p_c ** 2)\n    \n    # Return rounded to 4 decimal places\n    return round(gini_value, 4)\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Target vector of shape (m,).\n        learning_rate (float): Step size for gradient descent.\n        n_iterations (int): Number of iterations to perform.\n\n    Returns:\n        tuple: (list of weights rounded to 4 decimals, bias rounded to 4 decimals)\n               or -1 if input dimensions are inconsistent.\n    \"\"\"\n    # Check for dimension consistency\n    if X.shape[0] != y.shape[0]:\n        return -1\n\n    m, n = X.shape\n\n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n\n    for _ in range(n_iterations):\n        # Compute predictions\n        y_pred = X @ theta + b\n\n        # Compute errors\n        error = y_pred - y\n\n        # Compute gradients\n        dw = (2 / m) * (X.T @ error)\n        db = (2 / m) * np.sum(error)\n\n        # Update parameters\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n\n    # Round the results to 4 decimal places\n    weights_rounded = [round(w, 4) for w in theta]\n    bias_rounded = round(b, 4)\n\n    return (weights_rounded, bias_rounded)\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose,\n    considering floating-point tolerance.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert the input list of lists to a NumPy array\n    arr = np.array(X)\n\n    # Check if the array is 2D\n    if arr.ndim != 2:\n        return False\n\n    # Check if the matrix is square\n    rows, cols = arr.shape\n    if rows != cols:\n        return False\n\n    # Check symmetry using np.allclose with a tolerance of 1e-8\n    return np.allclose(arr, arr.T, atol=1e-8)\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Computes the negative gradient for logistic loss used in gradient boosting for binary classification.\n    \n    Parameters:\n    y (list): List of binary class labels (0 or 1).\n    f (list): List of current prediction scores (floats).\n    \n    Returns:\n    list: List of negative gradients rounded to 4 decimal places, or -1 if invalid labels are present.\n    \"\"\"\n    # Validate input labels\n    for label in y:\n        if label not in [0, 1]:\n            return -1\n    \n    # Convert labels from {0,1} to {-1,1}\n    y_prime = [2 * label - 1 for label in y]\n    \n    gradients = []\n    for y_i, f_i in zip(y_prime, f):\n        exp_term = np.exp(y_i * f_i)\n        g_i = y_i / (1 + exp_term)\n        gradients.append(round(g_i, 4))\n    \n    return gradients\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Constants from the original paper\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    # Convert input to numpy array for vectorized operations\n    arr = np.array(x, dtype=float)\n\n    # Compute ELU(x, alpha)\n    elu = np.where(arr > 0, arr, alpha * (np.exp(arr) - 1))\n\n    if order == 0:\n        # SELU(x) = scale * ELU(x)\n        result = scale * elu\n    elif order == 1:\n        # First derivative: d(SELU)/dx\n        # For x > 0: derivative = scale\n        # For x <= 0: derivative = scale * alpha * exp(x)\n        derivative = np.where(arr > 0, scale, scale * alpha * np.exp(arr))\n        result = derivative\n    elif order == 2:\n        # Second derivative: d\u00b2(SELU)/dx\u00b2\n        # For x > 0: second derivative = 0\n        # For x <= 0: second derivative = scale * alpha * exp(x)\n        second_derivative = np.where(arr > 0, 0.0, scale * alpha * np.exp(arr))\n        result = second_derivative\n    else:\n        raise ValueError(\"Order must be 0, 1, or 2.\")\n\n    # Round to 4 decimal places\n    result_rounded = np.round(result, 4)\n\n    # Convert to list or scalar\n    if np.isscalar(x):\n        return float(result_rounded)\n    else:\n        return result_rounded.tolist()\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x = np.array(x)\n    y = np.array(y)\n    n_samples = len(y)\n    \n    # Initialize predictions with the mean of y\n    y_pred = np.full(n_samples, np.mean(y))\n    \n    for _ in range(n_estimators):\n        # Compute residuals (negative gradients of squared error)\n        residuals = y - y_pred\n        \n        # Sort data by x\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        residuals_sorted = residuals[sorted_indices]\n        \n        # Initialize variables to track the best split\n        min_error = np.inf\n        best_split = None\n        best_left_mean = None\n        best_right_mean = None\n        \n        # Check if all x are identical\n        if np.allclose(x_sorted, x_sorted[0]):\n            # No valid split; stump predicts global residual mean\n            global_residual_mean = np.mean(residuals)\n            # Update predictions\n            y_pred += learning_rate * global_residual_mean\n            continue\n        \n        # Evaluate all possible splits (midpoints between consecutive distinct x)\n        for i in range(1, n_samples):\n            if x_sorted[i] == x_sorted[i - 1]:\n                # Skip identical consecutive x values\n                continue\n            # Candidate split is midpoint between x_sorted[i-1] and x_sorted[i]\n            t = (x_sorted[i - 1] + x_sorted[i]) / 2\n            \n            # Left residuals\n            left_mask = x <= t\n            right_mask = ~left_mask\n            \n            # Means of residuals on each side\n            if np.any(left_mask):\n                r_mean_left = np.mean(residuals[left_mask])\n            else:\n                r_mean_left = 0.0\n            if np.any(right_mask):\n                r_mean_right = np.mean(residuals[right_mask])\n            else:\n                r_mean_right = 0.0\n            \n            # Compute sum of squared residuals for this split\n            sse_left = np.sum((residuals[left_mask] - r_mean_left) ** 2)\n            sse_right = np.sum((residuals[right_mask] - r_mean_right) ** 2)\n            total_error = sse_left + sse_right\n            \n            # Check if this split is better\n            if total_error < min_error:\n                min_error = total_error\n                best_split = t\n                best_left_mean = r_mean_left\n                best_right_mean = r_mean_right\n        \n        # If no valid split found (all x identical), use global residual mean\n        if best_split is None:\n            global_residual_mean = np.mean(residuals)\n            y_pred += learning_rate * global_residual_mean\n        else:\n            # Make predictions based on the best split\n            # For each sample, assign residual prediction\n            residual_predictions = np.where(x <= best_split, best_left_mean, best_right_mean)\n            # Update the overall prediction\n            y_pred += learning_rate * residual_predictions\n    \n    # Round predictions to 4 decimals\n    return list(np.round(y_pred, 4))\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    epsilon = 1e-8\n\n    # Initialize Eg if None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n\n    # Update the running average of squared gradients\n    Eg_new = rho * Eg + (1 - rho) * (grad ** 2)\n\n    # Compute the adjusted gradient\n    adjusted_grad = grad / (np.sqrt(Eg_new + epsilon))\n\n    # Update parameters\n    w_new = w - learning_rate * adjusted_grad\n\n    # Round to 4 decimal places\n    w_rounded = np.round(w_new, 4)\n    Eg_rounded = np.round(Eg_new, 4)\n\n    # Convert to lists\n    w_list = w_rounded.tolist()\n    Eg_list = Eg_rounded.tolist()\n\n    return w_list, Eg_list\n```"}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to numpy array for vectorized operations\n    x_array = np.array(x, dtype=float)\n    \n    if not derivative:\n        # Compute ELU activation\n        result = np.where(\n            x_array >= 0,\n            x_array,\n            alpha * (np.exp(x_array) - 1)\n        )\n    else:\n        # Compute ELU derivative\n        elu_x = np.where(\n            x_array >= 0,\n            x_array,\n            alpha * (np.exp(x_array) - 1)\n        )\n        result = np.where(\n            x_array >= 0,\n            1,\n            elu_x + alpha\n        )\n    \n    # Round each element to 4 decimal places and convert to list\n    return [round(val, 4) for val in result]\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n\n    def is_iterable(x):\n        return isinstance(x, (list, tuple, np.ndarray))\n    \n    def process_data(data_list):\n        # Determine if data is tuple/list/array (multi-dimensional) or scalar\n        # All elements are assumed to have the same shape/dimensionality\n        if not data_list:\n            # Empty list, assume scalar with no dimensions\n            return {\n                'tuple': False,\n                'multidim': False,\n                'continuous': False,\n                'n_per_dim': [],\n                'n_ids': [],\n                'action_ids': [],\n                'shape': ()\n            }\n        first_elem = data_list[0]\n        # Check if first element is iterable\n        is_first_iterable = is_iterable(first_elem)\n        # Determine shape/dimensions\n        if is_first_iterable:\n            shape = np.shape(first_elem)\n            # All elements should have same shape\n            for elem in data_list:\n                if is_iterable(elem):\n                    if np.shape(elem) != shape:\n                        raise ValueError(\"Inconsistent shapes in data.\")\n                else:\n                    # If one is scalar and others are iterable, shape mismatch\n                    raise ValueError(\"Mixed scalar and iterable in data.\")\n            # For 1D shape, shape is (length,)\n            # For scalar, shape is ()\n        else:\n            shape = ()\n        return {\n            'tuple': is_first_iterable,\n            'multidim': len(shape) > 1,\n            'shape': shape\n        }\n\n    def flatten_data(data_list):\n        # Flatten all data into a 2D array: shape (n_samples, n_dims)\n        # For scalar data, shape is ()\n        # For multi-dimensional, shape is (length, ...), flatten to 1D per sample\n        flattened = []\n        for item in data_list:\n            if is_iterable(item):\n                arr = np.array(item)\n                flattened.append(arr.flatten())\n            else:\n                # scalar\n                flattened.append(np.array([item]))\n        return np.array(flattened)\n\n    # Process actions\n    action_info = process_data(actions)\n    # Process observations\n    obs_info = process_data(observations)\n\n    # Determine if data is tuple (multi-dimensional)\n    tuple_actions = action_info['tuple']\n    tuple_observations = obs_info['tuple']\n\n    # Determine if data is multidim (more than 1 dimension)\n    multidim_actions = action_info['multidim']\n    multidim_observations = obs_info['multidim']\n\n    # Flatten data for further analysis\n    actions_flat = flatten_data(actions)\n    observations_flat = flatten_data(observations)\n\n    # Check for continuous data\n    def is_continuous(flat_data):\n        # flat_data shape: (n_samples, n_dims)\n        # Check if any value is a non-integer float\n        # Use np.any with a tolerance for float comparison\n        # Values that are floats but not close to integers are continuous\n        # For integer detection, check if all values are close to their rounded values\n        # But since we need to detect non-integer floats, check for any float with fractional part\n        for val in flat_data.flatten():\n            if isinstance(val, float):\n                if not np.isclose(val, round(val)):\n                    return True\n        return False\n\n    continuous_actions = is_continuous(actions_flat)\n    continuous_observations = is_continuous(observations_flat)\n\n    # For each dimension, find unique values\n    def get_unique_per_dim(flat_data, shape):\n        # shape: tuple of dimensions\n        n_dims = len(shape)\n        n_samples = len(flat_data)\n        n_per_dim = []\n        n_ids = []\n        action_ids = []\n\n        # Reshape flat_data to shape (n_samples, shape...)\n        data_reshaped = flat_data.reshape((n_samples,) + shape)\n\n        for dim in range(n_dims):\n            # Extract all values in this dimension\n            dim_values = data_reshaped[:, dim]\n            # For multi-dimensional, need to handle accordingly\n            # For 1D shape, shape = (length,)\n            # For higher dims, shape = (d1, d2, ...)\n            # We want to get all values along this dimension\n            # For each sample, get the value at this dimension\n            # For multi-dimensional, the shape of data_reshaped is (n_samples, d1, d2, ...)\n            # So, for each sample, the value at position in this dimension is data_reshaped[:, dim]\n            # But for multi-dimensional, the dimension index is in the shape\n            # So, for each dimension, get all values along that axis\n            # We can do this by slicing data_reshaped along that axis\n            # For simplicity, we can iterate over all samples and get the values at that dimension\n            # But better to use np.take along axis\n            values_in_dim = np.take(data_reshaped, indices=range(shape[dim]), axis=1+dim)\n            # values_in_dim shape: (n_samples, shape[dim])\n            # For each position in shape, get unique values\n            # But we want to get unique values per dimension (i.e., across all samples)\n            # So, flatten all values in this dimension across samples\n            values_flat = values_in_dim.flatten()\n            unique_vals = np.unique(values_flat)\n            n_per_dim.append(len(unique_vals))\n            n_ids.append(list(unique_vals))\n            action_ids.append(list(unique_vals))\n        return n_per_dim, n_ids, action_ids\n\n    def get_unique_obs_per_dim(flat_data, shape):\n        # Similar to above, but for observations\n        n_dims = len(shape)\n        n_samples = len(flat_data)\n        n_per_dim = []\n        n_ids = []\n        obs_ids = []\n\n        data_reshaped = flat_data.reshape((n_samples,) + shape)\n\n        for dim in range(n_dims):\n            values_in_dim = np.take(data_reshaped, indices=range(shape[dim]), axis=1+dim)\n            values_flat = values_in_dim.flatten()\n            unique_vals = np.unique(values_flat)\n            n_per_dim.append(len(unique_vals))\n            n_ids.append(list(unique_vals))\n            obs_ids.append(list(unique_vals))\n        return n_per_dim, n_ids, obs_ids\n\n    # For actions\n    if action_info['shape'] == ():\n        # Scalar actions\n        n_actions_per_dim = []\n        n_action_ids = []\n        action_ids = []\n        action_dim = 0\n    else:\n        n_actions_per_dim, n_action_ids, action_ids = get_unique_per_dim(actions_flat, action_info['shape'])\n        action_dim = len(action_info['shape'])\n\n    # For observations\n    if obs_info['shape'] == ():\n        # Scalar observations\n        n_obs_per_dim = []\n        n_obs_ids = []\n        obs_ids = []\n        obs_dim = 0\n    else:\n        n_obs_per_dim, n_obs_ids, obs_ids = get_unique_obs_per_dim(observations_flat, obs_info['shape'])\n        obs_dim = len(obs_info['shape'])\n\n    # Prepare final dictionary\n    result = {\n        'tuple_actions': tuple_actions,\n        'tuple_observations': tuple_observations,\n        'multidim_actions': multidim_actions,\n        'multidim_observations': multidim_observations,\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': n_actions_per_dim,\n        'action_dim': action_dim,\n        'n_obs_per_dim': n_obs_per_dim,\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids\n    }\n\n    return result\n```"}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n\n    # Initialize predictions with the mean of y_train\n    y_pred = np.full(shape=y_train.shape, fill_value=np.mean(y_train))\n    # Store the final predictions for test data\n    test_predictions = np.zeros(X_test.shape[0])\n\n    def fit_tree(X, residuals, depth):\n        \"\"\"\n        Fit a regression tree to residuals with limited depth.\n        Returns a tree represented as a nested dictionary.\n        \"\"\"\n        # If max depth reached or no data, return mean\n        if depth == 0 or len(X) == 0:\n            return {'value': np.mean(residuals)}\n\n        n_samples, n_features = X.shape\n        best_feature = None\n        best_threshold = None\n        best_error = np.inf\n        best_left_idx = None\n        best_right_idx = None\n        best_left_mean = None\n        best_right_mean = None\n\n        # For each feature, try all possible splits\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            unique_values = np.unique(feature_values)\n            # To reduce computation, consider midpoints between sorted unique values\n            potential_thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n\n            for threshold in potential_thresholds:\n                left_idx = feature_values <= threshold\n                right_idx = feature_values > threshold\n\n                if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n                    continue\n\n                left_residuals = residuals[left_idx]\n                right_residuals = residuals[right_idx]\n\n                left_mean = np.mean(left_residuals)\n                right_mean = np.mean(right_residuals)\n\n                # Compute squared error\n                error = np.sum((left_residuals - left_mean) ** 2) + np.sum((right_residuals - right_mean) ** 2)\n\n                if error < best_error:\n                    best_error = error\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                    best_left_idx = left_idx\n                    best_right_idx = right_idx\n                    best_left_mean = left_mean\n                    best_right_mean = right_mean\n\n        # If no split improves error, return mean\n        if best_feature is None:\n            return {'value': np.mean(residuals)}\n\n        # Recursively build left and right subtrees\n        left_subtree = fit_tree(X[best_left_idx], residuals[best_left_idx], depth - 1)\n        right_subtree = fit_tree(X[best_right_idx], residuals[best_right_idx], depth - 1)\n\n        return {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_subtree,\n            'right': right_subtree\n        }\n\n    def predict_tree(tree, X):\n        \"\"\"\n        Predict using the regression tree.\n        \"\"\"\n        if 'value' in tree:\n            return np.full(shape=X.shape[0], fill_value=tree['value'])\n        feature_idx = tree['feature']\n        threshold = tree['threshold']\n        left_idx = X[:, feature_idx] <= threshold\n        right_idx = X[:, feature_idx] > threshold\n        predictions = np.empty(X.shape[0])\n        predictions[left_idx] = predict_tree(tree['left'], X[left_idx])\n        predictions[right_idx] = predict_tree(tree['right'], X[right_idx])\n        return predictions\n\n    for _ in range(n_estimators):\n        residuals = y_train - y_pred\n        # Fit a regression tree to residuals\n        tree = fit_tree(X_train, residuals, max_depth)\n        # Predict residuals on training data\n        residual_pred = predict_tree(tree, X_train)\n        # Update the overall prediction\n        y_pred += learning_rate * residual_pred\n\n        # For test data, predict residuals and update\n        test_residual_pred = predict_tree(tree, X_test)\n        test_predictions += learning_rate * test_residual_pred\n\n    # Round predictions to 4 decimal places and convert to list\n    final_predictions = np.round(test_predictions, 4).tolist()\n    return final_predictions\n```"}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculate the multivariate Gaussian PDF for each data point in X without using third-party libraries.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features) or (n_samples,) if 1D\n    - mean: 1D array-like of length n_features\n    - cov: 2D array-like of shape (n_features, n_features), positive-definite covariance matrix\n    \n    Returns:\n    - List of PDF values for each sample in X, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.atleast_2d(X)\n    mean = np.array(mean)\n    cov = np.array(cov)\n    \n    n_samples, n_features = X.shape\n    \n    # Compute the determinant of the covariance matrix\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive definite.\")\n    \n    # Compute the inverse of the covariance matrix\n    cov_inv = np.linalg.inv(cov)\n    \n    # Constant coefficient\n    denom_const = math.sqrt((2 * math.pi) ** n_features * det_cov)\n    \n    results = []\n    for x in X:\n        diff = x - mean\n        # Compute the exponent term: -0.5 * (x - mu)^T * cov_inv * (x - mu)\n        exponent = -0.5 * np.dot(np.dot(diff.T, cov_inv), diff)\n        pdf_value = (1.0 / denom_const) * math.exp(exponent)\n        results.append(round(pdf_value, 4))\n    \n    return results\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function returns the MAP predictive means for every row in `X_new`.\n    \"\"\"\n    # Step 1: Add intercept if needed\n    if fit_intercept:\n        intercept_column = np.ones((X.shape[0], 1))\n        X = np.hstack((intercept_column, X))\n        X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n    \n    N, M = X.shape\n\n    # Step 2: Handle prior mean mu\n    # If mu is scalar, replicate to length M\n    if np.isscalar(mu):\n        mu_vec = np.full(M, mu)\n    else:\n        mu_vec = np.array(mu)\n        # If mu is a vector but length doesn't match M, raise error\n        if mu_vec.shape[0] != M:\n            raise ValueError(f\"mu vector length {mu_vec.shape[0]} does not match number of features {M}\")\n\n    # Step 2: Handle prior covariance V\n    if V is None:\n        V_mat = np.eye(M)\n    elif np.isscalar(V):\n        V_mat = V * np.eye(M)\n    else:\n        V_array = np.array(V)\n        if V_array.ndim == 1:\n            # Diagonal matrix\n            V_mat = np.diag(V_array)\n        elif V_array.ndim == 2:\n            # Full matrix\n            V_mat = V_array\n            # Check if V is symmetric and positive-definite\n            if not np.allclose(V_mat, V_mat.T):\n                raise ValueError(\"V matrix must be symmetric.\")\n            # Optional: check positive definiteness\n            # But for simplicity, assume user provides valid V\n        else:\n            raise ValueError(\"V must be None, scalar, 1D array, or 2D array.\")\n    \n    # Step 3: Compute posterior covariance and mean\n    V_inv = np.linalg.inv(V_mat)\n    Sigma_p = np.linalg.inv(V_inv + X.T @ X)\n    mu_p = Sigma_p @ (V_inv @ mu_vec + X.T @ y)\n\n    # Step 4: Predictive mean for X_new\n    y_pred = X_new @ mu_p\n\n    # Round predictions to 4 decimal places and convert to list\n    return [round(val, 4) for val in y_pred]\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculate the RMSLE between actual and predicted values.\n    \n    Returns:\n        The RMSLE rounded to 4 decimal places, or -1 if inputs are invalid.\n    \"\"\"\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Validate that all values are non-negative\n    if any(a < 0 for a in actual) or any(p < 0 for p in predicted):\n        return -1\n    \n    # Convert lists to numpy arrays for efficient computation\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute the log(1 + x) for each element\n    log_actual = np.log1p(actual_np)\n    log_predicted = np.log1p(predicted_np)\n    \n    # Calculate the squared differences\n    squared_diffs = (log_predicted - log_actual) ** 2\n    \n    # Compute the mean of squared differences\n    mean_squared_diff = np.mean(squared_diffs)\n    \n    # Take the square root to get RMSLE\n    rmsle = np.sqrt(mean_squared_diff)\n    \n    # Return rounded result\n    return round(rmsle, 4)\n```"}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n\n    if window_len == 1:\n        # For a single sample, the window is just [1.0]\n        return [1.0]\n\n    if symmetric:\n        N = window_len\n    else:\n        N = window_len + 1\n\n    # Generate the window\n    n = np.arange(N)\n    window = 0.5 - 0.5 * np.cos(2 * math.pi * n / (N - 1))\n\n    if not symmetric:\n        # For periodic window, drop the last sample\n        window = window[:-1]\n\n    # Round to 4 decimal places\n    window_rounded = [round(val, 4) for val in window]\n\n    return window_rounded\n```"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    # Check the shape length\n    if len(shape) == 4:\n        # 2D convolutional kernel\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        # For other tensors, take the first two dimensions directly\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return float(fan_in), float(fan_out)\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                                y: list[float],\n                                n_estimators: int = 10,\n                                learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert input lists to numpy arrays of type float\n    X_np = np.array(X, dtype=float)\n    y_np = np.array(y, dtype=float)\n\n    m, d = X_np.shape\n\n    # Handle special cases\n    if n_estimators <= 0 or learning_rate == 0:\n        mean_y = np.mean(y_np)\n        return [round(mean_y, 4)] * m\n\n    # Initialize predictions with the mean of y\n    y_pred = np.full(m, np.mean(y_np))\n\n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_np - y_pred\n\n        # Fit linear regression (including intercept) to residuals\n        # To include intercept, add a column of ones\n        X_with_intercept = np.column_stack((np.ones(m), X_np))\n        # Solve for coefficients using normal equation: (X^T X) beta = X^T y\n        # To avoid explicit matrix inversion, use np.linalg.solve\n        A = X_with_intercept.T @ X_with_intercept\n        b = X_with_intercept.T @ residuals\n        # Solve for beta\n        beta = np.linalg.solve(A, b)\n\n        # Weak learner prediction\n        h = X_with_intercept @ beta\n\n        # Update overall prediction\n        y_pred += learning_rate * h\n\n    # Round predictions to 4 decimal places\n    final_predictions = [round(val, 4) for val in y_pred]\n    return final_predictions\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    # Check for valid order\n    if order not in [0, 1, 2]:\n        return -1\n\n    # Convert input to numpy array for uniform processing\n    arr = np.array(x, dtype=float)\n\n    # Compute sigmoid\n    sigma = 1 / (1 + np.exp(-arr))\n\n    # Compute derivatives if needed\n    if order == 1:\n        result = sigma * (1 - sigma)\n    elif order == 2:\n        result = sigma * (1 - sigma) * (1 - 2 * sigma)\n    else:\n        result = sigma\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # Return scalar if input was scalar\n    if np.isscalar(x):\n        return float(result)\n    else:\n        return result.tolist()\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Convert y to numpy array for easier processing\n    y_array = np.array(y)\n    \n    # Check for non-negative integers\n    if np.any(y_array < 0):\n        return -1\n    \n    # Determine number of classes if not provided\n    max_y = y_array.max()\n    if num_classes is None:\n        num_classes = max_y + 1\n    else:\n        # If provided, check if it's sufficient\n        if num_classes <= max_y:\n            return -1\n    \n    # Create the one-hot encoded matrix\n    # Initialize with zeros\n    one_hot = np.zeros((len(y_array), num_classes), dtype=int)\n    # Set the appropriate elements to 1\n    one_hot[np.arange(len(y_array)), y_array] = 1\n    \n    # Convert to list of lists\n    return one_hot.tolist()\n```"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\nfrom collections import defaultdict\nfrom typing import Hashable\n\ndef apriori_frequent_itemsets(transactions: list[list[Hashable]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # Total number of transactions\n    total_transactions = len(transactions)\n    if total_transactions == 0:\n        return []\n\n    # Count support for single items\n    item_counts = defaultdict(int)\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] += 1\n\n    # Generate frequent 1-itemsets\n    frequent_itemsets = []\n    current_frequent = []\n    for item, count in item_counts.items():\n        support = count / total_transactions\n        if support >= min_sup:\n            current_frequent.append((item,))\n    # Sort 1-itemsets lex order\n    current_frequent = sorted(current_frequent, key=lambda x: x[0])\n    frequent_itemsets.extend(current_frequent)\n\n    k = 2\n    prev_frequent = current_frequent\n\n    while prev_frequent:\n        # Generate candidate k-itemsets by self-joining previous frequent itemsets\n        candidates = set()\n        prev_frequent_sorted = [list(itemset) for itemset in prev_frequent]\n        len_prev = len(prev_frequent_sorted)\n        for i in range(len_prev):\n            for j in range(i + 1, len_prev):\n                # Join if first k-2 items are equal\n                if prev_frequent_sorted[i][:-1] == prev_frequent_sorted[j][:-1]:\n                    candidate = tuple(sorted(set(prev_frequent_sorted[i]) | set(prev_frequent_sorted[j])))\n                    if len(candidate) == k:\n                        candidates.add(candidate)\n\n        # Prune candidates: remove those with infrequent subsets\n        pruned_candidates = set()\n        for candidate in candidates:\n            # Generate all (k-1)-subsets\n            subsets = itertools.combinations(candidate, k - 1)\n            if all(tuple(sorted(subset)) in prev_frequent for subset in subsets):\n                pruned_candidates.add(candidate)\n\n        # Count support for candidates\n        candidate_counts = defaultdict(int)\n        for transaction in transactions:\n            transaction_set = set(transaction)\n            for candidate in pruned_candidates:\n                if set(candidate).issubset(transaction_set):\n                    candidate_counts[candidate] += 1\n\n        # Filter candidates by support\n        current_frequent = []\n        for candidate, count in candidate_counts.items():\n            support = count / total_transactions\n            if support >= min_sup:\n                current_frequent.append(candidate)\n\n        # Sort current frequent itemsets lex order\n        current_frequent = sorted(current_frequent, key=lambda x: x)\n        if current_frequent:\n            frequent_itemsets.extend(current_frequent)\n        else:\n            break\n\n        prev_frequent = current_frequent\n        k += 1\n\n    # Final output: sort by length, then lex order\n    # Already sorted within each size, so just sort overall\n    def sort_key(itemset):\n        return (len(itemset), itemset)\n\n    return sorted(frequent_itemsets, key=sort_key)\n```"}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the nested list into a NumPy array for easier manipulation\n    preds_array = np.array(predictions)  # shape: (n_estimators, n_samples, n_classes)\n    \n    # Compute the mean over estimators axis (axis=0)\n    mean_probs = np.mean(preds_array, axis=0)  # shape: (n_samples, n_classes)\n    \n    # Round the averaged probabilities to 4 decimal places\n    rounded_probs = np.round(mean_probs, 4)\n    \n    # Determine the predicted class for each sample (index of max probability)\n    predicted_classes = np.argmax(rounded_probs, axis=1).tolist()\n    \n    # Convert the probabilities back to a list of lists\n    probs_list = rounded_probs.tolist()\n    \n    return probs_list, predicted_classes\n```"}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Convert X to numpy array\n    try:\n        X_np = np.array(X, dtype=float)\n    except Exception:\n        return -1\n\n    # Validate X shape\n    if X_np.ndim != 2:\n        return -1\n\n    N, C = X_np.shape\n\n    # Handle Y\n    if Y is None:\n        Y_np = X_np\n    else:\n        try:\n            Y_np = np.array(Y, dtype=float)\n        except Exception:\n            return -1\n        # Validate Y shape\n        if Y_np.ndim != 2:\n            return -1\n        if Y_np.shape[1] != C:\n            return -1\n\n    # Validate sigma\n    if sigma is None:\n        # Default: sqrt(C/2)\n        sigma_value = np.sqrt(C / 2)\n        sigma_array = np.full(C, sigma_value)\n    elif isinstance(sigma, (float, int)):\n        # Single value for all features\n        if sigma <= 0:\n            return -1\n        sigma_array = np.full(C, float(sigma))\n    elif isinstance(sigma, list) or isinstance(sigma, np.ndarray):\n        # List of per-feature sigma\n        sigma_list = np.array(sigma, dtype=float)\n        if sigma_list.shape != (C,):\n            return -1\n        if np.any(sigma_list <= 0):\n            return -1\n        sigma_array = sigma_list\n    else:\n        return -1\n\n    # Compute the squared Euclidean distance matrix\n    # Efficient computation using broadcasting\n    # dist_{i,j}^2 = sum_j ((x_{i,j} - y_{j})/sigma_j)^2\n    # We can compute as:\n    # (X / sigma) and (Y / sigma), then compute pairwise squared distances\n    X_scaled = X_np / sigma_array\n    Y_scaled = Y_np / sigma_array\n\n    # Compute pairwise squared distances\n    # Using (a - b)^2 = a^2 + b^2 - 2ab\n    X_sq = np.sum(X_scaled ** 2, axis=1).reshape(-1, 1)  # shape (N,1)\n    Y_sq = np.sum(Y_scaled ** 2, axis=1).reshape(1, -1)  # shape (1,M)\n    cross_term = np.dot(X_scaled, Y_scaled.T)  # shape (N, M)\n\n    dist_sq = X_sq + Y_sq - 2 * cross_term\n    # Numerical stability: ensure no negative values due to floating point errors\n    dist_sq = np.maximum(dist_sq, 0)\n\n    # Compute the kernel matrix\n    kernel_matrix = np.exp(-0.5 * dist_sq)\n\n    # Round to 4 decimal places\n    kernel_matrix_rounded = np.round(kernel_matrix, 4)\n\n    # Convert to nested list\n    result = kernel_matrix_rounded.tolist()\n\n    return result\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n\n    # Convert inputs to numpy arrays for easier processing\n    X = np.array(X)\n    y = np.array(y)\n\n    n_samples, n_features = X.shape\n\n    # Function to compute entropy\n    def entropy(labels):\n        if len(labels) == 0:\n            return 0.0\n        counts = np.bincount(labels)\n        probs = counts / counts.sum()\n        # To avoid log(0), mask zero probabilities\n        probs = probs[probs > 0]\n        return -np.sum(probs * np.log2(probs))\n\n    # Compute parent entropy\n    parent_entropy = entropy(y)\n\n    # If all labels are the same, no split can improve\n    unique_labels, counts_labels = np.unique(y, return_counts=True)\n    if len(unique_labels) == 1:\n        # All labels are same, no split improves\n        majority_label = unique_labels[0]\n        return (-1, None, majority_label, majority_label)\n\n    # Initialize variables to track best split\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_majority = None\n    best_right_majority = None\n\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        # Get sorted unique values\n        unique_values = np.unique(feature_values)\n        if len(unique_values) == 1:\n            # No split possible on this feature\n            continue\n\n        # Candidate thresholds are midpoints between successive unique values\n        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n\n        for thr in thresholds:\n            # Split data\n            left_mask = feature_values <= thr\n            right_mask = ~left_mask\n\n            y_left = y[left_mask]\n            y_right = y[right_mask]\n\n            # Compute entropies\n            H_left = entropy(y_left)\n            H_right = entropy(y_right)\n\n            p_left = len(y_left) / n_samples\n            p_right = len(y_right) / n_samples\n\n            # Compute information gain\n            gain = parent_entropy - (p_left * H_left + p_right * H_right)\n\n            # Check if this is better than current best\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature_idx\n                best_threshold = round(thr, 4)\n                # Determine majority labels for left and right\n                if len(y_left) > 0:\n                    left_counts = np.bincount(y_left)\n                    left_majority = np.argmax(left_counts)\n                else:\n                    # If no samples go left, fallback to overall majority\n                    left_majority = np.argmax(np.bincount(y))\n                if len(y_right) > 0:\n                    right_counts = np.bincount(y_right)\n                    right_majority = np.argmax(right_counts)\n                else:\n                    right_majority = np.argmax(np.bincount(y))\n            elif np.isclose(gain, best_gain):\n                # Tie in gain: choose the split with smaller feature index\n                # and if same, smaller threshold\n                if feature_idx < best_feature:\n                    best_feature = feature_idx\n                    best_threshold = round(thr, 4)\n                    if len(y_left) > 0:\n                        left_counts = np.bincount(y_left)\n                        left_majority = np.argmax(left_counts)\n                    else:\n                        left_majority = np.argmax(np.bincount(y))\n                    if len(y_right) > 0:\n                        right_counts = np.bincount(y_right)\n                        right_majority = np.argmax(right_counts)\n                    else:\n                        right_majority = np.argmax(np.bincount(y))\n                elif feature_idx == best_feature:\n                    # Same feature, compare thresholds\n                    if thr < best_threshold:\n                        best_threshold = round(thr, 4)\n                        if len(y_left) > 0:\n                            left_counts = np.bincount(y_left)\n                            left_majority = np.argmax(left_counts)\n                        else:\n                            left_majority = np.argmax(np.bincount(y))\n                        if len(y_right) > 0:\n                            right_counts = np.bincount(y_right)\n                            right_majority = np.argmax(right_counts)\n                        else:\n                            right_majority = np.argmax(np.bincount(y))\n\n    # If no split improves (gain <= 0), return default\n    if best_gain <= 0:\n        # All labels are same or no split improves\n        majority_label = np.argmax(np.bincount(y))\n        return (-1, None, majority_label, majority_label)\n\n    # Return the best split info\n    # Determine majority labels for final split\n    y_left_final = y[X[:, best_feature] <= best_threshold]\n    y_right_final = y[X[:, best_feature] > best_threshold]\n    left_counts = np.bincount(y_left_final)\n    right_counts = np.bincount(y_right_final)\n    left_majority = np.argmax(left_counts)\n    right_majority = np.argmax(right_counts)\n\n    return (best_feature, best_threshold, left_majority, right_majority)\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef divide_on_feature(X: Union[np.ndarray, List[List]], feature_i: int, threshold) -> List[List]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Convert X to numpy array if it's a list\n    X_array = np.array(X)\n    \n    # Extract the feature column\n    feature_column = X_array[:, feature_i]\n    \n    # Determine if threshold is numeric\n    is_numeric_threshold = isinstance(threshold, (int, float))\n    \n    # Create boolean mask for the first subset\n    if is_numeric_threshold:\n        mask_first = feature_column >= threshold\n    else:\n        mask_first = feature_column == threshold\n    \n    # Use the mask to split the data\n    first_subset = X_array[mask_first]\n    second_subset = X_array[~mask_first]\n    \n    # Convert to list of lists\n    result = [first_subset.tolist(), second_subset.tolist()]\n    return result\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n\n    # Validate input lengths\n    if not (len(Q) == len(Q_next) == len(actions) == len(rewards) == len(dones)):\n        return -1\n\n    # Validate gamma\n    if not (0 <= gamma <= 1):\n        return -1\n\n    batch_size, n_actions = Q.shape\n\n    # Validate Q and Q_next shapes\n    if Q_next.shape != Q.shape:\n        return -1\n\n    # Validate actions are within valid range\n    if any((a < 0 or a >= n_actions) for a in actions):\n        return -1\n\n    # Initialize the target matrix as a copy of Q\n    targets = np.copy(Q)\n\n    for i in range(batch_size):\n        if dones[i]:\n            target_value = rewards[i]\n        else:\n            max_next_q = np.max(Q_next[i])\n            target_value = rewards[i] + gamma * max_next_q\n\n        # Replace only the action taken\n        targets[i][actions[i]] = target_value\n\n    # Round to 4 decimal places\n    targets = np.round(targets, 4)\n\n    # Convert to list of lists\n    return targets.tolist()\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Convert labels to numpy array for easier processing\n    labels_array = np.array(labels)\n    \n    # Check for negative labels\n    if np.any(labels_array < 0):\n        return -1\n    \n    # Infer num_classes if not provided\n    max_label = np.max(labels_array)\n    if num_classes is None:\n        num_classes = max_label + 1\n    else:\n        # If provided, check if num_classes is sufficient\n        if num_classes <= max_label:\n            return -1\n    \n    # Initialize the one-hot encoded matrix\n    one_hot = []\n    for label in labels_array:\n        # Create a row of zeros\n        row = [0] * num_classes\n        # Set the position of the label to 1\n        row[label] = 1\n        one_hot.append(row)\n    \n    return one_hot\n```"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(x)\n    Y = np.array(y)\n    n_samples = len(X)\n    \n    # Handle edge cases\n    if n_estimators <= 0:\n        # Return the mean target for all samples\n        mean_y = np.round(np.mean(Y), 4)\n        return [mean_y] * n_samples\n    \n    # Initialize predictions with the mean of y\n    y_pred = np.full(n_samples, np.mean(Y))\n    \n    for _ in range(n_estimators):\n        residuals = Y - y_pred  # current residuals\n        \n        # If residuals are all zero, no need to continue\n        if np.allclose(residuals, 0):\n            break\n        \n        # Get unique sorted x values\n        unique_x = np.unique(X)\n        # Candidate split points are midpoints between consecutive unique x values\n        split_candidates = (unique_x[:-1] + unique_x[1:]) / 2\n        \n        best_sse = None\n        best_split = None\n        best_v_left = 0\n        best_v_right = 0\n        \n        for t in split_candidates:\n            # Split data\n            left_mask = X <= t\n            right_mask = X > t\n            \n            # Ignore splits that produce empty sides\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n            \n            # Compute residual means on each side\n            v_left = np.mean(residuals[left_mask])\n            v_right = np.mean(residuals[right_mask])\n            \n            # Compute SSE for this split\n            sse_left = np.sum((residuals[left_mask] - v_left) ** 2)\n            sse_right = np.sum((residuals[right_mask] - v_right) ** 2)\n            total_sse = sse_left + sse_right\n            \n            # Check if this is the best split so far\n            if (best_sse is None) or (total_sse < best_sse):\n                best_sse = total_sse\n                best_split = t\n                best_v_left = v_left\n                best_v_right = v_right\n        \n        # If no valid split found, set stump to mean residual of entire data\n        if best_split is None:\n            overall_mean_residual = np.mean(residuals)\n            v_left = v_right = overall_mean_residual\n        else:\n            v_left = best_v_left\n            v_right = best_v_right\n        \n        # Make predictions with the stump\n        stump_pred = np.where(X <= best_split, v_left, v_right)\n        \n        # Update the overall prediction\n        y_pred += learning_rate * stump_pred\n    \n    # Round predictions to 4 decimal places\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n\n    # Number of actions\n    num_actions = len(q_table[0])\n\n    # Compute the expected Q value for the next state\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        q_next = q_table[next_state]\n        max_q = max(q_next)\n        # Find all actions with the max Q-value (ties)\n        greedy_actions = [a for a, q_val in enumerate(q_next) if q_val == max_q]\n        # Probability of selecting a greedy action\n        prob_greedy = 1 - epsilon + (epsilon / len(greedy_actions))\n        # Initialize expected value\n        expected_q = 0.0\n        for a in range(num_actions):\n            if a in greedy_actions:\n                prob_a = prob_greedy / len(greedy_actions)\n            else:\n                prob_a = epsilon / len(greedy_actions)\n            expected_q += prob_a * q_next[a]\n\n    # Compute the target\n    target = reward + gamma * expected_q\n\n    # Update the Q-value for the current state-action pair\n    q_table[state][action] += lr * (target - q_table[state][action])\n\n    # Round all entries to four decimal places\n    rounded_q_table = [\n        [round(q_value, 4) for q_value in row]\n        for row in q_table\n    ]\n\n    return rounded_q_table\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n\n    X_train_list = []\n    Y_train_list = []\n    X_val_list = []\n    Y_val_list = []\n\n    for i in range(m):\n        # Determine class label\n        label = (i - 0.5) * 2  # -1 for first class, +1 for second, etc.\n\n        # Determine class position parameters\n        row_idx = i // 2\n        col_idx = i % 2\n\n        # Horizontal (x) bounds\n        x_min = (row_idx + 0.1) * 10\n        x_max = (row_idx + 0.9) * 10\n\n        # Vertical (y) bounds\n        y_min = ((col_idx * interval) + 0.1) * 10\n        y_max = ((col_idx * interval) + 0.9) * 10\n\n        # Generate training samples\n        x_train = np.round(np.random.uniform(x_min, x_max, n_train), 4)\n        y_train = np.round(np.random.uniform(y_min, y_max, n_train), 4)\n        X_train_list.append(np.column_stack((x_train, y_train)))\n        Y_train_list.extend([label] * n_train)\n\n        # Generate validation samples\n        x_val = np.round(np.random.uniform(x_min, x_max, n_val), 4)\n        y_val = np.round(np.random.uniform(y_min, y_max, n_val), 4)\n        X_val_list.append(np.column_stack((x_val, y_val)))\n        Y_val_list.extend([label] * n_val)\n\n    # Concatenate all class data\n    X_train = np.vstack(X_train_list)\n    Y_train = np.array(Y_train_list)\n    X_val = np.vstack(X_val_list)\n    Y_val = np.array(Y_val_list)\n\n    return X_train, X_val, Y_train, Y_val\n```"}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Initialize m_prev and v_prev if they are None\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Update biased first moment estimate\n    m_t = b1 * m_prev + (1 - b1) * grad\n    \n    # Update biased second raw moment estimate\n    v_t = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_t / (1 - b1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v_t / (1 - b2 ** t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_t, v_t\n```"}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    num_steps = len(rewards)\n    if num_steps == 0:\n        return []\n\n    num_arms = len(rewards[0])\n    \n    # Initialize estimates and counters\n    estimates = [ev_prior for _ in range(num_arms)]\n    counts = [0 for _ in range(num_arms)]\n    \n    for t in range(num_steps):\n        # Decide whether to explore or exploit\n        if np.random.rand() < epsilon:\n            # Explore: choose a random arm\n            chosen_arm = np.random.randint(num_arms)\n        else:\n            # Exploit: choose the arm with the highest estimate (break ties by smallest index)\n            max_value = max(estimates)\n            # Find all arms with the max estimate\n            candidate_arms = [i for i, v in enumerate(estimates) if v == max_value]\n            # Choose the one with the smallest index among candidates\n            chosen_arm = candidate_arms[0]\n        \n        # Receive reward\n        reward = rewards[t][chosen_arm]\n        \n        # Update counts\n        counts[chosen_arm] += 1\n        # Update estimate using incremental mean\n        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / counts[chosen_arm]\n    \n    # Round estimates to 4 decimal places\n    return [round(v, 4) for v in estimates]\n```"}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.shape[0] == 0:\n        return []\n\n    n_points = data.shape[0]\n    labels = [-1] * n_points  # Initialize all points as noise\n    visited = [False] * n_points\n    cluster_id = 0\n\n    # Precompute pairwise distances for efficiency\n    # Alternatively, compute on the fly; here, for clarity, we'll compute on the fly\n    def region_query(point_idx):\n        \"\"\"Find all points within eps of point_idx.\"\"\"\n        point = data[point_idx]\n        neighbors = []\n        for idx in range(n_points):\n            if np.linalg.norm(data[idx] - point) <= eps:\n                neighbors.append(idx)\n        return neighbors\n\n    for idx in range(n_points):\n        if not visited[idx]:\n            visited[idx] = True\n            neighbors = region_query(idx)\n            if len(neighbors) < min_samples:\n                # Mark as noise (already initialized as -1)\n                continue\n            else:\n                # Start a new cluster\n                labels[idx] = cluster_id\n                # Expand cluster\n                seed_queue = neighbors.copy()\n                # Remove the current point from the queue if present\n                if idx in seed_queue:\n                    seed_queue.remove(idx)\n                while seed_queue:\n                    current_point = seed_queue.pop()\n                    if not visited[current_point]:\n                        visited[current_point] = True\n                        current_neighbors = region_query(current_point)\n                        if len(current_neighbors) >= min_samples:\n                            # Add new neighbors to the queue\n                            for neighbor in current_neighbors:\n                                if neighbor not in seed_queue:\n                                    seed_queue.append(neighbor)\n                    # Assign cluster label if not assigned yet\n                    if labels[current_point] == -1:\n                        labels[current_point] = cluster_id\n                cluster_id += 1\n\n    return labels\n```"}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if y.ndim != 1:\n        raise ValueError(\"Input array y must be one-dimensional.\")\n    if np.any(y < 0):\n        raise ValueError(\"Labels must be non-negative integers.\")\n\n    if num_classes is None:\n        num_classes = y.max() + 1\n    else:\n        if not isinstance(num_classes, int):\n            raise TypeError(\"num_classes must be an integer or None.\")\n        if num_classes <= 0:\n            raise ValueError(\"num_classes must be a positive integer.\")\n        if np.any(y >= num_classes):\n            raise ValueError(\"Some labels are >= num_classes.\")\n\n    # Initialize the output array with zeros\n    one_hot = np.zeros((len(y), num_classes), dtype=int)\n\n    # Set the appropriate elements to 1\n    one_hot[np.arange(len(y)), y] = 1\n\n    return one_hot\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test: list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    # Handle empty training data\n    if len(X_train) == 0:\n        return []\n\n    # Convert to numpy arrays for easier handling\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n\n    # Define the tree node structure\n    class Node:\n        def __init__(self, prediction=None, feature_index=None, threshold=None, left=None, right=None):\n            self.prediction = prediction      # mean target value at leaf\n            self.feature_index = feature_index  # index of feature to split\n            self.threshold = threshold        # split point\n            self.left = left                  # left child node\n            self.right = right                # right child node\n\n    def build_tree(X, y, depth):\n        # Stopping conditions\n        if depth >= max_depth or len(y) < 2:\n            return Node(prediction=np.mean(y))\n        \n        # Initialize variables to find the best split\n        best_sse = float('inf')\n        best_threshold = None\n        best_left_idx = None\n        best_right_idx = None\n\n        # Sort data by feature values for efficient split search\n        sorted_idx = np.argsort(X)\n        X_sorted = X[sorted_idx]\n        y_sorted = y[sorted_idx]\n\n        # Try all possible split points (between consecutive unique values)\n        for i in range(1, len(X_sorted)):\n            if X_sorted[i] == X_sorted[i - 1]:\n                continue  # skip identical values to avoid zero-width splits\n            threshold = (X_sorted[i] + X_sorted[i - 1]) / 2\n\n            # Split data\n            left_mask = X <= threshold\n            right_mask = X > threshold\n\n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue  # skip invalid splits\n\n            y_left = y[left_mask]\n            y_right = y[right_mask]\n\n            # Compute sum of squared errors for this split\n            sse_left = np.sum((y_left - np.mean(y_left)) ** 2)\n            sse_right = np.sum((y_right - np.mean(y_right)) ** 2)\n            total_sse = sse_left + sse_right\n\n            if total_sse < best_sse:\n                best_sse = total_sse\n                best_threshold = threshold\n                best_left_idx = left_mask\n                best_right_idx = right_mask\n\n        # If no valid split found, create a leaf node\n        if best_threshold is None:\n            return Node(prediction=np.mean(y))\n        \n        # Recursively build left and right subtrees\n        left_node = build_tree(X[best_left_idx], y[best_left_idx], depth + 1)\n        right_node = build_tree(X[best_right_idx], y[best_right_idx], depth + 1)\n        return Node(feature_index=0, threshold=best_threshold, left=left_node, right=right_node)\n\n    def predict_single(node, x):\n        # Traverse the tree to make a prediction\n        while node.left or node.right:\n            if node.feature_index is None:\n                # Leaf node\n                return node.prediction\n            if x <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n        return node.prediction\n\n    # Build the tree\n    tree = build_tree(X_train_np, y_train_np, depth=0)\n\n    # Generate predictions for X_test\n    predictions = [predict_single(tree, x) for x in X_test]\n    return predictions\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Convert input to a NumPy array for easy handling\n    arr = np.array(x)\n    \n    # Check if the array is one-dimensional\n    if arr.ndim != 1:\n        raise ValueError(\"Input must be a one-dimensional vector.\")\n    \n    # If the vector is empty, return an empty list\n    if arr.size == 0:\n        return []\n    \n    # Create a diagonal matrix with the vector elements on the diagonal\n    diag_matrix = np.diag(arr)\n    \n    # Convert the NumPy array to a list of lists\n    return diag_matrix.tolist()\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport itertools\nfrom typing import List, Tuple, Hashable, Iterable\n\ndef fp_growth(transactions: Iterable[Iterable[Hashable]], min_sup: int) -> List[Tuple]:\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n\n    # Step 1: Count item frequencies\n    item_counts = Counter()\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] += 1\n\n    # Remove items below min_sup\n    frequent_items = {item for item, count in item_counts.items() if count >= min_sup}\n\n    # If no items meet min_sup, return empty list\n    if not frequent_items:\n        return []\n\n    # Step 2: Build the header table with item counts\n    # and create a mapping for item to its total count\n    # Also, create a mapping for item to its node links\n    header_table = {}\n    for item in frequent_items:\n        header_table[item] = [item_counts[item], []]  # [count, list of node references]\n\n    # Step 3: Build the FP-tree\n    class FPNode:\n        def __init__(self, item, count, parent):\n            self.item = item\n            self.count = count\n            self.parent = parent\n            self.children = {}\n\n        def increment(self, count):\n            self.count += count\n\n        def path(self):\n            node = self\n            path_items = []\n            while node.parent is not None:\n                node = node.parent\n                if node.item is not None:\n                    path_items.append(node.item)\n            return path_items[::-1]  # reverse to get root-to-leaf order\n\n    # Initialize root of FP-tree\n    root = FPNode(None, 0, None)\n\n    # Function to insert a transaction into the FP-tree\n    def insert_transaction(transaction, node):\n        for item in transaction:\n            if item in node.children:\n                node.children[item].increment(1)\n            else:\n                new_node = FPNode(item, 1, node)\n                node.children[item] = new_node\n                # Update header table links\n                header_table[item][1].append(new_node)\n            node = node.children[item]\n\n    # Process transactions: filter and order items\n    for transaction in transactions:\n        # Filter out infrequent items\n        filtered = [item for item in transaction if item in frequent_items]\n        # Order items by descending frequency\n        ordered = sorted(filtered, key=lambda x: item_counts[x], reverse=True)\n        # Insert into FP-tree\n        insert_transaction(ordered, root)\n\n    # Step 4: Recursive mining function\n    def mine_fp_tree(node, suffix):\n        # Collect all items in header table sorted by increasing frequency\n        items = sorted(header_table.items(), key=lambda x: x[1][0])  # sort by count ascending\n        frequent_itemsets = []\n\n        for item, (count, node_links) in items:\n            new_suffix = (item,) + suffix\n            # Add the new frequent itemset\n            frequent_itemsets.append(tuple(sorted(new_suffix)))\n\n            # Build conditional pattern base\n            conditional_pattern_base = []\n            for n in node_links:\n                path = n.path()\n                if path:\n                    conditional_pattern_base.extend([path] * n.count)\n\n            # Count items in conditional pattern base\n            cond_item_counts = Counter()\n            for trans in conditional_pattern_base:\n                for itm in trans:\n                    cond_item_counts[itm] += 1\n\n            # Filter items below min_sup\n            cond_frequent_items = {itm for itm, cnt in cond_item_counts.items() if cnt >= min_sup}\n            if not cond_frequent_items:\n                continue\n\n            # Build conditional FP-tree\n            # Rebuild header table for conditional tree\n            cond_header_table = {}\n            for itm in cond_frequent_items:\n                cond_header_table[itm] = [cond_item_counts[itm], []]\n\n            # Build the conditional FP-tree\n            cond_root = FPNode(None, 0, None)\n\n            def insert_conditional_transaction(transaction, node):\n                for itm in transaction:\n                    if itm in cond_frequent_items:\n                        if itm in node.children:\n                            node.children[itm].increment(1)\n                        else:\n                            new_node = FPNode(itm, 1, node)\n                            node.children[itm] = new_node\n                            cond_header_table[itm][1].append(new_node)\n                        node = node.children[itm]\n\n            for trans in conditional_pattern_base:\n                filtered_trans = [itm for itm in trans if itm in cond_frequent_items]\n                ordered_trans = sorted(filtered_trans, key=lambda x: cond_item_counts[x], reverse=True)\n                insert_conditional_transaction(ordered_trans, cond_root)\n\n            # Recursive call\n            frequent_itemsets.extend(mine_fp_tree(cond_root, new_suffix))\n        return frequent_itemsets\n\n    # Step 5: Mine the FP-tree\n    result = mine_fp_tree(root, ())\n\n    # Remove duplicates and sort the result\n    unique_result = set(result)\n    # Sort by length, then lex order\n    sorted_result = sorted(unique_result, key=lambda x: (len(x), x))\n    return sorted_result\n```"}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    # Check for negative degree\n    if degree < 0:\n        return -1\n\n    # Convert input list to numpy array for easier manipulation\n    X_np = np.array(X)\n    n_samples, n_features = X_np.shape\n\n    # Generate all exponent combinations with total degree <= degree\n    exponents = []\n    for total_deg in range(degree + 1):\n        # Generate all combinations of exponents summing to total_deg\n        # using stars and bars approach\n        # But easier: generate all tuples of length n_features with sum = total_deg\n        # Using itertools.combinations_with_replacement is not directly suitable for this,\n        # so we can generate all possible exponent tuples with sum <= degree\n        # by iterating over all possible combinations with replacement of features\n        # and then generating exponents accordingly.\n        # But a more straightforward way:\n        # Generate all tuples of exponents with sum = total_deg\n        # Using integer partitioning approach:\n        # Generate all combinations of exponents with sum = total_deg\n        # via recursive or iterative method.\n\n        # To generate all exponent tuples with sum = total_deg:\n        # Use itertools.product with ranges, then filter sum\n        for exps in np.ndindex(*( [total_deg + 1] * n_features )):\n            if sum(exps) == total_deg:\n                exponents.append(exps)\n\n    # For each sample, compute all monomials\n    result = []\n    for sample in X_np:\n        row_features = []\n        for exp in exponents:\n            # Compute the product of feature^exponent\n            prod = 1.0\n            for feature_value, power in zip(sample, exp):\n                if power != 0:\n                    prod *= feature_value ** power\n            row_features.append(prod)\n        result.append(row_features)\n\n    return [row for row in result]\n```"}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    # Convert input to numpy array for consistency\n    y_array = np.array(y)\n    \n    # Check if the array is empty\n    if y_array.size == 0:\n        return -1\n    \n    # Calculate the mean of the sample\n    y_mean = np.mean(y_array)\n    \n    # Compute the squared differences\n    squared_diffs = (y_array - y_mean) ** 2\n    \n    # Calculate the MSE\n    mse_value = np.mean(squared_diffs)\n    \n    # Return the rounded result\n    return round(mse_value, 4)\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to numpy array for uniform processing\n    x_array = np.array(x, dtype=float)\n\n    # Compute the affine activation\n    y = slope * x_array + intercept\n\n    # First derivative is constant (slope)\n    grad = np.full_like(x_array, slope)\n\n    # Second derivative is zero\n    grad2 = np.zeros_like(x_array)\n\n    # Round all elements to 4 decimal places and convert to lists\n    y_list = [round(val, 4) for val in y]\n    grad_list = [round(val, 4) for val in grad]\n    grad2_list = [round(val, 4) for val in grad2]\n\n    return y_list, grad_list, grad2_list\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Validate k\n    n_samples, n_features = data.shape\n    if k < 1 or k > n_features:\n        return -1\n\n    # Step 1: Standardise each feature (zero mean, unit *population* variance)\n    # Compute mean for each feature\n    means = np.mean(data, axis=0)\n    # Center the data\n    centered_data = data - means\n    # Compute variance for each feature\n    variances = np.var(data, axis=0)\n    # Standardise features\n    # For features with zero variance, leave as zeros\n    std_devs = np.sqrt(variances)\n    # To avoid division by zero, handle zero variance features\n    std_devs_safe = np.where(std_devs == 0, 1, std_devs)\n    standardized_data = centered_data / std_devs_safe\n\n    # Step 2: Compute the sample covariance matrix (with Bessel's correction)\n    cov_matrix = np.cov(standardized_data, rowvar=False, ddof=1)\n\n    # Step 3: Eigen-decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Step 4: Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Fix the sign of each eigenvector\n    for i in range(sorted_eigenvectors.shape[1]):\n        # Find index of the entry with the largest absolute value\n        max_abs_idx = np.argmax(np.abs(sorted_eigenvectors[:, i]))\n        # If the entry is negative, flip the sign\n        if sorted_eigenvectors[max_abs_idx, i] < 0:\n            sorted_eigenvectors[:, i] *= -1\n\n    # Step 6: Project the standardized data onto the first k eigenvectors\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    projected_data = standardized_data @ top_k_eigenvectors\n\n    # Round to 4 decimal places\n    projected_rounded = np.round(projected_data, 4)\n\n    # Convert to list of lists\n    return projected_rounded.tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n\n    # Convert input lists to numpy arrays for easier manipulation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train).astype(float)\n    X_test_np = np.array(X_test)\n\n    # Initialize the model with the mean of y (initial prediction)\n    c = np.mean(y_train_np)\n    F = np.full_like(y_train_np, c)  # current prediction for each training sample\n\n    # Store the ensemble of weak learners: each is a dict with 'feature', 'threshold', 'left_value', 'right_value'\n    stumps = []\n\n    for m in range(n_estimators):\n        residuals = y_train_np - F  # residuals to fit the next stump\n\n        best_feature = None\n        best_threshold = None\n        best_loss = np.inf\n        best_left_value = None\n        best_right_value = None\n\n        # For each feature, find the best threshold\n        for feature_idx in range(X_train_np.shape[1]):\n            feature_values = X_train_np[:, feature_idx]\n            # Get sorted unique feature values to consider as thresholds\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                # Split data into left and right based on threshold\n                left_mask = feature_values <= threshold\n                right_mask = ~left_mask\n\n                # Compute mean residuals for left and right\n                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                    continue  # skip invalid splits\n\n                left_value = np.mean(residuals[left_mask])\n                right_value = np.mean(residuals[right_mask])\n\n                # Compute total squared error for this split\n                left_error = np.sum((residuals[left_mask] - left_value) ** 2)\n                right_error = np.sum((residuals[right_mask] - right_value) ** 2)\n                total_error = left_error + right_error\n\n                if total_error < best_loss:\n                    best_loss = total_error\n                    best_feature = feature_idx\n                    best_threshold = threshold\n                    best_left_value = left_value\n                    best_right_value = right_value\n\n        # After finding the best split, update the model\n        # Store the stump parameters\n        stumps.append({\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left_value': best_left_value,\n            'right_value': best_right_value\n        })\n\n        # Update the predictions F for training samples\n        feature_values = X_train_np[:, best_feature]\n        left_mask = feature_values <= best_threshold\n        right_mask = ~left_mask\n\n        # Update F with the new stump scaled by learning rate\n        F[left_mask] += learning_rate * best_left_value\n        F[right_mask] += learning_rate * best_right_value\n\n    # After training, compute the final predictions for X_test\n    F_test = np.full(X_test_np.shape[0], c)  # start from initial c\n    for stump in stumps:\n        feature_idx = stump['feature']\n        threshold = stump['threshold']\n        left_value = stump['left_value']\n        right_value = stump['right_value']\n        feature_values = X_test_np[:, feature_idx]\n        mask = feature_values <= threshold\n        F_test[mask] += learning_rate * left_value\n        F_test[~mask] += learning_rate * right_value\n\n    # Convert regression scores to binary class labels by thresholding at 0.5\n    predictions = (F_test >= 0.5).astype(int).tolist()\n\n    return predictions\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    # If the mini-batch is empty, return an empty list\n    if X.shape[0] == 0:\n        return []\n\n    m = X.shape[0]  # batch size\n\n    # Compute positive phase hidden probabilities\n    pos_hidden_probs = 1 / (1 + np.exp(-(np.dot(X, W) + hbias)))\n    # Sample hidden states (not needed for CD, only probabilities are used)\n    # But since the task specifies to use probabilities directly, we only need probs\n\n    # Initialize v_k and h_k with the probabilities\n    v_k = X.copy()\n    for _ in range(k):\n        # Compute hidden probabilities given visible\n        h_probs = 1 / (1 + np.exp(-(np.dot(v_k, W) + hbias)))\n        # Compute visible probabilities given hidden\n        v_probs = 1 / (1 + np.exp(-(np.dot(h_probs, W.T) + vbias)))\n        # Update v_k with probabilities (no stochastic sampling)\n        v_k = v_probs\n\n    # Compute hidden probabilities after k steps\n    h_k_probs = 1 / (1 + np.exp(-(np.dot(v_k, W) + hbias)))\n\n    # Compute positive and negative gradients\n    pos_grad = np.dot(X.T, pos_hidden_probs)\n    neg_grad = np.dot(v_k.T, h_k_probs)\n\n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n\n    # Round to 4 decimal places\n    W_rounded = np.round(W_new, 4)\n\n    # Convert to list of lists\n    return W_rounded.tolist()\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to numpy arrays if they aren't already\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n\n    # Check if vectors have the same length\n    if x_arr.shape != y_arr.shape:\n        return -1\n\n    # Compute the maximum absolute difference\n    max_diff = np.max(np.abs(x_arr - y_arr))\n\n    # Return the result rounded to four decimal places\n    return round(float(max_diff), 4)\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n\n    # Clip y_pred to avoid log(0)\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n\n    # Compute binary cross-entropy (reconstruction loss)\n    # Shape: (batch_size, n_features)\n    bce = - (y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n    # Sum over features for each sample\n    recon_loss = np.sum(bce, axis=1)\n\n    # Compute KL divergence\n    # Shape: (batch_size, latent_dim)\n    kl_div = -0.5 * (1 + t_log_var - t_mean**2 - np.exp(t_log_var))\n    # Sum over latent dimensions for each sample\n    kl_loss = np.sum(kl_div, axis=1)\n\n    # Total loss per sample\n    total_loss = recon_loss + kl_loss\n\n    # Average over batch\n    avg_loss = np.mean(total_loss)\n\n    # Round to 6 decimal places\n    return round(avg_loss, 6)\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X_np = np.array(X)  # shape: (n_samples, n_features)\n    w_np = np.array(w)  # shape: (n_features,)\n    V_np = np.array(V)  # shape: (n_features, k)\n\n    n_samples, n_features = X_np.shape\n    k = V_np.shape[1]\n\n    # Compute the linear part: w0 + X * w\n    linear_terms = np.dot(X_np, w_np)  # shape: (n_samples,)\n\n    # Compute the interaction terms\n    # Step 1: Compute (X * V) for each sample\n    # shape: (n_samples, k)\n    XV = np.dot(X_np, V_np)\n\n    # Step 2: Compute the squared sum: (\u03a3\u2c7c V_{j,f} x\u2c7c)^2\n    sum_squared = XV ** 2  # shape: (n_samples, k)\n\n    # Step 3: Compute the sum of squares: \u03a3\u2c7c V_{j,f}^2 x\u2c7c^2\n    V_squared = V_np ** 2  # shape: (n_features, k)\n    X_squared = X_np ** 2  # shape: (n_samples, n_features)\n    # For each sample, compute sum over features of V_{j,f}^2 * x_j^2\n    sum_Vx2 = np.dot(X_squared, V_squared)  # shape: (n_samples, k)\n\n    # Step 4: Compute the interaction term for each sample\n    interaction_terms = 0.5 * np.sum(sum_squared - sum_Vx2, axis=1)  # shape: (n_samples,)\n\n    # Final prediction\n    preds = w0 + linear_terms + interaction_terms\n\n    # Round to 4 decimal places\n    preds_rounded = np.round(preds, 4).tolist()\n\n    return preds_rounded\n```"}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Convert input to numpy array for uniform processing\n    z_array = np.array(z, dtype=np.float64)\n\n    # To ensure numerical stability, handle large positive and negative z\n    # For large positive z, sigmoid approaches 1\n    # For large negative z, sigmoid approaches 0\n    # Use the stable form:\n    # sigmoid(z) = 1 / (1 + exp(-z))\n    # but for large |z|, compute differently to avoid overflow\n\n    # Initialize result array\n    result = np.empty_like(z_array)\n\n    # For positive z, compute directly\n    pos_mask = z_array >= 0\n    result[pos_mask] = 1 / (1 + np.exp(-z_array[pos_mask]))\n\n    # For negative z, compute using exp(z) to avoid overflow\n    neg_mask = ~pos_mask\n    exp_z = np.exp(z_array[neg_mask])\n    result[neg_mask] = exp_z / (1 + exp_z)\n\n    # Round to four decimal places\n    result = np.round(result, 4)\n\n    # Return scalar if input was scalar\n    if np.isscalar(z):\n        return float(result)\n    else:\n        return result.tolist()\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Convert actual if needed\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        elif actual.ndim == 2 and actual.shape[1] == 1:\n            actual = actual.ravel()\n\n        # Convert predicted if needed\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        elif predicted.ndim == 2 and predicted.shape[1] == 1:\n            predicted = predicted.ravel()\n\n        return function(actual, predicted)\n\n    return wrapper\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a NumPy array for easy processing\n    arr = np.array(z, dtype=float)\n\n    # Compute the gradient element-wise\n    grad = arr * (1 - arr)\n\n    # Round the results to 4 decimal places\n    grad_rounded = np.round(grad, 4)\n\n    # Return in the same structure as input\n    if np.isscalar(z):\n        # If input was scalar, return scalar\n        return float(grad_rounded)\n    elif isinstance(z, list):\n        # If input was list, convert back to list\n        return grad_rounded.tolist()\n    else:\n        # If input was ndarray, return ndarray\n        return grad_rounded\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Convert input data to numpy array for easier manipulation\n    X_np = np.array(X)\n    w = np.array(W)\n\n    # Determine the range for each axis based on the data\n    x1_min, x1_max = X_np[:,0].min(), X_np[:,0].max()\n    x2_min, x2_max = X_np[:,1].min(), X_np[:,1].max()\n\n    # Create linearly spaced arrays for each axis\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n\n    # Create a meshgrid from these arrays\n    xx, yy = np.meshgrid(x1_plot, x2_plot)\n\n    # Flatten the grid to create a list of points for classification\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n\n    # Compute the linear combination for each point\n    # sign(w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b)\n    z = np.dot(grid_points, w) + b\n\n    # Apply the sign function: 1 if z >= 0, else -1\n    labels = np.where(z >= 0, 1, -1)\n\n    # Reshape labels back into a grid_n x grid_n list of lists\n    label_grid = labels.reshape((grid_n, grid_n))\n    return label_grid.tolist()\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    epsilon = 1e-9  # To avoid division by zero\n\n    # Separate training data by class\n    X_class0 = X_train[y_train == 0]\n    X_class1 = X_train[y_train == 1]\n\n    # Calculate mean and std for each feature per class\n    mu0 = np.mean(X_class0, axis=0)\n    sigma0 = np.std(X_class0, axis=0) + epsilon\n\n    mu1 = np.mean(X_class1, axis=0)\n    sigma1 = np.std(X_class1, axis=0) + epsilon\n\n    # Calculate class priors\n    prior0 = len(X_class0) / len(X_train)\n    prior1 = len(X_class1) / len(X_train)\n\n    predictions = []\n\n    for x in X_test:\n        # Compute likelihood for class 0\n        likelihood0 = np.prod(\n            (1 / (np.sqrt(2 * np.pi) * sigma0)) *\n            np.exp(- ((x - mu0) ** 2) / (2 * sigma0 ** 2))\n        )\n\n        # Compute likelihood for class 1\n        likelihood1 = np.prod(\n            (1 / (np.sqrt(2 * np.pi) * sigma1)) *\n            np.exp(- ((x - mu1) ** 2) / (2 * sigma1 ** 2))\n        )\n\n        # Compute posterior probabilities (unnormalized)\n        posterior0 = likelihood0 * prior0\n        posterior1 = likelihood1 * prior1\n\n        # Predict class with higher posterior\n        predicted_class = 0 if posterior0 > posterior1 else 1\n        predictions.append(predicted_class)\n\n    return predictions\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        AL (np.ndarray): Predicted probabilities, shape (m,) or (1,m).\n        Y (np.ndarray): True labels, shape (m,) or (1,m).\n\n    Returns:\n        float: The binary cross-entropy cost rounded to 4 decimal places.\n    \"\"\"\n    # Ensure AL and Y are 1D arrays\n    AL = AL.flatten()\n    Y = Y.flatten()\n\n    # Clip AL to avoid log(0)\n    epsilon = 1e-15\n    AL_clipped = np.clip(AL, epsilon, 1 - epsilon)\n\n    # Compute the cost\n    m = Y.shape[0]\n    cost = - (1 / m) * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n\n    # Return as a Python float rounded to 4 decimal places\n    return round(float(cost), 4)\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    pi = np.array(hmm['pi'])          # Initial state distribution\n    A = np.array(hmm['A'])            # Transition matrix\n    B = np.array(hmm['B'])            # Emission matrix\n\n    T = len(observations)\n    N = len(pi)\n\n    # Check if t is within valid range\n    if t < 0 or t >= T:\n        return []\n\n    # Forward pass: compute alpha\n    alpha = np.zeros((T, N))\n    # Initialization\n    alpha[0] = pi * B[:, observations[0]]\n    # Induction\n    for time in range(1, T):\n        alpha[time] = (alpha[time - 1] @ A) * B[:, observations[time]]\n\n    # Backward pass: compute beta\n    beta = np.zeros((T, N))\n    # Initialization\n    beta[T - 1] = np.ones(N)\n    # Induction\n    for time in range(T - 2, -1, -1):\n        beta[time] = A @ (B[:, observations[time + 1]] * beta[time + 1])\n\n    # Compute gamma for time t\n    numerator = alpha[t] * beta[t]\n    denominator = np.sum(numerator)\n    gamma = numerator / denominator\n\n    # Round to four decimal places\n    gamma_rounded = [round(prob, 4) for prob in gamma]\n\n    return gamma_rounded\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n\n    n_samples = data.shape[0]\n    # Step 1: Compute pairwise Euclidean distances\n    # Efficient way: use broadcasting or scipy.spatial.distance.cdist\n    # But since only numpy is allowed, we'll do broadcasting\n    # For large datasets, this might be slow, but acceptable here\n    distances = np.linalg.norm(data[:, np.newaxis, :] - data[np.newaxis, :, :], axis=2)\n\n    # Step 2: Build neighborhood list for each point\n    neighbors = [np.where(distances[i] <= eps)[0] for i in range(n_samples)]\n\n    # Step 3: Identify core points\n    core_points = [i for i in range(n_samples) if len(neighbors[i]) >= min_samples]\n\n    # Initialize labels: -1 for noise, 0 for unvisited\n    labels = [0] * n_samples\n    cluster_id = 0\n\n    # Step 4: Expand clusters\n    for point in range(n_samples):\n        if labels[point] != 0:\n            # Already visited\n            continue\n        if point not in core_points:\n            # Not a core point, mark as noise temporarily\n            labels[point] = -1\n            continue\n        # Start a new cluster\n        cluster_id += 1\n        labels[point] = cluster_id\n        queue = deque([point])\n\n        while queue:\n            current_point = queue.popleft()\n            for neighbor in neighbors[current_point]:\n                if labels[neighbor] == 0:\n                    # Unvisited\n                    labels[neighbor] = cluster_id\n                    if neighbor in core_points:\n                        queue.append(neighbor)\n                elif labels[neighbor] == -1:\n                    # Previously marked as noise, now assign to cluster\n                    labels[neighbor] = cluster_id\n\n    # Step 5: Remaining unlabeled points are noise\n    # (Already labeled as -1 or assigned to clusters)\n    return labels\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Performs Gaussian Mixture Model clustering using the Expectation-Maximization algorithm.\n    \n    Parameters:\n        data (np.ndarray): N x D array of samples.\n        k (int): Number of Gaussian components.\n        max_iter (int): Maximum number of EM iterations.\n        epsilon (float): Tolerance for convergence based on change in mixture weights.\n        \n    Returns:\n        list[int]: List of length N with cluster labels (0 to k-1), sorted by mean's first coordinate.\n    \"\"\"\n    N, D = data.shape\n    \n    # Handle the case when k=1: all samples belong to one cluster\n    if k == 1:\n        return [0] * N\n    \n    # 1. Initialization\n    # Randomly select k data points as initial means\n    rng = np.random.default_rng()\n    indices = rng.choice(N, size=k, replace=False)\n    means = data[indices]\n    \n    # Initialize covariance matrices to identity matrices\n    covariances = np.array([np.eye(D) for _ in range(k)])\n    \n    # Initialize mixture weights uniformly\n    weights = np.ones(k) / k\n    \n    # To store responsibilities\n    responsibilities = np.zeros((N, k))\n    \n    for iteration in range(max_iter):\n        # E-step: compute responsibilities\n        for j in range(k):\n            # Compute the multivariate normal density for all data points\n            diff = data - means[j]\n            # Compute the inverse and determinant for covariance\n            cov = covariances[j]\n            try:\n                inv_cov = np.linalg.inv(cov)\n                det_cov = np.linalg.det(cov)\n            except np.linalg.LinAlgError:\n                # In case covariance is singular, add small regularization\n                cov += np.eye(D) * 1e-6\n                inv_cov = np.linalg.inv(cov)\n                det_cov = np.linalg.det(cov)\n            norm_const = 1.0 / (np.power((2 * np.pi), D / 2) * np.sqrt(det_cov))\n            # Mahalanobis distance\n            mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n            responsibilities[:, j] = weights[j] * norm_const * np.exp(-0.5 * mahal)\n        \n        # Total responsibility sum for each data point\n        resp_sums = responsibilities.sum(axis=1, keepdims=True)\n        # To avoid division by zero\n        resp_sums[resp_sums == 0] = 1e-16\n        responsibilities /= resp_sums\n        \n        # M-step: update parameters\n        Nk = responsibilities.sum(axis=0)  # effective number of points per component\n        new_means = np.zeros((k, D))\n        new_covariances = np.zeros((k, D, D))\n        new_weights = Nk / N\n        \n        for j in range(k):\n            # Update means\n            weighted_sum = np.sum(responsibilities[:, j, np.newaxis] * data, axis=0)\n            new_means[j] = weighted_sum / Nk[j]\n            # Update covariances\n            diff = data - new_means[j]\n            weighted_cov = np.einsum('i,ij,ik->jk', responsibilities[:, j], diff, diff)\n            new_covariances[j] = weighted_cov / Nk[j]\n            # Regularize covariance to avoid singularity\n            new_covariances[j] += np.eye(D) * 1e-6\n        \n        # Check for convergence\n        weight_change = np.linalg.norm(new_weights - weights)\n        weights = new_weights\n        means = new_means\n        covariances = new_covariances\n        \n        if weight_change < epsilon:\n            break\n    \n    # Assign each sample to the component with the highest responsibility\n    labels = np.argmax(responsibilities, axis=1)\n    \n    # Re-label components based on the first coordinate of their means\n    # Sort components by means[:, 0]\n    sorted_indices = np.argsort(means[:, 0])\n    label_mapping = np.zeros(k, dtype=int)\n    for new_label, old_index in enumerate(sorted_indices):\n        label_mapping[old_index] = new_label\n    \n    # Map old labels to new labels\n    final_labels = [label_mapping[label] for label in labels]\n    \n    return final_labels\n```"}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    N = data.shape[0]\n    if n_cluster == 1:\n        return [0] * N\n\n    # 1. Build the similarity matrix W using Gaussian kernel\n    # Compute pairwise squared distances\n    sq_dists = np.sum((data[:, np.newaxis, :] - data[np.newaxis, :, :]) ** 2, axis=2)\n    W = np.exp(-gamma * sq_dists)\n\n    # 2. Construct the Laplacian\n    D = np.diag(np.sum(W, axis=1))\n    L = D - W\n\n    if method == 'normalized':\n        # Compute D^{-1/2}\n        with np.errstate(divide='ignore'):\n            D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\n        # Handle potential infinities if any degree is zero\n        D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0\n        # Compute normalized Laplacian\n        L = D_inv_sqrt @ L @ D_inv_sqrt\n\n    # 3. Compute eigenvectors corresponding to the smallest n_cluster eigenvalues\n    # Use np.linalg.eigh since L is symmetric\n    eigvals, eigvecs = np.linalg.eigh(L)\n    # Select the eigenvectors corresponding to the smallest eigenvalues\n    idx = np.argsort(eigvals)[:n_cluster]\n    U = eigvecs[:, idx]\n\n    # If normalized, row-normalize U\n    if method == 'normalized':\n        row_norms = np.linalg.norm(U, axis=1, keepdims=True)\n        # Avoid division by zero\n        row_norms[row_norms == 0] = 1\n        U = U / row_norms\n\n    # 4. Run k-means in the eigenvector space\n    # Initialize centroids: first n_cluster samples\n    centroids = U[:n_cluster, :].copy()\n\n    labels = np.zeros(N, dtype=int)\n    for _ in range(100):  # limit iterations for convergence\n        # Assign labels based on closest centroid\n        distances = np.linalg.norm(U[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        new_labels = np.argmin(distances, axis=1)\n\n        # Check for convergence\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n\n        # Update centroids\n        for k in range(n_cluster):\n            points_in_k = U[labels == k]\n            if len(points_in_k) > 0:\n                centroids[k] = np.mean(points_in_k, axis=0)\n            else:\n                # If a cluster gets empty, reinitialize its centroid\n                centroids[k] = U[np.random.randint(N)]\n\n    # 4b. Relabel clusters so that the cluster containing the smallest original index gets label 0, etc.\n    # Find the smallest original index in each cluster\n    cluster_min_indices = []\n    for k in range(n_cluster):\n        members = np.where(labels == k)[0]\n        if len(members) > 0:\n            cluster_min_indices.append((np.min(members), k))\n        else:\n            # If empty, assign a large index to avoid confusion\n            cluster_min_indices.append((N, k))\n    # Sort clusters by their smallest original index\n    cluster_min_indices.sort(key=lambda x: x[0])\n    # Create a mapping from old cluster label to new label\n    label_mapping = {old_k: new_k for new_k, (_, old_k) in enumerate(cluster_min_indices)}\n    relabeled = [label_mapping[lab] for lab in labels]\n\n    return relabeled\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N, d = X_data.shape\n    alpha = np.zeros(N)\n    b = 0.0\n\n    # Build Gram matrix G\n    G = X_data @ X_data.T  # shape: N x N\n\n    for iteration in range(max_iter):\n        error_made = False\n        for i in range(N):\n            # Compute g = sum_j alpha_j y_j G[j, i]\n            g = np.sum(alpha * y_data * G[:, i])\n            # Check the margin condition\n            if y_data[i] * (g + b) <= 0:\n                # Update alpha and b\n                alpha[i] += eta\n                b += eta * y_data[i]\n                error_made = True\n        # If no errors in this pass, convergence achieved\n        if not error_made:\n            break\n\n    # Recover weight vector w\n    w = np.sum((alpha * y_data)[:, np.newaxis] * X_data, axis=0)\n\n    # Round components to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    current_node = root\n    while True:\n        # If current node is a leaf, return its prediction\n        if hasattr(current_node, 'y'):\n            return current_node.y\n\n        # Internal node: decide based on task_type\n        feature_index = current_node.label\n        feature_value = features[feature_index]\n\n        if task_type == 'classification':\n            # Find the first child whose x matches the feature value\n            matched_child = None\n            for child in current_node.child:\n                if hasattr(child, 'x') and child.x == feature_value:\n                    matched_child = child\n                    break\n            # If no match found, default to second child\n            if matched_child is None:\n                matched_child = current_node.child[1]\n            current_node = matched_child\n\n        elif task_type == 'regression':\n            # Compare feature value with split point s\n            if feature_value <= current_node.s:\n                current_node = current_node.child[0]\n            else:\n                current_node = current_node.child[1]\n        else:\n            raise ValueError(\"Invalid task_type. Must be 'classification' or 'regression'.\")\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n\n    def entropy(labels):\n        \"\"\"Compute the entropy of a label distribution.\"\"\"\n        label_counts = np.bincount(labels)\n        total = len(labels)\n        probs = label_counts / total\n        # Filter out zero probabilities to avoid log(0)\n        probs = probs[probs > 0]\n        return -np.sum(probs * np.log2(probs))\n    \n    n_samples = y.shape[0]\n    # Compute entropy of the entire label set\n    H_y = entropy(y)\n\n    n_features = X.shape[1]\n    max_gain = -1\n    best_feature = -1\n\n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        unique_values = np.unique(feature_values)\n\n        # Initialize conditional entropy\n        H_y_given_A = 0.0\n\n        for val in unique_values:\n            # Get indices where feature == val\n            indices = np.where(feature_values == val)[0]\n            subset_labels = y[indices]\n            weight = len(indices) / n_samples\n            subset_entropy = entropy(subset_labels)\n            H_y_given_A += weight * subset_entropy\n\n        info_gain = H_y - H_y_given_A\n        # Round to 6 decimal places\n        info_gain_rounded = round(info_gain, 6)\n\n        if info_gain_rounded > max_gain:\n            max_gain = info_gain_rounded\n            best_feature = feature_idx\n        elif info_gain_rounded == max_gain:\n            # Tie-breaker: choose the smaller index\n            if feature_idx < best_feature:\n                best_feature = feature_idx\n\n    return (best_feature, max_gain)\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)  # Set seed for reproducibility\n    n_samples, n_features = data.shape\n\n    # Handle edge case: if epsilon=0, no outliers\n    if epsilon == 0:\n        return []\n\n    # Precompute c(n) for n=1..sample_size\n    # c(1)=0, for n>1: 2*ln(n-1)+0.5772156649 - 2*(n-1)/n\n    c_values = [0.0]\n    for n in range(2, sample_size + 1):\n        c_n = 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n        c_values.append(c_n)\n\n    # Function to compute c(n) for a given n\n    def c(n):\n        if n <= 1:\n            return 0.0\n        return c_values[n - 1]\n\n    # Function to build a single isolation tree\n    def build_tree(subsample_indices, depth, height_limit):\n        # If depth reaches limit or node has <=1 sample, create a leaf\n        if depth >= height_limit or len(subsample_indices) <= 1:\n            return {\n                'size': len(subsample_indices),\n                'indices': subsample_indices,\n                'split_feature': None,\n                'split_value': None,\n                'left': None,\n                'right': None\n            }\n\n        # Extract current data\n        current_data = data[subsample_indices]\n\n        # For splitting, choose a random feature\n        feature_idx = np.random.randint(n_features)\n        feature_values = current_data[:, feature_idx]\n\n        min_val = feature_values.min()\n        max_val = feature_values.max()\n\n        # If all values are the same, can't split further\n        if min_val == max_val:\n            return {\n                'size': len(subsample_indices),\n                'indices': subsample_indices,\n                'split_feature': None,\n                'split_value': None,\n                'left': None,\n                'right': None\n            }\n\n        # Choose a random threshold within [min, max]\n        threshold = np.random.uniform(min_val, max_val)\n\n        # Partition the data\n        left_mask = feature_values < threshold\n        right_mask = ~left_mask\n\n        left_indices = subsample_indices[left_mask]\n        right_indices = subsample_indices[right_mask]\n\n        # Recursively build left and right subtrees\n        left_subtree = build_tree(left_indices, depth + 1, height_limit)\n        right_subtree = build_tree(right_indices, depth + 1, height_limit)\n\n        return {\n            'size': len(subsample_indices),\n            'indices': subsample_indices,\n            'split_feature': feature_idx,\n            'split_value': threshold,\n            'left': left_subtree,\n            'right': right_subtree\n        }\n\n    # Function to compute path length for a single observation\n    def path_length(tree, x_idx):\n        node = tree\n        length = 0\n        while node['split_feature'] is not None:\n            feature_idx = node['split_feature']\n            threshold = node['split_value']\n            if data[x_idx, feature_idx] < threshold:\n                node = node['left']\n            else:\n                node = node['right']\n            length += 1\n        # At leaf, add c(n)\n        return length + c(node['size'])\n\n    # Build all trees\n    height_limit = math.ceil(math.log2(sample_size))\n    trees = []\n\n    for _ in range(n_trees):\n        # Sample without replacement if sample_size < n_samples\n        if sample_size >= n_samples:\n            sample_indices = np.arange(n_samples)\n        else:\n            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n        tree = build_tree(sample_indices, depth=0, height_limit=height_limit)\n        trees.append(tree)\n\n    # Compute average path length for each data point\n    path_lengths = np.zeros(n_samples)\n    for i in range(n_samples):\n        total_length = 0\n        for tree in trees:\n            total_length += path_length(tree, i)\n        path_lengths[i] = total_length / n_trees\n\n    # Compute the normalization factor \u03c6\n    phi = 2 * math.log(n_samples - 1) - 2 * (n_samples - 1) / n_samples\n\n    # Compute scores\n    scores = np.power(2, - (path_lengths / phi))\n\n    # Determine outliers: top epsilon fraction\n    if epsilon == 0:\n        return []\n\n    threshold = np.quantile(scores, 1 - epsilon)\n    outlier_indices = np.where(scores >= threshold)[0]\n    return sorted(outlier_indices.tolist())\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Implements a Maximum Entropy classifier using Generalized Iterative Scaling (GIS).\n    Args:\n        X_train: numpy array of shape (n_samples, n_features), categorical features.\n        y_train: numpy array of shape (n_samples,), labels.\n        X_test: numpy array of shape (m_samples, n_features), features to classify.\n        epsilon: convergence threshold.\n        n_iter: maximum number of iterations.\n    Returns:\n        List of predicted labels for X_test.\n    \"\"\"\n\n    # Step 1: Build the set of all possible labels\n    labels = np.unique(y_train)\n    label_to_index = {label: idx for idx, label in enumerate(labels)}\n    n_labels = len(labels)\n\n    n_samples, n_features = X_train.shape\n\n    # Step 2: Build feature-value pairs and empirical expectations\n    # For each feature index, find all unique feature values\n    feature_values = [np.unique(X_train[:, f]) for f in range(n_features)]\n    # Create a list of all feature-value pairs: (feature_index, feature_value)\n    feature_value_pairs = []\n    for f in range(n_features):\n        for val in feature_values[f]:\n            feature_value_pairs.append((f, val))\n    n_features_values = len(feature_value_pairs)\n\n    # Map each (feature_index, feature_value) to an index\n    f_idx_map = {fv: idx for idx, fv in enumerate(feature_value_pairs)}\n\n    # Step 3: Build empirical expectations\n    # For each (feature_value, label), count how many samples have that feature value and label\n    empirical_counts = np.zeros((n_features_values, n_labels))\n    for i in range(n_samples):\n        x = X_train[i]\n        y = y_train[i]\n        y_idx = label_to_index[y]\n        for f in range(n_features):\n            val = x[f]\n            fv = (f, val)\n            f_idx = f_idx_map[fv]\n            empirical_counts[f_idx, y_idx] += 1\n    # Normalize to get empirical expectations\n    empirical_expectation = empirical_counts / n_samples\n\n    # Step 4: Initialize weights to zero\n    weights = np.zeros(n_features_values)\n\n    # Precompute the feature indicator matrix for training data\n    # For each sample, for each feature-value pair, indicator is 1 if feature matches value, else 0\n    # But to save memory, we will compute feature expectations on the fly during updates\n\n    # Step 5: Define helper functions\n    def compute_Z(x, weights):\n        \"\"\"Compute normalization constant Z(x) for input x.\"\"\"\n        sum_exp = 0.0\n        for y_idx, y in enumerate(labels):\n            sum_wf = 0.0\n            for f in range(n_features):\n                val = x[f]\n                fv = (f, val)\n                f_idx = f_idx_map[fv]\n                sum_wf += weights[f_idx]\n            sum_exp += math.exp(sum_wf)\n        return sum_exp\n\n    def compute_conditional_probs(x, weights):\n        \"\"\"Compute P(y|x) for all labels.\"\"\"\n        denom = 0.0\n        scores = []\n        for y_idx, y in enumerate(labels):\n            sum_wf = 0.0\n            for f in range(n_features):\n                val = x[f]\n                fv = (f, val)\n                f_idx = f_idx_map[fv]\n                sum_wf += weights[f_idx]\n            score = math.exp(sum_wf)\n            scores.append(score)\n            denom += score\n        probs = [score / denom for score in scores]\n        return probs\n\n    # Step 6: Iterative optimization\n    for iteration in range(n_iter):\n        max_delta = 0.0\n        # Compute model expectations\n        model_expectation = np.zeros(n_features_values)\n        for i in range(n_samples):\n            x = X_train[i]\n            probs = compute_conditional_probs(x, weights)\n            for y_idx, y in enumerate(labels):\n                p_yx = probs[y_idx]\n                for f in range(n_features):\n                    val = x[f]\n                    fv = (f, val)\n                    f_idx = f_idx_map[fv]\n                    model_expectation[f_idx] += p_yx * (y == y_train[i])\n        # Normalize model expectation\n        model_expectation /= n_samples\n\n        # Update weights\n        for f_idx in range(n_features_values):\n            # To avoid division by zero, add a small epsilon if needed\n            empirical = empirical_expectation[f_idx]\n            model = model_expectation[f_idx]\n            # To prevent log(0), clip model to a small positive number\n            if model == 0:\n                model = 1e-12\n            delta = (1.0 / n_labels) * math.log(empirical / model) if model > 0 else 0.0\n            weights[f_idx] += delta\n            max_delta = max(max_delta, abs(delta))\n        if max_delta < epsilon:\n            break\n\n    # Step 7: Predict labels for X_test\n    predictions = []\n    for x in X_test:\n        probs = compute_conditional_probs(x, weights)\n        max_idx = np.argmax(probs)\n        predictions.append(labels[max_idx])\n\n    return predictions\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    total_docs = len(data)\n    word_doc_counts = Counter()\n\n    # Count in how many documents each word appears\n    for _, words in data:\n        unique_words = set(words)\n        for word in unique_words:\n            word_doc_counts[word] += 1\n\n    # Calculate ratios and sort from most common to least\n    ratios = []\n    for word, count in word_doc_counts.items():\n        ratio = round(count / total_docs, 4)\n        ratios.append((word, ratio))\n    # Sort by ratio descending; if tie, order doesn't matter\n    ratios.sort(key=lambda x: x[1], reverse=True)\n\n    # Determine the set of top words based on cut_off\n    if cut_off is None:\n        top_words = {word for word, _ in ratios}\n    elif cut_off == 0:\n        top_words = set()\n    else:\n        top_words = {word for word, _ in ratios[:cut_off]}\n\n    return ratios, top_words\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector w\n    # w = sum_i alpha_i * y_i * x_i\n    w = np.dot((alpha * y), X)  # shape: (n_features,)\n\n    # Round w to 4 decimal places\n    w_rounded = np.round(w, 4)\n\n    # Compute the bias term b\n    # b = (1/n) * sum_i ( y_i - w^T x_i )\n    n_samples = X.shape[0]\n    # Compute w^T x_i for all samples\n    wx = np.dot(X, w)\n    # Compute the differences y_i - (w^T x_i)\n    diffs = y - wx\n    b = np.mean(diffs)\n\n    # Round b to 4 decimal places\n    b_rounded = round(b, 4)\n\n    # Convert w to list\n    w_list = w_rounded.tolist()\n\n    return w_list, b_rounded\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Compute the LOF scores for each sample in data and identify outliers.\n\n    Parameters:\n    - data: numpy array of shape (n_samples, n_features)\n    - k: number of neighbors (integer)\n    - epsilon: threshold for outlier detection (float)\n\n    Returns:\n    - tuple: (list_of_LOF_scores, list_of_outlier_indices)\n    \"\"\"\n    n_samples = data.shape[0]\n    \n    # 1. Compute the full pairwise Euclidean distance matrix\n    # Efficiently compute using broadcasting\n    diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=2)\n    \n    # Initialize arrays\n    k_distance = np.zeros(n_samples)\n    neighbors_idx = [[] for _ in range(n_samples)]\n    \n    # 2. For each point, find its k-distance and neighbors\n    for i in range(n_samples):\n        # Exclude the point itself by setting its distance to infinity\n        dist_i = dist_matrix[i].copy()\n        dist_i[i] = np.inf\n        # Find the indices of the k nearest neighbors\n        neighbor_indices = np.argpartition(dist_i, k)[:k]\n        # Get the k-th nearest distance\n        kth_distance = np.max(dist_i[neighbor_indices])\n        k_distance[i] = kth_distance\n        neighbors_idx[i] = neighbor_indices.tolist()\n    \n    # 3. Define reachability distance\n    # reach-dist_k(p, q) = max(k-distance(q), dist(p, q))\n    reach_dist = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for q in neighbors_idx[i]:\n            reach_dist[i, q] = max(k_distance[q], dist_matrix[i, q])\n    \n    # 4. Compute Local Reachability Density (LRD)\n    LRD = np.zeros(n_samples)\n    for i in range(n_samples):\n        sum_reach_dist = 0.0\n        for q in neighbors_idx[i]:\n            sum_reach_dist += reach_dist[i, q]\n        # To avoid division by zero, add a small epsilon if sum_reach_dist is zero\n        if sum_reach_dist == 0:\n            LRD[i] = np.inf\n        else:\n            LRD[i] = k / sum_reach_dist\n    \n    # 5. Compute LOF scores\n    LOF_scores = np.zeros(n_samples)\n    for i in range(n_samples):\n        sum_LRD_q = 0.0\n        for q in neighbors_idx[i]:\n            sum_LRD_q += LRD[q]\n        LOF_scores[i] = (sum_LRD_q / k) / LRD[i]\n    \n    # Round scores to 4 decimal places\n    LOF_scores = np.round(LOF_scores, 4)\n    \n    # Identify outliers: scores > epsilon\n    outlier_indices = [i for i, score in enumerate(LOF_scores) if score > epsilon]\n    outlier_indices.sort()\n    \n    return (LOF_scores.tolist(), outlier_indices)\n```"}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    total_train_points = m * n_train\n    total_val_points = m * n_val\n    \n    X_train = np.empty((total_train_points, 2))\n    y_train = np.empty(total_train_points, dtype=int)\n    X_val = np.empty((total_val_points, 2))\n    y_val = np.empty(total_val_points, dtype=int)\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        # Generate training points\n        train_x = np.random.uniform(base + 1, base + 9, n_train)\n        train_y = np.random.uniform(base + 1, base + 9, n_train)\n        # Generate validation points\n        val_x = np.random.uniform(base + 1, base + 9, n_val)\n        val_y = np.random.uniform(base + 1, base + 9, n_val)\n        \n        # Round to 4 decimal places\n        train_x = np.round(train_x, 4)\n        train_y = np.round(train_y, 4)\n        val_x = np.round(val_x, 4)\n        val_y = np.round(val_y, 4)\n        \n        # Store in arrays\n        start_train_idx = c * n_train\n        start_val_idx = c * n_val\n        \n        X_train[start_train_idx:start_train_idx + n_train, 0] = train_x\n        X_train[start_train_idx:start_train_idx + n_train, 1] = train_y\n        y_train[start_train_idx:start_train_idx + n_train] = c\n        \n        X_val[start_val_idx:start_val_idx + n_val, 0] = val_x\n        X_val[start_val_idx:start_val_idx + n_val, 1] = val_y\n        y_val[start_val_idx:start_val_idx + n_val] = c\n    \n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n\n    # 1. Initialization: take the first k points as initial centers\n    centres = X[:k].copy()\n\n    labels = [-1] * m\n    for iteration in range(max_iters):\n        # 2. Assignment step\n        new_labels = []\n        for point in X:\n            # Compute distances to each centre\n            distances = np.linalg.norm(centres - point, axis=1)\n            # Assign to the nearest centre, tie-breaker by smaller index\n            label = np.argmin(distances)\n            new_labels.append(label)\n\n        # Check if assignments have changed\n        if new_labels == labels:\n            break\n        labels = new_labels\n\n        # 3. Update step\n        new_centres = np.zeros_like(centres)\n        counts = np.zeros(k, dtype=int)\n\n        for idx, label in enumerate(labels):\n            new_centres[label] += X[idx]\n            counts[label] += 1\n\n        for i in range(k):\n            if counts[i] > 0:\n                new_centres[i] /= counts[i]\n            else:\n                # Keep the centre unchanged if no points assigned\n                new_centres[i] = centres[i]\n\n        centres = new_centres\n\n    # Round centres to 4 decimal places\n    centres = np.round(centres, 4).tolist()\n\n    return labels, centres\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    # Initialize parameters\n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0.0\n\n    # If learning rate or iterations is zero, return initial parameters\n    if learning_rate == 0 or iterations == 0:\n        weights = w.round(4).tolist()\n        bias = round(b, 4)\n        return (weights, bias)\n\n    for _ in range(iterations):\n        # Predictions\n        y_hat = X @ w + b\n        # Residuals\n        r = y - y_hat\n\n        # Gradient for bias\n        grad_b = -2 * np.sum(r) / m\n\n        # Gradients for weights\n        # sign(w_j), with sign(0) = -1\n        sign_w = np.where(w >= 0, 1, -1)\n\n        # Compute gradient for each weight\n        grad_w = (-2 * X.T @ r) / m + (l1_penalty * sign_w) / m + (2 * l2_penalty * w) / m\n\n        # Update parameters\n        w = w - learning_rate * grad_w\n        b = b - learning_rate * grad_b\n\n    # Round results\n    weights = w.round(4).tolist()\n    bias = round(b, 4)\n\n    return (weights, bias)\n```"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Convert input vector to numpy array\n    current_input = np.array(x, dtype=float)\n\n    # Iterate through all layers except the last\n    for layer_idx in range(len(weights) - 1):\n        layer_weights = weights[layer_idx]\n        # Prepare weight matrix for current layer\n        # Each neuron: first weight is bias, remaining are input weights\n        # We will construct a matrix of shape (num_neurons, input_dim + 1)\n        weight_matrix = []\n        for neuron_weights in layer_weights:\n            weight_matrix.append(neuron_weights)\n        weight_matrix = np.array(weight_matrix, dtype=float)\n\n        # Add bias input '1' to current input\n        input_with_bias = np.concatenate(([1.0], current_input))\n        # Compute neuron pre-activations\n        pre_activations = weight_matrix @ input_with_bias\n        # Apply ReLU\n        current_input = np.maximum(pre_activations, 0)\n\n    # Process last layer (linear)\n    last_layer_weights = weights[-1]\n    weight_matrix = np.array(last_layer_weights, dtype=float)\n    input_with_bias = np.concatenate(([1.0], current_input))\n    output = weight_matrix @ input_with_bias\n\n    # Round the output(s) to 4 decimal places\n    if weight_matrix.shape[0] == 1:\n        # Single output neuron\n        return round(float(output), 4)\n    else:\n        # Multiple output neurons\n        return [round(float(val), 4) for val in output]\n```"}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Implement K-Means clustering to compute centroids only.\"\"\"\n    n_samples, n_features = X.shape\n    \n    # Initialize centroids with the first k samples\n    centroids = X[:k].astype(float)\n    \n    for _ in range(max_iters):\n        # Step 1: Assign samples to nearest centroid\n        # Compute squared Euclidean distances\n        # Efficient broadcasting: (n_samples, 1, n_features) - (1, k, n_features)\n        # Result: (n_samples, k)\n        distances = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2)\n        # Assign each sample to the closest centroid\n        labels = np.argmin(distances, axis=1)\n        \n        # Store old centroids for convergence check\n        old_centroids = centroids.copy()\n        \n        # Step 2: Update centroids\n        for i in range(k):\n            # Get all samples assigned to cluster i\n            cluster_points = X[labels == i]\n            if len(cluster_points) > 0:\n                # Update centroid to mean of assigned points\n                centroids[i] = np.mean(cluster_points, axis=0)\n            # If cluster is empty, keep centroid unchanged\n        \n        # Check for convergence: if all centroids move less than 1e-4\n        shifts = np.linalg.norm(centroids - old_centroids, axis=1)\n        if np.all(shifts < 1e-4):\n            break\n    \n    # Sort centroids lex order\n    sorted_centroids = sorted(centroids.tolist())\n    \n    # Round each coordinate to 4 decimal places\n    rounded_centroids = [\n        [round(coord, 4) for coord in centroid]\n        for centroid in sorted_centroids\n    ]\n    \n    return rounded_centroids\n```"}
