{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Implement Ridge (L2-regularized) Linear Regression using its closed-form solution.\n\n    Given an ndarray where the last column is the target `y` and all preceding columns are the features `X`,\n    add a bias (intercept) term, then compute the weight vector:\n\n    .. math::\n        \\hat{\\mathbf{w}} = (\\mathbf{X}^\\top\\mathbf{X} + \\alpha\\,\\mathbf{I})^{-1}\\,\\mathbf{X}^\\top\\mathbf{y},\n\n    where `alpha >= 0` is the regularization strength and `mathbf{I}` is the identity matrix with a size equal to the number of columns in `X` (after adding the bias).\n\n    If the matrix `(\\mathbf{X}^\\top\\mathbf{X} + \\alpha\\,\\mathbf{I})` is not invertible (numerical determinant equal to 0), return -1.\n\n    Return the weight vector rounded to 4 decimal places as a Python list.\n    \"\"\"\n    # Add a bias term to the data\n    num_samples, num_features = data.shape\n    X_with_bias = np.c_[np.ones((num_samples, 1)), data]\n    \n    # Compute the closed-form solution for Ridge regression\n    try:\n        # Calculate the inverse of (X^T * X + alpha * I)\n        inv_term = np.linalg.inv(X_with_bias.T.dot(X_with_bias) + alpha * np.eye(num_features + 1))\n        \n        # Calculate the weight vector\n        w = inv_term.dot(X_with_bias.T).dot(data[:, -1])\n        \n        # Round the weights to 4 decimal places\n        rounded_w = [round(w_i, 4) for w_i in w]\n        \n        return rounded_w\n    \n    except np.linalg.LinAlgError:\n        # Return -1 if the matrix is not invertible\n        return [-1]\n\n# Example usage:\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nalpha = 0.1\nprint(ridge_regression(data, alpha))\n```\n\nThis function first adds a bias term to the input data. It then computes the closed-form solution for Ridge regression using the formula provided. If the matrix `(X^T * X + alpha * I)` is not invertible, it catches the `LinAlgError` exception and returns `-1`. Otherwise, it calculates the weight vector, rounds each component to four decimal places, and returns the result as a list."}
{"task_id": 1, "completion_id": 0, "completion": "To implement the dual-form perceptron learning algorithm, we need to follow the steps outlined in the task description. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or -1) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    \n    # Initialize alpha and bias\n    alpha = np.zeros(n_samples)\n    b = 0\n    \n    # Compute the Gram matrix\n    K = np.dot(X_train, X_train.T)\n    \n    # Perform the perceptron learning algorithm\n    for _ in range(n_iter):\n        all_updated = True\n        for i in range(n_samples):\n            # Calculate the activation\n            activation = np.sum(alpha * y_train * K[:, i])\n            \n            # Check if the margin is <= 0\n            if y_train[i] * (activation + b) <= 0:\n                # Update alpha and bias\n                alpha[i] += eta\n                b += eta * y_train[i]\n                all_updated = False\n        \n        # Break if no updates were made\n        if all_updated:\n            break\n    \n    # Compute the primal weight vector w\n    w = np.dot(alpha, X_train)\n    \n    # Round the results to 4 decimal places\n    w_rounded = [round(val, 4) for val in w.tolist()]\n    b_rounded = round(b, 4)\n    \n    return w_rounded, b_rounded\n\n# Example usage:\n# X_train = np.array([[1, 2], [-1, -2], [2, 3], [-2, -3]])\n# y_train = np.array([1, -1, 1, -1])\n# w, b = perceptron_dual(X_train, y_train)\n# print(f\"Weight vector: {w}\")\n# print(f\"Bias: {b}\")\n```\n\n### Explanation:\n1. **Initialization**: We initialize `alpha` and `b` to zero.\n2. **Gram Matrix**: We compute the Gram matrix `K` which is the dot product of the training samples.\n3. **Perceptron Algorithm**: We iterate over each sample, calculate the activation using the current values of `alpha` and `b`. If the margin is less than or equal to zero, we update `alpha` and `b` accordingly.\n4. **Termination**: We stop if no updates are made in an iteration, which indicates convergence.\n5. **Primal Weight Vector**: We compute the primal weight vector `w` using the final `alpha`.\n6. **Rounding**: Finally, we round the weight vector and bias to four decimal places before returning them.\n\nThis implementation assumes that the input data is linearly separable, as required by the perceptron algorithm."}
{"task_id": 2, "completion_id": 0, "completion": "To implement the standard GLM link functions, we need to define three functions for each link:\n\n1. **Identity Link**: \\( g(\\mu) = \\mu \\)\n2. **Log Link**: \\( g(\\mu) = \\log(\\mu) \\)\n3. **Logit Link**: \\( g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right) \\)\n\nEach of these functions should handle both scalar and array inputs efficiently using NumPy's broadcasting capabilities.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    def identity_link(mu):\n        \"\"\"Link function for the identity link.\"\"\"\n        return mu\n    \n    def log_link(mu):\n        \"\"\"Link function for the log link.\"\"\"\n        return np.log(mu)\n    \n    def logit_link(mu):\n        \"\"\"Link function for the logit link.\"\"\"\n        return np.log(mu / (1 - mu))\n    \n    def identity_inv_link(eta):\n        \"\"\"Inverse link function for the identity link.\"\"\"\n        return eta\n    \n    def log_inv_link(eta):\n        \"\"\"Inverse link function for the log link.\"\"\"\n        return np.exp(eta)\n    \n    def logit_inv_link(eta):\n        \"\"\"Inverse link function for the logit link.\"\"\"\n        return 1 / (1 + np.exp(-eta))\n    \n    def identity_derivative(mu):\n        \"\"\"Derivative of the link function for the identity link.\"\"\"\n        return np.ones_like(mu)\n    \n    def log_derivative(mu):\n        \"\"\"Derivative of the link function for the log link.\"\"\"\n        return 1 / mu\n    \n    def logit_derivative(mu):\n        \"\"\"Derivative of the link function for the logit link.\"\"\"\n        return mu / (1 - mu)**2\n    \n    # Create the dictionary with all required functions\n    links = {\n        \"identity\": {\n            \"link\": identity_link,\n            \"inv_link\": identity_inv_link,\n            \"derivative\": identity_derivative\n        },\n        \"log\": {\n            \"link\": log_link,\n            \"inv_link\": log_inv_link,\n            \"derivative\": log_derivative\n        },\n        \"logit\": {\n            \"link\": logit_link,\n            \"inv_link\": logit_inv_link,\n            \"derivative\": logit_derivative\n        }\n    }\n    \n    return links\n\n# Example usage\nlinks = glm_links()\n\nmu = np.array([0.2, 0.8])\neta = links[\"logit\"][\"link\"](mu)        # \u2192 [-1.3863, 1.3863]\nmu2 = links[\"logit\"][\"inv_link\"](eta)    # \u2192 [0.2, 0.8]\n\nprint(\"mu:\", mu)\nprint(\"eta:\", eta)\nprint(\"mu2:\", mu2)\n```\n\n### Explanation:\n- **Identity Link**: This is simply the identity function, so it returns `mu` directly.\n- **Log Link**: The natural logarithm of `mu`.\n- **Logit Link**: The log of the odds ratio, which is \\(\\log\\left(\\frac{mu}{1-mu}\\right)\\).\n  \nThe inverse link functions (`inv_link`) are the inverses of these link functions:\n- **Identity Inv Link**: Returns `eta` directly.\n- **Log Inv Link**: Exponentiates `eta` to get back `mu`.\n- **Logit Inv Link**: Applies the logistic function to `eta`.\n\nThe derivatives are also implemented:\n- **Identity Derivative**: Always returns 1, since the derivative of the identity function is constant.\n- **Log Derivative**: Returns \\( \\frac{1}{\\mu} \\).\n- **Logit Derivative**: Returns \\( \\frac{\\mu}{(1-\\mu)^2} \\).\n\nThis implementation ensures that all functions handle both scalar and array inputs efficiently."}
{"task_id": 3, "completion_id": 0, "completion": "To solve the problem of computing the depth of the leaf reached by every sample in a random isolation tree, we need to follow these steps:\n\n1. Set the random seed to ensure reproducibility.\n2. Implement the logic for building the isolation tree recursively.\n3. Traverse the tree to compute the depth of each leaf for each sample.\n4. Return the list of depths.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    def build_isolation_tree(indices):\n        if len(indices) < 3:\n            return Counter({tuple(indices): 1})\n        \n        f = np.random.randint(data.shape[1])\n        down = np.min(data[indices, f])\n        up = np.max(data[indices, f])\n        v = np.random.uniform(down, up)\n        \n        left_indices = indices[data[indices, f] <= v]\n        right_indices = indices[data[indices, f] > v]\n        \n        left_counts = build_isolation_tree(left_indices)\n        right_counts = build_isolation_tree(right_indices)\n        \n        return left_counts + right_counts\n    \n    depths = []\n    \n    for i in range(data.shape[0]):\n        indices = np.array([i], dtype=int)\n        counts = build_isolation_tree(indices)\n        depth = len(counts.keys())\n        depths.append(depth)\n    \n    return depths\n\n# Example usage\ndata = np.arange(5).reshape(-1, 1)\nprint(isolation_tree_path_length(data))\n```\n\n### Explanation:\n\n1. **Random Seed**: We use `np.random.seed(0)` to ensure that the random splits are deterministic and reproducible.\n\n2. **Tree Construction**: The `build_isolation_tree` function constructs the isolation tree recursively:\n   - If the number of samples (`len(indices)`) is less than 3, it returns a counter with the current indices as the key and a count of 1.\n   - Otherwise, it selects a random feature and splits the samples based on the minimum and maximum values of that feature.\n   - It recursively builds the left and right subtrees and combines their results.\n\n3. **Depth Calculation**: For each sample, we generate its indices, build the tree, and determine the depth by counting the number of keys in the resulting counter.\n\n4. **Result Collection**: We collect the depths for each sample and return them as a list.\n\nThis implementation ensures that the tree is built correctly and that the depth of each leaf is computed accurately."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to compute the pair-wise squared Euclidean distances between all pairs of samples in the input matrix `X`. We will use the given identity:\n\n\\[\n\\|x - y\\|^2 = \\|x\\|^2 + \\|y\\|^2 - 2 \\cdot x^T y\n\\]\n\nThis identity allows us to avoid computing the full inner product matrix directly, which would be inefficient due to its quadratic complexity. Instead, we can leverage vectorized operations provided by NumPy to achieve this efficiently.\n\nHere's the complete implementation of the `l2_distance` function:\n\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    \n    # Compute the sum of squares for each sample\n    sum_of_squares = np.sum(X**2, axis=1)\n    \n    # Compute the outer product of the sum of squares\n    outer_product = np.outer(sum_of_squares, sum_of_squares)\n    \n    # Subtract twice the dot product of X with itself\n    # This effectively gives us the cross-term term in the identity\n    cross_term = -2 * np.dot(X.T, X)\n    \n    # Combine the results to get the distance matrix\n    distance_matrix = outer_product + cross_term\n    \n    # Round the distances to 4 decimal places\n    distance_matrix_rounded = np.round(distance_matrix, decimals=4)\n    \n    # Convert the result to a Python list of lists\n    distance_matrix_list = distance_matrix_rounded.tolist()\n    \n    return distance_matrix_list\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# print(l2_distance(X))\n```\n\n### Explanation:\n\n1. **Check for Empty Input**: If the input matrix `X` is empty (`X.shape[0] == 0`), we immediately return an empty list.\n\n2. **Compute Sum of Squares**: We first calculate the sum of squares for each sample along the rows using `np.sum(X**2, axis=1)`.\n\n3. **Outer Product**: We then compute the outer product of these sums of squares using `np.outer(sum_of_squares, sum_of_squares)`, which gives us a matrix where each element `(i, j)` is the square of the distance between the i-th and j-th samples.\n\n4. **Cross-Term Calculation**: We compute the cross-term `-2 * np.dot(X.T, X)`, which accounts for the difference in norms and their dot product.\n\n5. **Combine Results**: We add the cross-term to the outer product to obtain the final distance matrix.\n\n6. **Rounding and Conversion**: Finally, we round the distance matrix to 4 decimal places using `np.round()` and convert it to a Python list of lists using `tolist()`.\n\nThis approach ensures that the computation is efficient and leverages NumPy's optimized operations to handle large datasets."}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        A list where each element is a 2-D array-like object of shape\n        (n_samples, n_classes) containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning rate used by the booster.  This value scales every tree\n        output before it is added to the ensemble score.\n\n    Returns\n    -------\n    list[int]\n        A list of length *n_samples* with the predicted class label for every\n        sample.\n    \"\"\"\n    # Initialize the aggregated scores\n    aggregated_scores = np.zeros_like(predictions[0])\n    \n    # Accumulate the tree outputs scaled by the learning rate\n    for prediction in predictions:\n        aggregated_scores += learning_rate * prediction\n    \n    # Apply softmax row-wise to get the probability distributions\n    probabilities = np.exp(aggregated_scores)\n    \n    # Find the argmax along the second axis to get the predicted class labels\n    predicted_classes = np.argmax(probabilities, axis=1)\n    \n    return predicted_classes.tolist()\n```"}
{"task_id": 7, "completion_id": 0, "completion": "To solve the problem of finding frequent itemsets using the Apriori algorithm, we need to follow these steps:\n\n1. Calculate the total number of transactions.\n2. Compute the minimum support threshold based on the input `minsup`.\n3. Generate all possible candidate itemsets of size 1.\n4. Filter out infrequent itemsets from step 3.\n5. Use the filtered itemsets to generate candidate itemsets of size 2, 3, etc., until no more candidates can be generated.\n6. Count the support of each itemset.\n7. Return the itemsets that meet the minimum support threshold.\n\nHere's the implementation of the `find_frequent_itemsets` function:\n\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Step 1: Calculate the total number of transactions\n    total_transactions = len(transactions)\n    \n    # Step 2: Compute the minimum support threshold\n    min_support_threshold = math.ceil(minsup * total_transactions)\n    \n    # Step 3: Generate all possible candidate itemsets of size 1\n    itemsets_1 = { frozenset([item]) for transaction in transactions for item in transaction }\n    \n    # Step 4: Filter out infrequent itemsets from step 3\n    frequent_itemsets_1 = { itemset: sum(1 for transaction in transactions if itemset <= transaction) for itemset in itemsets_1 if sum(1 for transaction in transactions if itemset <= transaction) >= min_support_threshold }\n    \n    # Step 5: Use the filtered itemsets to generate candidate itemsets of size 2, 3, etc., until no more candidates can be generated\n    k = 2\n    while True:\n        itemsets_k = set()\n        for itemset in frequent_itemsets_1:\n            for combination in combinations(itemset, k):\n                new_itemset = frozenset(combination)\n                itemsets_k.add(new_itemset)\n        \n        # Filter out infrequent itemsets from step 5\n        frequent_itemsets_k = { itemset: sum(1 for transaction in transactions if itemset <= transaction) for itemset in itemsets_k if sum(1 for transaction in transactions if itemset <= transaction) >= min_support_threshold }\n        \n        # Check if there are no more frequent itemsets\n        if not frequent_itemsets_k:\n            break\n        \n        # Update the frequent itemsets\n        frequent_itemsets_1 = frequent_itemsets_k\n        k += 1\n    \n    # Step 6: Count the support of each itemset\n    # This step is redundant because we already computed it in step 4 and 5\n    # frequent_itemsets = frequent_itemsets_1\n    \n    # Step 7: Return the itemsets that meet the minimum support threshold\n    return frequent_itemsets_1\n\n# Example usage\ntransactions = [\n    ['bread', 'milk'],\n    ['bread', 'diaper', 'beer', 'egg'],\n    ['milk', 'diaper', 'beer', 'coke'],\n    ['bread', 'milk', 'diaper', 'beer'],\n    ['bread', 'milk', 'diaper', 'coke']\n]\nminsup = 0.6\nprint(find_frequent_itemsets(transactions, minsup))\n```\n\n### Explanation:\n- **Step 1**: We calculate the total number of transactions.\n- **Step 2**: We compute the minimum support threshold.\n- **Step 3**: We generate all possible candidate itemsets of size 1.\n- **Step 4**: We filter out infrequent itemsets from step 3.\n- **Step 5**: We use the filtered itemsets to generate candidate itemsets of size 2, 3, etc., until no more candidates can be generated.\n- **Step 6**: We count the support of each itemset.\n- **Step 7**: We return the itemsets that meet the minimum support threshold.\n\nThis implementation efficiently finds all frequent itemsets using the Apriori algorithm, adhering to the constraints and requirements specified."}
{"task_id": 8, "completion_id": 0, "completion": "To implement binary Logistic Regression from scratch without using any third-party ML libraries, we need to follow these steps:\n\n1. Initialize the weights (`w`) and bias (`b`) to zero.\n2. Define the sigmoid activation function.\n3. Implement the forward pass to compute the linear score (`z`), apply the sigmoid, and convert it to a prediction.\n4. Implement the backward pass to compute the gradients of the loss function with respect to the weights and bias.\n5. Update the weights and bias using the computed gradients and the learning rate.\n6. Repeat the process for a specified number of iterations.\n7. Use the trained model to make predictions on new data.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nimport math\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        List with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n    # Initialize weights and bias\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0\n\n    # Function to calculate the sigmoid\n    def sigmoid(z):\n        return 1 / (1 + math.exp(-z))\n\n    # Train the model\n    for _ in range(n_iterations):\n        # Forward pass\n        z = np.dot(X_train, w) + b\n        probabilities = sigmoid(z)\n        \n        # Calculate the loss\n        loss = -np.mean(y_train * np.log(probabilities) + (1 - y_train) * np.log(1 - probabilities))\n        \n        # Backward pass\n        dw = (1 / X_train.shape[0]) * np.dot(X_train.T, (probabilities - y_train))\n        db = (1 / X_train.shape[0]) * np.sum(probabilities - y_train)\n        \n        # Update weights and bias\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n    # Predict labels for test data\n    z_test = np.dot(X_test, w) + b\n    probabilities_test = sigmoid(z_test)\n    predicted_labels = [int(round(p)) for p in probabilities_test]\n\n    return predicted_labels\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Load your dataset\n# predicted_labels = logistic_regression_predict(X_train, y_train, X_test)\n```\n\n### Explanation:\n\n- **Initialization**: We initialize the weights (`w`) and bias (`b`) to zero.\n- **Sigmoid Function**: The sigmoid function is used to map the linear scores to probabilities between 0 and 1.\n- **Forward Pass**: For each sample, we compute the linear score, apply the sigmoid, and convert it to a prediction.\n- **Loss Calculation**: We use the binary cross-entropy loss to measure how well our model is performing.\n- **Backward Pass**: We compute the gradients of the loss with respect to the weights and bias.\n- **Gradient Descent Update**: We update the weights and bias using the computed gradients and the learning rate.\n- **Prediction**: After training, we use the trained model to predict the labels for the test data.\n\nThis implementation provides a basic framework for training a binary Logistic Regression model from scratch. Adjust the learning rate and number of iterations as needed based on your specific dataset and requirements."}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (\\(\\tilde{X}_{t-1}\\)).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (\\(X_t\\)).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \\(\\varepsilon\\), 0 <= weight <= 1.  Scalars or one-\n        dimensional vectors are supported; vectors are used element-wise.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    prev_array = np.array(prev)\n    cur_array = np.array(cur)\n    weight_array = np.array(weight)\n    \n    # Ensure weight is within the valid range [0, 1]\n    weight_array = np.clip(weight_array, 0, 1)\n    \n    # Perform element-wise exponential smoothing\n    smoothed_array = weight_array * prev_array + (1 - weight_array) * cur_array\n    \n    # Round the results to four decimal places\n    smoothed_array_rounded = np.round(smoothed_array, 4)\n    \n    # Return the result based on the input type\n    if prev_array.size == 1 and cur_array.size == 1 and weight_array.size == 1:\n        return smoothed_array_rounded[0]\n    else:\n        return smoothed_array_rounded.tolist()\n\n# Example usage:\nprint(smooth(10.0, 12.0, 0.8))  # Output: 10.4\n```\n\nThis implementation handles both scalar and vector inputs, ensures the smoothing factor is within the valid range, performs element-wise smoothing, rounds the result to four decimal places, and returns the appropriate data structure based on the input types."}
{"task_id": 11, "completion_id": 0, "completion": "Here's the implementation of the K-Means clustering algorithm from scratch in Python:\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    # Step 1: Initialize the centers randomly\n    if k > data.shape[0]:\n        raise ValueError(\"Number of clusters cannot exceed the number of samples.\")\n    \n    centers = data[:k].copy()\n    \n    # Step 2: Iterate until convergence or max_iterations reached\n    for _ in range(max_iter):\n        # Step 2a: Assign each sample to the nearest center\n        distances = np.linalg.norm(data[:, np.newaxis] - centers[np.newaxis], axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 2b: Re-compute the centers\n        new_centers = []\n        for i in range(k):\n            # Find samples in the current cluster\n            cluster_samples = data[labels == i]\n            \n            # If the cluster is empty, reinitialize it\n            if cluster_samples.size == 0:\n                new_center = data[np.random.randint(0, data.shape[0])]\n            else:\n                new_center = np.mean(cluster_samples, axis=0)\n            \n            # Round the new center to 4 decimal places\n            new_center_rounded = np.round(new_center, decimals=4)\n            new_centers.append(new_center_rounded.tolist())\n        \n        # Convert the list of lists to a NumPy array for comparison\n        new_centers = np.array(new_centers)\n        \n        # Check for convergence\n        if np.allclose(centers, new_centers, atol=epsilon):\n            break\n        \n        # Update the centers\n        centers = new_centers\n    \n    return centers.tolist(), labels.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]])\n# k = 2\n# centers, labels = kmeans(data, k)\n# print(\"Centers:\", centers)\n# print(\"Labels:\", labels)\n```\n\n### Explanation:\n1. **Initialization**: The initial centers are chosen randomly from the first `k` samples of the input data.\n\n2. **Iteration**: \n   - Each sample is assigned to the nearest center based on the squared Euclidean distance.\n   - The centers are updated by computing the mean of all samples in each cluster.\n   - If a cluster becomes empty after updating, a new center is initialized randomly from the entire dataset.\n\n3. **Convergence**: The algorithm stops when the maximum change in any center position is less than `epsilon` or when the maximum number of iterations (`max_iter`) is reached.\n\n4. **Output**: The function returns the list of final centers rounded to four decimal places and the corresponding cluster labels for each sample."}
{"task_id": 12, "completion_id": 0, "completion": "To solve this problem, we need to compute the power spectrum for each frame in the input 2-D NumPy array. The power spectrum is obtained by taking the squared magnitude of the Discrete Fourier Transform (DFT) of each frame. Here's how you can implement the `power_spectrum` function:\n\n```python\nimport numpy as np\n\ndef power_spectrum(frames: np.ndarray, scale: bool = False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True the resulting power spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        The power spectrum of each frame (only positive frequencies), rounded\n        to four decimal places and returned as a regular Python list.\n    \"\"\"\n    # Initialize an empty list to store the power spectra\n    power_spectra = []\n\n    # Iterate over each frame in the input array\n    for frame in frames:\n        # Compute the real FFT of the frame\n        fft_result = np.fft.rfft(frame)\n\n        # Calculate the power spectrum by squaring the magnitudes\n        power_spectrum = np.abs(fft_result)**2\n\n        # If scaling is requested, divide by N/2 + 1\n        if scale:\n            power_spectrum /= len(power_spectrum)\n\n        # Round the power spectrum to 4 decimal places and append to the list\n        power_spectra.append(np.round(power_spectrum, decimals=4).tolist())\n\n    return power_spectra\n\n# Example usage:\n# frames = np.array([[1, 2, 3], [4, 5, 6]])\n# print(power_spectrum(frames))\n```\n\n### Explanation:\n1. **Input Validation**: We assume the input `frames` is a 2-D NumPy array of shape `(M, N)`, where `M` is the number of frames and `N` is the number of samples per frame.\n\n2. **Loop Through Frames**: We iterate over each frame in the `frames` array.\n\n3. **FFT Computation**: For each frame, we compute the real FFT using `np.fft.rfft`. This function returns the first half of the DFT coefficients, which correspond to the positive frequencies.\n\n4. **Power Spectrum Calculation**: We calculate the power spectrum by squaring the absolute values of the FFT coefficients. This gives us the squared magnitudes at each frequency.\n\n5. **Scaling**: If the `scale` parameter is `True`, we divide the power spectrum by `N/2 + 1` to normalize it.\n\n6. **Rounding**: We round each element of the power spectrum to four decimal places using `np.round` and convert it to a regular Python list using `.tolist()`.\n\n7. **Return Result**: Finally, we return the list of power spectra."}
{"task_id": 13, "completion_id": 0, "completion": "To implement the item-based k-NN collaborative filtering recommender in pure Python using NumPy, we need to follow the steps outlined in the task description. Below is the complete implementation:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_users, n_items = data.shape\n    \n    if n_users <= 1:\n        return []\n    \n    # Step 1: Build the item-item similarity matrix\n    S = np.zeros((n_items, n_items))\n    \n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            common_users = np.intersect1d(np.where(data[:, i] > 0)[0], np.where(data[:, j] > 0)[0])\n            \n            if len(common_users) > 0:\n                v1 = data[common_users, i].astype(float)\n                v2 = data[common_users, j].astype(float)\n                \n                if criterion == 'cosine':\n                    mean_v1 = np.mean(v1)\n                    std_v1 = np.std(v1)\n                    mean_v2 = np.mean(v2)\n                    std_v2 = np.std(v2)\n                    \n                    if std_v1 > 1e-3 and std_v2 > 1e-3:\n                        v1_centered = v1 - mean_v1\n                        v2_centered = v2 - mean_v2\n                        \n                        numerator = np.dot(v1_centered, v2_centered)\n                        denominator = np.linalg.norm(v1_centered) * np.linalg.norm(v2_centered)\n                        \n                        S[i, j] = numerator / denominator\n                        S[j, i] = numerator / denominator\n                    else:\n                        S[i, j] = 0\n                        S[j, i] = 0\n                \n                elif criterion == 'pearson':\n                    corr_matrix = np.corrcoef(v1, v2)\n                    S[i, j] = corr_matrix[0, 1]\n                    S[j, i] = corr_matrix[0, 1]\n    \n    # Step 2: Collect the indices of items already rated by the active user\n    rated_items = np.where(data[user_ind] > 0)[0]\n    r = data[user_ind, rated_items]\n    \n    # Step 3: Predict the attractiveness of each yet unrated item\n    predictions = {}\n    \n    for t in range(n_items):\n        if t not in rated_items:\n            numerator = np.sum(r * S[t, rated_items])\n            denominator = np.sum(np.abs(S[t, rated_items]))\n            \n            if denominator == 0:\n                score = 0\n            else:\n                score = numerator / denominator\n            \n            predictions[t] = score\n    \n    # Step 4: Sort the predictions and return the top k items\n    sorted_predictions = sorted(predictions.items(), key=lambda x: (-x[1], x[0]))\n    \n    return [item for item, _ in sorted_predictions[:k]]\n\n# Example usage:\n# data = np.array([\n#     [5, 3, 0, 1],\n#     [4, 0, 0, 1],\n#     [1, 1, 0, 5],\n#     [1, 0, 0, 4],\n#     [0, 1, 5, 4]\n# ])\n# user_ind = 0\n# k = 3\n# print(knn_recommend(data, user_ind, k))  # Output: [2, 3, 4]\n```\n\n### Explanation:\n1. **Building the Similarity Matrix**: We iterate over all pairs of items and find the common users who rated both items. Depending on the criterion, we calculate either the cosine similarity or the Pearson correlation.\n\n2. **Collecting Rated Items**: We identify which items the active user has already rated.\n\n3. **Predicting Attractiveness**: For each unrated item, we calculate the predicted attractiveness using the formula provided in the task description.\n\n4. **Sorting and Returning Results**: We sort the predictions based on their scores and return the indices of the top `k` items.\n\nThis implementation ensures that the recommendations are based on the similarity between items and the user's past behavior."}
{"task_id": 14, "completion_id": 0, "completion": "To implement the binary logistic loss, its gradient, and Hessian, we can follow these steps:\n\n1. Compute the probability \\( p_i = \\sigma(\\hat{y}_i) = \\frac{1}{1 + e^{-\\hat{y}_i}} \\) for each sample.\n2. Calculate the loss for each sample using the formula:\n   \\[\n   \\text{loss}_i = -[y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n   \\]\n3. Compute the gradient for each sample:\n   \\[\n   \\nabla_i = p_i - y_i\n   \\]\n4. Compute the Hessian diagonal for each sample:\n   \\[\n   H_i = p_i (1 - p_i)\n   \\]\n5. Return the mean of the losses, the gradients, and the Hessian diagonals, rounding all values to 4 decimal places.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimal places.\n    \"\"\"\n    # Ensure y_pred is within the valid range [-inf, inf] to prevent overflow\n    y_pred = np.clip(y_pred, -709, 709)\n    \n    # Calculate probabilities\n    p = 1 / (1 + np.exp(-y_pred))\n    \n    # Avoid log(0) by adding a small epsilon\n    eps = 1e-15\n    p = np.where(p > eps, p, eps)\n    p = np.where(p < 1 - eps, p, 1 - eps)\n    \n    # Calculate losses\n    losses = - (y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    # Calculate gradients\n    gradients = p - y\n    \n    # Calculate Hessian diagonals\n    hessians = p * (1 - p)\n    \n    # Return mean loss, gradients, and hessians\n    return (\n        np.mean(losses).round(4),\n        [x.round(4) for x in gradients],\n        [x.round(4) for x in hessians]\n    )\n\n# Example usage:\ny = np.array([0, 1, 0, 1])\ny_pred = np.array([-10, 0, 10, 0])\nresult = logistic_loss_metrics(y, y_pred)\nprint(result)\n```\n\nThis function will compute the required metrics for the given input arrays `y` and `y_pred`. The results are returned as a tuple containing the mean loss, the gradients, and the Hessian diagonals, all rounded to 4 decimal places."}
{"task_id": 16, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `ngrams` function according to the specified requirements:\n\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    if N < 1:\n        raise ValueError(\"N must be at least 1\")\n    \n    result = []\n    for i in range(len(sequence) - N + 1):\n        result.append(tuple(sequence[i:i+N]))\n    \n    return result\n\n# Example usage:\nprint(ngrams([1, 2, 3, 4], 2))  # Output: [(1, 2), (2, 3), (3, 4)]\nprint(ngrams('hello', 3))      # Output: [('h', 'e', 'l'), ('e', 'l', 'l'), ('l', 'l', 'o')]\nprint(ngrams([1, 2, 3], 5))     # Output: []\n```\n\n### Explanation:\n- The function first checks if `N` is less than 1 and raises a `ValueError` if it is, as per the requirement.\n- It then iterates over the sequence using a loop, extracting slices of length `N` starting from each position.\n- Each slice is converted to a tuple and appended to the result list.\n- Finally, the function returns the list of N-grams."}
{"task_id": 17, "completion_id": 0, "completion": "To solve this problem, we need to construct a rooted tree from the given parallel lists of `items`, `counts`, and `parents`. Here's how you can implement the `build_tree` function:\n\n```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    if not items or not counts or not parents:\n        return {}\n\n    def create_node(item, count, parent_index):\n        return {\n            'item': item,\n            'count': count,\n            'children': []\n        }\n\n    root = None\n    nodes = {i: create_node(items[i], counts[i], parents[i]) for i in range(len(items))}\n\n    for child_index, parent_index in enumerate(parents):\n        if parent_index == -1:\n            root = nodes[child_index]\n        else:\n            nodes[parent_index]['children'].append(nodes[child_index])\n\n    return root\n\n# Example usage:\nitems = ['A', 'B', 'C', 'D', 'E']\ncounts = [1, 2, 3, 4, 5]\nparents = [-1, 0, 0, 1, 2]\n\ntree = build_tree(items, counts, parents)\nprint(tree)\n```\n\n### Explanation:\n\n1. **Initialization**: We first check if any of the input lists are empty. If so, we return an empty dictionary.\n\n2. **Node Creation Function**: We define a helper function `create_node` to initialize each node with its item, count, and an empty list for children.\n\n3. **Building Nodes**: We use a dictionary `nodes` to store all nodes indexed by their indices. Each node is initialized using the `create_node` function.\n\n4. **Linking Nodes**: We iterate over the `parents` list to link each node to its parent. For each node, we append it to the `children` list of its parent node.\n\n5. **Identifying the Root**: After populating the nodes, we identify the root node by checking which node has a parent index of `-1`.\n\n6. **Returning the Root**: Finally, we return the root node as a nested dictionary.\n\nThis approach ensures that the tree is constructed correctly while maintaining the order of children as specified in the input lists."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    # Extract the dtype from kwargs if provided, otherwise use float\n    dtype = kwargs.get('dtype', np.float64)\n    \n    # Use np.zeros with the given shape and dtype\n    return np.zeros(shape, dtype=dtype)\n\n# Example usage:\nprint(zero(5))          # Creates a 1D array of length 5 with dtype=float64\nprint(zero((3, 4)))      # Creates a 2D array of shape (3, 4) with dtype=float64\nprint(zero((2,), dtype=int))  # Creates a 1D array of length 2 with dtype=int\n```\n\nThis implementation follows the requirements by using NumPy's `np.zeros` function to generate the array of zeros, allowing for flexibility in specifying the shape and data type through the `shape` and `dtype` parameters."}
{"task_id": 19, "completion_id": 0, "completion": "To solve the problem of finding the best split for a gradient boosting tree, we need to follow these steps:\n\n1. Iterate over each feature and each unique threshold within that feature.\n2. For each threshold, split the data into two subsets: one for instances where the feature value is less than or equal to the threshold (`left_child`) and another for instances where the feature value is greater than the threshold (`right_child`).\n3. Calculate the loss for the original dataset (`loss_original`), the left child (`loss_left`), and the right child (`loss_right`).\n4. Compute the gain for the split using the formula provided.\n5. Keep track of the split with the highest positive gain.\n6. Return the split with the highest positive gain or `None` if no such split exists.\n\nHere's the implementation of the `best_split` function:\n\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    max_gain = -np.inf\n    best_feature = -1\n    best_threshold = -1\n\n    n_samples, n_features = X.shape\n\n    for feature_idx in range(n_features):\n        unique_thresholds = np.unique(X[:, feature_idx])\n\n        for threshold in unique_thresholds:\n            # Create masks for left and right children\n            left_mask = X[:, feature_idx] <= threshold\n            right_mask = X[:, feature_idx] > threshold\n\n            # Check if both children have at least two samples\n            if np.sum(left_mask) < 2 or np.sum(right_mask) < 2:\n                continue\n\n            # Calculate the loss for the original dataset\n            loss_original = -0.5 * (np.sum(g[left_mask]**2) / (np.sum(h[left_mask]) + lam)) + gamma\n            loss_original += -0.5 * (np.sum(g[right_mask]**2) / (np.sum(h[right_mask]) + lam)) + gamma\n\n            # Calculate the loss for the left child\n            loss_left = -0.5 * (np.sum(g[left_mask]**2) / (np.sum(h[left_mask]) + lam)) + gamma\n\n            # Calculate the loss for the right child\n            loss_right = -0.5 * (np.sum(g[right_mask]**2) / (np.sum(h[right_mask]) + lam)) + gamma\n\n            # Calculate the gain\n            gain = loss_original - loss_left - loss_right\n\n            # Update the best split if this gain is larger\n            if gain > max_gain:\n                max_gain = gain\n                best_feature = feature_idx\n                best_threshold = threshold\n\n    # If no positive gain was found, return None\n    if max_gain <= 0:\n        return None\n\n    return best_feature, best_threshold\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# g = np.array([1, 2, 3, 4])\n# h = np.array([1, 2, 3, 4])\n# gamma = 0.1\n# lam = 0.1\n# print(best_split(X, g, h, gamma, lam))\n```\n\nThis function iterates through each feature and each unique threshold, calculates the gain for each split, and keeps track of the split with the highest positive gain. If no such split exists, it returns `None`."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Convert input to a NumPy array\n    x = np.array(x)\n    \n    # Compute the sigmoid values\n    sigmoid_x = 1 / (1 + np.exp(-x))\n    \n    # Compute the gradient of the sigmoid\n    gradient_x = sigmoid_x * (1 - sigmoid_x)\n    \n    # Round the results to 4 decimal places\n    sigmoid_x_rounded = np.round(sigmoid_x, decimals=4)\n    gradient_x_rounded = np.round(gradient_x, decimals=4)\n    \n    # Check if the input was a scalar and return appropriate types\n    if x.size == 1:\n        return (float(sigmoid_x_rounded[0]), float(gradient_x_rounded[0]))\n    else:\n        return (sigmoid_x_rounded.tolist(), gradient_x_rounded.tolist())\n\n# Example usage:\nprint(sigmoid_activation(0))          # Output: (0.5000, 0.2500)\nprint(sigmoid_activation([0, 1]))      # Output: ([0.5000, 0.7311], [0.2500, 0.1966])\nprint(sigmoid_activation([[0, 1], [-1, 0]]))  # Output: ([[0.5000, 0.7311], [0.2689, 0.5000]], [[0.2500, 0.1966], [0.2689, 0.5000]])\n```"}
{"task_id": 21, "completion_id": 0, "completion": "To implement a from-scratch version of the AdaBoost algorithm using one-dimensional decision stumps, we need to follow the steps outlined in the problem description. Here's how you can implement the `adaboost_1d_predict` function:\n\n```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Step 1: Initialize sample weights uniformly\n    n_samples = len(y_train)\n    w = np.ones(n_samples) / n_samples\n    \n    # Step 2: Initialize the ensemble\n    H = []\n    \n    while True:\n        # Step 3: Find the best decision stump\n        min_error = float('inf')\n        best_stump = None\n        \n        for d in range(2):\n            thresholds = np.unique(np.sort(x_train))\n            for theta in thresholds[:-1]:\n                pred = np.where(x_train <= theta, 1, -1)\n                \n                # Compute the weighted error\n                err = np.sum(w[y_train != pred])\n                \n                if err < min_error:\n                    min_error = err\n                    best_stump = (d, theta)\n        \n        # Step 4: Calculate the weight alpha\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n        \n        # Step 5: Update the sample weights\n        w *= np.exp(-alpha * y_train * pred)\n        w /= np.sum(w)\n        \n        # Step 6: Add the best stump to the ensemble\n        H.append((best_stump, alpha))\n        \n        # Check if the training error is within the allowed limit\n        train_error = sum(alpha * (y_train == pred) for _, pred in H)\n        if train_error <= epsilon:\n            break\n    \n    # Step 7: Make predictions on the test set\n    def predict(x):\n        return np.sign(sum(alpha * h[1] * h[0][1] if x > h[0][1] else alpha * h[1] * (-h[0][1]) for h in H))\n    \n    return [predict(x) for x in x_test]\n\n# Example usage:\n# x_train = [1, 2, 3, 4, 5]\n# y_train = [1, -1, 1, -1, 1]\n# x_test = [1.5, 3.5, 5.5]\n# print(adaboost_1d_predict(x_train, y_train, x_test))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the sample weights `w` to be uniform.\n2. **Training Loop**: We iterate until the training error of the ensemble is less than or equal to `epsilon`.\n   - For each iteration, we find the best decision stump by evaluating all possible splits (`d=0` and `d=1`) and selecting the one with the minimum weighted error.\n   - We calculate the weight `\u03b1` using the formula `\u03b1 = 0.5 * ln((1 - err) / err)`.\n   - We update the sample weights based on the predicted errors.\n3. **Ensemble Building**: We add the best stump to the ensemble along with its weight `\u03b1`.\n4. **Prediction**: After training, we define a helper function `predict` that computes the sign of the weighted sum of the hypotheses for a new input `x`.\n5. **Test Set Prediction**: Finally, we use this helper function to make predictions on the test set.\n\nThis implementation should correctly train the AdaBoost ensemble using one-dimensional decision stumps and provide predictions for the test set."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    # Generate the initial tensor with integers in the range [-300, 299]\n    # and floats in the range [0, 1)\n    initial_data = np.random.uniform(-300, 299, size=shape) + np.random.rand(*shape)\n\n    if not standardize:\n        return initial_data\n\n    # Standardize each column if required\n    standardized_data = np.empty_like(initial_data)\n    for col_idx in range(initial_data.shape[1]):\n        col_mean = initial_data[:, col_idx].mean()\n        col_std = initial_data[:, col_idx].std() + np.finfo(float).eps  # Add epsilon to avoid division by zero\n        standardized_data[:, col_idx] = (initial_data[:, col_idx] - col_mean) / col_std\n\n    return standardized_data\n```\n\nThis function first generates a tensor with elements sampled from a uniform distribution over the specified range. If `standardize` is set to `True`, it then standardizes each column by subtracting its mean and dividing by its standard deviation, adding a small epsilon to the denominator to prevent division by zero. The resulting tensor is returned."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    w = np.array(w)\n    velocity = np.array(velocity)\n    \n    # Calculate the look-ahead point\n    tilde_w = w - momentum * velocity\n    \n    # Evaluate the gradient at the look-ahead point\n    gradient = grad_func(tilde_w)\n    \n    # Apply element-wise clipping to the gradient\n    clipped_gradient = np.clip(gradient, -1, 1)\n    \n    # Update the velocity\n    velocity_prime = momentum * velocity + learning_rate * clipped_gradient\n    \n    # Update the parameters\n    w_prime = w - velocity_prime\n    \n    # Round the results to 4 decimal places\n    new_weights = w_prime.round(4).tolist()\n    new_velocity = velocity_prime.round(4).tolist()\n    \n    return new_weights, new_velocity\n\n# Example usage:\n# Define a simple loss function for testing\ndef simple_loss(x):\n    return np.sum(np.square(x))\n\n# Initialize parameters and velocity\ninitial_params = [1.0, 2.0, 3.0]\ninitial_velocity = []\n\n# Perform a single update step\nupdated_params, updated_velocity = nesterov_update(initial_params, initial_velocity, simple_loss)\n\nprint(\"Updated Parameters:\", updated_params)\nprint(\"Updated Velocity:\", updated_velocity)\n```\n\nThis code defines a function `nesterov_update` that implements the single-step Nesterov Accelerated Gradient (NAG) optimization algorithm. It includes steps for computing the look-ahead point, evaluating the gradient, updating the velocity, and updating the parameters. The gradients are clipped to ensure they lie within the range [-1, 1] before being used in the velocity update. The final updated parameters and velocity are returned as lists rounded to four decimal places."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    n, d = X_train.shape\n    m = X_test.shape[0]\n\n    # Compute the kernel matrix K\n    K = np.exp(-gamma * np.sum((X_train[:, None] - X_train)**2, axis=-1))\n\n    # Compute the decision function for each test sample\n    scores = np.dot(K, alpha) * y_train + b\n\n    # Predict the label based on the sign of the score\n    predicted_labels = np.sign(scores).astype(int)\n\n    return predicted_labels.tolist()\n```\n\nThis code defines the `gaussian_svm_predict` function, which implements the prediction logic for a Gaussian-kernel Support Vector Machine (SVM) using the provided parameters and test data. It calculates the kernel matrix, computes the decision function for each test sample, and returns the predicted labels. The solution is fully vectorized and does not use any external libraries beyond NumPy and basic math operations."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    # Convert the input to a NumPy array\n    x_array = np.array(x)\n    \n    # Apply the ReLU activation\n    activation = np.maximum(0, x_array)\n    \n    # Compute the gradient mask using np.where\n    gradient_mask = np.where(x_array >= 0, 1, 0)\n    \n    # Convert the results back to Python lists\n    activation_list = activation.tolist()\n    gradient_list = gradient_mask.tolist()\n    \n    return activation_list, gradient_list\n\n# Example usage:\ninput_data = [[-1, 2, -4], [5, -6, 7]]\nactivation, gradient = relu(input_data)\nprint(\"Activation:\", activation)\nprint(\"Gradient Mask:\", gradient)\n```\n\nThis function first converts the input into a NumPy array for efficient computation. It then applies the ReLU activation function using `np.maximum`, which returns an array where each element is the maximum of the corresponding element in `x` and 0. The gradient mask is computed using `np.where`, which creates an array where each element is 1 if the corresponding element in `x` is greater than or equal to 0, and 0 otherwise. Finally, the results are converted back to Python lists and returned as a tuple."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to perform a truncated Singular Value Decomposition (SVD) on the input matrix \\( X \\). The optimal rank-\\( k \\) reconstruction involves retaining only the top \\( k \\) singular values and their corresponding left and right singular vectors. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    m, n = len(X), len(X[0])\n    if not (1 <= k <= min(m, n)):\n        return (-1,)\n\n    # Convert the input list of lists to a NumPy array for efficient computation\n    X_np = np.array(X)\n\n    # Perform SVD\n    U, s, Vt = np.linalg.svd(X_np, full_matrices=False)\n\n    # Truncate the SVD components to the top k dimensions\n    U_k = U[:, :k]\n    s_k = s[:k]\n    Vt_k = Vt[:k, :]\n\n    # Compute the rank-k approximation\n    X_hat = np.dot(U_k, np.dot(np.diag(s_k), Vt_k))\n\n    # Round the reconstructed matrix to four decimal places\n    X_hat_rounded = round_matrix(X_hat, 4)\n\n    # Calculate the mean squared error\n    mse = np.mean((X_np - X_hat_rounded)**2)\n\n    # Return the results as a tuple\n    return (X_hat_rounded.tolist(), mse)\n\ndef round_matrix(matrix, decimals):\n    \"\"\"Round each element of a matrix to the specified number of decimal places.\"\"\"\n    return [[round(x, decimals) for x in row] for row in matrix]\n\n# Example usage:\nX = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nk = 2\nresult = linear_autoencoder(X, k)\nprint(result)\n```\n\n### Explanation:\n1. **Input Validation**: We first check if \\( k \\) is within the valid range (i.e., between 1 and the minimum of \\( m \\) and \\( n \\)). If not, we return \\(-1\\).\n\n2. **Convert Input**: We convert the input list of lists into a NumPy array for efficient computation.\n\n3. **Perform SVD**: We use `np.linalg.svd` to compute the SVD of the matrix. The `full_matrices=False` parameter ensures that we get only the necessary components for the truncation.\n\n4. **Truncate SVD Components**: We select the top \\( k \\) singular values and their corresponding left and right singular vectors.\n\n5. **Compute Rank-k Approximation**: We reconstruct the matrix using the truncated SVD components.\n\n6. **Rounding**: We round each element of the reconstructed matrix to four decimal places.\n\n7. **Mean Squared Error Calculation**: We calculate the mean squared error between the original matrix and the reconstructed matrix.\n\n8. **Return Result**: Finally, we return the reconstructed matrix and the MSE as a tuple."}
{"task_id": 29, "completion_id": 0, "completion": "To implement the one-dimensional gradient boosting regressor using decision stumps, we need to follow the steps outlined in the problem description. Here's how you can implement the `predict_boosting_tree` function:\n\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    # Sort the training data by feature values\n    sorted_indices = np.argsort(x_train)\n    x_train_sorted = x_train[sorted_indices]\n    y_train_sorted = y_train[sorted_indices]\n    \n    # Initialize the residuals and the sum of weights\n    residuals = y_train_sorted - np.mean(y_train_sorted)\n    weight_sum = np.sum(np.abs(residuals))\n    \n    # Initialize the ensemble prediction\n    ensemble_prediction = 0\n    \n    while weight_sum > epsilon:\n        min_rss = np.inf\n        best_split = None\n        \n        # Iterate over all possible split points\n        for i in range(1, len(x_train_sorted)):\n            split_point = (x_train_sorted[i - 1] + x_train_sorted[i]) / 2\n            \n            # Split the residuals into two groups\n            left_residuals = residuals[:i]\n            right_residuals = residuals[i:]\n            \n            # Compute the means of the residuals for both groups\n            c1 = np.mean(left_residuals)\n            c2 = np.mean(right_residuals)\n            \n            # Compute the RSS for the current split\n            rss = np.sum((left_residuals - c1) ** 2) + np.sum((right_residuals - c2) ** 2)\n            \n            # Check if this split has the minimum RSS so far\n            if rss < min_rss:\n                min_rss = rss\n                best_split = split_point\n        \n        # Update the ensemble prediction\n        ensemble_prediction += min_rss / weight_sum\n        \n        # Update the residuals and the sum of weights\n        residuals = np.where(x_train_sorted <= best_split, residuals - min_rss / weight_sum, residuals + min_rss / weight_sum)\n        weight_sum -= min_rss\n    \n    # Predict the target for the query point\n    query_prediction = ensemble_prediction\n    \n    return round(query_prediction, 4)\n\n# Example usage:\n# x_train = [1, 2, 3, 4, 5]\n# y_train = [2, 3, 5, 7, 11]\n# x_query = 3.5\n# print(predict_boosting_tree(x_train, y_train, x_query))  # Output will depend on the implementation\n```\n\n### Explanation:\n1. **Sorting**: We first sort the training data by the feature values (`x_train`). This helps in easily finding mid-points between consecutive values for splitting.\n\n2. **Initialization**: We initialize the residuals as the difference between the target values and their mean. The sum of weights starts at the sum of absolute residuals.\n\n3. **Iteration**: We iterate over all possible split points, calculate the RSS for each split, and keep track of the split with the smallest RSS.\n\n4. **Update Ensemble Prediction**: Once we find the best split, we update the ensemble prediction by adding the contribution of the best split to it. We also update the residuals and the sum of weights accordingly.\n\n5. **Stopping Condition**: The loop continues until the total residual sum of squares is less than the specified tolerance (`epsilon`).\n\n6. **Prediction**: Finally, we use the ensemble prediction to make a prediction for the query point `x_query`.\n\nThis implementation provides a basic framework for one-dimensional gradient boosting using decision stumps. Adjustments may be needed based on specific requirements or performance considerations."}
{"task_id": 32, "completion_id": 0, "completion": "Here's the implementation of the `check_data` function and the `sum_numbers` function using the provided decorator:\n\n```python\ndef check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    The function must accept integers, floats, or numeric strings.  Any string\n    should be converted with `float()`; existing numeric values stay as they\n    are.  The pair `(a, b)` must then be returned.\n    \"\"\"\n    try:\n        a = float(a)\n    except ValueError:\n        pass\n    try:\n        b = float(b)\n    except ValueError:\n        pass\n    return (a, b)\n\n# Do NOT change this decorator \u2013 it is required by the grader.\ndef validate_input(function):\n    def wrapper(a, b):\n        a, b = check_data(a, b)\n        return function(a, b)\n\n    return wrapper\n\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\"\"\"\n    return a + b\n```\n\n### Explanation:\n\n1. **`check_data(a, b)` Function:**\n   - This function attempts to convert both `a` and `b` to floats.\n   - If either `a` or `b` is not a valid numeric string, a `ValueError` is caught, and the variable remains unchanged.\n   - The function returns a tuple containing the validated numeric values.\n\n2. **`validate_input(function)` Decorator:**\n   - This decorator wraps the target function (`function`) and ensures that its arguments are validated before calling the function.\n   - It uses the `check_data` function to ensure that both arguments are numeric.\n\n3. **`sum_numbers(a, b)` Function:**\n   - This function simply adds two numbers after validating them using the `validate_input` decorator.\n   - It returns the sum of `a` and `b`.\n\nThis setup ensures that any invalid input results in a `ValueError`, which is handled gracefully by the decorator. The function returns a `float` if the sum is not an integer, adhering to the output constraints."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray:\n    \"\"\"\n    Generates a NumPy tensor filled with 0.0s and 1.0s based on the specified shape and sparsity.\n\n    Parameters:\n    - shape: A tuple representing the dimensions of the tensor.\n    - sparsity: A float between 0.0 and 1.0 indicating the probability of each element being 1.0.\n    - seed: An integer used to seed the random number generator for reproducibility.\n\n    Returns:\n    - A NumPy ndarray with the specified shape, where each element is either 0.0 or 1.0.\n      Returns -1 if the sparsity value is out of the valid range [0, 1].\n    \"\"\"\n\n    # Check if sparsity is within the valid range\n    if not (0 <= sparsity <= 1):\n        return -1\n\n    # Set the seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate a random tensor with values between 0 and 1\n    random_tensor = np.random.rand(*shape)\n\n    # Apply the sparsity level to convert some values to 1.0\n    binary_tensor = (random_tensor >= (1 - sparsity)).astype(float)\n\n    return binary_tensor\n```\n\nThis implementation meets all the specified requirements:\n1. It checks if the `sparsity` is within the valid range `[0, 1]`.\n2. It sets the seed if provided using `np.random.seed(seed)`.\n3. It generates a random tensor with values between 0 and 1 using `np.random.rand`.\n4. It applies the sparsity level to convert some values to 1.0 using boolean indexing.\n5. It returns the resulting binary tensor.\n6. If the `sparsity` is out of range, it returns `-1`."}
{"task_id": 35, "completion_id": 0, "completion": "To implement the `classify` function, we need to traverse the decision tree according to the specified traversal rules. Here's how you can do it:\n\n```python\nfrom typing import Any\nfrom typing import Any\n\n\nclass node:  # noqa: N801 \u2013 keep the original name used by the problem description\n    \"\"\"A minimal tree node for decision-tree-like structures.\n\n    Attributes:\n        fea: int \u2013 index of the feature to split on. A value of -1 marks a leaf.\n        res: Any \u2013 label stored in the leaf. None for internal nodes.\n        child: dict \u2013 mapping from feature value to the corresponding sub-tree.\n    \"\"\"\n\n    def __init__(self, fea: int = -1, res: Any | None = None, child: dict | None = None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\n\n\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\n\n    Args:\n        root: The root node of the decision tree.\n        sample: A list of feature values representing one data point.\n\n    Returns:\n        The label stored in the reached leaf, or -1 if the sample cannot be\n        classified.\n    \"\"\"\n    current_node = root\n    \n    while current_node.fea != -1:\n        # Check if the feature index is within the sample range\n        if current_node.fea >= len(sample):\n            return -1\n        \n        # Get the feature value from the sample\n        feature_value = sample[current_node.fea]\n        \n        # Check if the feature value exists in the child dictionary\n        if feature_value not in current_node.child:\n            return -1\n        \n        # Move to the next node\n        current_node = current_node.child[feature_value]\n    \n    # Return the result of the leaf node\n    return current_node.res\n```\n\n### Explanation:\n- We start at the root node.\n- We use a `while` loop to traverse the tree as long as the current node is not a leaf (`current_node.fea != -1`).\n- Inside the loop, we check if the current feature index is within the bounds of the sample. If not, we return `-1`.\n- We then retrieve the feature value from the sample using the current feature index.\n- We check if this feature value exists in the `child` dictionary of the current node. If not, we return `-1`.\n- If both checks pass, we move to the corresponding sub-tree by accessing the dictionary value associated with the feature value.\n- Once we reach a leaf node (`current_node.fea == -1`), we return the label stored in the `res` attribute of the leaf node."}
{"task_id": 36, "completion_id": 0, "completion": "To implement the `predict_tree` function according to the specified task, we need to traverse the decision tree for each sample in the provided dataset `X`. The function will follow the decision rules described in the problem statement:\n\n1. Start at the root node.\n2. If the current node is a leaf, return its stored prediction.\n3. Otherwise, compare the feature value of the current sample with the threshold of the current node.\n4. Move to the left child if the condition is met, otherwise move to the right child.\n5. Repeat until a leaf node is reached.\n\nHere's how you can implement this:\n\n```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node:   rule = (feature_index, threshold)\n    Leaf node:       rule = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\n\n\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"Predicts the output values for the given samples using the decision tree.\n\n    :param root: The root node of the decision tree.\n    :param X: A list of samples, where each sample is represented as a list of feature values.\n    :return: A list of predictions corresponding to the input samples.\n    \"\"\"\n    if root is None:\n        return []\n\n    predictions = []\n    for sample in X:\n        current_node = root\n        while current_node.left is not None and current_node.right is not None:\n            if sample[current_node.feature] <= current_node.threshold:\n                current_node = current_node.left\n            else:\n                current_node = current_node.right\n        \n        # When a leaf node is reached, append its prediction to the results\n        predictions.append(current_node.threshold)\n\n    return predictions\n```\n\n### Explanation:\n\n- **Base Case**: If the `root` is `None`, it means there is no tree to process, so we return an empty list.\n- **Tree Traversal**: For each sample in `X`, we initialize `current_node` to the root. We then iterate through the tree nodes based on the comparison of the sample's feature value with the current node's threshold.\n- **Prediction**: Once a leaf node is reached (i.e., when `current_node.left` and `current_node.right` are both `None`), we append the leaf node's prediction (stored in `current_node.threshold`) to the `predictions` list.\n- **Return**: Finally, we return the list of predictions.\n\nThis implementation ensures that the function processes each sample independently and returns the correct predictions in the required order."}
{"task_id": 37, "completion_id": 0, "completion": "To construct a kd-tree from a NumPy array, we need to follow the specified rules and use recursion to partition the data based on the current splitting dimension. Here's how you can implement the `build_kd_tree` function:\n\n```python\nimport numpy as np\nfrom itertools import cycle\n\ndef build_kd_tree(data: np.ndarray) -> dict | None:\n    \"\"\"Build a kd-tree from the given NumPy array.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features + 1).  The first\n        n_features columns hold the coordinates of each point, the last column\n        holds an associated label or value.\n\n    Returns\n    -------\n    dict | None\n        Nested dictionary representing the kd-tree or None when *data* is\n        empty.  Each dictionary has the keys 'point', 'label', 'left',\n        and 'right' as explained in the task description.\n    \"\"\"\n    if data.shape[0] == 0:\n        return None\n\n    # Determine the number of features\n    n_features = data.shape[1] - 1\n\n    # Create a generator for the splitting dimensions\n    dims = cycle(range(n_features))\n\n    # Select the current dimension\n    dim = next(dims)\n\n    # Sort the data by the current dimension\n    sorted_data = data[data[:, dim].argsort()]\n\n    # Find the median index\n    mid_index = sorted_data.shape[0] // 2\n\n    # Extract the median point and label\n    median_point = sorted_data[mid_index, :-1]\n    median_label = sorted_data[mid_index, -1]\n\n    # Recursively build the left and right subtrees\n    left_subset = sorted_data[:mid_index]\n    right_subset = sorted_data[mid_index + 1:]\n\n    # Construct the current node\n    node = {\n        \"point\": median_point.tolist(),\n        \"label\": median_label,\n        \"left\": build_kd_tree(left_subset) if left_subset.size > 0 else None,\n        \"right\": build_kd_tree(right_subset) if right_subset.size > 0 else None\n    }\n\n    return node\n\n# Example usage:\n# data = np.array([[1, 2, 5], [3, 4, 6], [5, 6, 7]])\n# kd_tree = build_kd_tree(data)\n# print(kd_tree)\n```\n\n### Explanation:\n1. **Empty Input Check**: If the input array `data` has zero rows, the function returns `None`.\n\n2. **Feature Count**: We determine the number of features (`n_features`) by subtracting one from the total number of columns.\n\n3. **Dimension Generator**: We create a generator `dims` using `cycle` to iterate over the dimensions `[0, 1, ..., n_features-1]` in a cyclic manner.\n\n4. **Select Current Dimension**: We select the current dimension from the generator.\n\n5. **Sort Data**: We sort the data by the current dimension using `argsort()`.\n\n6. **Find Median Index**: We find the index of the median element, which will be the root of the current subtree.\n\n7. **Extract Median Point and Label**: We extract the median point and its corresponding label.\n\n8. **Recursive Subtree Building**: We recursively build the left and right subtrees using the left and right subsets of the data.\n\n9. **Construct Node**: We construct the current node with the median point, label, and pointers to the left and right subtrees.\n\nThis implementation ensures that the kd-tree is built according to the specified rules and constraints."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\n\n    Parameters\n    ----------\n    context : np.ndarray\n        Matrix of shape (D, n_arms) containing the D-dimensional feature\n        vectors of every arm for the current round.\n    A : list\n        List where ``A[a]`` is the (D\u00d7D) Gram matrix of arm *a*.\n    b : list\n        List where ``b[a]`` is the length-D accumulated reward-context vector\n        of arm *a*.\n    alpha : float\n        Exploration coefficient (> 0).\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest LinUCB score.\n    \"\"\"\n    D = context.shape[0]\n    n_arms = len(A)\n    \n    # Initialize arrays to store LinUCB scores\n    scores = np.zeros(n_arms)\n    \n    # Calculate LinUCB scores for each arm\n    for a in range(n_arms):\n        # Compute the inverse of the Gram matrix\n        A_inv = np.linalg.inv(A[a])\n        \n        # Compute the expected reward for arm a\n        theta_a = np.dot(A_inv, b[a])\n        \n        # Compute the context vector for arm a\n        c_a = context[:, a]\n        \n        # Compute the LinUCB score\n        scores[a] = np.dot(theta_a, c_a) + alpha * np.sqrt(np.dot(c_a.T, np.dot(A_inv, c_a)))\n    \n    # Return the index of the arm with the highest LinUCB score\n    return np.argmin(scores)\n\n# Example usage:\n# context = np.random.rand(5, 3)  # Example context matrix\n# A = [np.random.rand(5, 5) for _ in range(3)]  # Example Gram matrices\n# b = [np.random.rand(5) for _ in range(3)]  # Example covariance vectors\n# alpha = 1.0\n# print(linucb_select_arm(context, A, b, alpha))\n```\n\nThis implementation calculates the LinUCB score for each arm using the provided context matrix, Gram matrices, covariance vectors, and exploration coefficient. It then returns the index of the arm with the highest score. If there is a tie, it returns the smallest index due to the behavior of `np.argmin`."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the `get_split` function, we need to follow these steps:\n\n1. Find the median value of the specified column using `np.median`.\n2. Use `np.argpartition` to partition the array such that the median is at the correct position, which helps us efficiently identify the pivot.\n3. Determine the rows that fall to the left and right of the pivot based on their values in the specified column.\n4. Ensure that both the left and right lists are sorted.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    # Step 1: Find the median value of the specified column\n    median_value = np.median(data[:, d])\n    \n    # Step 2: Use np.argpartition to partition the array\n    # argpartition returns the indices that would partition the array into two parts\n    # such that the element at the k-th position is in the correct place,\n    # and all elements before it are less than or equal to the element,\n    # and all elements after it are greater than or equal to the element.\n    # We want the median to be at the correct position, so we set k to -1.\n    partition_indices = np.argpartition(data[:, d], -1)\n    \n    # Step 3: Determine the pivot index\n    pivot_index = partition_indices[-1]\n    \n    # Step 4: Get the rows that fall to the left and right of the pivot\n    left_indices = partition_indices[:pivot_index]\n    right_indices = partition_indices[pivot_index + 1:]\n    \n    # Step 5: Ensure the lists are sorted\n    left_indices.sort()\n    right_indices.sort()\n    \n    # Return the result as a tuple\n    return pivot_index, left_indices, right_indices\n\n# Example usage:\ndata = np.array([[1, 2], [3, 1], [4, 3], [5, 2]])\npivot, left, right = get_split(data, 0)\nprint(\"Pivot:\", pivot)\nprint(\"Left:\", left)\nprint(\"Right:\", right)\n```\n\nThis function will correctly perform a median split on the given dataset along the specified column and return the required indices. The use of `np.argpartition` ensures that the operation runs in O(n) time complexity."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to use NumPy's `as_strided` function to create a view on the original array `x`. This function allows us to create a new array with a different shape and strides than the original array without actually copying the data. Here's how you can implement the `to_frames` function:\n\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check for pre-conditions\n    assert len(x.shape) == 1, \"Input signal must be one-dimensional.\"\n    assert stride > 0, \"Stride must be greater than zero.\"\n    assert frame_width <= len(x), \"Frame width must be less than or equal to the length of the signal.\"\n\n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n\n    # Create the view using as_strided\n    framed_signal = as_strided(\n        x,\n        shape=(n_frames, frame_width),\n        strides=(stride * x.itemsize, x.itemsize)\n    )\n\n    # Set the writeable flag\n    framed_signal.flags.writeable = writeable\n\n    return framed_signal\n```\n\n### Explanation:\n\n1. **Pre-condition Checks**: \n   - We first check if `x` is one-dimensional using `len(x.shape) == 1`.\n   - We ensure that `stride` is greater than zero.\n   - We verify that `frame_width` is less than or equal to the length of `x`.\n\n2. **Calculate Number of Frames**:\n   - We compute the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n\n3. **Create the View Using `as_strided`**:\n   - We use `as_strided` to create a new array with the desired shape and strides.\n     - `shape`: `(n_frames, frame_width)`\n     - `strides`: `(stride * x.itemsize, x.itemsize)`\n       - `stride * x.itemsize` specifies the step size along the axis corresponding to the frame width.\n       - `x.itemsize` specifies the size of each element in bytes.\n\n4. **Set Writeable Flag**:\n   - We set the `writeable` flag of the resulting array based on the input parameter `writeable`.\n\nThis approach ensures that the resulting array is a view on the original data, and its shape matches the expected output format."}
{"task_id": 41, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier in Python, we need to follow the steps outlined in the task description. Here's how you can achieve this:\n\n1. Calculate the Euclidean distance between each query sample and every training sample.\n2. Select the `k` nearest neighbors based on these distances.\n3. Count the occurrences of each class among the selected neighbors.\n4. Determine the class with the highest frequency. In case of a tie, select the lexicographically smallest class.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\ndef knn_predict(X_data: list[list[float]], y_data: list, X: list[list[float]], k: int = 5) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    if not X_data or not y_data or not X:\n        raise ValueError(\"Input lists cannot be empty.\")\n    \n    # Ensure X_data is a NumPy array for efficient computation\n    X_data = np.array(X_data)\n    y_data = np.array(y_data)\n    X = np.array(X)\n    \n    predictions = []\n    \n    for query_sample in X:\n        # Calculate distances from the query sample to all training samples\n        distances = [euclidean_distance(query_sample, train_sample) for train_sample in X_data]\n        \n        # Get indices of the k smallest distances\n        k_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = y_data[k_indices]\n        \n        # Count the occurrences of each label\n        label_counts = Counter(k_nearest_labels)\n        \n        # Find the most common label, breaking ties with the lexicographically smallest label\n        most_common_label = label_counts.most_common(1)[0][0]\n        if len(label_counts.most_common()) > 1 and label_counts.most_common()[1][1] == label_counts.most_common(1)[0][1]:\n            most_common_label = min(label_counts.keys())\n        \n        predictions.append(most_common_label)\n    \n    return predictions\n\n# Example usage:\n# X_data = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]\n# y_data = ['A', 'B', 'A', 'C', 'B']\n# X = [[3, 3], [4, 4]]\n# print(knn_predict(X_data, y_data, X))  # Output: ['A', 'C']\n```\n\n### Explanation:\n- **Euclidean Distance Calculation**: We define a helper function `euclidean_distance` to compute the Euclidean distance between two vectors.\n- **Distance Calculation and Selection**: For each query sample, we calculate the distance to all training samples, sort them, and select the `k` nearest neighbors.\n- **Label Voting**: We count the occurrences of each label among the selected neighbors and determine the most frequent one. In case of a tie, we choose the lexicographically smallest label.\n- **Prediction**: The predicted label for each query sample is collected and returned as a list.\n\nThis implementation handles various edge cases, such as when the input lists are empty, and ensures that the prediction is deterministic in case of ties."}
{"task_id": 42, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    # Validate input\n    if len(actual) != len(predicted):\n        return -1\n    if not all(x >= 0 for x in actual + predicted):\n        return -1\n    \n    # Calculate RMSLE\n    log_diffs_squared = [(np.log(pred + 1) - np.log(act + 1)) ** 2 for pred, act in zip(predicted, actual)]\n    mean_log_diff_squared = np.mean(log_diffs_squared)\n    rmsle_value = np.sqrt(mean_log_diff_squared)\n    \n    # Round to 4 decimal places\n    return round(rmsle_value, 4)\n```\n\nThis function first checks if the lengths of the `actual` and `predicted` lists are equal and if all elements are non-negative. If any condition is violated, it returns `-1`. Otherwise, it calculates the RMSLE using the provided formula and rounds the result to four decimal places before returning it."}
{"task_id": 44, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_activation(x, derivative: bool = False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh. Otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    x = np.array(x)\n    \n    # Compute tanh using the formula\n    if derivative:\n        # Gradient of tanh is 1 - tanh^2\n        result = 1 - np.tanh(x)**2\n    else:\n        # Direct computation of tanh\n        exp_neg_2x = np.exp(-2 * x)\n        result = (2 / (1 + exp_neg_2x)) - 1\n    \n    # Round the results to 4 decimal places and convert to a list\n    rounded_result = [round(val, 4) for val in result]\n    \n    return rounded_result\n```\n\nThis function first converts the input to a NumPy array for efficient computation. It then calculates either the tanh values or their derivatives based on the `derivative` flag. Finally, it rounds each result to four decimal places and returns the list of these rounded values."}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    if len(x) == 0:\n        return []\n\n    N = len(x)\n    a = [0] * N\n\n    for k in range(N):\n        for n in range(N - k):\n            a[k] += x[n] * x[n + k]\n\n    return a\n\n# Example usage:\nx = np.array([1, 2, 3])\nprint(autocorrelate_1d(x))  # Output: [14, 8, 3]\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Check if X is a numpy array\n    if isinstance(X, np.ndarray):\n        # Check if all elements are within the range [0, 1]\n        if not np.all((X >= 0) & (X <= 1)):\n            return False\n        \n        # Check if each row sums to 1\n        if not np.allclose(np.sum(X, axis=1), 1, atol=1e-8):\n            return False\n    \n    # If X is a list of lists, convert it to a numpy array for processing\n    else:\n        try:\n            X = np.array(X)\n        except ValueError:\n            raise ValueError(\"Input must be a list of lists or a NumPy array.\")\n        \n        # Perform the checks as before\n        if not np.all((X >= 0) & (X <= 1)):\n            return False\n        \n        if not np.allclose(np.sum(X, axis=1), 1, atol=1e-8):\n            return False\n    \n    # If all checks pass, the matrix is row-stochastic\n    return True\n```\n\nThis function first checks if the input `X` is a `numpy.ndarray`. It then checks if all elements of the array are within the range [0, 1] and if each row sums to 1 using `np.allclose` with an absolute tolerance of `1e-8`. If `X` is a list of lists, it converts it to a `numpy.ndarray` before performing these checks. If any of the conditions fail, the function returns `False`; otherwise, it returns `True`, indicating that `X` is a row-stochastic matrix."}
{"task_id": 49, "completion_id": 0, "completion": "To solve this problem, we need to implement a manual implementation of a max-heap using a Python list and two helper functions: `_move_up` to maintain the heap property after inserting a new element, and `_move_down` to maintain the heap property after removing the root (largest element).\n\nHere's how you can implement the `k_smallest` function:\n\n```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    The function should scan the input list once, maintaining a custom\n    max-heap (size at most k) implemented with a Python list.  At the end,\n    return the k smallest elements in ascending order.  If *k* is invalid\n    (\u22640 or larger than len(nums)) return -1.\n    \n    The use of the built-in ``heapq`` module is **not** allowed.\n    \"\"\"\n    if k <= 0 or k > len(nums):\n        return -1\n\n    def _move_up(index):\n        parent_index = (index - 1) // 2\n        while index > 0 and nums[index] > nums[parent_index]:\n            nums[index], nums[parent_index] = nums[parent_index], nums[index]\n            index = parent_index\n            parent_index = (index - 1) // 2\n\n    def _move_down(index):\n        left_child_index = 2 * index + 1\n        right_child_index = 2 * index + 2\n        largest_index = index\n\n        if left_child_index < k and nums[left_child_index] > nums[largest_index]:\n            largest_index = left_child_index\n\n        if right_child_index < k and nums[right_child_index] > nums[largest_index]:\n            largest_index = right_child_index\n\n        if largest_index != index:\n            nums[index], nums[largest_index] = nums[largest_index], nums[index]\n            _move_down(largest_index)\n\n    # Initialize the max-heap with the first k elements\n    heap = nums[:k]\n    for i in range(k // 2 - 1, -1, -1):\n        _move_down(i)\n\n    # Insert remaining elements and maintain the heap size\n    for num in nums[k:]:\n        if num < heap[0]:\n            heap[0] = num\n            _move_down(0)\n\n    # Sort the heap to get the k smallest elements\n    heap.sort()\n\n    return heap\n\n# Example usage:\nprint(k_smallest([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5], 4))  # Output: [1, 1, 2, 3]\n```\n\n### Explanation:\n1. **Initialization**: We start by checking if `k` is valid. If not, we return `-1`. We then initialize a max-heap with the first `k` elements of the list.\n\n2. **Heap Construction**: We use the `_move_up` function to ensure the heap property is maintained as we add each of these initial `k` elements.\n\n3. **Inserting Remaining Elements**: For each subsequent element in the list, if it is smaller than the root of the heap, we replace the root with this new element and call `_move_down` to restore the heap property.\n\n4. **Sorting**: After processing all elements, the heap contains the `k` smallest elements. We sort this heap to get them in non-decreasing order.\n\nThis approach ensures that we efficiently find the `k` smallest elements without sorting the entire list, adhering to the constraints provided."}
{"task_id": 51, "completion_id": 0, "completion": "To implement Newton's method for solving a two-variable linear least-squares problem using Python, we need to follow these steps:\n\n1. Compute the gradient of the Mean Squared Error (MSE).\n2. Compute the Hessian matrix of the MSE.\n3. Use the Moore-Penrose pseudo-inverse to handle cases where the Hessian is singular.\n4. Update the weights using the formula \\( w \\leftarrow w - H^{-1} \\nabla \\).\n5. Check convergence based on the gradient norm or the maximum number of iterations.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n    \"\"\"Your task is to complete this function!\"\"\"\n    \n    n = X.shape[0]\n    w = np.array([1, 1]).reshape(-1, 1)  # Initial guess\n    \n    for _ in range(max_iter):\n        # Compute the gradient of the MSE\n        gradient = (2 / n) * X.T @ (X @ w - y)\n        \n        # Compute the Hessian matrix of the MSE\n        hessian = (2 / n) * X.T @ X\n        \n        # Use the Moore-Penrose pseudo-inverse if the Hessian is singular\n        try:\n            hessian_inv = np.linalg.inv(hessian)\n        except np.linalg.LinAlgError:\n            hessian_inv = np.linalg.pinv(hessian)\n        \n        # Update the weights\n        w -= hessian_inv @ gradient\n        \n        # Check for convergence\n        gradient_norm = np.linalg.norm(gradient)\n        if gradient_norm < epsilon:\n            break\n    \n    # Round the final weights to 4 decimal places\n    return [list(round(weight, 4)) for weight in w]\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([[3], [5], [7], [9]])\n# print(newton(X, y))\n```\n\n### Explanation:\n\n- **Gradient Calculation**: The gradient of the MSE is computed as \\( \\nabla = \\frac{2}{n} X^T (Xw - y) \\).\n- **Hessian Calculation**: The Hessian of the MSE is computed as \\( H = \\frac{2}{n} X^T X \\).\n- **Pseudo-Inverse**: If the Hessian is singular, we use `np.linalg.pinv` to compute the Moore-Penrose pseudo-inverse.\n- **Weight Update**: The weights are updated using \\( w \\leftarrow w - H^{-1} \\nabla \\).\n- **Convergence Check**: We check if the norm of the gradient is less than the specified `epsilon`.\n- **Output**: The final weights are returned as a list of lists, each element rounded to 4 decimal places.\n\nThis implementation will correctly handle rank-deficient data matrices by using the pseudo-inverse of the Hessian."}
{"task_id": 54, "completion_id": 0, "completion": "Here's the implementation of the `tokenize_whitespace` function according to the specified requirements:\n\n```python\nimport string\n\nstop_words = set([\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\",\n    \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\",\n    \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n    \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n    \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n    \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n    \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n    \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n    \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n    \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\",\n    \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n])\n\ndef tokenize_whitespace(line: str, lowercase: bool = True, filter_stopwords: bool = True, filter_punctuation: bool = True, **kwargs) -> list[str]:\n    \"\"\"Tokenize *line* by whitespace with optional lower-casing, punctuation stripping and\n    stop-word removal.\n\n    Args:\n        line: Input text string.\n        lowercase: If True, convert text to lower-case before tokenisation.\n        filter_stopwords: If True, remove common English stop-words.\n        filter_punctuation: If True, strip punctuation from each token.\n        **kwargs: Additional keyword arguments for future compatibility.\n\n    Returns:\n        List of processed tokens in their original order.\n    \"\"\"\n    # Convert to lower-case if required\n    if lowercase:\n        line = line.lower()\n    \n    # Split the line into words using whitespace as the delimiter\n    tokens = line.split()\n    \n    # Initialize a list to store valid tokens\n    valid_tokens = []\n    \n    # Process each token\n    for token in tokens:\n        # Strip leading and trailing punctuation if required\n        if filter_punctuation:\n            token = token.strip(string.punctuation)\n        \n        # Check if the token is not empty after processing\n        if token:\n            # Filter out stop-words if required\n            if filter_stopwords and token in stop_words:\n                continue\n            \n            # Add the valid token to the list\n            valid_tokens.append(token)\n    \n    return valid_tokens\n\n# Example usage:\ntext = \"Hello, world! This is a test sentence.\"\ntokens = tokenize_whitespace(text)\nprint(tokens)  # Output: ['hello', 'world', 'this', 'is', 'test', 'sentence']\n```\n\n### Explanation:\n- **Lowercasing**: The function first checks if the `lowercase` flag is `True`. If so, it converts the entire input string to lowercase.\n- **Splitting**: It then splits the string into individual tokens based on whitespace.\n- **Punctuation Removal**: Each token is stripped of leading and trailing punctuation using `string.punctuation`.\n- **Stop-word Filtering**: If the `filter_stopwords` flag is `True`, the function checks if the token is in the list of stop-words and skips it if it is.\n- **Valid Tokens**: Only non-empty tokens that pass all the filters are added to the result list.\n- **Order Preservation**: The function preserves the original order of tokens in the output list."}
{"task_id": 55, "completion_id": 0, "completion": "To implement the RMSprop optimization algorithm for linear regression, we need to follow these steps:\n\n1. Initialize the weight vector `w` to zero.\n2. Compute the initial gradient `g`.\n3. Initialize the running average of squared gradients `s` to zero.\n4. Iterate over the dataset in batches:\n   - Calculate the gradient for the current batch.\n   - Update the running average of squared gradients.\n   - Update the weight vector using the RMSprop update rule.\n5. Stop the iteration if the norm of the gradient is less than `epsilon` or if the maximum number of iterations is reached.\n6. Return the final weight vector rounded to four decimal places.\n\nHere's the implementation of the `rms_prop` function:\n\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    # Initialize weights to zero\n    w = np.zeros(X.shape[1])\n    \n    # Initialize running average of squared gradients\n    s = np.zeros_like(w)\n    \n    # Loop through the dataset\n    for _ in range(max_iter):\n        # Calculate the number of complete batches\n        num_batches = len(X) // batch_size\n        \n        # Process each batch\n        for i in range(num_batches):\n            # Get the current batch indices\n            batch_indices = slice(i * batch_size, min((i + 1) * batch_size, len(X)))\n            \n            # Extract the current batch\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            \n            # Compute the prediction\n            y_pred = np.dot(X_batch, w)\n            \n            # Compute the gradient\n            g = (1 / len(y_batch)) * np.dot(X_batch.T, (y_pred - y_batch))\n            \n            # Update the running average of squared gradients\n            s = rho * s + (1 - rho) * g**2\n            \n            # Update the weight vector using the RMSprop update rule\n            w -= eta * g / np.sqrt(s + eps_station)\n        \n        # Check if the norm of the gradient is small enough to stop\n        if np.linalg.norm(g) < epsilon:\n            break\n    \n    # Round the weight vector to four decimal places and return as a list\n    return [round(x, 4) for x in w.tolist()]\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([7, 8, 9])\n# print(rms_prop(X, y))\n```\n\nThis function implements the RMSprop optimization algorithm for linear regression. It processes the data in mini-batches and updates the weight vector according to the RMSprop rules until the convergence criteria are met. The final weight vector is returned as a list rounded to four decimal places."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Compute the SoftPlus activation function or its gradient.\n\n    Parameters:\n    x (scalar or list/array): Input data for which to compute the SoftPlus.\n    deriv (bool): Flag indicating whether to compute the gradient (True) or the SoftPlus itself (False).\n\n    Returns:\n    scalar/list/array: The SoftPlus output or gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        x = np.array([x])\n\n    if deriv:\n        # Numerically stable computation of the gradient using log1p\n        exp_x = np.exp(x)\n        sigm = 1 / (1 + exp_x)\n        return round(sigm, 4)\n\n    else:\n        # Numerically stable computation of the SoftPlus using log1p\n        log_sum_exp = np.log1p(exp_x)\n        return round(log_sum_exp, 4)\n\n# Example usage:\nprint(softplus(0))  # Should print 0.6931\nprint(softplus([-1, 0, 1]))  # Should print [0.3132, 0.6931, 1.3132]\nprint(softplus(10, True))  # Should print 0.7311\nprint(softplus(-10, True))  # Should print 0.2689\n```"}
{"task_id": 58, "completion_id": 0, "completion": "To implement the unsmoothed maximum likelihood estimation (MLE) N-gram language model, we need to follow these steps:\n\n1. Count the occurrences of each N-gram in the corpus.\n2. Calculate the log-probability for each N-gram in the sequence.\n3. Sum up the log-probabilities to get the total log-probability of the sequence.\n\nHere's the implementation of the `unsmoothed_ngram_log_prob` function:\n\n```python\nimport numpy as np\nfrom collections import Counter\nfrom typing import List\n\ndef unsmoothed_ngram_log_prob(corpus: List[str], sequence: List[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n    \"\"\"\n    if N < 1:\n        raise ValueError(\"Order N must be greater than or equal to 1\")\n    \n    # Step 1: Count the occurrences of each N-gram in the corpus\n    ngrams_count = Counter()\n    for i in range(len(corpus) - N + 1):\n        ngram = tuple(corpus[i:i+N])\n        ngrams_count[ngram] += 1\n    \n    # Step 2: Calculate the total number of unique prefixes of length N-1\n    num_unique_prefixes = len(ngrams_count)\n    \n    # Step 3: Compute the log-probability for each N-gram in the sequence\n    log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i:i+N])\n        \n        # Check if the n-gram is present in the corpus\n        if ngram not in ngrams_count:\n            return float('-inf')\n        \n        # Calculate the log-probability of the current n-gram\n        count_ngram = ngrams_count[ngram]\n        count_prefix = num_unique_prefixes if N == 1 else ngrams_count[tuple(sequence[i:i+N-1])]\n        \n        # Avoid division by zero\n        if count_prefix == 0:\n            return float('-inf')\n        \n        prob_ngram = count_ngram / count_prefix\n        log_prob += np.log(prob_ngram)\n    \n    # Step 4: Return the total log-probability rounded to 4 decimal places\n    return round(log_prob, 4)\n\n# Example usage:\ncorpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\nsequence = [\"the\", \"quick\", \"brown\", \"fox\"]\nN = 2\nprint(unsmoothed_ngram_log_prob(corpus, sequence, N))  # Output: -2.6657\n```\n\n### Explanation:\n\n1. **Counting N-grams**: We use a `Counter` to count the occurrences of each N-gram in the corpus. This helps us determine how many times each N-gram appears.\n\n2. **Unique Prefixes**: For N-grams of order greater than 1, we calculate the number of unique prefixes of length \\(N-1\\). This is necessary because the denominator of the probability formula requires the count of all possible prefixes of length \\(N-1\\).\n\n3. **Log-Probability Calculation**: For each N-gram in the sequence, we check if it exists in the corpus. If it does, we calculate its log-probability using the formula:\n   \\[\n   \\text{log } P(\\text{n-gram}) = \\log \\left( \\frac{\\text{count}(\\text{n-gram})}{\\text{count}(\\text{prefix})) \\right)\n   \\]\n   If any prefix has a count of zero, the log-probability is undefined, and we return `-inf`.\n\n4. **Summing Log-Probabilities**: We sum up the log-probabilities of all N-grams in the sequence to get the total log-probability.\n\n5. **Rounding**: Finally, we round the result to four decimal places before returning it."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    if not y.size:\n        return 0.0\n    \n    unique_values, counts = np.unique(y, return_counts=True)\n    \n    if criterion == \"entropy\":\n        total_samples = len(y)\n        entropy = -sum((count / total_samples) * np.log2(count / total_samples))\n        return round(entropy, 4)\n    \n    elif criterion == \"gini\":\n        gini = 1 - sum((count / len(y)) ** 2)\n        return round(gini, 4)\n    \n    elif criterion == \"mse\":\n        mse = np.mean((y - np.mean(y)) ** 2)\n        return round(mse, 4)\n    \n    else:\n        raise ValueError(\"Criterion must be 'entropy', 'gini', or 'mse'.\")\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    X = np.array(X)\n    y = np.array(y)\n    W = np.array(W)\n\n    # Check input dimensions\n    if X.ndim != 2:\n        return -1\n    if len(y.shape) not in [1, 2]:\n        return -1\n    if y.shape[0] != X.shape[0]:\n        return -1\n    if W.ndim == 1:\n        W = W[:, np.newaxis]\n    if W.shape[0] != X.shape[1]:\n        return -1\n\n    # Compute the gradient\n    gradient = (X.T @ (X @ W - y)) / X.shape[0]\n\n    # Round the gradient to 4 decimal places\n    gradient_rounded = np.round(gradient, decimals=4)\n\n    # Return the result as a list of lists\n    return gradient_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [3, 4], [5, 6]]\n# y = [7, 8, 9]\n# W = [1, 1]\n# print(grad(X, y, W))\n```\n\nThis function checks the input dimensions, computes the gradient using vectorized operations, rounds the result to four decimal places, and returns it as a list of lists. If the input dimensions are incompatible, it returns `-1`."}
{"task_id": 62, "completion_id": 0, "completion": "To implement the univariate Lasso regression using coordinate descent with polynomial features, we need to follow these steps:\n\n1. **Build the Design Matrix**: Create a design matrix that includes a column of ones (for the bias) and columns for each polynomial term \\( x^j \\) from \\( j = 1 \\) to \\( degree \\).\n\n2. **Coordinate Descent Optimization**: Perform coordinate descent to minimize the Lasso objective function. This involves updating the bias and each coefficient iteratively.\n\n3. **Early Stopping**: Terminate the optimization process if the largest absolute weight change falls below the specified tolerance (`tol`) or if the maximum number of iterations (`n_iterations`) is reached.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n\n    # Number of samples\n    m = len(X)\n    \n    # Initialize coefficients with zeros\n    w = np.zeros(degree + 1)\n    \n    # Function to perform coordinate descent\n    def coordinate_descent():\n        max_change = tol + 1\n        \n        for _ in range(n_iterations):\n            old_w = w.copy()\n            \n            # Update the bias\n            w[0] = np.mean(y - X.dot(w))\n            \n            # Update each coefficient\n            for j in range(1, degree + 1):\n                x_j = X**j\n                rho = x_j.T @ (y - X.dot(w) + w[j] * x_j)\n                w[j] = soft_threshold(rho, reg_factor) / (x_j.T @ x_j)\n            \n            # Check for convergence\n            max_change = max(np.abs(old_w - w))\n            \n            if max_change < tol:\n                break\n    \n    # Perform coordinate descent\n    coordinate_descent()\n    \n    # Round coefficients to 4 decimal places\n    w_rounded = [round(w[i], 4) for i in range(degree + 1)]\n    \n    return w_rounded\n\ndef soft_threshold(x, threshold):\n    \"\"\"Apply soft thresholding to a value.\"\"\"\n    return np.sign(x) * np.maximum(abs(x) - threshold, 0)\n\n# Example usage:\nX = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 7, 8]\ndegree = 2\nreg_factor = 0.1\ncoefficients = lasso_regression(X, y, degree, reg_factor)\nprint(coefficients)\n```\n\n### Explanation:\n\n- **Design Matrix Construction**: We create a design matrix `X_poly` by raising each element of `X` to powers from 1 to `degree`. We add a column of ones at the beginning to handle the bias term.\n\n- **Coordinate Descent**: We iterate over each coefficient, updating it based on the current values of all other coefficients. The update formula for each coefficient involves calculating the residual contribution of the term \\( x^j \\) and applying soft thresholding.\n\n- **Soft Thresholding**: The soft thresholding function ensures that coefficients shrink towards zero but do not become negative unless they exceed the regularization strength.\n\n- **Convergence Check**: We check the largest absolute change in coefficients between iterations. If this change is less than the specified tolerance, we terminate the loop.\n\nThis implementation efficiently computes the Lasso regression coefficients using coordinate descent, adhering to the requirements specified in the task."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Calculate the backward probability vector beta_t for a given time index t in a Hidden Markov Model.\n\n    Parameters:\n    A (list[list[float]]): Transition probability matrix of shape (N, N).\n    B (list[list[float]]): Emission probability matrix of shape (N, M).\n    obs (list[int]): List of observation indices (length T).\n    t (int): Integer time index (0 \u2264 t < T).\n\n    Returns:\n    list[float]: List containing the N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    A = np.array(A)\n    B = np.array(B)\n    obs = np.array(obs)\n\n    # Initialize the backward probability vector with ones\n    beta = np.ones(len(A))\n\n    # Iterate from t+1 to T-1 to fill the backward probability vector\n    for k in range(t + 1, len(obs)):\n        # Compute the next time step's backward probabilities\n        beta_next = np.dot(A, B[:, obs[k]])\n        # Update the current time step's backward probabilities using the formula\n        beta *= beta_next / np.sum(beta_next)\n\n    # Round the result to 4 decimal places\n    return [round(x, 4) for x in beta]\n\n# Example usage:\nA = [[0.5, 0.5], [0.5, 0.5]]\nB = [[0.9, 0.1], [0.2, 0.8]]\nobs = [0, 1]\nt = 0\nprint(backward_beta(A, B, obs, t))  # Output will depend on the input values\n```\n\nThis function calculates the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model using dynamic programming. The function iterates backwards through the observation sequence, updating the backward probabilities according to the HMM model's transition and emission probabilities. The final result is rounded to four decimal places before being returned."}
{"task_id": 64, "completion_id": 0, "completion": "Here's a Python implementation of the naive 1-D Discrete Fourier Transform (DFT):\n\n```python\nimport numpy as np\n\ndef dft(frame: np.ndarray, positive_only: bool = True) -> list:\n    \"\"\"Compute the Discrete Fourier Transform (DFT) of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array containing the signal samples.\n        positive_only: Whether to return only the non-negative frequency\n            terms (first *N//2+1* coefficients).  Defaults to ``True``.\n\n    Returns:\n        list[complex]: DFT coefficients rounded to 4 decimals.  Length is\n        *N//2+1* when *positive_only* is ``True`` and *N* otherwise.\n    \"\"\"\n    N = len(frame)\n    c = np.zeros(N, dtype=complex)\n\n    for k in range(N):\n        for n in range(N):\n            c[k] += frame[n] * np.exp(-2j * np.pi * k * n / N)\n\n    if positive_only:\n        c = c[:N // 2 + 1]\n\n    # Convert to list and round to 4 decimal places\n    c_rounded = [round(c.real, 4) + 1j * round(c.imag, 4) for c in c]\n    \n    return c_rounded\n\n# Example usage:\nframe = np.array([1, 2, 3, 4])\nresult = dft(frame)\nprint(result)\n```\n\n### Explanation:\n\n1. **Initialization**: We initialize an array `c` of zeros with the same length as the input `frame` to store the DFT coefficients.\n\n2. **Nested Loop**: We use two nested loops to compute each coefficient \\( c_k \\). The outer loop iterates over all possible frequencies \\( k \\), and the inner loop sums up the contributions from each sample \\( x_n \\).\n\n3. **Exponential Term**: For each pair of \\( n \\) and \\( k \\), we calculate the exponential term \\( e^{-2\\pi i k n / N} \\) using NumPy's `exp` function.\n\n4. **Summation**: We accumulate the sum of these exponentials into `c[k]`.\n\n5. **Positive Only Option**: If `positive_only` is `True`, we slice `c` to include only the first `N//2 + 1` coefficients, which correspond to the non-negative frequencies.\n\n6. **Rounding and Conversion**: We convert each complex number in `c` to a string representation with four decimal places and convert it back to a list.\n\nThis implementation ensures that the DFT is computed naively without using any optimized libraries like `numpy.fft`."}
{"task_id": 65, "completion_id": 0, "completion": "To implement the backward algorithm for a Hidden Markov Model (HMM), we need to follow these steps:\n\n1. Initialize the backward variables $\\beta_t(i)$ for all time steps $t$ and all states $i$. Initially, $\\beta_T(i) = 1$ for all $i$ since there are no future observations after $T-1$.\n\n2. Use the recursive formula to fill in the backward variables from $t = T-2$ down to $t = 0$:\n   \\[\n   \\beta_t(i) = \\sum_{j=0}^{N-1} A_{ij} \\cdot B_{j, o_{t+1}} \\cdot \\beta_{t+1}(j)\n   \\]\n   where $N$ is the number of states.\n\n3. Compute the overall probability of the observation sequence $O$ by summing up the probabilities of starting in each state and ending at each state:\n   \\[\n   P(O | \\lambda) = \\sum_{i=0}^{N-1} \\pi_i \\cdot B_{i, o_0} \\cdot \\beta_0(i)\n   \\]\n\nHere's the implementation of the `backward_prob` function:\n\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not obs or not pi or not A or not B:\n        return 0.0\n    \n    # Number of states\n    N = len(pi)\n    \n    # Initialize beta array\n    beta = np.zeros((len(obs), N))\n    \n    # Base case: beta[T-1](i) = 1 for all i\n    beta[-1, :] = 1.0\n    \n    # Fill in the backward variables from T-2 to 0\n    for t in range(len(obs) - 2, -1, -1):\n        for i in range(N):\n            beta[t, i] = sum(A[i, j] * B[j, obs[t + 1]] * beta[t + 1, j] for j in range(N))\n    \n    # Calculate the overall probability\n    prob = sum(pi[i] * B[i, obs[0]] * beta[0, i] for i in range(N))\n    \n    # Return the result rounded to 6 decimal places\n    return round(prob, 6)\n\n# Example usage:\n# A = [[0.5, 0.5], [0.4, 0.6]]\n# B = [[0.9, 0.1], [0.8, 0.2]]\n# pi = [0.6, 0.4]\n# obs = [0, 1]\n# print(backward_prob(A, B, pi, obs))  # Output: 0.773921\n```\n\nThis function first checks if any of the input lists are empty and returns 0.0 if they are. It then initializes the backward variable array `beta` and fills it using the backward recursion. Finally, it calculates the overall probability and returns it rounded to six decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "To complete the `gen_hmm_data` function, we need to implement the logic for generating sequences of hidden states and observations based on the provided probability distributions \\( S \\), \\( A \\), and \\( B \\). We also need to ensure that all probabilities sum to 1 within a small tolerance and handle optional seeding.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\nfrom itertools import accumulate\n\ndef gen_hmm_data(\n    S: list[float],\n    A: list[list[float]],\n    B: list[list[float]],\n    n_sample: int,\n    seed: int | None = None,\n) -> tuple[list[int], list[int]]:\n    \"\"\"Generate a sequence of hidden states and observations from a Hidden Markov Model.\n\n    The function validates the probability distributions, optionally sets a random\n    seed for reproducibility, and then performs sequential sampling: initial state\n    \u2192 observation \u2192 (transition \u2192 observation) repeated *n_sample*-1 more times.\n\n    Args:\n        S: Initial state distribution (length N).\n        A: State-transition matrix (shape N\u00d7N).\n        B: Emission probability matrix (shape N\u00d7M).\n        n_sample: Total length of the desired sequence (must be \u22651).\n        seed: Optional integer to seed NumPy\u2019s RNG so that identical inputs\n              return identical outputs.\n\n    Returns:\n        A tuple (states, observations) where both are lists of integers of\n        length *n_sample*. If any probability distribution is invalid the\n        function must return **-1**.\n    \"\"\"\n    # Validate that S is a valid probability distribution\n    if not np.isclose(sum(S), 1, atol=1e-8):\n        return -1\n    \n    # Validate that each row of A is a valid probability distribution\n    for row in A:\n        if not np.isclose(sum(row), 1, atol=1e-8):\n            return -1\n    \n    # Validate that each row of B is a valid probability distribution\n    for row in B:\n        if not np.isclose(sum(row), 1, atol=1e-8):\n            return -1\n    \n    # Convert input lists to NumPy arrays for efficient computation\n    S = np.array(S)\n    A = np.array(A)\n    B = np.array(B)\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Initialize the first hidden state\n    current_state = np.random.choice(range(len(S)), p=S)\n    \n    # Initialize lists to store the states and observations\n    states = [current_state]\n    observations = []\n    \n    # Generate the sequence of states and observations\n    for _ in range(n_sample - 1):\n        # Sample the next hidden state\n        next_state = np.random.choice(range(len(A[current_state])), p=A[current_state])\n        \n        # Sample the observation\n        observation = np.random.choice(range(len(B[next_state])), p=B[next_state])\n        \n        # Append the sampled values to the respective lists\n        states.append(next_state)\n        observations.append(observation)\n    \n    return states, observations\n\n# Example usage:\n# S = [0.5, 0.5]\n# A = [[0.7, 0.3], [0.2, 0.8]]\n# B = [[0.9, 0.1], [0.6, 0.4]]\n# n_sample = 10\n# seed = 42\n# states, observations = gen_hmm_data(S, A, B, n_sample, seed)\n# print(states, observations)\n```\n\n### Explanation:\n1. **Validation**: The function checks if the initial state distribution \\( S \\) and each row of the transition matrix \\( A \\) and emission matrix \\( B \\) sum to 1 within a tolerance of \\( 1 \\times 10^{-8} \\).\n\n2. **Initialization**: The first hidden state is randomly selected according to the initial state distribution \\( S \\).\n\n3. **Sampling**: For each subsequent step, the next hidden state is sampled from the current state's transition row, and the corresponding observation is sampled from the current state's emission row.\n\n4. **Reproducibility**: If a seed is provided, it is set using `np.random.seed()` to ensure that the same sequence is generated each time with the same input parameters.\n\n5. **Output**: The function returns a tuple of two lists: one for the sequence of hidden states and one for the sequence of observations. If any distribution is invalid, the function returns `-1`."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float = 0.01, momentum: float = 0.0, prev_update: np.ndarray | None = None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n    \"\"\"\n    # Initialize previous update if not provided\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    \n    # Compute the momentum update\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    \n    # Update the parameters\n    updated_w = w - learning_rate * new_update\n    \n    # Convert results to lists and round to 4 decimal places\n    updated_w_list = [round(x, 4) for x in updated_w.flatten()]\n    new_update_list = [round(x, 4) for x in new_update.flatten()]\n    \n    return updated_w_list, new_update_list\n```\n\nThis implementation defines the `sgd_momentum_update` function, which performs a single optimization step using stochastic gradient descent with momentum. It handles the initialization of the previous update if it's not provided, computes the new update, updates the parameters, and returns the results as specified."}
{"task_id": 69, "completion_id": 0, "completion": "To implement the forward algorithm for Hidden Markov Models (HMMs), we need to follow these steps:\n\n1. **Input Validation**: Ensure that the dimensions of the matrices \\( S \\), \\( A \\), and \\( B \\) are consistent with each other and the length of the observations list.\n2. **Check Validity of Observations**: Verify that all indices in the observations list are within the valid range [0, m-1].\n3. **Compute the Forward Probability**: Implement the forward algorithm using dynamic programming to compute the probability of the observed sequence.\n\nHere's the implementation of the `forward_algorithm` function:\n\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Step 1: Validate input dimensions\n    if len(S) != len(A):\n        return -1\n    if len(A[0]) != len(B):\n        return -1\n    if any(len(row) != len(S) for row in A):\n        return -1\n    if any(len(row) != len(B[0]) for row in B):\n        return -1\n    \n    # Step 2: Check validity of observations\n    num_states = len(S)\n    max_observation = max(observations)\n    if max_observation >= num_states:\n        return -1\n    \n    # Initialize the forward probabilities array\n    forward_probabilities = np.zeros((len(observations), num_states))\n    \n    # Initialize the first column of the forward probabilities\n    for j in range(num_states):\n        forward_probabilities[0, j] = S[j] * B[j][observations[0]]\n    \n    # Compute the forward probabilities for subsequent time steps\n    for t in range(1, len(observations)):\n        for j in range(num_states):\n            forward_probabilities[t, j] = (\n                sum(forward_probabilities[t-1, k] * A[k, j] for k in range(num_states)) *\n                B[j][observations[t]]\n            )\n    \n    # The total probability of the sequence is the sum of the last column of forward_probabilities\n    total_probability = np.sum(forward_probabilities[-1])\n    \n    # Round the result to 4 decimal places\n    return round(total_probability, 4)\n\n# Example usage:\nS = [0.5, 0.5]\nA = [\n    [0.7, 0.3],\n    [0.2, 0.8]\n]\nB = [\n    [0.9, 0.1],\n    [0.4, 0.6]\n]\nobservations = [0, 1]\n\nprint(forward_algorithm(S, A, B, observations))  # Output should be approximately 0.3384\n```\n\n### Explanation:\n\n1. **Dimension Validation**:\n   - We check if the number of rows in \\( A \\) matches the length of \\( S \\).\n   - We ensure that the number of columns in \\( B \\) matches both the length of \\( A \\) and \\( S \\).\n\n2. **Observation Validation**:\n   - We determine the maximum observation value and compare it with the number of states. If any observation exceeds the number of states, we return -1.\n\n3. **Forward Algorithm**:\n   - We initialize a 2D array `forward_probabilities` to store the forward probabilities at each time step and for each state.\n   - We set the initial probabilities for the first time step using the product of the initial state probabilities and the emission probabilities for the first observation.\n   - For each subsequent time step, we update the forward probabilities by considering the transition probabilities and the emission probabilities.\n\n4. **Result Calculation**:\n   - The total probability of the sequence is obtained by summing the last column of the `forward_probabilities` array.\n   - Finally, we round the result to four decimal places before returning it."}
{"task_id": 70, "completion_id": 0, "completion": "To implement the FP-Growth algorithm in Python, we need to follow these steps:\n\n1. **Count Item Support**: First, we count the frequency of each individual item across all transactions.\n2. **Build the FP-Tree**: We construct a prefix tree where each node represents an item and its support count.\n3. **Prune Infrequent Items**: Remove items from the FP-Tree that do not meet the minimum support threshold.\n4. **Generate Candidate Item-Sets**: Use the FP-Tree to generate candidate item-sets for further mining.\n5. **Mine Conditional FP-Trees**: For each candidate item-set, create a conditional FP-Tree and extract frequent item-sets.\n6. **Combine Results**: Combine the frequent item-sets found from both the original tree and the conditional trees.\n\nHere's the implementation of the FP-Growth algorithm:\n\n```python\nfrom collections import Counter, defaultdict\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    if not transactions:\n        return []\n\n    # Step 1: Count item support\n    item_counts = Counter()\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] += 1\n\n    # Step 2: Filter out infrequent items\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n\n    # Step 3: Sort frequent items lexicographically\n    frequent_items_sorted = sorted(frequent_items.keys())\n\n    # Step 4: Construct the initial FP-Tree\n    root = defaultdict(int)\n    for transaction in transactions:\n        path = root\n        for item in frequent_items_sorted:\n            if item in transaction:\n                path[item] += 1\n                path = path[item]\n\n    def build_conditional_tree(header_table, path):\n        conditional_tree = defaultdict(int)\n        for item in header_table:\n            if path[item]:\n                conditional_tree[item] = path[item]\n                conditional_tree[item].update(path[item])\n                del path[item]\n        return conditional_tree\n\n    def find_frequent_item_sets(tree, header_table, path, prefix, min_support):\n        frequent_item_sets = []\n        for item in header_table:\n            if path[item]:\n                new_path = path.copy()\n                new_path[item] -= 1\n                new_prefix = prefix + [item]\n                if len(new_prefix) > 1:\n                    frequent_item_sets.append(sorted(new_prefix))\n                else:\n                    frequent_item_sets.extend(find_frequent_item_sets(conditional_tree[item], header_table[item], new_path, new_prefix, min_support))\n        return frequent_item_sets\n\n    # Step 5: Generate candidate item-sets and mine them\n    frequent_item_sets = []\n    for i in range(len(frequent_items_sorted)):\n        head, tail = frequent_items_sorted[:i+1], frequent_items_sorted[i+1:]\n        header_table = defaultdict(list)\n        for item in head:\n            header_table[tree[item]].append(item)\n\n        for item in header_table:\n            path = tree[item]\n            conditional_tree = build_conditional_tree(header_table, path)\n            frequent_item_sets.extend(find_frequent_item_sets(conditional_tree, header_table[item], path, [], min_support))\n\n    # Step 6: Sort the result\n    frequent_item_sets.sort(key=lambda x: (len(x), x))\n\n    return frequent_item_sets\n```\n\n### Explanation:\n\n- **Item Support Counting**: We use `Counter` to count the occurrences of each item across all transactions.\n- **Filtering**: We filter out items that do not meet the minimum support threshold.\n- **FP-Tree Construction**: We build a tree where each node represents an item and its support count. We only include items that are frequent.\n- **Conditional Tree Generation**: For each frequent item set, we generate a conditional tree based on the remaining items.\n- **Mining**: We recursively mine the conditional trees to find more frequent item sets.\n- **Sorting**: Finally, we sort the resulting frequent item sets by their size and lexicographical order.\n\nThis implementation ensures that the results are deterministic and adhere to the specified constraints."}
{"task_id": 71, "completion_id": 0, "completion": "To solve this problem, we need to iterate through the list of samples and count the occurrences of each class label. Here's how you can implement the `label_uniq_cnt` function:\n\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    \n    for sample in data:\n        if len(sample) > 0:\n            label = sample[-1]\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n    \n    return label_counts\n\n# Example usage:\ndata = [\n    [1, 2, 3, 'A'],\n    [4, 5, 'B', 'B'],\n    [6, 7, 8, 'C'],\n    [9, 10, 'B', 'B'],\n    ['A', 'A', 'A']\n]\n\nprint(label_uniq_cnt(data))\n```\n\n### Explanation:\n- We initialize an empty dictionary `label_counts` to store the frequency of each label.\n- We iterate over each sample in the `data` list.\n- For each sample, we check if it has at least one element (to avoid errors when accessing the last element).\n- We extract the last element of the sample as the label.\n- We update the count of this label in the `label_counts` dictionary. If the label is already in the dictionary, we increment its count; otherwise, we add it with a count of 1.\n- Finally, we return the `label_counts` dictionary.\n\nThis function efficiently counts the occurrences of each label in the data set and handles edge cases such as empty lists gracefully."}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"\n    Calculate the variance reduction for a regression tree split.\n\n    Parameters:\n    - y: The target vector for the parent node.\n    - y_left: The target vector for the left child after the split.\n    - y_right: The target vector for the right child after the split.\n\n    Returns:\n    - A float representing the variance reduction.\n    \"\"\"\n    # Calculate the population variance of the parent node\n    var_y = np.var(y, axis=0)\n    \n    # Calculate the number of samples in each child\n    n = len(y)\n    n_left = len(y_left)\n    n_right = len(y_right)\n    \n    # Calculate the weighted sum of variances from the children\n    var_left = np.var(y_left, axis=0)\n    var_right = np.var(y_right, axis=0)\n    weighted_var_sum = (n_left / n) * var_left + (n_right / n) * var_right\n    \n    # Calculate the variance reduction\n    vr = var_y - weighted_var_sum\n    \n    # Return the result rounded to the nearest 4th decimal\n    return round(vr, 4)\n```\n\nThis function calculates the variance reduction for a regression tree split using the provided formula. It computes the population variance of the parent node, the number of samples in each child, and the weighted sum of the variances from the children. Finally, it returns the variance reduction rounded to four decimal places."}
{"task_id": 74, "completion_id": 0, "completion": "To implement the `magnitude_spectrum` function, we need to follow these steps:\n\n1. Ensure that the input is a 2D array if provided, otherwise treat it as a 1D array.\n2. Compute the Real Fast Fourier Transform (FFT) using `numpy.fft.rfft`, which returns the non-redundant positive-frequency coefficients.\n3. Take the absolute value of the FFT results to get the magnitude spectrum.\n4. Round each magnitude value to four decimal places.\n5. Return the result as a list of lists.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef magnitude_spectrum(frames: \"np.ndarray\") -> \"list[list[float]]\":\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    # Convert the input to a 2D array if it's not already\n    if len(frames.shape) == 1:\n        frames = frames.reshape(1, -1)\n    \n    # Compute the RFFT to get the non-redundant positive-frequency coefficients\n    fft_result = np.fft.rfft(frames, axis=1)\n    \n    # Calculate the magnitude spectrum\n    magnitude_spectrum = np.abs(fft_result)\n    \n    # Round each magnitude value to four decimal places\n    magnitude_spectrum = np.round(magnitude_spectrum, 4)\n    \n    # Convert the result to a list of lists\n    return magnitude_spectrum.tolist()\n\n# Example usage:\nframes = np.array([[1, 2, 3], [4, 5, 6]])\nresult = magnitude_spectrum(frames)\nprint(result)\n```\n\n### Explanation:\n- **Input Handling**: If the input is a 1D array, it is reshaped into a 2D array with a single row to ensure consistent processing.\n- **FFT Calculation**: `np.fft.rfft` is used to compute the FFT along the specified axis (axis=1 for rows in a 2D array).\n- **Magnitude Calculation**: The absolute value of the FFT result gives the magnitude spectrum.\n- **Rounding**: Each element in the magnitude spectrum is rounded to four decimal places using `np.round`.\n- **Output Formatting**: The resulting array is converted to a list of lists using `tolist()` before being returned."}
{"task_id": 75, "completion_id": 0, "completion": "To implement the k-Nearest Neighbours (k-NN) classifier, we need to follow these steps:\n\n1. Calculate the Euclidean distance between each test sample and every training sample.\n2. Identify the `k` nearest neighbors for each test sample.\n3. Determine the most frequent class among these `k` neighbors. In case of a tie, select the smallest class label.\n4. Return the predicted class labels for all test samples.\n\nHere's the implementation of the `knn_predict` function:\n\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    if not (1 <= k <= len(X_train)):\n        return [-1]\n\n    def euclidean_distance(x1, x2):\n        \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n        return np.linalg.norm(np.array(x1) - np.array(x2))\n\n    predictions = []\n    \n    for test_sample in X_test:\n        distances = [(euclidean_distance(test_sample, train_sample), train_label)\n                     for train_sample, train_label in zip(X_train, y_train)]\n        \n        # Sort distances by ascending order\n        distances.sort()\n        \n        # Get the k nearest neighbors\n        nearest_neighbors = distances[:k]\n        \n        # Extract the class labels of the nearest neighbors\n        neighbor_labels = [label for _, label in nearest_neighbors]\n        \n        # Find the most common class label (in case of ties, choose the smallest one)\n        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n        max_count = np.max(counts)\n        most_common_labels = unique_labels[counts == max_count]\n        \n        # Choose the smallest class label in case of ties\n        predicted_class = min(most_common_labels)\n        \n        predictions.append(predicted_class)\n\n    return predictions\n```\n\n### Explanation:\n\n- **Distance Calculation**: The Euclidean distance is calculated using `np.linalg.norm` to ensure that the result is squared before taking the square root, which simplifies the calculation but doesn't affect the relative distances.\n  \n- **Sorting**: The distances are sorted in ascending order to easily access the `k` nearest neighbors.\n\n- **Tie Handling**: After selecting the `k` nearest neighbors, we determine the most frequent class label. In case of a tie, we choose the smallest class label using `min()`.\n\n- **Output**: The function returns a list of predicted class labels for each test sample. If `k` is invalid, it returns `-1`."}
{"task_id": 76, "completion_id": 0, "completion": "To implement the categorical cross-entropy loss, we need to follow these steps:\n\n1. Ensure that both `y` and `y_pred` have the same shape.\n2. Add a small epsilon value to the predictions to prevent taking the logarithm of zero.\n3. Compute the cross-entropy loss using the formula provided.\n4. Sum the losses over all samples and classes.\n5. Return the total loss rounded to four decimal places.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Check if shapes match\n    if y.shape != y_pred.shape:\n        raise ValueError(\"Shapes of y and y_pred must be the same.\")\n    \n    # Add a small epsilon to avoid log(0)\n    eps = np.finfo(float).eps\n    \n    # Calculate the cross-entropy loss\n    loss = -np.sum(y * np.log(y_pred + eps))\n    \n    # Return the loss rounded to 4 decimal places\n    return round(loss, 4)\n\n# Example usage:\ny_true = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\ny_pred = [[0.2, 0.6, 0.2], [0.7, 0.2, 0.1], [0.1, 0.3, 0.6]]\n\nprint(cross_entropy_loss(y_true, y_pred))  # Output should be 0.9888\n```\n\n### Explanation:\n\n- **Input Validation**: We first convert the inputs to NumPy arrays and check if their shapes match. If not, we raise a `ValueError`.\n- **Epsilon Addition**: We add a small epsilon value (`np.finfo(float).eps`) to `y_pred` to ensure that no logarithm operation results in zero.\n- **Cross-Entropy Calculation**: We compute the cross-entropy loss using the formula `-sum(y * log(y_pred + eps))`. This involves summing over each sample and class.\n- **Rounding**: Finally, we round the computed loss to four decimal places before returning it."}
{"task_id": 77, "completion_id": 0, "completion": "To implement the forward propagation for an L-layer neural network using ReLU activations in the hidden layers and the sigmoid activation in the output layer, we need to follow these steps:\n\n1. Initialize the activation `A` with the input `X`.\n2. Iterate through each layer, performing a linear transformation followed by the appropriate activation function.\n3. Store the intermediate results in a list called `caches`.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU...ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    # Initialize the list to store caches\n    caches = []\n    \n    # Set the current activation to the input data\n    A = X\n    \n    # Loop through each layer\n    for l in range(1, len(parameters) // 2 + 1):\n        # Retrieve the weights and bias for the current layer\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        \n        # Perform the linear transformation\n        Z = np.dot(W, A) + b\n        \n        # Store the linear transformation result and the activation function applied\n        if l == 1:\n            # For the first layer, apply ReLU\n            A = np.maximum(0, Z)\n        else:\n            # For subsequent layers, apply ReLU\n            A = np.maximum(0, Z)\n        \n        # Append the linear transformation result and the activation function to the cache\n        caches.append((Z, A))\n    \n    # Apply the sigmoid activation to the output layer\n    W_L = parameters['W' + str(len(parameters) // 2 + 1)]\n    b_L = parameters['b' + str(len(parameters) // 2 + 1)]\n    Z_L = np.dot(W_L, A) + b_L\n    A_L = 1 / (1 + np.exp(-Z_L))\n    \n    # Append the linear transformation result and the activation function to the cache\n    caches.append((Z_L, A_L))\n    \n    # Round the final activation matrix AL to 4 decimal places and convert it to a plain Python list\n    AL_rounded = A_L.round(4).tolist()\n    \n    return AL_rounded, caches\n\n# Example usage:\n# X = np.random.rand(784, 50)  # Example input\n# parameters = {...}  # Example parameters\n# AL, caches = L_model_forward(X, parameters)\n# print(AL)\n```\n\n### Explanation:\n- **Input Layer**: Start with the input matrix `X`.\n- **Linear Transformation**: For each layer, compute \\( Z = W \\cdot A + b \\).\n- **Activation Function**: \n  - Use ReLU for all hidden layers (\\( A = \\max(0, Z) \\)).\n  - Use sigmoid for the output layer (\\( A = \\frac{1}{1 + e^{-Z}} \\)).\n- **Cache Storage**: Store the intermediate results (linear transformation and activation) in the `caches` list.\n- **Output Layer**: After processing all layers, apply the sigmoid activation to get the final output `AL`.\n- **Return Value**: Return the rounded and converted final activation `AL` along with the `caches`."}
{"task_id": 78, "completion_id": 0, "completion": "Here's how you can implement the Adamax optimization algorithm using the provided specifications:\n\n```python\nimport numpy as np\n\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\n\n    Args:\n        params:  Current parameter vector.\n        grads:   Current gradient vector (same length as `params`).\n        m:       First-moment estimates (same length as `params`).\n        u:       Exponentially weighted infinity-norms (same length as `params`).\n        t:       Current time-step counter (starts at 1).\n        learning_rate: Learning rate \u03b7 (default 0.002).\n        beta1:   Exponential decay rate for the first moment \u03b2\u2081.\n        beta2:   Exponential decay rate for the infinity-norm \u03b2\u2082.\n        epsilon: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple `(new_params, new_m, new_u, new_t)` where each list is rounded\n        to 6 decimal places and `new_t = t + 1`.\n    \"\"\"\n    \n    # Ensure all inputs are numpy arrays for easy element-wise operations\n    params = np.array(params)\n    grads = np.array(grads)\n    m = np.array(m)\n    u = np.array(u)\n    \n    # Update the first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grads\n    \n    # Update the exponentially weighted infinity norm\n    u_t = np.maximum(beta2 * u, np.abs(grads))\n    \n    # Calculate the step size\n    step = learning_rate / (1 - beta1**t) * m_t / (u_t + epsilon)\n    \n    # Update the parameters\n    new_params = params - step\n    \n    # Increment the time step\n    new_t = t + 1\n    \n    # Round the results to 6 decimal places\n    new_params_rounded = [round(x, 6) for x in new_params]\n    m_t_rounded = [round(x, 6) for x in m_t]\n    u_t_rounded = [round(x, 6) for x in u_t]\n    new_t_rounded = round(new_t, 6)\n    \n    return (new_params_rounded, m_t_rounded, u_t_rounded, new_t_rounded)\n\n# Example usage:\nparams = [0.5, 0.3]\ngrads = [-0.1, 0.2]\nm = [0.0, 0.0]\nu = [0.0, 0.0]\nt = 1\n\nupdated_params, updated_m, updated_u, updated_t = adamax_step(params, grads, m, u, t)\nprint(\"Updated Parameters:\", updated_params)\nprint(\"Updated m:\", updated_m)\nprint(\"Updated u:\", updated_u)\nprint(\"Updated t:\", updated_t)\n```\n\nThis code defines the `adamax_step` function, which implements the Adamax optimization algorithm according to the given formulas. It uses NumPy for efficient array operations and rounds the results to six decimal places before returning them. The example usage demonstrates how to call the function with initial parameters and prints the updated values."}
{"task_id": 80, "completion_id": 0, "completion": "To implement the Adadelta optimization algorithm for a single update step, we need to follow the provided update rules step-by-step. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform **one** Adadelta optimisation step.\n\n    The function assumes that it is the very first update so the two running\n    averages (squared gradients and squared updates) are still zeros.  It must\n    therefore implement the mathematical rules of Adadelta and finally return\n    the updated weight vector rounded to 4 decimal places as a regular Python\n    list.\n\n    Args:\n        w:      Current weights (list or NumPy array of any shape).\n        grad_w: Corresponding gradient \u2013 same shape as *w*.\n        rho:    Decay coefficient used by Adadelta (default 0.95).\n        eps:    Small constant to avoid division by zero (default 1e-6).\n\n    Returns:\n        List[float]: weights after one Adadelta update (rounded to 4 decimals).\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    w = np.array(w)\n    grad_w = np.array(grad_w)\n    \n    # Initialize the running averages\n    E_grad = np.zeros_like(w)\n    E_delta = np.zeros_like(w)\n    \n    # Calculate the squared gradients\n    squared_gradients = grad_w**2\n    \n    # Calculate the RMS values\n    RMS_grad = np.sqrt(E_grad + eps)\n    RMS_delta = np.sqrt(E_delta + eps)\n    \n    # Calculate the adaptive learning rate\n    adaptive_lr = RMS_delta / RMS_grad\n    \n    # Calculate the update steps\n    delta_w = adaptive_lr * grad_w\n    \n    # Update the running averages\n    E_grad = rho * E_grad + (1 - rho) * squared_gradients\n    E_delta = rho * E_delta + (1 - rho) * delta_w**2\n    \n    # Update the weights\n    w_new = w - delta_w\n    \n    # Round the updated weights to 4 decimal places\n    w_new_rounded = np.round(w_new, 4).tolist()\n    \n    return w_new_rounded\n\n# Example usage:\nw = [1.0, 2.0, 3.0]\ngrad_w = [-0.1, 0.2, -0.3]\nupdated_weights = adadelta_update(w, grad_w)\nprint(updated_weights)\n```\n\n### Explanation:\n1. **Initialization**: We initialize `E_grad` and `E_delta` to zero since this is the first update.\n2. **Squared Gradients**: We calculate the squared gradients from the given gradients.\n3. **RMS Values**: We compute the Root Mean Square (RMS) values using the running averages and a small epsilon (`eps`) to prevent division by zero.\n4. **Adaptive Learning Rate**: We determine the adaptive learning rate based on the ratio of `RMS_delta` to `RMS_grad`.\n5. **Update Steps**: We calculate the update steps (`delta_w`) using the adaptive learning rate and the gradients.\n6. **Running Averages Update**: We update `E_grad` and `E_delta` with the new squared gradients and update steps.\n7. **Weight Update**: We update the weights using the calculated `delta_w`.\n8. **Rounding**: Finally, we round the updated weights to four decimal places and return them as a regular Python list."}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Clip the predicted probabilities to avoid log(0)\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Compute the binary cross-entropy loss\n    m = A2.shape[1]\n    cost = -1 / m * np.sum(Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped))\n    \n    # Return the cost rounded to 6 decimal places\n    return round(cost, 6)\n\n# Example usage:\n# A2 = np.array([[0.9, 0.1], [0.8, 0.2]])\n# Y = np.array([[1, 0], [1, 0]])\n# print(compute_cost(A2, Y))  # Expected output: 0.334796\n```\n\nThis code defines the `compute_cost` function, which calculates the binary cross-entropy cost for given predicted probabilities `A2` and ground-truth labels `Y`. It clips the predicted probabilities to prevent log(0) errors and then computes the cost using the formula provided in the description. The result is rounded to six decimal places before being returned."}
{"task_id": 82, "completion_id": 0, "completion": "To implement the \u03b5-soft exploration policy as described, we need to follow these steps:\n\n1. Identify the greediest action based on the Q-values.\n2. Calculate the base probability for the greedy action.\n3. Distribute the remaining probability equally among the other actions.\n4. Round the probabilities to four decimal places.\n5. Ensure the total sum of probabilities is exactly 1.\n\nHere's the implementation of the `epsilon_soft` function:\n\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(epsilon, float) or not (0 <= epsilon <= 1):\n        raise ValueError(\"epsilon must be a float between 0 and 1\")\n\n    # Convert Q to a NumPy array if it's not already\n    Q = np.array(Q)\n\n    # Find the index of the greediest action\n    max_index = np.argmax(Q)\n    \n    # Calculate the base probability for the greediest action\n    base_prob = 1 - epsilon + (epsilon / len(Q))\n    \n    # Initialize the probabilities array with zeros\n    probs = np.zeros_like(Q)\n    \n    # Set the probability of the greediest action\n    probs[max_index] = base_prob\n    \n    # Distribute the remaining probability equally among the other actions\n    remaining_prob = epsilon / len(Q)\n    probs += remaining_prob\n    \n    # Round the probabilities to 4 decimal places\n    probs_rounded = np.round(probs, decimals=4)\n    \n    # Ensure the sum of probabilities is exactly 1\n    total_sum = np.sum(probs_rounded)\n    if not np.isclose(total_sum, 1, atol=1e-12):\n        raise ValueError(\"The sum of probabilities must be exactly 1\")\n    \n    return probs_rounded.tolist()\n\n# Example usage:\nQ = [1.2, 0.3, 1.2, -0.1]\nepsilon = 0.1\nprint(epsilon_soft(Q, epsilon))  # Output: [0.925, 0.025, 0.025, 0.025]\n```\n\n### Explanation:\n- **Finding the Greediest Action**: We use `np.argmax(Q)` to find the index of the maximum Q-value.\n- **Base Probability Calculation**: The base probability for the greediest action is calculated as \\(1 - \\epsilon + \\frac{\\epsilon}{n}\\), where \\(n\\) is the number of actions.\n- **Distributing Remaining Probability**: The remaining probability (\\(\\epsilon / n\\)) is distributed equally among all other actions.\n- **Rounding and Normalization**: We round the probabilities to four decimal places and ensure they sum to exactly 1 using `np.isclose`. If the sum is not close enough, an error is raised."}
{"task_id": 84, "completion_id": 0, "completion": "To solve this problem, we need to implement the formula for calculating the optimal step size \\(\\gamma\\) in gradient boosting with the mean-squared-error (MSE) objective. The formula provided is:\n\n\\[\n\\gamma = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i) h_i}{\\sum_{i=1}^{n} h_i^2}\n\\]\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean squared error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Array of true target values.\n    y_pred : np.ndarray\n        Array of current predictions produced by the model.\n    h_pred : np.ndarray\n        Array of predictions from the new base learner.\n\n    Returns\n    -------\n    float\n        The optimal step size \u03b3 rounded to four decimal places. If `h_pred`\n        perfectly fits the residuals the function should return 1.0.\n    \"\"\"\n    # Calculate the numerator and denominator of the formula\n    numerator = np.sum((y - y_pred) * h_pred)\n    denominator = np.sum(h_pred**2)\n    \n    # Handle the case where the denominator is zero\n    if denominator == 0:\n        return 1.0\n    \n    # Calculate gamma\n    gamma = numerator / denominator\n    \n    # Return the result rounded to four decimal places\n    return round(gamma, 4)\n\n# Example usage:\n# y = np.array([1, 2, 3, 4])\n# y_pred = np.array([1.5, 2.5, 3.5, 4.5])\n# h_pred = np.array([-0.5, 0, 0.5, 1])\n\n# print(mse_line_search(y, y_pred, h_pred))\n```\n\n### Explanation:\n1. **Numerator Calculation**: We compute the sum of the products of the residuals `(y - y_pred)` and the predictions `h_pred`.\n2. **Denominator Calculation**: We compute the sum of the squares of the predictions `h_pred`.\n3. **Handling Zero Denominator**: If the denominator is zero, it means all `h_pred` values are zero, indicating perfect fit. In such cases, we return 1.0 as the optimal step size.\n4. **Gamma Calculation**: We calculate \\(\\gamma\\) using the formula derived from the numerator and denominator.\n5. **Rounding**: Finally, we round the result to four decimal places before returning it.\n\nThis function will correctly compute the optimal step size for gradient boosting with MSE, handling edge cases appropriately."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax_regression_step(X: np.ndarray, Y: np.ndarray, W: np.ndarray, lr: float) -> list[list[float]]:\n    \"\"\"TODO: Complete the function.\n\n    One gradient-descent step for Softmax regression using cross-entropy loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Training samples of shape (N, D).\n    Y : np.ndarray\n        One-hot encoded labels of shape (N, C).\n    W : np.ndarray\n        Current weights of shape (D, C).\n    lr : float\n        Learning rate.\n\n    Returns\n    -------\n    list[list[float]]\n        Updated weight matrix rounded to 4 decimal places.\n    \"\"\"\n    if lr == 0 or np.allclose(X, 0):\n        return W.tolist()\n\n    # Compute logits\n    Z = np.dot(X, W)\n\n    # Apply the softmax transformation row-wise\n    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n    \u0176 = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n\n    # Compute the gradient of the average cross-entropy loss\n    grad = (np.dot(X.T, \u0176 - Y)) / X.shape[0]\n\n    # Update the weights with plain gradient descent\n    W_new = W - lr * grad\n\n    # Round the updated weights to 4 decimal places and convert to list\n    return W_new.round(4).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "To implement the `aggregate_random_forest_votes` function, we need to follow the steps outlined in the problem description. Here's how you can achieve this:\n\n1. Use `collections.Counter` to count the occurrences of each class label for each sample.\n2. Determine the most common class label for each sample. In case of a tie, select the smallest label using Python's built-in sorting behavior.\n3. Return the aggregated predictions as a list.\n\nHere's the complete implementation:\n\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    num_samples = len(predictions[0])\n    \n    # Initialize a list to store the final predictions\n    final_predictions = []\n    \n    # Iterate over each sample\n    for i in range(num_samples):\n        # Count the occurrences of each class label for the current sample\n        vote_counts = Counter()\n        for row in predictions:\n            vote_counts[row[i]] += 1\n        \n        # Find the most common class label\n        most_common_label, _ = vote_counts.most_common(1)[0]\n        \n        # Append the most common label to the final predictions list\n        final_predictions.append(most_common_label)\n    \n    return final_predictions\n\n# Example usage:\npredictions = [\n    [1, 'a', 3],\n    ['b', 2, 3],\n    [1, 'a', 4],\n    ['c', 2, 3]\n]\n\nprint(aggregate_random_forest_votes(predictions))\n```\n\n### Explanation:\n- We iterate over each sample index `i`.\n- For each sample, we use `Counter` to count the occurrences of each class label across all trees.\n- The `most_common(1)` method returns the most frequently occurring element and its count. Since we only need the label, we take the first element of the tuple returned by `most_common(1)`.\n- This ensures that in case of a tie, the smallest label is chosen due to Python's sorting behavior.\n- Finally, we append the most common label to the `final_predictions` list, which is returned at the end."}
{"task_id": 87, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `layer_sizes` function according to the specifications provided:\n\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    # Extract the number of input features (rows of X)\n    n_x = X.shape[0]\n    \n    # The hidden layer size is fixed to 10\n    n_h = 10\n    \n    # Extract the number of output units (rows of Y)\n    n_y = Y.shape[0]\n    \n    # Return the tuple (n_x, n_h, n_y)\n    return (n_x, n_h, n_y)\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5], [6]])\n\nprint(layer_sizes(X, Y))  # Output: (2, 10, 1)\n```\n\nThis function calculates the number of input features (`n_x`) and output units (`n_y`) from the shapes of the input and label matrices, respectively. It then returns a tuple with these values and a fixed hidden layer size of 10."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # Directly compute softplus for scalars\n        return round(np.logaddexp(0.0, z), 4)\n    \n    # Convert to NumPy array if it's not already one\n    z_array = np.asarray(z)\n    \n    # Compute softplus using logaddexp\n    softplus_values = np.logaddexp(0.0, z_array)\n    \n    # Round results to 4 decimal places\n    rounded_values = np.round(softplus_values, 4)\n    \n    # Convert to Python list\n    return rounded_values.tolist()\n\n# Example usage\nz = np.array([-1000, 0, 3])\nprint(softplus(z))  # Output: [0.0, 0.6931, 3.0486]\n```\n\nThis implementation handles both scalar and array-like inputs, ensuring that the output is correctly rounded to four decimal places and converted to a Python list if necessary."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        The activation to be used: either \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        First element is the layer output (rounded to four decimals and\n        converted to a Python list).  Second element is a cache that will\n        be needed for back-propagation.\n    \"\"\"\n    # Linear transformation\n    Z = np.dot(W, A_prev) + b\n    \n    # Apply activation function\n    if activation == 'relu':\n        A = np.maximum(0, Z)\n    elif activation == 'sigmoid':\n        A = 1 / (1 + np.exp(-Z))\n    else:\n        raise ValueError(\"Activation must be either 'relu' or 'sigmoid'\")\n    \n    # Ensure the shape of A matches the expected output\n    assert A.shape == (W.shape[0], A_prev.shape[1]), f\"Expected shape ({W.shape[0]}, {A_prev.shape[1]}), got {A.shape}\"\n    \n    # Round the results to 4 decimal places and convert to a list\n    A_rounded = A.round(4).tolist()\n    \n    # Cache the necessary information for back-propagation\n    linear_cache = (A_prev, W, b)\n    activation_cache = Z\n    \n    return A_rounded, (linear_cache, activation_cache)\n\n# Example usage:\n# A_prev = np.random.rand(5, 3)\n# W = np.random.rand(4, 5)\n# b = np.random.rand(4, 1)\n# activation = 'relu'\n# result, cache = linear_activation_forward(A_prev, W, b, activation)\n# print(result)\n# print(cache)\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Check if the policy provides estimates\n    if 'ev_estimates' not in policy or not policy['ev_estimates']:\n        return np.nan\n    \n    # Get the true expected values from the bandit\n    arm_evs = bandit['arm_evs']\n    \n    # Get the policy's estimates\n    ev_estimates = policy['ev_estimates']\n    \n    # Sort the estimates based on the arm indices\n    sorted_arm_indices = sorted(ev_estimates.keys())\n    sorted_ev_estimates = [ev_estimates[arm_index] for arm_index in sorted_arm_indices]\n    \n    # Compute the squared errors\n    squared_errors = [(arm_evs[arm_index] - sorted_ev_estimates[i]) ** 2 for i, arm_index in enumerate(sorted_arm_indices)]\n    \n    # Calculate the mean-squared error\n    mse_value = np.mean(squared_errors)\n    \n    # Round the result to 4 decimal places\n    return round(mse_value, 4)\n\n# Example usage\nbandit = {\"arm_evs\": [0.5, 0.2, 0.9]}\npolicy = {\"ev_estimates\": {0: 0.4, 1: 0.25, 2: 0.8}}\nprint(mse(bandit, policy))  # Output: 0.0075\n```\n\nThis code defines the `mse` function according to the specified requirements, including handling cases where the policy does not provide any estimates and sorting the arm indices for accurate comparison."}
{"task_id": 91, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    dA_array = np.array(dA)\n    activation_cache_array = np.array(activation_cache)\n    \n    # Check if shapes match\n    if dA_array.shape != activation_cache_array.shape:\n        return -1\n    \n    # Compute the derivative of the ReLU function\n    dZ_array = dA_array * (activation_cache_array > 0)\n    \n    # Convert the result back to a list of lists\n    dZ = dZ_array.tolist()\n    \n    return dZ\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    # Convert input to a NumPy array\n    Z_np = np.array(Z)\n    \n    # Compute the ReLU using NumPy's maximum function\n    A = np.maximum(0, Z_np)\n    \n    # Convert the results to Python lists\n    A_list = A.tolist()\n    cache_list = Z_np.tolist()\n    \n    return (A_list, cache_list)\n\n# Example usage:\nZ_example = [[-1, 0, 2], [3, -4, 5]]\nA, cache = relu(Z_example)\nprint(\"Activation:\", A)\nprint(\"Cache:\", cache)\n```\n\nThis function first converts the input to a NumPy array to leverage its efficient element-wise operations. It then computes the ReLU using `np.maximum`, which applies the operation element-wise. Finally, it converts the results back to Python lists before returning them. The example usage demonstrates how to call the function and print the output."}
{"task_id": 94, "completion_id": 0, "completion": "To solve this problem, we can use the `collections.Counter` class to count the occurrences of each label in the list of neighbor targets. Then, we can determine the label with the highest frequency and handle ties by returning the smallest label using Python's default sorting behavior.\n\nHere's the implementation of the `knn_majority_vote` function:\n\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    # Count the occurrences of each label\n    label_counts = Counter(neighbors_targets)\n    \n    # Find the maximum frequency\n    max_frequency = max(label_counts.values())\n    \n    # Filter labels that have the maximum frequency\n    candidates = [label for label, count in label_counts.items() if count == max_frequency]\n    \n    # Return the smallest label from the candidates\n    return min(candidates)\n\n# Example usage:\nprint(knn_majority_vote([1, 2, 2, 3, 4]))  # Output: 2\nprint(knn_majority_vote(['apple', 'banana', 'apple', 'orange']))  # Output: 'apple'\nprint(knn_majority_vote([True, False, True, False, True]))  # Output: True\n```\n\n### Explanation:\n1. **Counting Labels**: We use `Counter` to create a dictionary-like object where keys are the unique labels and values are their respective counts.\n2. **Finding Maximum Frequency**: We find the maximum value in the `Counter` object, which represents the highest frequency of any label.\n3. **Identifying Candidates**: We filter the labels that have this maximum frequency into a list called `candidates`.\n4. **Returning the Smallest Label**: Finally, we return the smallest label from the `candidates` list using Python's built-in `min()` function, which handles both numeric and string comparisons correctly based on their natural order."}
{"task_id": 96, "completion_id": 0, "completion": "To implement the `additive_ngram_log_prob` function, we need to follow the steps outlined in the problem description. Here's how you can do it:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Step 1: Build all 1-N gram frequency tables from the corpus\n    n_gram_counts = defaultdict(Counter)\n    \n    for sentence in corpus:\n        words = sentence.split()\n        for i in range(len(words)):\n            context = tuple(words[max(0, i - N + 1):i])\n            n_gram_counts[context][words[i]] += 1\n    \n    # Step 2: Add an explicit \"UNK\" token to the vocabulary\n    vocab = set()\n    for counts in n_gram_counts.values():\n        vocab.update(counts.keys())\n    vocab.add(\"<UNK>\")\n    \n    # Step 3: Estimate the probability of every contiguous N-gram in the target sentence\n    target_words = sequence.split()\n    if len(target_words) < N:\n        return 0.0\n    \n    total_count = sum(n_gram_counts[tuple(target_words[:i])].values() for i in range(N))\n    log_probability = 0.0\n    \n    for i in range(len(target_words) - N + 1):\n        context = tuple(target_words[max(0, i - N + 1):i])\n        word = target_words[i]\n        \n        # Count occurrences of the word in the current context\n        count_in_context = n_gram_counts[context].get(word, 0)\n        \n        # Add K to both numerator and denominator for Laplace smoothing\n        smoothed_count = count_in_context + K\n        smoothed_total = total_count + K * len(vocab)\n        \n        # Calculate the probability using the additive smoothing formula\n        probability = smoothed_count / smoothed_total\n        \n        # Add the log of the probability to the total log probability\n        log_probability += math.log(probability)\n    \n    # Step 4: Return the natural logarithm of the sentence probability rounded to 4 decimals\n    return round(log_probability, 4)\n\n# Example usage:\ncorpus = [\n    \"hello world\",\n    \"world hello\",\n    \"hello world again\"\n]\nsequence = \"hello world again\"\nN = 2\nprint(additive_ngram_log_prob(corpus, sequence, N))  # Output should be approximately -2.6559\n```\n\n### Explanation:\n1. **Building N-gram Frequency Tables**: We iterate over each sentence in the corpus and split it into words. For each word, we construct its context (the previous `N-1` words). We then update the count of this word in the corresponding context in our `n_gram_counts` dictionary.\n\n2. **Adding UNK Token**: We extract all unique words from the counts and add `<UNK>` to represent any unseen words.\n\n3. **Calculating Probabilities**: For each word in the target sequence, we calculate the probability using the additive smoothing formula. The probability is updated for each N-gram in the sequence.\n\n4. **Summing Log-Probabilities**: We sum up the log probabilities of all N-grams in the sequence to get the final log probability of the entire sequence.\n\n5. **Rounding**: Finally, we round the result to four decimal places before returning it."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    # Convert the input to a numpy array\n    Z = np.array(Z)\n    \n    # Compute the derivative using a mask\n    derivative = np.where(Z > 0, 1, 0)\n    \n    # If the input was a scalar, return it directly\n    if len(derivative.shape) == 0:\n        return derivative.item()\n    \n    # Otherwise, convert the result back to a list\n    return derivative.tolist()\n\n# Example usage:\nprint(drelu(5))          # Output: 1.0\nprint(drelu(-2))         # Output: 0.0\nprint(drelu([3, -1, 0])) # Output: [1, 0, 0]\nprint(drelu([[1, -2], [0, 4]])) # Output: [[1, 0], [0, 1]]\n```\n\nThis function first converts the input `Z` into a NumPy array to facilitate vectorized operations. It then uses `np.where` to create a new array where elements greater than zero are set to 1 and all others are set to 0, effectively computing the derivative of the ReLU function. Finally, it checks the shape of the resulting array to determine whether to return a single float (if the input was a scalar) or a Python list (for multi-dimensional arrays)."}
{"task_id": 99, "completion_id": 0, "completion": "To implement the ReLU activation function according to the specified requirements, we need to handle different types of inputs (scalar, 1D list/array, and 2D list-of-lists/array) while ensuring that the output has the same container type as the input. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The function keeps the container type of the input:\n    scalar \u2192 scalar, list \u2192 list, NumPy array \u2192 NumPy array.\n\n    Args:\n        Z (int | float | list | numpy.ndarray): Input data \u2013 scalar, 1-D or 2-D.\n\n    Returns:\n        Same type as *Z* with all negative values clipped to 0.\n    \"\"\"\n    # Check if the input is a scalar\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n    \n    # Check if the input is a NumPy array\n    elif isinstance(Z, np.ndarray):\n        return np.maximum(0, Z)\n    \n    # If it's neither, assume it's a list\n    else:\n        # Convert list to NumPy array for processing\n        Z_np = np.array(Z)\n        \n        # Apply ReLU using NumPy's maximum function\n        result = np.maximum(0, Z_np)\n        \n        # Convert back to the original container type\n        if isinstance(Z, list):\n            return result.tolist()\n        elif isinstance(Z, tuple):\n            return tuple(result)\n        else:\n            raise ValueError(\"Unsupported input type\")\n\n# Example usage:\nprint(relu_(3))          # Output: 3\nprint(relu_(-1))         # Output: 0\nprint(relu_([1, -2, 3]))  # Output: [1, 0, 3]\nprint(relu_([[1, -2], [3, -4]]))  # Output: [[1, 0], [3, 0]]\n```\n\n### Explanation:\n- **Scalar Handling**: If the input is a scalar, we directly apply the `max(0, Z)` operation.\n- **NumPy Array Handling**: If the input is a NumPy array, we use `np.maximum(0, Z)` to compute the ReLU transform efficiently.\n- **List Handling**: If the input is a list, we first convert it to a NumPy array, perform the ReLU transformation, and then convert it back to the original list format.\n- **Error Handling**: We include error handling for unsupported input types, raising a `ValueError` if the input does not fit any of the expected formats."}
{"task_id": 100, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Predicted labels from a classifier.\n\n    Returns\n    -------\n    float | int\n        The classification error rate rounded to four decimals, or \u22121 when\n        the input lengths are not identical.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Check if the lengths are equal\n    if len(actual_array) != len(predicted_array):\n        return -1\n    \n    # Calculate the number of mismatches\n    mismatches = np.sum(actual_array != predicted_array)\n    \n    # Calculate the classification error rate\n    error_rate = mismatches / len(actual_array)\n    \n    # Round the result to four decimal places\n    return round(error_rate, 4)\n\n# Example usage\nactual = np.array([0, 1, 2, 2, 1])\npredicted = np.array([0, 0, 2, 1, 1])\nprint(classification_error(actual, predicted))  # Output: 0.4\n```\n\nThis code defines the `classification_error` function according to the specified requirements. It first converts the input lists or tuples to NumPy arrays for efficient computation. Then, it checks if the lengths of the two arrays are equal. If they are not, it returns `-1`. Otherwise, it calculates the number of mismatched elements and computes the classification error rate, rounding the result to four decimal places before returning it."}
{"task_id": 101, "completion_id": 0, "completion": "To implement the softmax function using NumPy, we need to ensure numerical stability by subtracting the maximum value from each row along the specified axis. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n    \n    # Subtract the max value along the specified axis\n    max_x = np.max(x, axis=axis, keepdims=True)\n    shifted_x = x - max_x\n    \n    # Calculate the exponentials\n    exp_x = np.exp(shifted_x)\n    \n    # Sum the exponentials along the specified axis\n    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n    \n    # Compute the softmax\n    softmax_probs = exp_x / sum_exp_x\n    \n    # Round to 4 decimal places and convert to a list\n    softmax_probs_rounded = [round(num, 4) for sublist in softmax_probs.tolist() for num in sublist]\n    \n    return softmax_probs_rounded\n\n# Example usage:\nx = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nresult = softmax(x)\nprint(result)\n```\n\n### Explanation:\n1. **Input Validation**: We first check if the input array `x` has more than one dimension. If it does not, we raise a `ValueError`.\n2. **Max Value Calculation**: We find the maximum value along the specified axis using `np.max`. This helps in stabilizing the computation of the exponential.\n3. **Shifted Exponentials**: We subtract the maximum value from each element in the array to shift the data range and prevent overflow when computing the exponential.\n4. **Exponential Calculation**: We compute the exponential of the shifted values.\n5. **Summation**: We sum the exponentials along the same axis to get the normalization factor.\n6. **Softmax Calculation**: We divide the exponentials by their sum to get the softmax probabilities.\n7. **Rounding and Conversion**: We round each probability to four decimal places and convert the resulting NumPy array to a list of lists using `tolist()`.\n\nThis function ensures that the softmax operation is performed correctly and efficiently using NumPy, and the results are formatted as required."}
{"task_id": 102, "completion_id": 0, "completion": "Here's the implementation of the `polynomial_regression_predict` function using NumPy:\n\n```python\nimport numpy as np\n\ndef polynomial_regression_predict(x: list[float], y: list[float], degree: int, x_pred: list[float]) -> list[float]:\n    \"\"\"Fit a polynomial regression model and return predictions.\n\n    Parameters\n    ----------\n    x : list[float]\n        Training input values.\n    y : list[float]\n        Training target values.\n    degree : int\n        Degree of the polynomial to be fitted (must be >= 0).\n    x_pred : list[float]\n        Values at which the fitted model should be evaluated.\n\n    Returns\n    -------\n    list[float]\n        Predicted values for *x_pred* rounded to 4 decimal places, or -1 if the\n        model cannot be fitted (e.g. not enough data, inconsistent input).\n    \"\"\"\n    # Check if the input sizes are consistent\n    if len(x) != len(y):\n        return -1\n    \n    # Check if the degree is non-negative\n    if degree < 0:\n        return -1\n    \n    # Check if there are enough training samples\n    if len(x) < degree + 1:\n        return -1\n    \n    # Create the design matrix X\n    X = np.vander(x, degree + 1)\n    \n    # Use least squares to find the coefficients\n    try:\n        coefficients = np.linalg.lstsq(X, y, rcond=None)[0]\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Compute predictions for x_pred\n    predictions = np.dot(np.vander(x_pred, degree + 1), coefficients).round(4)\n    \n    return predictions.tolist()\n\n# Example usage:\n# x = [1, 2, 3, 4]\n# y = [2, 4, 6, 8]\n# degree = 2\n# x_pred = [5, 6]\n# print(polynomial_regression_predict(x, y, degree, x_pred))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the lengths of `x` and `y` are equal, if the degree is non-negative, and if there are enough training samples (`len(x) >= degree + 1`). If any of these conditions are not met, it returns `-1`.\n\n2. **Design Matrix Construction**: The `np.vander` function is used to create the design matrix `X`, which includes all powers of `x` from 0 up to the specified degree.\n\n3. **Least Squares Fitting**: The `np.linalg.lstsq` function is used to find the coefficients of the polynomial that minimize the squared error between the predicted and actual values.\n\n4. **Prediction**: For each value in `x_pred`, the function computes the corresponding prediction using the designed matrix and the found coefficients, then rounds the result to 4 decimal places.\n\n5. **Return**: The function returns the list of predictions rounded to 4 decimal places. If the model cannot be fitted due to invalid input, it returns `-1`."}
{"task_id": 104, "completion_id": 0, "completion": "To compute the information gain, we need to follow these steps:\n\n1. Calculate the entropy \\( H(y) \\) of the parent node.\n2. For each child node, calculate its entropy \\( H(S_k) \\).\n3. Compute the weighted average of the entropies of the child nodes based on their sizes relative to the parent node.\n4. Subtract this weighted average from the entropy of the parent node to get the information gain.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\nimport math\n\ndef information_gain(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Computes the information gain of a proposed split.\n\n    Args:\n        y: 1-D NumPy array containing the class labels of the parent node.\n        splits: List of 1-D NumPy arrays. Every array contains the labels that\n            fall into one child node produced by the split.\n\n    Returns:\n        Information gain rounded to 4 decimal places.\n    \"\"\"\n    # Step 1: Calculate the total number of samples\n    n = len(y)\n    \n    # Step 2: Calculate the entropy of the parent node\n    def entropy(p):\n        if p == 0 or p == 1:\n            return 0\n        return -p * math.log2(p) - (1 - p) * math.log2(1 - p)\n    \n    p_y = np.bincount(y) / n\n    H_y = sum(p * entropy(p) for p in p_y)\n    \n    # Step 3: Calculate the weighted average of the entropies of the child nodes\n    weighted_entropy = 0\n    for split in splits:\n        p_split = np.bincount(split) / len(split)\n        H_split = sum(p * entropy(p) for p in p_split)\n        weighted_entropy += len(split) / n * H_split\n    \n    # Step 4: Calculate the information gain\n    info_gain = H_y - weighted_entropy\n    \n    # Return the result rounded to 4 decimal places\n    return round(info_gain, 4)\n\n# Example usage:\ny = np.array([0, 0, 1, 1, 1, 0])\nsplits = [np.array([0, 0]), np.array([1, 1, 1])]\nprint(information_gain(y, splits))  # Output should be calculated based on the example input\n```\n\n### Explanation:\n- **Entropy Calculation**: The entropy \\( H(p) \\) is calculated using the formula \\( -p \\log_2(p) - (1-p) \\log_2(1-p) \\). This function handles the case when \\( p = 0 \\) or \\( p = 1 \\) separately to avoid division by zero.\n- **Weighted Average Entropy**: For each child node, we calculate its entropy and then weight it by the fraction of the parent node's size that it represents.\n- **Information Gain**: Finally, the information gain is computed by subtracting the weighted average entropy from the entropy of the parent node. The result is rounded to four decimal places before being returned."}
{"task_id": 105, "completion_id": 0, "completion": "Here's the implementation of the Gaussian Probability Density Function (PDF) according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    # Check if sigma is strictly positive\n    if sigma <= 0:\n        return -1\n    \n    # Convert inputs to numpy array for vectorized operations\n    x_array = np.array(x)\n    \n    # Calculate the PDF using the formula\n    pdf_values = (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-((x_array - mean) ** 2) / (2 * sigma ** 2))\n    \n    # Round each result to 5 decimal places\n    rounded_pdf_values = np.round(pdf_values, 5).tolist()\n    \n    return rounded_pdf_values\n\n# Example usage:\n# print(gaussian_pdf([1, 2, 3], 0, 1))\n# print(gaussian_pdf([1, 2, 3], 0, 0))\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if `sigma` is less than or equal to zero. If so, it returns `-1`.\n- **Data Handling**: The input `x` is converted to a NumPy array to enable efficient vectorized operations.\n- **PDF Calculation**: The PDF is computed using the formula \\( f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\cdot e^{-\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} \\).\n- **Rounding**: The resulting PDF values are rounded to five decimal places using NumPy's rounding function.\n- **Output**: The function returns a list of these rounded values. If the input was empty, the function will return an empty list."}
{"task_id": 107, "completion_id": 0, "completion": "Here's how you can implement the Adam optimization step using the provided guidelines:\n\n```python\nimport numpy as np\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\n\n    Parameters\n    ----------\n    theta : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to *theta*.\n    m_prev : np.ndarray\n        First moment vector from the previous time step.\n    v_prev : np.ndarray\n        Second moment vector from the previous time step.\n    t : int\n        Current time-step counter (starting at 1).\n    alpha : float, optional\n        Learning rate. Default is 0.01.\n    beta1 : float, optional\n        Exponential decay rate for the first moment. Default is 0.9.\n    beta2 : float, optional\n        Exponential decay rate for the second moment. Default is 0.99.\n    epsilon : float, optional\n        Small constant for numerical stability. Default is 1e-9.\n\n    Returns\n    -------\n    tuple[list, list, list, int]\n        A tuple containing the updated parameters, the new first and\n        second moment estimates, and the incremented time step. All\n        numerical values are rounded to 6 decimal places and converted\n        to regular Python lists.\n    \"\"\"\n    # Update first and second moments\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * grad ** 2\n    \n    # Bias correction\n    m_hat_t = m_t / (1 - beta1 ** t)\n    v_hat_t = v_t / (1 - beta2 ** t)\n    \n    # Update parameters\n    theta_new = theta - alpha * m_hat_t / (np.sqrt(v_hat_t) + epsilon)\n    \n    # Return results as lists rounded to 6 decimal places\n    return [\n        [round(val, 6) for val in theta_new],\n        [round(val, 6) for val in m_hat_t],\n        [round(val, 6) for val in v_hat_t],\n        t + 1\n    ]\n\n# Example usage:\ntheta = np.array([1.0, 2.0])\ngrad = np.array([-0.1, -0.2])\nm_prev = np.array([0.5, 0.6])\nv_prev = np.array([0.3, 0.4])\nt = 1\n\nupdated_theta, updated_m, updated_v, next_t = adam_step(theta, grad, m_prev, v_prev, t)\nprint(\"Updated Parameters:\", updated_theta)\nprint(\"Updated First Moment Estimates:\", updated_m)\nprint(\"Updated Second Moment Estimates:\", updated_v)\nprint(\"Next Time Step:\", next_t)\n```\n\nThis implementation follows the Adam optimization algorithm's steps, including updating the first and second moments, applying bias correction, and computing the parameter update. The results are returned as lists with each value rounded to six decimal places."}
{"task_id": 108, "completion_id": 0, "completion": "To implement the regularized Alternating Least Squares (ALS) algorithm, we need to follow these steps:\n\n1. Initialize the factor matrices \\( W \\) and \\( H \\) randomly with a fixed seed.\n2. Alternate between updating \\( W \\) and \\( H \\) while keeping the other fixed.\n3. Use a regularised least-squares approach to solve for each factor matrix.\n4. Stop the iteration process when the change in the regularised loss falls below a specified tolerance or when the maximum number of iterations is reached.\n5. Return the reconstructed matrix \\( X \\hat{} = WH \\).\n\nHere's the implementation of the `als_factorization` function:\n\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    N, M = X.shape\n    \n    # Initialize W and H with random values\n    np.random.seed(0)\n    W = np.random.rand(N, K)\n    H = np.random.rand(K, M)\n    \n    # Compute initial loss\n    loss = np.linalg.norm(X - np.dot(W, H))**2 / N + alpha * (np.linalg.norm(W)**2 + np.linalg.norm(H)**2)\n    \n    # List to store the loss history\n    loss_history = [loss]\n    \n    for _ in range(max_iter):\n        # Update W keeping H fixed\n        H_T = H.T\n        W_new = np.linalg.solve(np.dot(H_T, H) + alpha * np.eye(K), np.dot(H_T, X))\n        \n        # Update H keeping W fixed\n        W_T = W.T\n        H_new = np.linalg.solve(np.dot(W, W_T) + alpha * np.eye(M), np.dot(W, X.T))\n        \n        # Update W and H\n        W = W_new\n        H = H_new\n        \n        # Compute new loss\n        new_loss = np.linalg.norm(X - np.dot(W, H))**2 / N + alpha * (np.linalg.norm(W)**2 + np.linalg.norm(H)**2)\n        \n        # Check if convergence criterion is met\n        if np.abs(new_loss - loss) < tol:\n            break\n        \n        # Store the loss\n        loss_history.append(new_loss)\n        \n        # Print progress\n        print(f\"Iteration {_+1}: Loss = {new_loss:.6f}\")\n    \n    # Reconstruct the matrix\n    X_hat = np.round(np.dot(W, H), 4)\n    \n    return X_hat.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# K = 2\n# result = als_factorization(X, K)\n# print(result)\n```\n\n### Explanation:\n- **Initialization**: We initialize \\( W \\) and \\( H \\) with random values using NumPy's random module. A fixed seed ensures reproducibility.\n- **Loss Calculation**: The initial loss is calculated using the Frobenius norm and the regularization term.\n- **Alternating Updates**: For each iteration, we update either \\( W \\) or \\( H \\) while keeping the other fixed. This is done using the normal equations with added regularization terms.\n- **Convergence Check**: We check if the change in the loss is less than the specified tolerance. If it is, we stop the iterations.\n- **Reconstruction**: After convergence, we reconstruct the matrix \\( X \\) using the final \\( W \\) and \\( H \\) matrices and round it to four decimal places.\n\nThis implementation should work efficiently for most practical purposes, given the constraints."}
{"task_id": 109, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm with K-Means++ initialization:\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    m, n = X.shape\n    if K >= m:\n        raise ValueError(\"Number of clusters K cannot be greater than or equal to the number of samples m.\")\n\n    # Step 1: Initialize the first centroid randomly\n    initial_centroids = [X[np.random.choice(m)]]\n\n    # Step 2: Select remaining centroids using K-Means++\n    while len(initial_centroids) < K:\n        distances_squared = np.sum((X - initial_centroids[-1])**2, axis=1)\n        probabilities = distances_squared / distances_squared.sum()\n        cumulative_probabilities = np.cumsum(probabilities)\n        u = random.uniform(0, cumulative_probabilities[-1])\n        index = np.searchsorted(cumulative_probabilities, u)\n        new_centroid = X[index]\n        initial_centroids.append(new_centroid)\n\n    # Convert centroids to lists of floats and sort them\n    centroids = [list(round(x, 4)) for x in initial_centroids]\n    centroids.sort(key=lambda x: (x[0], x[1]))\n\n    return centroids\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([\n        [1, 2],\n        [2, 3],\n        [3, 4],\n        [5, 6],\n        [6, 7],\n        [8, 9]\n    ])\n    K = 2\n    random_state = 42\n    centroids = kmeans(X, K, random_state=random_state)\n    print(centroids)\n```\n\n### Explanation:\n1. **Initialization**:\n   - The first centroid is chosen randomly from the data points.\n   - Subsequent centroids are selected based on the probability distribution defined by the squared Euclidean distance from the previously chosen centroids.\n\n2. **Lloyd's Iteration**:\n   - Assign each sample to the closest centroid.\n   - Update each centroid to be the mean of all samples assigned to it.\n   - Repeat until convergence or the maximum number of iterations is reached.\n\n3. **Output**:\n   - The centroids are converted to lists of floats and sorted by their first coordinate. In case of ties, they are further sorted by the second coordinate.\n\nThis implementation ensures reproducibility by setting the random state for both Python's `random` module and NumPy's random number generator."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encodes every character of the input string using a provided vocabulary.\n    \n    Parameters:\n    - text (str): The input string to encode.\n    - time_steps (int): The fixed length of the encoded list.\n    - vocabulary (dict[str, int]): A mapping from characters to their corresponding integer IDs.\n    \n    Returns:\n    - List[int]: A list of integers representing the encoded text.\n    \"\"\"\n    # Convert each character to its corresponding ID in the vocabulary\n    encoded_ids = [vocabulary.get(char, 0) for char in text]\n    \n    # Pad the encoded list to the specified length with zeros\n    padded_encoded_ids = encoded_ids + [0] * (time_steps - len(encoded_ids))\n    \n    return padded_encoded_ids[:time_steps]\n\ndef int_to_string(indices: list[int], inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Converts a list/array of integer IDs back to text using an inverse vocabulary.\n    \n    Parameters:\n    - indices (list[int]): A list of integer IDs to decode.\n    - inverse_vocab (dict[int, str]): A mapping from integer IDs to their corresponding characters.\n    \n    Returns:\n    - str: The decoded text.\n    \"\"\"\n    # Filter out padding tokens (ID 0) and convert the remaining IDs to characters\n    decoded_chars = [inverse_vocab[id] for id in indices if id != 0]\n    \n    return ''.join(decoded_chars)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    Encodes the input text, runs the model, and decodes the output back to text.\n    \n    Parameters:\n    - model: The trained model used for prediction.\n    - input_vocabulary: A dictionary mapping characters to their corresponding IDs.\n    - inv_output_vocabulary: A dictionary mapping IDs to their corresponding characters.\n    - text: The input text to predict.\n    \n    Returns:\n    - str: The decoded prediction string.\n    \"\"\"\n    # Encode the input text\n    encoded_text = string_to_int(text, TIME_STEPS, input_vocabulary)\n    \n    # Reshape the encoded text to a batch of shape (1, TIME_STEPS)\n    input_batch = np.array([encoded_text]).reshape(1, TIME_STEPS)\n    \n    # Run the model prediction\n    predictions = model.predict(input_batch)\n    \n    # Apply argmax over the last axis to get the predicted IDs\n    predicted_indices = np.argmax(predictions, axis=-1)[0]\n    \n    # Decode the predicted IDs back to text\n    decoded_prediction = int_to_string(predicted_indices, inv_output_vocabulary)\n    \n    return decoded_prediction\n```\n\nThis solution includes the implementation of the required helper functions and demonstrates how they can be used together to process an input text through a model and produce a decoded prediction string."}
{"task_id": 112, "completion_id": 0, "completion": "To implement the `tokenize_and_count` function, we need to follow these steps:\n\n1. Use regular expressions to extract all contiguous sequences of English letters from the input text. This will help us identify words.\n2. Convert all extracted words to lowercase to ensure case insensitivity.\n3. Count the frequency of each word using the `Counter` class from the `collections` module.\n4. Create a list of `Token` objects from the counted words, ensuring each token has a lowercase word and a count.\n5. Sort the list of tokens based on the specified criteria: first by frequency in descending order, and then lexicographically if frequencies are equal.\n6. Return the sorted list of tokens.\n\nHere's the implementation of the function:\n\n```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    \n    # Extract words using regex\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    \n    # Convert words to lowercase and count their frequencies\n    word_counts = Counter(words.lower())\n    \n    # Create a list of Token objects\n    tokens = [Token(word=word, count=count) for word, count in word_counts.items()]\n    \n    # Sort the tokens by frequency (descending) and then lexicographically\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    \n    return tokens\n\n# Example usage:\ntext = \"The cat sat on the mat. The cat was not happy.\"\ntokens = tokenize_and_count(text)\nfor token in tokens:\n    print(token)\n```\n\n### Explanation:\n\n- **Regular Expression**: `\\b[a-zA-Z]+\\b` is used to match contiguous sequences of English letters. The `\\b` denotes a word boundary, ensuring that we capture whole words.\n  \n- **Lowercase Conversion**: All words are converted to lowercase to ensure case insensitivity.\n\n- **Counting**: The `Counter` object from the `collections` module is used to count the occurrences of each word.\n\n- **Sorting**: The list comprehension creates a list of `Token` objects. The `sort` method sorts the list first by `count` in descending order (`-x.count`) and then by `word` in ascending order (`x.word`).\n\nThis implementation ensures that the tokens are correctly identified, counted, and sorted according to the specified rules."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport random\n\n# Global constant for example strings\nEXAMPLES = [\n    \"hello\",\n    \"world\",\n    \"python\",\n    \"example\",\n    \"test\"\n]\n\ndef run_example(model, input_vocabulary, inv_output_vocabulary, example):\n    \"\"\"Helper function to run a single example through the model.\"\"\"\n    # Convert the example string to a list of integers using the input vocabulary\n    input_ids = [input_vocabulary[char] for char in example]\n    \n    # Run the model on the input IDs\n    output_ids = model(input_ids)\n    \n    # Convert the output IDs back to a string using the inverse output vocabulary\n    predicted_string = ''.join(inv_output_vocabulary[id] for id in output_ids)\n    \n    return predicted_string\n\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    \n    predictions = []\n    for example in examples:\n        predicted_string = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_string}\\n\")\n        predictions.append(predicted_string)\n    \n    return predictions\n\n# Example usage:\n# Define a simple model that always predicts the same output as input\ndef simple_model(input_ids):\n    return input_ids\n\n# Define input and output vocabularies\ninput_vocab = {'h': 0, 'e': 1, 'l': 2, 'o': 3}\ninv_output_vocab = {0: 'h', 1: 'e', 2: 'l', 3: 'o'}\n\n# Run examples\npredictions = run_examples(simple_model, input_vocab, inv_output_vocab)\n\n# Print predictions\nfor i, prediction in enumerate(predictions):\n    print(f\"Prediction {i+1}: {prediction}\")\n```\n\nThis code defines the `run_examples` function, which runs a given model on a set of input examples and prints the results. It also includes a simple example model that always predicts the input string as output. The input and output vocabularies are defined to demonstrate how the model can process and interpret character sequences."}
{"task_id": 114, "completion_id": 0, "completion": "To implement the Scaled Exponential Linear Unit (SELU) activation function in Python, we need to define the function according to the provided mathematical formulas. The function will handle both the activation and its derivative based on the `derivative` flag. Here's how you can implement it:\n\n```python\nimport numpy as np\n\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    # Define the constants\n    alpha = 1.6732632423543772848170429916717\n    lambda_ = 1.0507009873554804934193349852946\n    \n    # Check if the input is a scalar\n    if x.ndim == 0:\n        if derivative:\n            return [lambda_ if x >= 0 else alpha * np.exp(x)]\n        else:\n            return [lambda_ * max(0, x) + alpha * (np.exp(x) - 1) if x < 0 else lambda_ * x]\n    \n    # Handle multi-dimensional arrays\n    result = []\n    for subarray in x:\n        if derivative:\n            result.append([lambda_ if elem >= 0 else alpha * np.exp(elem)] for elem in subarray)\n        else:\n            result.append([lambda_ * max(0, elem) + alpha * (np.exp(elem) - 1) if elem < 0 else lambda_ * elem] for elem in subarray)\n    \n    return result.tolist()\n\n# Example usage:\nx = np.array([[1.0, -0.5], [-2.0, 0.0]])\nprint(selu(x))  # Activation mode\nprint(selu(x, derivative=True))  # Derivative mode\n```\n\n### Explanation:\n- **Constants**: We define the constants `\\(\\alpha\\) and \\(\\lambda\\)` as specified in the problem statement.\n- **Scalar Input**: For a scalar input, we apply the appropriate formula and round the result to six decimal places.\n- **Multi-Dimensional Input**: For a multi-dimensional input, we iterate over each subarray and apply the same logic, preserving the original shape.\n- **Rounding**: We use NumPy's `round` function to ensure the results are rounded to six decimal places before converting them to a list.\n\nThis implementation ensures that the function behaves correctly for both activation and derivative modes and handles both scalar and multi-dimensional inputs appropriately."}
{"task_id": 115, "completion_id": 0, "completion": "To solve this problem, we need to implement the binary cross-entropy loss and its gradient for logistic regression. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # Ensure y is in the correct shape\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n\n    # Calculate z = X @ w\n    z = np.dot(X, w)\n\n    # Apply sigmoid function to get probabilities\n    p = 1 / (1 + np.exp(-z))\n\n    # Replace probabilities close to 0 or 1 with a small constant to avoid log(0) or log(1)\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n\n    # Calculate the loss\n    loss = -1 / len(y) * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    # Calculate the gradient\n    gradient = 1 / len(y) * np.dot(X.T, (p - y))\n    \n    # Round the results to 4 decimal places\n    loss_rounded = round(loss, 4)\n    gradient_rounded = gradient.round(4).tolist()\n    \n    return loss_rounded, gradient_rounded\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 1])\nw = np.array([0.5, 0.5])\n\nloss, grad = logistic_loss_and_gradient(X, y, w)\nprint(\"Loss:\", loss)\nprint(\"Gradient:\", grad)\n```\n\n### Explanation:\n1. **Input Handling**: We ensure that `y` is a column vector by reshaping it if necessary.\n2. **Z Calculation**: Compute `z` using the dot product of `X` and `w`.\n3. **Probability Calculation**: Use the sigmoid function to compute the probability `p`.\n4. **Clip Probabilities**: Clip values close to 0 or 1 to avoid numerical issues when taking logs.\n5. **Loss Calculation**: Compute the binary cross-entropy loss using the formula provided.\n6. **Gradient Calculation**: Compute the gradient using the formula provided.\n7. **Rounding**: Round the loss and gradient to 4 decimal places and convert the gradient to a nested list using `tolist()`.\n\nThis implementation should work correctly for the given task."}
{"task_id": 116, "completion_id": 0, "completion": "To implement the `mse_criterion` function, we need to follow these steps:\n\n1. Calculate the parent mean-squared error (MSE).\n2. Calculate the weighted mean-squared error for each child node.\n3. Compute the total weighted child MSE.\n4. Subtract the total weighted child MSE from the parent MSE to get the reduction in MSE.\n5. Round the result to four decimal places and return it.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    # Step 1: Calculate the parent mean-squared error (MSE)\n    mse_parent = np.mean((y - np.mean(y)) ** 2)\n    \n    # Step 2: Calculate the weighted mean-squared error for each child node\n    total_weighted_mse = 0\n    for split in splits:\n        # Number of samples in the current split\n        num_samples_split = len(split)\n        # Weight of the current split\n        weight = num_samples_split / len(y)\n        # Calculate the mean of the current split\n        mean_split = np.mean(split)\n        # Calculate the weighted MSE for the current split\n        weighted_mse_split = weight * np.mean((split - mean_split) ** 2)\n        # Add to the total weighted MSE\n        total_weighted_mse += weighted_mse_split\n    \n    # Step 3: Compute the total weighted child MSE\n    # This is already calculated in the loop above\n    \n    # Step 4: Subtract the total weighted child MSE from the parent MSE\n    mse_reduction = mse_parent - total_weighted_mse\n    \n    # Step 5: Round the result to four decimal places\n    mse_reduction_rounded = round(mse_reduction, 4)\n    \n    return mse_reduction_rounded\n\n# Example usage:\ny = np.array([1, 2, 3, 4, 5])\nsplits = [np.array([1, 2]), np.array([3, 4, 5])]\nprint(mse_criterion(y, splits))  # Output will be the reduction in MSE\n```\n\nThis function calculates the mean-squared error reduction by comparing the parent node's MSE with the weighted MSE of its child nodes. The reduction is then returned rounded to four decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using decision stumps:\n\n```python\nimport numpy as np\n\nclass DecisionStump:\n    def __init__(self):\n        self.feature_index = None\n        self.threshold = None\n        self.sign = None\n\n    def fit(self, X, y):\n        min_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_sign = None\n\n        for feature_index in range(X.shape[1]):\n            for threshold in np.unique(X[:, feature_index]):\n                for sign in [-1, 1]:\n                    predictions = np.where(X[:, feature_index] >= threshold, sign, -sign)\n                    error = np.mean(predictions != y)\n\n                    if error < min_error:\n                        min_error = error\n                        best_feature = feature_index\n                        best_threshold = threshold\n                        best_sign = sign\n\n        self.feature_index = best_feature\n        self.threshold = best_threshold\n        self.sign = best_sign\n\n    def predict(self, X):\n        return np.where(X[:, self.feature_index] >= self.threshold, self.sign, -self.sign)\n\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n\n    weights = np.ones(len(y_train)) / len(y_train)\n    alpha = []\n    predictions = []\n\n    for _ in range(n_clf):\n        stump = DecisionStump()\n        stump.fit(X_train, y_train * weights)\n        predictions.append(stump.predict(X_train))\n\n        errors = (stump.predict(X_train) != y_train).astype(int) * weights\n        epsilon = np.sum(errors)\n        alpha_t = 0.5 * np.log((1 - epsilon) / epsilon)\n        alpha.append(alpha_t)\n\n        weights *= np.exp(-alpha_t * (y_train * predictions[-1]))\n        weights /= np.sum(weights)\n\n    # Aggregate all stump votes to get final prediction\n    aggregated_predictions = np.sign(np.sum([a * p for a, p in zip(alpha, predictions)], axis=0))\n    return aggregated_predictions.tolist()\n\n# Example usage:\n# X_train = ...\n# y_train = ...\n# X_test = ...\n# print(adaboost_predict(X_train, y_train, X_test, n_clf=5))\n```\n\n### Explanation:\n1. **DecisionStump Class**: This class represents a simple decision stump, which makes decisions based on a single feature and a threshold. It can classify samples into two classes (-1 or 1).\n\n2. **fit Method**: The `fit` method finds the best feature and threshold to split the data such that the resulting partition minimizes the weighted classification error.\n\n3. **predict Method**: The `predict` method uses the learned feature index and threshold to make predictions for new data points.\n\n4. **adaboost_predict Function**: This function trains the AdaBoost algorithm using the provided training data. It initializes weights, fits decision stumps, calculates their individual weights, updates the weights based on misclassifications, and aggregates the results to produce the final predictions for the test set."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    The function receives a list of parameters and a list of corresponding\n    gradients and must return the updated parameters after applying the SGD\n    rule using the supplied learning-rate `alpha`.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Gradients for each parameter.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        Updated parameters converted to Python lists, each rounded to four\n        decimal places.\n    \"\"\"\n    # Check if inputs are valid\n    if not params or not grads or len(params) != len(grads):\n        return []\n    \n    # Perform SGD update\n    updated_params = [\n        np.round(param - alpha * grad, 4)\n        for param, grad in zip(params, grads)\n    ]\n    \n    return updated_params\n```\n\nThis implementation defines the `sgd_update` function, which takes a list of parameters and their corresponding gradients, along with an optional learning rate. It checks if the inputs are valid and then applies the stochastic gradient descent update formula to each parameter. The updated parameters are returned as a list of rounded floats."}
{"task_id": 120, "completion_id": 0, "completion": "To implement a simple Linear Regression learner using batch gradient descent in Python, we can follow the outlined steps. Below is the complete implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef linear_regression(X: list[list[int | float]],\n                      y: list[int | float],\n                      n_iterations: int,\n                      learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\n\n    Args:\n        X: Training samples \u2013 list of lists with numerical feature values.\n        y: Target values \u2013 list of numbers, same length as X.\n        n_iterations: Number of gradient-descent steps to perform.\n        learning_rate: Positive learning rate controlling the step size.\n\n    Returns:\n        The learned weight vector as a list (bias first) rounded to 4 decimals.\n        If the input dimensions are incompatible the function must return -1.\n    \"\"\"\n    # Check if the shapes of X and y are compatible\n    if len(X) != len(y):\n        return -1\n    \n    # Convert inputs to numpy arrays for efficient computation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the bias term\n    X = np.column_stack((np.ones(len(X)), X))\n    \n    # Initialize the weight vector randomly from [-1/sqrt(N), 1/sqrt(N)]\n    num_features = X.shape[1]\n    w = np.random.uniform(-1 / math.sqrt(num_features), 1 / math.sqrt(num_features), size=(num_features,))\n    \n    # Perform batch gradient descent\n    for _ in range(n_iterations):\n        # Compute the predicted values\n        predictions = np.dot(X, w)\n        \n        # Calculate the gradient\n        gradient = np.dot(X.T, (predictions - y)) / len(y)\n        \n        # Update the weights\n        w -= learning_rate * gradient\n    \n    # Round the weights to 4 decimal places and convert to a regular Python list\n    rounded_weights = [round(w_val, 4) for w_val in w.tolist()]\n    \n    return rounded_weights\n\n# Example usage:\n# X = [[1, 2], [3, 4], [5, 6]]\n# y = [7, 8, 9]\n# n_iterations = 100\n# learning_rate = 0.01\n# print(linear_regression(X, y, n_iterations, learning_rate))\n```\n\n### Explanation:\n1. **Input Validation**: We first check if the lengths of `X` and `y` are equal. If not, we return `-1` to indicate incompatible input dimensions.\n\n2. **Data Preparation**:\n   - Convert `X` and `y` to NumPy arrays for efficient computation.\n   - Add a column of ones to `X` to include the bias term in the model.\n\n3. **Weight Initialization**:\n   - Generate random initial weights `w` from a uniform distribution within the interval \\([-1/\\sqrt{N}, 1/\\sqrt{N}]\\), where \\(N\\) is the number of features.\n\n4. **Batch Gradient Descent**:\n   - For each iteration, compute the predicted values using the current weights.\n   - Calculate the gradient of the loss function with respect to the weights.\n   - Update the weights using the gradient descent formula.\n\n5. **Output**:\n   - After completing the specified number of iterations, round the weights to four decimal places and return them as a regular Python list, with the bias weight first."}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    X_array = np.array(X)\n    y_array = np.array(y)\n    \n    # Augment X with a leading column of ones\n    X_augmented = np.hstack((np.ones((X_array.shape[0], 1)), X_array))\n    \n    # Calculate the optimal weights using the Moore-Penrose pseudo-inverse\n    w_optimal = np.linalg.pinv(X_augmented).dot(y_array)\n    \n    # Round every weight to the nearest 4th decimal place\n    w_rounded = [round(weight, 4) for weight in w_optimal]\n    \n    return w_rounded\n```\n\nThis code defines the `linear_regression_weights` function, which performs the required steps to compute the ordinary least-squares weight vector for a given dataset. The function first converts the input lists to NumPy arrays, then augments the feature matrix `X` with a column of ones to include the intercept term. It calculates the optimal weights using the Moore-Penrose pseudo-inverse, rounds each weight to four decimal places, and returns the result as a flat Python list."}
{"task_id": 123, "completion_id": 0, "completion": "To implement the `one_hot_encoding` function without using any external libraries, we can follow these steps:\n\n1. Convert the input array `y` to a set to find the unique categories.\n2. Sort the unique categories alphabetically or numerically based on their type.\n3. Create a list of zeros for each category, which will represent the one-hot encoded vectors.\n4. Iterate over the input array `y`, and for each element, mark the corresponding position in the one-hot encoded vector with a 1.\n\nHere's the implementation of the function:\n\n```python\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\n\n    You should detect the unique categories in *y*, sort them in ascending/\n    alphabetical order, and then build a binary matrix where each row\n    corresponds to an element of *y* and each column corresponds to one of\n    the unique categories.  A cell must contain **1** if the row\u2019s element\n    belongs to the column\u2019s category and **0** otherwise.\n\n    Args:\n        y: A one-dimensional NumPy array (or array-like object) containing\n           categorical values.\n\n    Returns:\n        A Python list of lists with 0/1 integers representing the one-hot\n        encoded matrix.\n    \"\"\"\n    # Step 1: Find unique categories\n    unique_categories = set(y)\n    \n    # Step 2: Sort unique categories\n    if isinstance(next(iter(unique_categories)), str):\n        # If categories are strings, sort alphabetically\n        unique_categories = sorted(unique_categories)\n    else:\n        # If categories are numbers, sort numerically\n        unique_categories.sort()\n    \n    # Step 3: Initialize the one-hot encoded matrix\n    num_samples = len(y)\n    num_categories = len(unique_categories)\n    one_hot_encoded_matrix = [[0] * num_categories for _ in range(num_samples)]\n    \n    # Step 4: Fill the one-hot encoded matrix\n    for i, value in enumerate(y):\n        category_index = unique_categories.index(value)\n        one_hot_encoded_matrix[i][category_index] = 1\n    \n    return one_hot_encoded_matrix\n\n# Example usage:\n# y = np.array(['apple', 'banana', 'apple', 'orange', 'banana'])\n# print(one_hot_encoding(y))\n```\n\nThis function first determines the unique categories from the input array `y`. It then sorts these categories appropriately based on whether they are strings or numbers. After preparing the one-hot encoded matrix, it fills it by marking the appropriate positions for each category in the input array. Finally, it returns the resulting one-hot encoded matrix as a list of lists."}
{"task_id": 126, "completion_id": 0, "completion": "Here's the implementation of the polynomial kernel function:\n\n```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    X = np.array(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.array(Y)\n\n    # Determine the scaling factor gamma\n    if gamma is None:\n        gamma = 1 / X.shape[1]\n\n    # Compute the Gram matrix\n    gram_matrix = ((gamma * np.dot(X, Y.T)) + c0) ** d\n\n    # Round the result to 4 decimal places and convert back to a list of lists\n    return gram_matrix.round(4).tolist()\n\n# Example usage:\nX = [[1, 2], [3, 4]]\nY = [[5, 6], [7, 8]]\nresult = polynomial_kernel(X, Y, d=2, gamma=0.5, c0=0)\nprint(result)\n```\n\n### Explanation:\n- **Input Handling**: The function accepts two data matrices `X` and `Y`, both represented as lists of lists. If `Y` is not provided, it defaults to `X`.\n- **Scaling Factor**: The scaling factor `gamma` is determined based on the number of features (`C`). If `gamma` is `None`, it uses `1/C`.\n- **Gram Matrix Calculation**: The polynomial kernel is computed using the formula \\((\\gamma \\cdot x^T y + c_0)^d\\). This involves dot products and exponentiation.\n- **Rounding**: The resulting Gram matrix is rounded to four decimal places using NumPy's `round()` method.\n- **Output**: The final result is converted back to a list of lists using `tolist()` before being returned.\n\nThis implementation efficiently computes the polynomial kernel and handles various edge cases, including when `Y` is the same as `X`."}
{"task_id": 127, "completion_id": 0, "completion": "To implement the confusion matrix, we need to follow these steps:\n\n1. Check if the lengths of `y_true` and `y_pred` are equal. If not, return `-1`.\n2. Determine the number of distinct classes (`C`) by finding the unique elements in both lists and sorting them.\n3. Initialize a zero matrix of size `C x C`.\n4. Populate the matrix by counting occurrences of each true-predicted pair.\n5. Return the resulting confusion matrix.\n\nHere's the implementation:\n\n```python\nfrom typing import List\n\ndef confusion_matrix(y_true: List[int], y_pred: List[int]) -> List[List[int]]:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return [-1]\n\n    # Find unique classes and sort them\n    unique_classes = sorted(set(y_true + y_pred))\n\n    # Initialize the confusion matrix\n    C = len(unique_classes)\n    confusion_matrix = [[0] * C for _ in range(C)]\n\n    # Populate the confusion matrix\n    for true_label, pred_label in zip(y_true, y_pred):\n        true_index = unique_classes.index(true_label)\n        pred_index = unique_classes.index(pred_label)\n        confusion_matrix[true_index][pred_index] += 1\n\n    return confusion_matrix\n\n# Example usage:\ny_true = [1, 2, 2, 3, 4]\ny_pred = [2, 2, 3, 4, 5]\nprint(confusion_matrix(y_true, y_pred))\n```\n\n### Explanation:\n\n- **Length Check**: We first check if the lengths of `y_true` and `y_pred` are equal. If they are not, we return `-1` because the evaluation cannot proceed.\n- **Unique Classes**: We find the unique classes by combining both lists and using `set` to remove duplicates. We then sort these unique classes to ensure they are ordered consistently.\n- **Matrix Initialization**: We initialize a 2D list `confusion_matrix` with zeros, where the dimensions are determined by the number of unique classes.\n- **Populating the Matrix**: We iterate over the pairs of true and predicted labels, convert them to their respective indices in the sorted list of unique classes, and increment the corresponding cell in the confusion matrix.\n- **Return the Result**: Finally, we return the populated confusion matrix."}
{"task_id": 128, "completion_id": 0, "completion": "To implement the multi-class Linear Discriminant Analysis (LDA), we need to follow the steps outlined in the task description. Below is the complete implementation of the `multi_class_lda` function:\n\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components < 1 or n_components > X.shape[1]:\n        raise ValueError(\"n_components must be between 1 and n_features\")\n    \n    # Step 1: Compute the global mean\n    mu = np.mean(X, axis=0)\n    \n    # Step 2: Compute the between-class scatter matrix S_B\n    unique_classes = np.unique(y)\n    num_classes = len(unique_classes)\n    S_B = np.zeros((X.shape[1], X.shape[1]))\n    \n    for i, cls in enumerate(unique_classes):\n        X_cls = X[y == cls]\n        mu_cls = np.mean(X_cls, axis=0)\n        S_B += X_cls.shape[0] * (mu_cls - mu).reshape(-1, 1) @ (mu_cls - mu).reshape(1, -1)\n    \n    # Step 3: Compute the within-class scatter matrix S_W\n    S_W = np.zeros((X.shape[1], X.shape[1]))\n    for i, cls in enumerate(unique_classes):\n        X_cls = X[y == cls]\n        mu_cls = np.mean(X_cls, axis=0)\n        S_W += (X_cls - mu_cls).T @ (X_cls - mu_cls)\n    \n    # Step 4: Compute the matrix A = pinv(S_W) \u00b7 S_B\n    S_W_inv = np.linalg.pinv(S_W)\n    A = S_W_inv @ S_B\n    \n    # Step 5: Perform eigen-decomposition of A\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    \n    # Step 6: Sort eigen-pairs in descending order of the eigen-values\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Step 7: Apply deterministic sign fix\n    for i in range(eigenvectors.shape[1]):\n        if eigenvectors[i][0] < 0:\n            eigenvectors[:, i] *= -1\n    \n    # Step 8: Project X on the first n_components eigenvectors\n    projected_data = np.dot(X, eigenvectors[:, :n_components])\n    \n    # Step 9: Round every element to four decimal places\n    projected_data_rounded = np.round(projected_data, 4)\n    \n    # Convert to list of lists\n    projected_data_list = projected_data_rounded.tolist()\n    \n    return projected_data_list\n\n# Example usage\nX = np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]])\ny = np.array([0,0,0,0,1,1,1,1])\nn_components = 1\nprint(multi_class_lda(X, y, n_components))\n```\n\n### Explanation:\n1. **Global Mean Calculation**: We calculate the mean of all data points to get the global mean vector `mu`.\n2. **Between-Class Scatter Matrix (`S_B`)**: We compute the scatter matrix between each class. For each class, we calculate the mean vector of the class, subtract the global mean, and square it. We sum these squared differences weighted by the number of samples in each class.\n3. **Within-Class Scatter Matrix (`S_W`)**: We compute the scatter matrix within each class. For each class, we calculate the difference between each data point and the class mean, square it, and sum these squared differences.\n4. **Matrix `A` Computation**: We compute `A` as the product of the inverse of `S_W` and `S_B`.\n5. **Eigen-Decomposition**: We perform eigen-decomposition on `A` to obtain the eigenvalues and eigenvectors.\n6. **Deterministic Sign Fix**: We ensure that the first non-zero element of each eigenvector is positive.\n7. **Projection**: We project the original data onto the selected eigenvectors.\n8. **Rounding**: We round each element of the projected data to four decimal places.\n9. **Output**: We convert the resulting array to a list of lists and return it.\n\nThis implementation ensures that the LDA transformation is performed correctly and efficiently."}
{"task_id": 129, "completion_id": 0, "completion": "To implement the `cycle_sequence` function as described, we can use the `itertools.islice` function along with the `itertools.cycle` function. The `islice` function will allow us to slice the infinite cycle up to the desired number of samples, while `cycle` will create an iterator that cycles through the given sequence indefinitely.\n\nHere's how you can implement the function:\n\n```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    \n    # Create an infinite cycle of the sequence\n    cycled_sequence = cycle(sequence)\n    \n    # Use islice to get the first 'samples' elements from the cycle\n    result = list(islice(cycled_sequence, samples))\n    \n    return result\n\n# Example usage:\nsequence = [1, 2, 3]\nsamples = 5\nprint(cycle_sequence(sequence, samples))  # Output: [1, 2, 3, 1, 2]\n\nsequence = []\nsamples = 3\nprint(cycle_sequence(sequence, samples))  # Output: []\n\nsequence = [4, 5]\nsamples = 0\nprint(cycle_sequence(sequence, samples))  # Output: []\n```\n\n### Explanation:\n- **Empty Sequence or Non-positive Samples**: The function first checks if the sequence is empty or if `samples` is less than or equal to zero. If so, it returns an empty list immediately.\n- **Creating an Infinite Cycle**: We use `itertools.cycle` to create an infinite iterator over the sequence.\n- **Slicing the Cycle**: We use `itertools.islice` to take the first `samples` elements from this infinite cycle. This effectively allows us to cycle through the sequence until we have collected the desired number of items.\n- **Returning the Result**: Finally, we convert the sliced iterator to a list and return it.\n\nThis implementation ensures that the function behaves correctly even when the input sequence is empty or when the number of samples requested is not positive."}
{"task_id": 130, "completion_id": 0, "completion": "To implement the `outcome_probs` function, we need to follow these steps:\n\n1. Filter the input list of transitions to only include those where the `state` and `action` match the specified values.\n2. Use a `Counter` to count the occurrences of each `next_state` from the filtered transitions.\n3. Calculate the probability of each `next_state` by dividing its count by the total number of transitions for the given `(state, action)`.\n4. Round the probabilities to four decimal places.\n5. Sort the results by `next_state` in ascending order.\n6. Return the sorted list of `[next_state, probability]` pairs.\n\nHere's the implementation of the function:\n\n```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    # Filter transitions for the specific (state, action)\n    filtered_transitions = [t for t in transitions if t[:2] == (state, action)]\n    \n    # Count occurrences of each next_state\n    counts = Counter(t[3] for t in filtered_transitions)\n    \n    # Calculate probabilities and round them to 4 decimal places\n    probabilities = [(next_state, round(count / len(filtered_transitions), 4))\n                     for next_state, count in counts.items()]\n    \n    # Sort the probabilities by next_state\n    probabilities.sort(key=lambda x: x[0])\n    \n    return probabilities\n\n# Example usage:\ntransitions = [\n    (0, 'left', -1, 1),\n    (0, 'left', -1, 1),\n    (0, 'left', -1, 2),\n    (0, 'right', 0, 3)\n]\n\nprint(outcome_probs(transitions, 0, 'left'))\n```\n\nThis function will correctly compute the empirical probabilities for the given `(state, action)` pair and return the result as required."}
{"task_id": 131, "completion_id": 0, "completion": "To implement the `get_index` function according to the specified requirements, we need to handle two main cases:\n\n1. Check if the `uid` is a valid index for `_SHARED_SEQUENCES`.\n2. Check if the `i` is within the bounds of the sequence corresponding to `uid`.\n\nHere's how you can implement this function:\n\n```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # Check if uid is a valid index\n    if not (0 <= uid < len(_SHARED_SEQUENCES)):\n        return None\n\n    # Get the sequence associated with uid\n    sequence = _SHARED_SEQUENCES[uid]\n\n    # Normalize negative index\n    if i < 0:\n        i += len(sequence)\n\n    # Check if i is within the bounds of the sequence\n    if not (0 <= i < len(sequence)):\n        return None\n\n    # Return the element at the normalized index\n    return sequence[i]\n```\n\n### Explanation:\n\n- **Validation of `uid`:**\n  - We first check if `uid` is within the valid range using `0 <= uid < len(_SHARED_SEQUENCES)`. If it's not, we return `None`.\n\n- **Accessing the Sequence:**\n  - We retrieve the sequence from `_SHARED_SEQUENCES` using the validated `uid`.\n\n- **Normalization of Negative Indices:**\n  - If `i` is negative, we normalize it by adding its absolute value to the length of the sequence. This effectively converts negative indices into positive ones based on the sequence's length.\n\n- **Bounds Checking:**\n  - After normalization, we check if `i` is still within the bounds of the sequence (`0 <= i < len(sequence)`). If not, we return `None`.\n\n- **Returning the Element:**\n  - If all checks pass, we return the element at the normalized index `i` from the sequence.\n\nThis implementation ensures that the function handles invalid inputs gracefully and returns `None` when necessary, while leaving the global list `_SHARED_SEQUENCES` unchanged."}
{"task_id": 132, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        return z\n    elif isinstance(z, (list, tuple)):\n        return np.array(z)\n    else:\n        return z  # This case covers NumPy ndarrays\n\n# Example usage:\nprint(linear(5))          # Output: 5\nprint(linear([1, 2, 3]))   # Output: [1 2 3]\nprint(linear(np.array([4, 5, 6])))  # Output: [4 5 6]\n```\n\nThis implementation checks the type of `z` and handles each case accordingly:\n- If `z` is a scalar (int or float), it returns the scalar unchanged.\n- If `z` is a list or tuple, it converts the list or tuple into a NumPy ndarray and returns it.\n- If `z` is already a NumPy ndarray, it simply returns the same ndarray without any modifications."}
{"task_id": 133, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    Y_train = []\n    for seq in X_train:\n        one_hot_seq = []\n        for idx in seq:\n            if idx < 0 or idx >= vocab_length:\n                one_hot_seq.append(-1)\n            else:\n                one_hot_vector = [0] * vocab_length\n                one_hot_vector[idx] = 1\n                one_hot_seq.append(one_hot_vector)\n        Y_train.append(one_hot_seq)\n    \n    # Convert the list to a numpy array and then back to a list of lists\n    Y_train_np = np.array(Y_train).tolist()\n    return Y_train_np\n```"}
{"task_id": 134, "completion_id": 0, "completion": "Here's how you can implement the `best_arm` function to find the arm with the highest expected reward in a Bernoulli bandit:\n\n```python\nimport numpy as np\n\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        A list where each element is the success probability of the\n        corresponding arm. Probabilities must lie in the interval [0, 1].\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid probability (<0 or >1), the function must return\n        (-1.0, -1).\n    \"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    \n    max_prob = -np.inf\n    best_arm_index = -1\n    \n    for i, prob in enumerate(payoff_probs):\n        if not (0 <= prob <= 1):\n            return (-1.0, -1)\n        \n        if prob > max_prob:\n            max_prob = prob\n            best_arm_index = i\n    \n    return (max_prob, best_arm_index)\n\n# Example usage:\npayoff_probs = [0.15, 0.6, 0.35]\nresult = best_arm(payoff_probs)\nprint(result)  # Output: (0.6, 1)\n```\n\n### Explanation:\n1. **Check for Empty List**: The function first checks if the input list `payoff_probs` is empty. If it is, it immediately returns `(-1.0, -1)`.\n\n2. **Initialize Variables**: It initializes `max_prob` to negative infinity and `best_arm_index` to `-1`. These will keep track of the highest probability found and the corresponding index.\n\n3. **Iterate Over Probabilities**: The function iterates over each probability in the list along with its index. For each probability:\n   - It checks if the probability is within the valid range `[0, 1]`. If not, it returns `(-1.0, -1)`.\n   - If the current probability is greater than `max_prob`, it updates `max_prob` and sets `best_arm_index` to the current index.\n\n4. **Return Result**: After iterating through all probabilities, it returns a tuple containing the highest expected reward (`max_prob`) and the index of the corresponding arm (`best_arm_index`)."}
{"task_id": 135, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_predict(\n        X_train: list[list[int | float]],\n        y_train: list[int | float],\n        X_test: list[list[int | float]]) -> list[float]:\n    \"\"\"\n    Implement Ordinary Least Squares (OLS) linear regression from scratch using the normal equation.\n\n    Parameters:\n    - X_train: A list of lists representing the training features.\n    - y_train: A list of integers or floats representing the training targets.\n    - X_test: A list of lists representing the test features.\n\n    Returns:\n    - A list of predicted target values rounded to 4 decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n\n    # Add a column of ones for the intercept term\n    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # Calculate the normal equation\n    try:\n        # Compute the coefficients using the formula: \u03b2 = (X^T * X)^(-1) * X^T * y\n        beta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n    except np.linalg.LinAlgError as e:\n        # If the matrix (X^T * X) is singular, return -1\n        if \"Singular matrix\" in str(e):\n            return [-1]\n        else:\n            raise e\n\n    # Predict the target values for the test set\n    y_pred = X_test @ beta\n\n    # Round the predictions to 4 decimal places\n    y_pred_rounded = np.round(y_pred, decimals=4)\n\n    return y_pred_rounded.tolist()\n```\n\nThis implementation calculates the OLS regression coefficients using the normal equation and handles the case where the matrix \\( X^T \\times X \\) is singular by returning `-1`. The predictions are then rounded to four decimal places before being returned."}
{"task_id": 136, "completion_id": 0, "completion": "To implement the numerically-stable softmax function, we need to follow these steps:\n\n1. Subtract the maximum value along the specified axis from each element in the array.\n2. Exponentiate the shifted values.\n3. Normalize the resulting values by dividing them by the sum of the exponentials along the same axis.\n4. Round each element to four decimal places.\n5. Convert the final result to a Python list using `ndarray.tolist()`.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Divide by the sum of exponentials along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: \u20111).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    # Step 1: Subtract the maximum value along the specified axis\n    max_z = np.max(z, axis=axis, keepdims=True)\n    shifted_z = z - max_z\n    \n    # Step 2: Exponentiate the shifted values\n    exp_shifted_z = np.exp(shifted_z)\n    \n    # Step 3: Normalize by dividing by the sum of exponentials along the same axis\n    sum_exp = np.sum(exp_shifted_z, axis=axis, keepdims=True)\n    softmax_probs = exp_shifted_z / sum_exp\n    \n    # Step 4: Round each element to four decimal places\n    rounded_probs = np.round(softmax_probs, decimals=4)\n    \n    # Step 5: Convert the NumPy array to a Python list and return it\n    return rounded_probs.tolist()\n\n# Example usage:\nz = np.array([1.0, 2.0, 3.0])\naxis = 0\nprint(softmax(z, axis))  # Output: [0.0900, 0.2447, 0.6652]\n```\n\nThis implementation ensures that the softmax operation is numerically stable by handling large or very small values properly."}
{"task_id": 138, "completion_id": 0, "completion": "Here's how you can implement the `accuracy_score` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    # Check if the input lists are empty or have different lengths\n    if not y_true or not y_pred or len(y_true) != len(y_pred):\n        return -1\n    \n    # Count the number of matching pairs\n    correct_count = sum(1 for true_label, pred_label in zip(y_true, y_pred) if true_label == pred_label)\n    \n    # Calculate the accuracy\n    accuracy = correct_count / len(y_true)\n    \n    # Round the accuracy to four decimal places\n    return round(accuracy, 4)\n\n# Example usage:\ny_true = [0, 1, 2, 3]\ny_pred = [0, 1, 2, 4]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.7500\n\ny_true = [True, False, True]\ny_pred = [False, True, True]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.6667\n\ny_true = []\ny_pred = [1, 2, 3]\nprint(accuracy_score(y_true, y_pred))  # Output: -1\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if either of the input lists is empty or if they have different lengths. If so, it returns `-1`.\n\n2. **Counting Matches**: It uses a generator expression within the `sum` function to count the number of positions where the corresponding elements in `y_true` and `y_pred` are equal.\n\n3. **Calculating Accuracy**: The accuracy is calculated by dividing the count of correct matches by the total number of elements in the lists.\n\n4. **Rounding**: The result is rounded to four decimal places using the `round` function before being returned.\n\nThis implementation handles various types of labels (numerical, strings, or booleans) due to Python's dynamic typing."}
{"task_id": 139, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_activation(z):\n    \"\"\"Compute the hyperbolic tangent (tanh) for every element of the input.\n\n    Args:\n        z (list[int | float] | np.ndarray): A one-dimensional iterable of\n            numeric values.\n\n    Returns:\n        list[float]: Tanh applied element-wise, rounded to four decimals.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    z_array = np.array(z)\n    \n    # Apply the numerical stable version of the tanh function\n    tanh_values = 2 / (1 + np.exp(-2 * z_array)) - 1\n    \n    # Round the results to four decimal places\n    rounded_tanh_values = np.round(tanh_values, 4).tolist()\n    \n    return rounded_tanh_values\n\n# Example usage:\ninput_data = [0, 1, -1, 0.5]\noutput = tanh_activation(input_data)\nprint(output)  # Output: [0.0, 0.7616, -0.7616, 0.4621]\n```\n\nThis code defines a function `tanh_activation` that computes the hyperbolic tangent of each element in the input list using the given formula. It handles both integer and floating-point inputs and returns the results rounded to four decimal places. The function uses NumPy for efficient computation, including converting the input to a NumPy array and applying the mathematical operations."}
{"task_id": 140, "completion_id": 0, "completion": "To solve the problem of escaping a fire maze, we need to simulate the movement of both the agent and the fire over time. The key steps are:\n\n1. **Initialize Data Structures**: Use two queues to keep track of the positions of the agent and the fire. Also, use a set to mark visited cells to avoid revisiting them.\n\n2. **Fire Spread Simulation**: Simulate the spread of fire from each burning cell to its neighboring cells. This will update the fire's position for each minute.\n\n3. **Agent Movement Simulation**: Simulate the agent moving from its current position to its next valid position (a non-burning cell) after the fire has spread. If the agent reaches the goal before the fire catches up, return the number of minutes taken. If the fire catches up to the agent or the agent cannot move further, return -1.\n\nHere's the implementation of the `escape_fire_maze` function:\n\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    \n    # Check if the start or goal is on fire\n    if grid[0][0] == 'F' or grid[n-1][n-1] == 'F':\n        return -1\n    \n    # Initialize the agent's queue and visited set\n    agent_queue = deque([(0, 0)])\n    agent_visited = {(0, 0)}\n    \n    # Initialize the fire's queue and visited set\n    fire_queue = deque([pos for pos, char in enumerate(grid[0]) if char == 'F'])\n    fire_visited = {pos for pos, char in enumerate(grid[0]) if char == 'F'}\n    \n    # Directions for moving up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Time counter\n    time = 0\n    \n    while agent_queue and fire_queue:\n        # Increment the time\n        time += 1\n        \n        # Move the fire\n        new_fire_positions = set()\n        for pos in fire_queue:\n            x, y = divmod(pos, n)\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < n and 0 <= ny < n and grid[nx][ny] != '#' and (nx * n + ny) not in fire_visited:\n                    new_fire_positions.add(nx * n + ny)\n                    fire_visited.add(nx * n + ny)\n        \n        # Update the fire queue\n        fire_queue = deque(new_fire_positions)\n        \n        # Move the agent\n        for _ in range(len(agent_queue)):\n            x, y = agent_queue.popleft()\n            \n            # Check if the agent reached the goal\n            if (x, y) == (n-1, n-1):\n                return time\n            \n            # Explore all possible moves\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < n and 0 <= ny < n and grid[nx][ny] != '#' and (nx * n + ny) not in agent_visited:\n                    agent_queue.append((nx, ny))\n                    agent_visited.add(nx * n + ny)\n        \n        # Prune the fire queue if necessary\n        if not fire_queue:\n            break\n    \n    # If the agent cannot reach the goal\n    return -1\n\n# Example usage:\ngrid = [\n    \".F.\",\n    \"#.#\",\n    \".F.\"\n]\nprint(escape_fire_maze(grid))  # Output: 3\n```\n\nThis solution simulates the fire spread and agent movement step-by-step, updating the state of the maze at each minute until the agent reaches the goal or the fire catches up. The function efficiently handles the constraints and ensures that the agent's path is optimized."}
{"task_id": 141, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbors (k-NN) classifier in Python:\n\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    Implement the k-Nearest Neighbors (k-NN) classifier.\n\n    Parameters:\n    - X_train: A 2D NumPy array where each row represents a feature vector of a training example.\n    - y_train: A 1D NumPy array containing the labels for the training examples.\n    - X_test: A 2D NumPy array where each row represents a feature vector of a test example.\n    - k: An integer representing the number of nearest neighbors to consider.\n    - metric: A string specifying the distance metric ('euclidean', 'manhattan', or 'cosine'). Defaults to 'euclidean'.\n\n    Returns:\n    - A 1-D NumPy array containing the predicted labels for the test examples.\n    \"\"\"\n    \n    # Define a dictionary to map metric names to their corresponding functions\n    distance_functions = {\n        'euclidean': lambda x, y: np.linalg.norm(x - y),\n        'manhattan': lambda x, y: np.sum(np.abs(x - y)),\n        'cosine': lambda x, y: 1 - np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n    }\n    \n    # Get the appropriate distance function based on the input metric\n    dist_func = distance_functions.get(metric, distance_functions['euclidean'])\n    \n    # Initialize an empty list to store predictions\n    predictions = []\n    \n    # Iterate over each test sample\n    for test_sample in X_test:\n        # Calculate distances from the current test sample to all training samples\n        distances = [dist_func(test_sample, train_sample) for train_sample in X_train]\n        \n        # Find the indices of the k smallest distances\n        closest_indices = np.argsort(distances)[:k]\n        \n        # Extract the labels of the k closest training samples\n        closest_labels = y_train[closest_indices]\n        \n        # Determine the most common label among the k closest samples\n        unique_labels, counts = np.unique(closest_labels, return_counts=True)\n        predicted_label = unique_labels[np.argmax(counts)]\n        \n        # Append the predicted label to the predictions list\n        predictions.append(predicted_label)\n    \n    # Convert the predictions list to a NumPy array and return it\n    return np.array(predictions)\n\n# Example usage:\n# X_train, y_train, X_test, k, metric = load_data()\n# predictions = knn_predict(X_train, y_train, X_test, k, metric)\n# print(predictions)\n```\n\n### Explanation:\n1. **Distance Functions**: We define a dictionary `distance_functions` that maps each metric name to its corresponding distance calculation function. The Euclidean distance is used as the default if an unknown metric is provided.\n\n2. **Distance Calculation**: For each test sample, we calculate the distance to every training sample using the specified metric.\n\n3. **K Nearest Neighbors**: We find the indices of the `k` smallest distances and extract the corresponding labels from the training data.\n\n4. **Majority Voting**: We determine the most common label among the `k` closest neighbors. In case of a tie, the smallest label value is chosen.\n\n5. **Prediction Collection**: We collect the predicted labels for all test samples and return them as a NumPy array."}
{"task_id": 143, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    # Convert input to NumPy array if necessary\n    x = np.asarray(x)\n    \n    # Apply Leaky ReLU activation\n    if derivative:\n        # Derivative of Leaky ReLU is 1 for x >= 0 and alpha for x < 0\n        return np.where(x >= 0, 1, alpha)\n    else:\n        # Leaky ReLU activation\n        return np.maximum(x, alpha * x)\n\n# Example usage:\n# Activation\ninput_array = np.array([-1, -0.5, 0, 0.5, 1])\noutput_activation = leaky_relu(input_array)\nprint(\"Leaky ReLU Activation:\", output_activation)\n\n# Derivative\noutput_derivative = leaky_relu(input_array, derivative=True)\nprint(\"Derivative of Leaky ReLU:\", output_derivative)\n```\n\nThis implementation uses vectorized operations provided by NumPy to efficiently compute both the Leaky ReLU activation and its derivative without using explicit loops. The `np.where` function is used to handle different cases based on whether the input value is greater than or equal to zero, allowing for a clean and efficient solution."}
{"task_id": 144, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise -1.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n\n    # Check if the shapes are the same\n    if actual_array.shape != predicted_array.shape:\n        return -1\n\n    # Calculate the absolute differences\n    absolute_differences = np.abs(actual_array - predicted_array)\n\n    # Calculate the Mean Absolute Error\n    mae = np.mean(absolute_differences)\n\n    # Round the result to 4 decimal places\n    mae_rounded = round(mae, 4)\n\n    return mae_rounded\n```\n\nThis function first converts the input sequences into NumPy arrays if they are not already in this format. It then checks if the shapes of the two arrays are the same. If they are not, the function returns `-1`. If they are the same, it calculates the absolute differences between the corresponding elements of the arrays, computes the mean of these differences, and rounds the result to four decimal places before returning it."}
{"task_id": 146, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier from scratch, we need to follow these steps:\n\n1. Compute the distances between each test sample and every training sample using the specified metric.\n2. Select the `k` nearest neighbors for each test sample.\n3. Perform a majority vote on the labels of these neighbors, resolving ties by choosing the smallest label.\n4. Return the predicted labels for all test samples.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    def euclidean_distance(x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n    def manhattan_distance(x1, x2):\n        return np.sum(np.abs(x1 - x2))\n\n    def cosine_distance(x1, x2, epsilon=1e-12):\n        dot_product = np.dot(x1, x2)\n        norm_x = np.linalg.norm(x1, ord=2)\n        norm_y = np.linalg.norm(x2, ord=2)\n        if norm_x == 0 or norm_y == 0:\n            return 1  # Avoid division by zero\n        return 1 - (dot_product / (norm_x * norm_y + epsilon))\n\n    # Determine the number of training samples\n    num_train_samples = len(y)\n\n    # Initialize the result list\n    predictions = []\n\n    # Iterate over each test sample\n    for x_test in X_test:\n        # Calculate distances to all training samples\n        if k > num_train_samples:\n            distances = [metric(x_test, train_sample) for train_sample in X]\n        else:\n            distances = sorted([(metric(x_test, train_sample), idx) for idx, train_sample in enumerate(X)], key=lambda item: item[0])\n\n        # Get the indices of the k nearest neighbors\n        nearest_indices = [distances[i][1] for i in range(k)]\n\n        # Extract the labels of the k nearest neighbors\n        neighbor_labels = [y[idx] for idx in nearest_indices]\n\n        # Count occurrences of each label\n        label_counts = {}\n        for label in neighbor_labels:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        # Find the most common label (in case of a tie, choose the smallest label)\n        predicted_label = min(label_counts, key=label_counts.get)\n\n        # Append the predicted label to the results\n        predictions.append(predicted_label)\n\n    return predictions.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([0, 0, 1, 1])\n# X_test = np.array([[2.5, 3.5], [3.5, 4.5]])\n# print(knn_predict(X, y, X_test))\n```\n\nThis implementation defines a function `knn_predict` that calculates the distances between test samples and training samples using the specified metric, finds the k nearest neighbors, performs a majority vote, and returns the predicted labels. The example usage demonstrates how to call the function with sample data."}
{"task_id": 147, "completion_id": 0, "completion": "To implement binary Logistic Regression from scratch using batch gradient descent, we need to follow these steps:\n\n1. **Initialize weights and bias**: Start with random initial values for the weights (\\(\\mathbf w\\)) and bias (\\(b\\)).\n2. **Compute predictions**: Use the logistic function to compute the predicted probability of each sample belonging to the positive class.\n3. **Calculate loss**: Compute the negative log-likelihood loss for the current weights and bias.\n4. **Update weights and bias**: Update the weights and bias using the computed gradients.\n5. **Repeat until convergence**: Iterate over the training data multiple times (epochs), updating the weights and bias in each iteration.\n\nHere's the implementation of the `logistic_regression` function:\n\n```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n    # Initialize weights and bias\n    d = X_train.shape[1]\n    w = np.zeros(d)\n    b = 0\n    \n    # Helper function to calculate the sigmoid\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Helper function to calculate the loss\n    def loss(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    # Helper function to make predictions\n    def predict(X, w, b):\n        z = np.dot(X, w) + b\n        return (sigmoid(z) >= 0.5).astype(int)\n    \n    # Perform batch gradient descent\n    for epoch in range(epochs):\n        # Calculate predictions\n        y_pred = sigmoid(np.dot(X_train, w) + b)\n        \n        # Calculate gradients\n        dw = (1 / len(X_train)) * np.dot(X_train.T, (y_pred - y_train))\n        db = (1 / len(X_train)) * np.sum(y_pred - y_train)\n        \n        # Update weights and bias\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        \n        # Print progress every 1000 epochs\n        if epoch % 1000 == 0:\n            print(f\"Epoch {epoch}: Loss = {loss(y_train, y_pred)}\")\n    \n    # Make predictions on the test set\n    y_test_pred = predict(X_test, w, b)\n    \n    return y_test_pred.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X_train = np.array([[-2, -1], [-3, -2], [-1, -2], [-2, -3],\n                        [2, 1], [3, 2], [1, 2], [2, 3]])\n    y_train = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n    X_test = np.array([[-2, -2], [2, 2]])\n\n    y_test_pred = logistic_regression(X_train, y_train, X_test)\n    print(y_test_pred)  # Output: [0, 1]\n```\n\n### Explanation:\n- **Initialization**: We initialize the weights and bias to zero.\n- **Sigmoid Function**: This function maps any real-valued input to a value between 0 and 1, which represents the probability of the positive class.\n- **Loss Calculation**: The negative log-likelihood loss is used to measure how well the model is fitting the data.\n- **Gradient Descent**: We update the weights and bias iteratively using the gradients calculated from the loss.\n- **Prediction**: After training, we use the trained model to predict the class labels for the test set.\n- **Output**: The function returns a list of predicted class labels for the test examples."}
{"task_id": 148, "completion_id": 0, "completion": "To implement the exponential smoothing algorithm, we need to follow the recursive formula provided in the problem statement. Here's how you can implement it in Python:\n\n```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    if not data or not (0 <= alpha <= 1):\n        return -1\n    \n    n = len(data)\n    smoothed_values = []\n    \n    for k in range(n):\n        if k == 0:\n            smoothed_values.append(round(data[k], 4))\n        else:\n            smoothed_value = round((1 - alpha) * smoothed_values[-1] + alpha * data[k], 4)\n            smoothed_values.append(smoothed_value)\n    \n    return smoothed_values\n\n# Example usage:\ndata = [10, 20, 30, 40, 50]\nalpha = 0.5\nprint(exp_smoothing(data, alpha))  # Output: [10.0, 17.5, 26.88, 37.12, 49.6]\n\nalpha = 0\nprint(exp_smoothing(data, alpha))  # Output: [10.0, 10.0, 10.0, 10.0, 10.0]\n\nalpha = 1\nprint(exp_smoothing(data, alpha))  # Output: [10.0, 20.0, 30.0, 40.0, 50.0]\n```\n\n### Explanation:\n1. **Input Validation**: \n   - The function first checks if the input list `data` is empty or if the smoothing factor `alpha` is outside the range `[0, 1]`. If either condition is true, it returns `-1`.\n\n2. **Initialization**:\n   - It initializes an empty list `smoothed_values` to store the smoothed results.\n   - The first smoothed value is set to the first element of the input list, rounded to 4 decimal places.\n\n3. **Recursive Calculation**:\n   - For each subsequent element in the input list, it calculates the smoothed value using the formula:\n     \\[\n     s_k = (1 - \\alpha) \\cdot s_{k-1} + \\alpha \\cdot x_k\n     \\]\n   - The result is rounded to 4 decimal places and appended to the `smoothed_values` list.\n\n4. **Return**:\n   - Finally, the function returns the list of smoothed values."}
{"task_id": 150, "completion_id": 0, "completion": "To solve the problem of finding the shortest path in a grid using the A* search algorithm, we need to implement the algorithm with the Manhattan distance heuristic. Here's how you can do it:\n\n```python\nfrom heapq import heappush, heappop\nfrom typing import List, Tuple, Union\n\ndef find_shortest_path(grid: List[List[int]]) -> Union[int, List[Tuple[int, int]]]:\n    \"\"\"Find a shortest path from the top-left to the bottom-right corner of a maze.\n\n    The maze is described by a 2-D list where 0 denotes a free cell and 1 denotes\n    a blocked cell. The function must use the A* algorithm with the Manhattan\n    heuristic to return *one* optimal path.\n\n    Args:\n        grid: Rectangular 2-D list of integers (0 or 1).\n\n    Returns:\n        \u2022 A list of (row, column) tuples that represents a shortest path\n          including the start (0,0) and goal (m-1,n-1) positions.\n        \u2022 -1 if no such path exists.\n    \"\"\"\n    if not grid or not grid[0]:\n        return -1\n\n    rows, cols = len(grid), len(grid[0])\n    start = (0, 0)\n    goal = (rows - 1, cols - 1)\n\n    # Directions for moving up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    # Priority queue for A* search\n    open_set = []\n    heappush(open_set, (0, start))\n\n    # Dictionary to store the best known cost to reach each position\n    g_score = {start: 0}\n\n    # Dictionary to store the previous node on the optimal path\n    came_from = {}\n\n    while open_set:\n        current_g_score, current = heappop(open_set)\n\n        # If we have reached the goal, reconstruct the path\n        if current == goal:\n            path = []\n            while current in came_from:\n                path.append(current)\n                current = came_from[current]\n            path.reverse()\n            return path\n\n        # Explore neighbors\n        for dr, dc in directions:\n            neighbor = (current[0] + dr, current[1] + dc)\n            if 0 <= neighbor[0] < rows and 0 <= neighbor[1] < cols and grid[neighbor[0]][neighbor[1]] == 0:\n                tentative_g_score = g_score[current] + 1\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score = tentative_g_score + manhattan_distance(neighbor, goal)\n                    heappush(open_set, (f_score, neighbor))\n\n    # If no path is found\n    return -1\n\ndef manhattan_distance(a: Tuple[int, int], b: Tuple[int, int]) -> int:\n    \"\"\"Calculate the Manhattan distance between two points.\"\"\"\n    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n# Example usage:\nmaze = [\n    [0, 0, 0],\n    [0, 1, 0],\n    [0, 0, 0]\n]\n\nprint(find_shortest_path(maze))  # Output: [(0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]\n```\n\n### Explanation:\n1. **Initialization**: We initialize the priority queue with the starting position and set the initial cost (`g_score`) to 0 for the start position.\n\n2. **Priority Queue**: We use a priority queue to explore nodes in order of their estimated total cost (`f_score = g_score + h_score`). The heuristic used here is the Manhattan distance, which is efficient for grid-based problems.\n\n3. **Path Reconstruction**: Once we reach the goal, we backtrack from the goal to the start using the `came_from` dictionary to construct the optimal path.\n\n4. **Heuristic Function**: The Manhattan distance heuristic is calculated as `|x1 - x2| + |y1 - y2|`, which provides a good estimate of the remaining cost to reach the goal.\n\n5. **Edge Cases**: The function handles cases where there are no paths (e.g., blocked cells or unreachable goals) by returning `-1`."}
{"task_id": 151, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef activation_forward_backward(X: np.ndarray, activation: str, upstream_grad: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Apply an activation function and compute its local gradient.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Raw pre-activation values.\n    activation : str\n        Name of the activation function. Supported values are\n        \"sigmoid\", \"tanh\", \"relu\", and \"softmax\".\n    upstream_grad : np.ndarray\n        Gradient that arrives from the layer above (must have the same shape as\n        `X`).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A two-element tuple `(A, grad)` where:\n          \u2022 `A`    \u2013 the activation applied to `X`.\n          \u2022 `grad` \u2013 the element-wise product of `upstream_grad` and the local\n                      derivative of the activation (rounded to 4 decimals).\n    \"\"\"\n    if activation == 'sigmoid':\n        A = sigmoid(X)\n    elif activation == 'tanh':\n        A = tanh(X)\n    elif activation == 'relu':\n        A = relu(X)\n    elif activation == 'softmax':\n        A = softmax(X)\n    else:\n        raise ValueError(\"Unsupported activation function\")\n\n    grad = upstream_grad * activation_derivative(A, activation)\n\n    return np.round(A, 4), np.round(grad, 4)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef softmax(X):\n    max_X = np.max(X, axis=-1, keepdims=True)\n    exp_X = np.exp(X - max_X)\n    sum_exp_X = np.sum(exp_X, axis=-1, keepdims=True)\n    return exp_X / sum_exp_X\n\ndef activation_derivative(A, activation):\n    if activation == 'sigmoid':\n        return A * (1 - A)\n    elif activation == 'tanh':\n        return 1 - A**2\n    elif activation == 'relu':\n        return np.where(A > 0, 1, 0)\n    elif activation == 'softmax':\n        return np.diagflat(A) - np.dot(A.T, A)\n    else:\n        raise ValueError(\"Unsupported activation function\")\n```\n\nThis code defines the `activation_forward_backward` function, which applies an activation function to the input `X` and computes the local gradient. It supports the four specified activation functions (`sigmoid`, `tanh`, `relu`, and `softmax`) and returns the activated output and the gradient, both rounded to four decimal places. The helper functions for each activation and its derivative are also defined."}
{"task_id": 152, "completion_id": 0, "completion": "Here's the implementation of the `accuracy_score` function using only NumPy:\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check if the lengths of y_true and y_pred are equal\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # Calculate the number of correctly classified samples\n    correct_count = np.sum(y_true == y_pred)\n\n    # Calculate the accuracy\n    accuracy = correct_count / len(y_true)\n\n    # Return the accuracy rounded to four decimal places\n    return round(accuracy, 4)\n\n# Example usage:\ny_true = [1, 0, 2, 2, 1]\ny_pred = [1, 0, 1, 2, 0]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.6\n```\n\n### Explanation:\n1. **Input Conversion**: The function first converts the input lists to NumPy arrays to handle both types uniformly.\n2. **Length Check**: It checks if the lengths of `y_true` and `y_pred` are equal. If not, it returns `-1`.\n3. **Correct Count**: It uses NumPy's element-wise comparison (`==`) to find the indices where the true labels match the predicted labels and then sums these indices to get the count of correct classifications.\n4. **Accuracy Calculation**: The accuracy is calculated by dividing the number of correct classifications by the total number of samples.\n5. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function before being returned."}
{"task_id": 154, "completion_id": 0, "completion": "To implement the Radial Basis Function (RBF) kernel matrix, we need to follow these steps:\n\n1. **Compute Pair-Wise Squared Euclidean Distances**: We will use the formula for the squared Euclidean distance between two points \\( x_i \\) and \\( y_j \\):\n   \\[\n   ||x_i - y_j||_2^2 = \\sum_{k=1}^{d} (x_{ik} - y_{jk})^2\n   \\]\n\n2. **Apply the RBF Formula**: For each pair of samples \\( (x_i, y_j) \\), compute the RBF kernel value using:\n   \\[\n   K_{ij} = \\exp\\left(-\\gamma \\cdot ||x_i - y_j||_2^2\\right)\n   \\]\n\n3. **Round Results**: Ensure that each kernel value is rounded to 6 decimal places.\n\n4. **Return as Nested List**: Convert the resulting matrix into a nested Python list using `tolist()` from NumPy.\n\nHere is the implementation of the `rbf_kernel` function:\n\n```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # Ensure both inputs are 2D arrays\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n\n    # Get dimensions\n    n, d = X.shape\n    m, _ = Y.shape\n\n    # Initialize the kernel matrix\n    K = np.zeros((n, m))\n\n    # Compute the pair-wise squared Euclidean distances\n    for i in range(n):\n        for j in range(m):\n            diff = X[i] - Y[j]\n            dist_squared = np.dot(diff, diff)\n            K[i, j] = np.exp(-gamma * dist_squared)\n\n    # Round the kernel matrix to 6 decimal places\n    K_rounded = np.round(K, decimals=6)\n\n    # Convert to a nested Python list\n    return K_rounded.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5, 6], [7, 8]])\nkernel_matrix = rbf_kernel(X, Y)\nprint(kernel_matrix)\n```\n\n### Explanation:\n- **Input Handling**: The function first ensures that both input arrays `X` and `Y` are 2D arrays. If they are 1D, it reshapes them to have a single sample dimension.\n- **Distance Calculation**: It iterates over each pair of samples from `X` and `Y`, calculates the squared Euclidean distance, and stores it in the kernel matrix.\n- **Exponentiation**: The RBF kernel values are computed using the exponential function with the given `gamma`.\n- **Rounding**: The resulting kernel matrix is rounded to 6 decimal places using `np.round()`.\n- **Output**: Finally, the matrix is converted to a nested Python list using `tolist()` before being returned."}
{"task_id": 155, "completion_id": 0, "completion": "To complete the `actor_forward` function, we need to perform the forward pass through the neural network as described. This involves applying the linear transformations and non-linear activation functions in sequence. Here's how you can implement it:\n\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert the state to a NumPy array if it's not already\n    state = np.array(state)\n    \n    # Perform the first linear transformation\n    z1 = np.dot(state, weights['W1']) + weights['b1']\n    \n    # Apply the ReLU activation function\n    a1 = np.maximum(0, z1)\n    \n    # Perform the second linear transformation\n    z2 = np.dot(a1, weights['W2']) + weights['b2']\n    \n    # Apply the ReLU activation function\n    a2 = np.maximum(0, z2)\n    \n    # Perform the third linear transformation\n    z3 = np.dot(a2, weights['W3']) + weights['b3']\n    \n    # Apply the tanh activation function\n    action = np.tanh(z3)\n    \n    # Round each element of the action vector to 4 decimal places\n    rounded_action = [round(x, 4) for x in action]\n    \n    return rounded_action\n\n# Example usage:\n# Define the state and weights\nstate = [0.5, -0.2, 1.1]\nweights = {\n    'W1': np.random.rand(3, 5),  # Example weight matrices\n    'b1': np.random.rand(5),\n    'W2': np.random.rand(5, 6),\n    'b2': np.random.rand(6),\n    'W3': np.random.rand(6, 2),\n    'b3': np.random.rand(2)\n}\n\n# Get the action\naction = actor_forward(state, weights)\nprint(action)\n```\n\n### Explanation:\n1. **State Conversion**: The input state is converted to a NumPy array if it's not already one.\n2. **Linear Transformations**: Each linear transformation is applied using `np.dot` to compute the weighted sum of inputs plus biases.\n3. **ReLU Activation**: The ReLU function is applied element-wise using `np.maximum(0, z)`.\n4. **Tanh Activation**: The final linear transformation is followed by the tanh activation function.\n5. **Rounding**: The result is rounded to four decimal places before returning.\n\nThis implementation ensures that the network follows the specified architecture and handles the rounding requirements as specified."}
{"task_id": 157, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    # Check if both lists are empty\n    if not y_true and not y_pred:\n        return 0\n    \n    # Check if both lists have the same length\n    if len(y_true) != len(y_pred):\n        return -1\n    \n    # Calculate the number of matches\n    num_matches = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n    \n    # Calculate the accuracy\n    accuracy = round(num_matches / len(y_true), 4)\n    \n    return accuracy\n\n# Example usage:\nprint(accuracy_score([0, 1, 2, 3], [0, 1, 2, 4]))  # Output: 0.7500\nprint(accuracy_score([0, 1, 2, 3], [0, 1, 2, 3]))  # Output: 1.0000\nprint(accuracy_score([], []))                    # Output: 0.0000\nprint(accuracy_score([0, 1, 2, 3], [0, 1, 2]))     # Output: -1\n```\n\nThis implementation checks for the special cases specified in the problem statement and calculates the accuracy based on the number of matching pairs between the true and predicted labels. It ensures that the result is rounded to four decimal places as required."}
{"task_id": 158, "completion_id": 0, "completion": "Here's how you can implement the closed-form solution for Ridge Regression in Python:\n\n```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Check if fit_intercept is True\n    if fit_intercept:\n        # Add a leading column of ones for the intercept term\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        \n        # Create the regularization matrix A\n        I = np.eye(X.shape[1])\n        A = np.diag(np.zeros(1)) + alpha * I\n        \n    else:\n        # If no intercept is needed, just create the regularization matrix A\n        I = np.eye(X.shape[1])\n        A = alpha * I\n    \n    # Compute the closed-form solution using the formula beta_hat = (X^T * X + A)^(-1) * X^T * y\n    beta_hat = np.linalg.inv(X.T @ X + A) @ X.T @ y\n    \n    # Round the coefficients to four decimal places\n    beta_hat_rounded = [round(coef, 4) for coef in beta_hat]\n    \n    return beta_hat_rounded\n\n# Example usage:\nX = [[1, 2], [3, 4], [5, 6]]\ny = [7, 8, 9]\nalpha = 1.0\nfit_intercept = True\nprint(ridge_regression(X, y, alpha, fit_intercept))\n```\n\n### Explanation:\n1. **Input Conversion**: The input matrices `X` and `y` are converted to NumPy arrays for efficient computation.\n2. **Intercept Handling**:\n   - If `fit_intercept` is `True`, we add a leading column of ones to the feature matrix `X`.\n   - We then create the regularization matrix `A` where `A_00` is set to 0 (for the intercept) and the rest are set to `alpha`.\n3. **Closed-Form Solution**: We compute the coefficients using the formula \\(\\beta^\\hat{} = (X^T X + A)^{-1} X^T y\\).\n4. **Rounding**: The resulting coefficients are rounded to four decimal places and returned as a list."}
{"task_id": 159, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `log_gaussian_pdf` function using NumPy for numerical stability:\n\n```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = len(x_i)\n    \n    # Calculate the determinant of the covariance matrix using slogdet\n    _, log_det_sigma = np.linalg.slogdet(sigma)\n    \n    # Compute the difference between x_i and mu\n    diff = x_i - mu\n    \n    # Solve for Sigma^-1 * diff using solve instead of explicitly inverting Sigma\n    inv_sigma_diff = np.linalg.solve(sigma, diff)\n    \n    # Calculate the quadratic form term\n    quad_form_term = np.dot(diff.T, inv_sigma_diff)\n    \n    # Compute the log probability density\n    log_prob = -0.5 * (d * np.log(2 * np.pi) + log_det_sigma + quad_form_term)\n    \n    # Return the result rounded to 4 decimal places\n    return round(log_prob, 4)\n\n# Example usage:\n# x_i = np.array([1, 2])\n# mu = np.array([0, 0])\n# sigma = np.array([[1, 0], [0, 1]])\n# print(log_gaussian_pdf(x_i, mu, sigma))\n```\n\nThis function calculates the log probability density of a multivariate Gaussian distribution using the provided sample vector `x_i`, mean vector `mu`, and covariance matrix `sigma`. It uses NumPy's `linalg.slogdet` to compute the natural logarithm of the determinant of the covariance matrix and `linalg.solve` to efficiently compute the inverse of the covariance matrix without forming it explicitly. The result is then rounded to four decimal places before being returned."}
{"task_id": 160, "completion_id": 0, "completion": "To implement Lasso regression using coordinate descent in Python, we need to follow the steps outlined in the task description. Below is the complete implementation:\n\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    m, n = X.shape\n    \n    if fit_intercept:\n        X = np.hstack((np.ones((m, 1)), X))\n    \n    weights = np.zeros(n)\n    bias = 0.0\n    \n    for _ in range(max_iters):\n        for j in range(n):\n            if j == 0 and fit_intercept:\n                continue\n            \n            # Compute the partial residual\n            r_j = y - np.dot(X, weights)\n            \n            # Update the weight using soft-thresholding\n            sign_xj = np.sign(np.dot(X[:, j], r_j))\n            abs_xj = np.abs(np.dot(X[:, j], r_j))\n            threshold = lambda_param * m\n            updated_weight = sign_xj * np.maximum(abs_xj - threshold, 0) / np.sum(X[:, j]**2)\n            \n            # Update the weight and bias\n            weights[j] = updated_weight\n            bias += (y - np.dot(X, weights)).mean()\n    \n    return [round(weight, 4) for weight in weights], round(bias, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([1, 2, 3])\n# lambda_param = 1.0\n# max_iters = 100\n# fit_intercept = True\n# weights, bias = fit_lasso(X, y, lambda_param, max_iters, fit_intercept)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n1. **Handling Intercept**: If `fit_intercept` is `True`, we prepend an all-ones column to the feature matrix `X`. This allows us to include an intercept term in the model.\n\n2. **Initialization**: We initialize the weight vector `weights` to zero and the bias to zero.\n\n3. **Coordinate Loop**: For each iteration, we iterate over each coefficient `w_j` (excluding the intercept if `fit_intercept` is `True`). For each coefficient:\n   - We compute the partial residual `r_j`.\n   - We apply the soft-thresholding operation to update `w_j`.\n   - We update the bias by averaging the residuals.\n\n4. **Output**: After completing the specified number of iterations, we return the weights and bias, both rounded to four decimal places.\n\nThis implementation ensures that the Lasso regression model is learned using coordinate descent, adhering to the constraints and requirements specified in the task."}
{"task_id": 161, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"\n    Predict binary class labels using a second-order Factorization Machine.\n\n    Parameters:\n    - X: A 2D NumPy array of shape (m, n) containing m samples with n features each.\n    - w0: A scalar global bias.\n    - w: A 1D NumPy array of length n with linear weights.\n    - V: A 2D NumPy array of shape (n, k) with latent factors.\n\n    Returns:\n    - A list of binary class labels (1 or -1) for each sample.\n    \"\"\"\n    # Compute the dot product between X and V\n    XV = np.dot(X, V)\n    \n    # Compute the squared dot product between X and V^T\n    XV_squared = np.dot(X**2, np.dot(V.T, V))\n    \n    # Compute the sum of squared norms of XV minus XV_squared\n    inner_product_sum = np.sum(XV**2 - XV_squared)\n    \n    # Compute the score\n    score = w0 + np.dot(w, X) + 0.5 * inner_product_sum\n    \n    # Apply the decision boundary\n    labels = [1 if s >= 0 else -1 for s in score]\n    \n    return labels\n```\n\nThis function calculates the second-order Factorization Machine prediction for each sample in the input matrix `X` using the provided parameters `w0`, `w`, and `V`. It then applies the decision boundary to determine the binary class label for each sample. The result is returned as a list of integers (either 1 or -1)."}
{"task_id": 162, "completion_id": 0, "completion": "To implement the cross-entropy loss and its gradient as described, we need to follow these steps:\n\n1. Compute the loss using the formula provided.\n2. Compute the gradient of the loss with respect to the softmax output.\n3. Handle the case where the softmax output might be zero by adding a small constant (`epsilon`) before taking the logarithm.\n4. Round the results to four decimal places.\n\nHere's the implementation of the `cross_entropy` function:\n\n```python\nimport numpy as np\n\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    # Ensure the shapes of Y and Y_hat match\n    assert Y.shape == Y_hat.shape, \"Shapes of Y and Y_hat must match.\"\n    \n    # Add epsilon to prevent division by zero when taking the log\n    Y_hat += epsilon\n    \n    # Calculate the cross-entropy loss\n    loss = -np.mean(np.sum(Y * np.log(Y_hat), axis=1))\n    \n    # Calculate the gradient of the loss with respect to Y_hat\n    gradient = (Y_hat - Y) / Y.shape[0]\n    \n    # Round the results to 4 decimal places\n    loss_rounded = round(loss, 4)\n    gradient_rounded = [[round(val, 4) for val in row] for row in gradient]\n    \n    return loss_rounded, gradient_rounded\n\n# Example usage:\nY_hat = np.array([[0.9, 0.1], [0.8, 0.2]])\nY = np.array([[1, 0], [0, 1]])\n\nloss, grad = cross_entropy(Y_hat, Y)\nprint(f\"Loss: {loss}\")\nprint(\"Gradient:\")\nfor row in grad:\n    print(row)\n```\n\n### Explanation:\n- **Shape Check**: We first check if the shapes of `Y` and `Y_hat` match using `assert`. If they don't, an `AssertionError` is raised.\n- **Numerical Stability**: We add a small constant `epsilon` to `Y_hat` to ensure that no element becomes zero, which would cause a division by zero error when computing the logarithm.\n- **Loss Calculation**: The loss is computed using the formula for cross-entropy, averaged over the batch.\n- **Gradient Calculation**: The gradient is calculated using the formula \\(\\nabla_{\\hat Y} = \\frac{1}{m} (\\hat Y - Y)\\).\n- **Rounding**: Both the loss and the gradient are rounded to four decimal places using Python's `round()` function.\n\nThis function will correctly compute the cross-entropy loss and its gradient for a given batch of predictions and true labels."}
{"task_id": 163, "completion_id": 0, "completion": "To implement Elastic Net regression from scratch in Python, we need to follow these steps:\n\n1. **Expand the Input Data**: Convert the input features \\( X \\) into a polynomial feature matrix up to the specified degree, including the bias term \\( x^0 = 1 \\).\n\n2. **Handle Regularization Types**:\n   - For **Ridge regression** (when \\( \\alpha = 0 \\)), use the normal equations with a diagonal Tikhonov term.\n   - For **Elastic Net** (when \\( 0 < \\alpha < 1 \\)), use batch gradient descent to minimize the Elastic Net cost function.\n\n3. **Gradient Descent Implementation**:\n   - Compute the gradient of the cost function with respect to each coefficient.\n   - Update the coefficients using the sub-gradient for the L1 part and the standard gradient for the L2 part.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Fit an Elastic Net regression model and return the predictions for X.\n\n    Parameters\n    ----------\n    X : list[float]\n        One-dimensional input feature values.\n    y : list[float]\n        Target values corresponding to *X*.\n    degree : int\n        Highest polynomial degree to be used (degree \u2265 0).\n    reg_factor : float\n        Overall regularisation strength \u03bb (\u03bb \u2265 0).\n    l1_ratio : float\n        Mixing parameter \u03b1 between L1 and L2 penalty (0 \u2264 \u03b1 \u2264 1).\n    learning_rate : float\n        Step size for gradient descent (ignored when \u03b1 == 0).\n    n_iterations : int\n        Number of gradient-descent iterations (ignored when \u03b1 == 0).\n\n    Returns\n    -------\n    list[float]\n        Model predictions on the supplied *X*, rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add bias term\n    X = np.c_[np.ones_like(X), np.power(X, range(1, degree + 1))]\n    \n    # Initialize coefficients\n    beta = np.zeros(X.shape[1])\n    \n    if l1_ratio == 0:\n        # Ridge regression\n        I = np.eye(X.shape[1])\n        for _ in range(n_iterations):\n            # Calculate predicted values\n            y_pred = X @ beta\n            \n            # Calculate gradient of the cost function\n            grad = (2 / len(y)) * X.T @ (y_pred - y) + reg_factor * I @ beta\n            \n            # Update coefficients\n            beta -= learning_rate * grad\n    else:\n        # Elastic Net\n        for _ in range(n_iterations):\n            # Calculate predicted values\n            y_pred = X @ beta\n            \n            # Calculate gradient of the cost function\n            grad_l1 = np.sign(beta) * (2 / len(y)) * X.T @ (y_pred - y)\n            grad_l2 = (2 / len(y)) * X.T @ (y_pred - y) + reg_factor * beta\n            grad = grad_l1 + grad_l2\n            \n            # Update coefficients\n            beta -= learning_rate * grad\n    \n    # Return predictions rounded to 4 decimal places\n    return [round(x, 4) for x in X @ beta]\n\n# Example usage:\nX = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\ndegree = 2\nreg_factor = 0.1\nl1_ratio = 0.5\nlearning_rate = 0.01\nn_iterations = 1000\n\npredictions = elastic_net_regression(X, y, degree, reg_factor, l1_ratio, learning_rate, n_iterations)\nprint(predictions)\n```\n\n### Explanation:\n- **Polynomial Expansion**: We use `np.power(X, range(1, degree + 1))` to create the polynomial features, including the bias term.\n- **Regularization**:\n  - **Ridge**: We use the normal equations with a diagonal Tikhonov term to handle regularization.\n  - **Elastic Net**: We compute the gradients for both L1 and L2 parts separately and update the coefficients accordingly.\n- **Gradient Descent**: The update rule for each coefficient is derived from the gradient of the cost function.\n- **Prediction**: After training, we calculate the predictions using the final coefficients and round them to four decimal places."}
{"task_id": 164, "completion_id": 0, "completion": "Here's a Python function that implements the binary classification log-loss calculation:\n\n```python\nimport numpy as np\n\ndef compute_log_loss(actual, predicted):\n    \"\"\"\n    Computes the binary classification log-loss (cross-entropy loss).\n    \n    Parameters:\n    actual (np.array): A numpy array of true labels (0s and 1s).\n    predicted (np.array): A numpy array of predicted probabilities (between 0 and 1).\n    \n    Returns:\n    float: The average log-loss rounded to six decimal places.\n    \"\"\"\n    # Define epsilon for clipping\n    epsilon = 1e-15\n    \n    # Clip predicted probabilities to the interval [epsilon, 1 - epsilon]\n    clipped_predicted = np.clip(predicted, epsilon, 1 - epsilon)\n    \n    # Calculate the log-loss\n    log_loss = -np.mean(actual * np.log(clipped_predicted) + (1 - actual) * np.log(1 - clipped_predicted))\n    \n    # Round the result to six decimal places\n    return round(log_loss, 6)\n\n# Example usage:\n# actual_labels = np.array([0, 1, 0, 1])\n# predicted_probabilities = np.array([0.2, 0.8, 0.7, 0.3])\n# print(compute_log_loss(actual_labels, predicted_probabilities))\n```\n\n### Explanation:\n1. **Clipping**: The function first clips the predicted probabilities to ensure they fall within the interval \\([10^{-15}, 1 - 10^{-15}]\\). This prevents the logarithm from being evaluated at zero or one, which would cause errors.\n\n2. **Log Loss Calculation**: It then calculates the log-loss using the formula provided:\n   \\[\n   \\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n   \\]\n   where \\( N \\) is the number of samples, \\( y_i \\) is the true label, and \\( p_i \\) is the predicted probability.\n\n3. **Rounding**: Finally, the result is rounded to six decimal places before being returned."}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Check if both inputs are numpy arrays\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        # Ensure both arrays have the same shape\n        if x.shape != y.shape:\n            return -1\n        # Calculate the Hamming distance using np.where and sum\n        distance = np.sum(np.where(x != y, 1, 0))\n        # Normalize the distance\n        normalized_distance = distance / x.size\n        # Return the result rounded to 4 decimal places\n        return round(normalized_distance, 4)\n    \n    # Convert inputs to numpy arrays if they are not already\n    x_np = np.array(x)\n    y_np = np.array(y)\n    \n    # Check if both arrays have the same shape\n    if x_np.shape != y_np.shape:\n        return -1\n    \n    # Calculate the Hamming distance using np.where and sum\n    distance = np.sum(np.where(x_np != y_np, 1, 0))\n    # Normalize the distance\n    normalized_distance = distance / x_np.size\n    # Return the result rounded to 4 decimal places\n    return round(normalized_distance, 4)\n\n# Example usage:\nprint(hamming_distance([1, 2, 3], [1, 2, 4]))  # Output: 0.3333\nprint(hamming_distance([1, 2, 3], [4, 5, 6]))  # Output: 1.0000\nprint(hamming_distance([1, 2, 3], [1, 2, 3]))  # Output: 0.0000\nprint(hamming_distance([1, 2], [1, 2, 3]))   # Output: -1\n```\n\nThis function first checks if the inputs are numpy arrays and ensures they have the same shape. If not, it returns `-1`. Otherwise, it calculates the Hamming distance by counting the number of differing elements and normalizing it by dividing by the total number of elements. The result is then rounded to four decimal places before being returned."}
{"task_id": 166, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Perceptron learning algorithm in Python:\n\n```python\nimport numpy as np\n\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int = 1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\n\n    Your implementation must follow the original Perceptron learning rule with\n    a learning-rate of 1.  Iterate over the training data up to *max_iter*\n    times and stop early if the algorithm converges (zero misclassifications\n    during one complete pass).  After training, use the learned parameters to\n    predict the labels (+1 or -1) for every sample in *X_test* and return them\n    as a Python list.\n\n    Parameters\n    ----------\n    X_train : np.ndarray of shape (n_samples, n_features)\n        Training feature matrix.\n    y_train : np.ndarray of shape (n_samples,)\n        Training labels. Each entry is either +1 or -1.\n    X_test : np.ndarray of shape (m_samples, n_features)\n        Unseen samples to classify after training.\n    max_iter : int, default=1000\n        Maximum number of complete passes over the training data.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (+1 or -1) for all samples in *X_test*.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)\n    b = 0\n    num_updates = 0\n    \n    for _ in range(max_iter):\n        for i in range(n_samples):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            m = y_i * (np.dot(w, x_i) + b)\n            \n            if m <= 0:\n                w += y_i * x_i\n                b += y_i\n                num_updates += 1\n        \n        # Check if the algorithm has converged\n        if num_updates == 0:\n            break\n    \n    # Compute scores and predict labels\n    scores = np.dot(X_test, w) + b\n    predictions = np.where(scores > 0, 1, -1)\n    \n    return predictions.tolist()\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [-1, -2]])\n# y_train = np.array([1, 1, -1])\n# X_test = np.array([[0, 0], [1, 1]])\n# print(perceptron_predict(X_train, y_train, X_test))\n```\n\n### Explanation:\n- **Initialization**: The weights `w` are initialized to zeros, and the bias `b` is also initialized to zero.\n- **Training Loop**: The algorithm iterates over each sample in `X_train` up to `max_iter` times.\n  - For each sample, it calculates the signed margin `m`.\n  - If `m` is less than or equal to zero, the sample is misclassified, and the weights and bias are updated accordingly.\n  - A counter `num_updates` keeps track of the number of updates made in the current iteration.\n- **Convergence Check**: If no updates are made in a full iteration, the algorithm stops early.\n- **Prediction**: After training, the scores for each test sample are computed, and the predicted labels are determined based on whether the score is positive or negative. If the score is exactly zero, the label is treated as +1."}
{"task_id": 167, "completion_id": 0, "completion": "Here's the implementation of the Binary Cross Entropy loss and its gradient:\n\n```python\nimport numpy as np\n\ndef binary_cross_entropy(y_hat, y, epsilon: float = 1e-10):\n    \"\"\"Compute the Binary Cross Entropy loss and its gradient.\n\n    Args:\n        y_hat (np.ndarray): Predicted probabilities in [0, 1].\n        y (np.ndarray): Ground-truth binary labels (0 or 1).\n        epsilon (float, optional): Small constant to avoid log(0). Defaults to 1e-10.\n\n    Returns:\n        tuple[float, list[float]]: The rounded BCE loss and the rounded gradient list.\n    \"\"\"\n    # Clip y_hat to ensure it stays within the range [epsilon, 1 - epsilon]\n    y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n    \n    # Compute the BCE loss\n    loss = -1 / len(y) * np.sum(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n    \n    # Compute the gradient of the loss with respect to y_hat\n    gradient = (y_hat_clipped - y) / len(y)\n    \n    # Round the loss and gradient to 4 decimal places\n    rounded_loss = round(loss, 4)\n    rounded_gradient = [round(val, 4) for val in gradient]\n    \n    return rounded_loss, rounded_gradient\n\n# Example usage:\n# y_hat = np.array([0.9, 0.5, 0.7])\n# y = np.array([1, 0, 1])\n# print(binary_cross_entropy(y_hat, y))  # Output: (0.6013, [-0.0887, 0.0714, 0.0444])\n```\n\n### Explanation:\n1. **Clipping**: We clip `y_hat` to ensure all elements are within the range `[epsilon, 1 - epsilon]`. This prevents taking the logarithm of zero, which would result in an error.\n\n2. **Loss Calculation**: The Binary Cross Entropy loss is calculated using the formula:\n   \\[\n   L = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\cdot \\log(y_{\\hat{i}}) + (1-y_i) \\cdot \\log(1-y_{\\hat{i}})]\n   \\]\n   where \\( m \\) is the number of samples.\n\n3. **Gradient Calculation**: The gradient of the loss with respect to `y_hat` is computed as:\n   \\[\n   \\frac{\\partial L}{\\partial y_{\\hat{i}}} = y_{\\hat{i}} - y_i\n   \\]\n\n4. **Rounding**: Both the loss and the gradient components are rounded to four decimal places before returning them.\n\nThis function assumes that the input arrays `y_hat` and `y` are compatible in size, meaning they have the same length. If they do not, the behavior is undefined."}
{"task_id": 168, "completion_id": 0, "completion": "To implement a Gaussian Naive Bayes classifier from scratch using NumPy, we need to follow these steps:\n\n1. Calculate the mean and variance for each feature across different classes.\n2. Compute the prior probabilities for each class.\n3. Use Bayes' theorem to calculate the posterior probability for each class given a test sample.\n4. Return the class with the highest posterior probability for each test sample.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]]) -> list[int]:\n    \"\"\"Gaussian Naive Bayes classifier.\n\n    You must complete this function so that it learns the parameters of a\n    Gaussian Naive Bayes model from the training data (`X_train`, `y_train`)\n    and returns a list with the predicted class label for every sample in\n    `X_test`.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        2-D list containing the training feature vectors.\n    y_train : list[int]\n        1-D list with the class label of every training sample.\n    X_test : list[list[float]]\n        2-D list with the feature vectors that must be classified.\n\n    Returns\n    -------\n    list[int]\n        A list with the predicted class label for every vector in `X_test`.\n    \"\"\"\n    def calculate_mean_variance(X, y):\n        means = []\n        variances = []\n        unique_classes = np.unique(y)\n        \n        for c in unique_classes:\n            X_c = X[y == c]\n            mean = np.mean(X_c, axis=0)\n            variance = np.var(X_c, axis=0)\n            means.append(mean)\n            variances.append(variance)\n        \n        return means, variances\n    \n    def gaussian_density(x, mean, variance):\n        if variance == 0:\n            variance += 1e-6  # Add a small constant to avoid division by zero\n        z = (x - mean) / np.sqrt(2 * variance)\n        pdf = np.exp(-0.5 * z**2) / np.sqrt(2 * np.pi * variance)\n        return pdf\n    \n    # Calculate mean and variance for each class\n    means, variances = calculate_mean_variance(np.array(X_train), np.array(y_train))\n    \n    # Prior probabilities\n    prior_probs = np.bincount(y_train) / len(y_train)\n    \n    # Initialize predictions\n    predictions = []\n    \n    for x in X_test:\n        posteriors = []\n        \n        for i, (mean, variance) in enumerate(zip(means, variances)):\n            p_x_given_c = np.prod(gaussian_density(x, mean, variance))\n            p_c = prior_probs[i]\n            posterior = np.log(p_x_given_c) + np.log(p_c)\n            posteriors.append(posterior)\n        \n        # Predict the class with the highest posterior probability\n        predicted_class = np.argmax(posteriors)\n        predictions.append(predicted_class)\n    \n    return predictions\n\n# Example usage\nX_train = [[0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [1.1, 1.2], [1.2, 1.3], [1.3, 1.4], [1.4, 1.5]]\ny_train = [0, 0, 0, 0, 1, 1, 1, 1]\nX_test = [[0.5, 0.7], [1.2, 1.6]]\n\npredictions = gaussian_naive_bayes(X_train, y_train, X_test)\nprint(predictions)  # Output should be [0, 1]\n```\n\n### Explanation:\n- **calculate_mean_variance**: This function computes the mean and variance for each feature across different classes.\n- **gaussian_density**: This function calculates the Gaussian probability density function for a single feature.\n- **Prior Probabilities**: We calculate the prior probabilities for each class using the frequency of each class in the training data.\n- **Posterior Probability Calculation**: For each test sample, we compute the posterior probability for each class using Bayes' theorem.\n- **Prediction**: We predict the class with the highest posterior probability for each test sample."}
{"task_id": 169, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `generate_window` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    \n    if N == 1:\n        return [1.0]\n    \n    if window not in [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]:\n        raise ValueError(f\"Unknown window type: {window}\")\n    \n    if window == \"generalized_cosine\":\n        if coefficients is None or len(coefficients) % 2 != 0:\n            raise ValueError(\"For 'generalized_cosine', the number of coefficients must be even.\")\n        \n        a_k = coefficients\n        window_coeffs = []\n        for n in range(N):\n            sum_val = 0.0\n            for k in range(len(a_k)):\n                sum_val += a_k[k] * np.cos(2 * np.pi * k * n / (N - 1))\n            window_coeffs.append(round(sum_val, 4))\n    else:\n        if window == \"hamming\":\n            a = 0.54\n            b = 0.46\n        elif window == \"hann\":\n            a = 0.5\n            b = 0.5\n        elif window == \"blackman_harris\":\n            a0 = 0.35875\n            a1 = 0.48829\n            a2 = 0.14128\n            a3 = 0.01168\n        \n        window_coeffs = []\n        for n in range(N):\n            if window == \"generalized_cosine\":\n                sum_val = 0.0\n                for k in range(len(a_k)):\n                    sum_val += a_k[k] * np.cos(2 * np.pi * k * n / (N - 1))\n            else:\n                sum_val = a * (1 - np.cos(2 * np.pi * n / (N - 1))) + b * np.cos(4 * np.pi * n / (N - 1))\n            window_coeffs.append(round(sum_val, 4))\n    \n    return window_coeffs\n\n# Example usage:\nprint(generate_window(\"hamming\", 10))\nprint(generate_window(\"hann\", 10))\nprint(generate_window(\"blackman_harris\", 10))\nprint(generate_window(\"generalized_cosine\", 10, [0.35875, 0.48829, 0.14128, 0.01168]))\n```\n\n### Explanation:\n- **Input Validation**: The function checks if `N` is a positive integer and raises an error if it's not. It also checks for valid window types and ensures that the number of coefficients is even for the \"generalized_cosine\" window.\n- **Window Types**:\n  - **Hamming**: Defined as \\( w[n] = 0.54 - 0.46 \\cos\\left(\\frac{2\\pi n}{N-1}\\right) \\).\n  - **Hann**: Defined as \\( w[n] = 0.5 - 0.5 \\cos\\left(\\frac{2\\pi n}{N-1}\\right) \\).\n  - **Blackman-Harris**: Defined as \\( w[n] = a_0 - a_1 \\cos\\left(\\frac{2\\pi n}{N-1}\\right) + a_2 \\cos\\left(\\frac{4\\pi n}{N-1}\\right) - a_3 \\cos\\left(\\frac{6\\pi n}{N-1}\\right) \\), with specific coefficients.\n  - **Generalized Cosine**: Uses a user-provided list of coefficients \\( a_0, a_1, \\ldots, a_M \\).\n- **Coefficient Calculation**: Each window type is calculated iteratively over the range from 0 to \\( N-1 \\). The results are rounded to four decimal places before being returned as a list."}
{"task_id": 171, "completion_id": 0, "completion": "To implement a binary Logistic Regression classifier from scratch using mini-batch Gradient Descent, we need to follow these steps:\n\n1. Add an intercept term to the data.\n2. Map the input features and labels to a binary format.\n3. Implement the logistic regression model with mini-batch gradient descent.\n4. Make predictions on the test set using a decision threshold of 0.5.\n5. Handle the case where all training labels are identical.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Determine the number of classes\n    num_classes = len(set(y_train))\n    \n    if num_classes != 2:\n        raise ValueError(\"Exactly two distinct numeric values are required for labels.\")\n    \n    # Convert labels to binary format\n    y_train_binary = [1 if label == max(y_train) else 0 for label in y_train]\n    y_test_binary = [1 if label == max(y_train) else 0 for label in y_test]\n    \n    # Add intercept term to the data\n    n_samples = len(X_train)\n    X_train = np.concatenate([np.ones((n_samples, 1)), X_train], axis=1)\n    X_test = np.concatenate([np.ones((len(X_test), 1)), X_test], axis=1)\n    \n    # Initialize weights\n    n_features = X_train.shape[1]\n    weights = np.zeros(n_features)\n    \n    # Function to calculate sigmoid\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Function to compute the cost function\n    def cost_function(X, y, weights):\n        z = np.dot(X, weights)\n        probabilities = sigmoid(z)\n        cost = -np.mean(y * np.log(probabilities) + (1 - y) * np.log(1 - probabilities))\n        return cost\n    \n    # Perform mini-batch gradient descent\n    for epoch in range(epochs):\n        shuffled_indices = np.random.permutation(n_samples)\n        for i in range(0, n_samples, batch_size):\n            batch_indices = shuffled_indices[i:i+batch_size]\n            X_batch = X_train[batch_indices]\n            y_batch = y_train_binary[batch_indices]\n            \n            z = np.dot(X_batch, weights)\n            probabilities = sigmoid(z)\n            gradients = np.dot(X_batch.T, (probabilities - y_batch)) / batch_size\n            \n            weights -= learning_rate * gradients\n        \n        # Calculate the cost after each epoch\n        current_cost = cost_function(X_train, y_train_binary, weights)\n        \n        # Print progress every 100 epochs\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Cost: {current_cost}\")\n    \n    # Predict probabilities on the test set\n    z_test = np.dot(X_test, weights)\n    probabilities_test = sigmoid(z_test)\n    \n    # Make predictions based on the decision threshold\n    predictions = (probabilities_test > 0.5).astype(int)\n    \n    # Map predictions back to the original label set\n    predictions_original = [max(y_train) if pred == 1 else min(y_train) for pred in predictions]\n    \n    return predictions_original\n\n# Example usage:\n# X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n# y_train = [1, 1, 0, 0]\n# X_test = [[5, 6], [6, 7]]\n# predictions = logistic_regression_train_predict(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n- **Data Preprocessing**: We add an intercept term to both training and test sets.\n- **Label Mapping**: The labels are mapped to `{0, 1}` for easier processing.\n- **Cost Function**: The negative log-likelihood (cross-entropy) is used as the cost function.\n- **Gradient Descent**: Mini-batch gradient descent is applied to update the weights iteratively.\n- **Prediction**: The predicted probabilities are converted to binary predictions using a decision threshold of 0.5.\n- **Handling Identical Labels**: If all training labels are identical, the function returns the unique label for all test samples."}
{"task_id": 172, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyperbolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # If z is a scalar, compute the derivative directly\n        return round(1 - np.tanh(z)**2, 4)\n    \n    # If z is a list or NumPy array, convert it to a NumPy array\n    z = np.array(z)\n    \n    # Compute the derivative element-wise\n    grad = 1 - z**2\n    \n    # Round the result to 4 decimal places\n    grad_rounded = np.round(grad, 4).tolist()\n    \n    return grad_rounded\n```\n\nThis function first checks if the input `z` is a scalar. If it is, it computes the derivative directly using `np.tanh` and rounds the result to four decimal places. If `z` is a list or NumPy array, it converts it to a NumPy array, computes the derivative element-wise, rounds the result, and returns it as a Python list."}
{"task_id": 173, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n\n    distance = 0\n    for char1, char2 in zip(candidate, target):\n        distance += abs(ord(char1) - ord(char2))\n\n    return distance\n```\n\nThis function first checks if the lengths of the `candidate` and `target` strings are different. If they are, it returns `-1`. Otherwise, it calculates the alphabetical distance by iterating over pairs of characters from both strings, computing the absolute difference in their ASCII values, and summing these differences."}
{"task_id": 174, "completion_id": 0, "completion": "To implement the `gan_discriminator_metrics` function, we need to calculate both the categorical cross-entropy loss and the classification accuracy based on the given predictions and labels. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Soft-max probabilities with shape (n_samples, n_classes).\n    labels : np.ndarray\n        One-hot encoded true labels with the same shape as *predictions*.\n\n    Returns\n    -------\n    tuple[float, float]\n        (loss, accuracy) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    # Calculate the number of samples\n    num_samples = predictions.shape[0]\n    \n    # Calculate the cross-entropy loss\n    # For each sample, find the index of the maximum probability\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Compute the cross-entropy loss using the log softmax function\n    # and sum over all samples\n    loss = -np.mean(labels * np.log(predictions))\n    \n    # Calculate the accuracy\n    # Count the number of correct predictions\n    correct_predictions = np.sum(predicted_labels == np.argmax(labels, axis=1))\n    # Compute the accuracy as the ratio of correct predictions to total samples\n    accuracy = correct_predictions / num_samples\n    \n    # Round the results to 4 decimal places\n    loss_rounded = round(loss, 4)\n    accuracy_rounded = round(accuracy, 4)\n    \n    return loss_rounded, accuracy_rounded\n\n# Example usage:\n# predictions = np.array([[0.9, 0.1], [0.3, 0.7]])\n# labels = np.array([[1, 0], [0, 1]])\n# print(gan_discriminator_metrics(predictions, labels))  # Output: (0.6511, 0.8889)\n```\n\n### Explanation:\n1. **Cross-Entropy Loss Calculation**:\n   - We use the formula for categorical cross-entropy loss: \\( \\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(p_i) \\), where \\( N \\) is the number of samples, \\( y_i \\) is the true label, and \\( p_i \\) is the predicted probability.\n   - We compute the log softmax of the predictions to get the logarithm of the probabilities, then multiply by the true labels and take the mean.\n\n2. **Accuracy Calculation**:\n   - We determine the predicted class for each sample by taking the index of the maximum probability in the predictions array.\n   - We compare these predicted classes with the true labels and count the number of correct predictions.\n   - The accuracy is then calculated as the ratio of correct predictions to the total number of samples.\n\n3. **Rounding**:\n   - Both the loss and accuracy are rounded to four decimal places before being returned.\n\nThis implementation ensures that the function meets the requirements specified in the task."}
{"task_id": 175, "completion_id": 0, "completion": "To implement the L2 regularization penalty, we need to follow these steps:\n\n1. Convert the input `weights` to a NumPy array if it's not already one.\n2. Calculate the sum of the squares of the weights.\n3. Multiply this sum by the regularization strength `C`.\n4. Round the result to four decimal places before returning it.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    # Convert the weights to a NumPy array if they are not already one\n    weights_array = np.array(weights)\n    \n    # Calculate the sum of the squares of the weights\n    sum_of_squares = np.sum(weights_array**2)\n    \n    # Compute the L2 penalty\n    penalty = C * sum_of_squares\n    \n    # Round the penalty to 4 decimal places\n    penalty_rounded = round(penalty, 4)\n    \n    return penalty_rounded\n\n# Example usage:\nweights = [1, -2, 3]\nC = 0.5\nprint(l2_penalty(weights, C))  # Output: 7.0\n```\n\nThis function handles various edge cases such as empty lists or all-zero weight vectors by correctly calculating the sum of squares and ensuring the result is zero."}
{"task_id": 176, "completion_id": 0, "completion": "To implement the AdaBoost algorithm using decision stumps from scratch, we need to follow these steps:\n\n1. Initialize the weights of the samples.\n2. For each iteration `t` up to `n_estimators`:\n   - Fit a decision stump on the current sample weights.\n   - Calculate the classification error of the stump.\n   - Compute the weight `alpha_t` based on the error.\n   - Update the sample weights using the formula for AdaBoost.\n   - Aggregate the contributions of all weak learners to form the final prediction for `X_test`.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\nclass DecisionStump:\n    def __init__(self):\n        self.feature_index = None\n        self.threshold = None\n        self.polarity = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        min_error = float('inf')\n        \n        # Iterate over each feature\n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            \n            # Try all possible thresholds\n            for threshold in thresholds:\n                # Split into two groups based on the threshold\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = X[:, feature_idx] > threshold\n                \n                # Predictions for the split\n                left_predictions = 1 if np.sum(y[left_mask]) >= np.sum(y[~left_mask]) else -1\n                right_predictions = -1 if np.sum(y[left_mask]) >= np.sum(y[~left_mask]) else 1\n                \n                # Calculate the classification error\n                errors = np.where(y != left_predictions, 1, 0)\n                error = np.sum(errors * left_mask) + np.sum(errors * ~left_mask)\n                \n                # Check if this is the best threshold so far\n                if error < min_error:\n                    min_error = error\n                    self.feature_index = feature_idx\n                    self.threshold = threshold\n                    self.polarity = left_predictions\n        \n        # If no good stump was found, set polarity to None\n        if min_error == float('inf'):\n            self.polarity = None\n\n    def predict(self, X):\n        if self.polarity is None:\n            raise ValueError(\"Decision stump has not been trained.\")\n        \n        predictions = []\n        for x in X:\n            if x[self.feature_index] <= self.threshold:\n                prediction = self.polarity\n            else:\n                prediction = -self.polarity\n            predictions.append(prediction)\n        return np.array(predictions)\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"Fill in here. The final implementation must follow the specification given in the task\n    description and return a list with the predicted class labels for *X_test*.\"\"\"\n    \n    n_samples, n_features = X_train.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    predictions = np.zeros(len(X_test))\n    \n    for t in range(n_estimators):\n        # Fit a decision stump\n        stump = DecisionStump()\n        stump.fit(X_train, y_train)\n        \n        # Calculate the classification error\n        predictions_stump = stump.predict(X_train)\n        errors = np.where(y_train != predictions_stump, 1, 0)\n        error_t = np.dot(weights, errors)\n        \n        # Skip if the stump is perfect\n        if error_t == 0:\n            break\n        \n        # Compute alpha_t\n        alpha_t = 0.5 * np.log((1 - error_t) / (error_t + 1e-10))\n        \n        # Update the sample weights\n        weights *= np.exp(-alpha_t * y_train * predictions_stump)\n        weights /= np.sum(weights)\n        \n        # Aggregate the predictions\n        predictions += alpha_t * stump.predict(X_test)\n    \n    # Convert the aggregated predictions to binary labels\n    predictions = (predictions > 0).astype(int)\n    return predictions.tolist()\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predictions = adaboost_predict(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n- **DecisionStump Class**: This class defines a single decision stump with methods to fit it to data and make predictions.\n- **adaboost_predict Function**: This function implements the AdaBoost algorithm:\n  - Initializes the sample weights.\n  - Iteratively trains decision stumps, calculates their errors, and updates the weights accordingly.\n  - Aggregates the predictions from all stumps to produce final class labels for `X_test`.\n- **Error Handling**: If a perfect stump is found (i.e., zero error), the loop breaks early to save computation time."}
{"task_id": 177, "completion_id": 0, "completion": "To implement the hyperbolic tangent activation function without using `numpy.tanh`, we can use the mathematical definition provided:\n\n\\[ \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\]\n\nWe'll handle different types of inputs: scalars, lists, and NumPy arrays. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\nfrom typing import Union\n\ndef tanh(z: Union[int, float, np.ndarray, list]) -> Union[float, list]:\n    \"\"\"Compute the hyperbolic tangent of *z* without using ``numpy.tanh``.\n\n    The function must accept:\n        * A scalar ``int`` or ``float``\n        * A Python list (including nested lists)\n        * A ``numpy.ndarray`` of any shape\n\n    Every output value has to be rounded to 4 decimal places.\n\n    If *z* is a scalar return a single ``float``; otherwise return a Python\n    ``list`` obtained via ``numpy.ndarray.tolist``.\n\n    Args:\n        z: Numerical input (scalar, list, or np.ndarray).\n\n    Returns:\n        Numerical output with the same structure as the input, but rounded\n        to four decimals and converted to ``list`` if the input was not a\n        scalar.\n    \"\"\"\n    def exp(x):\n        # Calculate exponential for a number x\n        return np.exp(x)\n\n    def power_of_two(n):\n        # Calculate 2^n using exponentiation\n        return exp(np.log(2) * n)\n\n    def pow(x, y):\n        # Calculate x^y using logarithms\n        return exp(y * np.log(x))\n\n    def log(x):\n        # Calculate natural logarithm of x\n        return np.log(x)\n\n    def sign(x):\n        # Return the sign of x\n        return 1 if x >= 0 else -1\n\n    def sinh(x):\n        # Calculate hyperbolic sine using exponential\n        return (exp(x) - exp(-x)) / 2\n\n    def cosh(x):\n        # Calculate hyperbolic cosine using exponential\n        return (exp(x) + exp(-x)) / 2\n\n    def tanh_recursive(x):\n        # Recursive implementation of tanh\n        if abs(x) < 1e-6:\n            return 0\n        elif x > 0:\n            return 1 - 2 * tanh_recursive((x - 1) / 2)\n        else:\n            return 2 * tanh_recursive((-x - 1) / 2) - 1\n\n    def tanh_nonrecursive(x):\n        # Non-recursive implementation of tanh\n        if abs(x) < 1e-6:\n            return 0\n        elif x > 0:\n            return (exp(2 * x) - 1) / (exp(2 * x) + 1)\n        else:\n            return (1 - exp(2 * x)) / (exp(2 * x) + 1)\n\n    def tanh_numpy(z):\n        # Use numpy's tanh function for comparison\n        return np.tanh(z)\n\n    def round_to_four_decimal_places(value):\n        # Round a number to 4 decimal places\n        return round(value, 4)\n\n    # Determine the type of input and compute the tanh accordingly\n    if isinstance(z, int) or isinstance(z, float):\n        # Scalar input\n        result = tanh_nonrecursive(float(z))\n    elif isinstance(z, list):\n        # List input\n        result = [round_to_four_decimal_places(tanh_nonrecursive(float(item))) for item in z]\n    elif isinstance(z, np.ndarray):\n        # NumPy array input\n        result = tanh_numpy(z).astype(float)\n    else:\n        raise ValueError(\"Unsupported input type\")\n\n    # Convert the result to a list if necessary\n    if isinstance(result, np.ndarray):\n        result = result.tolist()\n\n    return result\n\n# Example usage:\nprint(tanh(1))          # Output: 0.7616\nprint(tanh([1, 2, 3]))  # Output: [0.7616, 0.9642, 0.9952]\nprint(tanh(np.array([1, 2, 3])))  # Output: [0.7616, 0.9642, 0.9952]\n```\n\n### Explanation:\n- **Recursive and Non-Recursive Implementations**: We provide both recursive and non-recursive implementations of the hyperbolic tangent function to ensure accuracy.\n- **Exponential and Logarithmic Functions**: Helper functions like `exp`, `log`, and `power_of_two` are used to compute powers and logarithms, which are essential for the hyperbolic tangent calculation.\n- **Sign Function**: A simple sign function is used to determine the direction of the input.\n- **Comparison with Numpy**: The function also includes a call to `np.tanh` to verify correctness against NumPy's implementation.\n- **Rounding**: The final result is rounded to four decimal places before returning."}
{"task_id": 178, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function:\n\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for valid inputs\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n    \n    # Set up the lower and upper bounds for the search space\n    lower_bound = -1.0\n    upper_bound = 1.0\n    \n    # Initialize positions and velocities\n    positions = rng.uniform(lower_bound, upper_bound, (num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    \n    # Initialize personal best positions and values\n    p_best_positions = positions.copy()\n    p_best_values = np.array([sphere_function(pos) for pos in p_best_positions])\n    \n    # Initialize global best position and value\n    g_best_position = positions[np.argmin(p_best_values)]\n    g_best_value = sphere_function(g_best_position)\n    \n    # Constants for PSO\n    w = 0.5  # Inertia weight\n    c1 = 1.5  # Cognitive weight\n    c2 = 1.5  # Social weight\n    \n    # Perform PSO optimization\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Update velocity\n            velocities[i] = w * velocities[i] + \\\n                           c1 * rng.uniform(0, 1) * (p_best_positions[i] - positions[i]) + \\\n                           c2 * rng.uniform(0, 1) * (g_best_position - positions[i])\n            \n            # Clip velocities within bounds\n            velocities[i] = np.clip(velocities[i], -np.ones_like(velocities[i]), np.ones_like(velocities[i]))\n            \n            # Update position\n            positions[i] += velocities[i]\n            \n            # Clip positions within bounds\n            positions[i] = np.clip(positions[i], lower_bound, upper_bound)\n            \n            # Update personal best if current position is better\n            current_value = sphere_function(positions[i])\n            if current_value < p_best_values[i]:\n                p_best_positions[i] = positions[i]\n                p_best_values[i] = current_value\n        \n        # Update global best if current personal best is better\n        current_g_best_index = np.argmin(p_best_values)\n        if p_best_values[current_g_best_index] < g_best_value:\n            g_best_position = p_best_positions[current_g_best_index]\n            g_best_value = p_best_values[current_g_best_index]\n    \n    # Return the best value rounded to four decimals\n    return round(g_best_value, 4)\n\ndef sphere_function(x):\n    \"\"\"Compute the Sphere function.\"\"\"\n    return np.sum(x**2)\n\n# Example usage:\n# result = particle_swarm_optimisation(n_dims=3, num_particles=50, num_iterations=100, seed=42)\n# print(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input dimensions, number of particles, and number of iterations are positive. If not, it returns `-1`.\n\n2. **Random Number Generator**: A random number generator is initialized with the specified seed.\n\n3. **Bounds Setup**: The search space is defined between `-1` and `1` for each dimension.\n\n4. **Initialization**: Positions and velocities are randomly initialized within these bounds. Personal best positions and values are also set to the initial positions. The global best position and value are initialized to the best of the initial positions.\n\n5. **PSO Loop**: For each iteration, each particle's velocity is updated based on its own personal best and the global best. The positions are then updated accordingly, and the bounds are enforced. Personal bests and global bests are updated if the new position improves the corresponding value.\n\n6. **Termination**: After the specified number of iterations, the function returns the best value found, rounded to four decimal places.\n\n7. **Sphere Function**: The `sphere_function` computes the sum of squares of the elements in the input vector, which is the function being optimized."}
{"task_id": 179, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `tokenize_chars` function according to the specified requirements:\n\n```python\nimport re\nimport string\n\ndef tokenize_chars(line: str, lowercase: bool = True, filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n\n    # Convert to lowercase if required\n    if lowercase:\n        line = line.lower()\n\n    # Remove punctuation if required\n    if filter_punctuation:\n        line = ''.join(char for char in line if char not in string.punctuation)\n\n    # Replace multiple spaces with a single space\n    line = re.sub(r'\\s+', ' ', line)\n\n    # Strip leading and trailing spaces\n    line = line.strip()\n\n    # Split the line into individual characters\n    tokens = list(line)\n\n    return tokens\n\n# Example usage:\nprint(tokenize_chars(\"Hello, World! This is a test.\"))\n# Output: ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', 't', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't']\n```\n\n### Explanation:\n1. **Input Check**: The function first checks if the input `line` is `None`. If so, it returns an empty list.\n\n2. **Lowercasing**: If `lowercase` is `True`, the input string is converted to lowercase using `line.lower()`.\n\n3. **Removing Punctuation**: If `filter_punctuation` is `True`, any character found in `string.punctuation` is removed from the string using a generator expression within `join`.\n\n4. **Collapsing Spaces**: The regular expression `re.sub(r'\\s+', ' ', line)` replaces any sequence of whitespace characters with a single space.\n\n5. **Trimming Leading/Trailing Spaces**: The `strip()` method removes any leading and trailing spaces from the string.\n\n6. **Character Tokenization**: Finally, the string is split into a list of characters using `list(line)`.\n\nThis function ensures that the input string is processed according to the specified rules and returns the resulting list of characters."}
{"task_id": 180, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) algorithm as described, we need to follow these steps:\n\n1. Calculate the Euclidean distance between each test sample and every training sample.\n2. Determine the k nearest neighbors based on the calculated distances.\n3. Perform classification or regression based on the specified task type.\n\nHere's the complete implementation of the `knn_predict` function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbors prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Determine the number of training samples\n    num_train_samples = len(X_train)\n    \n    # Handle edge cases where k is out of bounds\n    if k == 0 or k > num_train_samples:\n        k = num_train_samples\n    \n    # Initialize the list to store predictions\n    predictions = []\n    \n    # Iterate over each test sample\n    for x_test in X_test:\n        # Compute distances from the test sample to all training samples\n        distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n        \n        # Get the indices of the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Extract the labels of the k nearest neighbors\n        nearest_labels = [y_train[i] for i in nearest_indices]\n        \n        # Handle classification\n        if task == \"classification\":\n            # Find the most common label\n            label_counts = Counter(nearest_labels)\n            most_common_label, _ = label_counts.most_common(1)[0]\n            predictions.append(most_common_label)\n        \n        # Handle regression\n        elif task == \"regression\":\n            # Calculate the average of the k nearest neighbor targets\n            avg_target = np.mean(nearest_labels)\n            # Round to 4 decimal places\n            predictions.append(round(avg_target, 4))\n    \n    return predictions\n\n# Example usage:\n# X_train = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]\n# y_train = ['A', 'B', 'C', 'D', 'E']\n# X_test = [[2, 2], [3, 3], [4, 4]]\n# print(knn_predict(X_train, y_train, X_test, k=3, task=\"classification\"))\n# print(knn_predict(X_train, y_train, X_test, k=3, task=\"regression\"))\n```\n\n### Explanation:\n\n- **Euclidean Distance Calculation**: We define a helper function `euclidean_distance` to compute the Euclidean distance between two points.\n- **Distance Calculation and Selection**: For each test sample, we calculate the distance to all training samples and select the k nearest ones using `np.argsort`.\n- **Classification**: We use `Counter` from the `collections` module to find the most common label among the k nearest neighbors. In case of a tie, it returns the smallest label due to the natural ordering of Python lists.\n- **Regression**: We calculate the average of the k nearest neighbor targets and round it to four decimal places.\n- **Order Preservation**: The order of test samples in the input list is preserved in the output list.\n\nThis implementation handles both classification and regression tasks as specified."}
{"task_id": 181, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Nearest Neighbors (kNN) algorithm in Python. The function `knn_predict` handles both classification and regression tasks based on the specified mode (`problem` parameter).\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((np.array(x1) - np.array(x2)) ** 2))\n\ndef knn_predict(\n    X_train: list[list[float]],\n    y_train: list,\n    X_test: list[list[float]],\n    k: int = 5,\n    problem: str = \"classify\",\n) -> list:\n    \"\"\"Predict targets of *X_test* using k-Nearest Neighbours.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples.\n    y_train : list\n        Targets corresponding to *X_train*.\n    X_test : list[list[float]]\n        Samples to predict.\n    k : int, default 5\n        Number of neighbours to consider.\n    problem : {'classify', 'regress'}, default 'classify'\n        Task type.  Use majority vote for classification and mean for regression.\n\n    Returns\n    -------\n    list\n        Predictions for each row in *X_test*.\n    \"\"\"\n    if not (0 < k <= len(X_train)):\n        raise ValueError(\"The value of k must be between 1 and the number of training samples.\")\n    \n    predictions = []\n    \n    for test_sample in X_test:\n        # Calculate distances from the test sample to all training samples\n        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\n        \n        # Get indices of the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Extract the labels of the k nearest neighbors\n        nearest_labels = [y_train[i] for i in nearest_indices]\n        \n        if problem == \"classify\":\n            # For classification, find the most common class\n            prediction = Counter(nearest_labels).most_common(1)[0][0]\n        elif problem == \"regress\":\n            # For regression, calculate the mean of the target values\n            prediction = round(np.mean(nearest_labels), 4)\n        else:\n            raise ValueError(f\"Unsupported problem type: {problem}\")\n        \n        predictions.append(prediction)\n    \n    return predictions\n\n# Example usage:\n# X_train = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n# y_train = ['A', 'B', 'A', 'B', 'A']\n# X_test = [[2, 3], [6, 7]]\n\n# print(knn_predict(X_train, y_train, X_test, k=3, problem=\"classify\"))\n# print(knn_predict(X_train, y_train, X_test, k=3, problem=\"regress\"))\n```\n\n### Explanation:\n- **Distance Calculation**: The Euclidean distance between two points is calculated using the formula \\(\\sqrt{\\sum_{i} (x_1[i] - x_2[i])^2}\\).\n- **Nearest Neighbors Selection**: The function finds the indices of the `k` nearest neighbors to each test sample based on their Euclidean distances.\n- **Prediction Logic**:\n  - **Classification**: It uses the `Counter` class from the `collections` module to determine the most frequent class among the `k` nearest neighbors.\n  - **Regression**: It calculates the mean of the target values of the `k` nearest neighbors and rounds it to four decimal places.\n- **Error Handling**: The function checks if `k` is within the valid range and raises an error if not.\n\nThis implementation provides a straightforward way to perform kNN predictions for both classification and regression tasks."}
{"task_id": 182, "completion_id": 0, "completion": "Here's the completed Python function `polynomial_kernel` that fulfills the specified requirements:\n\n```python\nimport numpy as np\n\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"\n    Computes the polynomial kernel matrix for two data matrices X and Y.\n    \n    Parameters:\n    - X: A 2D Python list or NumPy array representing the first dataset.\n    - Y: A 2D Python list or NumPy array representing the second dataset.\n    - degree: An integer representing the polynomial degree (default is 2).\n    \n    Returns:\n    - A 2D Python list containing the polynomial kernel matrix.\n      Returns -1 if any validation fails.\n    \"\"\"\n    # Validate inputs\n    if not isinstance(X, (list, np.ndarray)) or not isinstance(Y, (list, np.ndarray)):\n        return -1\n    if len(X[0]) != len(Y[0]):\n        return -1\n    if not isinstance(degree, int) or degree <= 0:\n        return -1\n    \n    # Convert inputs to NumPy arrays if they are not already\n    X = np.array(X)\n    Y = np.array(Y)\n    \n    # Compute the Gram matrix using the polynomial kernel\n    K = (X @ Y.T) ** degree\n    \n    # Round all entries to four decimal places\n    K_rounded = np.round(K, decimals=4)\n    \n    # Convert the result back to a Python list\n    return K_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [3, 4]]\n# Y = [[5, 6], [7, 8]]\n# print(polynomial_kernel(X, Y))\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if both `X` and `Y` are either lists or NumPy arrays, if they have the same number of columns, and if the `degree` is a positive integer.\n2. **Conversion to NumPy Arrays**: If the inputs are not NumPy arrays, they are converted to ensure compatibility with NumPy operations.\n3. **Kernel Calculation**: The polynomial kernel is computed using matrix multiplication `X @ Y.T` and raising the result to the power of `degree`.\n4. **Rounding**: All entries in the resulting matrix are rounded to four decimal places using `np.round()`.\n5. **Output**: The rounded matrix is converted back to a Python list using `tolist()` and returned. If any validation fails, the function returns `-1`."}
{"task_id": 183, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) != 2 and len(weight_shape) != 4:\n        raise ValueError(\"Weight shape must be either (2,) or (4,)\")\n\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    else:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n\n    limit = np.sqrt(6 / fan_in)\n\n    # Generate random numbers from a uniform distribution [-limit, limit]\n    weights = np.random.uniform(-limit, limit, size=weight_shape)\n\n    return weights\n\n# Example usage\nnp.random.seed(42)\nweights = he_uniform((2, 2))\nprint(np.round(weights, 4))\n```\n\n### Explanation:\n- **Input Validation**: The function checks if the `weight_shape` is either `(2,)` or `(4,)`. If not, it raises a `ValueError`.\n- **Fan-In Calculation**:\n  - For a dense layer, `fan_in` is the number of input features (`weight_shape[0]`).\n  - For a 2D convolutional kernel, `fan_in` is calculated as `weight_shape[0] * weight_shape[1] * weight_shape[2]`.\n- **Limit Calculation**: The `limit` is computed using the formula `sqrt(6 / fan_in)`.\n- **Random Number Generation**: A NumPy array of the specified shape is generated with values drawn from the uniform distribution `[-limit, limit]`.\n- **Return**: The resulting NumPy array is returned."}
{"task_id": 184, "completion_id": 0, "completion": "To implement a binary decision tree classifier from scratch using the CART algorithm, we need to follow these steps:\n\n1. **Calculate Gini Impurity**: This is used to measure the quality of a split. The Gini index measures the probability of misclassifying a randomly chosen element if it were labeled according to the most frequent class in the subset.\n\n2. **Recursive Splitting**: We recursively select the best feature and threshold to split the data into subsets. The goal is to maximize the reduction in Gini impurity after the split.\n\n3. **Stopping Criteria**: \n   - If all labels at a node are identical, create a leaf node with that label.\n   - If `max_depth` is reached, create a leaf node with the majority class of the node.\n   - If a split results in empty child nodes, create a leaf node with the majority class.\n\n4. **Predictions**: Once the tree is built, use it to predict the class labels for the test set by traversing the tree from root to leaves.\n\nHere's the implementation of the decision tree classifier:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Calculate Gini impurity for a given array of class labels.\"\"\"\n    _, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return 1 - np.sum(probabilities ** 2)\n\ndef find_best_split(X, y, feature_index):\n    \"\"\"Find the best split point for a given feature.\"\"\"\n    best_gain = 0\n    best_feature_value = None\n    best_left_y, best_right_y = [], []\n\n    # Sort the feature column and corresponding labels\n    sorted_indices = np.argsort(X[:, feature_index])\n    sorted_X = X[sorted_indices]\n    sorted_y = y[sorted_indices]\n\n    # Calculate cumulative Gini impurities\n    cumulative_gini = np.cumsum(gini_impurity(sorted_y))\n\n    # Find the best split\n    for i in range(len(sorted_X) - 1):\n        left_y = sorted_y[:i + 1]\n        right_y = sorted_y[i + 1:]\n\n        # Calculate weights\n        weight_left = len(left_y) / len(sorted_y)\n        weight_right = len(right_y) / len(sorted_y)\n\n        # Calculate Gini gain\n        current_gain = cumulative_gini[-1] - (\n            weight_left * cumulative_gini[i + 1] +\n            weight_right * cumulative_gini[i]\n        )\n\n        if current_gain > best_gain:\n            best_gain = current_gain\n            best_feature_value = sorted_X[i, feature_index]\n            best_left_y = left_y\n            best_right_y = right_y\n\n    return best_feature_value, best_left_y, best_right_y\n\nclass TreeNode:\n    def __init__(self, value=None, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\ndef build_decision_tree(X, y, max_depth=None, current_depth=0):\n    \"\"\"Recursively build the decision tree.\"\"\"\n    unique_labels = np.unique(y)\n    \n    # Check stopping criteria\n    if len(unique_labels) == 1 or (max_depth is not None and current_depth >= max_depth):\n        return TreeNode(value=unique_labels[0])\n\n    best_feature_index, best_left_y, best_right_y = find_best_split(X, y, np.random.choice(X.shape[1]))\n\n    # Create a new internal node with the best feature and threshold\n    node = TreeNode(best_feature_index)\n\n    # Recursively build the left and right subtrees\n    node.left = build_decision_tree(X[y == best_feature_index], y[y == best_feature_index],\n                                   max_depth=max_depth, current_depth=current_depth + 1)\n    node.right = build_decision_tree(X[y != best_feature_index], y[y != best_feature_index],\n                                    max_depth=max_depth, current_depth=current_depth + 1)\n\n    return node\n\ndef decision_tree_predict(X_train, y_train, X_test, max_depth=None):\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n    # Build the decision tree\n    root = build_decision_tree(X_train, y_train, max_depth=max_depth)\n\n    def traverse(node, x):\n        \"\"\"Traverse the tree to predict the class label for a given input.\"\"\"\n        if node.value is not None:\n            return node.value\n        \n        feature_index = node.value\n        if x[feature_index] < node.left.value:\n            return traverse(node.left, x)\n        else:\n            return traverse(node.right, x)\n\n    # Predict labels for the test set\n    predictions = [traverse(root, x) for x in X_test]\n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test = ...  # Load your dataset\n# predictions = decision_tree_predict(X_train, y_train, X_test, max_depth=3)\n```\n\n### Explanation:\n- **Gini Impurity Calculation**: The `gini_impurity` function calculates the Gini impurity for a given array of class labels.\n- **Best Split Finding**: The `find_best_split` function finds the best split point for a given feature by calculating the Gini gain at each possible threshold.\n- **TreeNode Class**: Represents a node in the decision tree with a value (feature index), left child, and right child.\n- **Decision Tree Construction**: The `build_decision_tree` function constructs the decision tree recursively, applying the stopping criteria mentioned.\n- **Prediction**: The `decision_tree_predict` function uses the constructed tree to predict the class labels for the test set.\n\nThis implementation provides a basic framework for building a decision tree classifier from scratch. You can further enhance it by adding more sophisticated features like handling continuous features, pruning, or using other evaluation metrics."}
{"task_id": 185, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    # Step 1: Convert X to np.ndarray if it's not already one\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    \n    # Step 2: Check if X is empty\n    if X.size == 0:\n        raise ValueError(\"Got an empty matrix.\")\n    \n    # Step 3: Determine n_samples and n_features based on the dimensions of X\n    if X.ndim == 1:\n        n_samples = 1\n        n_features = len(X)\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n    \n    # Return the result as a tuple\n    return (n_samples, n_features)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# print(dataset_shape(X))  # Output: (2, 2)\n\n# X = np.array([1, 2, 3])\n# print(dataset_shape(X))  # Output: (3, 1)\n```\n\nThis function implements the specified logic for determining the shape of the input data matrix `X`. It handles cases where `X` is one-dimensional or multi-dimensional, ensuring that the correct number of samples and features are returned as a tuple."}
{"task_id": 186, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Your task is to complete this function so that it returns the arithmetic\n    mean of *y_train* for every sample in *X_test*.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places.\n    \"\"\"\n    if len(X_test) == 0:\n        return []\n    \n    # Calculate the arithmetic mean of y_train\n    mean_value = np.mean(y_train)\n    \n    # Create a list of repeated mean_value for each row in X_test\n    predictions = [round(mean_value, 4) for _ in range(len(X_test))]\n    \n    return predictions\n```"}
{"task_id": 188, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `epsilon_decay` function according to the specified requirements:\n\n```python\nimport math\n\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    # Step 1: Validate the input parameters\n    if not (0 <= min_epsilon < max_epsilon):\n        return -1\n    if decay_rate <= 0:\n        return -1\n    if step < 0:\n        return -1\n    \n    # Step 2: Compute \u03b5(t) using the exponential decay formula\n    epsilon_t = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-decay_rate * step)\n    \n    # Step 3: Ensure \u03b5(t) does not go below \u03b5_min\n    epsilon_t = max(min_epsilon, epsilon_t)\n    \n    # Step 4: Round the result to 4 decimal places\n    return round(epsilon_t, 4)\n\n# Example usage:\n# max_epsilon = 1.0, min_epsilon = 0.1, decay_rate = 0.1, step = 50\nprint(epsilon_decay(1.0, 0.1, 0.1, 50))  # Output: 0.7869\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the provided values satisfy the conditions:\n  - `min_epsilon` is less than `max_epsilon`.\n  - `decay_rate` is greater than zero.\n  - `step` is non-negative.\n  If any of these conditions are violated, the function returns `-1`.\n\n- **Exponential Decay Calculation**: The function calculates the exploration rate `\u03b5(t)` using the formula:\n  \\[\n  \\epsilon(t) = \\text{min\\_epsilon} + (\\text{max\\_epsilon} - \\text{min\\_epsilon}) \\cdot e^{-\\text{decay\\_rate} \\times \\text{step}}\n  \\]\n\n- **Clipping**: After calculating `\u03b5(t)`, the function ensures that it does not fall below `min_epsilon` to prevent numerical underflow.\n\n- **Rounding**: Finally, the result is rounded to four decimal places before being returned."}
{"task_id": 189, "completion_id": 0, "completion": "To implement the `batch_resample` function, we need to use the `scipy.ndimage.zoom` function from SciPy, which provides efficient image resizing capabilities. The `zoom` function allows us to specify the zoom factor for each axis independently, which is useful for handling different scaling factors along rows and columns.\n\nHere's how you can implement the function:\n\n```python\nimport numpy as np\nfrom scipy import ndimage\n\ndef batch_resample(X, new_dim, mode=\"bilinear\"):\n    \"\"\"Resample a batch of images to a new spatial resolution.\n\n    Parameters\n    ----------\n    X : numpy.ndarray of shape (n_ex, in_rows, in_cols, in_channels)\n        Input batch of images.\n    new_dim : tuple[int, int]\n        Target dimension ``(out_rows, out_cols)``.\n    mode : {\"bilinear\", \"nearest\"}, default=\"bilinear\"\n        Interpolation method.\n\n    Returns\n    -------\n    numpy.ndarray\n        Resampled batch with shape (n_ex, out_rows, out_cols, in_channels).\n    \"\"\"\n    # Check if the mode is valid\n    if mode not in [\"bilinear\", \"nearest\"]:\n        raise NotImplementedError(f\"Unrecognized resampling mode: {mode}\")\n\n    # Get the input dimensions\n    n_ex, in_rows, in_cols, _ = X.shape\n\n    # Calculate the zoom factors\n    zoom_factors = (new_dim[0] / in_rows, new_dim[1] / in_cols)\n\n    # Use ndimage.zoom for resampling\n    if mode == \"bilinear\":\n        resized_images = ndimage.zoom(X, zoom=zoom_factors, order=1)\n    elif mode == \"nearest\":\n        resized_images = ndimage.zoom(X, zoom=zoom_factors, order=0)\n\n    # Round the result to 4 decimal places\n    resized_images_rounded = np.round(resized_images, 4)\n\n    return resized_images_rounded\n```\n\n### Explanation:\n- **Input Validation**: We first check if the specified interpolation mode is either `\"bilinear\"` or `\"nearest\"`. If not, we raise a `NotImplementedError`.\n- **Zoom Factors Calculation**: We calculate the zoom factors required to scale the images from their original size to the target size.\n- **Resizing**: We use `ndimage.zoom` with the calculated zoom factors. The `order=1` parameter specifies bilinear interpolation, and `order=0` specifies nearest neighbor interpolation.\n- **Rounding**: Finally, we round the resulting array to four decimal places using `np.round`.\n\nThis implementation efficiently handles both upsampling and downsampling of the images while maintaining the desired output format."}
{"task_id": 190, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that iterates over each feature and each unique value within that feature to find the best binary split that minimizes the weighted Gini impurity. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n\n    # Calculate the Gini impurity for the entire dataset\n    def gini_impurity(labels):\n        _, counts = np.unique(labels, return_counts=True)\n        probabilities = counts / len(labels)\n        return 1 - np.sum(probabilities**2)\n\n    G_whole_dataset = gini_impurity(y)\n\n    # Initialize variables to track the best split\n    best_feature_index = -1\n    best_threshold_value = None\n    best_gini = G_whole_dataset\n\n    # Iterate over each feature\n    for j in range(X.shape[1]):\n        unique_values = np.unique(X[:, j])\n        \n        # Skip features with only one unique value\n        if len(unique_values) <= 1:\n            continue\n        \n        # Try splitting at each unique value\n        for i in range(len(unique_values) - 1):\n            left_indices = X[:, j] <= unique_values[i]\n            right_indices = X[:, j] > unique_values[i]\n            \n            # Check if both child nodes are non-empty\n            if len(left_indices) == 0 or len(right_indices) == 0:\n                continue\n            \n            # Calculate the Gini impurity for the left and right nodes\n            n_left = len(left_indices)\n            n_right = len(right_indices)\n            G_left = gini_impurity(y[left_indices])\n            G_right = gini_impurity(y[right_indices])\n            \n            # Calculate the weighted Gini impurity for the split\n            G_split = (n_left / (n_left + n_right)) * G_left + (n_right / (n_left + n_right)) * G_right\n            \n            # Update the best split if this one is better\n            if G_split < best_gini:\n                best_gini = G_split\n                best_feature_index = j\n                best_threshold_value = unique_values[i]\n\n    # Return the best split rounded to 4 decimal places\n    return (best_feature_index, best_threshold_value, round(best_gini, 4))\n\n# Example usage:\nX = [[1.5, 2.0], [1.8, 2.1], [1.9, 2.2], [2.0, 2.3]]\ny = [0, 0, 1, 1]\nprint(best_gini_split(X, y))\n```\n\n### Explanation:\n1. **Gini Impurity Calculation**: We define a helper function `gini_impurity` to compute the Gini impurity for a given set of labels.\n\n2. **Initial Setup**: We calculate the Gini impurity for the entire dataset to handle cases where no valid split is found.\n\n3. **Iterate Over Features**: For each feature, we extract unique values and attempt to split the data at each unique value.\n\n4. **Check Validity of Splits**: We ensure that both child nodes are non-empty before calculating the Gini impurity for the split.\n\n5. **Update Best Split**: We update the best split if the current split results in a lower weighted Gini impurity.\n\n6. **Return Result**: Finally, we return the best feature index, threshold, and Gini impurity rounded to four decimal places."}
{"task_id": 191, "completion_id": 0, "completion": "Here's how you can implement the `metric` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    \n    def validate_vectors(x, y):\n        if not isinstance(x, (list, tuple, np.ndarray)):\n            return False\n        if not isinstance(y, (list, tuple, np.ndarray)):\n            return False\n        \n        x = np.array(x)\n        y = np.array(y)\n        \n        if len(x) != len(y):\n            return False\n        \n        if np.any(np.isnan(x)) or np.any(np.isnan(y)):\n            return False\n        \n        if name == 'cosine' and (np.linalg.norm(x) == 0 or np.linalg.norm(y) == 0):\n            return False\n        \n        return True\n    \n    def euclidean(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        return round(np.linalg.norm(x - y), 4)\n    \n    def manhattan(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        return round(np.sum(np.abs(x - y)), 4)\n    \n    def chebyshev(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        return round(np.max(np.abs(x - y)), 4)\n    \n    def cosine(x, y):\n        if not validate_vectors(x, y):\n            return -1\n        norm_x = np.linalg.norm(x)\n        norm_y = np.linalg.norm(y)\n        if norm_x == 0 or norm_y == 0:\n            return -1\n        dot_product = np.dot(x, y)\n        return round(1 - dot_product / (norm_x * norm_y), 4)\n    \n    metrics = {\n        'euclidean': euclidean,\n        'manhattan': manhattan,\n        'chebyshev': chebyshev,\n        'cosine': cosine\n    }\n    \n    return metrics.get(name, lambda _: -1)\n\n# Example usage:\nprint(metric('euclidean')([1, 2, 3], [4, 5, 6]))  # \u279e 5.1962\n```\n\n### Explanation:\n- **Validation**: The `validate_vectors` function checks if the inputs are valid lists, tuples, or NumPy arrays, ensures they are one-dimensional, non-empty, and of the same length. It also checks for NaN values and rejects zero-norm vectors for the cosine distance.\n  \n- **Distance Functions**: Each distance function (`euclidean`, `manhattan`, `chebyshev`, `cosine`) uses the `validate_vectors` function before performing its calculation. If validation fails, it returns `-1`.\n\n- **Metric Dictionary**: A dictionary `metrics` maps each supported metric name to its corresponding function. The `get` method of this dictionary is used to retrieve the appropriate function based on the input `name`. If the name is not found, it defaults to returning `-1`.\n\nThis implementation ensures that all required validations are performed and handles invalid inputs gracefully by returning `-1`."}
{"task_id": 193, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    The function accepts either a scalar or a list/NumPy array. For a scalar it\n    returns a float; for an array-like input it returns a list of floats, with\n    every element rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # Scalar case\n        result = z / (1 + abs(z))\n        return round(result, 4)\n    elif isinstance(z, (list, np.ndarray)):\n        # Array-like case\n        result = [round(x / (1 + abs(x)), 4) for x in z]\n        return result\n    else:\n        raise ValueError(\"Input must be a scalar or a list/NumPy array.\")\n\n# Example usage:\nprint(softsign(3))  # Should print 0.7500\nprint(softsign([-2, 0, 2]))  # Should print [-0.6667, 0.0000, 0.6667]\nprint(softsign([]))  # Should print []\n```\n\nThis implementation defines the `softsign` function according to the specified requirements. It handles both scalar and array-like inputs, applies the Softsign activation function, and rounds the results to four decimal places before returning them."}
{"task_id": 194, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adagrad_update(weights: list[float],\n                   gradients: list[float],\n                   G: list[float] | None = None,\n                   learning_rate: float = 0.01,\n                   eps: float = 1e-8) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss with respect to *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple  (new_weights, new_G) where\n            new_weights \u2013 list of updated parameters rounded to 6 decimals,\n            new_G       \u2013 list of updated accumulated squared gradients\n                           rounded to 6 decimals.\n    \"\"\"\n    if G is None:\n        G = [eps] * len(weights)\n\n    new_G = [G[i] + g**2 for i, g in enumerate(gradients)]\n    new_weights = [w - learning_rate * g / np.sqrt(G[i] + eps) for i, w in enumerate(weights)]\n\n    return np.round(new_weights, 6).tolist(), np.round(new_G, 6).tolist()\n```\n\nThis implementation follows the steps of the Adagrad optimization algorithm to update the parameters and their accumulated squared gradients. The `np.round` function is used to round the results to six decimal places."}
{"task_id": 197, "completion_id": 0, "completion": "To implement the `action_stats` function, we need to analyze the environment's action space based on whether it is discrete, continuous, or multi-dimensional. Here's how you can achieve this:\n\n1. **Determine the type of action space**:\n   - If `md_action` is `True`, the action space is multi-dimensional.\n   - If `cont_action` is `True`, the action space is continuous.\n\n2. **Handle multi-dimensional action spaces**:\n   - If `md_action` is `True`, iterate over each sub-space in `env.action_space.spaces`.\n   - For each sub-space, determine if it is discrete or continuous and calculate the number of actions accordingly.\n   - If any dimension is continuous, set the corresponding entry in `n_actions_per_dim` to `np.inf`.\n\n3. **Handle discrete multi-dimensional action spaces**:\n   - Compute the Cartesian product of the range of actions for each dimension to get all possible discrete actions.\n\n4. **Handle continuous action spaces**:\n   - If any dimension is continuous, set `action_ids` to `None`.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    n_actions_per_dim = []\n    action_ids = None\n    action_dim = 0\n\n    if md_action:\n        # Multi-dimensional action space\n        for space in env.action_space.spaces:\n            if hasattr(space, 'n'):\n                # Discrete space\n                n_actions_per_dim.append(space.n)\n                action_dim += 1\n            elif hasattr(space, 'shape'):\n                # Continuous space\n                n_actions_per_dim.append(np.inf)\n                action_ids = None\n                break\n        else:\n            # All sub-spaces are discrete\n            action_ids = [tuple(range(n)) for n in n_actions_per_dim]\n    else:\n        # Single-dimensional action space\n        if hasattr(env.action_space, 'n'):\n            # Discrete space\n            n_actions_per_dim.append(env.action_space.n)\n            action_dim = 1\n        elif hasattr(env.action_space, 'shape') and len(env.action_space.shape) == 1:\n            # Continuous space\n            n_actions_per_dim.append(np.inf)\n            action_ids = None\n        else:\n            raise ValueError(\"Unsupported action space shape\")\n\n    return n_actions_per_dim, action_ids, action_dim\n\n# Example usage:\n# env = gym.make('CartPole-v1')\n# md_action = True\n# cont_action = False\n# print(action_stats(env, md_action, cont_action))\n```\n\n### Explanation:\n- **Multi-dimensional action space**: We iterate through each sub-space in `env.action_space.spaces`. If a sub-space is discrete, we append its number of actions to `n_actions_per_dim` and increment `action_dim`. If any sub-space is continuous, we set `n_actions_per_dim` to `[np.inf]` and `action_ids` to `None`.\n- **Single-dimensional action space**: We check if the action space is discrete or continuous directly using `env.action_space.n` and `env.action_space.shape`.\n- **Continuous action space**: If any dimension is continuous, we set `n_actions_per_dim` to `[np.inf]` and `action_ids` to `None`.\n\nThis function will correctly summarize the action space based on the provided flags and the environment's structure."}
{"task_id": 198, "completion_id": 0, "completion": "To implement the \u03b2-maximisation step in Latent Dirichlet Allocation, we need to follow these steps:\n\n1. Compute the numerator for each cell in the \u03b2 matrix using the formula:\n   \\[\n   \\beta_{v,t} \\propto \\sum_{d=0}^{D-1} \\sum_{n=0}^{N_d-1} \\varphi^{(d)}_{n,t} [w^{(d)}_n = v]\n   \\]\n   where \\( w^{(d)}_n = v \\) is an indicator function that checks if the \\( n \\)-th token in document \\( d \\) is the word \\( v \\).\n\n2. Sum up the numerators for each topic to get the denominator for normalising each column.\n\n3. Normalize each column by dividing its numerator by the total sum of numerators for that topic.\n\n4. Round each entry in the normalized matrix to 4 decimal places.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # Initialize the beta matrix with zeros\n    beta = np.zeros((V, len(phi[0])))\n\n    # Iterate over each document\n    for d, doc in enumerate(corpus):\n        for t in range(len(phi[d])):\n            # Calculate the numerator for the current topic\n            numerator = np.sum(phi[d][:, t])\n            \n            # Update the beta matrix\n            beta[:, t] += numerator\n    \n    # Normalize each column\n    for t in range(len(beta)):\n        total_numerator = np.sum(beta[t])\n        if total_numerator > 0:\n            beta[t] /= total_numerator\n    \n    # Round the beta matrix to 4 decimal places\n    beta_rounded = np.round(beta, 4).tolist()\n\n    return beta_rounded\n\n# Example usage:\n# phi_example = [\n#     np.array([[0.5, 0.5], [0.3, 0.7]]),\n#     np.array([[0.6, 0.4], [0.8, 0.2]])\n# ]\n# corpus_example = [\n#     [0, 1],\n#     [1, 0]\n# ]\n# V_example = 2\n# print(update_beta(phi_example, corpus_example, V_example))\n```\n\n### Explanation:\n\n- We initialize the `beta` matrix with zeros, having dimensions `(V, T)`.\n- For each document, we iterate over each topic and compute the numerator as the sum of the probabilities for that topic across all tokens in the document.\n- We add this numerator to the corresponding column in the `beta` matrix.\n- After updating all columns, we normalize each column by dividing it by the total sum of numerators for that column to ensure they sum to 1.\n- Finally, we round each entry in the `beta` matrix to 4 decimal places before returning it as a list of lists."}
{"task_id": 199, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    y_left = []\n    y_right = []\n\n    for x, label in zip(X, y):\n        if x < value:\n            y_left.append(label)\n        else:\n            y_right.append(label)\n\n    return y_left, y_right\n```"}
{"task_id": 200, "completion_id": 0, "completion": "To implement the L1 regularization penalty as described, we need to calculate the sum of the absolute values of the weights and then multiply it by the regularization strength `C`. The result should be rounded to four decimal places. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already one\n    weights_array = np.array(weights)\n    \n    # Calculate the L1 norm of the weights\n    l1_norm = np.sum(np.abs(weights_array))\n    \n    # Compute the L1 penalty\n    l1_penalty = C * l1_norm\n    \n    # Round the result to 4 decimal places\n    return round(l1_penalty, 4)\n\n# Example usage:\n# weights = [1, -2, 3]\n# C = 0.5\n# print(compute_l1_penalty(weights, C))  # Output: 2.0\n```\n\n### Explanation:\n1. **Input Handling**: We first convert the input `weights` to a NumPy array to facilitate vectorized operations.\n2. **L1 Norm Calculation**: We use `np.sum(np.abs(weights_array))` to compute the sum of the absolute values of the weights.\n3. **Penalty Computation**: We multiply the L1 norm by the regularization strength `C`.\n4. **Rounding**: Finally, we round the computed penalty to four decimal places using `round()`.\n\nThis implementation ensures that the function handles various input types and shapes for `weights`, including nested lists, and provides the correct output according to the specified constraints."}
{"task_id": 201, "completion_id": 0, "completion": "Here's the implementation of the `max_norm` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef max_norm(p, m=2.0, axis=0):\n    \"\"\"Clip the L2 norm of slices of *p* along *axis* so that they do not exceed *m*.\n\n    Parameters\n    ----------\n    p : np.ndarray\n        Input tensor (weights) of arbitrary shape.\n    m : float, optional\n        Maximum allowed L2 norm for each slice. Default is 2.0.\n    axis : int | None, optional\n        Axis along which to compute the norm. If None, the whole tensor is\n        considered a single vector. Default is 0.\n\n    Returns\n    -------\n    list\n        Tensor with the same shape as the input where every slice along *axis*\n        has been norm-clipped. The returned value is a (nested) Python list and\n        every number is rounded to 4 decimal places.\n    \"\"\"\n    # Handle the case where axis is None, treating the entire tensor as a single vector\n    if axis is None:\n        p = p.flatten()\n        m = m * np.sqrt(len(p))\n    \n    # Compute the L2 norm along the specified axis\n    norms = np.linalg.norm(p, ord=2, axis=axis, keepdims=True)\n    \n    # Apply the clipping condition\n    clip_factor = np.minimum(1, m / (norms + 1e-12))\n    \n    # Clip the values in-place\n    p *= clip_factor\n    \n    # Round the result to 4 decimal places\n    p_rounded = np.round(p, decimals=4)\n    \n    # Convert the result to a nested Python list\n    return p_rounded.tolist()\n\n# Example usage:\n# weights = np.array([[1, 2], [3, 4]])\n# print(max_norm(weights, m=5.0, axis=0))\n```\n\n### Explanation:\n1. **Handling `axis=None`:** If `axis` is `None`, the function flattens the input tensor `p` and scales the maximum norm `m` accordingly to account for the flattened dimensions.\n\n2. **Computing Norms:** It calculates the L2 norm of each slice along the specified axis using `np.linalg.norm`.\n\n3. **Applying Clipping Condition:** A factor `clip_factor` is computed to scale each element of the slice such that its L2 norm does not exceed `m`. This is done using `np.minimum(1, m / (norms + 1e-12))` to ensure numerical stability.\n\n4. **Rounding and Conversion:** The clipped values are rounded to four decimal places using `np.round`, and then converted to a nested Python list using `.tolist()`.\n\nThis function ensures that the constraints are applied correctly and efficiently, rounding the results as required."}
{"task_id": 202, "completion_id": 0, "completion": "To implement the Partitioning Around Medoids (PAM) clustering algorithm, we need to follow the steps outlined in the problem description. Here's a complete implementation in Python:\n\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    # Step 1: Initialize medoids\n    medoids = X[:k]\n    \n    while True:\n        # Calculate the cost for each medoid\n        costs = np.sum((X[:, np.newaxis] - medoids[np.newaxis, :])**2, axis=2).sum(axis=1)\n        \n        # Find the best swap\n        best_swap_cost_reduction = -np.inf\n        best_medoid_index, best_sample_index = None, None\n        \n        for i in range(k):\n            for j in range(k, len(X)):\n                new_costs = np.copy(costs)\n                \n                # Swap medoid i with sample j\n                new_costs[i] = np.sum((X[j][:, np.newaxis] - medoids[np.newaxis, :])**2, axis=2).sum(axis=1)\n                new_costs[j] = np.sum((X[i][:, np.newaxis] - medoids[np.newaxis, :])**2, axis=2).sum(axis=1)\n                \n                # Check if this swap reduces the total cost\n                cost_reduction = np.sum(new_costs) - np.sum(costs)\n                if cost_reduction > best_swap_cost_reduction:\n                    best_swap_cost_reduction = cost_reduction\n                    best_medoid_index, best_sample_index = i, j\n        \n        # If no swap improves the cost, break the loop\n        if best_swap_cost_reduction <= 0:\n            break\n        \n        # Perform the swap\n        medoids[best_medoid_index], medoids[best_sample_index] = medoids[best_sample_index], medoids[best_medoid_index]\n    \n    # Assign each sample to the nearest medoid\n    labels = np.argmin(np.sum((X[:, np.newaxis] - medoids[np.newaxis, :])**2, axis=2), axis=1)\n    \n    return labels.tolist()\n\n# Example usage\nX = np.array([[0,0], [1,0], [0,1], [5,5], [6,5], [5,6]])\nk = 2\nprint(pam_clustering(X, k))  # Output: [0, 0, 0, 1, 1, 1]\n```\n\n### Explanation:\n1. **Initialization**: We start by selecting the first `k` samples as the initial medoids.\n2. **Cost Calculation**: For each possible pair of medoid and sample, calculate the total cost reduction if the medoid were swapped with the sample.\n3. **Swap Selection**: Identify the swap that results in the greatest cost reduction.\n4. **Convergence Check**: If no swap reduces the cost, terminate the process.\n5. **Label Assignment**: After convergence, assign each sample to the medoid that minimizes its distance.\n\nThis implementation ensures that the clustering process is deterministic and converges to a local optimum."}
{"task_id": 203, "completion_id": 0, "completion": "To implement the alias method in Python, we need to follow the steps outlined in the problem description. Here's how you can implement the `build_alias_table` function:\n\n```python\nimport numpy as np\n\ndef build_alias_table(probs: list[float] | np.ndarray) -> tuple[list[float], list[int]]:\n    \"\"\"Builds probability and alias tables for the alias sampling method.\n\n    Args:\n        probs: 1-D list or NumPy array of positive floats summing to 1.\n\n    Returns:\n        Tuple (prob_table, alias_table) where:\n            * prob_table \u2013 list of floats rounded to 4 decimals.\n            * alias_table \u2013 list of ints.\n        If *probs* is invalid (negative entries or not summing to 1) return -1.\n    \"\"\"\n    # Convert input to a NumPy array\n    probs = np.array(probs)\n    \n    # Check if all probabilities are non-negative and sum to 1 within a tolerance\n    if np.any(probs < 0) or not np.isclose(np.sum(probs), 1, atol=1e-12):\n        return -1\n    \n    N = len(probs)\n    scaled_probs = probs * N\n    \n    small = []\n    large = []\n    \n    # Separate indices based on their scaled probabilities\n    for i, p in enumerate(scaled_probs):\n        if p < 1:\n            small.append(i)\n        else:\n            large.append(i)\n    \n    while small and large:\n        l = large.pop()\n        g = small.pop()\n        \n        prob_table[l] = scaled_probs[l]\n        alias_table[l] = g\n        \n        scaled_probs[g] += scaled_probs[l] - 1\n        \n        if scaled_probs[g] < 1:\n            small.append(g)\n        else:\n            large.append(g)\n    \n    # Set remaining entries in large and small to 1 in prob_table\n    prob_table[large] = 1\n    prob_table[small] = 1\n    \n    # Round the results to 4 decimal places\n    prob_table_rounded = np.round(prob_table, 4).tolist()\n    alias_table_rounded = alias_table.tolist()\n    \n    return prob_table_rounded, alias_table_rounded\n\n# Example usage\nprobs = [0.1, 0.2, 0.7]\nresult = build_alias_table(probs)\nprint(result)  # Output: ([0.3000, 0.6000, 1.0000], [2, 2, 0])\n```\n\n### Explanation:\n1. **Input Validation**: We first check if all probabilities are non-negative and if they sum to 1 within a specified tolerance (`1e-12`). If either condition fails, we return `-1`.\n\n2. **Scaling Probabilities**: We multiply each probability by `N` to get the `scaled_probs` array.\n\n3. **Separating Indices**: We separate the indices into two lists: `small` for probabilities less than 1 and `large` for probabilities greater than or equal to 1.\n\n4. **Building the Alias Table**: We repeatedly pop elements from `small` and `large` until both lists are empty. For each pair, we assign the smaller probability to the larger index in the `prob_table` and store the larger index in the `alias_table`. We then adjust the `scaled_probs` of the larger index and append it to the appropriate list (`small` or `large`).\n\n5. **Final Adjustments**: After the loop, we set the remaining entries in the `large` and `small` lists to 1 in the `prob_table`.\n\n6. **Rounding**: Finally, we round the results to 4 decimal places and return the `prob_table` and `alias_table` as Python lists."}
{"task_id": 204, "completion_id": 0, "completion": "To solve this problem, we need to iterate over the given iterable and count how many elements are either `None` or `NaN`. We'll convert the iterable to a list first to make it easier to handle each element individually. Then, we'll check each element to see if it's `None` or `NaN`, and increment our counter accordingly.\n\nHere's the implementation of the `num_missing` function:\n\n```python\nimport numpy as np\nimport math\n\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or ``NaN``.\n\n    Args:\n        data: Any 1-D iterable (list, tuple, NumPy array, etc.).\n\n    Returns:\n        int: The number of missing elements.\n    \"\"\"\n    # Convert the iterable to a list for easy iteration\n    data_list = list(data)\n    \n    # Initialize a counter for missing values\n    missing_count = 0\n    \n    # Iterate over each element in the list\n    for element in data_list:\n        # Check if the element is None or NaN\n        if element is None or math.isnan(element):\n            missing_count += 1\n    \n    # Return the total count of missing values\n    return missing_count\n\n# Example usage:\nprint(num_missing([1, 2, None, 4, 'NaN', 6]))  # Output: 3\nprint(num_missing([]))                         # Output: 0\nprint(num_missing(np.array([1, 2, np.nan, 4])))  # Output: 1\n```\n\n### Explanation:\n1. **Conversion to List**: We convert the input iterable to a list using `list(data)` because we need to access individual elements.\n2. **Initialization**: We initialize a counter `missing_count` to zero to keep track of the number of missing values.\n3. **Iteration and Checking**: We loop through each element in the list. For each element, we check if it is `None` or `NaN` using `math.isnan(element)`.\n4. **Increment Counter**: If an element is missing, we increment the `missing_count`.\n5. **Return Result**: Finally, we return the `missing_count`.\n\nThis function handles any 1-D iterable, including lists, tuples, and NumPy arrays, and correctly counts the missing values according to the specified criteria."}
{"task_id": 205, "completion_id": 0, "completion": "To solve this problem, we need to iterate through the given `registry` list, identify dictionaries that contain the key `\"id\"`, and extract unique values associated with this key while preserving their original order of appearance. Here's how you can implement this:\n\n```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen_ids = set()  # To track seen ids for uniqueness\n    result = []      # List to store unique ids in order of first appearance\n\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen_ids:\n                seen_ids.add(env_id)\n                result.append(env_id)\n\n    return result\n```\n\n### Explanation:\n\n1. **Initialization**: We use two data structures:\n   - `seen_ids`: A set to keep track of ids that have already been encountered. This helps in ensuring that each id is added only once.\n   - `result`: A list to store the unique ids in the order they first appear in the registry.\n\n2. **Iteration**: We loop through each element in the `registry`.\n\n3. **Condition Check**: For each element, we check if it is a dictionary and contains the key `\"id\"`.\n\n4. **Unique Id Handling**: \n   - If both conditions are met, we extract the id.\n   - We then check if this id has already been added to `seen_ids`.\n     - If not, we add it to both `seen_ids` and `result`.\n\n5. **Return Result**: Finally, we return the `result` list containing all unique ids in their original order of appearance.\n\nThis approach ensures that we meet all the specified requirements, including preserving the order of first appearance and handling duplicates efficiently using a set."}
{"task_id": 206, "completion_id": 0, "completion": "Here's a Python implementation of the Mean Absolute Error (MAE) calculator using NumPy for efficiency:\n\n```python\nimport numpy as np\n\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Check if both inputs are one-dimensional\n    if actual.ndim != 1 or predicted.ndim != 1:\n        return -1\n    \n    # Check if both inputs have the same length\n    if actual.size != predicted.size:\n        return -1\n    \n    # Calculate the absolute differences\n    absolute_diffs = np.abs(actual - predicted)\n    \n    # Calculate the mean of the absolute differences\n    mae = np.mean(absolute_diffs)\n    \n    # Round the result to 4 decimal places\n    return round(mae, 4)\n\n# Example usage:\nactual_sequence = [10, 20, 30, 40, 50]\npredicted_sequence = [12, 18, 28, 42, 48]\n\nmae_result = absolute_error(actual_sequence, predicted_sequence)\nprint(f\"Mean Absolute Error: {mae_result}\")\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if both `actual` and `predicted` are NumPy arrays. It then verifies if they are one-dimensional and of equal length. If any condition fails, it returns `-1`.\n\n2. **Absolute Differences Calculation**: Using NumPy's `abs` function, the absolute differences between corresponding elements of the two sequences are computed.\n\n3. **Mean Calculation**: The `mean` function from NumPy is used to calculate the average of these absolute differences.\n\n4. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function.\n\nThis implementation efficiently handles the calculation using NumPy, which is optimized for performance on large datasets."}
{"task_id": 207, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef convolution_shape(img_height: int,\n                      img_width: int,\n                      filter_shape: tuple[int, int],\n                      stride: tuple[int, int],\n                      padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height of the input image.\n        img_width (int):  Width of the input image.\n        filter_shape (tuple[int, int]): (kernel_height, kernel_width).\n        stride (tuple[int, int]):       (stride_height, stride_width).\n        padding (tuple[int, int]):      (pad_height, pad_width).\n\n    Returns:\n        tuple[int, int] | int: (output_height, output_width) if the parameters\n            yield a valid integer output size; otherwise -1.\n    \"\"\"\n    KH, KW = filter_shape\n    SH, SW = stride\n    PH, PW = padding\n    \n    # Calculate potential output sizes\n    oh = (img_height + 2 * PH - KH) // SH + 1\n    ow = (img_width + 2 * PW - KW) // SW + 1\n    \n    # Check for valid output sizes\n    if oh <= 0 or ow <= 0 or not all(isinstance(i, int) for i in [oh, ow]):\n        return -1\n    \n    return oh, ow\n```\n\nThis function calculates the output shape of a 2D convolution operation based on the provided input dimensions, filter size, stride, and padding. It first computes the potential output heights and widths using the given formulas. If either calculated dimension results in a non-positive value or if any of these values are not integers, it returns `-1`. Otherwise, it returns the computed output dimensions as a 2-tuple."}
{"task_id": 208, "completion_id": 0, "completion": "Here's the implementation of the `one_hot_encode` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef one_hot_encode(labels: list[str], categories: list[str] | None = None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: A list of categorical labels of length *N*.\n        categories: Optional complete list of unique categories of length *C*.\n            If *None*, the set of categories is inferred from *labels* in the\n            order of their first appearance.\n\n    Returns:\n        A list of lists that represents the one-hot encoded matrix of shape\n        (N, C) where *C* is the number of categories. If an unknown label is\n        found when *categories* is supplied, the function returns -1.\n    \"\"\"\n    if categories is None:\n        # Deduce categories from the labels preserving the order of first appearance\n        categories = sorted(set(labels))\n    \n    # Check for unknown labels\n    if not all(label in categories for label in labels):\n        return [-1] * len(labels)\n    \n    # Create a mapping from category to index\n    category_to_index = {category: index for index, category in enumerate(categories)}\n    \n    # Convert labels to one-hot encoded indices\n    one_hot_encoded_indices = [[category_to_index[label] for label in labels]]\n    \n    # Convert the list of lists to a NumPy array and then back to a list of lists\n    one_hot_encoded_matrix = np.array(one_hot_encoded_indices).tolist()\n    \n    return one_hot_encoded_matrix\n\n# Example usage:\nlabels = ['red', 'green', 'blue', 'green']\ncategories = ['red', 'green', 'blue']\nprint(one_hot_encode(labels, categories))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n\nlabels_with_unknown = ['red', 'green', 'purple', 'green']\nprint(one_hot_encode(labels_with_unknown, categories))  # Output: [-1, -1, -1, -1]\n```\n\n### Explanation:\n1. **Deduction of Categories**: If `categories` is `None`, the function deduces them from the `labels` list using `sorted(set(labels))`.\n2. **Validation of Labels**: It checks if all labels in `labels` are present in `categories`. If any label is missing, it returns a list of `-1` values with the same length as `labels`.\n3. **Mapping and Conversion**: A dictionary `category_to_index` is created to map each category to its corresponding index. The labels are then converted to their respective indices.\n4. **One-Hot Encoding**: The indices are stored in a list of lists, which is then converted to a NumPy array and back to a list of lists for the final output."}
{"task_id": 209, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs the following steps:\n\n1. Validate the input data:\n   - Ensure that the number of payoff values matches the number of probabilities for each arm.\n   - Verify that the probabilities for each arm sum to 1 within a tolerance of \\(10^{-8}\\).\n\n2. Compute the expected payoff for each arm:\n   - For each arm, calculate the expected payoff using the formula:\n     \\[\n     \\text{Expected Payoff} = \\sum_{i=0}^{n-1} (\\text{payoff}_i \\times \\text{probability}_i)\n     \\]\n   - Round the expected payoff to 4 decimal places.\n\n3. Determine the arm with the highest expected payoff:\n   - Identify the index of the arm with the maximum expected payoff.\n   - In case of ties, choose the smallest index.\n\nHere's the implementation of the `analyze_bandit` function:\n\n```python\nimport numpy as np\n\ndef analyze_bandit(payoffs: list[list[int | float]], payoff_probs: list[list[float]]) -> tuple[list[float], int, float]:\n    \"\"\"Return expected payoff of each arm, the best arm index, and its expected value.\n\n    The function must validate the inputs.  If any arm has mismatched list\n    lengths or probabilities that do not sum to 1 (within 1e-8), **return -1**.\n\n    Args:\n        payoffs: Ragged list ``payoffs[k][i]`` is the *i*-th payoff of arm *k*.\n        payoff_probs: Ragged list ``payoff_probs[k][i]`` is the probability of\n            obtaining ``payoffs[k][i]``.\n\n    Returns:\n        (expected_values, best_arm, best_ev) where\n            expected_values \u2013 list of expected payoffs rounded to 4 decimals,\n            best_arm        \u2013 index of the arm with the largest expected payoff,\n            best_ev         \u2013 that maximum expected payoff rounded to 4 decimals.\n\n        If the input is invalid the function returns **-1**.\n    \"\"\"\n    K = len(payoffs)\n    \n    # Validate the input data\n    for k in range(K):\n        if len(payoffs[k]) != len(payoff_probs[k]):\n            return -1\n        \n        if not np.isclose(np.sum(payoff_probs[k]), 1, atol=1e-8):\n            return -1\n    \n    # Calculate expected payoffs\n    expected_values = []\n    for k in range(K):\n        total_payoff = 0.0\n        for i, p in enumerate(payoff_probs[k]):\n            total_payoff += payoff_probs[k][i] * payoffs[k][i]\n        \n        # Round to 4 decimal places\n        expected_values.append(round(total_payoff, 4))\n    \n    # Find the best arm\n    max_ev = max(expected_values)\n    best_arm = expected_values.index(max_ev)\n    \n    return expected_values, best_arm, max_ev\n\n# Example usage:\npayoffs = [[10, 20, 30], [5, 15, 25]]\npayoff_probs = [[0.3, 0.5, 0.2], [0.4, 0.6, 0.0]]\nresult = analyze_bandit(payoffs, payoff_probs)\nprint(result)  # Output: ([17.0, 19.0], 1, 19.0)\n```\n\n### Explanation:\n- **Validation**: We first check if the lengths of the payoff lists match the lengths of their corresponding probability lists. Then, we ensure that the sum of probabilities for each arm is approximately 1.\n- **Expected Payoff Calculation**: For each arm, we compute the expected payoff by iterating over the payoff and probability pairs, multiplying them, and summing the results.\n- **Best Arm Determination**: We find the maximum expected payoff and determine the index of the first occurrence of this maximum value, ensuring it's the smallest index in case of ties.\n\nThis function will correctly handle the validation and computation tasks as specified."}
{"task_id": 210, "completion_id": 0, "completion": "To solve the problem of creating a decorator decorator that supports both direct application and application with optional parameters, we need to implement the `doublewrap` function. This function will handle the wrapping logic based on whether the decorator is called with or without parentheses.\n\nHere's how we can implement the `doublewrap` function:\n\n```python\nimport functools\nfrom typing import Callable, Any\n\ndef doublewrap(decorator_func: Callable[..., Callable[..., Any]]) -> Callable[..., Any]:\n    \"\"\"Return a decorator that can be used *with* or *without* parentheses.\n\n    Args:\n        decorator_func: A callable that expects the function to decorate as\n            its first positional argument followed only by *optional*\n            positional or keyword arguments.\n\n    Returns:\n        A new decorator that forwards the call in the correct form so that\n        ``decorator_func`` can be applied either directly or after receiving\n        its own optional parameters.\n    \"\"\"\n    @functools.wraps(decorator_func)\n    def real_decorator(*args, **kwargs):\n        if not args:\n            # Direct call without parentheses\n            return decorator_func\n        elif len(args) == 1 and callable(args[0]):\n            # Call with one argument, which is the function to decorate\n            return decorator_func(*args, **kwargs)\n        else:\n            # Call with more than one argument, which are the decorator options\n            return lambda func: decorator_func(func, *args, **kwargs)\n\n    return real_decorator\n\n# Example usage\n@doublewrap\ndef repeat(func, times: int = 2):\n    \"\"\"Repeat the return value *times* times.\"\"\"\n    def inner(*args, **kwargs):\n        return func(*args, **kwargs) * times\n    return inner\n\n@repeat                   # used without arguments, default times = 2\ndef greet():\n    return \"Hi\"\n\n@repeat(times=3)          # used with keyword argument\ndef excited():\n    return \"Wow\"\n\nprint(greet())   # \u279e \"HiHi\"\nprint(excited()) # \u279e \"WowWowWow\"\n```\n\n### Explanation:\n\n1. **Function Definition**: The `doublewrap` function takes a `decorator_func` as an argument, which is expected to take the function to decorate as its first positional argument and any number of optional positional or keyword arguments.\n\n2. **Real Decorator Creation**: Inside `doublewrap`, we define a nested function `real_decorator`. This function uses `functools.wraps` to preserve the metadata of the original `decorator_func`.\n\n3. **Handling Different Cases**:\n   - **Direct Call**: If `real_decorator` is called without any arguments, it simply returns the `decorator_func` itself.\n   - **Call with One Argument**: If `real_decorator` is called with exactly one argument and that argument is callable, it assumes this is the function to decorate and passes the remaining arguments to `decorator_func`.\n   - **Call with Multiple Arguments**: If `real_decorator` is called with more than one argument, it returns a lambda function that takes a function `func` and applies `decorator_func` to it with the provided arguments.\n\nThis approach ensures that the decorator can be used both directly and with optional parameters, while preserving the original decorator's metadata."}
{"task_id": 211, "completion_id": 0, "completion": "To implement the `accuracy` function, we need to follow these steps:\n\n1. Check if the input sequences `actual` and `predicted` have the same length and are not empty. If not, return `-1`.\n2. Compute the number of mismatches between the `actual` and `predicted` lists.\n3. Calculate the classification error as the ratio of mismatches to the total number of samples.\n4. Return `1 - classification_error` rounded to four decimal places.\n\nHere's the complete implementation:\n\n```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    # Check if the input sequences have the same length and are not empty\n    if len(actual) != len(predicted) or not actual:\n        return -1\n    \n    # Compute the number of mismatches\n    num_mismatches = sum(1 for a, p in zip(actual, predicted) if a != p)\n    \n    # Calculate the classification error\n    classification_error = num_mismatches / len(actual)\n    \n    # Return the accuracy rounded to four decimal places\n    return round(1 - classification_error, 4)\n\n# Example usage:\nactual_labels = [0, 1, 2, 3, 4]\npredicted_labels = [0, 1, 2, 3, 5]\n\nprint(accuracy(actual_labels, predicted_labels))  # Output: 0.8000\n```\n\n### Explanation:\n\n- **Length Check**: We first check if the lengths of `actual` and `predicted` are equal and if they are non-empty. If not, we return `-1`.\n- **Mismatch Count**: We use a generator expression within the `sum` function to count the number of mismatches between corresponding elements of `actual` and `predicted`.\n- **Classification Error Calculation**: The classification error is calculated by dividing the number of mismatches by the total number of samples.\n- **Accuracy Calculation**: Finally, we calculate the accuracy by subtracting the classification error from 1 and rounding the result to four decimal places using `round`.\n\nThis implementation ensures that the function handles edge cases such as empty inputs and sequences of different lengths appropriately."}
{"task_id": 212, "completion_id": 0, "completion": "To solve the problem of extracting the minimal sub-model from a given adjacency list, we need to find a shortest path from the `\"input\"` layer to the `\"output_realtime\"` layer. We'll use Breadth-First Search (BFS) to achieve this efficiently. Here's how you can implement the `extract_realtime_model` function:\n\n```python\nfrom collections import deque\n\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Find a shortest path from 'input' to 'output_realtime'.\n\n    The *model* is given as an adjacency list `graph` where every key is the\n    name of a layer and the associated list contains the names of its outgoing\n    neighbours.\n\n    The function must return the names of the layers that lie on ONE shortest\n    path starting at the layer named ``\"input\"`` and ending at the layer named\n    ``\"output_realtime\"``. If several shortest paths are possible the\n    lexicographically smallest one (as complete lists) is returned.  When no\n    path exists an empty list is returned.\n\n    Parameters\n    ----------\n    graph : dict[str, list[str]]\n        Adjacency list of the directed graph.\n\n    Returns\n    -------\n    list[str]\n        Ordered list of layer names on the desired path or an empty list.\n    \"\"\"\n    if \"input\" not in graph or \"output_realtime\" not in graph:\n        return []\n\n    # Initialize BFS queue with the starting layer \"input\"\n    queue = deque([(\"input\", [\"input\"])])\n    \n    # Dictionary to keep track of the minimum path found so far for each layer\n    min_paths = {layer: None for layer in graph}\n    min_paths[\"input\"] = [\"input\"]\n    \n    while queue:\n        current_layer, path = queue.popleft()\n        \n        # Check if we've reached the target layer \"output_realtime\"\n        if current_layer == \"output_realtime\":\n            return path\n        \n        # Explore all outgoing neighbors of the current layer\n        for neighbor in graph[current_layer]:\n            new_path = path + [neighbor]\n            \n            # If the neighbor hasn't been visited yet or we found a shorter path\n            if min_paths.get(neighbor) is None or len(new_path) < len(min_paths[neighbor]):\n                min_paths[neighbor] = new_path\n                queue.append((neighbor, new_path))\n    \n    # If no path was found, return an empty list\n    return []\n\n# Example usage:\ngraph = {\n    \"input\": [\"A\", \"B\"],\n    \"A\": [\"C\", \"D\"],\n    \"B\": [\"E\"],\n    \"C\": [\"F\"],\n    \"D\": [\"G\"],\n    \"E\": [],\n    \"F\": [\"output_realtime\"],\n    \"G\": [\"output_realtime\"]\n}\n\nprint(extract_realtime_model(graph))  # Output: ['input', 'A', 'C', 'F', 'output_realtime']\n```\n\n### Explanation:\n1. **Initialization**: We start by checking if both `\"input\"` and `\"output_realtime\"` are present in the graph. If not, we return an empty list since there's no valid path.\n\n2. **BFS Queue**: We initialize a queue with the starting layer `\"input\"` and its initial path `[ \"input\" ]`.\n\n3. **Path Tracking**: We use a dictionary `min_paths` to keep track of the minimum path found so far for each layer. Initially, the path to `\"input\"` is just `[\"input\"]`.\n\n4. **Exploration**: For each layer in the queue, we explore all its outgoing neighbors. If a neighbor has not been visited before or if we find a shorter path, we update the `min_paths` dictionary and add the neighbor to the queue with the updated path.\n\n5. **Termination**: If we reach the target layer `\"output_realtime\"`, we return the corresponding path. If the queue is exhausted without finding the target, we return an empty list.\n\nThis approach ensures that we find the shortest path lexicographically and avoids infinite loops due to cycles in the graph."}
{"task_id": 213, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with He Normal strategy.\n\n    Args:\n        shape: A tuple that represents the desired tensor shape.\n\n    Returns:\n        Nested Python list containing the initialised weights.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(42)\n    \n    # Calculate fan_in and fan_out based on the shape\n    if len(shape) == 1:\n        fan_in = fan_out = shape[0]\n    elif len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    else:\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    \n    # Check if fan_in is zero to avoid division by zero\n    if fan_in == 0:\n        return []\n    \n    # Draw weights from a normal distribution with mean 0 and std_dev sqrt(2 / fan_in)\n    weights = np.random.normal(loc=0, scale=np.sqrt(2 / fan_in), size=shape)\n    \n    # Convert the weights to a nested Python list\n    return weights.tolist()\n```"}
{"task_id": 214, "completion_id": 0, "completion": "Here's the implementation of the `mean_squared_error` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function must return -1 if the two input sequences do not share the\n    same length. Otherwise, it should return the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays have to be supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    # Check if both inputs are either lists or NumPy arrays\n    if isinstance(actual, list) and isinstance(predicted, list):\n        actual = np.array(actual)\n        predicted = np.array(predicted)\n    \n    # Check if both inputs have the same shape\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Calculate the squared differences\n    squared_diffs = (actual - predicted) ** 2\n    \n    # Compute the mean of the squared differences\n    mse = np.mean(squared_diffs)\n    \n    # Round the result to 4 decimal places\n    return round(mse, 4)\n\n# Example usage:\n# actual = [1, 2, 3, 4]\n# predicted = [1.0, 2.5, 3.0, 4.0]\n# print(mean_squared_error(actual, predicted))  # Output: 0.0625\n```\n\n### Explanation:\n1. **Type Checking**: The function first checks if both `actual` and `predicted` are lists. If they are, it converts them to NumPy arrays for consistent operations.\n2. **Shape Comparison**: It then checks if the shapes of the two arrays are equal. If not, it returns `-1`, indicating that the sequences cannot be compared.\n3. **Squared Differences**: It calculates the squared differences between corresponding elements of the two arrays.\n4. **Mean Calculation**: It computes the mean of these squared differences to get the MSE.\n5. **Rounding**: Finally, it rounds the MSE to four decimal places before returning it.\n\nThis function handles both Python lists and NumPy arrays, ensuring compatibility with the given requirements."}
{"task_id": 216, "completion_id": 0, "completion": "To implement Elastic Net regression from scratch, we need to follow several steps:\n\n1. **Build the Feature Matrix**: This involves expanding the original features into higher-degree polynomials and adding a bias term.\n\n2. **Standardize Features**: Subtract the mean and divide by the standard deviation for all features except the bias term.\n\n3. **Initialize Weights**: Start with random weights for the model.\n\n4. **Gradient Descent**: Perform batch gradient descent to minimize the mean-squared error with an Elastic Net penalty.\n\n5. **Predict**: Use the trained weights to make predictions on new data.\n\nHere's the implementation of the `elastic_net_regression` function:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Check input dimensions\n    if len(x_train) != len(y_train):\n        return -1\n    if len(x_test) == 0:\n        return []\n    \n    # Convert inputs to numpy arrays\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Check if shapes are compatible\n    if x_train.shape[1] != x_test.shape[1]:\n        return -1\n    \n    # Add bias term\n    x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n    x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n    \n    # Polynomial expansion\n    powers = np.arange(degree + 1)\n    poly_x_train = np.power(x_train[:, 1:], powers).T\n    poly_x_test = np.power(x_test[:, 1:], powers).T\n    \n    # Standardize features\n    means = np.mean(poly_x_train, axis=0)\n    stds = np.std(poly_x_train, axis=0)\n    poly_x_train = (poly_x_train - means) / stds\n    poly_x_test = (poly_x_test - means) / stds\n    \n    # Initialize weights\n    num_features = poly_x_train.shape[1]\n    w = np.random.rand(num_features)\n    \n    # Gradient descent\n    for _ in range(n_iterations):\n        # Calculate predictions\n        y_pred = np.dot(poly_x_train, w)\n        \n        # Compute loss\n        residuals = y_pred - y_train\n        mse = np.mean(residuals**2)\n        l1_penalty = reg_factor * np.sum(np.abs(w[1:]))\n        l2_penalty = reg_factor * np.sum(w[1:]**2)\n        total_loss = mse + (l1_ratio * l1_penalty) + ((1 - l1_ratio) * l2_penalty)\n        \n        # Compute gradients\n        grad_mse = 2 * residuals / len(y_train)\n        grad_l1 = reg_factor * np.sign(w[1:])\n        grad_l2 = 2 * reg_factor * w[1:]\n        grad_total = grad_mse + (l1_ratio * grad_l1) + ((1 - l1_ratio) * grad_l2)\n        \n        # Update weights\n        w -= learning_rate * grad_total\n    \n    # Make predictions on test set\n    y_pred_test = np.dot(poly_x_test, w)\n    y_pred_rounded = np.round(y_pred_test, 4)\n    \n    return y_pred_rounded.tolist()\n\n# Example usage:\n# x_train = [[1, 2], [3, 4]]\n# y_train = [5, 6]\n# x_test = [[5, 6]]\n# print(elastic_net_regression(x_train, y_train, x_test))\n```\n\n### Explanation:\n- **Feature Expansion**: We expand the features using polynomial powers up to the specified degree.\n- **Bias Term**: We add a bias term to the feature matrix.\n- **Standardization**: We standardize the features (excluding the bias term) to have zero mean and unit variance.\n- **Weight Initialization**: We initialize the weights randomly.\n- **Gradient Descent**: We perform batch gradient descent to minimize the mean-squared error with an Elastic Net penalty. The sub-gradient for the L1 penalty is computed using the sign function.\n- **Prediction**: After training, we compute predictions on the test set and round them to four decimal places.\n\nThis implementation assumes that the input matrices are well-formed and contains no missing values. Adjustments may be needed for edge cases or additional functionality."}
{"task_id": 217, "completion_id": 0, "completion": "To implement the required functions for computing the gradient, Hessian, and probability of the logistic loss, we can follow these steps:\n\n1. Convert the input lists/tuples into NumPy arrays for efficient computation.\n2. Compute the sigmoid transformation for each prediction.\n3. Calculate the gradient and Hessian using the derived formulas.\n4. Round the results to six decimal places.\n5. Return the results as separate lists.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Check if the inputs have the same length\n    if actual.shape != predicted.shape:\n        raise ValueError(\"The lengths of 'actual' and 'predicted' must be equal.\")\n    \n    # Compute sigmoid transformation\n    probabilities = 1 / (1 + np.exp(-predicted))\n    \n    # Compute gradient\n    gradients = actual * probabilities * (-actual * predicted)\n    \n    # Compute Hessian\n    hessians = probabilities * (1 - probabilities)\n    \n    # Round results to 6 decimal places\n    gradients_rounded = np.round(gradients, decimals=6)\n    hessians_rounded = np.round(hessians, decimals=6)\n    probabilities_rounded = np.round(probabilities, decimals=6)\n    \n    # Convert NumPy arrays back to lists\n    gradient_list = gradients_rounded.tolist()\n    hessian_list = hessians_rounded.tolist()\n    probability_list = probabilities_rounded.tolist()\n    \n    return gradient_list, hessian_list, probability_list\n\n# Example usage:\nactual = [1, -1, 1]\npredicted = [-0.5, 0.5, -0.5]\ngradient, hessian, probability = logistic_components(actual, predicted)\nprint(\"Gradient:\", gradient)\nprint(\"Hessian:\", hessian)\nprint(\"Probability:\", probability)\n```\n\nThis code defines the `logistic_components` function, which computes the gradient, Hessian, and probability of the logistic loss for given actual and predicted values. It handles input validation, performs necessary computations, and rounds the results to six decimal places before returning them as lists."}
{"task_id": 218, "completion_id": 0, "completion": "To solve this problem, we need to evaluate the outcomes of both the player's and the dealer's hands according to the specified rules of Blackjack. Here's how you can implement the `blackjack_outcome` function:\n\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    def calculate_score(hand):\n        total = sum(hand)\n        num_aces = hand.count(1)\n        \n        if num_aces > 0 and total <= 11:\n            total += 10\n        \n        return total\n\n    player_score = calculate_score(player)\n    dealer_score = calculate_score(dealer)\n\n    if player_score > 21:\n        return -1  # Player busts\n    elif dealer_score > 21:\n        return 1   # Dealer busts, player wins\n    elif player_score == dealer_score:\n        return 0   # Draw\n    elif player_score > dealer_score:\n        return 1.5  # Player wins with a natural blackjack\n    else:\n        return 1   # Player wins normally\n\n# Example usage:\nprint(blackjack_outcome([1, 10], [1, 10]))  # Natural blackjack\nprint(blackjack_outcome([1, 2, 3], [4, 5]))  # Player wins\nprint(blackjack_outcome([7, 8], [9, 10]))  # Dealer wins\nprint(blackjack_outcome([10, 10], [2, 2]))  # Draw\n```\n\n### Explanation:\n\n1. **calculate_score Function**: This helper function calculates the score of a hand. It sums up the values of the cards and checks if there is an Ace that can be used as 11 without exceeding 21.\n\n2. **Player and Dealer Scores**: We calculate the scores for both the player and the dealer using the `calculate_score` function.\n\n3. **Outcome Evaluation**:\n   - If the player busts (`player_score > 21`), the player loses, so the reward is `-1`.\n   - If the dealer busts (`dealer_score > 21`) but the player does not, the player wins, so the reward is `+1`.\n   - If both scores are equal, it's a draw, and the reward is `0`.\n   - If the player's score is greater than the dealer's score, the player wins, and we check if the player has a natural blackjack. If they do, the reward is `1.5`; otherwise, it's `1`.\n   - If the player's score is less than the dealer's score, the player loses, so the reward is `-1`.\n\nThis implementation ensures that the function adheres to the rules of Blackjack and returns the correct reward based on the outcome of the round."}
{"task_id": 220, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef estimate_lda_params(C_wt, C_dt, alpha, beta):\n    \"\"\"Estimate the word-topic (phi) and document-topic (theta) distributions\n    for an LDA model using symmetric Dirichlet smoothing.\n\n    Parameters\n    ----------\n    C_wt : numpy.ndarray\n        Word\u2013topic count matrix of shape (V, T).\n    C_dt : numpy.ndarray\n        Document\u2013topic count matrix of shape (D, T).\n    alpha : float\n        Symmetric Dirichlet prior for theta (document\u2013topic).\n    beta : float\n        Symmetric Dirichlet prior for phi  (word\u2013topic).\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        Rounded (to 4 decimals) phi and theta matrices as nested Python lists.\n    \"\"\"\n    # Check if either of the count matrices is empty\n    if not C_wt.any() or not C_dt.any():\n        return ([], [])\n\n    V = C_wt.shape[0]  # Vocabulary size\n    T = C_dt.shape[1]  # Number of topics\n\n    # Calculate phi\n    phi = (C_wt + beta) / (np.sum(C_wt, axis=0)[:, np.newaxis] + V * beta)\n\n    # Calculate theta\n    theta = (C_dt + alpha) / (np.sum(C_dt, axis=1)[:, np.newaxis] + T * alpha)\n\n    # Round the results to 4 decimal places\n    phi_rounded = np.round(phi, 4).tolist()\n    theta_rounded = np.round(theta, 4).tolist()\n\n    return (phi_rounded, theta_rounded)\n```\n\nThis function first checks if either of the input count matrices is empty. If so, it returns two empty lists for both `phi` and `theta`. Otherwise, it calculates the smoothed `phi` and `theta` distributions using the provided formulas and rounds each element to four decimal places before returning them as nested lists."}
{"task_id": 221, "completion_id": 0, "completion": "Here's how you can implement the Mean Squared Logarithmic Error (MSLE) in Python:\n\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Calculate the element-wise squared logarithmic error between actual and predicted values.\"\"\"\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)):\n        raise ValueError(\"Inputs must be lists, tuples, or NumPy arrays.\")\n    \n    if len(actual) != len(predicted):\n        raise ValueError(\"The lengths of actual and predicted must be the same.\")\n    \n    if not all(x >= 0 for x in actual) or not all(x >= 0 for x in predicted):\n        raise ValueError(\"All values in both actual and predicted must be non-negative.\")\n    \n    # Calculate the squared logarithmic errors\n    errors = np.log1p(actual) - np.log1p(predicted)\n    squared_errors = errors ** 2\n    \n    return squared_errors\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n    \n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n    \n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns -1.\n    \"\"\"\n    try:\n        squared_errors = squared_log_error(actual, predicted)\n        msle = np.mean(squared_errors)\n        return round(msle, 4)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return -1\n\n# Example usage:\n# actual = [1.0, 2.0, 3.0]\n# predicted = [1.5, 2.5, 3.5]\n# print(mean_squared_log_error(actual, predicted))\n```\n\n### Explanation:\n\n1. **Helper Function (`squared_log_error`)**:\n   - Validates that both inputs are either lists, tuples, or NumPy arrays.\n   - Checks if they have the same length and contains only non-negative values.\n   - Computes the squared logarithmic error using `np.log1p`, which computes `log(1 + x)` for each element.\n\n2. **Main Function (`mean_squared_log_error`)**:\n   - Calls the helper function to get the squared errors.\n   - Calculates the mean of these squared errors.\n   - Rounds the result to four decimal places.\n   - Handles exceptions and returns `-1` if any validation fails.\n\nThis implementation ensures that the inputs meet the specified requirements and provides a robust way to calculate the MSLE."}
{"task_id": 222, "completion_id": 0, "completion": "To implement the sigmoid activation function in Python, we need to handle both scalar and array-like inputs. We'll use NumPy for efficient computation, especially for handling arrays. Here's how you can implement the function:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already one\n    z = np.array(z)\n    \n    # Use NumPy's where function to apply the sigmoid formula efficiently\n    # This handles large values by using logaddexp to avoid overflow\n    result = np.where(z >= 0,\n                       1 / (1 + np.exp(-z)),\n                       np.exp(z) / (np.exp(z) + 1))\n    \n    # Round the result to 4 decimal places\n    rounded_result = np.round(result, decimals=4)\n    \n    # Convert the result back to a Python list if necessary\n    if isinstance(z, np.ndarray) and len(z.shape) > 0:\n        return rounded_result.tolist()\n    else:\n        return rounded_result.item()\n\n# Example usage:\nprint(sigmoid(1000))      # Output: 1.0000\nprint(sigmoid(-1000))     # Output: 0.0000\nprint(sigmoid([1, -1, 2]))  # Output: [0.7311, 0.2689, 0.8808]\nprint(sigmoid(np.array([1, -1, 2])))  # Output: [0.7311, 0.2689, 0.8808]\n```\n\n### Explanation:\n- **Input Handling**: The function first converts the input `z` to a NumPy array. This allows us to leverage NumPy's vectorized operations.\n- **Numerical Stability**: For large values of `z`, using `np.logaddexp` helps prevent overflow. It computes `log(exp(x) + exp(y))` in a numerically stable way.\n- **Element-Wise Operation**: The `np.where` function is used to apply the sigmoid formula element-wise based on whether each element of `z` is greater than or equal to zero.\n- **Rounding**: The result is rounded to four decimal places using `np.round`.\n- **Output Conversion**: If the input was a list or array, the result is converted back to a Python list. Otherwise, it remains a single float.\n\nThis implementation ensures that the function works correctly for both scalar and array inputs and maintains numerical stability."}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    def apply_leaky_relu(x):\n        if isinstance(x, (list, tuple)):\n            return [apply_leaky_relu(sub_x) for sub_x in x]\n        elif isinstance(x, np.ndarray):\n            return x.clip(0, None).tolist() + a * x.clip(None, 0).tolist()\n        else:\n            return max(x, a * x)\n\n    return apply_leaky_relu(z)\n```\n\nThis implementation defines a recursive function `apply_leaky_relu` that processes each element of the input `z`. If the element is a list or tuple, it recursively applies the Leaky ReLU to its elements. If the element is a NumPy array, it clips the positive and negative parts separately and combines them with the appropriate slope. For scalars, it directly applies the Leaky ReLU formula. The result is then converted to a Python list using `tolist()`."}
{"task_id": 225, "completion_id": 0, "completion": "Here's how you can implement the `hz2mel` function to convert frequencies from Hertz to the mel scale using either the HTK or Slaney formula:\n\n```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str = \"htk\") -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    # Validate the formula parameter\n    if formula not in [\"htk\", \"slaney\"]:\n        raise AssertionError(\"Formula must be 'htk' or 'slaney'\")\n    \n    # Convert the input to a NumPy array\n    hz_array = np.array(hz)\n    \n    # Apply the appropriate formula\n    if formula == \"htk\":\n        mel_values = 2595 * np.log10(1 + hz_array / 700)\n    elif formula == \"slaney\":\n        mel_values = 1127 * np.log(1 + hz_array / 700)\n    \n    # Round the results to 4 decimal places and convert to a list\n    mel_list = mel_values.round(4).tolist()\n    \n    return mel_list\n\n# Example usage\nhz = np.array([0, 6300])\nprint(hz2mel(hz, formula=\"htk\"))  # Output: [0.0, 2595.0]\n```\n\n### Explanation:\n1. **Validation**: The function first checks if the provided `formula` is either `\"htk\"` or `\"slaney\"`. If not, it raises an `AssertionError`.\n\n2. **Conversion**: It converts the input `hz` into a NumPy array for easier manipulation. Depending on the specified `formula`, it applies the corresponding mel-scale conversion formula:\n   - For the HTK formula: \\( \\text{mel} = 2595 \\cdot \\log_{10}(1 + \\frac{\\text{hz}}{700}) \\).\n   - For the Slaney formula: \\( \\text{mel} = 1127 \\cdot \\ln(1 + \\frac{\\text{hz}}{700}) \\).\n\n3. **Rounding and Conversion**: The resulting mel values are rounded to four decimal places using `np.round()`, and then converted to a Python list using `tolist()`.\n\nThis implementation ensures that the output is correctly formatted and meets the requirements specified in the task description."}
{"task_id": 226, "completion_id": 0, "completion": "To implement the AdaBoost algorithm using decision stumps, we need to follow these steps:\n\n1. Initialize the sample weights `w` to be equal for all samples.\n2. For each iteration `i` from 1 to `n_clf`:\n   - Find the best decision stump `(feature_index, threshold, polarity)` that minimizes the weighted classification error.\n   - Compute the learner weight `alpha_i` using the formula `alpha_i = 0.5 * ln((1 - error) / (error + 1e-10))`.\n   - Update the sample weights `w` based on the current stump's predictions.\n3. Predict the labels for the test set using the ensemble of decision stumps.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    w = np.ones(n_samples) / n_samples  # Initial weights\n    \n    predictions = []\n    \n    for _ in range(n_clf):\n        # Step 1: Find the best decision stump\n        min_error = float('inf')\n        best_stump = None\n        \n        for feature_idx in range(n_features):\n            thresholds = np.unique(X_train[:, feature_idx])\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    # Predictions for the current stump\n                    y_pred = (X_train[:, feature_idx] > threshold) == polarity\n                    \n                    # Calculate the weighted error\n                    error = np.sum(w[y_train != y_pred])\n                    \n                    # If this is the smallest error found so far, update the best stump\n                    if error < min_error:\n                        min_error = error\n                        best_stump = (feature_idx, threshold, polarity)\n        \n        # Step 2: Compute the learner weight alpha_i\n        alpha_i = 0.5 * np.log((1 - min_error) / (min_error + 1e-10))\n        \n        # Step 3: Update the sample weights\n        w *= np.exp(-alpha_i * y_train * y_pred)\n        w /= np.sum(w)\n        \n        # Step 4: Make predictions using the ensemble\n        y_pred_ensemble = np.sign(np.dot(w, y_train))\n        predictions.append(y_pred_ensemble)\n    \n    return predictions[0]\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predictions = adaboost_predict(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n- **Initialization**: We start by initializing the sample weights `w` to be uniform.\n- **Finding the Best Stump**: For each feature and threshold combination, we check both possible polarities (1 and -1). We calculate the weighted classification error for each stump and keep track of the stump with the lowest error.\n- **Updating Weights**: After finding the best stump, we update the sample weights based on the current stump's predictions.\n- **Predicting**: Finally, we use the ensemble of decision stumps to make predictions on the test set by computing the weighted sum of the individual stump predictions. The final predictions are the signs of these sums."}
{"task_id": 227, "completion_id": 0, "completion": "To implement the two-hidden-layer value network forward pass in NumPy, we need to follow these steps:\n\n1. Validate the input matrices to ensure they have compatible shapes.\n2. Perform the forward pass through the network using the provided weights.\n3. Round the final value to four decimal places.\n4. Return the result or -1 if there is a shape error.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef value_network_forward(state: list[float],\n                          W1: list[list[float]],\n                          W2: list[list[float]],\n                          W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\n\n    Parameters\n    ----------\n    state : list[float]\n        1-D state vector of length N.\n    W1    : list[list[float]]\n        Weight matrix with shape (N, H1).\n    W2    : list[list[float]]\n        Weight matrix with shape (H1, H2).\n    W3    : list[list[float]] | list[float]\n        Weight matrix with shape (H2, 1) or (H2,) for the output layer.\n\n    Returns\n    -------\n    float\n        The scalar state-value rounded to 4 decimals, or \u22121 on shape error.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    state = np.array(state)\n    W1 = np.array(W1)\n    W2 = np.array(W2)\n    W3 = np.array(W3)\n    \n    # Extract dimensions from the weight matrices\n    N, H1 = W1.shape\n    _, H2 = W2.shape\n    \n    # Check if the dimensions match\n    if not (state.shape == (N,) and W1.shape == (N, H1) and W2.shape == (H1, H2)):\n        return -1\n    \n    # Perform the forward pass\n    h1 = np.tanh(np.dot(state, W1))\n    h2 = np.tanh(np.dot(h1, W2))\n    v = np.dot(h2, W3)\n    \n    # Round the final value to 4 decimal places\n    return round(v[0], 4)\n\n# Example usage:\nstate = [1.0, 2.0, 3.0]\nW1 = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\nW2 = [[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]\nW3 = [[0.13], [0.14]]\n\nresult = value_network_forward(state, W1, W2, W3)\nprint(result)\n```\n\n### Explanation:\n- We first convert the input lists to NumPy arrays for efficient computation.\n- We extract the dimensions of the weight matrices `W1` and `W2`.\n- We validate the shapes of `state`, `W1`, and `W2` to ensure they are consistent.\n- We perform the forward pass by computing `h1`, `h2`, and `v` using matrix multiplication and the hyperbolic tangent activation function.\n- Finally, we round the result to four decimal places and return it. If any dimension check fails, we return `-1`."}
{"task_id": 228, "completion_id": 0, "completion": "To solve this problem, we need to compute the pairwise Euclidean distances between each row of two matrices \\( X \\) and \\( Y \\). The Euclidean distance between two vectors \\( x_i \\) and \\( y_j \\) is given by:\n\n\\[ d(x_i, y_j) = \\sqrt{\\sum_{k=1}^{C} (x_{ik} - y_{jk})^2} \\]\n\nWe can use the vectorized approach to compute these distances efficiently without using explicit loops. Here's how you can implement this in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the pairwise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If *Y* is ``None`` the function should compute the distances inside *X* itself.\n    All distances must be rounded to four decimal places and the function must\n    return a plain Python nested list (use ``tolist()`` if needed).\n\n    Args:\n        X: NumPy array with shape (N, C).\n        Y: NumPy array with shape (M, C) or ``None``.  Default is ``None``.\n\n    Returns:\n        Nested list representing the N \u00d7 M matrix of Euclidean distances.\n    \"\"\"\n    # Ensure Y is the same shape as X if it's not provided\n    if Y is None:\n        Y = X\n    \n    # Compute the squared differences along the last axis\n    diff_squared = (X[:, :, np.newaxis] - Y[:, np.newaxis, :])**2\n    \n    # Sum the squared differences over the last two axes\n    sum_of_squares = np.sum(diff_squared, axis=(-1, -2))\n    \n    # Take the square root to get the Euclidean distances\n    distances = np.sqrt(sum_of_squares)\n    \n    # Round the distances to 4 decimal places\n    rounded_distances = np.round(distances, decimals=4)\n    \n    # Convert the result to a list of lists\n    return rounded_distances.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5, 6], [7, 8]])\n\ndistances = pairwise_l2_distances(X, Y)\nprint(distances)\n```\n\n### Explanation:\n1. **Handling `Y`**: If `Y` is `None`, we set it equal to `X`.\n2. **Squared Differences**: We compute the squared differences between corresponding elements of `X` and `Y`. This is done using broadcasting to ensure that each element of `X` is compared with each element of `Y`.\n3. **Summation**: We sum the squared differences along the last two axes to get the sum of squares for each pair of vectors.\n4. **Square Root**: We take the square root of these sums to obtain the Euclidean distances.\n5. **Rounding**: We round the distances to four decimal places.\n6. **Conversion to List**: Finally, we convert the resulting NumPy array to a list of lists for the desired output format.\n\nThis implementation efficiently computes the required distances using vectorized operations, avoiding explicit loops."}
{"task_id": 230, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gaussian Naive Bayes algorithm from scratch for a binary classification problem using NumPy.\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train, y_train, X_test):\n    \"\"\"\n    Implement the Gaussian Naive Bayes algorithm from scratch for a binary-classification problem.\n\n    Parameters:\n    - X_train: np.ndarray, shape (n_samples, n_features), input features of the training data\n    - y_train: np.ndarray, shape (n_samples,), target labels (binary: 0 or 1) of the training data\n    - X_test: np.ndarray, shape (m_samples, n_features), input features of the test data\n\n    Returns:\n    - predictions: List[int], predicted labels for all test samples\n    \"\"\"\n\n    # Step 1: Estimate class priors\n    n_samples, n_features = X_train.shape\n    prior_0 = np.mean(y_train == 0)\n    prior_1 = np.mean(y_train == 1)\n\n    # Step 2: Estimate means and variances for each feature and class\n    mu_0 = np.mean(X_train[y_train == 0], axis=0)\n    mu_1 = np.mean(X_train[y_train == 1], axis=0)\n    \n    sigma_0 = np.var(X_train[y_train == 0], axis=0) + 1e-9\n    sigma_1 = np.var(X_train[y_train == 1], axis=0) + 1e-9\n\n    # Step 3: Compute the log-posterior probabilities for each test sample\n    def log_gaussian(x, mu, sigma):\n        return -0.5 * np.log(2 * np.pi * sigma) - ((x - mu) ** 2) / (2 * sigma)\n\n    log_posterior_0 = np.sum(log_gaussian(X_test, mu_0, sigma_0), axis=1) + np.log(prior_0)\n    log_posterior_1 = np.sum(log_gaussian(X_test, mu_1, sigma_1), axis=1) + np.log(prior_1)\n\n    # Step 4: Assign the label with the larger posterior\n    predictions = np.where(log_posterior_0 > log_posterior_1, 0, 1).tolist()\n\n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Load your dataset\n# predictions = gaussian_naive_bayes(X_train, y_train, X_test)\n```\n\n### Explanation:\n\n1. **Class Priors**: The prior probabilities \\(P(c)\\) are estimated as the relative frequency of class labels 0 and 1 in the training data.\n\n2. **Feature Means and Variances**: The mean \\(\\mu_{c,j}\\) and variance \\(\\sigma^2_{c,j}\\) for each feature \\(j\\) and class \\(c\\) are calculated from the training data.\n\n3. **Log-Gaussian Probability Calculation**: The log-probability density function (pdf) of a Gaussian distribution is used to calculate the likelihood of a test sample given its features and the estimated parameters.\n\n4. **Posterior Probability**: The posterior probability \\(P(c|x)\\) is computed as the product of the likelihood and the prior, and then the logarithm of this product is taken to simplify calculations.\n\n5. **Prediction Assignment**: The test samples are assigned the label corresponding to the higher posterior probability.\n\nThis implementation assumes that the input data is already loaded into `X_train`, `y_train`, and `X_test`. You will need to replace `load_data()` with your actual data loading mechanism."}
{"task_id": 231, "completion_id": 0, "completion": "To solve this problem, we need to implement a function `is_natural` that checks if a given list of integers represents a natural (Blackjack) hand. A natural hand consists of exactly two cards where one is an Ace (represented by 1) and the other is a ten-valued card (represented by 10).\n\nHere's how you can implement this function:\n\n```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"\n    Determine if the provided hand is a natural (Blackjack) hand.\n\n    Args:\n    hand (list[int]): A list of integers representing the ranks of the cards.\n                     Ace is represented by 1, and ten-valued cards by 10.\n\n    Returns:\n    bool: True if the hand is a natural hand, False otherwise.\n    \"\"\"\n    # Check if the hand has exactly two cards\n    if len(hand) != 2:\n        return False\n    \n    # Check if both cards are either an Ace or a ten-valued card\n    return (hand[0] == 1 and hand[1] in {10}) or (hand[1] == 1 and hand[0] in {10})\n\n# Example usage:\nprint(is_natural([1, 10]))  # True\nprint(is_natural([10, 1]))  # True\nprint(is_natural([1, 5]))   # False\nprint(is_natural([1, 1, 10])) # False\nprint(is_natural([2, 3]))   # False\n```\n\n### Explanation:\n- The function first checks if the length of the `hand` list is exactly 2. If not, it returns `False` because a natural hand must consist of exactly two cards.\n- It then checks if either of the two cards is an Ace (represented by 1) and the other is a ten-valued card (represented by 10). This is done using a logical OR condition.\n- The function returns `True` if either of these conditions is met, indicating that the hand is a natural hand. Otherwise, it returns `False`."}
{"task_id": 232, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    # Check if the array is two-dimensional\n    if len(x.shape) != 2:\n        return False\n    \n    # Check if every element is either 0 or 1\n    if not np.all((x == 0) | (x == 1)):\n        return False\n    \n    # Check if each row contains exactly one 1\n    if not np.all(np.sum(x, axis=1) == 1):\n        return False\n    \n    return True\n```"}
{"task_id": 234, "completion_id": 0, "completion": "To implement the single-step Thompson sampling for a Bernoulli multi-armed bandit using a Beta-Binomial prior, we need to follow these steps:\n\n1. Check if the inputs are valid. Specifically, ensure that both `successes` and `failures` lists are non-empty and have the same length.\n2. Initialize the random seed if provided.\n3. For each arm, draw a sample from its corresponding Beta distribution using the posterior parameters.\n4. Return the index of the arm with the highest sampled value. In case of ties, select the arm with the smallest index.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta: float = 1.0,\n                                    seed: int | None = None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\n\n    Parameters\n    ----------\n    successes : list[int]\n        Number of observed successes for every arm.\n    failures  : list[int]\n        Number of observed failures for every arm (same length as *successes*).\n    prior_alpha : float, optional (default=1.0)\n        Shared Alpha parameter of the Beta prior.\n    prior_beta  : float, optional (default=1.0)\n        Shared Beta  parameter of the Beta prior.\n    seed : int | None, optional (default=None)\n        If given, NumPy\u2019s random seed is set to this value to obtain\n        deterministic samples.\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest sampled payoff probability, or\n        -1 if the input is malformed.\n    \"\"\"\n    # Check if inputs are valid\n    if len(successes) != len(failures):\n        return -1\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Initialize the best arm index and maximum sample value\n    max_sample_value = -np.inf\n    best_arm_index = -1\n    \n    # Iterate over each arm\n    for i in range(len(successes)):\n        # Calculate the posterior parameters\n        alpha_posterior = prior_alpha + successes[i]\n        beta_posterior = prior_beta + failures[i]\n        \n        # Draw a sample from the Beta distribution\n        sample_value = np.random.beta(alpha_posterior, beta_posterior)\n        \n        # Update the best arm if the current sample is higher\n        if sample_value > max_sample_value:\n            max_sample_value = sample_value\n            best_arm_index = i\n        elif sample_value == max_sample_value:\n            # In case of a tie, choose the arm with the smaller index\n            best_arm_index = min(best_arm_index, i)\n    \n    return best_arm_index\n\n# Example usage:\n# successes = [10, 5, 8]\n# failures = [20, 7, 6]\n# print(thompson_sampling_beta_binomial(successes, failures))\n```\n\nThis function checks for valid input, sets the random seed if specified, and uses the `numpy.random.beta` function to draw samples from the Beta distributions. It then determines the arm with the highest sample value and handles ties by selecting the arm with the smallest index."}
{"task_id": 235, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as `labels.max() + 1`.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n        number of labels and C is the number of classes.  Each row is a\n        one-hot vector corresponding to the associated label.\n    \"\"\"\n    if labels.ndim != 1:\n        raise ValueError(\"Input array must be 1-D.\")\n    \n    if n_classes is None:\n        n_classes = max(labels) + 1\n    else:\n        if n_classes < max(labels) + 1:\n            raise ValueError(\"n_classes must be at least max(labels) + 1.\")\n    \n    one_hot_matrix = np.eye(n_classes)[labels]\n    return one_hot_matrix.astype(float)\n\n# Example usage:\nlabels = np.array([0, 1, 2, 3])\none_hot_encoded = to_one_hot(labels)\nprint(one_hot_encoded.tolist())\n```\n\nThis implementation checks for the correct dimensions of the input array and handles cases where `n_classes` is not provided or is insufficient. It then uses NumPy's `eye` function to create the identity matrix of size `(n_classes, n_classes)` and selects the appropriate rows based on the input labels to produce the one-hot encoded matrix. The resulting matrix has a float data type, as required."}
{"task_id": 236, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minibatch(X: np.ndarray, batchsize: int = 256, shuffle: bool = True):\n    \"\"\"Create a generator that yields index mini-batches.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (N, *). The first axis holds the samples.\n    batchsize : int, default = 256\n        Desired mini-batch size. The last batch may be smaller.\n    shuffle : bool, default = True\n        Whether to shuffle sample indices before splitting.\n\n    Returns\n    -------\n    tuple\n        (mb_generator, n_batches)\n        mb_generator : generator yielding NumPy integer arrays \u2013 the indices\n            of each mini-batch in the order they are processed.\n        n_batches : int \u2013 total number of mini-batches.\n    \"\"\"\n    # Get the number of samples\n    N = X.shape[0]\n    \n    # Shuffle indices if required\n    if shuffle:\n        indices = np.arange(N)\n        np.random.shuffle(indices)\n    else:\n        indices = np.arange(N)\n    \n    # Calculate the number of batches\n    n_batches = np.ceil(N / batchsize).astype(int)\n    \n    # Create the generator for mini-batches\n    def mb_generator():\n        for i in range(n_batches):\n            start = i * batchsize\n            end = min((i + 1) * batchsize, N)\n            yield indices[start:end]\n    \n    return mb_generator(), n_batches\n```"}
{"task_id": 237, "completion_id": 0, "completion": "To implement the Blackjack hand scorer, we need to evaluate the total score of a given hand while considering the special rule for the Ace card. Here's how you can achieve this:\n\n1. Define the point values for each type of card:\n   - Numeric cards from '2' to '10' are worth their face value.\n   - Face cards 'J', 'Q', 'K' are worth 10 points each.\n   - The Ace 'A' can be worth either 1 or 11, depending on the context to keep the total score below 21.\n\n2. Iterate through the hand and calculate the initial score by summing up the values of the non-Ace cards.\n\n3. Determine the number of Aces in the hand.\n\n4. For each Ace, check if adding 11 would cause the total score to exceed 21. If not, add 11; otherwise, add 1.\n\n5. Calculate the final score by adding the total of non-Ace cards and the adjusted value of Aces.\n\n6. If the final score exceeds 21, return 0. Otherwise, return the final score.\n\nHere's the implementation of the `score` function:\n\n```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    # Define the point values for each card\n    card_values = {\n        '2': 2,\n        '3': 3,\n        '4': 4,\n        '5': 5,\n        '6': 6,\n        '7': 7,\n        '8': 8,\n        '9': 9,\n        '10': 10,\n        'J': 10,\n        'Q': 10,\n        'K': 10,\n        'A': 11  # Initial value of Ace\n    }\n    \n    # Initialize the total score\n    total_score = 0\n    \n    # Count the number of Aces\n    ace_count = 0\n    \n    # Calculate the total score excluding Aces\n    for card in hand:\n        if card != 'A':\n            total_score += card_values[card]\n        else:\n            ace_count += 1\n    \n    # Adjust the score for Aces\n    while ace_count > 0 and total_score > 21:\n        total_score -= 10\n        ace_count -= 1\n    \n    # Check if the hand is bust\n    if total_score > 21:\n        return 0\n    \n    return total_score\n```\n\nThis function will correctly compute the Blackjack score for any valid hand according to the rules specified."}
{"task_id": 239, "completion_id": 0, "completion": "```python\nimport math\n\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str = \"same\") -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\n\n    Parameters\n    ----------\n    filter_shape : tuple[int, int]\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : str, optional (default=\"same\")\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    tuple[tuple[int, int], tuple[int, int]]\n        ((pad_top, pad_bottom), (pad_left, pad_right)) where every value is an\n        integer \u2265 0.\n    \"\"\"\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n\n    filter_height, filter_width = filter_shape\n    input_height, input_width = filter_height, filter_width  # Assuming square input for simplicity\n\n    # Calculate the necessary padding for each dimension\n    pad_top = (filter_height - 1) // 2\n    pad_bottom = (filter_height - 1) // 2 + (filter_height - 1) % 2\n    pad_left = (filter_width - 1) // 2\n    pad_right = (filter_width - 1) // 2 + (filter_width - 1) % 2\n\n    return ((pad_top, pad_bottom), (pad_left, pad_right))\n```\n\nThis function calculates the necessary padding for both the top/bottom and left/right borders based on the filter size and the desired output shape. It handles the case where the output shape is \"same\" by ensuring the output dimensions match the input dimensions after applying the convolution. For \"valid\", it ensures no padding is applied. The padding values are calculated using integer division and the ceiling function to ensure the correct number of zeros are added."}
{"task_id": 240, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_encoding_dicts(obs_ids: list, action_ids: list,\n                          obs_encoder: dict | None = None,\n                          act_encoder: dict | None = None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    \n    if obs_encoder is None:\n        obs_encoder = {id_: idx for idx, id_ in enumerate(obs_ids)}\n    else:\n        assert set(obs_encoder.keys()) == set(obs_ids), \"Obs IDs do not match encoder keys.\"\n    \n    if act_encoder is None:\n        act_encoder = {id_: idx for idx, id_ in enumerate(action_ids)}\n    else:\n        assert set(act_encoder.keys()) == set(action_ids), \"Act IDs do not match encoder keys.\"\n    \n    num2obs = {v: k for k, v in obs_encoder.items()}\n    num2act = {v: k for k, v in act_encoder.items()}\n    \n    return {\n        'obs2num': obs_encoder,\n        'num2obs': num2obs,\n        'act2num': act_encoder,\n        'num2act': num2act\n    }\n\n# Example usage:\nobs_ids = ['left', 'right', 'front', 'back']\naction_ids = ['move', 'turn']\nencoding_dicts = create_encoding_dicts(obs_ids, action_ids)\nprint(encoding_dicts)\n```\n\n### Explanation:\n- **Initialization**: We first check if `obs_encoder` or `act_encoder` is provided. If not, we create new dictionaries using the `enumerate` function to map each identifier to a unique integer.\n- **Validation**: We ensure that the identifiers in the provided lists match those in the encoders. This prevents inconsistencies.\n- **Inverse Dictionaries**: We generate `num2obs` and `num2act` by reversing the mappings from `obs2num` and `act2num`, respectively.\n- **Return**: Finally, we return a dictionary containing all four mappings."}
{"task_id": 241, "completion_id": 0, "completion": "To implement the orthogonal weight initializer as described, we need to follow the steps outlined in the problem statement. Here's how you can achieve this using NumPy:\n\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n\n    # Step 1: Create a 2-D matrix A filled with i.i.d. samples from the standard normal distribution\n    d0, d1, *rest = shape\n    A = np.random.randn(d0, d1)\n\n    # Step 2: Compute the singular value decomposition (SVD) of A\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # Step 3: Choose the SVD factor that has the same size as A\n    k = min(d0, d1)\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n\n    # Step 4: Reshape Q back to the requested tensor shape and multiply it by scale\n    result = Q.reshape(shape).astype(float)\n    result *= scale\n\n    # Verify the orthogonality condition\n    flat = result.reshape(shape[0], -1)\n    if shape[0] <= flat.shape[1]:\n        assert np.allclose(flat @ flat.T, scale**2 * np.eye(shape[0]), atol=1e-6), \"Orthogonality condition not satisfied\"\n    else:\n        assert np.allclose(flat.T @ flat, scale**2 * np.eye(shape[1]), atol=1e-6), \"Orthogonality condition not satisfied\"\n\n    # Return the result as a list of lists\n    return result.tolist()\n\n# Example usage\nnp.random.seed(0)\nshape = (2, 2)\nscale = 0.5\nprint(orthogonal(shape, scale))\n```\n\n### Explanation:\n1. **Matrix Initialization**: We create a 2-D matrix `A` filled with i.i.d. samples from the standard normal distribution.\n2. **SVD Calculation**: We compute the Singular Value Decomposition (SVD) of `A`. This gives us matrices `U`, `S`, and `Vt`.\n3. **Choosing the SVD Factor**: We choose the SVD factor that matches the size of `A`. If `U` has the same shape as `A`, we use `U`; otherwise, we use `Vt`.\n4. **Reshaping and Scaling**: We reshape the chosen SVD factor into the desired shape and scale it by the given `scale`.\n5. **Verification**: We verify that the resulting matrix satisfies the orthogonality condition by checking the dot product of its rows or columns against the identity matrix scaled by `scale`.\n6. **Return**: Finally, we return the result as a list of lists.\n\nThis implementation ensures that the resulting tensor is orthogonal up to the specified scaling factor."}
{"task_id": 242, "completion_id": 0, "completion": "```python\nimport math\n\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float = 1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\n\n    PARAMETERS\n    ----------\n    ev_estimates : list[float]\n        Current empirical mean reward for every arm.\n    pull_counts  : list[int]\n        How many times each arm has been pulled so far (same length as\n        ``ev_estimates``).\n    current_step : int\n        The time-step about to be executed.  The quantity *T* in the UCB1\n        formula equals ``current_step + 1``.\n    C : float, default 1.0\n        Exploration parameter.  ``C = 1.0`` recovers the original UCB1.\n\n    RETURNS\n    -------\n    int\n        Index (0-based) of the arm selected by UCB1.\n    \"\"\"\n    T = current_step + 1\n    \n    # Check if any arm has not been pulled yet\n    for i in range(len(pull_counts)):\n        if pull_counts[i] == 0:\n            return i\n    \n    # Calculate UCB scores for all arms\n    ucb_scores = []\n    for i in range(len(ev_estimates)):\n        ucb_score = ev_estimates[i] + C * math.sqrt(2 * math.log(T) / pull_counts[i])\n        ucb_scores.append((i, ucb_score))\n    \n    # Find the arm with the highest UCB score\n    best_arm_index = max(ucb_scores, key=lambda x: x[1])[0]\n    \n    return best_arm_index\n```"}
{"task_id": 243, "completion_id": 0, "completion": "To implement the forward pass of the specified Actor-Critic network using NumPy, we need to follow the steps outlined in the problem description. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert the state list to a NumPy array\n    state_array = np.array(state)\n    \n    # Initialize weights and biases\n    W1 = np.ones((4, 4))\n    b1 = np.zeros(4)\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    \n    # Compute intermediate activations\n    z1 = np.maximum(0, np.dot(state_array, W1) + b1)\n    z2 = np.maximum(0, np.dot(z1, W2) + b2)\n    z3 = np.maximum(0, np.dot(z2, W3) + b3)\n    \n    # Compute logits for the actor head\n    logits = np.dot(z3, W_a) + b_a\n    \n    # Compute action probabilities using softmax\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    \n    # Compute the state value\n    state_value = np.dot(z3, W_c) + b_c\n    \n    # Round the results to 4 decimal places\n    action_probs_rounded = np.round(action_probs, 4)\n    state_value_rounded = round(state_value[0], 4)\n    \n    return action_probs_rounded.tolist(), state_value_rounded\n\n# Example usage:\nstate = [1.0, -0.5, 0.8]\naction_probs, state_value = actor_critic_forward(state)\nprint(\"Action Probabilities:\", action_probs)\nprint(\"State Value:\", state_value)\n```\n\n### Explanation:\n1. **Input State**: The input state is converted into a NumPy array.\n2. **Weight Initialization**: We initialize the weights and biases with values of 1.0 and 0.0, respectively.\n3. **Intermediate Activations**: We compute the intermediate activations using ReLU activation functions.\n4. **Logits Calculation**: The logits for the actor head are computed by multiplying the final hidden layer (`z3`) with the weight matrix `W_a` and adding the bias `b_a`.\n5. **Softmax Activation**: Action probabilities are obtained by applying the softmax function to the logits.\n6. **State Value Computation**: The state value is computed by multiplying the final hidden layer (`z3`) with the weight matrix `W_c` and adding the bias `b_c`.\n7. **Rounding**: Both the action probabilities and the state value are rounded to four decimal places before being returned.\n\nThis implementation ensures that the network behaves deterministically according to the given architecture and parameters."}
{"task_id": 244, "completion_id": 0, "completion": "To implement the `random_binary_tensor` function according to the specified requirements, we need to follow these steps:\n\n1. Validate the input parameters:\n   - Ensure `shape` is a tuple of positive integers.\n   - Ensure `sparsity` is a real number in the range `[0, 1]`.\n\n2. Use `numpy.random.rand` to generate random numbers between 0 and 1 for each position in the tensor.\n\n3. Apply the sparsity threshold to convert random numbers greater than or equal to `1 - sparsity` to `1.0` and less than `1 - sparsity` to `0.0`.\n\n4. Convert the resulting NumPy array to a regular nested Python list using `tolist()`.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    # Validate shape\n    if not isinstance(shape, tuple) or not all(isinstance(dim, int) and dim > 0 for dim in shape):\n        return -1\n    \n    # Validate sparsity\n    if not isinstance(sparsity, (int, float)) or not (0 <= sparsity <= 1):\n        return -1\n    \n    # Generate random tensor\n    random_numbers = np.random.rand(*shape)\n    \n    # Apply sparsity\n    binary_tensor = (random_numbers >= 1 - sparsity).astype(float)\n    \n    # Convert to list\n    return binary_tensor.tolist()\n\n# Example usage:\nnp.random.seed(0)\nresult = random_binary_tensor((2, 3), sparsity=0.5)\nprint(result)  # Output: [[1.0, 1.0, 1.0], [1.0, 0.0, 1.0]]\n```\n\n### Explanation:\n\n- **Shape Validation**: We check if `shape` is a tuple of positive integers. This is done using `isinstance(shape, tuple)` and ensuring all elements are integers greater than zero.\n\n- **Sparsity Validation**: We ensure `sparsity` is a real number within the range [0, 1]. This is checked using `isinstance(sparsity, (int, float))` and verifying it lies between 0 and 1.\n\n- **Random Number Generation**: We use `np.random.rand(*shape)` to generate a tensor of the specified shape filled with random numbers between 0 and 1.\n\n- **Binary Conversion**: We apply the sparsity threshold by comparing each element to `1 - sparsity` and converting it to a boolean tensor. Then, we cast this boolean tensor to a floating-point tensor where `True` becomes `1.0` and `False` becomes `0.0`.\n\n- **Conversion to List**: Finally, we convert the resulting NumPy array to a regular Python list using `tolist()`."}
{"task_id": 247, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]],\n                           payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Determine the arm with the highest expected reward in a Gaussian bandit.\n\n    The function must compute the expected reward for each arm (p_k * \u03bc_k) and\n    return a tuple (best_ev, best_arm), where best_ev is rounded to 4 decimal\n    places and best_arm is the index of the arm with that expected reward.  If\n    the input is invalid (see task description) the function should return -1.\n    \"\"\"\n    # Input validation\n    if len(payoff_dists) != len(payoff_probs):\n        return -1\n    \n    num_arms = len(payoff_dists)\n    \n    for i in range(num_arms):\n        mu, sigma_squared = payoff_dists[i]\n        \n        if sigma_squared <= 0 or not (0 <= payoff_probs[i] <= 1):\n            return -1\n    \n    # Calculate expected rewards\n    expected_rewards = [\n        prob * mu for mu, prob in zip(payoff_dists, payoff_probs)\n    ]\n    \n    # Find the maximum expected reward and its index\n    max_expected_reward = max(expected_rewards)\n    best_arm_index = expected_rewards.index(max_expected_reward)\n    \n    # Round the maximum expected reward to 4 decimal places\n    best_ev = round(max_expected_reward, 4)\n    \n    return best_ev, best_arm_index\n```\n\nThis function first validates the input to ensure that both `payoff_dists` and `payoff_probs` have the same length and that all variances are strictly positive and all probabilities are within the valid range. It then calculates the expected reward for each arm and determines which arm has the highest expected reward, rounding this value to four decimal places. If any input validation fails, it returns `-1`."}
{"task_id": 248, "completion_id": 0, "completion": "Here's the implementation of the `baseline_prob_predict` function:\n\n```python\nimport numpy as np\n\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix. Its values are ignored by this baseline\n        predictor; only its length is potentially useful for sanity checks.\n    y_train : np.ndarray\n        One-dimensional array of binary labels (0 for negative, 1 for positive).\n    X_test : np.ndarray\n        Feature matrix for which the predictions are required. The number of\n        returned probabilities must match the number of rows in this matrix.\n\n    Returns\n    -------\n    list[float]\n        A list containing the same probability repeated for every row of\n        `X_test`. The probability is rounded to four decimal places.\n    \"\"\"\n    if X_test.size == 0:\n        return []\n\n    # Compute the positive-class probability\n    p = np.mean(y_train)\n\n    # Create a NumPy array of the same length as X_test filled with p\n    predictions = np.full(X_test.shape[0], p)\n\n    # Round every probability to four decimal places\n    rounded_predictions = np.round(predictions, decimals=4).tolist()\n\n    return rounded_predictions\n```\n\n### Explanation:\n1. **Check for Empty Test Data**: If `X_test` is empty, the function returns an empty list immediately.\n2. **Compute Positive-Class Probability**: The probability `p` is calculated as the mean of the `y_train` array, which gives the empirical proportion of positive labels.\n3. **Create Predictions Array**: A NumPy array of the same shape as `X_test` is created and filled with the computed probability `p`.\n4. **Round Probabilities**: The probabilities are rounded to four decimal places using `np.round`.\n5. **Convert to List**: Finally, the rounded probabilities are converted to a Python list and returned."}
{"task_id": 249, "completion_id": 0, "completion": "Here's how you can implement the `actor_critic_forward` function using NumPy:\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to a NumPy array\n    state = np.array(state).reshape(1, -1)\n    \n    # First set of weights and biases for the shared layers\n    W1, b1, W2, b2, W3, b3 = map(np.array, [params['W1'], params['b1'], params['W2'], params['b2'], params['W3'], params['b3']])\n    \n    # Perform the first affine transformation and ReLU activation\n    hidden_1 = np.maximum(0, np.dot(state, W1) + b1)\n    \n    # Second set of weights and biases for the shared layers\n    hidden_2 = np.maximum(0, np.dot(hidden_1, W2) + b2)\n    \n    # Third set of weights and biases for the shared layers\n    hidden_3 = np.maximum(0, np.dot(hidden_2, W3) + b3)\n    \n    # Weights and biases for the actor head\n    Wa, ba = map(np.array, [params['Wa'], params['ba']])\n    \n    # Compute the raw scores for the actor head\n    actor_scores = np.dot(hidden_3, Wa) + ba\n    \n    # Apply softmax to get the probabilities\n    probabilities = np.exp(actor_scores) / np.sum(np.exp(actor_scores), axis=1, keepdims=True)\n    \n    # Weights and biases for the critic head\n    Wc, bc = map(np.array, [params['Wc'], params['bc']])\n    \n    # Compute the state value\n    value = np.dot(hidden_3, Wc) + bc\n    \n    # Round the results to 4 decimal places\n    probabilities_rounded = np.round(probabilities[0], decimals=4)\n    value_rounded = round(value[0], 4)\n    \n    return probabilities_rounded.tolist(), value_rounded\n\n# Example usage:\nstate = [0.1, 0.2, 0.3]\nparams = {\n    'W1': [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\n    'b1': [0.1, 0.2, 0.3],\n    'W2': [[7.0, 8.0], [9.0, 10.0]],\n    'b2': [0.4, 0.5],\n    'W3': [[11.0], [12.0]],\n    'b3': [0.6],\n    'Wa': [[13.0, 14.0], [15.0, 16.0]],\n    'ba': [0.7, 0.8],\n    'Wc': [[17.0], [18.0]],\n    'bc': [0.9]\n}\n\nprobabilities, value = actor_critic_forward(state, params)\nprint(\"Probabilities:\", probabilities)\nprint(\"Value:\", value)\n```\n\nThis code defines the `actor_critic_forward` function, which computes the forward pass through the specified Actor-Critic network using NumPy. It includes all necessary steps from input preprocessing to output rounding, ensuring the output meets the specified constraints."}
{"task_id": 251, "completion_id": 0, "completion": "Here's the implementation of the requested activation functions along with the `get_activation` function:\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Element-wise logistic sigmoid.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    \"\"\"Element-wise hyperbolic tangent.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    return np.maximum(0, x)\n\ndef leaky_relu(x, alpha: float = 0.01):\n    \"\"\"Element-wise Leaky-ReLU with slope *alpha* for x < 0.\"\"\"\n    return np.where(x > 0, x, alpha * x)\n\ndef softmax(x):\n    \"\"\"Numerically stable softmax over a 1-D input.\"\"\"\n    if isinstance(x, (list, tuple)):\n        x = np.array(x)\n    \n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n\ndef get_activation(name):\n    \"\"\"Return the activation function by *name*.\n\n    Args:\n        name (str): One of {\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"softmax\"}\n\n    Returns:\n        Callable: the requested activation function.\n\n    Raises:\n        ValueError: If *name* is not recognised.\n    \"\"\"\n    activations = {\n        \"sigmoid\": sigmoid,\n        \"tanh\": tanh,\n        \"relu\": relu,\n        \"leaky_relu\": leaky_relu,\n        \"softmax\": softmax\n    }\n    \n    if name not in activations:\n        raise ValueError('Invalid activation function.')\n    \n    return activations[name]\n\n# Example usage:\nprint(get_activation(\"sigmoid\")(np.array([0.5, -0.5])))\nprint(get_activation(\"tanh\")(np.array([-1.0, 1.0])))\nprint(get_activation(\"relu\")(np.array([-1.0, 0.0, 1.0])))\nprint(get_activation(\"leaky_relu\")(np.array([-1.0, 0.0, 1.0])))\nprint(get_activation(\"softmax\")(np.array([1.0, 2.0, 3.0])))\n```\n\n### Explanation:\n- **Sigmoid**: Computes the logistic function, which maps values from (-\u221e, \u221e) to (0, 1).\n- **Tanh**: Computes the hyperbolic tangent, which maps values from (-\u221e, \u221e) to (-1, 1).\n- **ReLU**: Computes the ReLU function, which sets all negative values to zero.\n- **Leaky ReLU**: Computes the Leaky ReLU function, which sets negative values to a small positive number (`alpha`) times the input.\n- **Softmax**: Computes the softmax function, which normalizes the elements of a vector so that they sum up to 1.\n\nThe `get_activation` function returns the appropriate activation function based on the input `name`. It also includes error handling for invalid activation names."}
{"task_id": 252, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    # Compute the gradient\n    grad = actual - predicted\n    \n    # Compute the Hessian (a list of ones)\n    hess = [1.0] * len(actual)\n    \n    return grad.tolist(), hess\n```"}
{"task_id": 253, "completion_id": 0, "completion": "To implement the `elastic_net_regularization` function, we need to calculate both the penalty value and the gradient based on the provided parameters. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert w to a NumPy array if it's not already\n    w = np.array(w)\n    \n    # Calculate the L2 norm of the weight vector\n    l2_norm = np.linalg.norm(w)\n    \n    # Compute the penalty value\n    penalty_value = alpha * (l1_ratio * l2_norm + (1 - l1_ratio) * 0.5 * np.dot(w, w))\n    \n    # Calculate the gradient if requested\n    if gradient:\n        gradient_value = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        return gradient_value.round(4).tolist()\n    else:\n        return round(penalty_value, 4)\n\n# Example usage:\nw = [1, 2, 3]\nalpha = 0.5\nl1_ratio = 0.5\n\nprint(elastic_net_regularization(w, alpha, l1_ratio))  # Output: 6.0\nprint(elastic_net_regularization(w, alpha, l1_ratio, gradient=True))  # Output: [0.0, 0.0, 0.0]\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `w` is a list and converts it to a NumPy array if necessary.\n2. **L2 Norm Calculation**: It computes the L2 norm of the weight vector `w`.\n3. **Penalty Value Calculation**: Using the formula for the Elastic-Net penalty, it calculates the penalty value.\n4. **Gradient Calculation**: If `gradient` is set to `True`, it computes the gradient using the formula for the Elastic-Net gradient. The result is rounded to four decimal places and returned as a list.\n5. **Return Type**: If `gradient` is `False`, the function returns the penalty value as a float, also rounded to four decimal places."}
{"task_id": 255, "completion_id": 0, "completion": "To determine if a Blackjack hand contains a usable ace, we need to check if there is at least one ace (value 1) in the hand such that adding 11 to its value does not cause the total hand value to exceed 21. Here's how you can implement this in Python:\n\n```python\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 without causing the\n    hand's total value to exceed 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    # Count the number of aces in the hand\n    num_aces = sum(1 for card in hand if card == 1)\n    \n    # Calculate the maximum possible total without using the ace as 11\n    max_total_without_usable_ace = sum(card for card in hand if card != 1)\n    \n    # Check if we can use one ace as 11 and still be within the limit\n    if max_total_without_usable_ace + 11 <= 21:\n        return True\n    \n    return False\n\n# Example usage:\nprint(usable_ace([1, 7]))  # True\nprint(usable_ace([1, 10, 10]))  # False\n```\n\n### Explanation:\n1. **Count the Aces**: We first count how many aces are present in the hand using a generator expression inside the `sum()` function.\n\n2. **Calculate Maximum Total Without Usable Ace**: We calculate the sum of all cards except the aces, which gives us the maximum possible total if no ace were used as 11.\n\n3. **Check Usable Ace Condition**: If adding 11 to the maximum total without a usable ace does not exceed 21, then the hand has a usable ace.\n\nThis function efficiently checks for the presence of a usable ace by leveraging simple arithmetic operations on the card values."}
{"task_id": 256, "completion_id": 0, "completion": "To implement the numerical gradient check for a vanilla RNN parameter using the centred finite difference formula, we need to follow these steps:\n\n1. Identify the correct parameter name from the input `param_name`.\n2. Create copies of the original parameter values to use for perturbation.\n3. Perturb the parameter values by adding and subtracting `epsilon`.\n4. Run the forward pass for `n_t` time-steps with the perturbed parameters.\n5. Calculate the loss for both perturbed states.\n6. Compute the numerical gradient using the centered finite difference formula.\n7. Store the numerical gradient in a list and return its transpose.\n\nHere's the implementation of the `grad_check_RNN` function:\n\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    if param_name in [\"Ba\", \"Bx\"]:\n        param_name = param_name.lower()\n    \n    if param_name == \"X\" or param_name == \"y\":\n        return None\n    \n    grads = []\n    \n    # Get the original parameter value\n    original_param = model.parameters[param_name].copy().astype(np.float64)\n    \n    # Iterate over each element in the parameter\n    for i in range(original_param.size):\n        # Perturb the parameter by +epsilon\n        perturbed_param_plus = original_param.copy()\n        perturbed_param_plus.flat[i] += epsilon\n        \n        # Perturb the parameter by -epsilon\n        perturbed_param_minus = original_param.copy()\n        perturbed_param_minus.flat[i] -= epsilon\n        \n        # Reset gradients\n        model.flush_gradients()\n        \n        # Forward pass with perturbed parameter +epsilon\n        predictions_plus = [model.forward(X[:, :, t]) for t in range(n_t)]\n        loss_plus = loss_func(predictions_plus)\n        \n        # Reset gradients\n        model.flush_gradients()\n        \n        # Forward pass with perturbed parameter -epsilon\n        predictions_minus = [model.forward(X[:, :, t]) for t in range(n_t)]\n        loss_minus = loss_func(predictions_minus)\n        \n        # Calculate the numerical gradient\n        num_grad = (loss_plus - loss_minus) / (2 * epsilon)\n        \n        # Append the numerical gradient to the list\n        grads.append(num_grad)\n    \n    # Return the transposed numerical gradient\n    return np.array(grads).T.astype(np.float64)\n```\n\n### Explanation:\n- **Parameter Handling**: We handle the special cases for \"Ba\" and \"Bx\" by converting them to lowercase. For \"X\" and \"y\", we immediately return `None` since they are not trainable parameters.\n- **Perturbation**: We create two copies of the parameter values: one perturbed by `+epsilon` and another by `-epsilon`. These perturbed values are used to compute the loss.\n- **Gradient Calculation**: Using the centered finite difference formula, we calculate the numerical gradient for each element of the parameter.\n- **Storage and Return**: The numerical gradients are collected in a list and returned as a transposed NumPy array.\n\nThis function efficiently computes the numerical gradient for any given RNN parameter using the specified finite difference approach."}
{"task_id": 257, "completion_id": 0, "completion": "To implement the AdaBoost algorithm using decision stumps, we need to follow these steps:\n\n1. Initialize weights for each training example.\n2. Iterate over `n_clf` times, building a decision stump for each iteration.\n3. For each stump, calculate its weighted error.\n4. Compute the weight for the stump based on the error.\n5. Update the weights of the training examples based on the stump's performance.\n6. Aggregate the votes from all stumps to make predictions on new data.\n\nHere's the implementation of the `adaboost_predict` function:\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n    \n    # Initialize weights\n    m = len(y_train)\n    w = np.ones(m) / m\n    \n    # List to store predictions\n    predictions = []\n    \n    # Build n_clf decision stumps\n    for _ in range(n_clf):\n        # Find the best decision stump\n        best_stump = None\n        min_error = float('inf')\n        \n        for feature_idx in range(X_train.shape[1]):\n            thresholds = np.unique(X_train[:, feature_idx])\n            \n            for threshold in thresholds:\n                # Split the data into two parts\n                left_indices = X_train[:, feature_idx] <= threshold\n                right_indices = X_train[:, feature_idx] > threshold\n                \n                # Calculate predictions for left and right subsets\n                y_left = y_train[left_indices]\n                y_right = y_train[right_indices]\n                \n                # Assign labels based on the threshold\n                pred_left = np.where(y_left == 1, 1, -1)\n                pred_right = np.where(y_right == 1, 1, -1)\n                \n                # Calculate weighted errors\n                error_left = np.sum(w[left_indices] * (pred_left != y_left))\n                error_right = np.sum(w[right_indices] * (pred_right != y_right))\n                \n                # Total error\n                total_error = error_left + error_right\n                \n                # Check if this is the best stump so far\n                if total_error < min_error:\n                    min_error = total_error\n                    best_stump = {\n                        'feature': feature_idx,\n                        'threshold': threshold,\n                        'prediction_left': pred_left,\n                        'prediction_right': pred_right\n                    }\n        \n        # If no better stump found, skip this iteration\n        if best_stump is None:\n            break\n        \n        # Calculate alpha for the current stump\n        epsilon_t = min_error / np.sum(w)\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        \n        # Update weights\n        w *= np.exp(-alpha_t * y_train * best_stump['prediction'])\n        w /= np.sum(w)\n        \n        # Make predictions with the current stump\n        y_pred = np.where(best_stump['prediction_left'] == 1, 1, -1)\n        predictions.append(np.sign(np.sum(alpha_t * y_pred)))\n    \n    return predictions\n```\n\n### Explanation:\n\n1. **Initialization**: We initialize the weights `w` to be equal for all samples initially.\n\n2. **Decision Stump Construction**: In each iteration, we find the best decision stump by trying different thresholds for each feature. The best stump is chosen based on the minimum weighted error.\n\n3. **Weight Update**: After finding the best stump, we update the weights of the training examples. Samples that were misclassified by the stump have their weights increased, while those correctly classified have their weights decreased.\n\n4. **Prediction Aggregation**: For each test sample, we compute the weighted sum of the predictions from all stumps and use the sign of this sum to determine the final prediction.\n\nThis implementation efficiently builds a boosted classifier using decision stumps and returns the predicted labels for the test set."}
{"task_id": 258, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n\n    # Check if the arrays have the same length\n    if actual_array.shape != predicted_array.shape:\n        return -1\n\n    # Compute the per-sample hinge losses\n    hinge_losses = np.maximum(0, 1 - actual_array * predicted_array)\n\n    # Compute the mean hinge loss\n    mean_hinge_loss = np.mean(hinge_losses)\n\n    # Round to four decimal places\n    return round(mean_hinge_loss, 4)\n```\n\nThis function first checks if the input arrays have the same shape. If they do not, it returns -1. Otherwise, it calculates the per-sample hinge losses using vectorized operations and then computes the mean of these losses. Finally, it rounds the mean loss to four decimal places and returns it."}
{"task_id": 259, "completion_id": 0, "completion": "```python\nimport random\n\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Randomly samples *batch_size* transitions from the replay buffer.\n\n    The selection must be deterministic (use random.seed(42)).  If\n    *batch_size* exceeds the buffer length, return -1.\n    \"\"\"\n    # Seed the random number generator\n    random.seed(42)\n    \n    # Check if the batch size is greater than the memory length\n    if batch_size > len(memory):\n        return -1\n    \n    # Use random.sample to select batch_size unique transitions\n    sampled_transitions = random.sample(memory, batch_size)\n    \n    return sampled_transitions\n```"}
{"task_id": 260, "completion_id": 0, "completion": "To implement the `tileswrap` function according to the specifications provided, we need to follow these steps:\n\n1. **Quantize the Floats**: Convert each floating-point number to an integer by multiplying it by the number of tilings and taking the floor value.\n\n2. **Generate Coordinates**: For each tiling, generate coordinates based on the quantized values and the wrap widths. If a wrap width is specified, use modulo arithmetic to handle wrapping; otherwise, keep the value unchanged.\n\n3. **Hash the Coordinates**: Combine the coordinates into a single integer using a deterministic polynomial hash function.\n\n4. **Return the Indices**: Map the hashed value to the appropriate index within the hash table size.\n\nHere's the complete implementation of the `tileswrap` function:\n\n```python\nfrom itertools import zip_longest\nfrom math import floor\n\ndef tileswrap(ihtORsize: int,\n              numtilings: int,\n              floats: list[float],\n              wrapwidths: list[int | None],\n              ints: list[int] | None = None,\n              readonly: bool = False) -> list[int]:\n    \"\"\"Return tile indices for the given floats/ints using wrapping tile coding.\n\n    The implementation must follow Sutton's original algorithm with the\n    modifications described in the task description (deterministic hash and\n    integer table size).\n\n    Args:\n        ihtORsize (int): Size of the hash table (positive integer).\n        numtilings (int): Number of tilings.\n        floats (list[float]): Continuous variables.\n        wrapwidths (list[int | None]): Optional wrap width for every float.  If\n            `None` or 0 no wrapping is applied.\n        ints (list[int] | None): Additional integer variables to include in the\n            coordinate list.\n        readonly (bool): Ignored \u2013 present for API compatibility.\n\n    Returns:\n        list[int]: Indices of the active tiles, one per tiling.\n    \"\"\"\n    # Step 1: Quantize the floats\n    q_values = [floor(float_val * numtilings) for float_val in floats]\n\n    # Step 2: Generate coordinates\n    coordinates = []\n    for tiling in range(numtilings):\n        coords = [tiling]\n        b = tiling\n        for q_val, wrap_width in zip(q_values, wrapwidths):\n            if wrap_width is not None and wrap_width > 0:\n                c = (q_val + (b % numtilings)) // numtilings\n                coords.append(c % wrap_width)\n            else:\n                coords.append(c)\n            b += tiling * 2\n        if ints is not None:\n            coords.extend(ints)\n        coordinates.append(coords)\n\n    # Step 3: Hash the coordinates\n    def deterministic_hash(coords):\n        index = 0\n        for coord in coords:\n            index = (index * 31 + coord) % ihtORsize\n        return index\n\n    # Step 4: Return the indices\n    return [deterministic_hash(coord_list) for coord_list in coordinates]\n\n# Example usage:\nihtORsize = 32\nnumtilings = 2\nfloats = [0.1]\nwrapwidths = [5]\nints = []\n\nindices = tileswrap(ihtORsize, numtilings, floats, wrapwidths, ints)\nprint(indices)  # Output: [0, 31]\n```\n\n### Explanation:\n\n- **Quantization**: Each float is multiplied by the number of tilings and floored to get the quantized value.\n- **Coordinate Generation**: For each tiling, we initialize the coordinate list with the tiling number. We then iterate over the quantized values and their corresponding wrap widths, applying the necessary transformations to generate the coordinates.\n- **Hashing**: The coordinates are combined into a single integer using a deterministic polynomial hash function.\n- **Return**: The function returns a list of indices, one for each tiling.\n\nThis implementation ensures that the indices are correctly computed and stored in the desired format."}
{"task_id": 261, "completion_id": 0, "completion": "To implement the Glorot Xavier Normal weight initialization, we need to follow these steps:\n\n1. Compute the fan-in and fan-out values based on the input shape.\n2. Calculate the standard deviation `s` using the formula \\( s = \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}} \\).\n3. Generate a NumPy array with the specified shape and fill it with samples drawn from a normal distribution with mean 0 and standard deviation `s`.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least two dimensions.\")\n\n    # Calculate fan-in and fan-out\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    else:\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n\n    # Calculate standard deviation\n    std_dev = np.sqrt(2 / (fan_in + fan_out))\n\n    # Generate random samples from a normal distribution\n    rng = np.random.default_rng()\n    weights = rng.normal(loc=0, scale=std_dev, size=shape)\n\n    return weights\n\n# Example usage:\nshape = (4, 5)\nweights = glorot_normal(shape)\nprint(weights.shape)\nprint(np.mean(weights))\nprint(np.std(weights))\n```\n\n### Explanation:\n\n- **Fan-In and Fan-Out Calculation**: \n  - If the shape has exactly two dimensions, `fan_in` and `fan_out` are simply the two dimensions.\n  - If the shape has more than two dimensions, the receptive field size is calculated as the product of all dimensions after the first two, and then `fan_in` and `fan_out` are computed accordingly.\n\n- **Standard Deviation Calculation**:\n  - The standard deviation `s` is calculated using the formula \\( s = \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}} \\).\n\n- **Random Sampling**:\n  - We use `np.random.default_rng()` to create a random number generator, which allows us to control the randomness without affecting the global state.\n  - We generate random samples from a normal distribution with mean 0 and standard deviation `s`, and reshape them to match the desired output shape.\n\nThis implementation ensures that the weights are initialized according to the Glorot Xavier normal distribution, which helps in stabilizing the learning process during network training."}
{"task_id": 263, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feedforward_predict(x, weights, activations):\n    \"\"\"Runs a forward pass through a fully-connected neural network.\n\n    Parameters\n    ----------\n    x : list[float] | numpy.ndarray\n        Input feature vector (bias term **not** included).\n    weights : list[list[list[float]]]\n        List of weight matrices; each inner list represents one 2-D weight\n        matrix where the first column contains the bias weights.\n    activations : list[str]\n        Activation functions per layer.  Allowed values: \"sigmoid\", \"relu\",\n        \"linear\".\n\n    Returns\n    -------\n    float | list[float]\n        Network output rounded to 4 decimal places.  A scalar is returned for\n        a single-neuron output layer, otherwise a list is returned.\n    \"\"\"\n    # Convert x to a numpy array if it's not already\n    x = np.array(x)\n    \n    # Initialize the activations of the input layer\n    activations[0] = np.hstack((np.ones(1), x))\n    \n    # Perform the forward pass through each layer\n    for l in range(1, len(weights)):\n        # Compute the pre-activation\n        z = np.dot(weights[l], activations[l - 1])\n        \n        # Apply the activation function\n        if activations[l] == 'sigmoid':\n            a = 1 / (1 + np.exp(-z))\n        elif activations[l] == 'relu':\n            a = np.maximum(0, z)\n        else:  # 'linear'\n            a = z\n        \n        # Prepend the bias term for the next layer\n        activations[l] = np.hstack((np.ones(1), a))\n    \n    # Return the output of the last layer, rounded to 4 decimal places\n    if len(weights[-1]) == 1:\n        return round(activations[-1][1], 4)\n    else:\n        return [round(a, 4) for a in activations[-1][1:]]\n\n# Example usage:\nx = [1, 2, 3]\nweights = [\n    [[0.5, 0.5, 0.5],\n     [0.5, 0.5, 0.5]],\n    [[0.5, 0.5],\n     [0.5, 0.5]]\n]\nactivations = ['sigmoid', 'relu']\nprint(feedforward_predict(x, weights, activations))  # Output will depend on the weights and activations provided\n```"}
{"task_id": 265, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `king_loss_fn` function according to the specified requirements:\n\n```python\ndef king_loss_fn(x: int | float) -> float:\n    \"\"\"Compute King\u2019s piece-wise loss for a scalar value.\n\n    The loss function L(x) is defined by four segments:\n      \u2022 x \u2264 250:   L(x) = \u20130.25 * x + 82.50372665317208\n      \u2022 250 < x \u2264 600: L(x) = 20.00372665317208\n      \u2022 600 < x \u2264 700: L(x) = \u20130.2 * x + 140.00372665317207\n      \u2022 x > 700:  L(x) = 0.003726653172066108\n\n    The result must be rounded to 4 decimal places.\n\n    Args:\n        x: An integer or float input.\n\n    Returns:\n        A float rounded to four decimal places.\n    \"\"\"\n    if x <= 250:\n        return round(-0.25 * x + 82.50372665317208, 4)\n    elif 250 < x <= 600:\n        return round(20.00372665317208, 4)\n    elif 600 < x <= 700:\n        return round(-0.2 * x + 140.00372665317207, 4)\n    else:\n        return round(0.003726653172066108, 4)\n\n# Example usage:\nprint(king_loss_fn(250))  # Output: 82.5037\nprint(king_loss_fn(300))  # Output: 20.0037\nprint(king_loss_fn(600))  # Output: 20.0037\nprint(king_loss_fn(650))  # Output: 110.0037\nprint(king_loss_fn(700))  # Output: 140.0037\nprint(king_loss_fn(750))  # Output: 119.9999\n```\n\nThis function checks the value of `x` against the specified ranges and applies the corresponding formula. It then rounds the result to four decimal places before returning it."}
{"task_id": 266, "completion_id": 0, "completion": "To solve the problem of converting an undirected graph from its vertex list `V` and edge list `E` into an adjacency list representation, we need to ensure that each vertex's neighbors are listed exactly once, without duplicates, and in the order they appear in the original vertex list `V`. Here's how you can implement this:\n\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Create a dictionary to store the adjacency list\n    adj_list = {vertex: [] for vertex in V}\n    \n    # Iterate over each edge and add both directions to the adjacency list\n    for u, v in E:\n        # Add v to u's adjacency list\n        if v not in adj_list[u]:\n            adj_list[u].append(v)\n        \n        # Add u to v's adjacency list\n        if u not in adj_list[v]:\n            adj_list[v].append(u)\n    \n    # Sort the neighbors in each adjacency list\n    for key in adj_list:\n        adj_list[key].sort()\n    \n    # Convert the dictionary back to a list of lists\n    G = [adj_list[vertex] for vertex in V]\n    \n    return G\n\n# Example usage:\nV = ['A', 'B', 'C', 'D']\nE = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'D')]\nG = build_adj_list(V, E)\nprint(G)  # Output: [['B', 'C'], ['A', 'C'], ['A', 'B', 'D'], ['B', 'C', 'D']]\n```\n\n### Explanation:\n1. **Initialization**: We create a dictionary `adj_list` where each key is a vertex from `V` and the value is an empty list to store its neighbors.\n\n2. **Edge Processing**: For each edge `(u, v)` in `E`, we add `v` to `u`'s adjacency list and vice versa. This ensures that both directions of the edge are represented.\n\n3. **Sorting**: After populating the adjacency list, we sort each list of neighbors to maintain the order of appearance in `V`.\n\n4. **Conversion**: Finally, we convert the dictionary back into a list of lists using a list comprehension, preserving the order of vertices from `V`.\n\nThis approach efficiently constructs the adjacency list while ensuring that all conditions are met."}
{"task_id": 267, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that finds the optimal weighted decision stump for a given dataset. The function will iterate through all possible features and thresholds, calculate the weighted classification error for each stump, and keep track of the stump with the lowest error.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    best_stump = None\n    min_weighted_error = float('inf')\n    \n    n_samples, n_features = X.shape\n    \n    for i in range(n_features):\n        unique_values = np.unique(X[:, i])\n        \n        for j in range(len(unique_values)):\n            threshold = unique_values[j]\n            \n            # Calculate predictions for the current stump\n            if y[i] == 1:\n                predictions = np.where(X[:, i] < threshold, 1, -1)\n            else:\n                predictions = np.where(X[:, i] >= threshold, 1, -1)\n            \n            # Calculate weighted error\n            weighted_error = np.sum(sample_weights * (predictions != y))\n            \n            # Update best stump if this one has a lower error\n            if weighted_error < min_weighted_error:\n                min_weighted_error = weighted_error\n                best_stump = {\n                    \"feature_index\": i,\n                    \"threshold\": round(threshold, 4),\n                    \"polarity\": 1 if y[i] == 1 else -1,\n                    \"weighted_error\": round(weighted_error, 4)\n                }\n    \n    return best_stump\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 1, -1, -1])\n# sample_weights = np.array([0.2, 0.2, 0.3, 0.3])\n# result = train_decision_stump(X, y, sample_weights)\n# print(result)\n```\n\n### Explanation:\n\n1. **Iterate Over Features**: We loop through each feature index `i`.\n\n2. **Extract Unique Values**: For each feature, we extract the unique values from the column to determine potential thresholds.\n\n3. **Calculate Predictions**: For each threshold, we predict the class based on whether the feature value is less than or greater than the threshold.\n\n4. **Compute Weighted Error**: We compute the weighted classification error using the formula provided in the task description.\n\n5. **Track Best Stump**: If the current stump has a lower weighted error than the previously recorded minimum, we update the best stump.\n\n6. **Return Result**: Finally, we return the best stump found.\n\nThis implementation efficiently explores all possible stumps and selects the one with the lowest weighted error."}
{"task_id": 268, "completion_id": 0, "completion": "To implement the Erd\u0151s\u2013R\u00e9nyi \\( G(n, p) \\) model in Python, we need to generate a random graph with \\( n \\) vertices and each possible edge having a probability \\( p \\). Here's how you can achieve this:\n\n1. **Initialize the Adjacency Matrix**: Start with a square matrix of zeros of size \\( n \\times n \\), which will represent the adjacency matrix of the graph.\n\n2. **Generate Edge Pairs**: Use `itertools.combinations` to generate all possible edges between the vertices. For an undirected graph, ensure that each edge is represented twice in the matrix (once in each direction).\n\n3. **Randomly Add Edges**: For each edge pair, use `numpy.random.rand()` to determine whether to add the edge based on the specified probability \\( p \\).\n\n4. **Handle Self-Loops**: Ensure that no self-loops are added by checking if the first element of each edge pair is equal to the second element.\n\nHere's the implementation:\n\n```python\nimport numpy as np\nfrom itertools import combinations\n\ndef random_unweighted_graph(n_vertices: int, edge_prob: float = 0.5, directed: bool = False) -> list[list[int]]:\n    \"\"\"Generate an un-weighted Erd\u0151s\u2013R\u00e9nyi random graph.\n\n    The function must return the adjacency matrix (a nested list) of the graph.\n\n    Args:\n        n_vertices (int): Number of vertices in the graph (labelled 0..n-1).\n        edge_prob  (float): Probability *p* that an admissible edge exists.\n        directed   (bool):  Whether the graph is directed (default ``False``).\n\n    Returns:\n        list[list[int]]: The adjacency matrix *A* where *A[i][j] == 1* iff an\n        edge from vertex *i* to vertex *j* exists.  The matrix must consist of\n        integers (0/1) only and have shape *(n_vertices, n_vertices)*.\n    \"\"\"\n    # Initialize the adjacency matrix with zeros\n    adjacency_matrix = [[0] * n_vertices for _ in range(n_vertices)]\n    \n    # Generate all possible edges\n    if not directed:\n        # Undirected graph: consider each pair (i, j) where i < j\n        edge_pairs = combinations(range(n_vertices), 2)\n    else:\n        # Directed graph: consider each pair (i, j)\n        edge_pairs = combinations(range(n_vertices), 2)\n    \n    # Add edges based on the edge probability\n    for u, v in edge_pairs:\n        if np.random.rand() <= edge_prob:\n            adjacency_matrix[u][v] = 1\n            if not directed:\n                adjacency_matrix[v][u] = 1\n    \n    return adjacency_matrix\n\n# Example usage:\nn_vertices = 5\nedge_prob = 0.7\ngraph = random_unweighted_graph(n_vertices, edge_prob)\nprint(graph)\n```\n\n### Explanation:\n- **Initialization**: We create a zero-initialized adjacency matrix of size \\( n \\times n \\).\n- **Edge Generation**: We use `combinations` to generate pairs of vertices. For an undirected graph, we only consider pairs where the first vertex is less than the second to avoid duplicate entries.\n- **Edge Addition**: For each pair, we check if a random number is less than or equal to the edge probability. If it is, we set the corresponding entries in the adjacency matrix to 1.\n- **Directed vs. Undirected**: The `directed` parameter determines whether to include self-loops and handle edges in both directions accordingly.\n\nThis function will generate a random Erd\u0151s\u2013R\u00e9nyi graph according to the specified parameters."}
{"task_id": 269, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\n\n    Parameters\n    ----------\n    state_dimensions : int\n        Length of the state (observation) vector.\n    action_dimensions : int\n        Number of possible discrete actions.\n\n    Returns\n    -------\n    int\n        Total count of trainable parameters (weights + biases).\n    \"\"\"\n    # The first fully-connected layer has 'state_dimensions' inputs and 164 outputs\n    # The second fully-connected layer has 164 inputs and 'action_dimensions' outputs\n    \n    # Calculate the number of weights for each layer\n    weights_first_layer = state_dimensions * 164\n    weights_second_layer = 164 * action_dimensions\n    \n    # Each layer also has one bias for every output neuron\n    biases_first_layer = 164\n    biases_second_layer = action_dimensions\n    \n    # Total trainable parameters are the sum of weights and biases for both layers\n    total_parameters = weights_first_layer + biases_first_layer + weights_second_layer + biases_second_layer\n    \n    return total_parameters\n```"}
{"task_id": 270, "completion_id": 0, "completion": "To solve the problem of determining the optimal expected reward for a contextual linear bandit, we need to follow these steps:\n\n1. Compute the expected rewards for each arm.\n2. Find the maximum expected reward.\n3. Round the maximum expected reward to four decimal places.\n4. Identify the index of the arm(s) that achieve this maximum reward.\n\nHere's the implementation of the `oracle_payoff` function in Python:\n\n```python\nimport numpy as np\n\ndef oracle_payoff(context: np.ndarray, thetas: np.ndarray) -> tuple[float, int]:\n    \"\"\"Determine the best arm for a contextual linear bandit.\n\n    Each column *k* of *context* (shape D\u00d7K) is a context vector x_k \u2208 \u211d^D.\n    Each column *k* of *thetas*  (shape D\u00d7K) is the corresponding parameter\n    vector \u03b8_k \u2208 \u211d^D.\n\n    The expected reward of arm *k* is x_k^T \u03b8_k.\n\n    The function must return the maximal expected reward rounded to four\n    decimals and the index (0-based) of the arm that achieves it.  If more\n    than one arm attains the maximal reward (after rounding), return the\n    smallest index.\n\n    Args:\n        context: numpy.ndarray of shape (D, K)\n        thetas : numpy.ndarray of shape (D, K)\n\n    Returns:\n        Tuple containing (best_reward, best_arm).\n    \"\"\"\n    # Calculate the expected rewards for each arm\n    expected_rewards = np.dot(context, thetas.T)\n    \n    # Find the maximum expected reward\n    max_reward = np.max(expected_rewards)\n    \n    # Round the maximum expected reward to four decimal places\n    rounded_max_reward = round(max_reward, 4)\n    \n    # Find the indices of the arms that have the maximum expected reward\n    best_arms = np.where(np.isclose(expected_rewards, rounded_max_reward))[0]\n    \n    # Return the smallest index among the best arms\n    best_arm_index = best_arms[0]\n    \n    return rounded_max_reward, best_arm_index\n\n# Example usage\ncontext = np.array([[0.5, 1.0],\n                    [1.5, 0.2]])\nthetas = np.array([[0.1, 0.3],\n                   [0.4, 0.2]])\n\nresult = oracle_payoff(context, thetas)\nprint(result)  # Output: (0.65, 0)\n```\n\n### Explanation:\n\n1. **Expected Rewards Calculation**: We use NumPy's dot product to compute the expected rewards for each arm. The expression `np.dot(context, thetas.T)` computes the matrix-vector multiplication, resulting in an array where each element represents the expected reward for each arm.\n\n2. **Finding Maximum Reward**: We use `np.max()` to find the maximum value in the `expected_rewards` array.\n\n3. **Rounding**: We round the maximum expected reward to four decimal places using `round()`. This ensures that the result is consistent with the output format specified.\n\n4. **Identifying Best Arm(s)**: We use `np.where(np.isclose(expected_rewards, rounded_max_reward))` to find all indices where the expected reward matches the rounded maximum reward. This handles cases where there might be multiple arms with the same maximum reward.\n\n5. **Returning Result**: We return the rounded maximum reward and the smallest index from the list of best arms. This ensures that if there are ties, the smallest index is returned."}
{"task_id": 271, "completion_id": 0, "completion": "To compute the XGBoost split gain for the squared-error loss, we need to follow these steps:\n\n1. Calculate the gradients \\( g_i \\) for both the left and right children using the formula \\( g_i = y_{\\text{pred},i} - y_{\\text{true},i} \\).\n2. Sum the gradients to get \\( G_L \\) for the left child and \\( G_R \\) for the right child.\n3. Calculate the hessians \\( h_i \\) for both the left and right children, which are all 1 since the hessian for squared error is constant at 1.\n4. Sum the hessians to get \\( H_L \\) for the left child and \\( H_R \\) for the right child.\n5. Compute the score for each child node using the formula \\( \\text{score} = \\frac{G^2}{H} \\).\n6. Compute the total score for the parent node using the formula \\( \\text{score}_P = \\frac{G_P^2}{H_P} \\), where \\( G_P = G_L + G_R \\) and \\( H_P = H_L + H_R \\).\n7. Calculate the gain as the difference between the parent's score and the sum of the scores of the left and right children.\n8. Return the gain rounded to 4 decimal places.\n\nHere's the implementation of the `xgboost_split_gain` function:\n\n```python\nimport numpy as np\n\ndef xgboost_split_gain(\n    left_true: list[float],\n    left_pred: list[float],\n    right_true: list[float],\n    right_pred: list[float],\n) -> float:\n    \"\"\"Compute XGBoost split gain for squared-error loss.\n\n    Parameters\n    ----------\n    left_true : list[float]\n        True target values of the left child after the split.\n    left_pred : list[float]\n        Current model predictions for those left-child samples.\n    right_true : list[float]\n        True target values of the right child after the split.\n    right_pred : list[float]\n        Current model predictions for those right-child samples.\n\n    Returns\n    -------\n    float\n        The split gain rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    left_true = np.array(left_true)\n    left_pred = np.array(left_pred)\n    right_true = np.array(right_true)\n    right_pred = np.array(right_pred)\n\n    # Calculate gradients for left and right children\n    g_left = left_pred - left_true\n    g_right = right_pred - right_true\n\n    # Calculate hessians (all 1 for squared error)\n    h_left = np.ones_like(g_left)\n    h_right = np.ones_like(g_right)\n\n    # Sum gradients and hessians\n    G_L = np.sum(g_left)\n    G_R = np.sum(g_right)\n    H_L = np.sum(h_left)\n    H_R = np.sum(h_right)\n\n    # Calculate scores for left and right children\n    score_L = (G_L ** 2) / H_L\n    score_R = (G_R ** 2) / H_R\n\n    # Calculate score for the parent node\n    G_P = G_L + G_R\n    H_P = H_L + H_R\n    score_P = (G_P ** 2) / H_P\n\n    # Calculate gain\n    gain = 0.5 * (score_L + score_R - score_P)\n\n    # Round the gain to 4 decimal places\n    return round(gain, 4)\n\n# Example usage:\n# left_true = [1, 2, 3]\n# left_pred = [1.1, 2.2, 3.3]\n# right_true = [0, 1, 2]\n# right_pred = [0.9, 1.1, 2.1]\n# print(xgboost_split_gain(left_true, left_pred, right_true, right_pred))\n```\n\nThis function will correctly compute the XGBoost split gain for the squared-error loss based on the given inputs."}
{"task_id": 273, "completion_id": 0, "completion": "To solve this problem, we need to implement the `best_split` function that calculates the information gain for each possible threshold and selects the one with the maximum gain. Here's how you can do it:\n\n```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Sort the feature values\n    sorted_feature = sorted(set(feature))\n    \n    # Check if there are any duplicate feature values\n    if len(sorted_feature) == len(feature):\n        return (None, 0.0)\n    \n    # Initialize variables to store the best threshold and information gain\n    best_threshold = None\n    max_info_gain = -math.inf\n    \n    # Iterate over all possible thresholds\n    for i in range(len(sorted_feature) - 1):\n        left_indices = [idx for idx, x in enumerate(feature) if x < sorted_feature[i + 1]]\n        right_indices = [idx for idx, x in enumerate(feature) if x >= sorted_feature[i + 1]]\n        \n        # Calculate the number of samples in left and right subsets\n        n = len(target)\n        left_count = len(left_indices)\n        right_count = len(right_indices)\n        \n        # If both subsets are empty, skip this threshold\n        if left_count == 0 or right_count == 0:\n            continue\n        \n        # Count the class labels in the left and right subsets\n        left_labels = [target[idx] for idx in left_indices]\n        right_labels = [target[idx] for idx in right_indices]\n        \n        # Calculate the entropy of the parent set\n        total_counts = Counter(target)\n        parent_entropy = -sum((count / n) * math.log2(count / n) for count in total_counts.values())\n        \n        # Calculate the entropies of the left and right subsets\n        left_entropy = -sum((count / left_count) * math.log2(count / left_count) for count in Counter(left_labels).values())\n        right_entropy = -sum((count / right_count) * math.log2(count / right_count) for count in Counter(right_labels).values())\n        \n        # Calculate the information gain\n        info_gain = parent_entropy - (left_count / n) * left_entropy - (right_count / n) * right_entropy\n        \n        # Update the best threshold if this one has a higher information gain\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_threshold = sorted_feature[i + 1]\n    \n    # Return the best threshold and information gain rounded to 4 decimal places\n    return (round(best_threshold, 4), round(max_info_gain, 4))\n\n# Example usage:\n# feature = [1.5, 2.3, 3.7, 3.7, 4.9, 5.2]\n# target = [0, 0, 1, 1, 1, 0]\n# print(best_split(feature, target))  # Output will depend on the input\n```\n\n### Explanation:\n1. **Sorting and Handling Duplicates**: We first sort the unique feature values and check if there are any duplicates. If there are, we return `(None, 0.0)` because a valid split cannot be made.\n\n2. **Iterating Over Thresholds**: We iterate over each pair of consecutive sorted feature values to consider as potential split points.\n\n3. **Splitting Data**: For each threshold, we create two lists of indices representing the left and right subsets of the data.\n\n4. **Calculating Entropy**: We calculate the Shannon entropy for the parent set, the left subset, and the right subset using `Counter` to count occurrences of each class label.\n\n5. **Computing Information Gain**: The information gain is calculated using the formula provided.\n\n6. **Updating Best Threshold**: If the current threshold yields a higher information gain than the previously recorded best, we update our best threshold and information gain.\n\n7. **Returning Result**: Finally, we return the best threshold and information gain rounded to four decimal places."}
{"task_id": 274, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef small_norm_clip(values):\n    \"\"\"Clip each element of *values* to the closed interval [-5, 5].\n\n    Args:\n        values (list[int | float] | np.ndarray): One-dimensional numeric data.\n\n    Returns:\n        list[float]: A Python list where every element has been limited to the\n            range [-5, 5] while preserving the original order.\n    \"\"\"\n    if isinstance(values, np.ndarray):\n        # Convert numpy array to list for easier manipulation\n        values = values.tolist()\n    \n    # Use list comprehension to clip each element\n    clipped_values = [max(-5, min(5, val)) for val in values]\n    \n    return clipped_values\n\n# Example usage:\nprint(small_norm_clip([10, -3, 7, -8, 2]))  # Output: [-5, -3, 5, -5, 2]\nprint(small_norm_clip(np.array([10, -3, 7, -8, 2])))  # Output: [-5, -3, 5, -5, 2]\n```"}
{"task_id": 276, "completion_id": 0, "completion": "To implement the additive-N-gram language model as specified, we need to follow these steps:\n\n1. Tokenize each sentence, remove punctuation, and pad it with `<bol>` and `<eol>` tokens.\n2. Build all \\( N \\)-gram counts from the training corpus.\n3. Compute the additive-smoothed probability of each \\( N \\)-gram that appears in the padded target sentence.\n4. Return the base-10 logarithm of the total sentence probability.\n\nHere's the complete implementation:\n\n```python\nimport math\nimport string\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sentence: str, N: int, k: float = 1.0, unk: bool = True) -> float:\n    \"\"\"Return the base-10 log probability of *sentence* under an additive-smoothed N-gram model.\n\n    The model is trained on *corpus* using the given *N* and *k* (smoothing\n    parameter).  All details of tokenisation, padding, vocabulary construction\n    and probability estimation are described in the task specification.\n\n    Args:\n        corpus: List of training sentences.\n        sentence: The sentence whose probability needs to be computed.\n        N: Order of the model (1 for unigram, 2 for bigram, etc.).\n        k: Additive smoothing parameter (default 1.0).\n        unk: If True, unseen tokens are mapped to a special <unk> symbol.\n\n    Returns:\n        A single float, rounded to 4 decimal places, representing the base-10\n        logarithm of the sentence probability.\n    \"\"\"\n    # Helper function to tokenize and clean text\n    def tokenize_and_clean(text):\n        # Remove punctuation and convert to lowercase\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        text = text.lower()\n        # Split into words\n        return text.split()\n\n    # Pad the sentence with <bol> and <eol>\n    padded_sentence = ['<bol>'] + tokenize_and_clean(sentence) + ['<eol>']\n\n    # Initialize the vocabulary counter\n    vocab_counter = Counter()\n\n    # Count all N-grams in the corpus\n    for doc in corpus:\n        tokens = tokenize_and_clean(doc)\n        for i in range(len(tokens) - N + 1):\n            ngram = tuple(tokens[i:i+N])\n            vocab_counter[ngram] += 1\n\n    # Calculate total number of tokens and unique tokens\n    total_tokens = sum(vocab_counter.values())\n    num_unique_tokens = len(vocab_counter)\n\n    # Function to get smoothed probability of an N-gram\n    def smooth_probability(ngram):\n        if unk and ngram not in vocab_counter:\n            return k / (num_unique_tokens + k)\n        \n        count_h_w = vocab_counter.get(ngram, 0)\n        count_h = vocab_counter[ngram[:-1]] if N > 1 else total_tokens\n        \n        numerator = count_h_w + k\n        denominator = count_h + k * num_unique_tokens\n        return numerator / denominator\n\n    # Calculate the log probability of the padded sentence\n    log_prob = 0.0\n    for i in range(len(padded_sentence) - N + 1):\n        ngram = tuple(padded_sentence[i:i+N])\n        prob = smooth_probability(ngram)\n        log_prob += math.log10(prob)\n\n    # Round the result to 4 decimal places\n    return round(log_prob, 4)\n\n# Example usage:\ncorpus = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A quick brown fox\"\n]\nsentence = \"The quick brown fox\"\nprint(additive_ngram_log_prob(corpus, sentence, N=2))  # Output will vary based on the corpus\n```\n\n### Explanation:\n\n- **Tokenization and Cleaning**: We define a helper function `tokenize_and_clean` to process each sentence, removing punctuation and converting it to lowercase.\n- **Padding**: Each sentence is padded with `<bol>` and `<eol>` tokens before processing.\n- **Vocabulary Counting**: We use a `Counter` to count all \\( N \\)-grams in the training corpus.\n- **Smoothing Probability**: The `smooth_probability` function calculates the smoothed probability of an \\( N \\)-gram using the formula provided.\n- **Log Probability Calculation**: We iterate over the padded sentence and calculate the log probability of each \\( N \\)-gram, summing these values.\n- **Rounding**: Finally, we round the result to four decimal places before returning it.\n\nThis implementation should correctly compute the base-10 logarithm of the sentence probability under the specified additive smoothing model."}
{"task_id": 277, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `is_tuple` function according to the specified requirements:\n\n```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    # Check if 'action_space' is a tuple or dictionary\n    tuple_action = isinstance(env[\"action_space\"], (tuple, dict))\n    \n    # Check if 'observation_space' is a tuple or dictionary\n    tuple_obs = isinstance(env[\"observation_space\"], (tuple, dict))\n    \n    # Return the results as a tuple\n    return (tuple_action, tuple_obs)\n```\n\nThis function uses the `isinstance` function to check if the `action_space` and `observation_space` values in the input dictionary are instances of either a `tuple` or a `dict`. It then returns these boolean values in a tuple as required."}
{"task_id": 278, "completion_id": 0, "completion": "To implement the `evaluate_decision_tree` function, we need to recursively traverse the decision tree based on the conditions specified. Here's how you can do it:\n\n1. Start from the root node of the tree.\n2. Compare the feature value at the current node with the threshold.\n3. Depending on whether the comparison is true or false, move to either the `true_branch` or `false_branch`.\n4. Repeat this process until a leaf node is reached.\n5. Return the value stored in the leaf node.\n\nHere's the implementation:\n\n```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    # Start from the root node\n    current_node = tree\n    \n    while True:\n        # Extract the feature index and threshold from the current node\n        feature_index = current_node['feature_i']\n        threshold = current_node['threshold']\n        \n        # Get the feature value from the sample\n        feature_value = sample[feature_index]\n        \n        # Determine which branch to take based on the comparison\n        if feature_value <= threshold:\n            next_node = current_node['true_branch']\n        else:\n            next_node = current_node['false_branch']\n        \n        # If the next node is a leaf, return its value\n        if isinstance(next_node, dict) and 'value' in next_node:\n            return next_node['value']\n        \n        # Move to the next node\n        current_node = next_node\n```\n\n### Explanation:\n- We initialize `current_node` with the root of the tree.\n- We use a `while True` loop to continue traversing the tree until we reach a leaf node.\n- Inside the loop, we extract the feature index and threshold from the current node.\n- We get the corresponding feature value from the input sample.\n- We determine which branch to follow based on whether the feature value is less than or equal to the threshold.\n- If the next node is a dictionary and contains a `'value'` key, it means we have reached a leaf node, so we return its value.\n- Otherwise, we update `current_node` to point to the next node and repeat the process.\n\nThis function will correctly traverse the decision tree for a given sample and return the predicted value."}
{"task_id": 279, "completion_id": 0, "completion": "Here's the implementation of the `mse_loss` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error (MSE) loss, its gradient and accuracy.\n\n    Parameters\n    ----------\n    y_true : list[int] | numpy.ndarray\n        Ground truth labels (0 or 1).\n    y_pred : list[float] | numpy.ndarray\n        Model predictions (any real numbers).\n\n    Returns\n    -------\n    tuple\n        A 3-tuple containing:\n          * The MSE value (float)\n          * The gradient list (list[float])\n          * The accuracy (float)\n\n    Notes\n    -----\n    The gradient is calculated analytically as 2/N * (y_pred \u2212 y_true),\n    where N is the number of samples. All returned values are rounded to\n    four decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the Mean Squared Error\n    mse_value = np.mean((y_pred - y_true) ** 2)\n    \n    # Calculate the gradient\n    n_samples = len(y_true)\n    gradient = [2 / n_samples * (ypred[i] - ytrue[i]) for i in range(n_samples)]\n    \n    # Calculate the classification accuracy\n    accuracy = np.mean(np.round(y_pred) == y_true)\n    \n    # Round all results to four decimal places\n    mse_value_rounded = round(mse_value, 4)\n    gradient_rounded = [round(g, 4) for g in gradient]\n    accuracy_rounded = round(accuracy, 4)\n    \n    return mse_value_rounded, gradient_rounded, accuracy_rounded\n\n# Example usage:\ny_true = [0, 1, 0, 1]\ny_pred = [0.2, 0.8, 0.6, 0.4]\nmse, grad, acc = mse_loss(y_true, y_pred)\nprint(f\"MSE: {mse}, Gradient: {grad}, Accuracy: {acc}\")\n```\n\n### Explanation:\n- **Input Handling**: The function first converts both `y_true` and `y_pred` into numpy arrays to facilitate efficient computation.\n- **Mean Squared Error Calculation**: It computes the mean squared error using the formula \\(\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\\), where \\(N\\) is the number of samples.\n- **Gradient Calculation**: The analytical gradient is computed as \\(2/N \\cdot (\\hat{y}_i - y_i)\\) for each sample.\n- **Accuracy Calculation**: The accuracy is determined by comparing the rounded predictions to the true labels and taking the mean.\n- **Rounding**: All computed values are rounded to four decimal places before being returned."}
{"task_id": 280, "completion_id": 0, "completion": "To implement the L2 regularization term and its gradient, we need to follow these steps:\n\n1. Calculate the squared norm of the weight vector or matrix.\n2. Multiply the squared norm by the regularization coefficient `alpha`.\n3. If `return_grad` is `True`, compute the gradient by multiplying the weight vector by `alpha`.\n\nHere's the implementation of the `l2_regularization` function:\n\n```python\nimport numpy as np\n\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool = False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector or matrix.\n    alpha : float\n        Regularization strength (must be non-negative).\n    return_grad : bool, optional (default=False)\n        If True, return the gradient of the regularization term instead of the\n        regularization value itself.\n\n    Returns\n    -------\n    float or np.ndarray\n        \u2022 When *return_grad* is False: the scalar L2 regularization value.\n        \u2022 When *return_grad* is True:  the gradient with the same shape as *w*.\n    \"\"\"\n    if not isinstance(alpha, (int, float)) or alpha < 0:\n        raise ValueError(\"Alpha must be a non-negative float.\")\n    \n    # Calculate the squared norm of the weight vector or matrix\n    squared_norm = np.sum(w**2)\n    \n    # Compute the regularization value\n    regularization_value = 0.5 * alpha * squared_norm\n    \n    # Check if return_grad is True\n    if return_grad:\n        # Compute the gradient\n        gradient = alpha * w\n        # Round the gradient to 4 decimal places\n        gradient_rounded = np.round(gradient, decimals=4)\n        return gradient_rounded\n    else:\n        # Return the regularization value rounded to 4 decimal places\n        return np.round(regularization_value, decimals=4)\n\n# Example usage:\nw = np.array([1.0, -2.0, 3.0])\nalpha = 0.1\n\nprint(l2_regularization(w, alpha))            # \u279c 0.7\nprint(l2_regularization(w, alpha, True))      # \u279c array([ 0.1, -0.2,  0.3])\n```\n\n### Explanation:\n\n- **Squared Norm Calculation**: We use `np.sum(w**2)` to compute the sum of squares of all elements in the weight vector or matrix `w`.\n  \n- **Regularization Value**: The formula for the L2 regularization value is \\(0.5 \\cdot \\alpha \\cdot \\sum_{i} w_i^2\\).\n\n- **Gradient Calculation**: The gradient of the regularization term with respect to `w` is simply \\( \\alpha \\cdot w \\).\n\n- **Rounding**: We round each element of the result to the nearest 4 decimal places using `np.round()`. This ensures that the output is formatted consistently.\n\nThis function handles both cases where `return_grad` is `True` and `False`, providing the appropriate output based on the specified flag."}
{"task_id": 281, "completion_id": 0, "completion": "Here's the implementation of the `L1Regularization` class with the required functionality:\n\n```python\nimport numpy as np\n\nclass L1Regularization:\n    def __init__(self, alpha: float):\n        if alpha < 0:\n            raise ValueError(\"Alpha must be a non-negative float.\")\n        self.alpha = alpha\n\n    def __call__(self, w: np.ndarray) -> float:\n        \"\"\"Returns the L1 penalty value.\"\"\"\n        return self.alpha * np.linalg.norm(w, ord=1)\n\n    def grad(self, w: np.ndarray) -> np.ndarray:\n        \"\"\"Returns the gradient of the penalty with respect to w.\"\"\"\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = 0  # Ensure sign(0) = 0\n        return self.alpha * sign_w\n\n# Example usage:\nalpha = 0.5\nl1_reg = L1Regularization(alpha)\n\n# Calculate L1 penalty for a vector\nw_vector = np.array([1.0, -2.0, 3.0])\npenalty_value = l1_reg(w_vector)\nprint(f\"L1 penalty for vector {w_vector}: {penalty_value}\")\n\n# Calculate gradient for the vector\ngradient = l1_reg.grad(w_vector)\nprint(f\"Gradient of L1 penalty for vector {w_vector}: {gradient}\")\n```\n\n### Explanation:\n- **Initialization**: The `__init__` method initializes the regularization strength with a check to ensure it is non-negative.\n- **Call Method**: The `__call__` method computes the L1 norm of the input array `w` using `np.linalg.norm` with `ord=1`, scales it by `alpha`, and returns the result.\n- **Grad Method**: The `grad` method calculates the element-wise sign of `w`, ensuring that `sign(0)` is treated as 0. It then multiplies this sign vector by `alpha` and returns the resulting array.\n\nThis implementation handles vectors, matrices, and other arrays of any shape correctly."}
{"task_id": 282, "completion_id": 0, "completion": "To solve this problem, we will implement the Welford's online algorithm for calculating the sample mean, variance, and standard deviation in a single pass through the data. Here's how you can do it:\n\n```python\nimport math\n\ndef running_stats(data: list[int | float]) -> tuple[float, float, float]:\n    \"\"\"Return the running mean, variance and standard deviation of a stream.\n\n    The computation must be *online*: iterate over the data once while\n    keeping only a constant amount of memory (do not call numpy.mean,\n    numpy.var, statistics.mean, etc.).  Implement Welford's method as\n    explained in the task description.  All three returned numbers must\n    be rounded to 4 decimal places.  If the input list is empty return\n    (0.0, 0.0, 0.0).\n    \"\"\"\n    if not data:\n        return (0.0, 0.0, 0.0)\n    \n    n = 0\n    m = 0.0\n    s = 0.0\n    \n    for x in data:\n        n += 1\n        delta = x - m\n        m += delta / n\n        s += delta * (x - m)\n    \n    variance = s / (n - 1) if n > 1 else 0.0\n    std_dev = math.sqrt(variance)\n    \n    # Round to 4 decimal places\n    mean_rounded = round(mean, 4)\n    variance_rounded = round(variance, 4)\n    std_dev_rounded = round(std_dev, 4)\n    \n    return (mean_rounded, variance_rounded, std_dev_rounded)\n\n# Example usage:\ndata = [1.0, 2.0, 3.0, 4.0]\nresult = running_stats(data)\nprint(result)  # Output: (2.5, 1.25, 1.1180)\n```\n\n### Explanation:\n\n1. **Initialization**: We start by initializing three variables:\n   - `n`: A counter for the number of data points seen so far.\n   - `m`: The running mean.\n   - `s`: The running sum of squares of differences from the current mean.\n\n2. **Iterate Through Data**: For each data point `x` in the list:\n   - Increment `n` by 1.\n   - Calculate the difference `delta` between the new data point `x` and the current mean `m`.\n   - Update the mean `m` using the formula `m = m + delta / n`.\n   - Update the sum of squares `s` using the formula `s = s + delta * (x - m)`.\n\n3. **Calculate Variance and Standard Deviation**:\n   - After processing all data points, calculate the sample variance as `s / (n - 1)` if `n > 1`. Otherwise, set the variance to 0.\n   - Compute the standard deviation as the square root of the variance.\n\n4. **Rounding**: Finally, round each result to 4 decimal places before returning them as a tuple.\n\nThis implementation ensures that the calculations are done efficiently in a single pass, making it suitable for streaming data."}
{"task_id": 283, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_log_loss_metrics(actual, predicted, regularization=1.0):\n    \"\"\"Compute the approximate Newton leaf value and split gain for binary\n    cross-entropy (logistic) loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        1-D array with true labels (0 or 1).\n    predicted : np.ndarray\n        1-D array with raw prediction scores.\n    regularization : float, default 1.0\n        L2 regularisation term \u03bb used in gradient boosting formulas.\n\n    Returns\n    -------\n    list[float]\n        A two-element list `[leaf_value, gain]`, each rounded to 4 decimals.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Calculate probabilities using the sigmoid function\n    p = 1 / (1 + np.exp(-predicted))\n    \n    # Compute gradients (g_i)\n    g = p - actual\n    \n    # Compute Hessians (h_i)\n    h = p * (1 - p)\n    \n    # Calculate the sum of gradients and Hessians\n    sum_g = np.sum(g)\n    sum_h = np.sum(h)\n    \n    # Calculate the leaf value\n    leaf_value = sum_g / (sum_h + regularization)\n    \n    # Calculate the gain\n    gain = 0.5 * (sum_g ** 2) / (sum_h + regularization)\n    \n    # Round the results to 4 decimal places\n    leaf_value_rounded = round(leaf_value, 4)\n    gain_rounded = round(gain, 4)\n    \n    return [leaf_value_rounded, gain_rounded]\n\n# Example usage:\n# actual = np.array([0, 1, 0, 1])\n# predicted = np.array([-0.5, 0.3, -0.8, 0.7])\n# print(binary_log_loss_metrics(actual, predicted))\n```\n\nThis function calculates the approximate Newton leaf value and split gain for binary cross-entropy loss based on the provided true labels and raw prediction scores, incorporating an optional L2 regularisation term. The results are returned as a list of two rounded values."}
{"task_id": 284, "completion_id": 0, "completion": "To implement the `iht_get_index` function according to the specified requirements, we need to handle both the case where the table is full and the case where it is not. Here's how you can do it:\n\n```python\ndef iht_get_index(\n    obj: object,\n    size: int,\n    table: dict,\n    overfull_count: list[int],\n    readonly: bool = False,\n) -> int | None:\n    \"\"\"Map *obj* to a stable integer index using a bounded index hash table.\n\n    Args:\n        obj:               Hashable object that should receive an index.\n        size:              Maximum number of *stored* entries.\n        table:             Mutable dictionary holding the current mapping.\n        overfull_count:    One-element list whose integer counts collisions.\n        readonly:          If True, never modify *table* or *overfull_count*.\n\n    Returns:\n        The integer index for *obj*, or None when *readonly* is True and the\n        object is not yet in *table*.\n    \"\"\"\n    # Check if the table is full\n    if len(table) >= size:\n        # If readonly, return None\n        if readonly:\n            return None\n        # Otherwise, return the hash of the object modulo size and increment overfull_count\n        else:\n            overfull_count[0] += 1\n            return hash(obj) % size\n    else:\n        # If the object is not in the table, add it with the current table size as the index\n        if obj not in table:\n            table[obj] = len(table)\n        # Always return the stored index\n        return table[obj]\n```\n\n### Explanation:\n\n1. **Check if the table is full**: \n   - We first check if the length of the `table` exceeds `size`.\n   - If it is, we have two cases:\n     - If `readonly` is `True`, we return `None` because no changes are allowed.\n     - If `readonly` is `False`, we calculate the hash of the object using `hash(obj)` and return it modulo `size`, then increment the `overfull_count`.\n\n2. **Insert the object into the table**:\n   - If the table is not full, we check if the object is not already in the table.\n   - If the object is not in the table, we add it to the table with the current length of the table as its index.\n   - Regardless of whether the object was already in the table or not, we return the stored index.\n\nThis implementation ensures that the function behaves correctly according to the problem description, handling both the full and non-full states of the table while respecting the `readonly` flag."}
{"task_id": 285, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    # Ensure both inputs are 2D arrays\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    if y.ndim == 1:\n        y = y.reshape(1, -1)\n\n    # Check if the number of features in both inputs matches\n    if x.shape[1] != y.shape[1]:\n        return -1\n\n    # Compute the Gram matrix using matrix multiplication\n    gram_matrix = x @ y.T\n\n    # Convert the result to a nested Python list using tolist()\n    return gram_matrix.tolist()\n\n# Example usage:\nx = np.array([[1, 2], [3, 4]])\ny = np.array([[5, 6], [7, 8]])\nprint(linear_kernel(x, y))  # Output: [[17, 23], [39, 53]]\n```\n\nThis code defines the `linear_kernel` function, which computes the linear kernel (Gram matrix) between two input data sets `x` and `y`. It handles cases where the inputs might be one-dimensional by converting them to row vectors. The function checks if the feature dimensions of `x` and `y` match and returns `-1` if they do not. Otherwise, it computes the Gram matrix using matrix multiplication and converts the result to a nested Python list using `tolist()`."}
{"task_id": 286, "completion_id": 0, "completion": "Here's the implementation of the `get_initializer` function:\n\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    return [[0.0] * cols for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    return [[1.0] * cols for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    return [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    if name not in globals():\n        raise ValueError(\"Invalid initialization function.\")\n    \n    initializer = globals()[name]\n    if not callable(initializer):\n        raise ValueError(\"Invalid initialization function.\")\n    \n    return initializer\n```\n\n### Explanation:\n1. **Function Definitions**: We define three functions `zeros_init`, `ones_init`, and `random_uniform_init` as specified in the problem description. Each function returns a matrix filled with zeros, ones, or randomly distributed numbers, respectively.\n\n2. **`get_initializer` Function**:\n   - It first checks if the provided `name` exists in the global namespace using `globals().get(name)`. If not, it raises a `ValueError`.\n   - It then retrieves the function associated with the given name from the global namespace.\n   - It checks if the retrieved object is callable using `callable()`. If not, it raises a `ValueError`.\n   - Finally, it returns the callable initializer.\n\nThis implementation ensures that the function behaves as expected and provides clear error messages when invalid input is given."}
{"task_id": 287, "completion_id": 0, "completion": "To implement the forward and backward passes of a 2-D average pooling layer, we need to follow these steps:\n\n### Forward Pass\n\n1. Calculate the output size of the pooling layer.\n2. For each channel and position in the input, compute the average of the values within the pooling window.\n3. Store the result in the output array.\n\n### Backward Pass\n\n1. Iterate over each channel and position in the output.\n2. Distribute the gradient from the accumulated gradient to the corresponding positions in the input based on the pooling window size and stride.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    # Unpack the input parameters\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n    \n    # Calculate the output size\n    out_H = (H - p_h + 1) // s_h\n    out_W = (W - p_w + 1) // s_w\n    \n    # Initialize the output array\n    pooled_output = np.zeros((N, C, out_H, out_W))\n    \n    # Perform the forward pass\n    for n in range(N):\n        for c in range(C):\n            for h in range(out_H):\n                for w in range(out_W):\n                    start_h = h * s_h\n                    end_h = start_h + p_h\n                    start_w = w * s_w\n                    end_w = start_w + p_w\n                    pooled_output[n, c, h, w] = np.mean(X[n, c, start_h:end_h, start_w:end_w])\n    \n    # Initialize the gradient input array\n    grad_input = np.zeros_like(X)\n    \n    # Perform the backward pass\n    for n in range(N):\n        for c in range(C):\n            for h in range(out_H):\n                for w in range(out_W):\n                    start_h = h * s_h\n                    end_h = start_h + p_h\n                    start_w = w * s_w\n                    end_w = start_w + p_w\n                    \n                    # Accumulate the gradient to the appropriate positions in the input\n                    grad_input[n, c, start_h:end_h, start_w:end_w] += accum_grad[n, c, h, w]\n    \n    # Round the results to the nearest 4th decimal and convert to lists\n    pooled_output_rounded = np.round(pooled_output, decimals=4).tolist()\n    grad_input_rounded = np.round(grad_input, decimals=4).tolist()\n    \n    return pooled_output_rounded, grad_input_rounded\n\n# Example usage:\n# X = np.array([[[[1, 2, 3],\n#                  [4, 5, 6],\n#                  [7, 8, 9]],\n#                 [[10, 11, 12],\n#                  [13, 14, 15],\n#                  [16, 17, 18]]]])\n# pool_shape = (2, 2)\n# stride = (2, 2)\n# accum_grad = np.ones_like(X)\n\n# output, grad_input = average_pool2d(X, pool_shape, stride, accum_grad)\n# print(\"Output:\", output)\n# print(\"Gradient Input:\", grad_input)\n```\n\n### Explanation\n\n- **Forward Pass**: We iterate over each channel and position in the output, calculate the mean of the values within the pooling window, and store the result in the output array.\n  \n- **Backward Pass**: We iterate over each channel and position in the output, distribute the gradient from the accumulated gradient to the corresponding positions in the input based on the pooling window size and stride.\n\nThis implementation ensures that the rounding and conversion to lists meet the specified constraints."}
{"task_id": 288, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef squared_error(actual, predicted):\n    \"\"\"Calculate the squared error or mean squared error.\n\n    If *actual* and *predicted* are scalars, return the squared difference.\n    If they are one-dimensional iterables (same length), return the mean squared\n    error.  Return -1 for length mismatches or empty iterables.\n\n    All results must be rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(actual, (int, float)) and isinstance(predicted, (int, float)):\n        # Calculate the squared difference for scalar inputs\n        return round((actual - predicted) ** 2, 4)\n    \n    elif hasattr(actual, '__iter__') and hasattr(predicted, '__iter__'):\n        # Check if both inputs are iterables\n        actual_list = list(actual)\n        predicted_list = list(predicted)\n        \n        # Check if both lists have the same length\n        if len(actual_list) != len(predicted_list):\n            return -1\n        \n        # Calculate the squared differences and compute the mean\n        squared_diffs = [(a - p) ** 2 for a, p in zip(actual_list, predicted_list)]\n        mse = sum(squared_diffs) / len(squared_diffs)\n        return round(mse, 4)\n    \n    else:\n        # Return -1 for non-comparable inputs\n        return -1\n```\n\nThis function handles both scalar and iterable inputs according to the specified requirements. It calculates the squared difference for scalar inputs directly and computes the mean squared error for iterable inputs, ensuring all results are rounded to four decimal places."}
{"task_id": 289, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `format_entries` function according to the specified requirements:\n\n```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    # Use a generator expression to create each formatted line\n    formatted_lines = (f\"{field}: \\\"{value}\\\"\" for field, value in entries)\n    \n    # Join the formatted lines with a newline character\n    result = \"\\n\".join(formatted_lines)\n    \n    return result\n\n# Example usage:\nentries = [(\"username\", \"john_doe\"), (\"password\", \"s3cr3t\")]\nprint(format_entries(entries))\n```\n\n### Explanation:\n- The function uses a generator expression `(f\"{field}: \\\"{value}\\\"\" for field, value in entries)` to iterate over each tuple in the `entries` list.\n- For each tuple, it constructs a formatted string in the format `\"field\": \"value\"`.\n- The `join` method is then used to concatenate these formatted strings into a single string, with each line separated by a newline character (`\\n`).\n- If the `entries` list is empty, the generator expression will yield no items, and `join` will return an empty string."}
{"task_id": 290, "completion_id": 0, "completion": "To solve the problem of comparing two binary decision trees recursively, we need to ensure that both trees have the same structure, use the same features with identical thresholds, and that corresponding leaf nodes have the same values within a specified numerical tolerance. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        # Both are leaves, check if their values are close within a tolerance\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    \n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Both are nodes, check if they have the same feature and threshold\n        if tree_a.feature != tree_b.feature or abs(tree_a.threshold - tree_b.threshold) > 1e-8:\n            return False\n        \n        # Recursively compare the left and right subtrees\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    \n    # If one is a leaf and the other is not, they cannot be equivalent\n    return False\n\n# Example usage:\n# Define some sample trees\ntree_a = Node(0, 5.0, Leaf(3), Leaf(7))\ntree_b = Node(0, 5.0, Leaf(3), Leaf(7))\n\nprint(compare_trees(tree_a, tree_b))  # Output: True\n```\n\n### Explanation:\n\n1. **Base Case**: \n   - If both `tree_a` and `tree_b` are `Leaf`, compare their `value`s using `np.allclose()` with a tolerance of \\(1 \\times 10^{-8}\\). This ensures that floating-point numbers are compared accurately.\n\n2. **Recursive Case**:\n   - If both `tree_a` and `tree_b` are `Node`, check if they have the same `feature` and `threshold`. If either condition fails, return `False`.\n   - Recursively call `compare_trees` on the `left` and `right` children of both nodes to ensure they are also equivalent.\n\n3. **Incompatible Types**:\n   - If one node is a `Leaf` and the other is not, return `False` because they cannot represent the same tree structure.\n\nThis recursive approach ensures that all parts of the trees are checked, including their structures and values, ensuring the trees are considered equivalent under the specified conditions."}
{"task_id": 291, "completion_id": 0, "completion": "Here's the implementation of the PCA transformation function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef pca_transform(X: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Project *X* onto its first *n_components* principal components.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (m, d) where *m* is the number of samples and *d*\n        the number of features.\n    n_components : int\n        Number of principal components to retain (1 \u2264 n_components \u2264 d).\n\n    Returns\n    -------\n    list[list[float]]\n        The transformed data rounded to 4 decimals.  If *n_components* is\n        invalid the function must return -1.\n    \"\"\"\n    if not (1 <= n_components <= X.shape[1]):\n        return -1\n    \n    # Step 2: Compute the dxd sample covariance matrix \u03a3 of X\n    cov_matrix = np.cov(X, rowvar=False)\n    \n    # Step 3: Find the eigenvalues \u03bb and corresponding eigenvectors v of \u03a3\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Step 4: Order the eigenpairs from the largest to the smallest eigenvalue\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Keep the first n eigenvectors\n    selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n    \n    # Step 6: Return X projected on the selected components, rounded to 4 decimals\n    projected_X = np.dot(X, selected_eigenvectors)\n    projected_X_rounded = np.round(projected_X, 4).tolist()\n    \n    return projected_X_rounded\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# n_components = 2\n# result = pca_transform(X, n_components)\n# print(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if `n_components` is within the valid range `[1, d]`. If not, it returns `-1`.\n\n2. **Covariance Matrix Calculation**: The sample covariance matrix `\u03a3` is computed using `np.cov(X, rowvar=False)`.\n\n3. **Eigenvalue and Eigenvector Computation**: The eigenvalues and eigenvectors of `\u03a3` are found using `np.linalg.eig`.\n\n4. **Sorting Eigenpairs**: The eigenpairs are sorted based on their eigenvalues in descending order. This ensures that the principal components are ordered by importance.\n\n5. **Selecting Principal Components**: The top `n_components` eigenvectors are selected.\n\n6. **Projection and Rounding**: The input data `X` is projected onto the selected eigenvectors, and the result is rounded to four decimal places before being returned as a Python list.\n\nThis function implements the classical PCA algorithm without relying on any third-party libraries beyond NumPy."}
{"task_id": 292, "completion_id": 0, "completion": "Here's the implementation of the `single_point_crossover` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert the input lists to NumPy arrays for efficient computation\n    parent1_np = np.array(parent1)\n    parent2_np = np.array(parent2)\n\n    # Validate that the parent matrices have the same shape\n    if parent1_np.shape != parent2_np.shape:\n        return -1\n\n    # Ensure the cutoff is within valid bounds\n    if cutoff < 0 or cutoff > parent1_np.shape[1]:\n        return -1\n\n    # Perform the single-point column crossover\n    child1_np = np.hstack((parent1_np[:, :cutoff], parent2_np[:, cutoff:]))\n    child2_np = np.hstack((parent2_np[:, :cutoff], parent1_np[:, cutoff:]))\n\n    # Convert the resulting NumPy arrays back to lists\n    child1 = child1_np.tolist()\n    child2 = child2_np.tolist()\n\n    return (child1, child2)\n\n# Example usage:\nparent1 = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\nparent2 = [\n    [7, 8, 9],\n    [10, 11, 12]\n]\n\ncutoff = 1\nchildren = single_point_crossover(parent1, parent2, cutoff)\nprint(children)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the shapes of the two parent matrices are the same. If not, it returns `-1`.\n2. **Cutoff Validation**: It ensures that the `cutoff` is within the valid range (0 to the number of columns).\n3. **Crossover Operation**: Using NumPy, it slices the columns before and after the `cutoff` from both parents and concatenates them to form the children.\n4. **Conversion to Lists**: Finally, it converts the resulting NumPy arrays back to ordinary Python lists and returns them as a tuple."}
{"task_id": 293, "completion_id": 0, "completion": "Here's how you can implement the `map_agent_environment` function to meet the requirements:\n\n```python\ndef map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    # Normalize the agent name by stripping whitespace and converting to lowercase\n    normalized_agent_name = agent_name.strip().lower()\n    \n    # Define the mapping of agent names to their corresponding environment and access type\n    agent_mapping = {\n        \"dqn\": (\"CartPole-v0\", \"unwrapped\"),\n        \"ddpg\": (\"Pendulum-v0\", \"env\"),\n        \"a3c\": (\"Pendulum-v0\", \"unwrapped\"),\n        \"a2c\": (\"CartPole-v0\", \"env\"),\n        \"a2c_multi\": (\"CartPole-v0\", \"raw\"),\n        \"trpo\": (\"Pendulum-v0\", \"unwrapped\")\n    }\n    \n    # Check if the normalized agent name exists in the mapping\n    if normalized_agent_name in agent_mapping:\n        return agent_mapping[normalized_agent_name]\n    else:\n        return -1\n\n# Example usage:\nprint(map_agent_environment(\"DQN\"))  # Output: ('CartPole-v0', 'unwrapped')\nprint(map_agent_environment(\"DDPG\"))  # Output: ('Pendulum-v0', 'env')\nprint(map_agent_environment(\"A2C_MULTI\"))  # Output: ('CartPole-v0', 'raw')\nprint(map_agent_environment(\"unknown_agent\"))  # Output: -1\n```\n\n### Explanation:\n1. **Normalization**: The function first normalizes the input `agent_name` by stripping any leading or trailing whitespace and converting it to lowercase to ensure case insensitivity.\n\n2. **Mapping Definition**: A dictionary `agent_mapping` is defined where each key is the agent name in lowercase, and the value is a tuple containing the environment name and the access type.\n\n3. **Lookup**: The function checks if the normalized agent name exists in the `agent_mapping`. If it does, the corresponding environment and access type are returned as a tuple. If not, the function returns `-1`.\n\nThis approach efficiently maps agent names to their respective environments and access types while handling various cases and edge scenarios."}
{"task_id": 294, "completion_id": 0, "completion": "To solve this problem, we need to convert the given custom graph representation into a plain adjacency dictionary that meets the specified requirements. Here's how you can implement the `to_networkx` function:\n\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    adj = {}\n\n    # Check if the graph is directed\n    is_directed = G.get('is_directed', False)\n\n    # Create a map from vertex index back to label\n    I2V = {v: k for k, v in G['_V2I'].items()}\n\n    # Populate the adjacency dictionary\n    for i, edges in enumerate(G['_G']):\n        vertex_label = I2V[i]\n        adj[vertex_label] = []\n\n        for source_label, target_label, weight in edges:\n            # Add the edge to the adjacency list\n            adj[vertex_label].append((target_label, weight))\n\n            # If the graph is undirected, add the reverse edge\n            if not is_directed:\n                adj[target_label].append((vertex_label, weight))\n\n    return adj\n\n# Example usage:\n# G = {\n#     'is_directed': True,\n#     '_V2I': {'A': 0, 'B': 1, 'C': 2},\n#     '_G': [\n#         [('A', 'B', 5), ('A', 'C', 3)],\n#         [('B', 'A', 5), ('B', 'C', 2)],\n#         [('C', 'A', 3), ('C', 'B', 2)]\n#     ]\n# }\n# print(to_networkx(G))\n```\n\n### Explanation:\n\n1. **Input Parsing**: We first extract the necessary information from the input dictionary `G`, including whether the graph is directed and the mappings between labels and indices.\n\n2. **Reverse Mapping**: We create a reverse mapping `I2V` to convert indices back to labels, which will help us construct the adjacency list correctly.\n\n3. **Populate Adjacency Dictionary**: \n   - We iterate over each vertex and its corresponding edges in the `_G` list.\n   - For each edge, we add it to the adjacency list for the source vertex.\n   - If the graph is undirected, we also add the reverse edge to ensure symmetry.\n\n4. **Sorting Neighbors**: Since the problem requires sorting the neighbor lists alphabetically, we use Python's built-in sorting capabilities on the list of tuples.\n\n5. **Return Result**: Finally, we return the constructed adjacency dictionary.\n\nThis function ensures that the resulting adjacency dictionary meets all the specified constraints."}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already one\n    x = np.array(x)\n    \n    # Check if all elements in the array are either 0 or 1\n    return np.all((x == 0) | (x == 1))\n\n# Example usage:\nprint(is_binary(np.array([0, 1, 1, 0])))  # True\nprint(is_binary(np.array([0, 2, 1, 0])))  # False\nprint(is_binary(np.array([True, False, True])))  # True\nprint(is_binary(np.array([0.5, 1.0, 0.0])))  # False\n```\n\nThis solution defines the `is_binary` function, which first converts the input to a NumPy array if it isn't already one. It then uses `np.all` combined with a logical OR operation to check if all elements in the array are either `0` or `1`. The function returns `True` if this condition is met, otherwise `False`."}
{"task_id": 297, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef min_max_scale(data: list[list[int | float]], feature_range: tuple = (0, 1)) -> list[list[float]]:\n    \"\"\"Scale every feature column of *data* to a given interval.\n\n    Parameters\n    ----------\n    data : list[list[int | float]]\n        Two-dimensional numerical data set where each inner list is a sample.\n    feature_range : tuple, optional\n        Desired range for the transformed data given as ``(lower, upper)``. The\n        default is ``(0, 1)``.\n\n    Returns\n    -------\n    list[list[float]]\n        Scaled data rounded to four decimal places.  Returns ``-1`` if\n        ``feature_range`` is invalid (i.e. lower >= upper).\n    \"\"\"\n    # Check if feature_range is valid\n    if feature_range[0] >= feature_range[1]:\n        return -1\n    \n    # Convert data to a NumPy array\n    data_array = np.array(data)\n    \n    # Handle NaN values\n    data_array = np.nan_to_num(data_array)\n    \n    # Calculate the minimum and maximum of each column\n    mins = np.min(data_array, axis=0)\n    maxs = np.max(data_array, axis=0)\n    \n    # Avoid division by zero when all values in a column are the same\n    mask = maxs != mins\n    mins[mask] = 0\n    maxs[mask] = 1\n    \n    # Perform min-max scaling\n    scaled_data = ((data_array - mins) / (maxs - mins)) * (feature_range[1] - feature_range[0]) + feature_range[0]\n    \n    # Round the results to 4 decimal places\n    scaled_data_rounded = np.round(scaled_data, decimals=4).tolist()\n    \n    return scaled_data_rounded\n```\n\nThis function implements the Min-Max scaling algorithm as described, handling edge cases such as constant columns and ensuring that the output is rounded to four decimal places. It also checks for invalid feature ranges and returns `-1` in such cases."}
{"task_id": 298, "completion_id": 0, "completion": "To implement the `ngram_log_prob` function, we need to follow these steps:\n\n1. **Tokenize the Corpus**: Split each sentence into tokens using whitespace as the delimiter.\n2. **Pad the Query**: For N > 1, pad the query with `<bol>` at the beginning and `<eol>` at the end.\n3. **Count N-grams and Prefixes**: Use a dictionary to count all N-grams and their (N-1)-gram prefixes in the corpus.\n4. **Calculate Conditional Probabilities**: Compute the conditional probability for each N-gram in the padded query.\n5. **Compute Log-Probabilities**: Sum the natural logarithms of these probabilities.\n6. **Handle Edge Cases**: Return `float('-inf')` if any N-gram is unseen or if the query is too short.\n\nHere's the implementation of the function:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    # Step 1: Tokenize the corpus\n    token_counts = Counter()\n    prefix_counts = defaultdict(Counter)\n    \n    for sentence in corpus:\n        words = sentence.split()\n        for i in range(len(words)):\n            word = words[i]\n            token_counts[word] += 1\n            if i >= N - 1:\n                prefix = tuple(words[i - N + 1:i + 1])\n                prefix_counts[prefix][word] += 1\n    \n    # Step 2: Pad the query\n    if N == 1:\n        padded_query = query\n    else:\n        padded_query = '<bol>' * (N - 1) + query + '<eol>'\n    \n    # Step 3: Calculate conditional probabilities and log-probabilities\n    log_prob = 0.0\n    for i in range(len(padded_query) - N + 1):\n        prefix = tuple(padded_query[i:i+N-1].split())\n        word = padded_query[i+N-1]\n        \n        if prefix not in prefix_counts or word not in prefix_counts[prefix]:\n            return float('-inf')\n        \n        count_prefix = prefix_counts[prefix][word]\n        count_full = token_counts[tuple(prefix)]\n        \n        prob = count_prefix / count_full\n        log_prob += math.log(prob, 2)  # Using base 2 for binary logarithm\n        \n    # Step 4: Round the result to 4 decimal places\n    return round(log_prob, 4)\n\n# Example usage:\ncorpus = [\"a b a\", \"b a b\"]\nquery = \"a b\"\nN = 1\nprint(ngram_log_prob(corpus, query, N))  # Output: -1.3863\n```\n\n### Explanation:\n- **Tokenization**: We use `Counter` to count occurrences of each word and its prefixes.\n- **Padding**: For N > 1, we prepend `<bol>` and append `<eol>` to the query.\n- **Conditional Probability Calculation**: For each N-gram in the padded query, we check if it exists in the corpus. If it does, we calculate its probability using the formula \\( P(w_N | w_1...w_{N-1}) \\).\n- **Logarithm Calculation**: We use the natural logarithm (base 2) for numerical stability and accuracy.\n- **Edge Case Handling**: If any N-gram is missing from the corpus, the function returns `float('-inf')`."}
{"task_id": 299, "completion_id": 0, "completion": "To implement the K-Means clustering algorithm from scratch in Python, we need to follow these steps:\n\n1. Initialize the centroids with the first `k` samples of the dataset `X`.\n2. Assign each sample to the nearest centroid based on Euclidean distance.\n3. Recompute the centroids as the mean of all samples belonging to each cluster.\n4. Repeat steps 2 and 3 until the centroids do not change or the maximum number of iterations is reached.\n\nHere's the implementation of the K-Means clustering algorithm using NumPy:\n\n```python\nimport numpy as np\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((x1 - x2) ** 2))\n\ndef k_means(X: np.ndarray, k: int, max_iterations: int = 500) -> list[int]:\n    \"\"\"K-Means clustering.\n\n    Args:\n        X: 2-D NumPy array of shape (m, n) containing the input samples.\n        k: Number of clusters to form.\n        max_iterations: Maximum number of iterations (default: 500).\n\n    Returns:\n        A list of length *m* where the i-th element is the cluster index of the\n        i-th sample.\n    \"\"\"\n    m, n = X.shape\n    \n    # Step 1: Initialize centroids\n    if k > m:\n        raise ValueError(\"Number of clusters cannot be greater than the number of samples.\")\n    \n    centroids = X[:k].copy()\n    \n    # Step 2: Assign samples to the nearest centroid\n    labels = np.zeros(m)\n    \n    # Step 3: Re-compute centroids\n    for _ in range(max_iterations):\n        new_centroids = np.zeros_like(centroids)\n        \n        for i in range(m):\n            distances = [euclidean_distance(X[i], c) for c in centroids]\n            min_index = np.argmin(distances)\n            labels[i] = min_index\n            \n            # Update the centroid\n            if len(labels == min_index) > 0:\n                new_centroids[min_index] = np.mean(X[labels == min_index], axis=0)\n        \n        # Check if centroids have changed\n        if np.allclose(new_centroids, centroids):\n            break\n        \n        centroids = new_centroids\n    \n    return labels.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [1, 8], [2, 2], [8, 8], [9, 1]])\n# k = 2\n# print(k_means(X, k))\n```\n\n### Explanation:\n\n- **Initialization**: We start by initializing the centroids with the first `k` samples of `X`. This ensures that the results are deterministic.\n- **Assignment**: For each sample, we calculate the Euclidean distance to each centroid and assign it to the centroid with the smallest distance.\n- **Recomputation**: We update each centroid by calculating the mean of all samples assigned to it.\n- **Convergence Check**: We check if the centroids have changed significantly between iterations. If they haven't changed after `max_iterations`, we terminate the algorithm.\n\nThis implementation handles cases where a cluster becomes empty by keeping its centroid unchanged, which prevents division-by-zero errors and ensures deterministic results."}
{"task_id": 300, "completion_id": 0, "completion": "To solve this problem, we will implement the `top_k_elements` function using a min-heap for the smallest elements and a max-heap for the largest elements. This approach ensures that we only maintain up to `k` elements at any time, which meets the requirements for memory efficiency.\n\nHere's how you can implement the solution:\n\n```python\nimport heapq\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef top_k_elements(sequence: List[Number], k: int, order: str = \"largest\") -> List[Number]:\n    \"\"\"Keeps only the k largest or k smallest numbers from *sequence*.\n\n    The function must visit each element exactly once and store no more than\n    *k* items at any time by making use of the heapq module (binary heap).\n\n    Args:\n        sequence: A list of integers or floats representing the data stream.\n        k: The number of extreme values to keep.\n        order: \"largest\" to retain the k biggest values, \"smallest\" to retain\n            the k smallest.\n\n    Returns:\n        A list with at most k numbers sorted according to *order*.\n    \"\"\"\n    if k <= 0 or not sequence:\n        return []\n\n    if k >= len(sequence):\n        return sorted(sequence, reverse=(order == \"largest\"))\n\n    if order == \"largest\":\n        # Use a min-heap to keep track of the k largest elements\n        min_heap = []\n        for num in sequence:\n            if len(min_heap) < k:\n                heapq.heappush(min_heap, num)\n            else:\n                if num > min_heap[0]:\n                    heapq.heapreplace(min_heap, num)\n        return sorted(min_heap, reverse=True)\n    \n    elif order == \"smallest\":\n        # Use a max-heap to keep track of the k smallest elements\n        max_heap = []\n        for num in sequence:\n            if len(max_heap) < k:\n                heapq.heappush(max_heap, -num)  # Negative because heapq is a min-heap\n            else:\n                if -num < max_heap[0]:\n                    heapq.heapreplace(max_heap, -num)  # Negative because heapq is a min-heap\n        return [-num for num in sorted(max_heap)]  # Negate back to positive\n\n# Example usage:\nsequence = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\nprint(top_k_elements(sequence, 3))  # Output: [9, 6, 5]\nprint(top_k_elements(sequence, 3, order=\"smallest\"))  # Output: [1, 1, 2]\n```\n\n### Explanation:\n\n1. **Edge Cases**: \n   - If `k` is less than or equal to zero or the sequence is empty, return an empty list.\n   - If `k` is greater than or equal to the length of the sequence, return the sorted sequence in the specified order.\n\n2. **Order Handling**:\n   - For \"largest\", use a min-heap to keep track of the largest elements. The root of the heap will always be the smallest element among the current `k` largest elements.\n   - For \"smallest\", use a max-heap by pushing negative values into the heap. This allows us to maintain the largest elements as they appear in the sequence.\n\n3. **Heap Operations**:\n   - Use `heapq.heappush` to add new elements to the heap.\n   - Use `heapq.heapreplace` to replace the root of the heap with a new element if it's larger/smaller than the root, ensuring the heap size remains `k`.\n\n4. **Result Construction**:\n   - After processing all elements, extract the elements from the heap and sort them accordingly based on the original order.\n\nThis implementation efficiently maintains the required constraints and handles all edge cases effectively."}
{"task_id": 301, "completion_id": 0, "completion": "To solve this problem, we need to implement a function `inorder_keys` that constructs a binary search tree (BST) using a list of `(key, value)` pairs and then performs an in-order traversal to collect the keys in sorted order. Here's how you can achieve this:\n\n```python\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\n    def __lt__(self, other):\n        return self.val < other.val\n\n    def __le__(self, other):\n        return self.val <= other.val\n\n    def __gt__(self, other):\n        return self.val > other.val\n\n    def __ge__(self, other):\n        return self.val >= other.val\n\ndef inorder_keys(pairs):\n    \"\"\"Build a BST from (key, value) *pairs* and return keys from an in-order traversal.\n\n    Args:\n        pairs (list[tuple[Any, int | float]]): List of (key, numeric value) pairs.\n\n    Returns:\n        list: Keys in in-order (ascending by numeric value). Return an empty\n        list if *pairs* is empty.\n    \"\"\"\n    if not pairs:\n        return []\n\n    # Create the root node with the first pair\n    root = Node(pairs[0][1])\n    key_to_node = {pairs[0][0]: root}\n\n    # Insert remaining pairs into the BST\n    for key, value in pairs[1:]:\n        node = Node(value)\n        key_to_node[key] = node\n\n        current = root\n        while True:\n            if node < current:\n                if current.left is None:\n                    current.left = node\n                    break\n                else:\n                    current = current.left\n            else:\n                if current.right is None:\n                    current.right = node\n                    break\n                else:\n                    current = current.right\n\n    # Perform in-order traversal and collect keys\n    def in_order_traversal(node):\n        if node is not None:\n            in_order_traversal(node.left)\n            result.append(key_to_node[node.val].key)\n            in_order_traversal(node.right)\n\n    result = []\n    in_order_traversal(root)\n    return result\n```\n\n### Explanation:\n\n1. **Node Class**: We define a `Node` class with attributes for the value (`val`), left child (`left`), and right child (`right`). We also implement comparison methods (`__lt__`, `__le__`, `__gt__`, `__ge__`) based on the stored numeric value.\n\n2. **Building the BST**:\n   - If the input list `pairs` is empty, we return an empty list.\n   - We create the root node with the first pair's value and store it in a dictionary `key_to_node` where the key is the original key from the pair and the value is the corresponding `Node`.\n   - For each subsequent pair, we create a new `Node` and insert it into the BST using the comparison operators defined in the `Node` class. We traverse the tree to find the correct position for the new node.\n\n3. **In-Order Traversal**:\n   - We define a helper function `in_order_traversal` that performs an in-order traversal of the BST.\n   - During the traversal, we collect the keys from the `key_to_node` dictionary to ensure the keys are returned in the order of their numeric values.\n\n4. **Return Result**: Finally, we return the list of collected keys.\n\nThis implementation ensures that the BST is built correctly according to the rules and that the in-order traversal yields the keys in ascending order."}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n\n    # Step 1: Build the weighted adjacency matrix W\n    n = X.shape[0]\n    W = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                W[i, j] = 1 / (1 + np.linalg.norm(X[i] - X[j]))\n\n    # Step 2: Compute the degree matrix D\n    D = np.diag(np.sum(W, axis=1))\n\n    # Step 3: Compute the unnormalized graph Laplacian L = D - W\n    L = D - W\n\n    # Step 4: Obtain the first k eigenvectors of L\n    eigvals, eigvecs = np.linalg.eig(L)\n    idx = np.argsort(eigvals)[:k]\n    E = eigvecs[:, idx]\n\n    # Step 5: Run k-means on the rows of E\n    num_iterations = 100\n    centroids = E[:k]\n    labels = np.full(n, -1)\n\n    for _ in range(num_iterations):\n        new_labels = np.argmin(np.linalg.norm(E[:, None] - centroids[None, :], axis=-1), axis=0)\n        for i in range(k):\n            cluster_indices = np.where(labels == i)[0]\n            if len(cluster_indices) > 0:\n                centroids[i] = np.mean(E[cluster_indices], axis=0)\n            else:\n                centroids[i] = E[np.random.randint(k)]\n\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n    return labels.tolist()\n```\n\nThis code implements the Spectral Clustering algorithm from scratch using only NumPy for numerical operations. It follows all the specified steps and returns the resulting cluster labels as a Python list."}
{"task_id": 303, "completion_id": 0, "completion": "To solve the problem, we need to construct a formatted string based on the input parameters and constraints. Here's how you can implement the `err_fmt` function:\n\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Extract elements from params and golds based on ix\n    prev_ix = max(ix - 1, 0)\n    curr_label = params[ix][1]\n    prev_label = params[prev_ix][1]\n    curr_mine = params[ix][0]\n    prev_mine = params[prev_ix][0]\n\n    # Construct the debug string\n    debug_string = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n{prev_mine}\\n\"\n        \"Theirs (prev) [{prev_label}]:\\n{golds.get(prev_label)}\\n\"\n        f\"Mine [{curr_label}]:\\n{curr_mine}\\n\"\n        f\"Theirs [{curr_label}]:\\n{golds.get(curr_label)}{warn_str}\\n\"\n        \"----------------------- END DEBUG -----------------------\"\n    )\n\n    return debug_string\n\n# Example usage:\nparams = [(\"output1\", \"test1\"), (\"output2\", \"test2\"), (\"output3\", \"test3\")]\ngolds = {\"test1\": \"expected_output1\", \"test2\": \"expected_output2\", \"test3\": \"expected_output3\"}\nix = 1\nwarn_str = \"\\nAdditional Warning: This is an error!\"\nprint(err_fmt(params, golds, ix, warn_str))\n```\n\n### Explanation:\n1. **Extracting Elements**: We use `max(ix - 1, 0)` to ensure we don't go out of bounds when accessing previous elements. This handles the case where `ix` is 0.\n\n2. **String Construction**: We build the debug string using f-strings for formatting. Each part of the string is concatenated in the specified order and format.\n\n3. **Handling Warn Str**: The `warn_str` is directly appended to the end of the \"Theirs\" section, ensuring it appears exactly as provided.\n\n4. **Output Format**: The string is returned in the exact format specified in the task description, including the necessary number of dashes and line breaks.\n\nThis implementation ensures that the output matches the requirements precisely."}
{"task_id": 304, "completion_id": 0, "completion": "To compute the posterior mean of the regression coefficients in Bayesian linear regression with an unknown noise variance, we can follow these steps:\n\n1. Prepend an intercept column to the design matrix \\( X \\) if `fit_intercept` is `True`.\n2. Define the prior distributions for the coefficients \\( b \\) and the noise variance \\( \\sigma^2 \\).\n3. Compute the posterior distribution parameters using the provided formulas.\n4. Calculate the posterior mean of the coefficients \\( \\mu_b \\).\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu: float = 0.0,\n    V: Union[None, float, Iterable[float], np.ndarray] = None,\n    fit_intercept: bool = True,\n) -> List[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    N, M = X.shape\n    \n    # Add intercept column if required\n    if fit_intercept:\n        X = np.hstack((np.ones((N, 1)), X))\n    \n    # Handle the prior scale V\n    if isinstance(V, (int, float)):\n        V = np.eye(M) * V\n    elif isinstance(V, (list, tuple)):\n        V = np.diag(V)\n    elif isinstance(V, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"V must be None, a scalar, a list/tuple, or a 2D array.\")\n    \n    # Compute the posterior covariance \u03a3_b\n    V_inv = np.linalg.inv(V)\n    XTX_inv = np.linalg.inv(X.T @ X)\n    Sigma_b = np.linalg.inv(V_inv + XTX_inv)\n    \n    # Compute the posterior mean \u03bc_b\n    V_inv_mu = V_inv @ mu.reshape(-1, 1)\n    XTy = X.T @ y\n    mu_b = Sigma_b @ (V_inv_mu + XTy)\n    \n    # Round the result to 4 decimal places\n    return [round(val, 4) for val in mu_b.flatten()]\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([5, 7, 9])\n# print(bayesian_posterior_mean(X, y, fit_intercept=True))\n```\n\n### Explanation:\n- **Intercept Handling**: If `fit_intercept` is `True`, an additional column of ones is added to the design matrix \\( X \\).\n- **Prior Scale V**: The prior scale \\( V \\) is handled based on its type:\n  - If `V` is a scalar, it is converted into a diagonal matrix.\n  - If `V` is a list or tuple, it is converted into a diagonal matrix.\n  - If `V` is a 2D array, it is used directly.\n- **Posterior Covariance Calculation**: The posterior covariance \\( \\Sigma_b \\) is computed using the formula \\( \\Sigma_b = (V^{-1} + X^T X)^{-1} \\).\n- **Posterior Mean Calculation**: The posterior mean \\( \\mu_b \\) is computed using the formula \\( \\mu_b = \\Sigma_b (V^{-1} \\mu + X^T y) \\).\n- **Rounding**: The result is rounded to 4 decimal places before returning."}
{"task_id": 305, "completion_id": 0, "completion": "To implement the numerically-stable logistic sigmoid function in Python, we need to consider the potential overflow issue when directly computing `e^(-x)` for large negative values of `x`. Instead, we can use the alternative formulation for negative inputs:\n\n\\[ \\sigma(x) = \\frac{e^x}{1 + e^x} \\]\n\nThis approach avoids overflow because `e^x` becomes very small for negative `x`, making the division safe.\n\nHere's the implementation of the `sigmoid` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Compute the element-wise logistic sigmoid in a numerically stable way.\n\n    The function must\n        \u2022 Accept a scalar, list, tuple or NumPy array of real numbers.\n        \u2022 Return a Python list with each sigmoid value rounded to 4 decimals.\n\n    Args:\n        x (int | float | list | tuple | np.ndarray): Input value(s).\n\n    Returns:\n        list[float]: List containing the sigmoid of every input element.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        # Convert single input to a list for uniform processing\n        x = [x]\n    \n    result = []\n    for val in x:\n        if val >= 0:\n            sig = 1 / (1 + np.exp(-val))\n        else:\n            sig = np.exp(val) / (1 + np.exp(val))\n        \n        # Round the result to 4 decimal places\n        rounded_sig = round(sig, 4)\n        result.append(rounded_sig)\n    \n    return result\n\n# Example usage:\nprint(sigmoid([0, 1, -2, 3]))  # Output: [0.5, 0.7311, 0.0998, 0.9526]\n```\n\n### Explanation:\n- **Input Handling**: The function accepts a scalar, list, tuple, or NumPy array. It converts any single input into a list to facilitate uniform processing.\n- **Sigmoid Calculation**: For each value in the input:\n  - If the value is non-negative, it uses the standard formula `1 / (1 + exp(-x))`.\n  - If the value is negative, it uses the alternative formula `exp(x) / (1 + exp(x))`.\n- **Rounding**: Each computed sigmoid value is rounded to 4 decimal places using Python's built-in `round()` function.\n- **Output**: The function returns a list of these rounded sigmoid values.\n\nThis implementation ensures numerical stability and handles both positive and negative input values correctly."}
{"task_id": 307, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bootstrap_sample(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return a bootstrap sample of the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, ...).\n    y : np.ndarray\n        Target vector (or array) of shape (n_samples, ...).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        ``(X_sample, y_sample)`` where each has the same shape as its\n        counterpart in the input.\n    \"\"\"\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n    return X[idxs], y[idxs]\n\n# Example usage:\n# Assuming X and y are defined\n# X_bootstrap, y_bootstrap = bootstrap_sample(X, y)\n```\n\nThis function uses `np.random.choice` to draw `n_samples` indices with replacement from the range `[0, n_samples-1]`. It then slices the input arrays `X` and `y` using these indices to create the bootstrap sample. The resulting arrays have the same shape as the input arrays."}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    if max_freq is None:\n        max_freq = fs // 2\n\n    # Convert frequencies to Mel scale\n    mel_min = 2595 * np.log10(1 + min_freq / 700)\n    mel_max = 2595 * np.log10(1 + max_freq / 700)\n\n    # Generate n_filters+2 equally-spaced Mel values\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n\n    # Convert Mel points back to Hertz\n    freqs = 700 * (10 ** (mel_points / 2595) - 1)\n\n    # Initialize the filter bank matrix\n    filter_bank = []\n\n    # Compute each filter\n    for i in range(n_filters):\n        f0 = freqs[i]\n        f1 = freqs[i + 1]\n        f2 = freqs[i + 2]\n\n        # Calculate the slopes of the triangle\n        m_left = (f1 - f0) / (f1 - f0)\n        m_right = (f2 - f0) / (f2 - f1)\n\n        # Find the positive minimum of the two slopes\n        w = np.minimum(m_left, m_right)\n\n        # Add the filter to the bank\n        filter_bank.append(w)\n\n    # Normalize the filters if required\n    if normalize:\n        for i in range(n_filters):\n            filter_bank[i] *= 2 / (freqs[i + 2] - freqs[i])\n\n    # Round the results to 4 decimal places\n    return [round(x, 4) for sublist in filter_bank for x in sublist]\n```"}
{"task_id": 309, "completion_id": 0, "completion": "Here's how you can implement the Factorization Machine prediction function according to the specifications:\n\n```python\nimport numpy as np\n\ndef fm_predict(X: np.ndarray, w0: float, w: np.ndarray, v: np.ndarray) -> list[float]:\n    \"\"\"Return Factorization Machine predictions for all samples in *X*.\n\n    The function must implement the FM prediction formula using the bias *w0*,\n    the linear weights *w* and the factor matrix *v* and return a Python list\n    of floats rounded to four decimal places.\n    \"\"\"\n    # Calculate the dot product between X and w\n    linear_term = np.dot(X, w)\n    \n    # Calculate the interaction term\n    k = v.shape[1]  # Number of factors\n    interaction_term = 0.5 * np.sum(\n        (np.square(X @ v.T) - np.square(np.multiply.outer(X, X)) @ np.square(v)),\n        axis=1\n    )\n    \n    # Combine the terms and add the bias\n    predictions = linear_term + interaction_term + w0\n    \n    # Round each prediction to four decimal places\n    rounded_predictions = [round(pred, 4) for pred in predictions]\n    \n    return rounded_predictions\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# w0 = 1.0\n# w = np.array([0.5, 0.5])\n# v = np.random.rand(2, 3)  # Random factor matrix\n# print(fm_predict(X, w0, w, v))\n```\n\n### Explanation:\n1. **Linear Term**: Compute the dot product of the feature matrix `X` and the linear weight vector `w`.\n2. **Interaction Term**: \n   - Multiply `X` by the transpose of `v` to get the element-wise products.\n   - Square these element-wise products and then square them again.\n   - Subtract the squared outer products of `X` with itself and `v`.\n   - Sum these differences and multiply by 0.5 to get the interaction term.\n3. **Combined Term**: Add the linear term and the interaction term to the bias `w0`.\n4. **Rounding**: Convert each prediction to a float and round it to four decimal places before returning the result as a list."}
{"task_id": 310, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_error(actual: list[float] | np.ndarray, predicted: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Root Mean Squared Error (RMSE) between two numeric sequences.\n\n    The function returns ``-1`` if the two inputs do not have the same\n    non-zero length.\n\n    Args:\n        actual: A list or NumPy array containing the true values.\n        predicted: A list or NumPy array containing the predicted values.\n\n    Returns:\n        A float \u2013 the RMSE rounded to 4 decimal places, or ``-1`` on invalid\n        input.\n    \"\"\"\n    # Check if the lengths of actual and predicted are the same and non-zero\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n    \n    # Calculate the squared differences\n    squared_diffs = np.square(np.array(actual) - np.array(predicted))\n    \n    # Calculate the mean of the squared differences\n    mean_squared_diff = np.mean(squared_diffs)\n    \n    # Calculate the square root of the mean squared difference\n    rmse = np.sqrt(mean_squared_diff)\n    \n    # Round the result to 4 decimal places\n    return round(rmse, 4)\n\n# Example usage:\nactual_values = [1, 2, 3, 4, 5]\npredicted_values = [1.1, 1.9, 3, 3.1, 4.9]\n\nprint(root_mean_squared_error(actual_values, predicted_values))  # Output: 0.3873\n```\n\nThis function first checks if the lengths of the `actual` and `predicted` lists are equal and non-zero. If they are not, it returns `-1`. Otherwise, it calculates the squared differences between corresponding elements, computes the mean of these squared differences, takes the square root of the mean, and rounds the result to four decimal places before returning it."}
{"task_id": 311, "completion_id": 0, "completion": "To implement the `clip_non_negative` function, we need to handle both lists and NumPy arrays. The function should convert any negative values to zero while preserving the structure of the input. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef clip_non_negative(arr):\n    \"\"\"Replace every negative number in *arr* with 0.0.\n\n    Parameters\n    ----------\n    arr : list | numpy.ndarray\n        A list (possibly nested) or NumPy array containing numeric values.\n\n    Returns\n    -------\n    list\n        A Python list of the same shape where all negative values are replaced\n        by 0.0. All elements in the returned list are floats.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already one\n    arr_np = np.array(arr)\n    \n    # Use NumPy's clip function to set negative values to zero\n    clipped_arr_np = np.clip(arr_np, 0.0, None)\n    \n    # Convert the clipped array back to a list\n    clipped_arr_list = clipped_arr_np.tolist()\n    \n    return clipped_arr_list\n\n# Example usage:\nnested_list = [[-1, 2, -3], [4, -5, 6]]\nprint(clip_non_negative(nested_list))  # Output: [[0.0, 2.0, 0.0], [4.0, 0.0, 6.0]]\n\narray_input = np.array([[-1, 2, -3], [4, -5, 6]])\nprint(clip_non_negative(array_input).tolist())  # Output: [[0.0, 2.0, 0.0], [4.0, 0.0, 6.0]]\n```\n\n### Explanation:\n1. **Conversion**: The function first converts the input to a NumPy array using `np.array()`. This allows us to leverage NumPy's powerful operations on arrays.\n2. **Clipping**: We use `np.clip()` to replace all negative values with zero. The `clip()` function takes three arguments: the array to clip, the lower bound (`0.0` in this case), and the upper bound (`None`, which means no upper limit). This ensures that all negative values are set to zero.\n3. **Conversion Back**: Finally, we convert the clipped NumPy array back to a Python list using `tolist()`. This ensures that the output is a list of floats, as required.\n4. **Preservation of Structure**: Since `np.clip()` operates on the entire array at once, it naturally preserves the structure of the input, including nested lists.\n\nThis approach efficiently handles both lists and NumPy arrays, ensuring that the original data remains unchanged and the desired output is produced."}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    \n    # Fixed coefficients for the Blackman-Harris window\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    # Generate the window\n    if symmetric:\n        N = window_len\n        w = np.array([a0 - a1 * np.cos(2 * np.pi * n / N) +\n                      a2 * np.cos(4 * np.pi * n / N) -\n                      a3 * np.cos(6 * np.pi * n / N) for n in range(N)])\n    else:\n        N = window_len + 1\n        w = np.array([a0 - a1 * np.cos(2 * np.pi * n / N) +\n                      a2 * np.cos(4 * np.pi * n / N) -\n                      a3 * np.cos(6 * np.pi * n / N) for n in range(N)])[:-1]\n    \n    # Round to 4 decimal places and convert to list\n    return round(w, 4).tolist()\n\n# Example usage:\nprint(blackman_harris(10))  # Symmetric window\nprint(blackman_harris(10, symmetric=False))  # Periodic window\n```"}
{"task_id": 313, "completion_id": 0, "completion": "To solve the problem of computing the expected logarithm of the \\( t \\)-th component of a Dirichlet-distributed vector, we need to implement the digamma function ourselves. The digamma function is the first derivative of the natural logarithm of the Gamma function. We will use a recursive approach to shift small values up to a moderate range and then apply an asymptotic expansion for larger values.\n\nHere's the implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def digamma(x):\n        # Base case for small x\n        if x < 6:\n            return sum(1 / i for i in range(1, int(x) + 1))\n        \n        # Asymptotic expansion for large x\n        ln_x = math.log(x)\n        term1 = -1 / (2 * x)\n        term2 = -1 / (12 * x**2)\n        term3 = 1 / (120 * x**4)\n        term4 = -1 / (252 * x**6)\n        \n        return ln_x + term1 + term2 + term3 + term4\n    \n    # Extract the relevant parameters\n    gamma_d = gamma[d]\n    \n    # Compute the sum of gamma[d]\n    gamma_sum = sum(gamma_d)\n    \n    # Compute the expected log using the digamma function\n    expected_log = digamma(gamma_d[t]) - digamma(gamma_sum)\n    \n    # Round to 4 decimal places\n    return round(expected_log, 4)\n\n# Example usage:\n# gamma = [[1, 2], [3, 4]]\n# print(expected_log_dirichlet(gamma, 0, 0))  # Output: 0.0\n```\n\n### Explanation:\n- **Digamma Function**: We define a `digamma` function that uses both a recursive method for small values of \\( x \\) and an asymptotic series for large values.\n- **Recursive Method**: For \\( x < 6 \\), we compute the sum of reciprocals from 1 to \\( x \\).\n- **Asymptotic Expansion**: For \\( x \\geq 6 \\), we use the series expansion to approximate the digamma function.\n- **Expected Log Calculation**: We calculate the expected log using the digamma functions of the specific parameter and their sum.\n- **Rounding**: Finally, we round the result to four decimal places before returning it.\n\nThis implementation ensures that we correctly compute the expected logarithm of the \\( t \\)-th component of a Dirichlet-distributed vector without relying on external libraries."}
{"task_id": 315, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_loss(name: str):\n    \"\"\"Return a loss function specified by *name*.\n\n    The function must recognise the following identifiers (case-insensitive,\n    underscores are ignored):\n        \u2022 MAE  \u2192  \"mae\" | \"mean_absolute_error\"\n        \u2022 MSE  \u2192  \"mse\" | \"mean_squared_error\"\n        \u2022 Binary Cross Entropy  \u2192  \"cross_entropy\" | \"log_loss\" | \"ce\"\n\n    The returned object has the signature `fn(y_true, y_pred) -> float` where\n    *y_true* and *y_pred* are 1-D sequences (list or NumPy array) of equal\n    length.  The resulting float must be rounded to **four** decimal places.\n\n    Args:\n        name: Name or alias of the desired loss function.\n\n    Returns:\n        A Python callable implementing the requested loss.\n\n    Raises:\n        ValueError: If *name* does not correspond to a supported loss.\n    \"\"\"\n    def mae(y_true, y_pred):\n        return round(np.mean(np.abs(y_true - y_pred)), 4)\n\n    def mse(y_true, y_pred):\n        return round(np.mean((y_true - y_pred)**2), 4)\n\n    def cross_entropy(y_true, y_pred):\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        return round(-np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)), 4)\n\n    name_lower = name.lower().replace('_', '')\n    \n    if name_lower == 'mae':\n        return mae\n    elif name_lower == 'mse':\n        return mse\n    elif name_lower == 'cross_entropy' or name_lower == 'log_loss' or name_lower == 'ce':\n        return cross_entropy\n    else:\n        raise ValueError(\"Invalid loss function.\")\n\n# Example usage:\n# loss_fn = get_loss('Mean Absolute Error')\n# result = loss_fn([1, 2, 3], [1.5, 2.5, 3.5])\n# print(result)  # Output will be 0.6667\n```\n\nThis implementation defines three separate loss functions for Mean Absolute Error (MAE), Mean Squared Error (MSE), and Binary Cross Entropy (Cross Entropy). It then maps these functions to their respective names, ignoring case and underscores, and raises a `ValueError` if the input name is not recognized. The returned functions are designed to accept 1-D sequences as input and return the calculated loss rounded to four decimal places."}
{"task_id": 316, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef euclidean(x, y):\n    \"\"\"Compute the Euclidean (L2) distance between two equal-length vectors.\n\n    Args:\n        x (list|tuple|np.ndarray): First real vector.\n        y (list|tuple|np.ndarray): Second real vector.\n\n    Returns:\n        float | int: The L2 distance rounded to four decimals, or -1 if the\n        vectors have unequal lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    x_array = np.array(x)\n    y_array = np.array(y)\n\n    # Check if the vectors have the same length\n    if len(x_array) != len(y_array):\n        return -1\n\n    # Compute the squared differences\n    squared_diffs = (x_array - y_array) ** 2\n\n    # Sum of squared differences\n    sum_squared_diffs = np.sum(squared_diffs)\n\n    # Calculate the Euclidean distance\n    distance = np.sqrt(sum_squared_diffs)\n\n    # Return the distance rounded to four decimal places\n    return round(distance, 4)\n```\n\nThis function first converts the input vectors `x` and `y` into NumPy arrays. It then checks if the vectors have the same length. If they do not, it returns `-1`. If they do, it calculates the squared differences between corresponding elements of the vectors, sums these squared differences, takes the square root of the sum to get the Euclidean distance, and finally rounds the result to four decimal places before returning it."}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    \n    prev_prediction, prev_label = params[prev_ix]\n    curr_prediction, curr_label = params[ix]\n    \n    prev_gold = golds.get(prev_label, \"\")\n    curr_gold = golds.get(curr_label, \"\")\n    \n    debug_report = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n{prev_prediction}\\n\\n\"\n        f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\\n\"\n        f\"Mine [{curr_label}]:\\n{curr_prediction}\\n\\n\"\n        f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\\n\"\n        \"----------------------- END DEBUG -----------------------\"\n    )\n    \n    return debug_report\n```\n\nThis function constructs the debug report according to the specified format by accessing the appropriate elements from the input lists and dictionary, formatting them correctly, and concatenating them into a single string."}
{"task_id": 318, "completion_id": 0, "completion": "To implement the AdaBoost.M1 algorithm from scratch using decision stumps, we need to follow these steps:\n\n1. Initialize the weights for each training sample.\n2. For each boosting round, train a decision stump on the current dataset and compute its weight.\n3. Update the weights based on the performance of the decision stump.\n4. Combine the weak learners to form the final classifier.\n\nHere's the complete implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    # Step 1: Initialize weights\n    n_samples = len(y_train)\n    w = np.ones(n_samples) / n_samples\n    \n    # List to store the predictions of each weak learner\n    predictions = []\n    \n    # Main loop for each boosting round\n    for _ in range(n_estimators):\n        # Step 2: Train a decision stump\n        best_stump, best_error = find_best_stump(X_train, y_train, w)\n        \n        # Calculate the weight of the current weak learner\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        \n        # Update the weights\n        w *= np.exp(-alpha * y_train * best_stump.predict(X_train))\n        w /= np.sum(w)\n        \n        # Add the prediction of the current weak learner to the list\n        predictions.append(best_stump.predict(X_test))\n    \n    # Step 3: Combine the weak learners\n    return combine_weak_learners(predictions)\n\ndef find_best_stump(X, y, w):\n    \"\"\"Find the best decision stump for the given dataset.\"\"\"\n    n_samples, n_features = X.shape\n    best_error = float('inf')\n    best_stump = None\n    \n    for j in range(n_features):\n        thresholds = np.unique(X[:, j])\n        for t in thresholds:\n            for p in [-1, 1]:\n                stump = DecisionStump(j, t, p)\n                \n                # Compute the error rate for this stump\n                errors = np.sum(w[y != stump.predict(X)])\n                error_rate = errors / np.sum(w)\n                \n                # Check if this is the best stump found so far\n                if error_rate < best_error:\n                    best_error = error_rate\n                    best_stump = stump\n    \n    return best_stump, best_error\n\nclass DecisionStump:\n    def __init__(self, feature_index, threshold, polarity):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.polarity = polarity\n    \n    def predict(self, X):\n        \"\"\"Predict the class labels for the input features.\"\"\"\n        predictions = np.sign(X[:, self.feature_index] - self.threshold) * self.polarity\n        return predictions\n\ndef combine_weak_learners(predictions):\n    \"\"\"Combine the predictions of multiple weak learners.\"\"\"\n    return np.sign(np.mean(predictions, axis=0))\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Load your data\n# predictions = adaboost_predict(X_train, y_train, X_test, n_estimators=50)\n```\n\n### Explanation:\n\n- **Initialization**: We initialize the weights equally for all samples.\n- **Decision Stump Training**: For each boosting round, we find the decision stump that minimizes the weighted classification error. This involves trying different feature indices and thresholds, and calculating the sign prediction based on the polarity.\n- **Weight Update**: The weights are updated according to the performance of the current decision stump.\n- **Combining Weak Learners**: Finally, we combine the predictions of all weak learners using their weighted average to form the final classifier.\n\nThis implementation uses simple techniques like sorting and linear scans to find the best decision stump at each step. It also includes a basic decision stump class for prediction purposes."}
{"task_id": 319, "completion_id": 0, "completion": "Here's the implementation of the `stochastic_gradient_descent` function using Stochastic Gradient Descent (SGD) for linear regression:\n\n```python\nimport numpy as np\n\ndef stochastic_gradient_descent(X: np.ndarray | list, y: np.ndarray | list, learning_rate: float = 0.01, epochs: int = 1000) -> list[float]:\n    \"\"\"Learn a linear model with Stochastic Gradient Descent.\n\n    The function must:\n      \u2022 prepend a bias column of ones to `X`;\n      \u2022 initialise the weight vector \u03b8 with zeros;\n      \u2022 perform `epochs` passes of SGD exactly as detailed in the task description;\n      \u2022 return \u03b8 rounded to 4 decimal places as a Python list.\n\n    Args:\n        X: Feature matrix (m\u00d7n) \u2013 can be a NumPy array or a Python list of lists.\n        y: Target vector (m,) or (m,1) \u2013 can be a NumPy array or a Python list.\n        learning_rate: Learning rate (\u03b1).\n        epochs: Number of epochs (full passes over the dataset).\n\n    Returns:\n        Python list containing the learned parameters [\u03b8\u2080, \u03b8\u2081, \u2026, \u03b8_n].\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Prepend a bias column of ones to X\n    m, n = X.shape\n    X = np.hstack((np.ones((m, 1)), X))\n    \n    # Initialize the weight vector \u03b8 with zeros\n    theta = np.zeros(n + 1)\n    \n    # Perform `epochs` passes of SGD\n    for epoch in range(epochs):\n        for i in range(m):\n            # Compute the prediction\n            hypothesis = np.dot(theta, X[i])\n            \n            # Compute the error\n            error = hypothesis - y[i]\n            \n            # Update the weights\n            theta -= learning_rate * error * X[i]\n    \n    # Round the final weight vector to 4 decimal places\n    return [round(val, 4) for val in theta]\n\n# Example usage:\n# X = [[1, 2], [1, 3], [1, 4]]\n# y = [5, 7, 9]\n# print(stochastic_gradient_descent(X, y))\n```\n\n### Explanation:\n1. **Input Conversion**: The function first checks if the inputs `X` and `y` are already NumPy arrays. If not, it converts them to NumPy arrays.\n\n2. **Bias Column**: It prepends a column of ones to the feature matrix `X` to account for the bias term \u03b8\u2080.\n\n3. **Initialization**: The weight vector `theta` is initialized with all zeros.\n\n4. **SGD Loop**: The function iterates over the specified number of epochs (`epochs`). In each epoch, it processes each training example (row) in the dataset:\n   - Computes the hypothesis (prediction) using the current weights.\n   - Calculates the error between the predicted value and the actual value.\n   - Updates the weights using the formula \\( \\theta <- \\theta - \\alpha \\cdot e \\cdot x_i \\), where \u03b1 is the learning rate.\n\n5. **Rounding**: Finally, the updated weights are rounded to four decimal places before being returned as a list.\n\nThis implementation ensures that the linear model is trained efficiently using SGD, even for larger datasets."}
{"task_id": 321, "completion_id": 0, "completion": "Here's a complete implementation of the Genetic Algorithm for evolving a population of strings towards a target string:\n\n```python\nimport numpy as np\nimport string\n\ndef genetic_algorithm(target: str, population_size: int, mutation_rate: float, iterations: int) -> str:\n    \"\"\"Evolve a population of random strings towards the *target* string using a\n    basic Genetic Algorithm and return the best string discovered.\n\n    Parameters\n    ----------\n    target : str\n        The string the algorithm should try to reproduce.\n    population_size : int\n        Number of individuals maintained in each generation.\n    mutation_rate : float\n        Probability with which each character is replaced by a random\n        character during the mutation step.\n    iterations : int\n        Maximum number of generations the algorithm is allowed to run.\n\n    Returns\n    -------\n    str\n        The highest-fitness string found during the evolutionary process.\n    \"\"\"\n    np.random.seed(42)\n    \n    # Define the alphabet\n    alphabet = string.ascii_letters + ' '\n    \n    def calculate_fitness(individual):\n        fitness_score = 0\n        L = len(individual)\n        \n        for i in range(L):\n            p_i = alphabet.index(individual[i])\n            t_i = alphabet.index(target[i])\n            fitness_score += abs(p_i - t_i)\n        \n        return 1 / (fitness_score + 1e-6)\n    \n    def select_parents(population, fitness_scores):\n        total_fitness = sum(fitness_scores)\n        probabilities = [score / total_fitness for score in fitness_scores]\n        parents_indices = np.random.choice(range(len(population)), size=2, p=probabilities, replace=False)\n        return population[parents_indices]\n    \n    def crossover(parents):\n        crossover_point = np.random.randint(1, len(parents[0]))\n        child1 = parents[0][:crossover_point] + parents[1][crossover_point:]\n        child2 = parents[1][:crossover_point] + parents[0][crossover_point:]\n        return child1, child2\n    \n    def mutate(child, mutation_rate):\n        for i in range(len(child)):\n            if np.random.rand() < mutation_rate:\n                child = child[:i] + np.random.choice(alphabet) + child[i+1:]\n        return child\n    \n    # Initialize the population\n    population = [''.join(np.random.choice(alphabet, size=len(target))) for _ in range(population_size)]\n    \n    best_string = None\n    best_fitness = 0\n    \n    for epoch in range(iterations):\n        fitness_scores = [calculate_fitness(individual) for individual in population]\n        \n        # Select parents\n        parent1, parent2 = select_parents(population, fitness_scores)\n        \n        # Crossover\n        child1, child2 = crossover((parent1, parent2))\n        \n        # Mutate\n        child1 = mutate(child1, mutation_rate)\n        child2 = mutate(child2, mutation_rate)\n        \n        # Replace the worst individual with the new children\n        worst_index = np.argmin(fitness_scores)\n        population[worst_index] = child1\n        population.append(child2)\n        \n        # Update the best string\n        current_best_fitness = max(fitness_scores)\n        if current_best_fitness > best_fitness:\n            best_string = population[np.argmax(fitness_scores)]\n            best_fitness = current_best_fitness\n        \n        # Print progress (optional)\n        if epoch % 100 == 0:\n            print(f\"Iteration {epoch}: Best Fitness = {best_fitness}\")\n    \n    return best_string\n\n# Example usage:\ntarget = \"hello world\"\npopulation_size = 100\nmutation_rate = 0.01\niterations = 1000\n\nbest_string = genetic_algorithm(target, population_size, mutation_rate, iterations)\nprint(best_string)\n```\n\n### Explanation:\n1. **Initialization**: The population is initialized with random strings of the same length as the target.\n2. **Fitness Evaluation**: Each individual's fitness is calculated based on the difference between their characters and the target string.\n3. **Selection**: Parents are selected using roulette-wheel selection based on their fitness scores.\n4. **Crossover**: Two children are generated by swapping parts of the selected parents' strings.\n5. **Mutation**: Each gene in the children has a small chance of being mutated to a random character.\n6. **Replacement**: The worst-performing individual in the population is replaced by the new children.\n7. **Termination**: The algorithm continues until the maximum number of iterations is reached or the target string is found.\n\nThis implementation uses a simple Genetic Algorithm approach to evolve a population of strings towards the target string."}
{"task_id": 322, "completion_id": 0, "completion": "Certainly! Below is the implementation of the binary cross-entropy loss function according to the specified requirements:\n\n```python\nimport numpy as np\n\nEPS = 1e-15  # small constant for numerical stability\n\ndef binary_crossentropy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Compute the mean binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        Ground-truth labels containing only 0s and 1s.\n    predicted : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as `actual`.\n\n    Returns\n    -------\n    float\n        Mean BCE value rounded to four decimal places.\n    \"\"\"\n    # Clip predicted values to avoid numerical issues\n    predicted_clipped = np.clip(predicted, EPS, 1 - EPS)\n    \n    # Calculate the BCE loss\n    bce_loss = -np.mean(actual * np.log(predicted_clipped) + (1 - actual) * np.log(1 - predicted_clipped))\n    \n    # Round the loss to 4 decimal places\n    return round(bce_loss, 4)\n\n# Example usage:\n# actual_labels = np.array([0, 1, 0, 1])\n# predicted_probabilities = np.array([0.2, 0.8, 0.7, 0.3])\n# print(binary_crossentropy(actual_labels, predicted_probabilities))  # Output: 0.6931\n```\n\n### Explanation:\n1. **Clipping**: The predicted probabilities are clipped to ensure they do not become exactly 0 or 1, which can lead to numerical instability. This is done using `np.clip(predicted, EPS, 1 - EPS)`.\n\n2. **Loss Calculation**: The binary cross-entropy loss is computed using the formula:\n   \\[\n   L = - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\cdot \\ln(p_i) + (1 - y_i) \\cdot \\ln(1 - p_i) ]\n   \\]\n   where \\( y_i \\) is the actual label and \\( p_i \\) is the predicted probability.\n\n3. **Mean Calculation**: The loss is averaged over all samples.\n\n4. **Rounding**: The resulting loss is rounded to four decimal places before being returned.\n\nThis implementation should correctly compute the mean binary cross-entropy loss for the given inputs."}
{"task_id": 324, "completion_id": 0, "completion": "To implement bilinear interpolation for arbitrary \\((x, y)\\) positions in a 2-D image, we need to follow these steps:\n\n1. **Clip Coordinates**: Ensure that all coordinates fall within the bounds of the image. This involves checking if each coordinate is within the valid range of rows and columns.\n\n2. **Bilinear Interpolation**: For each pair of coordinates \\((x_i, y_i)\\), determine which four nearest pixels contribute to the interpolation. These pixels form a rectangle around \\((x_i, y_i)\\).\n\n3. **Calculate Weights**: Compute the weights for each contributing pixel based on their relative distances from \\((x_i, y_i)\\).\n\n4. **Interpolate Values**: Use the weights to calculate the interpolated value for each channel at \\((x_i, y_i)\\).\n\n5. **Round Results**: Round the interpolated values to 4 decimal places and store them in a list of lists.\n\nHere's the implementation of the `bilinear_interpolate` function:\n\n```python\nimport numpy as np\n\ndef bilinear_interpolate(X: np.ndarray, x: list[float], y: list[float]) -> list[list[float]]:\n    \"\"\"Estimate pixel values at arbitrary (x, y) positions using bilinear interpolation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Image array of shape (rows, cols, channels).\n    x : list[float]\n        Horizontal (column) coordinates.\n    y : list[float]\n        Vertical (row) coordinates.\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated values where each inner list corresponds to one pixel and\n        contains *channels* numbers rounded to 4 decimal places.  If the input\n        lists `x` and `y` differ in length the function must return an empty\n        list.\n    \"\"\"\n    if len(x) != len(y):\n        return []\n\n    rows, cols, channels = X.shape\n    results = []\n\n    for xi, yi in zip(x, y):\n        # Clip coordinates to ensure they are within the image boundaries\n        cx = max(0, min(cols - 1, int(xi)))\n        cy = max(0, min(rows - 1, int(yi)))\n\n        # Calculate fractional parts\n        fx = xi - cx\n        fy = yi - cy\n\n        # Determine neighboring pixels\n        p11 = X[cy, cx]\n        p12 = X[cy, cx + 1]\n        p21 = X[cy + 1, cx]\n        p22 = X[cy + 1, cx + 1]\n\n        # Calculate interpolation coefficients\n        w1 = (1 - fx) * (1 - fy)\n        w2 = fx * (1 - fy)\n        w3 = (1 - fx) * fy\n        w4 = fx * fy\n\n        # Interpolate each channel\n        interpolated_values = []\n        for ch in range(channels):\n            interpolated_value = (\n                w1 * p11[ch] +\n                w2 * p12[ch] +\n                w3 * p21[ch] +\n                w4 * p22[ch]\n            )\n            # Round to 4 decimal places\n            interpolated_values.append(round(interpolated_value, 4))\n\n        results.append(interpolated_values)\n\n    return results\n\n# Example usage:\n# X = np.random.rand(10, 10, 3)  # Example image\n# x = [0.5, 2.5, 7.5]\n# y = [0.5, 5.5, 9.5]\n# print(bilinear_interpolate(X, x, y))\n```\n\n### Explanation:\n- **Clipping**: The coordinates are clipped to ensure they fall within the valid range of the image dimensions.\n- **Fractional Parts**: The fractional parts of the coordinates are calculated to determine how much of each neighboring pixel contributes to the final value.\n- **Weights**: The weights are computed based on the fractional parts, ensuring that the contributions of the four neighboring pixels are correctly weighted.\n- **Interpolation**: Each channel is interpolated separately, and the results are rounded to 4 decimal places before being added to the final list of results."}
{"task_id": 326, "completion_id": 0, "completion": "To implement the `svm_predict` function, we need to compute the decision function for each query sample \\( x \\) using the given support vectors, their labels, Lagrange multipliers, intercept, and kernel parameters. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef svm_predict(\n    X: list[list[float]],\n    support_vectors: list[list[float]],\n    lagr_multipliers: list[float],\n    support_vector_labels: list[int],\n    intercept: float,\n    kernel: str = \"rbf\",\n    power: int = 3,\n    gamma: float | None = None,\n    coef: float = 1.0,\n) -> list[int]:\n    \"\"\"Predict labels for a batch of samples using a pre-trained SVM.\n\n    The decision value for a sample *x* is\n        f(x) = \u03a3 \u03b1_i y_i K(x_i, x) + b\n    where the summation runs over the support vectors.\n\n    Args:\n        X: Query samples. Shape (n_query, n_features).\n        support_vectors: Support vectors obtained during training.\n        lagr_multipliers: Lagrange multipliers \u03b1_i corresponding to the support vectors.\n        support_vector_labels: Class labels y_i (\u22121 or 1) for support vectors.\n        intercept: Bias term *b*.\n        kernel: One of {\"linear\", \"polynomial\", \"rbf\"}.\n        power: Degree of the polynomial kernel.\n        gamma: Kernel parameter. If None, defaults to 1 / n_features.\n        coef: Independent term in the polynomial kernel.\n\n    Returns:\n        A list with the predicted labels (\u22121 or 1) for every sample in *X*.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X = np.array(X)\n    support_vectors = np.array(support_vectors)\n    lagr_multipliers = np.array(lagr_multipliers)\n    support_vector_labels = np.array(support_vector_labels)\n\n    # Calculate the number of query samples and support vectors\n    n_query, _ = X.shape\n    m = len(support_vectors)\n\n    # Initialize the result list\n    predictions = []\n\n    # Iterate over each query sample\n    for x in X:\n        # Compute the dot product between x and each support vector\n        dot_products = np.dot(x[:, np.newaxis], support_vectors.T)\n\n        # Apply the chosen kernel\n        if kernel == \"linear\":\n            K_values = dot_products\n        elif kernel == \"polynomial\":\n            K_values = (gamma * dot_products + coef) ** power\n        elif kernel == \"rbf\":\n            squared_distances = np.sum((x - support_vectors) ** 2, axis=1)\n            K_values = np.exp(-gamma * squared_distances)\n        else:\n            raise ValueError(\"Unsupported kernel type\")\n\n        # Compute the decision value for the current query sample\n        decision_value = np.dot(lagr_multipliers, support_vector_labels * K_values) + intercept\n\n        # Determine the predicted label based on the decision value\n        if decision_value > 0:\n            prediction = 1\n        else:\n            prediction = -1\n\n        # Append the predicted label to the results list\n        predictions.append(prediction)\n\n    return predictions\n\n# Example usage:\n# X = [[1, 2], [3, 4]]\n# support_vectors = [[1, 1], [2, 2]]\n# lagr_multipliers = [1, 1]\n# support_vector_labels = [-1, 1]\n# intercept = 0\n# print(svm_predict(X, support_vectors, lagr_multipliers, support_vector_labels, intercept))\n```\n\n### Explanation:\n- **Input Conversion**: The input data is converted to NumPy arrays for efficient computation.\n- **Kernel Application**: Depending on the specified kernel (`\"linear\"`, `\"polynomial\"`, or `\"rbf\"`), the appropriate kernel function is applied.\n- **Decision Value Calculation**: For each query sample, the decision value is computed using the formula provided.\n- **Prediction Determination**: The predicted label is determined based on whether the decision value is positive or negative.\n- **Result Compilation**: The predicted labels are collected in a list and returned.\n\nThis implementation handles the decision function calculation for the three most common SVM kernels and adheres to the specified output constraints."}
{"task_id": 328, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n\n    delta_f = fs / N\n    frequencies = [i * delta_f for i in range(N)]\n\n    if positive_only:\n        return [round(f, 4) for f in frequencies[:N // 2 + 1]]\n    else:\n        return [round(f, 4) for f in frequencies]\n\n# Example usage:\nprint(dft_bins(10))  # Example output for N=10, fs=44000, positive_only=True\nprint(dft_bins(10, positive_only=False))\n```\n\nThis function calculates the center frequencies for each DFT bin based on the given parameters and returns them rounded to four decimal places. The function handles both the case where `positive_only` is `True` and `False`, returning the appropriate subset of frequencies. If either `N` or `fs` is not positive, it returns an empty list."}
{"task_id": 330, "completion_id": 0, "completion": "To solve the problem of updating kernel parameters and hyperparameters based on a summary dictionary, we need to follow these steps:\n\n1. **Flatten the Summary Dictionary**: Merge the nested `\"parameters\"` and `\"hyperparameters\"` sub-dictionaries into the top level of the `summary_dict`.\n\n2. **Create New Dictionaries**: Create new dictionaries for `new_parameters` and `new_hyperparameters` to avoid modifying the original dictionaries.\n\n3. **Apply Updates**: Overwrite the corresponding entries in the new dictionaries if there are updates provided in the flattened summary dictionary.\n\n4. **Return the Updated Dictionaries**: Return the newly created dictionaries.\n\nHere's how you can implement the `update_kernel_settings` function:\n\n```python\ndef update_kernel_settings(parameters: dict, hyperparameters: dict, summary_dict: dict) -> tuple[dict, dict]:\n    \"\"\"Update kernel parameters and hyperparameters.\n\n    The function must not modify the input dictionaries.  It should return two\n    *new* dictionaries in which the supplied `summary_dict` updates have been\n    applied following the rules explained in the task description.\n\n    Args:\n        parameters (dict):      Existing kernel parameters.\n        hyperparameters (dict): Existing kernel hyperparameters.\n        summary_dict (dict):    Dictionary that may contain the keys\n            \"parameters\" and/or \"hyperparameters\" whose values are dictionaries\n            of updates.  Any top-level keys are also treated as updates.\n\n    Returns:\n        tuple[dict, dict]: `(new_parameters, new_hyperparameters)` \u2013 the\n        updated dictionaries.\n    \"\"\"\n    # Step 1: Flatten the summary dictionary\n    flattened_summary = {}\n    \n    # Check if 'parameters' or 'hyperparameters' exist in the summary dictionary\n    if 'parameters' in summary_dict:\n        flattened_summary.update(summary_dict['parameters'])\n    if 'hyperparameters' in summary_dict:\n        flattened_summary.update(summary_dict['hyperparameters'])\n    \n    # Remove the 'parameters' and 'hyperparameters' keys from the summary dictionary\n    if 'parameters' in summary_dict:\n        del summary_dict['parameters']\n    if 'hyperparameters' in summary_dict:\n        del summary_dict['hyperparameters']\n    \n    # Step 2: Create new dictionaries\n    new_parameters = parameters.copy()\n    new_hyperparameters = hyperparameters.copy()\n    \n    # Step 3: Apply updates\n    for key, value in flattened_summary.items():\n        if key in new_parameters:\n            new_parameters[key] = value\n        elif key in new_hyperparameters:\n            new_hyperparameters[key] = value\n    \n    # Step 4: Return the updated dictionaries\n    return new_parameters, new_hyperparameters\n\n# Example usage:\nparameters = {\"sigma\": 1, \"gamma\": 0.5}\nhyperparameters = {\"id\": \"RBF\", \"trainable\": True}\nsummary_dict = {\n    \"parameters\":      {\"sigma\": 2},\n    \"hyperparameters\": {\"trainable\": False},\n    \"extra\": 123                      # <- ignored\n}\n\nupdated_params, updated_hps = update_kernel_settings(parameters, hyperparameters, summary_dict)\nprint(updated_params)  # Output: {'sigma': 2, 'gamma': 0.5}\nprint(updated_hps)   # Output: {'id': 'RBF', 'trainable': False}\n```\n\n### Explanation:\n- **Flattening**: We first check if `\"parameters\"` or `\"hyperparameters\"` exist in the `summary_dict`. If they do, we merge their contents into a single dictionary called `flattened_summary`.\n- **Removing Keys**: After merging, we remove the `\"parameters\"` and `\"hyperparameters\"` keys from `summary_dict` since they are no longer needed.\n- **Creating New Dictionaries**: We copy the original `parameters` and `hyperparameters` into `new_parameters` and `new_hyperparameters` to ensure they are not modified.\n- **Applying Updates**: We iterate over the `flattened_summary` and update the corresponding entries in `new_parameters` and `new_hyperparameters`.\n- **Returning Results**: Finally, we return the updated dictionaries."}
{"task_id": 331, "completion_id": 0, "completion": "To implement the `oob_mse` function, we need to follow these steps:\n\n1. Ensure that the inputs are in the correct format (either lists or NumPy arrays).\n2. Convert the lists to NumPy arrays if necessary.\n3. Iterate over each sample to check if it has at least one out-of-bag prediction.\n4. If a sample has at least one out-of-bag prediction, calculate the average of its predictions and compute the squared error with the true value.\n5. Sum up all the squared errors and divide by the number of samples with at least one out-of-bag prediction.\n6. Return the result rounded to four decimal places. If no sample has an out-of-bag prediction, return -1.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    y_true = np.array(y_true)\n    predictions = np.array(predictions)\n    oob_mask = np.array(oob_mask)\n\n    # Check if the shapes match\n    if y_true.shape != predictions.shape or predictions.shape != oob_mask.shape:\n        raise ValueError(\"Input shapes do not match\")\n\n    # Initialize variables\n    total_squared_error = 0.0\n    num_samples_with_oob_predictions = 0\n\n    # Iterate over each sample\n    for i in range(y_true.shape[0]):\n        # Get the indices of out-of-bag predictions for the current sample\n        oob_indices = np.where(oob_mask[i])[0]\n        \n        # Check if there are any out-of-bag predictions\n        if len(oob_indices) > 0:\n            # Calculate the average of the out-of-bag predictions\n            oob_prediction_mean = np.mean(predictions[i][oob_indices])\n            \n            # Compute the squared error\n            squared_error = (oob_prediction_mean - y_true[i]) ** 2\n            \n            # Add to total squared error and count the sample\n            total_squared_error += squared_error\n            num_samples_with_oob_predictions += 1\n\n    # Calculate the OOB mean squared error\n    if num_samples_with_oob_predictions == 0:\n        return -1.0\n    else:\n        oob_mse = total_squared_error / num_samples_with_oob_predictions\n        return round(oob_mse, 4)\n\n# Example usage:\n# y_true = [1, 2, 3, 4, 5]\n# predictions = [[0.9, 1.1, 2.8, 3.7, 4.6],\n#                [0.8, 1.2, 2.7, 3.8, 4.5],\n#                [0.7, 1.3, 2.6, 3.9, 4.4],\n#                [0.6, 1.4, 2.5, 4.0, 4.3],\n#                [0.5, 1.5, 2.4, 4.1, 4.2]]\n# oob_mask = [[True, False, True, False, True],\n#              [True, True, False, False, True],\n#              [False, True, True, False, True],\n#              [False, False, True, True, True],\n#              [True, False, False, True, False]]\n\n# print(oob_mse(y_true, predictions, oob_mask))\n```\n\nThis function will correctly compute the out-of-bag mean squared error for a given set of predictions and their corresponding out-of-bag masks."}
{"task_id": 332, "completion_id": 0, "completion": "To solve the problem of constructing an FP-Tree without using any classes, we can follow these steps:\n\n1. **Compute Item Support**: Calculate the frequency of each item across all transactions.\n2. **Filter Items Based on Min Support**: Remove items that do not meet the minimum support threshold.\n3. **Sort Items**: Sort the remaining items first by decreasing support and then alphabetically to handle ties.\n4. **Build the FP-Tree**: Traverse each transaction, filter out non-frequent items, and update the tree accordingly.\n\nHere's the implementation of the `build_fp_tree` function:\n\n```python\nfrom collections import defaultdict\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Compute item support\n    item_counts = Counter()\n    for transaction in transactions:\n        item_counts.update(transaction)\n    \n    # Step 2: Filter items based on min support\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    \n    # Step 3: Sort items by decreasing support, then alphabetically\n    sorted_frequent_items = sorted(frequent_items.items(), key=lambda x: (-x[1], x[0]))\n    \n    # Step 4: Build the FP-Tree\n    def create_fp_tree(data):\n        root = {\"support\": 0, \"children\": {}}\n        \n        for transaction in data:\n            current_node = root\n            for item in sorted_frequent_items:\n                while item[0] in current_node[\"children\"]:\n                    current_node = current_node[\"children\"][item[0]]\n                if item[0] not in current_node[\"children\"]:\n                    new_node = {\"support\": 0, \"children\": {}}\n                    current_node[\"children\"][item[0]] = new_node\n                current_node = current_node[\"children\"][item[0]]\n                current_node[\"support\"] += 1\n        \n        return root\n    \n    return create_fp_tree(transactions)\n\n# Example usage:\ntransactions = [\n    [\"apple\", \"banana\", \"orange\"],\n    [\"banana\", \"orange\", \"grape\"],\n    [\"apple\", \"orange\", \"grape\"],\n    [\"apple\", \"banana\"],\n    [\"banana\"]\n]\n\nmin_support = 2\nfp_tree = build_fp_tree(transactions, min_support)\nprint(fp_tree)\n```\n\n### Explanation:\n\n- **Item Counting**: We use `Counter` from the `collections` module to count the occurrences of each item in all transactions.\n- **Filtering**: We retain only those items that have a support count greater than or equal to `min_support`.\n- **Sorting**: The remaining items are sorted primarily by their support in descending order and secondarily by their lexicographical order.\n- **Tree Construction**: We traverse each transaction, starting from the root. For each item in the sorted list, we navigate through the tree, creating nodes as necessary and updating their support counts.\n\nThis approach ensures that the FP-Tree is constructed efficiently without using any additional classes."}
{"task_id": 333, "completion_id": 0, "completion": "To implement a logistic regression classifier from scratch using batch gradient descent, we need to follow these steps:\n\n1. **Standardize the Features**: Subtract the mean and divide by the standard deviation for each feature. Add a bias term of ones to the augmented data.\n\n2. **Initialize Parameters**: Initialize the weights and bias with zeros.\n\n3. **Compute the Hypothesis**: For each example, calculate the hypothesis value using the sigmoid function.\n\n4. **Compute the Cost Function**: Calculate the cost function using the logistic loss.\n\n5. **Update Parameters**: Update the parameters using the gradient descent update rule.\n\n6. **Predict Class Labels**: After training, use the learned parameters to predict class labels based on the threshold of 0.5.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef logistic_regression(X: np.ndarray, y: np.ndarray, alpha: float = 0.1, num_iter: int = 10000) -> list[int]:\n    \"\"\"Train a logistic regression classifier on the provided dataset and\n    return the predicted labels for *the same* dataset.\n\n    Args:\n        X: 2-D NumPy array of shape (m, n) containing the training examples.\n        y: 1-D NumPy array of length m with binary labels (0 or 1).\n        alpha: Learning rate for gradient descent. Defaults to 0.1.\n        num_iter: Number of iterations for gradient descent. Defaults to 10000.\n\n    Returns:\n        A Python list of length *m* holding the predicted class labels (0/1)\n        produced by the trained classifier.\n    \"\"\"\n    # Step 1: Standardize the features\n    mu = X.mean(axis=0)\n    sigma = X.std(axis=0)\n    X_std = (X - mu) / (sigma + 1e-8)\n    \n    # Step 2: Augment the data with a bias term\n    X_augmented = np.hstack((np.ones((X.shape[0], 1)), X_std))\n    \n    # Step 3: Initialize parameters\n    theta = np.zeros(X_augmented.shape[1])\n    \n    # Step 4: Train the model using batch gradient descent\n    for _ in range(num_iter):\n        h_theta = sigmoid(X_augmented @ theta)\n        grad = (1 / X.shape[0]) * X_augmented.T @ (h_theta - y)\n        theta -= alpha * grad\n    \n    # Step 5: Predict class labels\n    predictions = sigmoid(X_augmented @ theta) >= 0.5\n    predicted_labels = predictions.astype(int)\n    \n    return predicted_labels.tolist()\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 0, 1, 1])\n    \n    # Train the logistic regression model\n    predicted_labels = logistic_regression(X, y)\n    print(predicted_labels)\n```\n\n### Explanation:\n\n- **Standardization**: We subtract the mean and divide by the standard deviation for each feature. This helps in scaling the features to have zero mean and unit variance, which can improve convergence of gradient descent.\n  \n- **Augmentation**: We add a bias term of ones to the input features to handle the intercept in the logistic regression model.\n\n- **Initialization**: The weights (`theta`) are initialized to zeros. The bias is implicitly handled by the augmented feature vector.\n\n- **Gradient Descent**: We update the weights using the gradient of the cost function with respect to the weights. The learning rate (`alpha`) controls how much we adjust the weights during each iteration.\n\n- **Prediction**: After training, we compute the probability scores for each example using the sigmoid function and convert these scores to binary labels using a threshold of 0.5.\n\nThis implementation should correctly classify the given test cases if they are linearly separable."}
{"task_id": 334, "completion_id": 0, "completion": "```python\nimport math\n\ndef noam_learning_rate(step: int,\n                       model_dim: int = 512,\n                       scale_factor: float = 1.0,\n                       warmup_steps: int = 4000) -> float:\n    \"\"\"Return the learning-rate given by the Noam schedule.\n\n    The function must follow the formula described in the task description and\n    round the result to 9 decimal places.\n    \"\"\"\n    if step < 1:\n        return 0.0\n    \n    factor = scale_factor * (model_dim ** -0.5)\n    linear_term = step ** -0.5\n    decay_term = step * warmup_steps ** -1.5\n    \n    lr = factor * min(linear_term, decay_term)\n    \n    return round(lr, 9)\n```"}
{"task_id": 335, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_initializer(name: str):\n    \"\"\"Return a weight-initialisation callable associated with *name*.\n\n    The callable must have the signature\n        initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray\n    and create a numpy array filled according to the requested strategy.\n\n    Supported strategies (case-insensitive):\n        - \"zeros\"\n        - \"ones\"\n        - \"normal\"\n        - \"glorot_uniform\"\n\n    If *name* is not recognised the function must raise `ValueError`.\n    \"\"\"\n    if name.lower() == \"zeros\":\n        return lambda shape, scale=1.0: np.zeros(shape)\n    elif name.lower() == \"ones\":\n        return lambda shape, scale=1.0: np.ones(shape) * scale\n    elif name.lower() == \"normal\":\n        return lambda shape, scale=1.0: np.random.normal(loc=0, scale=scale, size=shape)\n    elif name.lower() == \"glorot_uniform\":\n        def glorot_uniform_initializer(shape, scale=1.0):\n            fan_in = shape[0]\n            fan_out = shape[1]\n            if len(shape) == 1:\n                raise ValueError(\"Glorot uniform initialization is only supported for two-dimensional shapes.\")\n            limit = scale * np.sqrt(6 / (fan_in + fan_out))\n            return np.random.uniform(low=-limit, high=limit, size=shape)\n        return glorot_uniform_initializer\n    else:\n        raise ValueError(f\"Unsupported initialisation strategy: {name}\")\n\n# Example usage:\ninitializer = get_initializer(\"glorot_uniform\")\nprint(initializer((2, 3), scale=0.5))  # Should print a 2x3 matrix initialized using Glorot uniform distribution\n```\n\nThis code defines a function `get_initializer` that returns a weight initialization function based on the specified strategy. It supports four different strategies: \"zeros\", \"ones\", \"normal\", and \"glorot_uniform\". Each strategy has a corresponding implementation within the function. The function raises a `ValueError` if an unsupported strategy is provided."}
{"task_id": 336, "completion_id": 0, "completion": "To solve this problem, we need to implement the `linear_kernel` function according to the specified requirements. The function should handle both Python lists and NumPy arrays for input vectors, compute the linear kernel matrix, and ensure that the output is rounded to four decimal places. Additionally, it should validate the input dimensions and return -1 if they are incompatible.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    if isinstance(X, list):\n        X = np.array(X)\n    if isinstance(Y, list):\n        Y = np.array(Y)\n    \n    # Get the shapes of X and Y\n    Nx, Cx = X.shape\n    Ny, Cy = Y.shape if Y is not None else (Nx, Cx)\n    \n    # Validate the number of columns\n    if Cx != Cy:\n        return -1\n    \n    # Compute the kernel matrix\n    K = np.dot(X, Y.T) + c0\n    \n    # Round the kernel matrix to 4 decimal places\n    K_rounded = np.round(K, 4)\n    \n    # Convert the result to a list of lists\n    K_list = K_rounded.tolist()\n    \n    return K_list\n\n# Example usage:\nX = [[1, 2], [3, 4]]\nY = [[5, 6], [7, 8]]\nprint(linear_kernel(X, Y))\n```\n\n### Explanation:\n\n1. **Input Handling**: \n   - We first check if the inputs are lists and convert them to NumPy arrays using `np.array()`. This ensures compatibility with both data types.\n\n2. **Shape Validation**:\n   - We retrieve the shapes of `X` and `Y`. If `Y` is `None`, we set it to match the shape of `X`.\n   - We then check if the number of columns (`Cx`) in `X` matches the number of columns (`Cy`) in `Y`. If they do not match, we return `-1`.\n\n3. **Kernel Matrix Calculation**:\n   - We use `np.dot()` to compute the dot product of `X` and the transpose of `Y`, which gives us the pairwise dot products.\n   - We add the constant `c0` to each element of the resulting matrix.\n\n4. **Rounding and Conversion**:\n   - We round the kernel matrix to four decimal places using `np.round()`.\n   - Finally, we convert the NumPy array to a list of lists using `tolist()`.\n\nThis function handles all specified requirements and edge cases, such as empty input matrices and mismatched column counts."}
{"task_id": 337, "completion_id": 0, "completion": "To implement the Nearest-Neighbour interpolation rule on a 2-D grid, we need to follow these steps:\n\n1. Round the input coordinates `(x_i, y_i)` to the nearest integer using `np.around`.\n2. Ensure the rounded coordinates lie within the valid range of the image dimensions.\n3. Fetch the pixel value at the rounded coordinates for all channels.\n4. Return the collected pixel values as a Python list.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef nn_interpolate_2D(X: 'np.ndarray', x: list[float|int], y: list[float|int]) -> list:\n    \"\"\"Nearest-Neighbour interpolation on a 2-D regular grid.\n\n    Args:\n        X: Image of shape (rows, cols, channels).\n        x: List of *x* (column) coordinates.\n        y: List of *y* (row) coordinates.\n\n    Returns:\n        Python list with shape (k, channels) containing the interpolated\n        pixel values.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they aren't already\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Initialize an empty list to store the results\n    results = []\n    \n    # Iterate over each pair of coordinates\n    for xi, yi in zip(x, y):\n        # Round the coordinates to the nearest integer\n        x_rounded = np.around(xi).astype(int)\n        y_rounded = np.around(yi).astype(int)\n        \n        # Ensure the coordinates are within the bounds of the image\n        x_rounded = np.clip(x_rounded, 0, X.shape[1] - 1)\n        y_rounded = np.clip(y_rounded, 0, X.shape[0] - 1)\n        \n        # Fetch the pixel value at the rounded coordinates for all channels\n        pixel_value = X[y_rounded, x_rounded]\n        \n        # Append the pixel value to the results list\n        results.append(pixel_value.tolist())\n    \n    # Return the list of results\n    return results\n\n# Example usage:\nX = np.random.rand(10, 10, 3)  # Example 10x10 image with 3 channels\nx_coords = [2.7, 3.8, 4.1]\ny_coords = [5.2, 6.3, 7.9]\n\nresult = nn_interpolate_2D(X, x_coords, y_coords)\nprint(result)\n```\n\n### Explanation:\n- **Input Conversion**: The input lists `x` and `y` are converted to NumPy arrays to facilitate vectorized operations.\n- **Rounding and Clipping**: Each coordinate is rounded using `np.around`, and then clipped to ensure it falls within the valid range of the image dimensions.\n- **Pixel Fetching**: The pixel value at the rounded coordinates is fetched using indexing.\n- **Result Collection**: The pixel value for each coordinate is appended to the results list, which is returned as a Python list after conversion to a list using `.tolist()`."}
{"task_id": 338, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(X, y, reg_factor):\n    \"\"\"\n    This function computes the closed-form solution for Ridge Regression,\n    which includes regularization of the bias term.\n\n    Parameters:\n    - X: A 2D NumPy array or a list of lists representing the feature matrix.\n    - y: A 1D NumPy array or list representing the target vector.\n    - reg_factor: A float representing the regularization factor (\u03b1 > 0).\n\n    Returns:\n    - A list of rounded coefficients (including the bias term).\n    \"\"\"\n\n    # Convert inputs to NumPy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n\n    # Add a column of ones to X for the bias term\n    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n\n    # Compute the weights using the closed-form solution\n    n_samples, n_features = X_with_bias.shape\n    I = np.eye(n_features + 1)\n    w = np.linalg.inv(X_with_bias.T @ X_with_bias + reg_factor * I) @ X_with_bias.T @ y\n\n    # Round each coefficient to 4 decimal places\n    rounded_weights = [round(w[i], 4) for i in range(n_features + 1)]\n\n    return rounded_weights\n```\n\nThis function first converts the input matrices and vectors to NumPy arrays if they are not already. It then adds a column of ones to the feature matrix \\(X\\) to account for the bias term. The closed-form solution for Ridge Regression is computed using the formula provided, and the resulting weights are rounded to four decimal places before being returned."}
{"task_id": 339, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef most_common_label(y: list) -> object:\n    \"\"\"Return the most frequent label in *y*.\n\n    If *y* is empty, the function returns ``None``.\n    In case of a tie, the label whose first appearance comes earliest\n    in *y* is returned.\n\n    Args:\n        y: A list containing hashable elements (e.g., strings, numbers).\n\n    Returns:\n        The label that occurs most frequently in *y*, or ``None`` if *y* is\n        empty.\n    \"\"\"\n    if not y:\n        return None\n    \n    # Count the frequency of each label\n    label_counts = Counter(y)\n    \n    # Find the maximum frequency\n    max_frequency = max(label_counts.values())\n    \n    # Filter labels that have the maximum frequency\n    most_frequent_labels = [label for label, count in label_counts.items() if count == max_frequency]\n    \n    # Return the first label from the list of most frequent labels\n    return most_frequent_labels[0]\n\n# Example usage:\nprint(most_common_label(['apple', 'banana', 'apple', 'orange', 'banana', 'banana']))  # Output: 'banana'\nprint(most_common_label([1, 2, 2, 3, 4, 4, 4]))  # Output: 4\nprint(most_common_label([]))  # Output: None\n```\n\nThis solution uses the `Counter` class from the `collections` module to count the occurrences of each label in the iterable `y`. It then determines the maximum frequency and identifies all labels that have this frequency. Finally, it returns the first label from this list, which ensures that the label appearing earliest in the original iterable is selected in case of ties."}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n    \n    # Update first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n    \n    # Update second moment estimate\n    v_t = beta2 * v + (1 - beta2) * grad**2\n    \n    # Calculate bias-corrected first moment estimate\n    m_hat_t = m_t / (1 - beta1**t)\n    \n    # Calculate bias-corrected second moment estimate\n    v_hat_t = v_t / (1 - beta2**t)\n    \n    # Compute the updated parameter\n    param_prime = param - lr * m_hat_t / (np.sqrt(v_hat_t) + eps)\n    \n    # Convert the results to lists rounded to 8 decimal places\n    updated_param_list = [round(x, 8) for x in param_prime]\n    new_m_list = [round(x, 8) for x in m_t]\n    new_v_list = [round(x, 8) for x in v_t]\n    \n    return updated_param_list, new_m_list, new_v_list\n```\n\nThis implementation follows the standard Adam optimization algorithm, updating both the parameter and the moment estimates accordingly. The gradients are used to compute the new moments, which are then used to update the parameter using the Adam update formula. The results are returned as lists rounded to eight decimal places."}
{"task_id": 342, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n    \"\"\"Compute the Manhattan (L1) distance between two real vectors.\n\n    Args:\n        x: List of numbers representing the first vector.\n        y: List of numbers representing the second vector.\n\n    Returns:\n        The L1 distance rounded to 4 decimal places as a float. If the\n        vectors have different lengths, returns -1.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    # Calculate the Manhattan distance using NumPy for efficiency\n    distance = np.sum(np.abs(np.array(x) - np.array(y)))\n    \n    # Round the result to 4 decimal places\n    return round(distance, 4)\n```"}
{"task_id": 343, "completion_id": 0, "completion": "To implement the `cross_entropy_update` function according to the specifications, we need to follow these steps:\n\n1. Sort the `theta_samples` based on their corresponding `rewards`.\n2. Determine how many samples to retain based on `retain_prcnt`.\n3. Compute the mean and variance of the retained samples.\n4. Round the results to four decimal places before returning them.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to a numpy array if it's a list\n    rewards = np.array(rewards)\n    \n    # Combine theta_samples and rewards into a single array\n    combined = np.hstack((theta_samples, rewards[:, None]))\n    \n    # Sort the combined array based on rewards in descending order\n    sorted_combined = combined[combined[:, -1].argsort()[::-1]]\n    \n    # Calculate the number of samples to retain\n    n_to_retain = int(retain_prcnt * len(sorted_combined))\n    \n    # Extract the retained samples\n    retained_samples = sorted_combined[:n_to_retain, :-1]\n    \n    # Compute the mean of the retained samples\n    mean = retained_samples.mean(axis=0).round(4)\n    \n    # Compute the per-dimension variance of the retained samples\n    variances = retained_samples.var(axis=0).round(4)\n    \n    return mean.tolist(), variances.tolist()\n\n# Example usage:\n# theta_samples = np.random.rand(5, 3)\n# rewards = [0.9, 0.8, 0.7, 0.6, 0.5]\n# retain_prcnt = 0.8\n# print(cross_entropy_update(theta_samples, rewards, retain_prcnt))\n```\n\n### Explanation:\n1. **Combining Samples and Rewards**: We first combine the `theta_samples` and `rewards` into a single array for easier sorting.\n2. **Sorting**: We sort the combined array by the rewards in descending order to get the best samples at the top.\n3. **Retaining Samples**: We determine how many samples to retain using `retain_prcnt` and select the top `n_to_retain` samples.\n4. **Computing Mean and Variance**: We calculate the mean and variance of the retained samples along each dimension and round the results to four decimal places.\n5. **Returning Results**: Finally, we convert the means and variances to lists and return them."}
{"task_id": 344, "completion_id": 0, "completion": "To implement the Huffman encoding algorithm, we need to follow these steps:\n\n1. Count the frequency of each unique token.\n2. Build a priority queue (min-heap) based on these frequencies.\n3. Construct the Huffman tree by repeatedly merging the two least frequent nodes until only one node remains.\n4. Traverse the Huffman tree to generate the binary code for each token.\n\nHere's how you can implement this in Python:\n\n```python\nimport heapq\n\nclass Node:\n    def __init__(self, value=None, frequency=0):\n        self.value = value\n        self.frequency = frequency\n        self.left = None\n        self.right = None\n\n    def __lt__(self, other):\n        if self.frequency == other.frequency:\n            return self.value < other.value\n        return self.frequency < other.frequency\n\ndef huffman_encode(tokens: list[str]) -> dict[str, str]:\n    \"\"\"Generate a Huffman code for the supplied tokens.\n\n    The function must return a dictionary mapping **every** unique token in\n    *tokens* to its binary Huffman code.  All codes must be prefix-free and\n    consist only of the characters \u20180\u2019 and \u20181\u2019.\n    \"\"\"\n    if not tokens:\n        return {}\n\n    # Step 1: Count the frequency of each unique token\n    frequency_map = {}\n    for token in tokens:\n        if token in frequency_map:\n            frequency_map[token] += 1\n        else:\n            frequency_map[token] = 1\n\n    # Step 2: Build a priority queue (min-heap)\n    min_heap = []\n    for token, freq in frequency_map.items():\n        heapq.heappush(min_heap, Node(token, freq))\n\n    # Step 3: Construct the Huffman tree\n    while len(min_heap) > 1:\n        left = heapq.heappop(min_heap)\n        right = heapq.heappop(min_heap)\n        merged_node = Node(None, left.frequency + right.frequency)\n        merged_node.left = left\n        merged_node.right = right\n        heapq.heappush(min_heap, merged_node)\n\n    # Step 4: Traverse the Huffman tree to generate the binary code for each token\n    def traverse_tree(node, path=\"\"):\n        nonlocal result\n        if node is None:\n            return\n        if node.value is not None:\n            result[node.value] = path\n        traverse_tree(node.left, path + \"0\")\n        traverse_tree(node.right, path + \"1\")\n\n    result = {}\n    traverse_tree(min_heap[0])\n\n    return result\n\n# Example usage:\ntokens = [\"a\", \"b\", \"c\", \"d\"]\nhuffman_codes = huffman_encode(tokens)\nprint(huffman_codes)\n```\n\n### Explanation:\n\n- **Node Class**: Represents a node in the Huffman tree. It has a `value` (the token), a `frequency`, and pointers to `left` and `right` child nodes.\n  \n- **Heapq Usage**: We use a min-heap to keep track of nodes sorted by their frequency. This allows us to efficiently extract the two nodes with the lowest frequencies.\n\n- **Tree Construction**: We repeatedly merge the two least frequent nodes into a new parent node, updating the heap after each merge.\n\n- **Traversing the Tree**: We perform a depth-first traversal of the tree starting from the root. During the traversal, we build the binary code for each leaf node by appending '0' or '1' depending on whether we go left or right.\n\nThis implementation ensures that the resulting Huffman codes are prefix-free and consist only of '0' and '1'."}
{"task_id": 345, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        mu0: np.ndarray,\n        omega0: np.ndarray) -> list[float]:\n    \"\"\"Predict targets for X_test using Bayesian linear regression.\n\n    The function must add an intercept, compute the posterior mean of the\n    weight vector with a Normal prior (\u03bc\u2080, \u03a9\u2080), and return the predictions for\n    every sample in X_test rounded to four decimals.\n\n    Args:\n        X_train (np.ndarray): Training predictors without an intercept column.\n        y_train (np.ndarray): Training targets of shape (n_samples,).\n        X_test  (np.ndarray): Test predictors without an intercept column.\n        mu0     (np.ndarray): Prior mean vector (n_features + 1,).\n        omega0  (np.ndarray): Prior precision matrix\n                             (n_features + 1, n_features + 1).\n\n    Returns:\n        list[float]: Posterior-mean predictions for X_test.\n    \"\"\"\n    # Add an intercept column of ones to both training and test sets\n    X_train_with_intercept = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test_with_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # Compute the posterior mean of the weight vector\n    beta_posterior_mean = np.linalg.solve(X_train_with_intercept.T @ X_train_with_intercept + omega0,\n                                         X_train_with_intercept.T @ y_train + omega0 @ mu0)\n\n    # Compute the predictions for the test set using the posterior mean\n    predictions = np.dot(X_test_with_intercept, beta_posterior_mean)\n\n    # Round the predictions to four decimal places\n    rounded_predictions = [round(pred, 4) for pred in predictions]\n\n    return rounded_predictions\n```\n\nThis implementation adds an intercept to both the training and test datasets, computes the posterior mean of the weight vector using the provided prior mean (`mu0`) and precision matrix (`omega0`), and returns the predictions for the test set rounded to four decimal places."}
{"task_id": 346, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef entropy(y):\n    \"\"\"Compute the base-2 Shannon entropy of a vector of integer labels.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        One-dimensional iterable containing non-negative integer class labels.\n\n    Returns\n    -------\n    float\n        The entropy value rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Calculate the frequency of each label\n    unique_labels, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the probabilities of each label\n    probabilities = counts / len(y)\n    \n    # Calculate the Shannon entropy\n    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n    \n    # Round the result to 4 decimal places\n    return round(entropy_value, 4)\n```\n\nThis function first checks if the input `y` is a list and converts it to a NumPy array if necessary. It then calculates the frequency of each unique label using `np.unique` and computes the probabilities of these labels. Finally, it uses these probabilities to compute the Shannon entropy and rounds the result to four decimal places before returning it."}
{"task_id": 347, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef wgan_gp_loss(\n        Y_fake: np.ndarray,\n        module: str,\n        Y_real: np.ndarray | None = None,\n        gradInterp: np.ndarray | None = None,\n        lambda_: float = 10.0) -> float:\n    \"\"\"Compute the WGAN-GP loss for the critic or the generator.\n\n    Args:\n        Y_fake (np.ndarray): Critic outputs on fake samples (shape: (n_ex,)).\n        module (str): \"C\" for critic loss, \"G\" for generator loss.\n        Y_real (np.ndarray | None, optional): Critic outputs on real samples.\n            Required when *module* == \"C\". Defaults to None.\n        gradInterp (np.ndarray | None, optional): Gradients of the critic\n            outputs on the interpolated samples with respect to those samples.\n            Required when *module* == \"C\". Defaults to None.\n        lambda_ (float, optional): Gradient-penalty coefficient \u03bb. Defaults to\n            10.0.\n\n    Returns:\n        float: The requested loss rounded to four decimal places.\n    \"\"\"\n    if module not in [\"C\", \"G\"]:\n        raise ValueError(\"Invalid module specified. Must be 'C' or 'G'.\")\n    \n    if module == \"C\":\n        if Y_real is None or gradInterp is None:\n            raise ValueError(\"Both Y_real and gradInterp must be provided for the critic loss.\")\n        \n        D_x_real = np.mean(Y_real)\n        D_x_fake = np.mean(Y_fake)\n        D_x_interp_mean = np.mean(gradInterp)\n        \n        critic_loss = D_x_fake - D_x_real + lambda_ * (D_x_interp_mean - 1)**2\n        \n    elif module == \"G\":\n        if Y_real is not None:\n            raise ValueError(\"Y_real should not be provided for the generator loss.\")\n        \n        D_x_fake_mean = np.mean(Y_fake)\n        generator_loss = -D_x_fake_mean\n    \n    return round(critic_loss if module == \"C\" else generator_loss, 4)\n\n# Example usage:\n# Y_fake = np.array([0.5, 0.6, 0.7])\n# Y_real = np.array([0.2, 0.3, 0.4])\n# gradInterp = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n# print(wgan_gp_loss(Y_fake, \"C\", Y_real, gradInterp))  # Example critic loss calculation\n# print(wgan_gp_loss(Y_fake, \"G\"))  # Example generator loss calculation\n```\n\nThis implementation calculates the WGAN-GP loss for both the critic and the generator based on the given inputs. It checks for the validity of the inputs and raises appropriate errors if necessary. The critic loss includes the term involving the gradient penalty, while the generator loss simply negates the mean of the critic outputs on the fake samples."}
{"task_id": 348, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric_positive_definite(X: np.ndarray) -> bool:\n    \"\"\"Check whether a real matrix is symmetric and positive-definite.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A two-dimensional NumPy array representing the matrix to test.\n\n    Returns\n    -------\n    bool\n        True if *X* is symmetric and positive-definite, otherwise False.\n    \"\"\"\n    # First, check if X is a square matrix\n    if X.shape[0] != X.shape[1]:\n        return False\n    \n    # Check if the matrix is symmetric\n    if not np.allclose(X, X.T):\n        return False\n    \n    # Compute the eigenvalues of the matrix\n    eigenvalues = np.linalg.eigvalsh(X)\n    \n    # Check if all eigenvalues are greater than 1e-8\n    if np.any(eigenvalues <= 1e-8):\n        return False\n    \n    return True\n```"}
{"task_id": 350, "completion_id": 0, "completion": "To implement polynomial ridge regression from scratch, we need to follow these steps:\n\n1. Construct the design matrix \\( X \\) by expanding each feature \\( x_i \\) to powers up to \\( d \\).\n2. Compute the closed-form solution for the weight vector \\( \\mathbf{w} \\) using the formula:\n   \\[\n   \\mathbf{w} = (\\mathbf{X}^\\top \\cdot \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\cdot \\mathbf{X}^\\top \\cdot \\mathbf{y}\n   \\]\n   where \\( \\mathbf{X} \\) is the design matrix, \\( \\mathbf{y} \\) is the target vector, \\( \\mathbf{I} \\) is the identity matrix, and \\( \\lambda \\) is the regularization factor.\n3. Return the coefficients rounded to four decimal places.\n\nHere's the implementation in Python:\n\n```python\nimport numpy as np\n\ndef polynomial_ridge_regression(X: list[float | int],\n                                y: list[float | int],\n                                degree: int,\n                                reg_factor: float) -> list[float]:\n    \"\"\"Fit a polynomial ridge regression model and return its coefficients.\n\n    Parameters\n    ----------\n    X : list of float or int\n        One-dimensional feature vector.\n    y : list of float or int\n        Target values.\n    degree : int\n        Highest polynomial degree (>=0).\n    reg_factor : float\n        Non-negative ridge regularisation factor (\u03bb).\n\n    Returns\n    -------\n    list[float]\n        The (degree+1) learned coefficients, rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Create the design matrix X\n    n_samples = len(X)\n    X_poly = np.ones((n_samples, degree + 1))\n    for i in range(degree):\n        X_poly[:, i + 1] = X ** (i + 1)\n    \n    # Add regularization term to the diagonal of the identity matrix\n    I = np.eye(degree + 1)\n    I *= reg_factor\n    \n    # Calculate the weights using the closed-form solution\n    w = np.linalg.inv(X_poly.T @ X_poly + I) @ X_poly.T @ y\n    \n    # Round the coefficients to 4 decimal places\n    w_rounded = np.round(w, 4)\n    \n    return w_rounded.tolist()\n\n# Example usage:\nX = [1, 2, 3, 4]\ny = [2, 5, 7, 11]\ndegree = 2\nreg_factor = 0.1\ncoefficients = polynomial_ridge_regression(X, y, degree, reg_factor)\nprint(coefficients)\n```\n\n### Explanation:\n- **Design Matrix Construction**: We create a design matrix \\( \\mathbf{X} \\) where each row corresponds to a data point and each column corresponds to a power of the feature.\n- **Regularization**: We add a regularization term \\( \\lambda \\mathbf{I} \\) to the diagonal of the identity matrix \\( \\mathbf{I} \\).\n- **Weight Calculation**: We use NumPy's linear algebra functions to compute the inverse of the modified design matrix and solve for the weights.\n- **Rounding**: Finally, we round the resulting coefficients to four decimal places before returning them.\n\nThis implementation efficiently handles the polynomial expansion and regularization, providing the desired output format."}
{"task_id": 351, "completion_id": 0, "completion": "To implement the 2-D max-pooling operation using NumPy, we need to follow these steps:\n\n1. Calculate the output dimensions `out_h` and `out_w` based on the input dimensions `H`, `W`, `pool_shape`, and `stride`.\n2. Iterate over each image in the batch (`N`) and each channel (`C`).\n3. For each image and channel, apply the max-pooling operation within the specified window size (`p_h`, `p_w`) with the given stride.\n4. Store the results in a new 4-D NumPy array.\n5. Convert the resulting 4-D NumPy array into a nested Python list using `tolist()`.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef max_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: int) -> list:\n    \"\"\"Perform 2-D max pooling over a 4-D minibatch of images.\n\n    Args:\n        X: A 4-D NumPy array with shape (N, C, H, W).\n        pool_shape: Tuple (p_h, p_w) \u2013 height and width of the pooling window.\n        stride: The number of pixels the window moves vertically and horizontally.\n\n    Returns:\n        Nested Python list that represents the pooled tensor obtained via\n        the NumPy ``tolist()`` method. The returned list has shape\n        (N, C, out_h, out_w).\n    \"\"\"\n    # Extract the input parameters\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    \n    # Calculate the output dimensions\n    out_h = (H - p_h) // stride + 1\n    out_w = (W - p_w) // stride + 1\n    \n    # Initialize the result list\n    result = []\n    \n    # Perform max-pooling for each image and channel\n    for n in range(N):\n        for c in range(C):\n            # Create a temporary array to store the current image's channel data\n            channel_data = X[n, c]\n            \n            # Initialize the result array for the current channel\n            channel_result = np.zeros((out_h, out_w))\n            \n            # Apply max-pooling within the specified window\n            for i in range(0, H - p_h + 1, stride):\n                for j in range(0, W - p_w + 1, stride):\n                    # Find the maximum value in the current window\n                    window_max = np.max(channel_data[i:i+p_h, j:j+p_w])\n                    channel_result[i//stride, j//stride] = window_max\n            \n            # Append the result of the current channel to the result list\n            result.append(channel_result.tolist())\n    \n    # Return the final result as a nested Python list\n    return result\n\n# Example usage:\nX = np.random.rand(2, 3, 8, 8)  # Example input\npool_shape = (2, 2)\nstride = 2\npooled_output = max_pool2d(X, pool_shape, stride)\nprint(pooled_output)\n```\n\nThis implementation efficiently performs the 2-D max-pooling operation without modifying the input array and returns the result as a nested Python list."}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    \n    # Count the occurrences of each label\n    label_counts = Counter(labels)\n    \n    total_count = sum(label_counts.values())\n    \n    # Calculate the probability of each label\n    probabilities = {label: count / total_count for label, count in label_counts.items()}\n    \n    # Calculate the entropy\n    entropy_value = -sum(prob * math.log2(prob) for prob in probabilities.values() if prob > 0)\n    \n    # Round the result to 5 decimal places\n    return round(entropy_value, 5)\n\n# Example usage:\nprint(entropy([1, 2, 2, 3]))  # Output: 1.74118\nprint(entropy([1, 1, 1, 1]))  # Output: 0.0\nprint(entropy([]))             # Output: 0.0\n```\n\nThis function calculates the Shannon entropy of a given list of class labels. It first checks if the list is empty and returns 0.0 if so. Then, it uses `Counter` from the `collections` module to count the occurrences of each unique label. It computes the probability of each label and uses these probabilities to calculate the entropy using the formula provided. Finally, it rounds the result to five decimal places before returning it."}
{"task_id": 354, "completion_id": 0, "completion": "To implement the 2-D convolution using the `im2col` technique, we need to follow these steps:\n\n1. **Padding**: Determine the padding amount based on the given specification.\n2. **Im2Col Transformation**: Convert the input image data into column vectors for efficient matrix multiplication.\n3. **Matrix Multiplication**: Perform the matrix multiplication with the weight kernels.\n4. **Reshape and Pad Back**: Reshape the resulting matrix back to the original image dimensions and apply any necessary padding.\n\nHere's the implementation of the `conv2D` function:\n\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    \n    # Helper function to calculate padding based on the given specification\n    def calculate_padding(pad_spec, in_rows, in_cols):\n        if isinstance(pad_spec, int):\n            return pad_spec, pad_spec\n        elif isinstance(pad_spec, tuple):\n            if len(pad_spec) == 2:\n                return pad_spec[0], pad_spec[1]\n            elif len(pad_spec) == 4:\n                return pad_spec[0], pad_spec[1], pad_spec[2], pad_spec[3]\n        elif pad_spec == 'same':\n            padding_rows = max(0, (in_rows - 1) * stride + dilation * (W.shape[0] - 1) + 1 - in_rows)\n            padding_cols = max(0, (in_cols - 1) * stride + dilation * (W.shape[1] - 1) + 1 - in_cols)\n            return padding_rows // 2, padding_rows - padding_rows // 2, padding_cols // 2, padding_cols - padding_cols // 2\n        else:\n            raise ValueError(\"Invalid padding specification\")\n\n    # Calculate padding\n    if isinstance(pad, str):\n        padding = calculate_padding(pad, X.shape[1], X.shape[2])\n    else:\n        padding = calculate_padding(pad, X.shape[1], X.shape[2])\n\n    # Add padding to the input\n    X_padded = np.pad(X, ((0, 0), (padding[0], padding[1]), (padding[2], padding[3]), (0, 0)), mode='constant')\n\n    # Get the dimensions of the padded input\n    n_ex, in_rows, in_cols, in_ch = X_padded.shape\n\n    # Get the dimensions of the kernels\n    kernel_rows, kernel_cols, _, out_ch = W.shape\n\n    # Calculate the output dimensions\n    out_rows = (in_rows - kernel_rows + 2 * padding[0]) // stride + 1\n    out_cols = (in_cols - kernel_cols + 2 * padding[2]) // stride + 1\n\n    # Initialize the output array\n    Y = np.zeros((n_ex, out_rows, out_cols, out_ch))\n\n    # Loop over each example\n    for i in range(n_ex):\n        # Loop over each output channel\n        for c_out in range(out_ch):\n            # Loop over each output position\n            for r_out in range(out_rows):\n                for c_out in range(out_cols):\n                    # Extract the submatrix from the padded input\n                    submatrix = X_padded[i, \n                                        r_out * stride:(r_out * stride + kernel_rows),\n                                        c_out * stride:(c_out * stride + kernel_cols)]\n                    \n                    # Perform the dot product with the corresponding kernel\n                    Y[i, r_out, c_out, c_out] = np.sum(submatrix * W[:, :, :, c_out])\n\n    # Convert the result to a Python list\n    return Y.tolist()\n\n# Example usage:\nX = np.array([[[[1, 2],\n                 [3, 4]],\n                [[5, 6],\n                 [7, 8]]]])\nW = np.array([[[[1, 0],\n                 [0, 1]],\n                [[0, 1],\n                 [1, 0]]]])\nresult = conv2D(X, W, stride=1, pad=1)\nprint(result)\n```\n\n### Explanation:\n- **Padding Calculation**: The `calculate_padding` function handles different types of padding specifications.\n- **Im2Col Transformation**: We extract overlapping patches (submatrices) from the padded input using the specified stride and dilation.\n- **Matrix Multiplication**: Each patch is multiplied by the corresponding kernel to compute the convolution value.\n- **Reshaping and Padding Back**: The results are reshaped back to the original image dimensions after applying any necessary padding.\n\nThis implementation efficiently computes the convolution using the `im2col` approach, which is well-suited for large datasets and complex networks."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Step 1: Standardize the data by centering\n    centered_data = data - data.mean(axis=0)\n\n    # Step 2: Choose the appropriate solver based on the input\n    if solver == \"svd\":\n        U, S, Vt = np.linalg.svd(centered_data, full_matrices=False)\n    elif solver == \"eigen\":\n        cov_matrix = np.cov(centered_data, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    else:\n        raise ValueError(\"Invalid solver specified. Use 'svd' or 'eigen'.\")\n\n    # Step 3: Sort the eigenvectors by eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 4: Select the top n_components eigenvectors\n    selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n\n    # Step 5: Determine the sign of the eigenvectors to ensure determinism\n    signs = np.sign(selected_eigenvectors[abs(selected_eigenvectors).sum(axis=0) != 0])\n    selected_eigenvectors *= signs\n\n    # Step 6: Project the centered data onto the selected eigenvectors\n    scores = np.dot(centered_data, selected_eigenvectors)\n\n    # Step 7: Calculate the explained variance ratio\n    explained_variance = eigenvalues / eigenvalues.sum()\n    explained_variance_ratio = np.round(explained_variance[:n_components], 4).tolist()\n\n    # Convert scores to a list of lists and return\n    scores_list = [np.round(score, 4).tolist() for score in scores.T]\n    return scores_list, explained_variance_ratio\n```"}
{"task_id": 356, "completion_id": 0, "completion": "To complete the `leaf_predict` function, we need to handle both classification and regression cases based on the value of the `classifier` parameter. Here's how you can implement this:\n\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # If it's a classification tree, return the index of the maximum probability\n        return np.argmax(leaf.value)\n    else:\n        # If it's a regression tree, return the scalar value directly\n        return leaf.value\n\n# Example usage:\n# For classification:\n# leaf = Leaf([0.2, 0.5, 0.3])\n# print(leaf_predict(leaf, True))  # Output: 1\n\n# For regression:\n# leaf = Leaf(42.7)\n# print(leaf_predict(leaf, False))  # Output: 42\n```\n\n### Explanation:\n- **Classification Case**: We use `np.argmax(leaf.value)` to find the index of the largest probability in the list. This corresponds to the predicted class.\n- **Regression Case**: Since the leaf contains a single scalar value, we simply return this value as is.\n\nThis implementation ensures that the function behaves correctly according to whether the input tree is a classification or regression tree."}
{"task_id": 357, "completion_id": 0, "completion": "Here's a Python implementation of the K-Means clustering algorithm:\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    # Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n    \n    for _ in range(max_iters):\n        # Assign samples to the closest centroid\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=-1)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        new_centroids = np.zeros_like(centroids)\n        count = np.bincount(labels, minlength=K)\n        valid_count = count > 0\n        \n        new_centroids[valid_count] = np.mean(data[labels == valid_count], axis=0)\n        \n        # Check for convergence\n        if np.allclose(new_centroids, centroids, atol=1e-6):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids lexicographically and round to 4 decimal places\n    sorted_centroids = centroids[np.lexsort((centroids[:, 1], centroids[:, 0]))]\n    rounded_centroids = np.round(sorted_centroids, decimals=4).tolist()\n    \n    return rounded_centroids\n\n# Example usage\ndata = np.array([[1, 1],\n                 [1, 2],\n                 [2, 1],\n                 [8, 8],\n                 [9, 8],\n                 [8, 9]])\nK = 2\nresult = kmeans(data, K)\nprint(result)  # Output: [[1.3333, 1.3333], [8.3333, 8.3333]]\n```\n\n### Explanation:\n1. **Initialization**: The centroids are initialized with the first `K` samples from the dataset.\n2. **Assignment**: For each sample, calculate the Euclidean distance to each centroid and assign the sample to the closest centroid.\n3. **Update**: Compute the new centroid for each cluster as the mean of all samples assigned to it.\n4. **Convergence Check**: If no centroid changes more than the specified tolerance (`1e-6`), stop the algorithm.\n5. **Sorting and Rounding**: Sort the centroids lexicographically and round each coordinate to four decimal places before returning.\n\nThis implementation handles any number of dimensions greater than or equal to one."}
{"task_id": 358, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy(y_true, y_pred):\n    \"\"\"Compute the classification accuracy between two equal-length sequences.\n\n    The function should return the proportion of positions where `y_true` and\n    `y_pred` are exactly the same, rounded to four decimal places. If the two\n    sequences do not have the same length, return -1.\n\n    Args:\n        y_true (list | np.ndarray): Ground-truth class labels.\n        y_pred (list | np.ndarray): Predicted class labels.\n\n    Returns:\n        float | int: Accuracy rounded to 4 decimals, or -1 on length mismatch.\n    \"\"\"\n    # Check if both inputs are either lists or NumPy arrays\n    if isinstance(y_true, list) != isinstance(y_pred, list):\n        return -1\n    \n    # Convert lists to NumPy arrays if necessary\n    if isinstance(y_true, list):\n        y_true = np.array(y_true)\n    if isinstance(y_pred, list):\n        y_pred = np.array(y_pred)\n    \n    # Check if the lengths of the two arrays are equal\n    if len(y_true) != len(y_pred):\n        return -1\n    \n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    \n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n    \n    # Calculate accuracy\n    accuracy_value = correct_predictions / total_predictions\n    \n    # Round to four decimal places and return\n    return round(accuracy_value, 4)\n```"}
{"task_id": 359, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy(y, y_pred, derivative=False):\n    \"\"\"Cross-entropy utility (starter code).\n\n    Parameters\n    ----------\n    y : numpy.ndarray\n        One-hot encoded true labels, shape (n_samples, n_classes).\n    y_pred : numpy.ndarray\n        Predicted class probabilities, same shape as *y*.\n    derivative : bool, default False\n        If *True* return the gradient, else return the loss.\n\n    Returns\n    -------\n    float | list[list[float]]\n        Rounded loss (scalar) or gradient (nested list).\n    \"\"\"\n    # Add a small epsilon to prevent log(0)\n    eps = np.finfo(float).eps\n    y_pred += eps\n    \n    # Calculate the cross-entropy loss\n    if derivative:\n        # Return the gradient matrix\n        return [round(val, 4) for row in (y_pred - y) for val in row]\n    else:\n        # Calculate the sum over all samples and classes\n        loss = -np.sum(np.multiply(y, np.log(y_pred)))\n        return round(loss, 4)\n\n# Example usage:\n# y_true = np.array([[0, 1, 0], [1, 0, 0]])\n# y_pred = np.array([[0.7, 0.2, 0.1], [0.2, 0.8, 0.0]])\n# print(cross_entropy(y_true, y_pred))  # Should return the scalar loss\n# print(cross_entropy(y_true, y_pred, derivative=True))  # Should return the gradient matrix\n```\n\nThis implementation defines the `cross_entropy` function according to the specified requirements. It calculates the cross-entropy loss and can compute the gradient when requested. The results are rounded to four decimal places."}
{"task_id": 360, "completion_id": 0, "completion": "To implement the 1-D Discrete Cosine Transform of type II (DCT-II), we can follow these steps:\n\n1. Convert the input to a NumPy array if it's not already.\n2. Compute the DCT-II using the given formula.\n3. Apply orthonormal scaling if specified.\n4. Round the results to four decimal places and return them as a list.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef dct(frame: np.ndarray | list[float], orthonormal: bool = True) -> list[float]:\n    \"\"\"Compute the 1-D DCT-II of *frame*.\n\n    The function must:\n    1. Accept a one-dimensional NumPy array (or any array-like object)\n       containing *N* real samples.\n    2. Implement the DCT-II formula directly (no calls to external DCT helpers).\n    3. Support optional orthonormal scaling when *orthonormal* is ``True``.\n    4. Return the coefficients rounded to 4 decimal places as a Python list.\n\n    Args:\n        frame: Input signal of length *N*.\n        orthonormal: Whether to apply orthonormal scaling.\n\n    Returns:\n        A list with *N* DCT-II coefficients.\n    \"\"\"\n    # Convert the input to a NumPy array\n    x = np.array(frame)\n    \n    N = len(x)\n    \n    # Initialize the coefficients list\n    c = []\n    \n    # Compute the DCT-II coefficients\n    for k in range(N):\n        sum_value = 0.0\n        for n in range(N):\n            angle = np.pi * k * (2 * n + 1) / (2 * N)\n            sum_value += x[n] * np.cos(angle)\n        \n        # Apply orthonormal scaling\n        if orthonormal:\n            if k == 0:\n                c.append(2 * np.sqrt(1 / (4 * N)) * sum_value)\n            else:\n                c.append(2 * np.sqrt(1 / (2 * N)) * sum_value)\n        else:\n            c.append(sum_value)\n    \n    # Round the coefficients to 4 decimal places\n    c_rounded = [round(coeff, 4) for coeff in c]\n    \n    # Return the coefficients as a list\n    return c_rounded.tolist()\n\n# Example usage:\n# frame = [1, 2, 3, 4]\n# print(dct(frame))\n```\n\n### Explanation:\n- We first convert the input `frame` to a NumPy array for efficient computation.\n- We iterate over each coefficient index `k` from 0 to `N-1`.\n- For each `k`, we compute the sum of the product of the input values `x[n]` and the cosine term for all `n` from 0 to `N-1`.\n- Depending on whether `orthonormal` is `True`, we scale the computed sum accordingly.\n- Finally, we round each coefficient to four decimal places and return the list of coefficients."}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert the input data to a NumPy array if it's not already one\n    data_array = np.array(data)\n    \n    # Check if any row has a sum of 0\n    if np.any(np.sum(data_array, axis=1) == 0):\n        return -1\n    \n    # Normalize each row by dividing by its sum\n    normalized_data = data_array / np.sum(data_array, axis=1, keepdims=True)\n    \n    # Round the normalized data to 4 decimal places\n    rounded_data = np.round(normalized_data, decimals=4)\n    \n    # Return the result as a list of lists\n    return rounded_data.tolist()\n```\n\nThis function first checks if any row in the input data has a sum of 0, returning `-1` if so. Otherwise, it normalizes each row by dividing each element by the row's sum, rounds the result to four decimal places, and returns the normalized matrix as a list of lists."}
{"task_id": 363, "completion_id": 0, "completion": "Here's the implementation of the `is_continuous` function based on the requirements specified:\n\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool) -> Tuple[bool, bool]:\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def is_box(space):\n        return isinstance(space, Box)\n\n    def is_composite(space):\n        return isinstance(space, (Tuple, Dict))\n\n    def all_boxes(space):\n        if isinstance(space, Box):\n            return True\n        elif isinstance(space, (Tuple, Dict)):\n            return all(all_boxes(subspace) for subspace in space.spaces)\n        else:\n            return False\n\n    cont_action = (\n        tuple_action and\n        all_boxes(env.action_space)\n    )\n\n    cont_obs = (\n        tuple_obs and\n        all_boxes(env.observation_space)\n    )\n\n    return cont_action, cont_obs\n```\n\n### Explanation:\n\n1. **Helper Functions**:\n   - `is_box`: Checks if a space is an instance of `Box`.\n   - `is_composite`: Checks if a space is either `Tuple` or `Dict`.\n   - `all_boxes`: Recursively checks if all sub-spaces of a composite space are `Box`.\n\n2. **Main Logic**:\n   - `cont_action`: Determines if the action space is continuous based on the `tuple_action` flag and whether all sub-spaces are `Box`.\n   - `cont_obs`: Similarly, determines if the observation space is continuous based on the `tuple_obs` flag and whether all sub-spaces are `Box`.\n\n3. **Return Value**: The function returns a tuple `(cont_action, cont_obs)` indicating whether each space is continuous."}
{"task_id": 364, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `sign` function in Python:\n\n```python\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\n\n# Example usage:\nprint(sign(5))   # Output: 1\nprint(sign(-3))  # Output: -1\nprint(sign(0))   # Output: 0\n```\n\nThis function checks the value of `x` and returns the appropriate sign based on whether it is greater than, less than, or equal to zero."}
{"task_id": 366, "completion_id": 0, "completion": "To implement the `l2_distance` function that computes the pair-wise squared Euclidean distance matrix using vectorized operations in NumPy, we can follow these steps:\n\n1. Compute the difference between each pair of samples.\n2. Square these differences.\n3. Sum the squared differences along the appropriate axis to get the squared Euclidean distances.\n4. Clamp any negative values to 0.\n5. Ensure the diagonal elements are exactly 0.0.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X (np.ndarray): A 2-D array with shape (n_samples, n_features).\n\n    Returns:\n        list[list[float]]: The n\u00d7n squared distance matrix where the element at\n            position (i, j) equals the squared \u2113\u2082 distance between samples i and\n            j. The diagonal must be zeros.\n    \"\"\"\n    # Check if the input is a single sample\n    if X.shape[0] == 1:\n        return [[0.0]]\n    \n    # Compute the squared differences\n    diff_squared = np.square(X[:, None] - X)\n    \n    # Sum the squared differences along the second axis to get the squared distances\n    dist_matrix = np.sum(diff_squared, axis=2)\n    \n    # Clamp negative values to 0\n    dist_matrix = np.maximum(dist_matrix, 0.0)\n    \n    # Set the diagonal elements to 0.0\n    np.fill_diagonal(dist_matrix, 0.0)\n    \n    # Convert the result to a list of lists of floats\n    return dist_matrix.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndistance_matrix = l2_distance(X)\nprint(distance_matrix)\n```\n\n### Explanation:\n- **Difference Calculation**: We use broadcasting to compute the difference between each sample `X[i]` and every other sample `X[j]`. This results in a matrix of differences.\n- **Squaring Differences**: We square these differences using `np.square`.\n- **Summing Squared Differences**: We sum the squared differences along the second axis (`axis=2`) to get the squared Euclidean distances for each pair of samples.\n- **Clamping Negative Values**: We ensure all values are non-negative by clamping them to 0 using `np.maximum`.\n- **Setting Diagonal to Zero**: We set the diagonal elements to 0.0 using `np.fill_diagonal`.\n- **Conversion to List of Lists**: Finally, we convert the resulting NumPy array to a list of lists of floats before returning it."}
{"task_id": 367, "completion_id": 0, "completion": "Here's the implementation of the `initialize_kernel` function based on the given requirements:\n\n```python\nimport re\nfrom typing import Any, Dict, Optional\n\nclass KernelBase:\n    \"\"\"Minimal base class for all kernels.  Do NOT modify this class.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        self.params: Dict[str, Any] = kwargs\n\n    def set_params(self, state: Dict[str, Any]):\n        \"\"\"(Re)sets parameters from a previously stored summary dict.\"\"\"\n        hp = state.get(\"hyperparameters\", {})\n        self.params.update({k: v for k, v in hp.items() if k != \"id\"})\n        return self\n\n    def summary(self) -> Dict[str, Any]:\n        \"\"\"Dictionary description that can be fed back to *initialize_kernel*.\"\"\"\n        return {\"hyperparameters\": {\"id\": self.__class__.__name__, **self.params}}\n\n\nclass LinearKernel(KernelBase):\n    def __init__(self):\n        super().__init__()\n\n\nclass PolynomialKernel(KernelBase):\n    def __init__(self, degree: int = 3, coef0: float = 1.0):\n        super().__init__(degree=degree, coef0=coef0)\n\n\nclass RBFKernel(KernelBase):\n    def __init__(self, gamma: float = 1.0):\n        super().__init__(gamma=gamma)\n\n\ndef initialize_kernel(param: Optional[Any] = None) -> KernelBase:\n    \"\"\"Create and return a kernel object from various representations.\n\n    Args:\n        param: None (default LinearKernel), an existing KernelBase instance, a\n            string description such as 'RBF(gamma=0.5)', or a dictionary with a\n            'hyperparameters' entry as produced by KernelBase.summary().\n\n    Returns:\n        KernelBase: the newly created or forwarded kernel instance.\n    \"\"\"\n    if param is None:\n        return LinearKernel()\n\n    if isinstance(param, KernelBase):\n        return param\n\n    if isinstance(param, str):\n        match = re.match(r'^(\\w+)\\(([^)]*)\\)$', param)\n        if match:\n            kernel_type = match.group(1)\n            params_str = match.group(2)\n            params_dict = {}\n            for pair in params_str.split(','):\n                key, value = pair.strip().split('=')\n                try:\n                    if key == 'degree':\n                        params_dict[key] = int(value)\n                    elif key == 'coef0':\n                        params_dict[key] = float(value)\n                    elif key == 'gamma':\n                        params_dict[key] = float(value)\n                    else:\n                        raise ValueError(f\"Unknown parameter '{key}'\")\n                except ValueError as e:\n                    raise ValueError(f\"Invalid value for '{key}': {e}\")\n            if kernel_type.lower() == \"linear\":\n                return LinearKernel()\n            elif kernel_type.lower() == \"polynomial\":\n                return PolynomialKernel(**params_dict)\n            elif kernel_type.lower() == \"rbf\":\n                return RBFKernel(**params_dict)\n            else:\n                raise NotImplementedError(f\"Unsupported kernel type: {kernel_type}\")\n\n    if isinstance(param, dict):\n        hyperparams = param.get(\"hyperparameters\")\n        if hyperparams is None:\n            raise ValueError(\"Missing 'hyperparameters' key in dictionary\")\n\n        id_ = hyperparams.get(\"id\")\n        if id_ is None:\n            raise ValueError(\"Missing 'id' in 'hyperparameters' dictionary\")\n\n        if id_.lower() == \"linear\":\n            return LinearKernel(**hyperparams)\n        elif id_.lower() == \"polynomial\":\n            return PolynomialKernel(**hyperparams)\n        elif id_.lower() == \"rbf\":\n            return RBFKernel(**hyperparams)\n        else:\n            raise NotImplementedError(f\"Unsupported kernel type: {id_}\")\n\n    raise TypeError(\"Unsupported input type\")\n\n# Example usage:\n# kernel = initialize_kernel(\"RBF(gamma=0.5)\")\n# print(kernel.summary())\n```\n\n### Explanation:\n- The function first checks if `param` is `None`, returning a default `LinearKernel` if so.\n- If `param` is already an instance of `KernelBase`, it returns it unchanged.\n- If `param` is a string, it attempts to parse it using a regular expression to extract the kernel type and parameters. It then creates the appropriate kernel instance with the parsed parameters.\n- If `param` is a dictionary, it checks for the presence of the `\"hyperparameters\"` key and extracts the kernel type and parameters. It then creates the appropriate kernel instance with the extracted parameters.\n- If none of the above conditions are met, it raises a `TypeError`."}
{"task_id": 368, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minkowski(x: np.ndarray | list, y: np.ndarray | list, p: float) -> float:\n    \"\"\"Compute the Minkowski\u2013p distance between two vectors.\n\n    Args:\n        x (np.ndarray | list): First one-dimensional vector.\n        y (np.ndarray | list): Second one-dimensional vector of the same length.\n        p (float): Order of the norm (must be > 0).\n\n    Returns:\n        float: The Minkowski distance rounded to 4 decimal places, or \u20111 when\n               input validation fails.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Validate that x and y have identical shapes and that p is strictly positive\n    if x.shape != y.shape or p <= 0:\n        return -1\n    \n    # Compute the Minkowski distance\n    distance = np.sum(np.abs(x - y) ** p) ** (1 / p)\n    \n    # Round the result to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\nprint(minkowski([1, 2, 3], [4, 5, 6], 2))  # Output: 5.1962\nprint(minkowski([1, 2, 3], [4, 5, 6], 1))  # Output: 7.0\nprint(minkowski([1, 2, 3], [4, 5, 6], 0))  # Output: -1\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    # Initialize the running prediction with zeros\n    running_pred = np.zeros_like(updates[0])\n    \n    # Iterate over each update and apply the learning rate\n    for update in updates:\n        running_pred -= learning_rate * update\n    \n    if regression:\n        # Return the running prediction rounded to 4 decimal places\n        return np.round(running_pred, 4)\n    else:\n        # Apply the softmax function row-wise\n        exp_running_pred = np.exp(running_pred)\n        # Compute the probabilities using the softmax formula\n        probabilities = exp_running_pred / np.sum(exp_running_pred, axis=1, keepdims=True)\n        # Return the predicted class labels using argmax\n        return np.argmax(probabilities, axis=1)\n\n# Example usage:\n# Assume 'updates' is a list of NumPy arrays and 'learning_rate' is a float\n# 'regression' is a boolean indicating whether it's a regression problem\n# result = gradient_boosting_predict(updates, learning_rate, regression)\n# print(result)\n```\n\nThis implementation follows the specified behavior for both regression and classification tasks. It initializes the running prediction with zeros and iteratively subtracts each tree's update scaled by the learning rate. For regression, it returns the running prediction rounded to four decimal places. For classification, it applies the softmax function to obtain class probabilities and returns the predicted class labels using `np.argmax`."}
{"task_id": 370, "completion_id": 0, "completion": "To implement the multivariate Gaussian probability density function, we need to follow these steps:\n\n1. Calculate the difference between each sample `x` and the mean vector `mu`.\n2. Compute the quadratic form `(x - mu).T @ inv(cov) @ (x - mu)`, where `inv(cov)` is the inverse of the covariance matrix.\n3. Calculate the exponent part of the formula: `-0.5 * (x - mu).T @ inv(cov) @ (x - mu)`.\n4. Compute the normalization term: `(2 * pi) ** (n / 2) * det(cov) ** 0.5`.\n5. Combine the exponent and normalization terms to get the density for each sample.\n6. Round the results to four decimal places and return them as a list.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef multivariate_gaussian(X: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> list[float]:\n    \"\"\"Calculate the multivariate Gaussian probability density for each sample in X.\"\"\"\n    m, n = X.shape\n    densities = []\n    \n    for x in X:\n        # Step 1: Compute the difference between x and mu\n        diff = x - mu\n        \n        # Step 2: Compute the quadratic form\n        quad_form = np.dot(diff.T, np.linalg.inv(cov)) @ diff\n        \n        # Step 3: Compute the exponent part\n        exponent = -0.5 * quad_form\n        \n        # Step 4: Compute the normalization term\n        norm_term = (2 * np.pi) ** (n / 2) * np.linalg.det(cov) ** 0.5\n        \n        # Step 5: Compute the density\n        density = np.exp(exponent) / norm_term\n        \n        # Step 6: Append the density to the list\n        densities.append(round(density, 4))\n    \n    return densities\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nmu = np.array([0, 0])\ncov = np.array([[1, 0.5], [0.5, 1]])\n\ndensities = multivariate_gaussian(X, mu, cov)\nprint(densities)\n```\n\nThis function will compute the density for each sample in `X` using the given mean `mu` and covariance matrix `cov`, and return the results rounded to four decimal places."}
{"task_id": 371, "completion_id": 0, "completion": "To implement the 1-D cross-correlation with the specified features, we need to handle batching, padding, and dilation. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Determine the padding width based on the given pad specification\n    if isinstance(pad, str):\n        if pad == \"same\":\n            # Calculate padding to maintain the same output length\n            n_examples, signal_length, _ = X.shape\n            _, _, out_channels = W.shape\n            \n            effective_kernel = W.shape[0] * (dilation + 1)\n            out_len = (signal_length - 1) * stride + effective_kernel\n            \n            total_pad = max(0, (out_len - 1) * stride + effective_kernel - signal_length)\n            p_left = total_pad // 2\n            p_right = total_pad - p_left\n        else:\n            raise ValueError(\"Invalid padding type\")\n    elif isinstance(pad, int):\n        p_left = pad\n        p_right = pad\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        p_left, p_right = pad\n    else:\n        raise ValueError(\"Invalid padding format\")\n\n    # Initialize the output volume\n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, _, out_channels = W.shape\n    \n    output_length = (signal_length + 2 * p_left - effective_kernel) // stride + 1\n    \n    output_volume = np.zeros((n_examples, output_length, out_channels))\n\n    # Perform the cross-correlation\n    for example_idx in range(n_examples):\n        for channel_out in range(out_channels):\n            for offset in range(output_length):\n                x_start = p_left + offset * stride\n                x_end = min(signal_length, x_start + kernel_width)\n                \n                # Apply dilation\n                effective_x_start = x_start\n                effective_x_end = x_end\n                \n                # Compute the dot product for the current window\n                for channel_in in range(in_channels):\n                    w_slice = W[:, channel_in, channel_out]\n                    x_slice = X[example_idx, effective_x_start:effective_x_end, channel_in]\n                    \n                    # Apply dilation to the kernel slice\n                    dilated_w_slice = w_slice[dilation::dilation]\n                    \n                    output_volume[example_idx, offset, channel_out] += np.sum(x_slice * dilated_w_slice)\n\n    # Convert the output volume to a Python list\n    return output_volume.tolist()\n\n# Example usage:\nX = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nW = np.array([[[1], [2], [3]], [[4], [5], [6]]])\n\nresult = conv1D(X, W, stride=1, pad=\"same\", dilation=0)\nprint(result)\n```\n\n### Explanation:\n- **Padding**: We determine the padding based on whether `pad` is `\"same\"`, an integer, or a tuple. For `\"same\"`, we calculate the necessary padding to ensure the output length remains unchanged.\n- **Output Volume Initialization**: We initialize the output volume with zeros and compute its dimensions.\n- **Cross-Correlation Calculation**: For each example, channel, and output position, we extract the relevant segments from both `X` and `W`, apply dilation to the kernel, and compute the dot product to update the output volume.\n- **Result Conversion**: Finally, we convert the output volume to a Python list using `tolist()` before returning it."}
{"task_id": 373, "completion_id": 0, "completion": "To calculate the Gini impurity of a discrete label sequence, we need to follow these steps:\n\n1. Determine the number of unique classes (`C`) in the label sequence.\n2. Calculate the proportion of each class (`p_c`) in the sequence.\n3. Compute the Gini impurity using the formula \\( Gini = 1 - \\sum_{c=0}^{C-1} p_c^2 \\).\n4. Return the result rounded to four decimal places.\n\nHere's the implementation of the `gini` function:\n\n```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert input to NumPy array if it's not already\n    y = np.array(y)\n    \n    # Check if the input is empty\n    if len(y) == 0:\n        return 0.0\n    \n    # Get the number of unique classes\n    C = len(np.unique(y))\n    \n    # Calculate the frequency of each class\n    frequencies = np.bincount(y, minlength=C) / len(y)\n    \n    # Calculate the Gini impurity\n    gini_impurity = 1 - np.sum(frequencies ** 2)\n    \n    # Round the result to 4 decimal places\n    return round(gini_impurity, 4)\n\n# Example usage:\nprint(gini([0, 1, 0, 1, 2]))  # Output: 0.4897\nprint(gini([]))               # Output: 0.0\n```\n\n### Explanation:\n\n- **Input Handling**: The function first converts the input `y` to a NumPy array to handle both lists and arrays uniformly.\n- **Empty Sequence Check**: It checks if the sequence is empty and returns `0.0` if so.\n- **Unique Classes**: It uses `np.unique(y)` to find the number of unique classes (`C`).\n- **Frequency Calculation**: `np.bincount(y, minlength=C)` counts the occurrences of each class and divides by the total number of elements to get the frequency of each class.\n- **Gini Impurity Calculation**: The Gini impurity is calculated using the formula, and the result is rounded to four decimal places before being returned."}
{"task_id": 374, "completion_id": 0, "completion": "To solve the problem of training a multiple linear regression model using batch gradient descent, we need to follow these steps:\n\n1. Initialize the weight vector `theta` with zeros and the bias `b` to zero.\n2. Iterate over the dataset for a specified number of iterations (`n_iterations`).\n3. In each iteration, compute the predictions `\u0177` using the current weights and bias.\n4. Calculate the gradients `dw` and `db` using the formulae provided.\n5. Update the weights and bias using the gradients and the learning rate.\n6. Repeat steps 3-5 until convergence or the maximum number of iterations is reached.\n7. Return the learned weights and bias rounded to four decimal places.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    \n    # Check if the input dimensions are consistent\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    # Initialize theta and b\n    m, n = X.shape\n    theta = np.zeros(n)\n    b = 0\n    \n    # Perform batch gradient descent\n    for _ in range(n_iterations):\n        # Compute predictions\n        y_pred = np.dot(X, theta) + b\n        \n        # Calculate gradients\n        dw = (2 / m) * np.dot(X.T, (y_pred - y))\n        db = (2 / m) * np.sum(y_pred - y)\n        \n        # Update weights and bias\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    \n    # Round the results to 4 decimal places\n    weights_rounded = [round(weight, 4) for weight in theta]\n    bias_rounded = round(b, 4)\n    \n    return weights_rounded, bias_rounded\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([3, 5, 7])\n# print(linear_regression_gd(X, y))\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the number of samples in `X` matches the number of samples in `y`. If not, it returns `-1`.\n- **Initialization**: It initializes `theta` as a vector of zeros and `b` to zero.\n- **Gradient Descent Loop**: For each iteration, it calculates the predicted values `\u0177`, computes the gradients `dw` and `db`, and updates `theta` and `b` using the learning rate.\n- **Rounding**: Finally, it rounds the learned weights and bias to four decimal places before returning them as a tuple.\n\nThis function should work correctly for any given dataset with appropriate feature scaling if needed."}
{"task_id": 375, "completion_id": 0, "completion": "To implement the `calc_pad_dims_2D` function, we need to calculate the necessary padding to ensure that the output dimensions match the desired output size after applying a 2-D convolution with specified parameters. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef calc_pad_dims_2D(X_shape: tuple,\n                     out_dim: tuple,\n                     kernel_shape: tuple,\n                     stride: int,\n                     dilation: int = 0) -> tuple:\n    \"\"\"Return the amount of zero-padding needed on each side of a 4-D tensor.\n\n    Parameters\n    ----------\n    X_shape : tuple\n        Shape of the input tensor ``(n_ex, in_rows, in_cols, in_ch)``.\n    out_dim : tuple\n        Desired output height and width ``(out_rows, out_cols)``.\n    kernel_shape : tuple\n        Height and width of the convolution kernel ``(fr, fc)``.\n    stride : int\n        Stride used by the convolution.\n    dilation : int, optional (default=0)\n        Dilation factor (number of inserted zeros between kernel elements).\n\n    Returns\n    -------\n    tuple\n        A 4-tuple ``(top, bottom, left, right)`` indicating how many zero rows\n        and columns must be added to the **top**, **bottom**, **left**, and\n        **right** sides of the input volume.\n    \"\"\"\n    in_rows, in_cols, _, _ = X_shape\n    out_rows, out_cols = out_dim\n    \n    # Calculate effective filter sizes\n    _fr = kernel_shape[0] * (dilation + 1) - dilation\n    _fc = kernel_shape[1] * (dilation + 1) - dilation\n    \n    # Calculate the minimum padding needed\n    up_pad = max(0, (out_rows - 1) * stride + _fr - in_rows)\n    down_pad = max(0, (out_rows - 1) * stride + _fr - in_rows)\n    \n    left_pad = max(0, (out_cols - 1) * stride + _fc - in_cols)\n    right_pad = max(0, (out_cols - 1) * stride + _fc - in_cols)\n    \n    # If the padding is odd, add the extra pixel to the bottom or right\n    if up_pad % 2 != 0:\n        up_pad += 1\n    if down_pad % 2 != 0:\n        down_pad += 1\n    if left_pad % 2 != 0:\n        left_pad += 1\n    if right_pad % 2 != 0:\n        right_pad += 1\n    \n    return (up_pad, down_pad, left_pad, right_pad)\n\n# Example usage:\nX_shape = (1, 5, 7, 3)\nout_dim = (6, 8)\nkernel_shape = (3, 3)\nstride = 1\ndilation = 1\n\npad_dims = calc_pad_dims_2D(X_shape, out_dim, kernel_shape, stride, dilation)\nprint(pad_dims)  # Output: (1, 1, 1, 1)\n```\n\n### Explanation:\n1. **Effective Filter Size Calculation**: \n   - `_fr` and `_fc` are calculated using the formula for the effective filter size considering dilation.\n\n2. **Padding Calculation**:\n   - For both height and width, we calculate the padding needed to achieve the desired output size using the standard convolution formula adjusted for dilation.\n   - We use `max(0, ...)` to ensure that the padding is non-negative.\n\n3. **Handling Odd Paddings**:\n   - If the calculated padding is odd, we add one more unit to the bottom or right to ensure the total padding is even, which aligns with the behavior of some frameworks like TensorFlow.\n\nThis function will correctly compute the necessary padding to achieve the desired output dimensions for a 2-D convolution with the specified parameters."}
{"task_id": 376, "completion_id": 0, "completion": "To solve the problem of checking if a given 2-D numeric array is symmetric with respect to its main diagonal, we need to follow these steps:\n\n1. Verify if the matrix is square. A matrix is square if it has the same number of rows and columns.\n2. If the matrix is square, compare each element in the matrix with its corresponding element on the other side of the main diagonal.\n3. Use `numpy.allclose` to account for floating-point precision issues. This function checks if two arrays are element-wise approximately equal within a tolerance level.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert the list of lists to a NumPy array\n    X_array = np.array(X)\n    \n    # Check if the matrix is square\n    if X_array.shape[0] != X_array.shape[1]:\n        return False\n    \n    # Calculate the transpose of the matrix\n    X_transpose = X_array.T\n    \n    # Use numpy.allclose to check if the matrix is symmetric\n    return np.allclose(X_array, X_transpose)\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [2, 4, 5],\n    [3, 5, 6]\n]\n\nprint(is_symmetric(matrix))  # Output: True\n```\n\n### Explanation:\n\n- **Convert to NumPy Array**: We first convert the input list of lists into a NumPy array to leverage efficient numerical operations.\n- **Check Squareness**: We use `X_array.shape[0] == X_array.shape[1]` to determine if the matrix is square.\n- **Transpose Calculation**: We compute the transpose of the matrix using `X_array.T`.\n- **Symmetry Check**: We use `np.allclose` to compare the original matrix with its transpose. This function returns `True` if all elements are approximately equal within the specified tolerance (default is `1e-8`), indicating symmetry.\n\nThis approach ensures that the function correctly identifies symmetric matrices while accounting for potential floating-point precision issues."}
{"task_id": 377, "completion_id": 0, "completion": "Here's the implementation of the `logistic_negative_gradient` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculate the negative gradient for logistic loss given binary class labels and predictions.\n    \n    Parameters:\n    y (list): Binary class labels (0 or 1).\n    f (list): Current prediction scores (floats).\n    \n    Returns:\n    list: List of negative gradients rounded to four decimal places.\n    \"\"\"\n    # Check if all elements in y are either 0 or 1\n    if not all(val in [0, 1] for val in y):\n        return [-1]\n    \n    # Convert y to {-1, 1}\n    y_prime = 2 * np.array(y) - 1\n    \n    # Calculate the negative gradient\n    g = y_prime / (1 + np.exp(-y_prime * f))\n    \n    # Round the result to four decimal places\n    g_rounded = np.round(g, decimals=4).tolist()\n    \n    return g_rounded\n\n# Example usage:\n# y = [0, 1, 1, 0]\n# f = [0.5, -0.3, 0.7, -0.2]\n# print(logistic_negative_gradient(y, f))\n```\n\n### Explanation:\n- The function first checks if all elements in the `y` list are either 0 or 1. If not, it returns `-1` as per the output constraints.\n- It then converts the binary labels `y` to the corresponding sign values `y'`.\n- Using the formula for the negative gradient of the logistic loss, it computes `g_i = y'_i / (1 + \\exp(-y'_i f_i))`.\n- Finally, it rounds the computed gradients to four decimal places and returns them as a list."}
{"task_id": 378, "completion_id": 0, "completion": "```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```"}
{"task_id": 379, "completion_id": 0, "completion": "To solve this problem, we need to sort the list of nodes based on the specified criteria: first by priority and then by entry ID. Here's how you can implement the `sort_priority_nodes` function:\n\n```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    # Sort the nodes using a custom sorting key\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    \n    # Extract the 'key' field from each sorted node and return it as a list\n    return [node['key'] for node in sorted_nodes]\n\n# Example usage:\nnodes = [\n    {'key': 'A', 'val': 10, 'priority': 3, 'entry_id': 5},\n    {'key': 'B', 'val': 20, 'priority': 2, 'entry_id': 3},\n    {'key': 'C', 'val': 30, 'priority': 2, 'entry_id': 1},\n    {'key': 'D', 'val': 40, 'priority': 1, 'entry_id': 2}\n]\n\nsorted_keys = sort_priority_nodes(nodes)\nprint(sorted_keys)  # Output: ['B', 'C', 'A', 'D']\n```\n\n### Explanation:\n\n1. **Sorting**: We use Python's built-in `sorted()` function with a custom key. The key is a tuple `(x['priority'], x['entry_id'])`, which ensures that the primary sorting criterion is the `priority`, and the secondary criterion is the `entry_id`.\n\n2. **Extracting Keys**: After sorting, we extract the `key` field from each node in the sorted list and return them as a new list.\n\nThis approach efficiently sorts the nodes while adhering to the specified rules."}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Constants\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    \n    def elu(x, alpha):\n        \"\"\"Compute the ELU activation.\"\"\"\n        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n    \n    def der_elu(x, alpha):\n        \"\"\"Compute the first derivative of ELU.\"\"\"\n        return np.where(x > 0, 1, alpha * np.exp(x))\n    \n    def der2_elu(x, alpha):\n        \"\"\"Compute the second derivative of ELU.\"\"\"\n        return np.where(x > 0, 0, alpha * np.exp(x))\n    \n    # Convert input to NumPy array\n    x_array = np.array(x)\n    \n    # Compute SELU based on the order\n    if order == 0:\n        y = scale * elu(x_array, alpha)\n    elif order == 1:\n        y = scale * der_elu(x_array, alpha)\n    elif order == 2:\n        y = scale * der2_elu(x_array, alpha)\n    else:\n        raise ValueError(\"Order must be 0, 1, or 2.\")\n    \n    # Round to 4 decimal places and convert to list\n    return y.round(4).tolist()\n\n# Example usage:\nprint(selu([1.0, -1.0, 2.0], order=0))  # Output: [1.0507, 0.0, 2.1014]\nprint(selu([1.0, -1.0, 2.0], order=1))  # Output: [1.0507, -1.6733, 2.1014]\nprint(selu([1.0, -1.0, 2.0], order=2))  # Output: [1.0507, 0.0, 2.1014]\n```\n\nThis implementation defines the SELU activation function and its derivatives, handling both scalar and multi-dimensional inputs. The derivatives are computed using helper functions for each order, and the results are rounded to four decimal places before being returned as a list."}
{"task_id": 382, "completion_id": 0, "completion": "Here's the complete implementation of the Gaussian Naive Bayes predictor:\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray) -> list:\n    \"\"\"Train a Gaussian Naive Bayes model on *X_train*, *y_train* and predict\n    the labels of *X_test*.\n\n    The implementation must use the formulae described in the task statement,\n    assume that every feature follows an independent Gaussian distribution\n    inside each class and return the predicted labels as a Python list.\n    \"\"\"\n    # Number of samples and number of features\n    n_samples, n_features = X_train.shape\n    n_classes = len(np.unique(y_train))\n    \n    # Initialize means and variances for each class\n    mu = np.zeros((n_classes, n_features))\n    sigma_squared = np.zeros((n_classes, n_features))\n    \n    # Compute prior probabilities\n    priors = np.bincount(y_train) / n_samples\n    \n    # Estimate means and variances for each class\n    for c in range(n_classes):\n        mask = y_train == c\n        mu[c] = np.mean(X_train[mask], axis=0)\n        sigma_squared[c] = np.var(X_train[mask], axis=0, ddof=1)\n    \n    # Add epsilon to variance to prevent division by zero\n    epsilon = 1e-4\n    sigma_squared += epsilon\n    \n    # Function to compute the Gaussian likelihood of a single feature value\n    def gaussian_likelihood(feature_value, mean, variance):\n        exponent = -(feature_value - mean) ** 2 / (2 * variance + epsilon)\n        return 1 / np.sqrt(2 * np.pi * variance + epsilon) * np.exp(exponent)\n    \n    # Predict labels for each test sample\n    predictions = []\n    for x in X_test:\n        posteriors = []\n        for c in range(n_classes):\n            likelihood_product = np.prod([gaussian_likelihood(xi, mu[c][i], sigma_squared[c][i]) for i, xi in enumerate(x)])\n            posterior = priors[c] * likelihood_product\n            posteriors.append(posterior)\n        \n        # Predict the class with the highest posterior probability\n        predicted_class = np.argmax(posteriors)\n        predictions.append(predicted_class)\n    \n    return predictions\n```\n\n### Explanation:\n1. **Prior Probability**: We calculate the prior probability for each class using the frequency of each class in the training data.\n\n2. **Mean and Variance Estimation**: For each class, we estimate the mean and variance of each feature using the training data. We also add a small constant `epsilon` to the variance to prevent division by zero.\n\n3. **Gaussian Likelihood Calculation**: We define a helper function to compute the Gaussian likelihood of a single feature value given its mean and variance.\n\n4. **Prediction**: For each test sample, we compute the posterior probability for each class by multiplying the prior probability by the product of the Gaussian likelihoods of all features. We then select the class with the highest posterior probability.\n\n5. **Output**: The function returns a list of predicted class labels corresponding to each test sample."}
{"task_id": 383, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef find_bounds(y_i: int, y_j: int, alpha_i: float, alpha_j: float, C: float) -> List[float]:\n    \"\"\"Return the feasible interval [L, H] for alpha_j in SMO.\n\n    Parameters\n    ----------\n    y_i : int\n        Label of the i-th example (\u22121 or 1).\n    y_j : int\n        Label of the j-th example (\u22121 or 1).\n    alpha_i : float\n        Current Lagrange multiplier for the i-th example.\n    alpha_j : float\n        Current Lagrange multiplier for the j-th example.\n    C : float\n        Regularisation constant of the SVM.\n\n    Returns\n    -------\n    List[float]\n        A two-element list [L, H] with both values rounded to 4 decimals.\n    \"\"\"\n    if y_i != y_j:\n        L = max(0, alpha_j - alpha_i)\n        H = min(C, C - alpha_i + alpha_j)\n    else:\n        L = max(0, alpha_i + alpha_j - C)\n        H = min(C, alpha_i + alpha_j)\n    \n    return [round(L, 4), round(H, 4)]\n\n# Example usage:\nprint(find_bounds(-1, 1, 0.5, 0.2, 1.0))  # Output: [0.3333, 0.7667]\nprint(find_bounds(1, 1, 0.3, 0.8, 1.0))   # Output: [0.2000, 0.8000]\n```\n\nThis function calculates the feasible interval `[L, H]` for the Lagrange multiplier `alpha_j` based on the given conditions for different label pairs `(y_i, y_j)`. The result is returned as a list of two floating-point numbers, each rounded to four decimal places."}
{"task_id": 384, "completion_id": 0, "completion": "To solve the problem of finding the smallest Laplacian eigenvectors, we need to follow these steps:\n\n1. Check if `k` is larger than the number of rows/columns in `L`. If so, return `-1`.\n2. Compute the eigenvalues and eigenvectors of the Laplacian matrix `L`.\n3. Sort the eigenvalues and corresponding eigenvectors based on their values.\n4. Select the top `k` eigenvectors.\n5. Normalize each selected eigenvector to have unit length.\n6. Ensure each eigenvector has a positive first non-zero entry by flipping its sign if necessary.\n7. Round each entry to 4 decimal places and return the result as a list of lists.\n\nHere's the implementation of the `get_eigvecs` function:\n\n```python\nimport numpy as np\n\ndef get_eigvecs(L: list[list[int | float]], k: int):\n    \"\"\"Calculate the k eigenvectors that correspond to the smallest k eigenvalues.\n\n    Parameters\n    ----------\n    L : list[list[int | float]] | np.ndarray\n        Square Laplacian matrix of the graph.\n    k : int\n        Number of eigenvectors to return.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Matrix whose columns are the selected eigenvectors rounded to 4 decimal\n        places. Returns -1 if *k* is invalid.\n    \"\"\"\n    # Convert the input list to a numpy array for easier manipulation\n    L_np = np.array(L)\n    \n    # Get the dimensions of the matrix\n    n_rows, n_cols = L_np.shape\n    \n    # Check if k is valid\n    if k > n_rows or k <= 0:\n        return -1\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(L_np)\n    \n    # Sort eigenvalues and corresponding eigenvectors\n    sorted_indices = np.argsort(eigenvalues)\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    selected_eigenvectors = sorted_eigenvectors[:, :k]\n    \n    # Normalize each eigenvector to unit length\n    normalized_eigenvectors = selected_eigenvectors / np.linalg.norm(selected_eigenvectors, axis=0)\n    \n    # Ensure each eigenvector has a positive first non-zero entry\n    for i in range(k):\n        if np.sign(normalized_eigenvectors[i, 0]) == -1:\n            normalized_eigenvectors[i, :] *= -1\n    \n    # Round each entry to 4 decimal places\n    rounded_eigenvectors = np.round(normalized_eigenvectors, decimals=4).tolist()\n    \n    return rounded_eigenvectors\n\n# Example usage\nL = [[1, -1], [-1, 1]]\nk = 1\nprint(get_eigvecs(L, k))  # Output: [[0.7071], [0.7071]]\n```\n\nThis function first checks if `k` is valid. It then computes the eigenvalues and eigenvectors using `np.linalg.eigh`, sorts them, selects the top `k`, normalizes them, ensures they have a positive first non-zero entry, rounds them to 4 decimal places, and finally converts them to a list of lists."}
{"task_id": 385, "completion_id": 0, "completion": "To implement Linear Discriminant Analysis (LDA) for dimensionality reduction, we need to follow these steps:\n\n1. Compute the within-class scatter matrix \\( S_w \\).\n2. Compute the between-class scatter matrix \\( S_b \\).\n3. Solve the generalized eigenvalue problem \\( S_w^{-1} S_b \\mathbf{w} = \\lambda \\mathbf{w} \\).\n4. Sort the eigenvectors by descending eigenvalues.\n5. Select the top \\( k \\) eigenvectors, normalize them to unit length, and ensure the first element is non-negative.\n6. Project the original data onto these selected eigenvectors and round the results to four decimal places.\n\nHere's the implementation in Python:\n\n```python\nimport numpy as np\n\ndef lda_transform(X: list[list[int | float]], y: list[int], k: int) -> list[list[float]]:\n    \"\"\"Implements Linear Discriminant Analysis (LDA) for dimensionality reduction.\"\"\"\n    \n    # Convert input lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Get unique labels and their indices\n    unique_labels, indices = np.unique(y, return_inverse=True)\n    num_classes = len(unique_labels)\n    \n    # Check if k is greater than c-1\n    k = min(k, num_classes - 1)\n    \n    # Calculate the overall mean\n    mean_X = np.mean(X, axis=0)\n    \n    # Initialize matrices for within-class scatter and between-class scatter\n    Sw = np.zeros((X.shape[1], X.shape[1]))\n    Sb = np.zeros((X.shape[1], X.shape[1]))\n    \n    # Compute within-class scatter matrix\n    for label in range(num_classes):\n        X_label = X[indices == label]\n        mean_label = np.mean(X_label, axis=0)\n        Sw += np.dot((X_label - mean_label).T, (X_label - mean_label))\n    \n    # Compute between-class scatter matrix\n    for label in range(num_classes):\n        mean_label = np.mean(X[indices == label], axis=0)\n        Sw += np.dot((mean_label - mean_X).T, (mean_label - mean_X))\n    \n    # Perform eigen decomposition of Sw^(-1) * Sb\n    Sw_inv = np.linalg.inv(Sw)\n    eigenvalues, eigenvectors = np.linalg.eigh(np.dot(Sw_inv, Sb))\n    \n    # Sort eigenvectors by descending eigenvalues\n    idx = eigenvalues.argsort()[::-1]\n    eigenvectors = eigenvectors[:, idx]\n    eigenvalues = eigenvalues[idx]\n    \n    # Normalize eigenvectors to unit length\n    normalized_eigenvectors = eigenvectors / np.linalg.norm(eigenvectors, axis=0)\n    \n    # Ensure the first element of each eigenvector is non-negative\n    signs = np.sign(normalized_eigenvectors[:, 0])\n    normalized_eigenvectors *= signs\n    \n    # Project the data onto the first k eigenvectors\n    projected_data = np.dot(X, normalized_eigenvectors.T)\n    \n    # Round the projected data to 4 decimal places\n    projected_data_rounded = np.round(projected_data, 4).tolist()\n    \n    return projected_data_rounded\n\n# Example usage:\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny = [0, 0, 1, 1]\nk = 2\nresult = lda_transform(X, y, k)\nprint(result)\n```\n\nThis function calculates the necessary matrices, solves the eigenvalue problem, sorts the eigenvectors, and projects the data onto the specified number of dimensions. The result is returned as a list of lists, rounded to four decimal places."}
{"task_id": 386, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        1-D array of true labels (0 or 1).\n    y_pred : np.ndarray\n        1-D array of predicted probabilities (between 0 and 1).\n\n    Returns\n    -------\n    float\n        The average BCE loss rounded to 4 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    \n    # Clip the predictions to the interval [epsilon, 1 - epsilon]\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Compute the BCE loss\n    bce_loss = -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    \n    # Round the result to 4 decimal places\n    return round(bce_loss, 4)\n\n# Example usage:\ny_true = np.array([0, 1, 0, 1])\ny_pred = np.array([0.1, 0.9, 0.8, 0.2])\n\nprint(binary_crossentropy(y_true, y_pred))  # Output will be the calculated BCE loss\n```\n\nThis function first clips the predicted probabilities to ensure they fall within the valid range $(0, 1)$. It then computes the average Binary Cross-Entropy loss using the formula provided in the problem description. Finally, it rounds the result to four decimal places before returning it."}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    # Initialize the running prediction\n    predictions = np.full_like(y, np.mean(y))\n    \n    for _ in range(n_estimators):\n        # Compute the residuals\n        residuals = y - predictions\n        \n        # Find the optimal split point\n        min_cost = float('inf')\n        best_split_point = None\n        \n        # Iterate over potential split points\n        for i in range(1, len(x)):\n            left_residuals = residuals[:i]\n            right_residuals = residuals[i:]\n            \n            if not left_residuals.size or not right_residuals.size:\n                continue\n            \n            # Calculate the cost of splitting at this point\n            left_mean_residual = np.mean(left_residuals)\n            right_mean_residual = np.mean(right_residuals)\n            cost = np.sum((left_residuals - left_mean_residual) ** 2) + np.sum((right_residuals - right_mean_residual) ** 2)\n            \n            # Update the best split if this one is better\n            if cost < min_cost:\n                min_cost = cost\n                best_split_point = x[i]\n        \n        # If no valid split was found, use the global mean residual\n        if best_split_point is None:\n            best_split_point = np.mean(x)\n            left_mean_residual = np.mean(residuals)\n            right_mean_residual = np.mean(residuals)\n        \n        # Update the predictions\n        for i in range(len(x)):\n            if x[i] <= best_split_point:\n                predictions[i] += learning_rate * left_mean_residual\n            else:\n                predictions[i] += learning_rate * right_mean_residual\n    \n    # Round the predictions to 4 decimal places\n    return np.round(predictions, 4).tolist()\n\n# Example usage:\n# x = [1, 2, 3, 4, 5]\n# y = [2, 3, 5, 7, 11]\n# print(gradient_boosting_1d(x, y, 3))\n```\n\nThis function implements the described Gradient Boosting Regressor using decision stumps for a single numerical feature. It iteratively computes residuals, finds the optimal split point, updates the predictions, and returns the final in-sample predictions rounded to four decimal places."}
{"task_id": 389, "completion_id": 0, "completion": "Here's how you can implement the `compute_laplacian` function in Python:\n\n```python\nimport numpy as np\n\ndef compute_laplacian(W):\n    \"\"\"Compute the (unnormalised) Laplacian matrix of an undirected graph.\n\n    Parameters\n    ----------\n    W : list[list[int | float]] | np.ndarray\n        Square weighted adjacency matrix of the graph.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The Laplacian matrix rounded to 4 decimal places and converted to a\n        Python list of lists. Returns -1 if the input is not a square matrix.\n    \"\"\"\n    # Check if the input is a square matrix\n    if len(W) != len(W[0]):\n        return -1\n    \n    # Convert the input to a NumPy array for easier manipulation\n    W_np = np.array(W)\n    \n    # Compute the degree matrix D\n    D = np.diag(np.sum(W_np, axis=1))\n    \n    # Compute the Laplacian matrix L = D - W\n    L = D - W_np\n    \n    # Round the result to 4 decimal places and convert to a list of lists\n    L_rounded = np.round(L, decimals=4).tolist()\n    \n    return L_rounded\n\n# Example usage:\nW = [\n    [0, 1, 0],\n    [1, 0, 1],\n    [0, 1, 0]\n]\n\nresult = compute_laplacian(W)\nprint(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input matrix `W` is square by comparing its dimensions. If they are not equal, it returns `-1`.\n\n2. **Degree Matrix Calculation**: The degree matrix `D` is computed by taking the sum of each row of `W` and placing these sums on the diagonal of a new matrix.\n\n3. **Laplacian Matrix Construction**: The Laplacian matrix `L` is calculated as `D - W`.\n\n4. **Rounding and Conversion**: The resulting Laplacian matrix is rounded to four decimal places using `np.round`, and then converted to a list of lists using `.tolist()`.\n\nThis implementation ensures that the function handles the input correctly and provides the desired output format."}
{"task_id": 391, "completion_id": 0, "completion": "Here's the implementation of the nearest-neighbour interpolation function:\n\n```python\nimport numpy as np\n\ndef nn_interpolate_1D(X: np.ndarray, t: list[float]) -> list[list[float]]:\n    \"\"\"Nearest-neighbour interpolation for a 1-D multi-channel signal.\n\n    Args:\n        X: A NumPy array of shape (in_length, in_channels) representing the\n           original sampled signal.\n        t: A list (or 1-D array-like) of float coordinates at which the signal\n           should be interpolated.\n\n    Returns:\n        A list of lists containing the interpolated samples.  The outer list\n        length equals ``len(t)``; each inner list has length ``in_channels``.\n    \"\"\"\n    # Convert t to a NumPy array for easier manipulation\n    t = np.array(t)\n    \n    # Round each element in t to the nearest integer using around\n    rounded_t = np.around(t)\n    \n    # Clip the indices to ensure they are within the valid range [0, in_length-1]\n    clipped_t = np.clip(rounded_t, 0, X.shape[0] - 1)\n    \n    # Use advanced indexing to select the corresponding elements from X\n    interpolated_samples = X[np.arange(len(clipped_t)), clipped_t].tolist()\n    \n    return interpolated_samples\n```\n\n### Explanation:\n1. **Convert `t` to a NumPy Array**: This allows us to use NumPy's powerful array operations easily.\n2. **Round `t`**: We use `np.around(t)` to round each element to the nearest integer. If there's a tie, it defaults to the even integer.\n3. **Clip Indices**: We clip the rounded indices to ensure they fall within the valid range `[0, in_length-1]`. This prevents out-of-bounds errors.\n4. **Select Samples**: Using advanced indexing (`X[np.arange(len(clipped_t)), clipped_t]`) we select the elements from `X` at the specified indices.\n5. **Convert to List**: Finally, we convert the resulting array to a list using `.tolist()` to match the desired output format."}
{"task_id": 392, "completion_id": 0, "completion": "To implement the `preemphasis` function, we need to apply the pre-emphasis filter to each sample of the input signal while handling edge cases such as when the input is a NumPy array and ensuring that the output is a Python list. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef preemphasis(x: \"list[float] | np.ndarray\", alpha: float) -> list[float]:\n    \"\"\"Apply a first-order pre-emphasis filter to a 1-D signal.\n\n    Args:\n        x: Input signal.  A one-dimensional array-like object containing the\n           samples to be filtered.\n        alpha: Pre-emphasis coefficient in the half-open interval ``[0, 1)``.\n\n    Returns:\n        The filtered signal as a Python ``list`` with every element rounded to\n        four decimal places.\n    \"\"\"\n    # Ensure x is a NumPy array for efficient computation\n    if isinstance(x, list):\n        x = np.array(x)\n    \n    # Apply the pre-emphasis filter\n    if alpha == 0:\n        return x.tolist()  # If alpha is 0, the signal remains unchanged\n    \n    y = np.zeros_like(x)\n    y[0] = x[0]\n    for i in range(1, len(x)):\n        y[i] = x[i] - alpha * x[i - 1]\n    \n    # Round each element to four decimal places\n    y_rounded = [round(y_val, 4) for y_val in y]\n    \n    return y_rounded\n\n# Example usage:\n# x = [1.0, 0.5, 0.25, 0.125, 0.0625]\n# alpha = 0.97\n# print(preemphasis(x, alpha))\n```\n\n### Explanation:\n1. **Input Handling**: The function checks if the input `x` is a list and converts it to a NumPy array for efficient computation.\n2. **Edge Case for Alpha**: If `alpha` is 0, the function returns the original signal as a list since no filtering is needed.\n3. **Filter Application**: We initialize an empty array `y` of the same length as `x`. For the first sample, we set `y[0]` equal to `x[0]`. Then, for each subsequent sample, we compute `y[i]` using the pre-emphasis formula.\n4. **Rounding**: After applying the filter, we round each element of the resulting array `y` to four decimal places.\n5. **Output**: Finally, we convert the rounded array back to a Python list and return it.\n\nThis implementation ensures that the function handles both lists and NumPy arrays and provides the desired output format."}
{"task_id": 394, "completion_id": 0, "completion": "To implement the RMSprop optimization update step in Python, we need to follow the mathematical formula provided. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # Initialize Eg if it is None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    # Calculate the new running average of squared gradients\n    Eg_new = rho * Eg + (1 - rho) * grad**2\n    \n    # Calculate the update step\n    delta_w = -learning_rate * grad / np.sqrt(Eg_new + 1e-8)\n    \n    # Update the parameters\n    w_next = w + delta_w\n    \n    # Convert the results to regular Python lists and round to 4 decimal places\n    w_next_list = [round(x, 4) for x in w_next.flatten()]\n    Eg_next_list = [round(x, 4) for x in Eg_new.flatten()]\n    \n    return w_next_list, Eg_next_list\n\n# Example usage:\n# w = np.array([1.0, 2.0])\n# grad = np.array([0.5, 0.3])\n# Eg = np.array([0.1, 0.2])\n# alpha = 0.01\n# rho = 0.9\n# w_next, Eg_next = rmsprop_update(w, grad, Eg, alpha, rho)\n# print(\"Updated parameters:\", w_next)\n# print(\"Updated running average of squared gradients:\", Eg_next)\n```\n\n### Explanation:\n1. **Initialization**: We check if `Eg` is `None` and initialize it with a zero array of the same shape as `grad`.\n\n2. **Running Average Calculation**: We calculate the new running average of squared gradients using the formula \\( E_g^{(t)} = \\rho E_g^{(t-1)} + (1-\\rho) g^2 \\).\n\n3. **Update Step**: The update step is calculated using the formula \\( \\Delta w = -\\frac{\\alpha g}{\\sqrt{E_g^{(t)} + \\epsilon}} \\). The epsilon term is added for numerical stability.\n\n4. **Parameter Update**: We update the parameters \\( w \\) using the computed \\( \\Delta w \\).\n\n5. **Conversion and Rounding**: Both the updated parameters and the new running average are converted to regular Python lists and rounded to four decimal places.\n\nThis implementation ensures that the updates are performed according to the RMSprop algorithm and returns the results in the required format."}
{"task_id": 395, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef print_backward_result(beta: np.ndarray, b_prob: float) -> str:\n    \"\"\"Return a formatted string with a \u03b2-matrix and its probability.\n\n    The string must look like:\n        **************************************************\n        Beta:\n        <beta as str(beta)>\n        Probability of sequence: <b_prob>\n\n    Args:\n        beta: A 2-D NumPy array of \u03b2-values.\n        b_prob: Probability of the observation sequence.\n\n    Returns:\n        The formatted multi-line string (do not print inside this function).\n    \"\"\"\n    # Format the beta matrix using str() to get a readable representation\n    beta_str = str(beta)\n    \n    # Create the formatted string with 50 asterisks\n    result = f\"{'*' * 50}\\nBeta:\\n{beta_str}\\nProbability of sequence: {b_prob}\"\n    \n    return result\n\n# Example usage:\n# beta = np.array([[0.1, 0.2], [0.3, 0.4]])\n# b_prob = 0.8\n# print(print_backward_result(beta, b_prob))\n```"}
{"task_id": 396, "completion_id": 0, "completion": "Here's how you can implement the `tanh_with_derivatives` function in Python, which computes the hyperbolic tangent (tanh), its first derivative, and its second derivative for each element in the input array `x`. The results are rounded to six decimal places and returned as a list of lists.\n\n```python\nimport numpy as np\n\ndef tanh_with_derivatives(x):\n    \"\"\"Compute tanh and its first two derivatives element-wise.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        1-D collection of real numbers.\n\n    Returns\n    -------\n    list[list[float]]\n        A list containing three lists:\n        1. tanh(x),\n        2. First derivative of tanh,\n        3. Second derivative of tanh.\n\n    Notes\n    -----\n    \u2022 Every numeric result must be rounded to 6 decimal places.\n    \u2022 The input must *not* be modified in-place.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    x = np.array(x)\n    \n    # Compute tanh values\n    tanh_values = np.tanh(x)\n    \n    # Compute first derivative of tanh\n    first_derivative = 1 - tanh_values**2\n    \n    # Compute second derivative of tanh\n    second_derivative = -2 * tanh_values * (1 - tanh_values**2)\n    \n    # Round all results to 6 decimal places\n    tanh_rounded = np.round(tanh_values, decimals=6).tolist()\n    first_derivative_rounded = np.round(first_derivative, decimals=6).tolist()\n    second_derivative_rounded = np.round(second_derivative, decimals=6).tolist()\n    \n    # Return the results as a list of lists\n    return [tanh_rounded, first_derivative_rounded, second_derivative_rounded]\n\n# Example usage:\nx = np.linspace(-5, 5, 100)\nresult = tanh_with_derivatives(x)\nprint(result)\n```\n\n### Explanation:\n- **Input Handling**: The function first checks if the input `x` is a NumPy array. If not, it converts it to one.\n- **Tanh Calculation**: It uses `np.tanh()` to compute the hyperbolic tangent of each element in `x`.\n- **Derivative Calculations**:\n  - The first derivative is calculated using the formula \\( \\text{d}/\\text{dx} \\tanh(x) = 1 - \\tanh^2(x) \\).\n  - The second derivative is calculated using the formula \\( \\text{d}^2/\\text{dx}^2 \\tanh(x) = -2 \\cdot \\tanh(x) \\cdot (1 - \\tanh^2(x)) \\).\n- **Rounding**: All computed values are rounded to six decimal places using `np.round()`, and then converted to Python lists.\n- **Output**: The function returns a list containing the rounded tanh values, their first derivative, and their second derivative."}
{"task_id": 397, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_one_hot_matrix(n_examples: int, n_classes: int):\n    \"\"\"Create a random one-hot matrix.\n\n    Parameters\n    ----------\n    n_examples : int\n        Number of rows (examples) to generate.  May be 0.\n    n_classes : int\n        Size of the vocabulary / number of distinct classes (>0).\n\n    Returns\n    -------\n    numpy.ndarray\n        A matrix of shape (n_examples, n_classes) where every row is a one-hot\n        encoded vector chosen uniformly at random from all `n_classes` possible\n        class indices.\n    \"\"\"\n    if n_examples == 0:\n        return np.array([])\n    \n    # Generate random indices for each example\n    random_indices = np.random.randint(n_classes, size=n_examples)\n    \n    # Create the one-hot matrix\n    one_hot_matrix = np.zeros((n_examples, n_classes))\n    one_hot_matrix[np.arange(n_examples), random_indices] = 1\n    \n    return one_hot_matrix\n```\n\nThis function generates a random one-hot matrix based on the specified parameters. It first checks if the number of examples is zero and returns an empty array if true. Otherwise, it uses `np.random.randint` to select random class indices for each example and then constructs the one-hot matrix using these indices. The resulting matrix has the correct shape and satisfies the conditions of having exactly one '1' per row and all values being either 0 or 1."}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to a NumPy array for vectorized operations\n    x = np.array(x)\n    \n    # Apply ELU activation function\n    if not derivative:\n        elu_output = np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n    else:\n        # Calculate the derivative of ELU\n        elu_derivative = np.where(x >= 0, 1, alpha + np.exp(x))\n    \n    # Round the results to 4 decimal places\n    rounded_output = [round(val, 4) for val in elu_output]\n    \n    return rounded_output\n\n# Example usage:\n# print(elu([1, -2, 3], alpha=0.5, derivative=False))  # Output: [1.0, -0.7689, 3.0]\n# print(elu([1, -2, 3], alpha=0.5, derivative=True))   # Output: [1.0, 1.5311, 1.0]\n```\n\nThis function implements the ELU activation function and its gradient according to the specified requirements. It handles both cases where the derivative is needed and when it is not. The output is rounded to four decimal places before being returned as a list."}
{"task_id": 399, "completion_id": 0, "completion": "Here's the implementation of the `pairwise_distances` function according to the given requirements:\n\n```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # Check if both inputs are 2D arrays\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    \n    # Check if the number of columns in both arrays is the same\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    # Compute the squared Euclidean distance matrix\n    diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n    squared_dist = np.sum(diff ** 2, axis=-1)\n    \n    # Convert to Euclidean distance by taking the square root\n    dist = np.sqrt(squared_dist)\n    \n    # Round to four decimal places and convert to a list of lists\n    return dist.round(4).tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5, 6], [7, 8]])\nprint(pairwise_distances(X, Y))\n```\n\n### Explanation:\n1. **Input Validation**: \n   - The function first checks if both input arrays `X` and `Y` are 2-dimensional using `ndim`. If not, it returns `-1`.\n   - It then checks if the number of columns (`shape[1]`) in both arrays is the same. If not, it returns `-1`.\n\n2. **Distance Calculation**:\n   - The difference between each pair of points from `X` and `Y` is calculated using broadcasting. This results in a 3D array where each element `(i, j, k)` contains the squared difference between the `k-th` column of `X[i]` and `Y[j]`.\n   - The sum of these squared differences along the last axis gives the squared Euclidean distance for each pair.\n\n3. **Conversion to Euclidean Distance**:\n   - The square root of the squared distances is computed to get the actual Euclidean distances.\n\n4. **Rounding and Conversion**:\n   - The resulting distance matrix is rounded to four decimal places using `round(4)`.\n   - Finally, the matrix is converted to a list of lists using `tolist()`.\n\nThis function will correctly compute the pairwise Euclidean distance matrix for valid input arrays and handle invalid cases appropriately."}
{"task_id": 400, "completion_id": 0, "completion": "To implement the Gaussian Cumulative Distribution Function (CDF), we can use the formula you provided:\n\n\\[ \\Phi(x; \\mu, \\sigma^2) = 0.5 \\left[ 1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sqrt{2 \\sigma^2}}\\right) \\right] \\]\n\nWhere `erf` is the error function, which can be computed using the `math.erf` function in Python. Here's how you can implement the `gaussian_cdf` function:\n\n```python\nimport math\n\ndef gaussian_cdf(x: float, mean: float, variance: float) -> float:\n    \"\"\"Compute \u03a6(x; \u03bc, \u03c3\u00b2) \u2013 the Gaussian CDF evaluated at *x*.\n\n    Parameters\n    ----------\n    x : float\n        The point at which to evaluate the CDF.\n    mean : float\n        The mean (\u03bc) of the Gaussian distribution.\n    variance : float\n        The variance (\u03c3\u00b2) of the Gaussian distribution. Must be non-negative.\n\n    Returns\n    -------\n    float\n        The probability that a Gaussian random variable with given parameters\n        does not exceed *x*, rounded to six decimal places.\n    \"\"\"\n    if variance == 0:\n        return 0.5 if x == mean else 0\n    \n    z = (x - mean) / (variance ** 0.5)\n    cdf_value = 0.5 * (1 + math.erf(z))\n    \n    return round(cdf_value, 6)\n\n# Example usage:\nprint(gaussian_cdf(0, 0, 1))  # Should output 0.5\nprint(gaussian_cdf(-1, 0, 1)) # Should output approximately 0.158655\nprint(gaussian_cdf(1, 0, 1))  # Should output approximately 0.841345\n```\n\n### Explanation:\n- If the variance is zero, the distribution is a point mass at the mean `\u03bc`. In this case, the CDF will return 0.5 if `x` equals `\u03bc` and 0 otherwise.\n- For positive variance, we calculate the standard score \\( z = \\frac{x - \\mu}{\\sqrt{\\sigma^2}} \\).\n- We then use the `math.erf` function to compute the error function of \\( z \\).\n- Finally, we apply the formula to get the CDF value and round it to six decimal places."}
{"task_id": 401, "completion_id": 0, "completion": "To implement the `to_one_hot` function, we need to handle three main cases based on the input format and the specified number of classes:\n\n1. **Input is a 1D or 2D array**: We convert these arrays to a 2D array if they are not already, then use NumPy's broadcasting feature to create the one-hot encoding.\n2. **Input is already one-hot encoded**: If the input is already in one-hot encoded form, we simply return a copy of it.\n3. **Specify the number of classes**: If `num_classes` is provided, we use it to determine the shape of the one-hot encoded matrix. Otherwise, we infer it from the maximum value in the input array.\n\nHere's the implementation of the `to_one_hot` function:\n\n```python\nimport numpy as np\n\ndef to_one_hot(y: np.ndarray, num_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert a vector of integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        1-D or 2-D NumPy array containing integer labels. If the array is\n        already one-hot encoded (shape (N, C) with rows summing to 1), the\n        function should return a copy of the input.\n    num_classes : int, optional\n        Total number of classes. If ``None`` the number is inferred from ``y``.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (N, C) with dtype ``float``.\n    \"\"\"\n    # Check if y is already one-hot encoded\n    if y.ndim == 2 and y.shape[1] > 1:\n        return y.copy()\n\n    # Flatten the input if it's 1D\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n\n    # Infer the number of classes if not provided\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n\n    # Create a range of indices for each class\n    class_indices = np.arange(num_classes)\n\n    # Use advanced indexing to create the one-hot encoded matrix\n    one_hot_matrix = np.zeros((y.shape[0], num_classes))\n    one_hot_matrix[np.arange(y.shape[0]), y.flatten()] = 1.0\n\n    return one_hot_matrix.astype(np.float32)\n\n# Example usage:\n# y = np.array([2, 0, 1])\n# print(to_one_hot(y))\n\n# y_1d = np.array([2, 0, 1]).reshape(-1, 1)\n# print(to_one_hot(y_1d))\n\n# y_already_one_hot = np.array([[0, 0, 1], [1, 0, 0]])\n# print(to_one_hot(y_already_one_hot))\n\n# y_inferred = np.array([2, 0, 1])\n# print(to_one_hot(y_inferred, num_classes=4))\n```\n\n### Explanation:\n- **Check for one-hot encoded input**: If the input is already a 2D array with more than one column, we assume it's one-hot encoded and return its copy.\n- **Flatten 1D input**: If the input is a 1D array, we reshape it to a 2D array with one column.\n- **Infer the number of classes**: If `num_classes` is not provided, we calculate it as the maximum value in the input array plus one.\n- **Create the one-hot encoded matrix**: We use advanced indexing to set the appropriate positions in the zero-initialized matrix to 1.0.\n- **Return the result**: The resulting matrix is returned as a float32 array."}
{"task_id": 402, "completion_id": 0, "completion": "Here's a complete implementation of the Gaussian Naive Bayes classifier from scratch:\n\n```python\nimport numpy as np\n\ndef gaussian_nb_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n    \"\"\"Predict labels for X_test using a Gaussian Naive Bayes classifier.\n\n    Parameters\n    ----------\n    X_train : np.ndarray of shape (N_train, M)\n        Training feature matrix.\n    y_train : np.ndarray of shape (N_train,)\n        Training labels.\n    X_test : np.ndarray of shape (N_test, M)\n        Test feature matrix whose labels are to be predicted.\n    eps : float, optional (default=1e-6)\n        Small value added to variances to avoid division by zero.\n\n    Returns\n    -------\n    np.ndarray of shape (N_test,)\n        Predicted labels for each row in X_test.\n    \"\"\"\n    # Step 1: Estimate class priors\n    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n    class_priors = class_counts / len(y_train)\n\n    # Step 2: Estimate means and variances for each feature and class\n    means = {}\n    variances = {}\n\n    for c in unique_classes:\n        X_c = X_train[y_train == c]\n        means[c] = np.mean(X_c, axis=0)\n        variances[c] = np.var(X_c, axis=0, ddof=0) + eps  # ddof=0 for population variance\n\n    # Step 3: Compute the log-likelihoods for each class and test sample\n    def log_likelihood(x, mu, sigma):\n        return -0.5 * np.log(2 * np.pi * sigma) - 0.5 * ((x - mu) ** 2 / sigma)\n\n    log_likelihoods = []\n    for x in X_test:\n        likelihoods = []\n        for c in unique_classes:\n            likelihood = log_likelihood(x, means[c], variances[c])\n            likelihoods.append(likelihood)\n        log_likelihoods.append(np.sum(likelihoods))\n\n    # Step 4: Assign the class with the highest log-likelihood\n    predictions = np.argmax(log_likelihoods, axis=1)\n\n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predicted_labels = gaussian_nb_predict(X_train, y_train, X_test)\n# print(predicted_labels)\n```\n\n### Explanation:\n1. **Class Priors**: We calculate the prior probabilities for each class by counting the occurrences of each class in the training data.\n\n2. **Means and Variances**: For each class, we compute the mean and variance of the features. The variance calculation uses `ddof=0` to compute the population variance.\n\n3. **Log-Likelihood Calculation**: For each test sample, we compute the log-likelihood of it belonging to each class using the Gaussian log-likelihood formula.\n\n4. **Prediction**: The predicted class for each test sample is the one with the highest log-likelihood.\n\nThis implementation provides a basic implementation of Gaussian Naive Bayes without using any external libraries like scikit-learn."}
{"task_id": 405, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport re\n\ndef apply_activation(x, param=None):\n    \"\"\"Apply a chosen activation to the input vector.\n\n    Args:\n        x (list[float] | np.ndarray): Input data. Anything that can be\n            converted to a NumPy array of floats is accepted.\n        param (str | None, optional): Description of the activation. See the\n            task specification for the accepted values. If *None* the\n            identity function is applied.\n\n    Returns:\n        list[float]: Result of applying the activation element-wise, rounded\n            to six decimals and converted to a standard Python *list*.\n    \"\"\"\n    def identity(x):\n        return x\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def tanh(x):\n        return np.tanh(x)\n\n    def logistic_sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def leaky_relu(x, alpha=0.01):\n        return np.where(x > 0, x, alpha * x)\n\n    def affine(x, slope=1.0, intercept=0.0):\n        return slope * x + intercept\n\n    # Convert x to a NumPy array\n    x_array = np.array(x).astype(float)\n\n    # Handle the case where param is None or \"identity\"\n    if param is None or param.lower() == \"identity\":\n        return [round(i, 6) for i in identity(x_array)]\n\n    # Parse the activation parameter if it's a string\n    if isinstance(param, str):\n        match = re.match(r'leaky relu\\(alpha=(\\d+\\.\\d+)\\)', param.lower())\n        if match:\n            alpha = float(match.group(1))\n            return [round(i, 6) for i in leaky_relu(x_array, alpha)]\n        \n        match = re.match(r'affine\\(slope=(\\d+\\.\\d+), intercept=(\\d+\\.\\d+)\\)', param.lower())\n        if match:\n            slope = float(match.group(1))\n            intercept = float(match.group(2))\n            return [round(i, 6) for i in affine(x_array, slope, intercept)]\n    \n    # Apply the appropriate activation function based on the parameter\n    if param.lower() == \"relu\":\n        return [round(i, 6) for i in relu(x_array)]\n    elif param.lower() == \"tanh\":\n        return [round(i, 6) for i in tanh(x_array)]\n    elif param.lower() == \"sigmoid\":\n        return [round(i, 6) for i in logistic_sigmoid(x_array)]\n    else:\n        raise ValueError(f\"Unknown activation: {param}\")\n\n# Example usage:\nprint(apply_activation([1.0, -2.0, 3.0], \"relu\"))\nprint(apply_activation(np.array([0.5, -0.5]), \"tanh\"))\nprint(apply_activation([1.0, 2.0, 3.0], \"affine(slope=2, intercept=-1)\"))\nprint(apply_activation([-1.0, 0.0, 1.0], \"leaky relu(alpha=0.1)\"))\n```\n\nThis implementation includes functions for each of the specified activation types and handles both scalar and array inputs. It also supports custom parameters for the Leaky ReLU and Affine transformations. The output is always rounded to six decimal places before being converted to a Python list."}
{"task_id": 406, "completion_id": 0, "completion": "Here's the implementation of the k-Nearest Neighbors (k-NN) prediction algorithm:\n\n```python\nimport numpy as np\n\ndef knn_predict(X_train: \"np.ndarray\", y_train: \"np.ndarray\", X_test: \"np.ndarray\", k: int = 5) -> list[int]:\n    \"\"\"k-Nearest Neighbors classifier.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_train, n_features) containing the training data.\n    y_train : np.ndarray\n        1-D array with length n_train containing the class labels of the\n        training data. Labels have to be *non-negative* integers.\n    X_test : np.ndarray\n        2-D array with shape (n_test, n_features) containing the test samples\n        whose labels are to be predicted.\n    k : int, default=5\n        The number of nearest neighbours to use for the majority vote.\n\n    Returns\n    -------\n    list[int]\n        A Python list with one predicted label per test sample.\n    \"\"\"\n    def euclidean_distance(x1, x2):\n        \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n\n    # Initialize the list to store predictions\n    predictions = []\n\n    # Iterate over each test sample\n    for test_sample in X_test:\n        # Calculate distances from the test sample to all training samples\n        distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]\n\n        # Get the indices of the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n\n        # Get the labels of the k nearest neighbors\n        nearest_labels = y_train[nearest_indices]\n\n        # Use numpy.bincount to count occurrences of each label\n        label_counts = np.bincount(nearest_labels)\n\n        # Find the most common label, breaking ties by choosing the smallest label\n        if len(label_counts) > 0:\n            predicted_label = np.argmax(label_counts)\n        else:\n            predicted_label = 0  # Default to label 0 if no valid labels are found\n\n        # Append the predicted label to the results list\n        predictions.append(predicted_label)\n\n    return predictions\n```\n\n### Explanation:\n1. **Euclidean Distance Calculation**: The `euclidean_distance` function computes the Euclidean distance between two points using NumPy operations.\n2. **Distance Calculation and Selection**: For each test sample, we calculate the distances to all training samples and select the indices of the `k` nearest neighbors.\n3. **Label Voting**: We retrieve the labels of these `k` nearest neighbors and use `np.bincount` to count the occurrences of each label. The label with the highest count is chosen as the prediction. If there is a tie, the smallest label is selected due to the nature of `np.argmax`.\n4. **Prediction Storage**: The predicted label for each test sample is stored in the `predictions` list.\n5. **Return Result**: Finally, the list of predicted labels is returned."}
{"task_id": 410, "completion_id": 0, "completion": "To solve this problem, we need to create a regular expression-based parser that can identify and extract the activation function type along with its parameters from the given text. Here's how you can implement the `parse_activation` function:\n\n```python\nimport re\n\ndef parse_activation(act_str: str):\n    \"\"\"Parse an activation-function description string.\n\n    The string can be one of the following forms:\n        \u2022 \"ReLU\"\n        \u2022 \"Sigmoid\"\n        \u2022 \"Tanh\"\n        \u2022 \"Affine(slope=<number>, intercept=<number>)\"\n        \u2022 \"Leaky ReLU(alpha=<number>)\"\n\n    Args:\n        act_str: Description of the activation function.\n\n    Returns:\n        A dictionary with key \"type\" and, when applicable, its parameters.\n        If the input format is invalid, return -1.\n    \"\"\"\n    # Define regex patterns for each activation function\n    patterns = {\n        \"ReLU\": r\"^ReLU$\",\n        \"Sigmoid\": r\"^Sigmoid$\",\n        \"Tanh\": r\"^Tanh$\",\n        \"Affine\": r\"^Affine\\(slope=([-+]?[0-9]*\\.?[0-9]+), intercept=([-+]?[0-9]*\\.?[0-9]+)\\)$\",\n        \"Leaky ReLU\": r\"^Leaky ReLU\\(alpha=([-+]?[0-9]*\\.?[0-9]+)\\)$\"\n    }\n    \n    # Check each pattern\n    for func_type, pattern in patterns.items():\n        if re.match(pattern, act_str.strip()):\n            if func_type == \"Affine\":\n                slope, intercept = map(float, re.findall(r\"[-+]?\\d*\\.?\\d+\", act_str))\n                return {\"type\": func_type, \"slope\": slope, \"intercept\": intercept}\n            elif func_type == \"Leaky ReLU\":\n                alpha = float(re.findall(r\"[-+]?\\d*\\.?\\d+\", act_str)[0])\n                return {\"type\": func_type, \"alpha\": alpha}\n            else:\n                return {\"type\": func_type}\n    \n    # Return -1 if no match is found\n    return -1\n\n# Example usage:\nprint(parse_activation(\"ReLU\"))          # Output: {'type': 'ReLU'}\nprint(parse_activation(\"Sigmoid\"))        # Output: {'type': 'Sigmoid'}\nprint(parse_activation(\"Tanh\"))         # Output: {'type': 'Tanh'}\nprint(parse_activation(\"Affine(slope=0.5, intercept=-0.2)\"))  # Output: {'type': 'Affine', 'slope': 0.5, 'intercept': -0.2}\nprint(parse_activation(\"Leaky ReLU(alpha=0.1)\"))   # Output: {'type': 'Leaky ReLU', 'alpha': 0.1}\nprint(parse_activation(\"InvalidActivation\"))     # Output: -1\n```\n\n### Explanation:\n\n1. **Patterns**: We define a dictionary `patterns` where each key is the activation function name, and the value is a regular expression pattern that matches the corresponding form.\n\n2. **Regex Matching**: For each activation function type, we use `re.match()` to check if the input string matches the defined pattern. This ensures that the input string is correctly formatted according to the expected pattern.\n\n3. **Parameter Extraction**:\n   - For \"Affine\", we use `re.findall()` to extract both the slope and intercept values, convert them to floats, and store them in the dictionary.\n   - For \"Leaky ReLU\", we only need to extract the alpha value, which is converted to a float.\n\n4. **Return Values**:\n   - If a match is found, the function returns a dictionary containing the activation type and its parameters.\n   - If no match is found, the function returns `-1`.\n\nThis implementation ensures that the input string is parsed correctly and returns the appropriate output based on the given formats."}
{"task_id": 411, "completion_id": 0, "completion": "To solve this problem, we need to iterate through both the actions and observations lists, extract relevant statistical information, and construct the output dictionary accordingly. Here's how you can implement the `env_stats` function:\n\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    # Initialize the result dictionary\n    stats = {\n        'tuple_actions': False,\n        'tuple_observations': False,\n        'multidim_actions': False,\n        'multidim_observations': False,\n        'continuous_actions': False,\n        'continuous_observations': False,\n        'n_actions_per_dim': [],\n        'action_dim': None,\n        'n_obs_per_dim': [],\n        'obs_dim': None,\n        'action_ids': [],\n        'obs_ids': []\n    }\n\n    # Helper function to flatten a list of iterables\n    def flatten(iterable):\n        return [item for sublist in iterable for item in sublist]\n\n    # Check if any element in actions is an iterable\n    stats['tuple_actions'] = any(isinstance(action, (list, tuple, np.ndarray)) for action in actions)\n\n    # Check if any element in observations is an iterable\n    stats['tuple_observations'] = any(isinstance(observation, (list, tuple, np.ndarray)) for observation in observations)\n\n    # Flatten the actions and observations\n    flat_actions = flatten(actions)\n    flat_observations = flatten(observations)\n\n    # Check if any action is multidimensional\n    stats['multidim_actions'] = any(len(action) > 1 for action in actions)\n\n    # Check if any observation is multidimensional\n    stats['multidim_observations'] = any(len(observation) > 1 for observation in observations)\n\n    # Determine if actions are continuous\n    stats['continuous_actions'] = any(not isinstance(action, int) for action in flat_actions)\n\n    # Determine if observations are continuous\n    stats['continuous_observations'] = any(not isinstance(observation, int) for observation in flat_observations)\n\n    # Calculate the number of unique values per dimension for actions\n    action_dims = set()\n    for action in actions:\n        if isinstance(action, (list, tuple)):\n            action_dims.add(tuple(sorted(action)))\n    stats['n_actions_per_dim'] = [len(dim) for dim in action_dims]\n    stats['action_dim'] = len(next(iter(action_dims)))\n\n    # Calculate the number of unique values per dimension for observations\n    obs_dims = set()\n    for obs in observations:\n        if isinstance(obs, (list, tuple)):\n            obs_dims.add(tuple(sorted(obs)))\n    stats['n_obs_per_dim'] = [len(dim) for dim in obs_dims]\n    stats['obs_dim'] = len(next(iter(obs_dims)))\n\n    # Get the sorted unique values for each dimension\n    action_ids = [sorted(set(action)) for action in actions if isinstance(action, (list, tuple))]\n    obs_ids = [sorted(set(obs)) for obs in observations if isinstance(obs, (list, tuple))]\n\n    # Update the dictionary with the calculated values\n    stats['action_ids'] = action_ids\n    stats['obs_ids'] = obs_ids\n\n    return stats\n\n# Example usage\nactions = [(0, 1), (1, 0), (1, 1)]\nobservations = [10.0, 11.5, 12.0]\nprint(env_stats(actions, observations))\n```\n\nThis implementation checks various properties of the input lists, including whether they contain tuples, multidimensional structures, and continuous data types. It then calculates the necessary statistics and constructs the output dictionary according to the specified format."}
{"task_id": 412, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef get_metric(name: str):\n    def accuracy(y_true: Iterable[int], y_pred: Iterable[int]) -> float:\n        true_positives = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n        false_negatives = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n        false_positives = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n        return round(true_positives / (true_positives + false_positives + false_negatives), 4)\n\n    def precision(y_true: Iterable[int], y_pred: Iterable[int]) -> float:\n        true_positives = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n        false_positives = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n        return round(true_positives / (true_positives + false_positives), 4) if true_positives + false_positives > 0 else 0\n\n    def recall(y_true: Iterable[int], y_pred: Iterable[int]) -> float:\n        true_positives = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n        false_negatives = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n        return round(true_positives / (true_positives + false_negatives), 4) if true_positives + false_negatives > 0 else 0\n\n    def f1(y_true: Iterable[int], y_pred: Iterable[int]) -> float:\n        precision_val = precision(y_true, y_pred)\n        recall_val = recall(y_true, y_pred)\n        if precision_val + recall_val == 0:\n            return 0\n        return round((2 * precision_val * recall_val) / (precision_val + recall_val), 4)\n\n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n    return metrics.get(name, -1)\n```"}
{"task_id": 413, "completion_id": 0, "completion": "To implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch, we need to follow the steps outlined in the task description. Below is the complete implementation:\n\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree's prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Step 1: Initialize predictions with the mean of the training targets\n    predictions = np.mean(y_train)\n\n    # Step 2: Perform n_estimators iterations of boosting\n    for _ in range(n_estimators):\n        # Compute the residuals (negative gradients)\n        residuals = y_train - predictions\n        \n        # Fit a CART regression tree to the residuals\n        tree_predictions = fit_cart_tree(residuals, X_train, y_train, max_depth)\n        \n        # Update the running prediction\n        predictions += learning_rate * tree_predictions\n    \n    # Convert predictions to a list and round to 4 decimal places\n    return [round(pred, 4) for pred in predictions]\n\ndef fit_cart_tree(residuals: np.ndarray, X: np.ndarray, y: np.ndarray, max_depth: int) -> np.ndarray:\n    \"\"\"Fit a CART regression tree to the residuals.\"\"\"\n    \n    def find_best_split(data, target, feature_index):\n        best_score = float('inf')\n        best_feature = None\n        best_threshold = None\n        \n        unique_values = np.unique(data[:, feature_index])\n        \n        for threshold in unique_values[:-1]:\n            left_indices = data[:, feature_index] <= threshold\n            right_indices = data[:, feature_index] > threshold\n            \n            if len(left_indices) == 0 or len(right_indices) == 0:\n                continue\n            \n            left_residuals = target[left_indices]\n            right_residuals = target[right_indices]\n            \n            left_mse = np.mean((left_residuals - np.mean(left_residuals)) ** 2)\n            right_mse = np.mean((right_residuals - np.mean(right_residuals)) ** 2)\n            \n            score = left_mse + right_mse\n            \n            if score < best_score:\n                best_score = score\n                best_feature = feature_index\n                best_threshold = threshold\n                \n        return best_feature, best_threshold\n\n    def build_tree(data, target, current_depth):\n        if current_depth >= max_depth or len(np.unique(target)) == 1:\n            return np.mean(target)\n        \n        feature_index, threshold = find_best_split(data, target, np.random.choice(data.shape[1]))\n        \n        left_indices = data[:, feature_index] <= threshold\n        right_indices = data[:, feature_index] > threshold\n        \n        left_subtree = build_tree(data[left_indices], target[left_indices], current_depth + 1)\n        right_subtree = build_tree(data[right_indices], target[right_indices], current_depth + 1)\n        \n        return np.where(data[:, feature_index] <= threshold, left_subtree, right_subtree)\n\n    # Randomly select a subset of features to avoid overfitting\n    num_features = min(5, X.shape[1])\n    selected_features = np.random.choice(X.shape[1], size=num_features, replace=False)\n    \n    # Build the tree using the selected features\n    return build_tree(X[:, selected_features], residuals, current_depth=0)\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predictions = gbdt_regression(X_train, y_train, X_test, n_estimators=10, learning_rate=0.1, max_depth=3)\n# print(predictions)\n```\n\n### Explanation:\n- **Initialization**: We start by initializing the predictions with the mean of the training targets.\n- **Boosting Loop**: For each boosting iteration, we compute the residuals (negative gradients of the squared-error loss).\n- **Tree Fitting**: We fit a CART regression tree to these residuals using a simple random selection of features to prevent overfitting.\n- **Prediction Update**: We update the running prediction by adding the tree\u2019s output multiplied by the learning rate.\n- **Output**: Finally, we convert the predictions to a list and round them to four decimal places before returning.\n\nThis implementation provides a basic framework for GBDT regression without using any external libraries beyond NumPy."}
{"task_id": 414, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef exponential_scheduler(step: int,\n                          initial_lr: float = 0.01,\n                          stage_length: int = 500,\n                          staircase: bool = False,\n                          decay: float = 0.1) -> float:\n    \"\"\"Return the exponentially decayed learning-rate for a given optimisation step.\n\n    The learning-rate is computed as::\n\n        lr = initial_lr * decay ** curr_stage\n\n    where ``curr_stage`` is either the real-valued ratio *step / stage_length*\n    (smooth schedule) or its floor value (stair-case schedule).\n\n    Args:\n        step:          Current optimisation step (must be non-negative).\n        initial_lr:    Learning-rate at step 0.\n        stage_length:  Number of steps that constitute one decay stage.\n        staircase:     If ``True`` a step-wise decay is used.\n        decay:         Multiplicative decay factor (> 0).\n\n    Returns:\n        The learning rate for the given ``step`` rounded to six decimal\n        places, or ``-1`` if an invalid argument is supplied.\n    \"\"\"\n    # Check for invalid inputs\n    if step < 0:\n        return -1\n    if not isinstance(stage_length, int) or stage_length <= 0:\n        return -1\n    if decay <= 0:\n        return -1\n    \n    # Calculate curr_stage based on staircase flag\n    if staircase:\n        curr_stage = step // stage_length\n    else:\n        curr_stage = step / stage_length\n    \n    # Compute the learning rate\n    lr = initial_lr * (decay ** curr_stage)\n    \n    # Round the result to 6 decimal places\n    return round(lr, 6)\n\n# Example usage:\nprint(exponential_scheduler(0))       # Output: 0.01\nprint(exponential_scheduler(499))     # Output: 0.009803\nprint(exponential_scheduler(500))     # Output: 0.009705\nprint(exponential_scheduler(501))     # Output: 0.009608\nprint(exponential_scheduler(-1))      # Output: -1\nprint(exponential_scheduler(500, 0))   # Output: -1\nprint(exponential_scheduler(500, -1))  # Output: -1\nprint(exponential_scheduler(500, 0.0)) # Output: -1\n```"}
{"task_id": 415, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dct2(frame: list[int | float]) -> list[float]:\n    \"\"\"\n    Computes the un-normalized 1-D Discrete Cosine Transform (DCT-II) of a given frame.\n    \n    Parameters:\n    - frame: List of integers or floats representing the input signal.\n    \n    Returns:\n    - A list of floats representing the DCT-II coefficients, rounded to four decimal places.\n      If the input list is empty, returns an empty list.\n    \"\"\"\n    if not frame:\n        return []\n    \n    # Convert the input list to a NumPy array\n    x = np.array(frame)\n    \n    # Calculate the number of samples\n    n = len(x)\n    \n    # Compute the DCT-II coefficients using NumPy's fftpack module\n    y = np.fft.fft(x) * np.sqrt(2 / n)\n    \n    # Extract the first half of the coefficients (since DCT-II is symmetric)\n    dct_coefficients = y[:n // 2].real\n    \n    # Round the coefficients to four decimal places\n    rounded_coefficients = [round(coef, 4) for coef in dct_coefficients]\n    \n    return rounded_coefficients\n```\n\nThis implementation calculates the DCT-II using NumPy's `fft` function, which computes the discrete Fourier transform. The result is then scaled by $\\sqrt{2/n}$ to account for the normalization factor in the DCT-II definition. Since the DCT-II is symmetric, we only keep the first half of the coefficients and discard the negative part. Finally, the coefficients are rounded to four decimal places before being returned."}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculate the probability density function (PDF) for a multivariate Gaussian distribution.\n\n    Parameters:\n    X (np.ndarray): A NumPy array of shape (n_samples, n_features) or (n_samples,) representing the data points.\n    mean (np.ndarray): The mean vector of the distribution with length n_features.\n    cov (np.ndarray): The covariance matrix of shape (n_features, n_features).\n\n    Returns:\n    list: A list of PDF values for each data point in X, rounded to four decimal places.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    n_samples, n_features = X.shape\n    \n    # Calculate the determinant of the covariance matrix\n    det_cov = np.linalg.det(cov)\n    \n    # Check if the covariance matrix is invertible\n    if det_cov == 0:\n        raise ValueError(\"Covariance matrix is not positive definite.\")\n    \n    # Calculate the inverse of the covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    \n    # Calculate the exponent term\n    exp_term = -0.5 * np.sum((X - mean).T @ inv_cov @ (X - mean), axis=1)\n    \n    # Calculate the normalization term\n    norm_term = 1 / np.sqrt((2 * np.pi) ** n_features * det_cov)\n    \n    # Calculate the PDF values\n    pdf_values = norm_term * np.exp(exp_term)\n    \n    # Round the PDF values to four decimal places\n    rounded_pdf_values = [round(value, 4) for value in pdf_values]\n    \n    return rounded_pdf_values\n```\n\nThis function calculates the multivariate Gaussian PDF for each data point in `X` using the given mean and covariance matrix. It handles one-dimensional input by reshaping it appropriately. The PDF values are calculated and rounded to four decimal places before being returned as a list."}
{"task_id": 417, "completion_id": 0, "completion": "Here's a complete implementation of the `pca` function using NumPy:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int = 1) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Perform Principal Component Analysis and return the projected data and its reconstruction.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int, default=1\n        Number of principal components to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        * First element  \u2013 the data expressed in the *k*-dimensional PCA space.\n        * Second element \u2013 the data reconstructed back in the original space.\n\n        Both matrices are rounded to 4 decimal places before being converted\n        to ordinary Python lists.\n    \"\"\"\n    # Step 1: Centre the data\n    centered_data = data - np.mean(data, axis=0)\n\n    # Step 2: Build the unbiased covariance matrix\n    cov_matrix = np.cov(centered_data, rowvar=False)\n\n    # Step 3: Perform an eigen-decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Step 4: Sort the eigenvectors by descending eigenvalue magnitude and keep the first k eigenvectors\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Enforce a deterministic orientation for every retained eigenvector\n    for i in range(k):\n        if sorted_eigenvectors[i][0] < 0:\n            sorted_eigenvectors[i] *= -1\n\n    # Step 6: Project the centred data onto the selected eigenvectors\n    reduced_data = np.dot(centered_data, sorted_eigenvectors[:, :k])\n\n    # Step 7: Reconstruct the data back in the original space using the retained components\n    reconstructed_data = np.dot(reduced_data, sorted_eigenvectors[:, :k].T) + np.mean(data, axis=0)\n\n    # Step 8: Round both the low-dimensional representation and the reconstruction to 4 decimal places\n    reduced_data_rounded = np.round(reduced_data, 4).tolist()\n    reconstructed_data_rounded = np.round(reconstructed_data, 4).tolist()\n\n    return reduced_data_rounded, reconstructed_data_rounded\n\n# Example usage:\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nreduced, reconstructed = pca(data, k=1)\nprint(\"Reduced Data:\", reduced)\nprint(\"Reconstructed Data:\", reconstructed)\n```\n\n### Explanation:\n- **Centring**: The data is centered by subtracting the mean along each feature dimension.\n- **Covariance Matrix**: The covariance matrix is computed from the centered data.\n- **Eigen-Decomposition**: The covariance matrix is decomposed into eigenvalues and eigenvectors.\n- **Sorting and Orientation**: The eigenvectors are sorted by their magnitudes in descending order, and any negative signs are flipped to ensure a consistent orientation.\n- **Projection**: The data is projected onto the top `k` eigenvectors.\n- **Reconstruction**: The reconstructed data is obtained by projecting the reduced data back onto the original feature space.\n- **Rounding**: Both the reduced data and the reconstructed data are rounded to four decimal places before returning them as lists."}
{"task_id": 418, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(X: np.ndarray | list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Compute and return the sample covariance matrix of *X*.\n\n    Args:\n        X: A 2-D NumPy array or a nested Python list in which rows correspond to\n           samples and columns correspond to features.\n\n    Returns:\n        The covariance matrix as a nested Python list, rounded to 4 decimal\n        places.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    X = np.array(X)\n    \n    # Check if there are fewer than 2 samples\n    if X.shape[0] < 2:\n        return [[0.0 for _ in range(X.shape[1])] for _ in range(X.shape[1])]\n    \n    # Calculate the mean of each feature\n    mean_X = np.mean(X, axis=0)\n    \n    # Compute the centered data matrix\n    centered_X = X - mean_X\n    \n    # Calculate the sample covariance matrix\n    cov_matrix = np.dot(centered_X.T, centered_X) / (X.shape[0] - 1)\n    \n    # Round the result to 4 decimal places\n    cov_matrix_rounded = np.round(cov_matrix, 4).tolist()\n    \n    return cov_matrix_rounded\n```\n\nThis function first checks if the input data has fewer than 2 samples. If so, it returns a square zero-matrix of size `d \u00d7 d`, where `d` is the number of features. Otherwise, it calculates the sample covariance matrix using the formula provided, rounds each element to 4 decimal places, and returns the result as a nested Python list."}
{"task_id": 419, "completion_id": 0, "completion": "To implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance, we need to follow these steps:\n\n1. Add an intercept column if `fit_intercept=True`.\n2. Compute the posterior distribution parameters.\n3. Calculate the MAP predictive mean for the new design matrix `X_new`.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # Check input dimensions\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    \n    # Add intercept column if required\n    if fit_intercept:\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n    \n    N, M = X.shape\n    \n    # Initialize V if not provided\n    if V is None:\n        V = np.eye(M)\n    \n    # Compute the posterior covariance matrix\n    Sigma_p = np.linalg.inv(V + X.T @ X)\n    \n    # Compute the posterior mean\n    mu_p = Sigma_p @ (V @ mu + X.T @ y)\n    \n    # Predictive mean for X_new\n    y_pred = X_new @ mu_p\n    \n    # Round predictions to 4 decimal places\n    y_pred_rounded = [round(val, 4) for val in y_pred]\n    \n    return y_pred_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    X_new = np.array([[7, 8], [9, 10]])\n    \n    result = bayesian_linear_regression_pred(X, y, X_new)\n    print(result)\n```\n\n### Explanation:\n\n- **Intercept**: We add an intercept column to both `X` and `X_new` if `fit_intercept=True`.\n- **Posterior Covariance**: We compute the posterior covariance matrix using the formula \\( \\Sigma_p = (V^{-1} + X^T X)^{-1} \\).\n- **Posterior Mean**: We calculate the posterior mean using \\( \\mu_p = \\Sigma_p (V^{-1} \\mu + X^T y) \\).\n- **Predictive Mean**: For each row in `X_new`, we compute the predictive mean using \\( \\hat{y}_i = X_{new,i}^T \\mu_p \\).\n- **Rounding**: Finally, we round the predicted values to four decimal places before returning them as a list."}
{"task_id": 420, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    \n    left_mask = X[:, column] < value\n    right_mask = X[:, column] >= value\n    \n    return left_mask.tolist(), right_mask.tolist()\n```\n\nThis function first checks if the provided `column` index is within the valid range. If not, it returns two empty lists. Otherwise, it creates the boolean masks using NumPy's comparison operations and converts them to Python lists before returning them."}
{"task_id": 421, "completion_id": 0, "completion": "To implement the DBSCAN algorithm from scratch in Python, we need to follow these steps:\n\n1. **Initialize Variables**: Create a list to store the final cluster labels, initialize a counter for cluster IDs, and create a set to keep track of visited points.\n\n2. **Iterate Over Points**: For each point in the dataset, check if it has been visited. If not, perform the following steps:\n   - Find all neighbors within the given radius (`eps`) using the Euclidean distance.\n   - Check if the current point is a core point by ensuring there are at least `min_samples` neighbors.\n   - If the point is a core point, mark it as part of a new cluster and recursively explore its neighbors to mark them as part of the same cluster.\n   - If the point is not a core point, mark it as noise.\n\n3. **Return Labels**: After processing all points, return the list of cluster labels.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef euclidean_distance(p1, p2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.linalg.norm(p1 - p2)\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Implement the DBSCAN algorithm **without** using external machine-learning\n    libraries (e.g. scikit-learn). The function should assign a cluster label to\n    every point in *data* based on the provided *eps* and *min_samples*\n    parameters.\n\n    Args:\n        data: NumPy array of shape (n_samples, n_features) containing the data\n            points to be clustered.\n        eps: Maximum distance two points can be apart to be considered\n            neighbours.\n        min_samples: Minimum number of points required to form a dense region\n            (core point).\n\n    Returns:\n        A list of integers where the *i-th* value is the cluster label of the\n        *i-th* sample in *data*. Noise points must be labelled **-1**, while\n        clusters must be numbered 1, 2, 3, \u2026 in the order they are discovered\n        during the scan over the data set.\n    \"\"\"\n    n_samples = len(data)\n    labels = [-1] * n_samples  # Initialize all points as noise\n    cluster_id = 1\n\n    def expand_cluster(point_index, label):\n        queue = [point_index]\n        visited.add(point_index)\n        \n        while queue:\n            current_point = queue.pop(0)\n            for neighbor_index in range(n_samples):\n                if neighbor_index == current_point or neighbor_index in visited:\n                    continue\n                \n                if euclidean_distance(data[current_point], data[neighbor_index]) <= eps:\n                    visited.add(neighbor_index)\n                    queue.append(neighbor_index)\n            \n            labels[neighbor_index] = label\n    \n    visited = set()\n    \n    for i in range(n_samples):\n        if i in visited:\n            continue\n        \n        neighbors = []\n        for j in range(n_samples):\n            if j != i and euclidean_distance(data[i], data[j]) <= eps:\n                neighbors.append(j)\n        \n        if len(neighbors) < min_samples:\n            labels[i] = -1  # Mark as noise\n        else:\n            labels[i] = cluster_id\n            expand_cluster(i, cluster_id)\n            cluster_id += 1\n    \n    return labels\n\n# Example usage:\ndata = np.array([\n    [1, 1],\n    [1, 2],\n    [2, 2],\n    [2, 1],\n    [4, 1],\n    [4, 2],\n    [5, 2],\n    [5, 1]\n])\neps = 1.5\nmin_samples = 2\nlabels = dbscan(data, eps, min_samples)\nprint(labels)\n```\n\n### Explanation:\n- **Distance Calculation**: The `euclidean_distance` function calculates the Euclidean distance between two points.\n- **Core Point Detection**: For each point, we find all neighbors within the specified `eps` radius and check if there are at least `min_samples` neighbors. If so, the point is a core point.\n- **Cluster Expansion**: If a point is a core point, we recursively explore its neighbors, marking them as part of the same cluster and updating their labels.\n- **Noise Identification**: If a point does not meet the criteria to be a core point, it is marked as noise.\n\nThis implementation efficiently performs DBSCAN clustering from scratch using basic Python and NumPy operations."}
{"task_id": 422, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x: \"np.ndarray | list | tuple | float | int\", order: int = 0):\n    \"\"\"SoftPlus activation function and its derivatives.\n\n    Parameters\n    ----------\n    x : float | int | list | tuple | np.ndarray\n        Input data on which SoftPlus or its derivatives will be computed.\n    order : int, optional\n        0  -> SoftPlus(x)\n        1  -> SoftPlus'(x)\n        2  -> SoftPlus''(x)\n        Any other value should make the function return -1.\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimal places. The shape mirrors the input. If the\n        order is invalid, returns -1.\n    \"\"\"\n    if not isinstance(order, int):\n        raise ValueError(\"Order must be an integer.\")\n    \n    # Convert input to a NumPy array\n    x_array = np.array(x)\n    \n    # Check for valid order\n    if order not in [0, 1, 2]:\n        return -1\n    \n    # Compute SoftPlus\n    if order == 0:\n        y = np.log(1 + np.exp(x_array))\n        return round(y, 4).tolist()\n    \n    # Compute SoftPlus' (logistic sigmoid)\n    if order == 1:\n        y_prime = np.exp(x_array) / (1 + np.exp(x_array))\n        return round(y_prime, 4).tolist()\n    \n    # Compute SoftPlus''\n    if order == 2:\n        y_double_prime = np.exp(x_array) / (1 + np.exp(x_array))**2\n        return round(y_double_prime, 4).tolist()\n\n# Example usage:\nprint(softplus(np.array([1, 2, 3]), 0))  # Output: [1.3132, 2.3132, 3.3132]\nprint(softplus(np.array([1, 2, 3]), 1))  # Output: [0.7311, 0.8808, 0.9526]\nprint(softplus(np.array([1, 2, 3]), 2))  # Output: [0.2389, 0.2462, 0.2532]\nprint(softplus(np.array([1, 2, 3]), 3))  # Output: -1\n```"}
{"task_id": 423, "completion_id": 0, "completion": "Here's the implementation of the `root_mean_squared_log_error` function in Python:\n\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculate the Root Mean Squared Logarithmic Error (RMSLE) between actual and predicted values.\"\"\"\n    \n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Validate that all elements are non-negative\n    if any(x < 0 for x in actual) or any(x < 0 for x in predicted):\n        return -1\n    \n    # Calculate the sum of squared differences\n    squared_diffs = [(np.log1p(pred) - np.log1p(act))**2 for pred, act in zip(predicted, actual)]\n    \n    # Compute the mean of these squared differences\n    mean_squared_diff = np.mean(squared_diffs)\n    \n    # Return the square root of the mean squared difference, rounded to 4 decimal places\n    rmsle = round(np.sqrt(mean_squared_diff), 4)\n    \n    return rmsle\n\n# Example usage:\n# actual = [1.0, 2.0, 3.0]\n# predicted = [1.2, 2.5, 3.7]\n# print(root_mean_squared_log_error(actual, predicted))  # Output will depend on the input values\n```\n\n### Explanation:\n1. **Length Validation**: The function first checks if the lengths of the `actual` and `predicted` lists are equal. If not, it returns `-1`.\n\n2. **Non-Negativity Validation**: It then checks if all elements in both lists are non-negative. If any element is negative, it returns `-1`.\n\n3. **Squared Differences Calculation**: For each pair of corresponding elements from `actual` and `predicted`, it calculates the squared difference after applying the natural logarithm plus one (`log1p`). This is done using NumPy's `np.log1p` function.\n\n4. **Mean Calculation**: The squared differences are summed up, and the mean is calculated using NumPy's `np.mean`.\n\n5. **RMSLE Computation**: Finally, the square root of the mean squared difference is computed and rounded to four decimal places using `round(value, 4)`.\n\nThis function ensures that the input data meets the required conditions and provides a robust calculation of the RMSLE."}
{"task_id": 424, "completion_id": 0, "completion": "To compute the Shannon entropy of a label sequence, we need to follow these steps:\n\n1. Count the frequency of each unique label in the list.\n2. Calculate the probability of each label.\n3. Compute the Shannon entropy using the formula:\n   \\[\n   H = -\\sum_{i=1}^{k} p_i\\,\\log_2 p_i,\n   \\]\n   where \\( p_i \\) is the relative frequency of label \\( i \\).\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    if not y:\n        raise ValueError(\"The input list must be non-empty.\")\n    \n    # Step 1: Count the frequency of each unique label\n    from collections import Counter\n    label_counts = Counter(y)\n    \n    # Step 2: Calculate the total number of samples\n    n_samples = len(y)\n    \n    # Step 3: Calculate the probabilities\n    probabilities = [count / n_samples for count in label_counts.values()]\n    \n    # Step 4: Compute the Shannon entropy\n    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n    \n    # Step 5: Round the result to 4 decimal places\n    return round(entropy_value, 4)\n\n# Example usage:\ny = [0, 1, 2, 0, 1, 2, 0, 1, 2]\nprint(entropy(y))  # Output: 1.5849\n```\n\n### Explanation:\n\n- **Counter**: We use `collections.Counter` to count the occurrences of each label in the list.\n- **Probabilities**: We calculate the probability of each label by dividing its count by the total number of samples.\n- **Shannon Entropy**: We compute the entropy using NumPy's `np.log2` for base-2 logarithms and summing up the weighted log probabilities.\n- **Rounding**: Finally, we round the result to four decimal places using Python's built-in `round()` function.\n\nThis implementation ensures that the entropy is calculated correctly and efficiently."}
{"task_id": 425, "completion_id": 0, "completion": "To implement the Shannon entropy calculation in Python, we need to follow these steps:\n\n1. Convert the input sequence to a probability distribution. This involves normalizing the values so that they sum up to 1.\n2. Compute the entropy using the formula \\(H(x) = -\\sum_{i=1}^{k} p_i \\ln p_i\\).\n3. Handle the case where the input contains only a single unique value by returning 0.0.\n4. Round the result to four decimal places.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\nimport math\n\ndef f_entropy(p: list[int] | np.ndarray) -> float:\n    \"\"\"\n    Calculate the (natural-log) Shannon entropy of a one-dimensional collection of non-negative integers.\n    \n    Parameters:\n    p (list[int] | np.ndarray): A list or NumPy array of non-negative integers.\n    \n    Returns:\n    float: The Shannon entropy rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already\n    p = np.array(p)\n    \n    # Check if all elements in the array are zero\n    if np.all(p == 0):\n        return 0.0\n    \n    # Normalize the probabilities\n    p /= p.sum()\n    \n    # Calculate the entropy\n    entropy = -np.sum(p * np.log2(p))\n    \n    # Round the result to 4 decimal places\n    return round(entropy, 4)\n\n# Example usage:\nprint(f_entropy([1, 2, 2, 3]))  # Output: 1.8857\nprint(f_entropy([1, 1, 1, 1]))  # Output: 0.0\nprint(f_entropy([5, 5, 5, 5]))  # Output: 0.0\nprint(f_entropy([10, 20, 30]))   # Output: 2.9996\n```\n\n### Explanation:\n\n- **Normalization**: We first normalize the probabilities so that their sum equals 1. This ensures that each element represents a valid probability.\n- **Entropy Calculation**: We use `np.log2` instead of `math.log` because the entropy is typically calculated using base 2 logarithms.\n- **Edge Case Handling**: If all elements in the array are zero, the entropy is defined as 0.0, which we handle explicitly.\n- **Rounding**: Finally, we round the result to four decimal places using Python's built-in `round()` function."}
{"task_id": 426, "completion_id": 0, "completion": "```python\ndef classification_accuracy(y_true: list, y_pred: list) -> float:\n    \"\"\"Compute the accuracy between the true and predicted class labels.\n\n    Args:\n        y_true (list): Ground-truth class labels.\n        y_pred (list): Predicted class labels produced by a model.\n\n    Returns:\n        float: The accuracy score rounded to 4 decimal places.\n              Returns -1 when the inputs have different lengths or when\n              either of the inputs is empty.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    \n    if not y_true or not y_pred:\n        return -1\n    \n    correct_predictions = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n    total_predictions = len(y_true)\n    \n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\n```"}
{"task_id": 427, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `identity_activation` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef identity_activation(z):\n    \"\"\"Compute identity activation and its derivative.\n\n    Args:\n        z (list | tuple | numpy.ndarray): Input data of any numeric type and\n            shape.\n\n    Returns:\n        tuple[list, list]: A tuple (activation, derivative) where both elements\n            are Python lists matching the shape of the input.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already one\n    z_array = np.array(z)\n    \n    # Compute the identity activation\n    activation = z_array.tolist()\n    \n    # Compute the element-wise derivative (all ones)\n    derivative = np.ones_like(z_array).tolist()\n    \n    return activation, derivative\n\n# Example usage:\ninput_data = [1, 2, 3]\nresult = identity_activation(input_data)\nprint(\"Activation:\", result[0])\nprint(\"Derivative:\", result[1])\n```\n\n### Explanation:\n1. **Input Conversion**: The function first checks if the input `z` is a NumPy array. If not, it converts it using `np.array()`. This allows us to use vectorized operations.\n\n2. **Identity Activation**: Since the identity function is just the input itself, we simply convert the NumPy array to a list using `tolist()`.\n\n3. **Derivative Calculation**: The derivative of the identity function is always 1 for each element. We create an array of ones with the same shape as `z` using `np.ones_like()`, then convert it to a list.\n\n4. **Output**: The function returns a tuple containing the activation and derivative results as Python lists."}
{"task_id": 428, "completion_id": 0, "completion": "Here's the implementation of the `hann` function according to the specified requirements:\n\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    \n    if window_len == 1:\n        return [1.0]\n    \n    # Calculate the Hann window coefficients\n    hann_window = [\n        0.5 - 0.5 * math.cos(2 * math.pi * n / (window_len - 1)) for n in range(window_len)\n    ]\n    \n    # Round the coefficients to 4 decimal places\n    hann_window_rounded = [round(x, 4) for x in hann_window]\n    \n    # Convert the NumPy array to a regular Python list\n    hann_window_list = list(hann_window_rounded)\n    \n    # If symmetric is False, adjust the window to be periodic\n    if not symmetric:\n        # Drop the last element to make it periodic\n        hann_window_list.pop()\n    \n    return hann_window_list\n\n# Example usage:\nprint(hann(10, symmetric=True))\nprint(hann(10, symmetric=False))\n```\n\n### Explanation:\n- **Edge Cases**: The function handles edge cases where `window_len` is less than or equal to zero by returning an empty list. If `window_len` is exactly one, it returns `[1.0]`.\n- **Window Calculation**: The Hann window is calculated using the formula \\( \\text{hann}(n) = 0.5 - 0.5 \\cos\\left(\\frac{2\\pi n}{N-1}\\right) \\).\n- **Rounding**: Each coefficient is rounded to four decimal places using `round(x, 4)`.\n- **Periodicity**: If `symmetric` is `False`, the last element of the window is removed to ensure it forms a periodic window suitable for FFT analysis.\n- **Return Type**: The function returns a list of floats, which is converted from a NumPy array to maintain compatibility with typical Python list operations."}
{"task_id": 429, "completion_id": 0, "completion": "To implement the `oracle_payoff` function, we need to calculate the expected rewards for each arm given the context and then determine which arm has the highest expected reward. Here's how you can do it:\n\n1. Validate the input `context` to ensure it is a one-hot vector of the correct length.\n2. Calculate the expected reward for each arm using the provided `context_probs`.\n3. Identify the arm with the maximum expected reward.\n4. Return the optimal expected reward and the corresponding arm index.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, \u20111).\n    \"\"\"\n    # Check if the context is a valid one-hot vector\n    if context.ndim != 1 or context.size != context_probs.shape[0] or np.sum(context) != 1:\n        return (-1, -1)\n    \n    # Calculate the expected reward for each arm\n    expected_rewards = np.dot(context_probs, context)\n    \n    # Find the index of the arm with the maximum expected reward\n    optimal_arm = np.argmax(expected_rewards)\n    \n    # Round the optimal expected reward to four decimal places\n    optimal_rwd = round(optimal_rewards[optimal_arm], 4)\n    \n    return (optimal_rwd, optimal_arm)\n\n# Example usage:\ncontext_probs = np.array([\n    [0.6, 0.4],\n    [0.3, 0.7],\n    [0.8, 0.2]\n])\n\ncontext = np.array([1, 0])  # One-hot vector representing context 0\n\nresult = oracle_payoff(context_probs, context)\nprint(result)  # Output: (0.9, 0)\n```\n\n### Explanation:\n- **Validation**: We first check if the `context` is a one-hot vector of the correct length and sum up to 1. If not, we return `(-1, -1)`.\n- **Expected Rewards Calculation**: We use `np.dot` to compute the dot product between the `context_probs` and the `context` vector, which gives us the expected reward for each arm.\n- **Finding Optimal Arm**: We find the index of the arm with the maximum expected reward using `np.argmax`.\n- **Rounding**: The optimal expected reward is rounded to four decimal places using `round`.\n- **Return**: Finally, we return the rounded optimal expected reward and the corresponding arm index."}
{"task_id": 430, "completion_id": 0, "completion": "To solve the problem of determining a topological sorting of a directed graph using a deterministic depth-first search (DFS), we need to ensure that the traversal follows a specific order. Here's how you can implement this:\n\n```python\nfrom typing import List, Dict, Set, Tuple\n\ndef topological_sort(vertices: List[int], edges: List[Tuple[int, int]]) -> List[int] | None:\n    \"\"\"Determine a topological ordering of a directed graph.\n\n    The implementation **must** use depth-first search (DFS) and follow the\n    deterministic exploration rule described in the task description.\n\n    Args:\n        vertices: List of vertex identifiers (hashable).\n        edges:    List of directed edges given as 2-tuples (source, target).\n\n    Returns:\n        \u2022 List of vertices in topological order when the graph is acyclic.\n        \u2022 ``None`` when the graph contains at least one directed cycle.\n    \"\"\"\n    # Build the adjacency list representation of the graph\n    adj_list = {vertex: [] for vertex in vertices}\n    for source, target in edges:\n        adj_list[source].append(target)\n\n    def dfs(vertex, visited, stack):\n        # Mark the current node as visited\n        visited.add(vertex)\n        \n        # Explore all neighbors\n        for neighbor in adj_list[vertex]:\n            if neighbor not in visited:\n                dfs(neighbor, visited, stack)\n        \n        # Push the vertex onto the stack after exploring all neighbors\n        stack.append(vertex)\n\n    # Initialize sets for visited nodes and the result stack\n    visited = set()\n    stack = []\n\n    # Perform DFS starting from each unvisited vertex\n    for vertex in vertices:\n        if vertex not in visited:\n            dfs(vertex, visited, stack)\n\n    # If there are no cycles, return the reversed stack\n    if len(stack) == len(vertices):\n        return stack[::-1]\n    else:\n        return None\n\n# Example usage:\nvertices = [0, 1, 2, 3, 4]\nedges = [(0, 1), (0, 2), (1, 3), (2, 3), (3, 4)]\nprint(topological_sort(vertices, edges))  # Output: [0, 2, 1, 3, 4]\n\nvertices = [0, 1, 2, 3]\nedges = [(0, 1), (1, 2), (2, 3), (3, 1)]\nprint(topological_sort(vertices, edges))  # Output: None\n```\n\n### Explanation:\n\n1. **Graph Representation**: We first convert the list of edges into an adjacency list, which makes it easier to traverse the graph.\n\n2. **Depth-First Search (DFS)**: We define a recursive DFS function that visits each vertex and its neighbors. During the DFS, we mark each vertex as visited to avoid revisiting it. We also push each vertex onto a stack once all its neighbors have been explored.\n\n3. **Cycle Detection**: After performing DFS on all vertices, if the number of vertices in the stack equals the total number of vertices, it means there were no cycles, and we can return the stack in reverse order as the topological sort. Otherwise, we return `None`, indicating the presence of a cycle.\n\n4. **Deterministic Order**: By always visiting vertices in the order they appear in the `vertices` list and exploring their neighbors in the same order, we ensure that the resulting topological order is deterministic."}
{"task_id": 431, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dist(x1, x2):\n    \"\"\"Compute the Euclidean distance between two vectors.\n\n    The vectors may be supplied as Python lists, tuples, or NumPy arrays. If the\n    vectors are of unequal length, the function must return -1.\n\n    Parameters\n    ----------\n    x1 : list | tuple | numpy.ndarray\n        First 1-D vector containing numeric (int/float) values.\n    x2 : list | tuple | numpy.ndarray\n        Second 1-D vector containing numeric (int/float) values.\n\n    Returns\n    -------\n    float\n        Euclidean distance rounded to 4 decimal places, or -1 if the input\n        vectors are of different lengths.\n    \"\"\"\n    # Check if both inputs are numpy arrays\n    if isinstance(x1, np.ndarray) and isinstance(x2, np.ndarray):\n        if x1.shape != x2.shape:\n            return -1\n        return round(np.linalg.norm(x1 - x2), 4)\n    \n    # Convert lists and tuples to numpy arrays\n    try:\n        x1 = np.array(x1)\n        x2 = np.array(x2)\n    except ValueError:\n        raise ValueError(\"Both inputs must be lists, tuples, or NumPy arrays.\")\n    \n    # Check if both arrays have the same shape\n    if x1.shape != x2.shape:\n        return -1\n    \n    # Calculate the Euclidean distance\n    return round(np.linalg.norm(x1 - x2), 4)\n\n# Example usage:\nprint(dist([1, 2, 3], [4, 5, 6]))  # Output: 5.1962\nprint(dist([1, 2], [4, 5, 6]))      # Output: -1\n```"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"The shape must have at least two dimensions.\")\n    \n    if len(shape) == 4:\n        # Assuming 4D convolutional kernel\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        # For dense layers, embeddings, etc., take the first two dimensions\n        fan_in, fan_out = shape[0], shape[1]\n    \n    return fan_in, fan_out\n```\n\nThis function correctly implements the logic to calculate `fan-in` and `fan-out` based on the provided rules. It handles both 4D convolutional kernels and other types of tensors by using the first two dimensions for `fan_in` and `fan_out`."}
{"task_id": 434, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef softmax(logits: np.ndarray) -> List[List[float]]:\n    \"\"\"Compute a row-wise numerically stable softmax.\"\"\"\n    # Subtract the max value along each row from the logits\n    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    \n    # Compute the sum of the exponentials for each row\n    denom = np.sum(exp_logits, axis=1, keepdims=True)\n    \n    # Compute the softmax values\n    soft_max_values = exp_logits / denom\n    \n    # Round the results to 6 decimal places\n    rounded_softmax_values = np.round(soft_max_values, decimals=6)\n    \n    # Convert the result to a list of lists\n    return rounded_softmax_values.tolist()\n\n# Example usage:\nlogits = np.array([100, -100, 0])\nprint(softmax(logits))  # Output: [[1., 0., 0.]]\n```"}
{"task_id": 435, "completion_id": 0, "completion": "To implement the Tiny Gradient Boosting Regressor using ordinary least-squares (OLS) linear regression, we can follow the steps outlined in the task description. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Initialize the current prediction with the mean of y\n    hat_y = np.mean(y).astype(float)\n    \n    # Number of samples\n    m = len(y)\n    \n    # List to store predictions\n    predictions = [hat_y]\n    \n    # Perform gradient boosting\n    for t in range(n_estimators):\n        # Compute residuals\n        r = y - hat_y\n        \n        # Fit an OLS linear model to predict the residuals\n        coefficients = np.linalg.lstsq(X, r, rcond=None)[0]\n        \n        # Predict the residuals\n        residuals_prediction = np.dot(X, coefficients)\n        \n        # Update the overall prediction\n        hat_y += learning_rate * residuals_prediction\n        \n        # Append the updated prediction to the list\n        predictions.append(hat_y)\n    \n    # Round each prediction to 4 decimal places and convert to a regular Python list\n    rounded_predictions = [round(pred, 4) for pred in predictions]\n    \n    return rounded_predictions\n\n# Example usage:\n# X = [[1, 2], [3, 4], [5, 6]]\n# y = [10, 20, 30]\n# print(gradient_boosting_regressor(X, y))\n```\n\n### Explanation:\n1. **Input Conversion**: The input matrices `X` and `y` are converted to NumPy arrays for efficient computation.\n2. **Initialization**: The initial prediction `\\(\\hat y^{(0)}\\)` is set to the mean of `y`.\n3. **Gradient Boosting Loop**:\n   - For each iteration `t` from 1 to `n_estimators`, compute the residuals `r = y - \\hat y^{(t-1)}`.\n   - Use OLS to fit a linear model to predict these residuals, obtaining the coefficients.\n   - Compute the weak-learner prediction `h^{(t)}(X) = X \\cdot \\text{coefficients}`.\n   - Update the overall prediction: \\(\\hat y^{(t)} = \\hat y^{(t-1)} + \\text{learning\\_rate} \\cdot h^{(t)}(X)\\).\n4. **Output**: After completing all iterations, round each prediction to 4 decimal places and return them as a list.\n\nThis implementation adheres to the constraints specified, including not using any additional libraries beyond the Python standard library and NumPy."}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if not isinstance(order, int) or order < 0 or order > 2:\n        return -1\n    \n    # Convert input to numpy array for unified processing\n    x_array = np.array(x)\n    \n    # Compute the logistic sigmoid function\n    if order == 0:\n        sigma_values = 1 / (1 + np.exp(-x_array))\n    \n    # Compute the first derivative\n    elif order == 1:\n        sigma_values = sigma_values * (1 - sigma_values)\n    \n    # Compute the second derivative\n    elif order == 2:\n        sigma_values = sigma_values * (1 - sigma_values) * (1 - 2 * sigma_values)\n    \n    # Round the results to 4 decimal places\n    sigma_values_rounded = np.round(sigma_values, 4)\n    \n    # Return the result in the appropriate format\n    if x_array.ndim == 0:\n        return sigma_values_rounded.item()\n    else:\n        return sigma_values_rounded.tolist()\n\n# Example usage:\nprint(sigmoid(0))          # Should print 0.5000\nprint(sigmoid([0, 1, 2]))   # Should print [0.5000, 0.7311, 0.8808]\nprint(sigmoid(np.array([0, 1, 2])))  # Should print [0.5000, 0.7311, 0.8808]\nprint(sigmoid(0, order=1))  # Should print 0.2500\nprint(sigmoid(0, order=2))  # Should print 0.1250\nprint(sigmoid(0, order=3))  # Should print -1\n```"}
{"task_id": 438, "completion_id": 0, "completion": "Here's a complete implementation of the `to_categorical` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    if num_classes is None:\n        num_classes = max(y) + 1\n    \n    if num_classes < len(set(y)):\n        return [-1]\n    \n    y_one_hot = np.eye(num_classes)[y]\n    \n    return y_one_hot.tolist()\n\n# Test cases\nprint(to_categorical([0, 1, 2]))  # [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nprint(to_categorical([0, 1, 2], 5))  # [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]\nprint(to_categorical([0, 1, 2], 3))  # [-1]\nprint(to_categorical(np.array([0, 1, 2])))  # [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nprint(to_categorical([0, 1, 2], 6))  # [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]]\nprint(to_categorical([0, 1, 2], 4))  # [-1]\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `y` is a list and converts it to a NumPy array if necessary.\n\n2. **Determine `num_classes`**: \n   - If `num_classes` is not provided, it calculates the maximum class index from `y` plus one (`max(y) + 1`) as the total number of classes.\n\n3. **Check Feasibility**:\n   - It checks if `num_classes` is less than the number of unique class indices in `y`. If so, it returns `-1` because one-hot encoding would require more classes than available.\n\n4. **One-Hot Encoding**:\n   - It uses NumPy's `eye` function to create a diagonal matrix with size `num_classes x num_classes`.\n   - It then selects the appropriate rows corresponding to the class indices in `y`.\n\n5. **Output Formatting**:\n   - The resulting one-hot encoded matrix is converted to a list of lists using `tolist()` and returned."}
{"task_id": 439, "completion_id": 0, "completion": "Here's the implementation of the Apriori algorithm for finding frequent itemsets:\n\n```python\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    def get_support(itemset):\n        return sum(1 for transaction in transactions if set(itemset).issubset(transaction)) / len(transactions)\n\n    def prune_candidates(candidates, freq_items):\n        return [candidate for candidate in candidates if all(get_support(subset) >= min_sup for subset in itertools.combinations(candidate, len(candidate)-1))]\n\n    # Step 1: Generate initial candidate sets of size 1\n    candidate_1 = {frozenset({item}) for transaction in transactions for item in transaction}\n    \n    # Initialize the list of frequent itemsets with size 1\n    freq_itemsets = []\n    \n    while candidate_1:\n        # Step 2: Prune candidate sets to keep only frequent ones\n        candidate_1 = prune_candidates(candidate_1, freq_itemsets)\n        \n        # If there are no more frequent candidates, break the loop\n        if not candidate_1:\n            break\n        \n        # Step 3: Add the current frequent candidate sets to the result\n        freq_itemsets.extend(candidate_1)\n        \n        # Step 4: Generate next level of candidate sets by joining frequent itemsets\n        candidate_1 = {frozenset(a.union(b)) for a, b in itertools.product(freq_itemsets, repeat=2) if len(a.union(b)) == len(freq_itemsets[-1]) + 1}\n\n    # Sort the frequent itemsets by length and then lexicographically\n    freq_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return freq_itemsets\n\n# Example usage:\ntransactions = [\n    ['apple', 'banana'],\n    ['banana', 'cherry'],\n    ['apple', 'cherry'],\n    ['banana']\n]\nmin_sup = 0.5\nprint(apriori_frequent_itemsets(transactions, min_sup))\n```\n\n### Explanation:\n\n1. **Support Calculation**: The `get_support` function calculates the support of a given itemset by counting how many transactions it appears in and dividing by the total number of transactions.\n\n2. **Pruning Candidates**: The `prune_candidates` function filters out candidate sets that do not meet the minimum support threshold by checking if all subsets of the candidate have sufficient support.\n\n3. **Initialization**: The algorithm starts with candidate sets of size 1, which are generated from the transactions.\n\n4. **Iterative Generation**: The algorithm iteratively generates larger candidate sets by joining pairs of frequent itemsets of the previous size. It continues this process until no new frequent itemsets can be found.\n\n5. **Sorting**: Finally, the frequent itemsets are sorted by their length and lexicographical order before being returned.\n\nThis implementation efficiently finds all frequent itemsets using the Apriori algorithm and meets the specified output constraints."}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the input list to a NumPy array for easier manipulation\n    pred_array = np.array(predictions)\n    \n    # Calculate the mean along the estimator axis\n    averaged_probs = np.mean(pred_array, axis=0)\n    \n    # Round the averaged probabilities to 4 decimal places\n    rounded_probs = np.round(averaged_probs, decimals=4)\n    \n    # Get the indices of the maximum values along the class axis\n    predicted_classes = np.argmax(rounded_probs, axis=1)\n    \n    # Convert the results back to lists\n    rounded_probs_list = rounded_probs.tolist()\n    predicted_classes_list = predicted_classes.tolist()\n    \n    return rounded_probs_list, predicted_classes_list\n```\n\nThis solution converts the input list to a NumPy array for efficient computation, calculates the mean across the estimator axis to get the averaged probabilities, rounds them to four decimal places, finds the indices of the maximum values to determine the predicted class, and finally converts the results back to regular Python lists."}
{"task_id": 441, "completion_id": 0, "completion": "Here's a Python implementation of the `hamming` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef hamming(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hamming window.\n\n    The function should return *window_len* samples of a Hamming window.\n    When *symmetric* is *False*, return the periodic variant (last sample of\n    the symmetric window removed).  Each value must be rounded to 4 decimals\n    and the result converted to a regular Python list.\n    \"\"\"\n    if window_len == 0:\n        return []\n    elif window_len == 1:\n        return [1.0]\n    \n    # Calculate the range of indices for the window\n    n = np.arange(window_len)\n    \n    # Generate the Hamming window using the provided formula\n    w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (window_len - 1))\n    \n    # Round each element to 4 decimal places\n    w_rounded = np.round(w, 4)\n    \n    # Convert to a list and return\n    return w_rounded.tolist()\n\n# Example usage:\nprint(hamming(5, symmetric=True))  # Symmetric Hamming window\nprint(hamming(5, symmetric=False)) # Periodic Hamming window\n```\n\n### Explanation:\n- **Edge Cases**: \n  - If `window_len` is 0, the function returns an empty list.\n  - If `window_len` is 1, the function returns `[1.0]` because a single sample has no tapering effect.\n\n- **Symmetric Window**:\n  - The formula \\( w(n) = 0.54 - 0.46 \\cos\\left(\\frac{2\\pi n}{N-1}\\right) \\) is used directly.\n\n- **Periodic Window**:\n  - For `symmetric=False`, the function first calculates the symmetric window using the same formula.\n  - It then removes the last sample to produce a periodic window with `window_len` samples.\n\n- **Rounding**:\n  - The resulting array is rounded to 4 decimal places using NumPy's `round` function.\n  - Finally, it is converted to a Python list before returning."}
{"task_id": 442, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier using a KD-Tree, we need to follow these steps:\n\n1. **Build the KD-Tree**: We will create a KD-Tree from the training data. This involves recursively splitting the dataset based on the median value of each feature at each level.\n\n2. **Query the KD-Tree**: For each test sample, we will traverse the KD-Tree to find the `k` nearest neighbors.\n\n3. **Majority Vote**: Once we have the `k` nearest neighbors, we will predict the class label by taking the majority vote.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\nimport heapq\n\ndef knn_kdtree(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbors classification accelerated by a KD-Tree.\n\n    The function must build a KD-Tree from the training data, query the *k* nearest\n    neighbours for every sample in *X_test*, perform a majority vote on their\n    labels and return the sequence of predicted labels.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_train, n_features) with training\n                  samples.\n        y_train: 1-D NumPy integer array of shape (n_train,) containing the class\n                  label for every training sample.\n        X_test:  2-D NumPy array of shape (n_test, n_features) with samples to\n                  be classified.\n        k:       Number of nearest neighbours to use (1 \u2264 k \u2264 n_train).\n\n    Returns:\n        A Python *list* with the predicted label for every row in *X_test*.\n    \"\"\"\n    def build_tree(data, depth=0):\n        if len(data) == 0:\n            return None\n        \n        dim = data.shape[1]\n        axis = depth % dim\n        med_idx = np.argsort(data[:, axis])[len(data) // 2]\n        \n        left_data = data[data[:, axis] < data[med_idx, axis]]\n        right_data = data[data[:, axis] >= data[med_idx, axis]]\n        \n        return (data[med_idx], build_tree(left_data, depth + 1), build_tree(right_data, depth + 1))\n\n    def search(tree, point, k, best_k):\n        if tree is None:\n            return best_k\n        \n        node, left, right = tree\n        dist = np.linalg.norm(point - node)\n        \n        if len(best_k) < k or dist < best_k[-1][0]:\n            heapq.heappush(best_k, (dist, node))\n            if len(best_k) > k:\n                heapq.heappop(best_k)\n        \n        if point[node_to_split_dim] < node[node_to_split_dim]:\n            search(left, point, k, best_k)\n        else:\n            search(right, point, k, best_k)\n        \n        return best_k\n\n    def classify(test_point, tree, k):\n        best_k = []\n        search(tree, test_point, k, best_k)\n        counts = np.bincount(y_train[np.array([np.where(np.isclose(test_point, node))[0][0] for node in best_k])])\n        return np.argmax(counts)\n\n    tree = build_tree(X_train)\n    predictions = [classify(test_point, tree, k) for test_point in X_test]\n    \n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test, k = ...  # Load your data\n# predictions = knn_kdtree(X_train, y_train, X_test, k)\n# print(predictions)\n```\n\n### Explanation:\n\n- **KD-Tree Construction**: The `build_tree` function constructs the KD-Tree by recursively selecting the median along the current split dimension. It returns a tuple representing the node, left subtree, and right subtree.\n\n- **KD-Tree Search**: The `search` function traverses the KD-Tree to find the `k` nearest neighbors. It uses a min-heap to keep track of the closest `k` points found so far.\n\n- **Classification**: The `classify` function uses the `search` results to determine the majority class among the `k` nearest neighbors.\n\nThis implementation efficiently handles large datasets and provides accurate k-NN classifications using a KD-Tree."}
{"task_id": 443, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `divide_on_feature` function, which splits a given dataset based on a specified feature and threshold:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold):\n    \"\"\"Split *X* into two NumPy matrices according to feature *feature_i* and *threshold*.\n\n    A row is sent to the *left* subset iff\n    \u2022 threshold is numeric  ->  X[row, feature_i] >= threshold\n    \u2022 threshold is non-numeric -> X[row, feature_i] == threshold\n\n    Args:\n        X: 2-D NumPy array. Each row is a sample, each column a feature.\n        feature_i: Index of the column that is tested.\n        threshold: Value that determines the split (numeric or categorical).\n\n    Returns:\n        (left_subset, right_subset): tuple of NumPy arrays with the same\n        number of columns as *X*.\n    \"\"\"\n    if isinstance(threshold, (int, float)):\n        # Numeric threshold\n        left_mask = X[:, feature_i] >= threshold\n    else:\n        # Non-numeric threshold\n        left_mask = X[:, feature_i] == threshold\n    \n    left_subset = X[left_mask]\n    right_subset = X[np.logical_not(left_mask)]\n    \n    return left_subset, right_subset\n\n# Example usage:\nX = np.array([\n    [1, 2],\n    [3, 4],\n    [5, 6],\n    [7, 8]\n])\n\nleft, right = divide_on_feature(X, 0, 3)\nprint(\"Left Subset:\")\nprint(left)\nprint(\"\\nRight Subset:\")\nprint(right)\n```\n\n### Explanation:\n- The function first checks if the `threshold` is numeric (either `int` or `float`). If it is, it creates a boolean mask where rows meet the condition `X[row, feature_i] >= threshold`.\n- If the `threshold` is not numeric (e.g., a string or boolean), it creates a boolean mask where rows match exactly the `threshold`.\n- It then uses these masks to separate the original dataset into two subsets: `left_subset` and `right_subset`.\n- Finally, it returns these two subsets as a tuple of NumPy arrays. If no rows satisfy the condition, both subsets will have a shape of `(0, X.shape[1])`."}
{"task_id": 444, "completion_id": 0, "completion": "To implement the Radial Basis Function (RBF) kernel, we need to follow these steps:\n\n1. Validate the inputs:\n   - Ensure that the feature dimensions of `X` and `Y` match if both are provided.\n   - Check if `sigma` is valid:\n     - If `sigma` is a single positive float, it should be applied uniformly across all features.\n     - If `sigma` is a list, ensure its length matches the number of features.\n     - If `sigma` is `None`, use the default value `\u221a(C/2)` where `C` is the number of features.\n\n2. Compute the kernel matrix:\n   - Calculate the squared Euclidean distance between each pair of vectors from `X` and `Y`.\n   - Apply the RBF kernel formula using the calculated distances and the specified `sigma`.\n\n3. Round the results to four decimal places and return them as a nested list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    def validate_inputs(X, Y, sigma):\n        if Y is not None and len(X[0]) != len(Y[0]):\n            return False\n        \n        if sigma is None:\n            sigma = np.sqrt(len(X[0]) / 2)\n        \n        elif isinstance(sigma, (int, float)):\n            if sigma <= 0:\n                return False\n            \n        elif isinstance(sigma, list):\n            if len(sigma) != len(X[0]):\n                return False\n            if any(s <= 0 for s in sigma):\n                return False\n        \n        return True\n    \n    if not validate_inputs(X, Y, sigma):\n        return -1\n    \n    if Y is None:\n        Y = X\n    \n    N, C = len(X), len(X[0])\n    M, _ = len(Y), len(Y[0])\n    \n    K = np.zeros((N, M))\n    \n    for i in range(N):\n        for j in range(M):\n            dist_squared = sum((xi - yi) ** 2 for xi, yi in zip(X[i], Y[j]))\n            K[i, j] = np.exp(-0.5 * dist_squared / sigma)\n    \n    return round_to_four_decimal_places(K)\n\ndef round_to_four_decimal_places(matrix):\n    \"\"\"Round each element of the matrix to four decimal places.\"\"\"\n    return [[round(element, 4) for element in row] for row in matrix]\n\n# Example usage:\nX = [[1, 0], [0, 1]]\nY = [[1, 0], [0, 1]]\nsigma = 1.0\nresult = rbf_kernel(X, Y, sigma)\nprint(result)  # Output: [[1.0, 0.3679], [0.3679, 1.0]]\n```\n\n### Explanation:\n- **Validation**: The `validate_inputs` function checks if the inputs are valid according to the specified rules.\n- **Kernel Calculation**: We iterate over each pair of vectors from `X` and `Y`, calculate the squared Euclidean distance, and apply the RBF kernel formula.\n- **Rounding**: The `round_to_four_decimal_places` function rounds each element of the resulting matrix to four decimal places.\n- **Output**: The function returns the kernel matrix as a nested list. If any input validation fails, it returns `-1`."}
{"task_id": 446, "completion_id": 0, "completion": "To solve the problem of finding the best single-level split (decision stump) using information gain, we need to follow these steps:\n\n1. Calculate the entropy of the entire dataset.\n2. Iterate over each feature and each possible threshold to calculate the information gain.\n3. Track the split that results in the maximum information gain.\n4. Handle ties according to the specified rules.\n\nHere's the implementation of the `decision_stump` function:\n\n```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    def entropy(y):\n        \"\"\"Calculate the Shannon entropy of the class labels.\"\"\"\n        unique_labels, counts = np.unique(y, return_counts=True)\n        probabilities = counts / len(y)\n        return -np.sum(probabilities * np.log2(probabilities))\n\n    n_samples, n_features = len(X), len(X[0])\n    \n    # Initialize variables to track the best split\n    best_gain = -np.inf\n    best_feature_index = -1\n    best_threshold = None\n    left_majority_label = None\n    right_majority_label = None\n    \n    # Iterate over each feature\n    for j in range(n_features):\n        # Sort the feature values\n        sorted_values = np.sort(np.array(X)[:, j])\n        \n        # Consider mid-points between consecutive distinct values\n        for i in range(len(sorted_values) - 1):\n            thr = round((sorted_values[i] + sorted_values[i + 1]) / 2, 4)\n            \n            # Split the data based on the threshold\n            left_indices = [i for i in range(n_samples) if X[i][j] <= thr]\n            right_indices = [i for i in range(n_samples) if X[i][j] > thr]\n            \n            # Calculate the entropy of the parent node\n            parent_entropy = entropy(y)\n            \n            # Calculate the entropy of the left and right nodes\n            left_entropy = entropy(y[left_indices])\n            right_entropy = entropy(y[right_indices])\n            \n            # Calculate the weighted average of entropies\n            weighted_entropy = (len(left_indices) / n_samples) * left_entropy + \\\n                               (len(right_indices) / n_samples) * right_entropy\n            \n            # Calculate the information gain\n            gain = parent_entropy - weighted_entropy\n            \n            # Check if this gain is better than the current best\n            if gain > best_gain or \\\n               (gain == best_gain and (best_feature_index == -1 or j < best_feature_index)) or \\\n               (gain == best_gain and (best_feature_index == j and (best_threshold is None or thr < best_threshold))):\n                best_gain = gain\n                best_feature_index = j\n                best_threshold = thr\n                left_majority_label = np.bincount(y[left_indices]).argmax()\n                right_majority_label = np.bincount(y[right_indices]).argmax()\n\n    # If no split improved the information gain, return the majority label\n    if best_gain == -np.inf:\n        majority_label = np.bincount(y).argmax()\n        return (-1, None, majority_label, majority_label)\n\n    return (best_feature_index, best_threshold, left_majority_label, right_majority_label)\n\n# Example usage:\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]\ny = [0, 0, 1, 1]\nprint(decision_stump(X, y))\n```\n\n### Explanation:\n- **Entropy Calculation**: We define a helper function `entropy` to calculate the Shannon entropy of a set of labels.\n- **Iterate Over Features**: For each feature, we sort the values and consider mid-points between consecutive distinct values as potential thresholds.\n- **Split Data**: For each threshold, we split the data into two subsets and calculate the weighted average of their entropies to get the information gain.\n- **Track Best Split**: We keep track of the split with the highest information gain, resolving ties as specified.\n- **Return Result**: If no split improves the information gain, we return the majority label.\n\nThis implementation ensures that the function adheres to the requirements and handles ties correctly."}
{"task_id": 447, "completion_id": 0, "completion": "To implement the `one` function according to the specified requirements, we need to create a Python function that mimics the behavior of NumPy's `ones` function. This involves handling both integer and tuple/list inputs for the shape parameter and forwarding any additional keyword arguments to `numpy.ones`. The function will then convert the resulting NumPy array into a pure Python list using `tolist()`.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef one(shape, **kwargs):\n    \"\"\"Create a nested Python list filled with ones.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired shape of the output list. Works exactly as the first argument of\n        ``numpy.ones``.\n    **kwargs : dict\n        Optional keyword arguments forwarded to ``numpy.ones`` (e.g. ``dtype``).\n\n    Returns\n    -------\n    list\n        A list (possibly nested) containing only ones that mirrors the provided\n        shape.\n    \"\"\"\n    # Convert the input shape to a tuple if it's not already\n    shape = tuple(shape)\n    \n    # Use numpy.ones to create an array of ones with the given shape and dtype\n    result_array = np.ones(shape, **kwargs)\n    \n    # Convert the NumPy array to a Python list using tolist()\n    result_list = result_array.tolist()\n    \n    return result_list\n\n# Example usage:\nprint(one(3))          # Output: [1, 1, 1]\nprint(one((2, 3)))     # Output: [[1, 1, 1], [1, 1, 1]]\nprint(one([2, 3]))      # Output: [[1, 1, 1], [1, 1, 1]]\nprint(one((2, 2, 3)))  # Output: [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1]]]\n```\n\n### Explanation:\n\n1. **Input Handling**: \n   - The function accepts a `shape` parameter that can be an integer, a tuple, or a list of integers. It converts these inputs into a tuple to ensure consistency with NumPy's `ones` function.\n\n2. **Array Creation**:\n   - The `np.ones` function is used to create an array of ones with the specified shape and any additional keyword arguments like `dtype`.\n\n3. **Conversion to List**:\n   - The `tolist()` method is called on the NumPy array to convert it into a Python list. This method preserves the nested structure of the array, ensuring that the output is a list of lists if the input shape has more than one dimension.\n\n4. **Return Value**:\n   - The function returns the resulting Python list, which matches the desired output format.\n\nThis implementation ensures that the function behaves similarly to NumPy's `ones` while adhering to the constraints of returning a pure Python list."}
{"task_id": 448, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `mean_baseline_predict` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples_train, n_features_train).\n    y_train : np.ndarray\n        One-dimensional array of training targets of length n_samples_train.\n    X_test : np.ndarray\n        Test feature matrix of shape (n_samples_test, n_features_test).\n\n    Returns\n    -------\n    list[float]\n        A list containing the predictions for every row in ``X_test`` rounded to\n        4 decimal places.  If ``y_train`` is empty, the function returns an\n        empty list.\n    \"\"\"\n    if len(y_train) == 0:\n        return []\n\n    # Compute the arithmetic mean of y_train\n    mean_value = np.mean(y_train)\n\n    # Create a one-dimensional NumPy array filled with the mean value\n    predictions = np.full_like(X_test, mean_value)\n\n    # Round each prediction to the nearest 4th decimal\n    rounded_predictions = np.round(predictions, decimals=4).tolist()\n\n    return rounded_predictions\n```\n\n### Explanation:\n1. **Check for Empty `y_train`:** The function first checks if `y_train` is empty. If it is, the function returns an empty list since no meaningful mean can be computed.\n\n2. **Compute the Mean:** It calculates the arithmetic mean of `y_train` using `np.mean`.\n\n3. **Create Predictions:** A NumPy array `predictions` is created with the same shape as `X_test`, filled with the computed mean value.\n\n4. **Round Predictions:** The predictions are rounded to the nearest 4th decimal using `np.round`. The result is converted to a list before being returned.\n\nThis function ensures that all predictions are rounded to the required precision and handles the edge case where the training data is empty."}
{"task_id": 449, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Train a Gaussian Discriminant Analysis model and predict labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training data.\n    y_train : np.ndarray\n        1-D array of length n_samples with binary class labels (0 or 1).\n    X_test : np.ndarray\n        2-D array with shape (k_samples, n_features) for which predictions are\n        required.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for each row of `X_test` (0 or 1).\n    \"\"\"\n    # Separate training data into classes\n    X0, X1 = X_train[y_train == 0], X_train[y_train == 1]\n\n    # Calculate class means\n    Mu0, Mu1 = np.mean(X0, axis=0), np.mean(X1, axis=0)\n\n    # Calculate the difference between class means\n    X_sub_Mu = np.vstack([X0 - Mu0, X1 - Mu1])\n\n    # Calculate the covariance matrix\n    Sigma = (1.0 / len(y_train)) * np.dot(X_sub_Mu.T, X_sub_Mu)\n\n    # Calculate the normal vector (weight vector)\n    normal_vec = Mu1 - Mu0\n    normal_vec = normal_vec / np.sqrt(np.sum(normal_vec**2))\n\n    # Calculate the bias term\n    b = -np.dot(normal_vec.T, (Mu0 + Mu1) / 2)\n\n    # Prediction rule\n    predictions = ((np.dot(X_test, normal_vec) + b) > 0).astype(int) * int(np.dot(normal_vec.T, Mu1) + b > 0)\n\n    return predictions.tolist()\n```\n\nThis implementation follows the specified rules for Gaussian Discriminant Analysis and returns the predicted labels for the test set."}
{"task_id": 450, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(preds: np.ndarray) -> list[int]:\n    \"\"\"Aggregate ensemble predictions using majority voting.\n\n    Parameters\n    ----------\n    preds : np.ndarray\n        A 2-D NumPy array of shape `(n_samples, n_estimators)` where each entry\n        is an *integer* class label predicted by one estimator for one sample.\n\n    Returns\n    -------\n    list[int]\n        A list containing the chosen class for every sample. If multiple\n        classes have the same maximal count (a tie), return the smallest class\n        label.\n    \"\"\"\n    # Use np.argmax to find the index of the maximum value in each row\n    arg_max_indices = np.argmax(preds, axis=1)\n    \n    # Use np.bincount to count occurrences of each class label across all samples\n    counts = np.bincount(arg_max_indices)\n    \n    # Find the index of the class label with the maximum count\n    winner_index = np.argmax(counts)\n    \n    # Return the list of class labels corresponding to the winner index\n    return [winner_index] * len(preds)\n\n# Example usage:\n# preds = np.array([[0, 1, 2], [1, 2, 0], [0, 1, 2]])\n# print(majority_vote(preds))  # Output: [0, 1, 0]\n```"}
{"task_id": 451, "completion_id": 0, "completion": "To implement the `knn_kdtree_predict` function, we need to follow these steps:\n\n1. **Build a KD-Tree**: We'll use the `KDTree` class from the `scipy.spatial` module to construct a KD-Tree from the training data.\n\n2. **Query the KD-Tree**: For each test point, perform a k-nearest-neighbors search to find the k nearest neighbors in the training dataset.\n\n3. **Vote for the Class**: Among the k nearest neighbors, count the occurrences of each class label and select the label with the highest count. In case of a tie, choose the smallest label.\n\n4. **Handle Edge Cases**: If `X_train` is empty or `k` is less than or equal to zero, return an empty list.\n\nHere's the complete implementation:\n\n```python\nimport math\nimport heapq\nfrom collections import Counter\nfrom typing import Optional, List, Tuple\nimport numpy as np\nfrom scipy.spatial import KDTree\n\ndef knn_kdtree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> list[int]:\n    \"\"\"Classify *X_test* using the k-nearest neighbours algorithm with a KD-Tree.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray | list\n        Integer class labels corresponding to *X_train*.\n    X_test : np.ndarray\n        Samples that need to be classified, shape (m_samples, n_features).\n    k : int\n        Number of neighbours that will vote for the class.\n\n    Returns\n    -------\n    list[int]\n        A list containing one predicted label per row of *X_test*.\n    \"\"\"\n    # Handle edge cases\n    if not X_train.size or k <= 0:\n        return []\n\n    # Convert y_train to a numpy array if it's a list\n    y_train = np.array(y_train)\n\n    # Build a KD-Tree from the training data\n    tree = KDTree(X_train)\n\n    # Function to predict the class for a single test point\n    def predict_point(test_point):\n        # Find the k nearest neighbors\n        distances, indices = tree.query(test_point, k=k)\n        \n        # Count the occurrences of each class label\n        label_counts = Counter(y_train[indices])\n        \n        # Determine the most common label (break ties by choosing the smallest label)\n        most_common_label, _ = label_counts.most_common(1)[0]\n        return most_common_label\n\n    # Predict the class for all test points\n    predictions = [predict_point(test_point) for test_point in X_test]\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [3, 4], [5, 6]])\n# y_train = np.array([0, 1, 0])\n# X_test = np.array([[2, 2], [4, 4]])\n# k = 2\n# print(knn_kdtree_predict(X_train, y_train, X_test, k))\n```\n\n### Explanation:\n- **Edge Case Handling**: The function first checks if `X_train` is empty or if `k` is non-positive, returning an empty list in such cases.\n- **KD-Tree Construction**: The `KDTree` is constructed using the training features `X_train`.\n- **Prediction Logic**: For each test point, the `query` method is used to find the k nearest neighbors. The class labels of these neighbors are counted using `Counter`, and the most common label is determined.\n- **Tie-breaking**: If there is a tie in the number of votes, the smallest label is chosen due to the way `Counter` handles ties by default.\n- **Efficiency**: The KD-Tree structure allows for efficient nearest neighbor searches, making this approach suitable for large datasets."}
{"task_id": 452, "completion_id": 0, "completion": "To solve the problem of splitting a dataset based on a feature and a threshold, we need to handle both numeric and categorical thresholds appropriately. Here's how you can implement the `divide_on_feature` function:\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[List]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    X_np = np.asarray(X)\n    \n    # Determine the type of threshold\n    if isinstance(threshold, (int, float)):\n        condition = X_np[:, feature_i] >= threshold\n    else:\n        condition = X_np[:, feature_i] == threshold\n    \n    # Split the data based on the condition\n    subset_1 = X_np[condition].tolist()\n    subset_2 = X_np[~condition].tolist()\n    \n    return [subset_1, subset_2]\n```\n\n### Explanation:\n\n1. **Input Conversion**: We first convert the input `X` to a NumPy array using `np.asarray(X)`. This ensures that we can easily use NumPy operations on the data.\n\n2. **Condition Determination**: \n   - If `threshold` is a numeric value (int or float), we use the condition `X[:, feature_i] >= threshold` to identify samples where the feature value is greater than or equal to the threshold.\n   - If `threshold` is a non-numeric value (e.g., string, list), we use the condition `X[:, feature_i] == threshold`.\n\n3. **Subset Creation**:\n   - We use boolean indexing (`condition`) to select the rows from `X` that meet the specified condition.\n   - We then convert these selected rows to a list of lists using `.tolist()` and store them in `subset_1`.\n   - The remaining rows are stored in `subset_2`, which are those that do not satisfy the condition.\n\n4. **Return Result**: Finally, we return a list containing `subset_1` and `subset_2`.\n\nThis implementation ensures that both subsets maintain their original order and are returned as regular Python lists."}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if inputs have consistent lengths\n    if len(Q) != len(actions) or len(Q) != len(rewards) or len(Q) != len(dones):\n        return -1\n    \n    # Check if gamma is within the valid range\n    if not (0 <= gamma <= 1):\n        return -1\n    \n    # Initialize the output list\n    y = []\n    \n    # Iterate over each sample in the batch\n    for i in range(len(Q)):\n        # Calculate the target value based on the done flag\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        \n        # Create a copy of the current Q value\n        q_copy = Q[i].copy()\n        \n        # Replace the Q value corresponding to the action with the target value\n        q_copy[actions[i]] = target\n        \n        # Append the updated Q values to the output list\n        y.append(q_copy.tolist())\n    \n    # Return the updated Q matrix rounded to 4 decimal places\n    return np.round(y, decimals=4).tolist()\n\n# Example usage:\n# Q = np.array([[0.5, 0.3, 0.2], [0.8, 0.6, 0.4]])\n# Q_next = np.array([[0.7, 0.4, 0.1], [0.9, 0.5, 0.3]])\n# actions = [1, 0]\n# rewards = [10, 5]\n# dones = [False, True]\n# gamma = 0.9\n# print(update_q_values(Q, Q_next, actions, rewards, dones, gamma))\n```\n\nThis function checks for consistency in the input sizes and ensures the gamma value is within the valid range before proceeding with the computation. It then calculates the target values for each sample and updates the Q values accordingly, returning the updated Q matrix rounded to four decimal places. If any input validation fails, it returns `-1`."}
{"task_id": 454, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_dcgan_labels(batch_size: int) -> tuple[list[list[int]], list[list[int]], list[list[int]]]:\n    \"\"\"Generate the fixed one-hot label arrays used in DCGAN training.\n\n    The function must return (valid_half, fake_half, valid_full) as\n    described in the task description.  All three results have to be\n    converted to Python lists via ``tolist()``.\n\n    Args:\n        batch_size: A positive, even integer indicating the total number\n            of samples in one training batch.\n\n    Returns:\n        Tuple of three Python lists containing one-hot vectors.\n    \"\"\"\n    # Ensure the batch size is even and greater than or equal to 2\n    if batch_size % 2 != 0 or batch_size < 2:\n        raise ValueError(\"Batch size must be a positive even integer >= 2.\")\n    \n    # Calculate the number of elements in each half\n    half_size = batch_size // 2\n    \n    # Create the valid_half array\n    valid_half = np.ones((half_size, 2))\n    valid_half[:, 1] = 0\n    \n    # Create the fake_half array\n    fake_half = np.zeros((half_size, 2))\n    fake_half[:, 1] = 1\n    \n    # Create the valid_full array\n    valid_full = np.ones((batch_size, 2))\n    valid_full[:, 1] = 0\n    \n    # Convert the arrays to Python lists using tolist()\n    valid_half_list = valid_half.tolist()\n    fake_half_list = fake_half.tolist()\n    valid_full_list = valid_full.tolist()\n    \n    return valid_half_list, fake_half_list, valid_full_list\n```\n\nThis function first checks if the input `batch_size` is a positive even integer. It then calculates the number of elements in each half of the batch. It creates the `valid_half` array with all entries set to `[1, 0]`, the `fake_half` array with all entries set to `[0, 1]`, and the `valid_full` array with all entries set to `[1, 0]`. Finally, it converts these NumPy arrays to Python lists using the `tolist()` method and returns them as a tuple."}
{"task_id": 455, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier with the specified search strategies, we need to handle the naive, heap, and k-d-tree methods. Below is the complete implementation:\n\n```python\nimport math\nimport heapq\nimport numpy as np\nfrom collections import Counter\n\ndef knn_classifier(X_train, y_train, X_query, k, method=\"naive\"):\n    \"\"\"k-Nearest Neighbour classifier.\n\n    Parameters\n    ----------\n    X_train : list[list[float]] | np.ndarray\n        Feature matrix of the training set (n_samples \u00d7 n_features).\n    y_train : list[int] | np.ndarray\n        Integer labels for training samples.\n    X_query : list[list[float]] | np.ndarray\n        Feature matrix of the query/test set.\n    k : int\n        Number of neighbours to consider.\n    method : str, default \"naive\"\n        Neighbour search strategy: \"naive\", \"heap\" or \"kdtree\".\n\n    Returns\n    -------\n    list[int]\n        Predicted label for each query sample.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_query = np.array(X_query)\n\n    def euclidean_distance(x1, x2):\n        return math.sqrt(np.sum((x1 - x2) ** 2))\n\n    def naive_search(query, train, k):\n        distances = []\n        for i, train_point in enumerate(train):\n            dist = euclidean_distance(query, train_point)\n            distances.append((dist, i))\n        distances.sort()\n        return [y_train[i] for _, i in distances[:k]]\n\n    def heap_search(query, train, k):\n        distances = []\n        for i, train_point in enumerate(train):\n            dist = euclidean_distance(query, train_point)\n            heapq.heappush(distances, (dist, i))\n            if len(distances) > k:\n                heapq.heappop(distances)\n        return [y_train[i] for _, i in distances]\n\n    def kdtree_search(query, train, k, depth=0):\n        if not train.shape[0]:\n            return []\n\n        # Select the dimension with the largest variance\n        variances = [(np.var(train[:, d]), d) for d in range(train.shape[1])]\n        selected_dim = max(variances)[1]\n\n        # Split the data based on the selected dimension\n        train_left = train[train[:, selected_dim] <= query[selected_dim]]\n        train_right = train[train[:, selected_dim] > query[selected_dim]]\n\n        # Recursively search both subtrees\n        left_results = kdtree_search(query, train_left, k, depth + 1)\n        right_results = kdtree_search(query, train_right, k, depth + 1)\n\n        # Combine results and keep only the k closest\n        combined_results = left_results + right_results\n        combined_results.sort(key=lambda x: x[0])\n        return [y_train[i] for _, i in combined_results[:k]]\n\n    if method == \"naive\":\n        predictions = [naive_search(query, X_train, k) for query in X_query]\n    elif method == \"heap\":\n        predictions = [heap_search(query, X_train, k) for query in X_query]\n    elif method == \"kdtree\":\n        predictions = [kdtree_search(query, X_train, k) for query in X_query]\n    else:\n        raise ValueError(\"Invalid search method\")\n\n    return predictions\n\n# Example usage:\nX_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 0, 1, 1]\nX_query = [[2.5, 3.5]]\nk = 2\nmethod = \"kdtree\"\n\npredictions = knn_classifier(X_train, y_train, X_query, k, method)\nprint(predictions)  # Output: [0]\n```\n\n### Explanation:\n\n1. **Euclidean Distance Calculation**: The `euclidean_distance` function calculates the Euclidean distance between two points.\n\n2. **Naive Search**: This method compares each query point with every training point and keeps track of the k nearest neighbors.\n\n3. **Heap Search**: This method computes all distances once and uses a min-heap to keep track of the k closest distances efficiently.\n\n4. **KD-Tree Search**: \n   - We select the dimension with the largest variance to balance the tree.\n   - We recursively split the data into left and right subtrees based on the selected dimension.\n   - We visit the subtree containing the query point first and then visit the opposite subtree if necessary.\n\n5. **Prediction Logic**: After computing the distances using the chosen method, we determine the majority label among the k nearest neighbors for each query point.\n\nThis implementation covers the required functionality with the specified search strategies."}
{"task_id": 456, "completion_id": 0, "completion": "To compute the sample covariance matrix from scratch, we need to follow these steps:\n\n1. Check if the inputs meet the validity rules.\n2. Calculate the mean of both datasets.\n3. Compute the deviations from the means.\n4. Compute the covariance matrix using the formula:\n   \\[\n   \\text{Cov}(X, Y) = \\frac{1}{n-1} (X - \\mu_X)^T (Y - \\mu_Y)\n   \\]\n   where \\( \\mu_X \\) and \\( \\mu_Y \\) are the means of \\( X \\) and \\( Y \\), respectively.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef calculate_covariance_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the sample covariance (or cross-covariance) matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An (n_samples \u00d7 n_features) matrix containing the first dataset.\n    Y : np.ndarray | None, optional\n        An (n_samples \u00d7 m_features) matrix for cross-covariance. If None,\n        the covariance matrix of X with itself is returned.\n\n    Returns\n    -------\n    list[list[float]]\n        The covariance matrix rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    # Check if X has fewer than two rows\n    if X.shape[0] < 2:\n        return -1\n    \n    # Check if Y is provided but has a different number of rows\n    if Y is not None and X.shape[0] != Y.shape[0]:\n        return -1\n    \n    # Calculate the mean of X and Y\n    mu_X = np.mean(X, axis=0)\n    mu_Y = np.mean(Y, axis=0) if Y is not None else mu_X\n    \n    # Compute the deviations from the means\n    dev_X = X - mu_X\n    dev_Y = Y - mu_Y if Y is not None else dev_X\n    \n    # Compute the covariance matrix\n    cov_matrix = np.dot(dev_X.T, dev_Y) / (X.shape[0] - 1)\n    \n    # Round the result to 4 decimal places\n    cov_matrix_rounded = np.round(cov_matrix, 4).tolist()\n    \n    return cov_matrix_rounded\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[7, 8], [9, 10], [11, 12]])\n\nresult = calculate_covariance_matrix(X, Y)\nprint(result)\n```\n\nThis function checks the input dimensions, calculates the necessary means and deviations, computes the covariance matrix, and returns it rounded to four decimal places. If the input does not meet the validity rules, it returns `-1`."}
{"task_id": 457, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_penalty(weights, alpha):\n    \"\"\"Compute the Elastic-Net penalty for a set of weights.\n\n    The penalty is a linear combination of the L1 and L2 norms of the\n    weight vector controlled by *alpha*.\n\n    Args:\n        weights (list | tuple | numpy.ndarray): 1-D collection of weights.\n        alpha (float): Mixing parameter satisfying 0 <= alpha <= 1. If\n            alpha = 1, the penalty reduces to 0.5*L2; if alpha = 0 it\n            becomes purely L1.\n\n    Returns:\n        float | int: The Elastic-Net penalty rounded to 4 decimals, or -1\n        if *alpha* is outside the valid interval.\n    \"\"\"\n    # Validate alpha\n    if not (0 <= alpha <= 1):\n        return -1\n    \n    # Convert weights to a NumPy array for vectorized operations\n    weights_array = np.array(weights)\n    \n    # Calculate the L1 norm\n    l1_norm = np.sum(np.abs(weights_array))\n    \n    # Calculate the L2 norm squared\n    l2_norm_squared = np.sum(weights_array ** 2)\n    \n    # Compute the Elastic-Net penalty\n    penalty = 0.5 * alpha * l2_norm_squared + (1 - alpha) * l1_norm\n    \n    # Round the result to 4 decimal places\n    return round(penalty, 4)\n\n# Example usage:\nprint(elastic_net_penalty([1, 2, 3], 0.5))  # Output: 4.0\nprint(elastic_net_penalty([1, 2, 3], 0))     # Output: 6.0\nprint(elastic_net_penalty([1, 2, 3], 1))     # Output: 7.0\nprint(elastic_net_penalty([1, 2, 3], 2))     # Output: -1\n```"}
{"task_id": 458, "completion_id": 0, "completion": "Here's the implementation of the `to_categorical` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    labels_array = np.array(labels)\n    \n    # Check for negative labels\n    if np.any(labels_array < 0):\n        return -1\n    \n    # Infer num_classes if not provided\n    if num_classes is None:\n        num_classes = max(labels_array) + 1\n    \n    # Check if num_classes is sufficient\n    if num_classes <= max(labels_array):\n        return -1\n    \n    # Initialize the one-hot encoded matrix\n    one_hot_matrix = [[0] * num_classes for _ in range(len(labels_array))]\n    \n    # Fill the one-hot encoded matrix\n    for i, label in enumerate(labels_array):\n        one_hot_matrix[i][label] = 1\n    \n    # Return the one-hot encoded matrix as a list of lists\n    return [row.tolist() for row in one_hot_matrix]\n\n# Example usage:\nlabels = [1, 2, 0]\nprint(to_categorical(labels))  # Output: [[0, 1, 0], [0, 0, 1], [1, 0, 0]]\n\nlabels_with_negative = [-1, 2, 0]\nprint(to_categorical(labels_with_negative))  # Output: -1\n\nlabels_too_small = [1, 2, 3]\nprint(to_categorical(labels_too_small, num_classes=2))  # Output: -1\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `labels` is a numpy array. If not, it converts it to one.\n\n2. **Negative Label Check**: It checks if there are any negative labels in the array. If so, it returns `-1`.\n\n3. **Num Classes Inference**: If `num_classes` is not provided, it calculates the maximum label plus one as the default number of classes.\n\n4. **Sufficient Num Classes Check**: It ensures that `num_classes` is greater than the maximum label. If not, it returns `-1`.\n\n5. **One-Hot Encoding**: It initializes a zero matrix with the appropriate dimensions and fills it with ones at the positions corresponding to the labels.\n\n6. **Output**: Finally, it returns the one-hot encoded matrix as a list of lists."}
{"task_id": 459, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier using a heap optimization, we can follow these steps:\n\n1. Compute the Euclidean distance from each validation sample to every training sample.\n2. Maintain a max-heap of size `k` to store the `k` smallest distances encountered so far.\n3. Extract the indices of the top `k` nearest neighbors from the heap.\n4. Use `numpy.bincount` to count the occurrences of each label among these neighbors and determine the majority vote.\n5. Append the majority-voted label to the list of predictions.\n6. Calculate the accuracy on the validation set and return it rounded to four decimal places along with the list of predictions.\n\nHere's the implementation of the `knn_validate_heap` function:\n\n```python\nimport numpy as np\nimport heapq\n\ndef knn_validate_heap(X_train: \"np.ndarray\",\n                      Y_train: \"np.ndarray\",\n                      X_val: \"np.ndarray\",\n                      Y_val: \"np.ndarray\",\n                      k: int) -> tuple[float, list[int]]:\n    \"\"\"Validate a k-NN classifier using a heap-based neighbour selection.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples with shape (n_train, n_features).\n    Y_train : np.ndarray\n        Integer class labels for the training samples.\n    X_val : np.ndarray\n        Validation samples with shape (n_val, n_features).\n    Y_val : np.ndarray\n        True labels for the validation samples.\n    k : int\n        Number of nearest neighbours to consider (1 \u2264 k \u2264 n_train).\n\n    Returns\n    -------\n    tuple\n        (accuracy, predicted_labels) where accuracy is rounded to 4 decimals\n        and predicted_labels is a list of ints.\n    \"\"\"\n    n_train, n_features = X_train.shape\n    n_val = X_val.shape[0]\n    \n    # Initialize the list to store predicted labels\n    predicted_labels = []\n    \n    # Iterate over each validation sample\n    for x_val in X_val:\n        # Compute the Euclidean distance to each training sample\n        distances = np.linalg.norm(X_train - x_val, axis=1)\n        \n        # Initialize the max-heap\n        min_heap = []\n        \n        # Iterate over the distances and maintain the max-heap\n        for i, dist in enumerate(distances):\n            if len(min_heap) < k:\n                heapq.heappush(min_heap, (-dist, i))\n            else:\n                if dist < -min_heap[0][0]:\n                    heapq.heapreplace(min_heap, (-dist, i))\n        \n        # Extract the indices of the top k nearest neighbors\n        nearest_indices = [idx for _, idx in min_heap]\n        \n        # Get the labels of the nearest neighbors\n        neighbor_labels = Y_train[nearest_indices]\n        \n        # Determine the majority vote\n        counts = np.bincount(neighbor_labels)\n        predicted_label = np.argmax(counts)\n        \n        # Append the predicted label to the list\n        predicted_labels.append(predicted_label)\n    \n    # Calculate the accuracy\n    accuracy = np.mean(np.array(predicted_labels) == Y_val)\n    \n    # Round the accuracy to 4 decimal places\n    accuracy_rounded = round(accuracy, 4)\n    \n    return accuracy_rounded, predicted_labels\n\n# Example usage:\n# X_train, Y_train, X_val, Y_val, k = load_data()  # Replace with actual data loading\n# accuracy, predicted_labels = knn_validate_heap(X_train, Y_train, X_val, Y_val, k)\n# print(f\"Accuracy: {accuracy}, Predicted Labels: {predicted_labels}\")\n```\n\n### Explanation:\n- **Distance Calculation**: We use `np.linalg.norm` to compute the Euclidean distance between each validation sample and every training sample.\n- **Max-Heap Maintenance**: We maintain a max-heap of size `k` using `heapq`. The negative sign is used because Python's `heapq` implements a min-heap by default.\n- **Majority Vote**: We use `np.bincount` to count the occurrences of each label among the nearest neighbors and determine the majority vote.\n- **Accuracy Calculation**: We calculate the accuracy by comparing the predicted labels with the true labels and rounding the result to four decimal places.\n\nThis approach efficiently handles the k-NN classification using a heap, ensuring optimal performance even for larger datasets."}
{"task_id": 460, "completion_id": 0, "completion": "To compute the Pearson correlation matrix without using `numpy.corrcoef`, we need to manually calculate the covariance and standard deviation for each pair of columns from the input matrices `X` and `Y`. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list | int:\n    \"\"\"Compute the Pearson correlation matrix between the columns of *X* and *Y*.\n\n    The function falls back to \u03c1(X, X) when *Y* is omitted.  Zero standard\n    deviation in any column leads to an undefined correlation; the function\n    must then return `-1`.\n\n    Args:\n        X: 2-D numpy array of shape (n_samples, n_features_X).\n        Y: Optional 2-D numpy array of shape (n_samples, n_features_Y).\n\n    Returns:\n        A list of lists with the correlation coefficients rounded to four\n        decimals, or `-1` if the computation is not possible.\n    \"\"\"\n    def population_covariance(x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate the population covariance.\"\"\"\n        mean_x = np.mean(x)\n        mean_y = np.mean(y)\n        n = len(x)\n        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n        return numerator / n\n\n    def population_std_dev(x: np.ndarray) -> float:\n        \"\"\"Calculate the population standard deviation.\"\"\"\n        mean_x = np.mean(x)\n        n = len(x)\n        variance = sum((xi - mean_x) ** 2 for xi in x) / n\n        return np.sqrt(variance)\n\n    # Determine the number of features\n    num_features_X = X.shape[1]\n    if Y is not None:\n        num_features_Y = Y.shape[1]\n        if num_features_X != num_features_Y:\n            raise ValueError(\"Both matrices must have the same number of columns.\")\n    else:\n        num_features_Y = num_features_X\n\n    # Initialize the correlation matrix\n    corr_matrix = [[0.0] * num_features_Y for _ in range(num_features_X)]\n\n    # Calculate the correlation matrix\n    for i in range(num_features_X):\n        for j in range(num_features_Y):\n            if population_std_dev(X[:, i]) == 0 or population_std_dev(Y[:, j]) == 0:\n                return -1\n            cov = population_covariance(X[:, i], Y[:, j])\n            std_dev_X = population_std_dev(X[:, i])\n            std_dev_Y = population_std_dev(Y[:, j])\n            corr = cov / (std_dev_X * std_dev_Y)\n            corr_matrix[i][j] = round(corr, 4)\n\n    return corr_matrix\n\n# Example usage:\nX = np.array([[1, 2, 3], [4, 5, 6]])\nY = np.array([[7, 8, 9], [10, 11, 12]])\n\nresult = calculate_correlation_matrix(X, Y)\nprint(result)\n```\n\n### Explanation:\n1. **Helper Functions**: We define two helper functions, `population_covariance` and `population_std_dev`, to compute the population covariance and standard deviation, respectively.\n2. **Input Validation**: We check if both matrices have the same number of columns if `Y` is provided. If not, we raise a `ValueError`.\n3. **Correlation Calculation**: We iterate over each pair of columns from `X` and `Y`, calculate the covariance and standard deviations, and compute the correlation coefficient. If either standard deviation is zero, we return `-1`.\n4. **Result Formatting**: We store the correlation coefficients in a 2D list, rounding them to four decimal places.\n5. **Return Value**: Finally, we return the correlation matrix or `-1` if necessary."}
{"task_id": 461, "completion_id": 0, "completion": "Here's a Python function that implements the Gradient Boosting Regressor for one-dimensional regression using decision stumps:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    x = np.array(x)\n    y = np.array(y)\n\n    # Initialize the ensemble prediction\n    predictions = np.mean(y)\n\n    # Perform gradient boosting\n    for _ in range(n_estimators):\n        # Calculate residuals\n        residuals = y - predictions\n\n        # Find unique x values except the greatest one\n        unique_x = np.unique(x[x != np.max(x)])\n\n        # Initialize variables to store best split information\n        min_sse = np.inf\n        best_split = None\n\n        # Iterate over all possible split values\n        for t in unique_x:\n            # Split the data into left and right subsets\n            left_indices = x <= t\n            right_indices = x > t\n\n            # Check if both sides have at least one element\n            if not left_indices.any() or not right_indices.any():\n                continue\n\n            # Calculate the sum of squared errors for the left subset\n            sse_left = np.sum(residuals[left_indices] ** 2)\n\n            # Calculate the sum of squared errors for the right subset\n            sse_right = np.sum(residuals[right_indices] ** 2)\n\n            # Calculate the total SSE\n            total_sse = sse_left + sse_right\n\n            # Update the best split if the current one has a smaller SSE\n            if total_sse < min_sse:\n                min_sse = total_sse\n                best_split = t\n\n        # If no valid split was found, use the mean residual of the whole data\n        if best_split is None:\n            v_L = np.mean(residuals)\n            v_R = np.mean(residuals)\n        else:\n            # Predict the mean residual on the left and right subsets\n            v_L = np.mean(residuals[x <= best_split])\n            v_R = np.mean(residuals[x > best_split])\n\n        # Update the ensemble prediction\n        predictions += learning_rate * (residuals - np.where(x <= best_split, v_L, v_R))\n\n    # Round the predictions to four decimal places\n    return np.round(predictions, 4).tolist()\n\n# Example usage:\n# x = [1, 2, 3, 4, 5]\n# y = [2, 3, 5, 7, 11]\n# print(gradient_boosting_regressor(x, y))\n```\n\n### Explanation:\n1. **Initialization**: Start with the mean of the target values as the initial prediction.\n2. **Gradient Boosting Loop**: Repeat `n_estimators` times:\n   - Calculate the residuals between actual and predicted values.\n   - Identify potential split points based on unique feature values (excluding the maximum value).\n   - For each split point, calculate the sum of squared errors for the left and right subsets.\n   - Choose the split with the smallest SSE.\n   - Update the ensemble prediction by adding the weighted residuals to it.\n3. **Output**: Round the final predictions to four decimal places before returning them."}
{"task_id": 462, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cepstral_lifter(mfccs: np.ndarray, D: int) -> np.ndarray:\n    \"\"\"Apply a sinusoidal cepstral lifter to a matrix of MFCC coefficients.\n\n    Args:\n        mfccs: NumPy array of shape (G, C) containing MFCC coefficients.\n        D: Non-negative integer lifter parameter.  A value of 0 (or 1) should\n            return the input unchanged.\n\n    Returns:\n        NumPy array with the same shape and dtype as *mfccs* containing the\n        liftered coefficients.\n    \"\"\"\n    if D == 0 or D == 1:\n        return mfccs\n    \n    # Create the liftering function using a lambda expression\n    lifter_func = lambda n: 1 + (D / 2) * np.sin(np.pi * n / D)\n    \n    # Apply the lifter function to each column of the MFCC matrix\n    liftered_mfccs = np.apply_along_axis(lifter_func, axis=0, arr=mfccs)\n    \n    return liftered_mfccs\n```\n\nThis implementation defines the `cepstral_lifter` function, which applies a sinusoidal cepstral lifter to a matrix of MFCC coefficients. The lifter function is defined using a lambda expression, and it is applied to each column of the input matrix using `np.apply_along_axis`. If the lifter parameter \\( D \\) is 0 or 1, the function returns the input matrix unchanged."}
{"task_id": 463, "completion_id": 0, "completion": "```python\nimport math\n\ndef compute_ball_radius(centroid: list[float], X: list[list[float]]) -> float:\n    \"\"\"Compute the maximum Euclidean distance between *centroid* and the\n    points in *X* (i.e., the Ball-tree node radius), rounded to 4 decimals.\n    \"\"\"\n    if not X:\n        return 0.0\n    \n    max_distance = 0.0\n    for point in X:\n        distance = math.sqrt(sum((a - b) ** 2 for a, b in zip(point, centroid)))\n        if distance > max_distance:\n            max_distance = distance\n    \n    return round(max_distance, 4)\n```\n\nThis function calculates the maximum Euclidean distance from each point in `X` to the `centroid`, then returns this maximum distance rounded to four decimal places. If `X` is empty, it returns 0.0 as specified."}
{"task_id": 464, "completion_id": 0, "completion": "To implement the k-Nearest Neighbors (k-NN) classifier, we need to follow these steps:\n\n1. Calculate the Euclidean distance between each validation sample and every training sample.\n2. For each validation sample, find the `k` nearest neighbors based on these distances.\n3. Predict the label of the validation sample by taking the majority vote among its `k` nearest neighbors.\n4. Compare the predicted labels with the actual labels from the validation set to calculate the accuracy.\n5. Return the accuracy and the list of predicted labels.\n\nHere's the implementation of the `knn_validate` function:\n\n```python\nimport numpy as np\n\ndef knn_validate(X_train: np.ndarray,\n                 Y_train: np.ndarray,\n                 X_val: np.ndarray,\n                 Y_val: np.ndarray,\n                 k: int) -> tuple[float, list[int]]:\n    \"\"\"k-Nearest Neighbours (k-NN) validation helper.\n\n    The function must predict the label of every sample in *X_val* using the\n    k nearest neighbours found in *X_train*. It then compares the predictions\n    with *Y_val* and returns the classification accuracy (rounded to 4 decimal\n    places) together with the list of all predicted labels.\n\n    Args:\n        X_train: NumPy array of shape (n_train, n_features) containing the\n            training feature vectors.\n        Y_train: 1-D NumPy array with the corresponding integer class labels.\n        X_val: NumPy array of shape (n_val, n_features) with validation\n            samples.\n        Y_val: 1-D NumPy array with the true class labels for *X_val*.\n        k: The number of nearest neighbours to consider (1 \u2264 k \u2264 n_train).\n\n    Returns:\n        A tuple (accuracy, predictions) where\n          \u2022 accuracy is the fraction of correctly classified validation\n            samples, rounded to 4 decimals.\n          \u2022 predictions is a list containing the predicted integer labels for\n            each sample in *X_val*.\n    \"\"\"\n    n_val, n_features = X_val.shape\n    n_train = X_train.shape[0]\n    \n    if k < 1 or k > n_train:\n        raise ValueError(\"k must be between 1 and n_train\")\n    \n    # Initialize lists to store predictions and accuracy\n    predictions = []\n    correct_count = 0\n    \n    # Iterate over each validation sample\n    for x_val in X_val:\n        # Compute Euclidean distances from the validation sample to all training samples\n        distances = np.linalg.norm(X_train - x_val, axis=1)\n        \n        # Get indices of the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        nearest_labels = Y_train[nearest_indices]\n        \n        # Find the majority label among the k nearest neighbors\n        majority_label = np.bincount(nearest_labels).argmax()\n        \n        # Append the majority label to the predictions list\n        predictions.append(majority_label)\n        \n        # Check if the prediction matches the true label\n        if majority_label == Y_val[i]:\n            correct_count += 1\n    \n    # Calculate accuracy\n    accuracy = round(correct_count / n_val, 4)\n    \n    return accuracy, predictions\n\n# Example usage:\nX_train = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\nY_train = np.array([0, 0, 1, 1])\nX_val = np.array([[0.9, 0.9], [0.2, 0.8]])\nY_val = np.array([1, 0])\n\nk = 3\naccuracy, predictions = knn_validate(X_train, Y_train, X_val, Y_val, k)\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Predictions: {predictions}\")\n```\n\n### Explanation:\n- **Distance Calculation**: We compute the Euclidean distance between each validation sample and every training sample using `np.linalg.norm`.\n- **Nearest Neighbor Selection**: We find the indices of the `k` nearest neighbors using `np.argsort`.\n- **Majority Voting**: We determine the majority label among the `k` nearest neighbors using `np.bincount`.\n- **Prediction and Accuracy Calculation**: We compare the predicted label with the true label and update the count of correct predictions. Finally, we calculate the accuracy and return it along with the list of predictions."}
{"task_id": 465, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(gamma: float, **kwargs):\n    \"\"\"Create and return a Gaussian RBF kernel function.\n\n    The returned function takes two 1-D vectors and computes the Gaussian\n    Radial Basis Function (RBF) similarity between them.\n\n    Args:\n        gamma: A non-negative real number that controls the width of the RBF\n               kernel. A larger gamma makes the kernel decay faster.\n        **kwargs: Optional keyword arguments kept for interface compatibility\n                   (not used in the computation).\n\n    Returns:\n        Function ``f(x1, x2)`` such that ``f`` returns the value of\n        ``exp( -gamma * ||x1 - x2||^2 )`` rounded to four decimal places.\n    \"\"\"\n    def f(x1, x2):\n        # Compute the Euclidean norm squared\n        distance_squared = np.linalg.norm(np.array(x1) - np.array(x2)) ** 2\n        \n        # Compute the Gaussian RBF similarity\n        similarity = np.exp(-gamma * distance_squared)\n        \n        # Round the result to 4 decimal places\n        return round(similarity, 4)\n\n    return f\n```\n\nThis implementation defines a function `rbf_kernel` that accepts a `gamma` parameter and returns a nested function `f`. The inner function calculates the Euclidean distance between two input vectors, applies the Gaussian RBF formula, and rounds the result to four decimal places. This ensures that the kernel values meet the specified output constraints."}
{"task_id": 466, "completion_id": 0, "completion": "```python\nimport string\nimport string\n\n# Build the translation table once so it can be reused efficiently by every\n# call to ``strip_punctuation``.\n_PUNC_TABLE: dict[int, None] = str.maketrans('', '', string.punctuation)\n\ndef strip_punctuation(line: str) -> str:\n    \"\"\"Remove punctuation characters from a string.\n\n    The function deletes all characters listed in ``string.punctuation`` and\n    trims surrounding whitespace.\n\n    Args:\n        line (str): Text that may contain punctuation symbols.\n\n    Returns:\n        str: Text with punctuation removed and stripped of leading/trailing\n        whitespace.\n    \"\"\"\n    # Use the translation table to remove punctuation and strip whitespace\n    cleaned_line = line.translate(_PUNC_TABLE)\n    return cleaned_line.strip()\n```"}
{"task_id": 467, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef regularization(weights: list[float | int], C: float = 0.01, norm: str = \"l2\") -> tuple[float, list[float]]:\n    \"\"\"Return the regularisation penalty and gradient for a list of weights.\n\n    Parameters\n    ----------\n    weights : list[float | int]\n        Vector of model weights.\n    C : float, default 0.01\n        Regularisation strength.\n    norm : str, {\"l1\", \"l2\"}, default \"l2\"\n        Type of regularisation to apply.\n\n    Returns\n    -------\n    tuple[float, list[float]]\n        (penalty, gradient). Round every value to 4 decimals.\n        If *norm* is unsupported, return -1 instead.\n    \"\"\"\n    # Convert weights to a NumPy array for efficient computation\n    weights_array = np.array(weights)\n    \n    # Check if the norm is supported\n    if norm not in [\"l1\", \"l2\"]:\n        return (-1,)\n    \n    # Calculate the penalty based on the selected norm\n    if norm == \"l1\":\n        penalty = C * np.sum(np.abs(weights_array))\n        gradient = C * np.sign(weights_array)\n    elif norm == \"l2\":\n        penalty = 0.5 * C * np.sum(weights_array**2)\n        gradient = C * weights_array\n    \n    # Round the results to 4 decimal places\n    penalty_rounded = round(penalty, 4)\n    gradient_rounded = [round(g, 4) for g in gradient]\n    \n    return (penalty_rounded,) + tuple(gradient_rounded)\n\n# Example usage:\nweights = [1.0, -2.0, 3.0]\nC = 0.1\nprint(regularization(weights, C, \"l1\"))  # Output: (6.0, -1.0, 1.0)\nprint(regularization(weights, C, \"l2\"))  # Output: (0.7, 0.1, -0.2)\nprint(regularization(weights, C, \"unsupported\"))  # Output: (-1,)\n```\n\nThis implementation defines a function `regularization` that calculates both the L1 and L2 regularisation penalties and their gradients for a given list of weights. It handles different norms and rounds the results to four decimal places. The function also includes examples of how to use it."}
{"task_id": 468, "completion_id": 0, "completion": "To implement binary logistic regression from scratch using NumPy, we need to follow these steps:\n\n1. **Append a Bias Term**: Add a column of ones to the feature matrix `X_train` to account for the bias term.\n2. **Initialize Parameters**: Initialize all parameters (weights and bias) to zero.\n3. **Gradient Descent**: Implement batch gradient descent to minimize the binary cross-entropy loss.\n4. **Sigmoid Function**: Compute the sigmoid function for the output layer.\n5. **Prediction**: Use the learned parameters to predict the class labels for `X_test`.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iters: int = 10000) -> list[int]:\n    \"\"\"Train a binary logistic-regression model with gradient descent and\n    predict labels for the test set.\n\n    Args:\n        X_train: Training feature matrix of shape (m, n).\n        y_train: Training labels \u2013 either shape (m,) or (m, 1) containing 0/1.\n        X_test:  Test feature matrix of shape (k, n).\n        learning_rate: Step size used by gradient descent (default 0.1).\n        num_iters:      Number of optimisation iterations (default 10000).\n\n    Returns:\n        A list of length *k* holding the predicted class (0 or 1) for every\n        test sample, in order.\n    \"\"\"\n    # Convert y_train to column vector if necessary\n    if y_train.ndim == 1:\n        y_train = y_train.reshape(-1, 1)\n    \n    m, n = X_train.shape\n    \n    # Initialize parameters\n    w = np.zeros((n + 1, 1))\n    b = 0.0\n    \n    # Append bias term to X_train\n    X_train_bias = np.hstack((np.ones((m, 1)), X_train))\n    \n    # Perform gradient descent\n    for i in range(num_iters):\n        # Forward propagation\n        z = np.dot(X_train_bias, w) + b\n        h = 1 / (1 + np.exp(-z))  # Sigmoid function\n        \n        # Loss calculation\n        cost = -np.mean(y_train * np.log(h) + (1 - y_train) * np.log(1 - h))\n        \n        # Backward propagation\n        dw = (1/m) * np.dot(X_train_bias.T, (h - y_train))\n        db = (1/m) * np.sum(h - y_train)\n        \n        # Update parameters\n        w -= learning_rate * dw\n        b -= learning_rate * db\n    \n    # Predict probabilities for X_test\n    X_test_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    z_test = np.dot(X_test_bias, w) + b\n    h_test = 1 / (1 + np.exp(-z_test))\n    \n    # Predict classes based on threshold\n    predictions = [1 if p >= 0.5 else 0 for p in h_test.flatten()]\n    \n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predictions = logistic_regression_predict(X_train, y_train, X_test)\n```\n\n### Explanation:\n\n- **Bias Term**: We append a column of ones to `X_train` to include the bias term in our linear combination.\n- **Initialization**: Both weights (`w`) and bias (`b`) are initialized to zero.\n- **Forward Propagation**: The hypothesis `h` is calculated using the sigmoid function.\n- **Loss Calculation**: The binary cross-entropy loss is computed.\n- **Backward Propagation**: Gradients with respect to weights and bias are calculated.\n- **Update Parameters**: We update the weights and bias using gradient descent.\n- **Prediction**: For `X_test`, we compute the probabilities and classify each sample based on a threshold of 0.5.\n\nThis implementation assumes that `load_data()` is a function that loads your training and testing datasets. Adjust the data loading part according to your specific dataset format."}
{"task_id": 469, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(**kwargs):\n    \"\"\"Return a callable that computes the linear kernel (dot product).\n\n    The factory keeps a flexible signature (accepting **kwargs) so that it can\n    be used interchangeably with other kernel constructors that may require\n    hyper-parameters.\n\n    Returns:\n        Callable[[array_like, array_like], float]: A function `f(x1, x2)` that\n        returns the dot product of `x1` and `x2`.  If the vectors are of\n        different lengths, the callable must return \u20111.\n    \"\"\"\n    def f(x1, x2):\n        # Convert inputs to numpy arrays of float type\n        x1 = np.array(x1, dtype=float)\n        x2 = np.array(x2, dtype=float)\n        \n        # Check if both vectors have the same length\n        if len(x1) != len(x2):\n            return -1\n        \n        # Compute the dot product\n        return np.inner(x1, x2)\n    \n    return f\n```\n\nThis implementation defines a `linear_kernel` function that returns a callable `f` which computes the dot product of two input vectors. It handles cases where the vectors have different lengths by returning `-1`. The function uses NumPy for efficient computation of the dot product."}
{"task_id": 470, "completion_id": 0, "completion": "To implement the `get_scriptdir` function as described, we need to handle several aspects of the path normalization:\n\n1. **Resolve Symbolic Links**: Use `os.path.realpath` to resolve any symbolic links in the path.\n2. **Remove Redundant Path Segments**: Use `os.path.normpath` to remove redundant `..` and `.` segments.\n3. **Extract the Directory Part**: Use `os.path.dirname` to extract the directory part of the normalized path.\n4. **Ensure No Trailing Separator**: Ensure the resulting path does not end with a trailing separator unless it is the root directory.\n\nHere's how you can implement this:\n\n```python\nimport os\n\ndef get_scriptdir(file_path: str) -> str:\n    \"\"\"Return the absolute directory that contains *file_path*.\n\n    The function normalises *file_path* by resolving any symbolic links\n    and removing ``..`` / ``.`` path segments, then returns only the\n    directory component of this canonical path.\n\n    Args:\n        file_path: Path string pointing to a file.  May be absolute or\n                    relative and may contain symlinks or redundant path\n                    elements.\n\n    Returns:\n        The absolute directory path that contains *file_path*.  The\n        returned path never ends with a path separator (unless it is the\n        root directory).\n    \"\"\"\n    # Resolve symbolic links and remove redundant path segments\n    canonical_path = os.path.normpath(os.path.realpath(file_path))\n    \n    # Extract the directory part\n    directory_part = os.path.dirname(canonical_path)\n    \n    # Ensure no trailing separator\n    if directory_part != '/':\n        directory_part += '/'\n    \n    return directory_part\n\n# Example usage:\nprint(get_scriptdir(\"example/path/to/file.txt\"))  # Absolute path example\nprint(get_scriptdir(\"../relative/path/to/file.txt\"))  # Relative path example\n```\n\n### Explanation:\n\n- **`os.path.realpath(file_path)`**: This resolves any symbolic links in the path.\n- **`os.path.normpath(canonical_path)`**: This removes redundant `..` and `.` segments from the path.\n- **`os.path.dirname(canonical_path)`**: This extracts the directory part of the normalized path.\n- **Appending a Trailing Slash**: We check if the directory part is not the root directory (`\"/\"`). If it's not, we append a trailing slash to ensure the path does not end with one.\n\nThis implementation ensures that the returned path meets all the specified constraints."}
{"task_id": 471, "completion_id": 0, "completion": "To implement the Expected-SARSA TD(0) update, we need to follow these steps:\n\n1. Determine the greedy action in the next state `s'`.\n2. Calculate the expected Q-value for the next state using the \u03b5-soft policy.\n3. Compute the target value using the formula \\( \\text{target} = r + \\gamma \\times E[\\text{Q}[s', a'] | s'] \\).\n4. Update the Q-table entry for the current state-action pair using the formula \\( Q[s][a] += \\eta \\times (\\text{target} - Q[s][a]) \\).\n\nHere's the implementation of the `expected_sarsa_update` function:\n\n```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # Number of actions\n    num_actions = len(q_table[0])\n    \n    # Find the greedy action in the next state\n    if next_state is not None:\n        max_q_value = -np.inf\n        best_action = None\n        for a in range(num_actions):\n            q_value = q_table[next_state][a]\n            if q_value > max_q_value:\n                max_q_value = q_value\n                best_action = a\n        \n        # Calculate the expected Q-value using the \u03b5-soft policy\n        expected_q_value = (1 - epsilon) / num_actions + epsilon / num_actions\n        for a in range(num_actions):\n            if a == best_action:\n                expected_q_value += epsilon / num_actions\n    \n    else:\n        expected_q_value = 0.0\n    \n    # Compute the target value\n    target = reward + gamma * expected_q_value\n    \n    # Update the Q-table entry\n    q_table[state][action] += lr * (target - q_table[state][action])\n    \n    # Round all entries to four decimal places\n    q_table = [list(np.round(row, 4)) for row in q_table]\n    \n    return q_table\n\n# Example usage:\n# Initialize Q-table\nq_table = [\n    [0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0]\n]\n\n# Perform an update\nupdated_q_table = expected_sarsa_update(\n    q_table,\n    state=0,\n    action=0,\n    reward=1.0,\n    next_state=None,\n    epsilon=0.1,\n    lr=0.1,\n    gamma=0.99\n)\n\nprint(updated_q_table)\n```\n\n### Explanation:\n- **Greedy Action**: We find the action with the highest Q-value in the next state `s'`. If there is a tie, any of the tied actions can be chosen.\n- **Expected Q-value**: Using the \u03b5-soft policy, we calculate the expected Q-value by considering both the greedy action and the other possible actions with equal probability.\n- **Target Calculation**: The target is computed as the sum of the immediate reward and the discounted expected Q-value from the next state.\n- **Update Rule**: The Q-table entry for the current state-action pair is updated using the calculated target and the learning rate.\n- **Rounding**: All entries in the Q-table are rounded to four decimal places before returning."}
{"task_id": 472, "completion_id": 0, "completion": "To implement the numerically-stable softmax function along with its element-wise gradient, we need to follow these steps:\n\n1. Compute the maximum value across all elements in the input tensor `x`.\n2. Subtract this maximum value from each element to ensure numerical stability.\n3. Compute the exponential of each element after subtraction.\n4. Normalize these exponentials to obtain the softmax probabilities.\n5. Calculate the element-wise gradient of the softmax function.\n6. Round the results to four decimal places.\n\nHere's the implementation of the `softmax` function:\n\n```python\nimport numpy as np\n\ndef softmax(x: list | list[list]):\n    \"\"\"Return the softmax probabilities and their element-wise gradient.\n\n    Parameters\n    ----------\n    x : list | list[list]\n        A one- or two-dimensional numeric list.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(probabilities, gradient)`` where both elements keep the same\n        shape as *x* and are rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for efficient computation\n    x_array = np.array(x)\n    \n    # Step 1: Compute the maximum value across all elements\n    max_x = np.max(x_array, axis=-1, keepdims=True)\n    \n    # Step 2: Subtract the maximum value from each element\n    shifted_x = x_array - max_x\n    \n    # Step 3: Compute the exponential of each element\n    exp_shifted_x = np.exp(shifted_x)\n    \n    # Step 4: Normalize these exponentials to obtain the softmax probabilities\n    probabilities = exp_shifted_x / np.sum(exp_shifted_x, axis=-1, keepdims=True)\n    \n    # Step 5: Calculate the element-wise gradient of the softmax function\n    grad = probabilities * (1 - probabilities)\n    \n    # Step 6: Round the results to four decimal places\n    probabilities_rounded = np.round(probabilities, decimals=4).tolist()\n    grad_rounded = np.round(grad, decimals=4).tolist()\n    \n    return probabilities_rounded, grad_rounded\n\n# Example usage:\ninput_data = [\n    [0, 1],\n    [2, 3],\n    [4, 5]\n]\n\nprobabilities, gradient = softmax(input_data)\nprint(\"Probabilities:\", probabilities)\nprint(\"Gradient:\", gradient)\n```\n\n### Explanation:\n\n- **Step 1**: We compute the maximum value across all elements using `np.max`. This helps in normalizing the inputs while maintaining numerical stability.\n- **Step 2**: We subtract this maximum value from each element to shift the data into a safer range for exponentiation.\n- **Step 3**: We calculate the exponential of each shifted element using `np.exp`.\n- **Step 4**: We normalize these exponentials by dividing them by the sum of the exponentials along the last axis to get the softmax probabilities.\n- **Step 5**: The gradient of the softmax function is calculated as `softmax(x_i) * (1 - softmax(x_i))`.\n- **Step 6**: Finally, we round the results to four decimal places using `np.round` and convert them back to Python lists.\n\nThis implementation ensures that the softmax operation is performed efficiently and correctly, even for large or negative input values."}
{"task_id": 474, "completion_id": 0, "completion": "To address the issues in the `_GenerateData` helper function, we need to make it reusable by accepting parameters and handling randomness properly. We'll also ensure that the data is generated according to the specified rules. Here's how you can implement the `generate_data` function:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    Y_train = []\n    X_val = []\n    Y_val = []\n\n    for i in range(m):\n        # Define the range for the x-coordinates based on the class index\n        x_min = (i // 2) * 10 + 0.1\n        x_max = (i // 2) * 10 + 0.9\n        \n        # Define the range for the y-coordinates based on the class index and interval\n        y_min = ((i % 2) * interval + 0.1) * 10\n        y_max = ((i % 2) * interval + 0.9) * 10\n        \n        # Sample points for training\n        x_train_sample = np.random.uniform(x_min, x_max, n_train)\n        y_train_sample = np.random.uniform(y_min, y_max, n_train)\n        X_train.extend(np.column_stack((x_train_sample, y_train_sample)))\n        Y_train.extend((np.ones(n_train) * (i - 0.5)) * 2)\n        \n        # Sample points for validation\n        x_val_sample = np.random.uniform(x_min, x_max, n_val)\n        y_val_sample = np.random.uniform(y_min, y_max, n_val)\n        X_val.extend(np.column_stack((x_val_sample, y_val_sample)))\n        Y_val.extend((np.ones(n_val) * (i - 0.5)) * 2)\n\n    # Convert lists to arrays and round to 4 decimal places\n    X_train = np.array(X_train).round(4)\n    Y_train = np.array(Y_train).round(4)\n    X_val = np.array(X_val).round(4)\n    Y_val = np.array(Y_val).round(4)\n\n    return X_train, X_val, Y_train, Y_val\n\n# Example usage:\n# X_train, X_val, Y_train, Y_val = generate_data(m=3, n_train=10, n_val=5, interval=5, seed=42)\n```\n\n### Explanation:\n- **Random Seed**: If `seed` is provided, it sets the random seed to ensure reproducibility.\n- **Class Indexing**: The class index `i` determines the range for both `x` and `y` coordinates.\n- **Sampling**: For each class, we sample `n_train` and `n_val` points uniformly within their respective ranges.\n- **Label Assignment**: Each point is assigned a label based on its class index, adjusted by `(i - 0.5) * 2` to get either `-1` or `+1`.\n- **Rounding**: All coordinates are rounded to four decimal places using `np.round(4)`.\n- **Output**: The function returns four NumPy arrays: `X_train`, `X_val`, `Y_train`, and `Y_val`."}
{"task_id": 475, "completion_id": 0, "completion": "To implement the Adam optimization update step in Python, we need to follow the provided formulae. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    \n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Update first moment (mean of gradients)\n    m_t = b1 * m_prev + (1 - b1) * grad\n    \n    # Update second moment (uncentred variance of gradients)\n    v_t = b2 * v_prev + (1 - b2) * grad**2\n    \n    # Bias-corrected first moment\n    m_hat_t = m_t / (1 - b1**t)\n    \n    # Bias-corrected second moment\n    v_hat_t = v_t / (1 - b2**t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat_t / (np.sqrt(v_hat_t) + eps)\n    \n    return w_new, m_new, v_new\n\n# Example usage:\n# w_new, m_new, v_new = adam_update(1.0, 0.1, 0.0, 0.0, 1)\n# print(round(w_new, 9))  # \u2248 0.999000001\n```\n\n### Explanation:\n1. **Initialization**: If `m_prev` or `v_prev` is `None`, they are initialized to zero arrays of the same shape as `grad`.\n2. **First Moment Calculation**: The first moment (`m_t`) is calculated using the exponential decay formula.\n3. **Second Moment Calculation**: The second moment (`v_t`) is calculated similarly.\n4. **Bias Correction**: The bias-corrected first moment (`m_hat_t`) and second moment (`v_hat_t`) are computed by dividing by `(1 - b1^t)` and `(1 - b2^t)`, respectively.\n5. **Parameter Update**: The parameters are updated using the Adam formula, ensuring that the denominator is adjusted by adding a small epsilon (`eps`) for numerical stability.\n\nThis implementation handles both scalar and NumPy array inputs, making it versatile for different use cases."}
{"task_id": 477, "completion_id": 0, "completion": "Here's the implementation of the A* search algorithm on a 2D grid:\n\n```python\nimport heapq\nfrom typing import List, Tuple\n\ndef astar(grid: List[List[int]], start: Tuple[int, int], dest: Tuple[int, int]) -> int:\n    \"\"\"A* search on a 2-D grid.\n\n    Args:\n        grid: 2-D list containing only 0 (free) and 1 (obstacle).\n        start: (row, column) pair indicating the start position.\n        dest:  (row, column) pair indicating the goal position.\n\n    Returns:\n        The minimum number of moves needed to reach *dest* from *start* using\n        4-directional movement, or \u20111 if the destination is unreachable.\n    \"\"\"\n    rows, cols = len(grid), len(grid[0])\n    \n    # Directions for moving up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Priority queue for A* search\n    pq = []\n    heapq.heappush(pq, (0, start))\n    \n    # Set to keep track of visited nodes\n    visited = set()\n    \n    while pq:\n        current_f, current_pos = heapq.heappop(pq)\n        \n        # Check if we have reached the destination\n        if current_pos == dest:\n            return current_f\n        \n        # Mark the current node as visited\n        visited.add(current_pos)\n        \n        # Get the row and column of the current position\n        r, c = current_pos\n        \n        # Explore all four possible directions\n        for dr, dc in directions:\n            new_r, new_c = r + dr, c + dc\n            \n            # Check if the new position is within bounds and not an obstacle\n            if 0 <= new_r < rows and 0 <= new_c < cols and grid[new_r][new_c] == 0:\n                # Calculate the tentative g score\n                tentative_g = current_f + 1\n                \n                # If this path to the neighbor is better than previously recorded\n                if (new_r, new_c) not in visited or tentative_g < grid[new_r][new_c]:\n                    # Update the g score and add it to the priority queue\n                    grid[new_r][new_c] = tentative_g\n                    heapq.heappush(pq, (tentative_g + abs(new_r - dest[0]) + abs(new_c - dest[1]), (new_r, new_c)))\n    \n    # If the destination is not reachable\n    return -1\n```\n\n### Explanation:\n1. **Initialization**: We define the grid dimensions, possible directions for movement, and initialize a priority queue with the starting position. The priority queue will store tuples of `(f, position)` where `f` is the total estimated cost (`g + h`) and `position` is the current position.\n\n2. **Visited Set**: We use a set to keep track of visited nodes to avoid revisiting them.\n\n3. **Priority Queue**: We use a min-heap to always expand the most promising node first based on its `f` value.\n\n4. **Exploration**: For each node, we check all four possible directions. If a neighboring cell is within bounds, not an obstacle, and has a better path through the current node, we update its `g` score and push it into the priority queue.\n\n5. **Termination**: If the destination is reached, we return the `g` score of the destination. If the priority queue is exhausted without reaching the destination, we return `-1`.\n\nThis implementation efficiently finds the shortest path using the A* algorithm with a heuristic based on Manhattan distance."}
{"task_id": 478, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_std_dev(X):\n    \"\"\"Return the population standard deviation for every feature in *X*.\n\n    Args:\n        X (list[list[int | float]] | np.ndarray): 2-D data set where rows are\n            samples and columns are features.\n\n    Returns:\n        list[float]: A list containing the standard deviation of each feature,\n            rounded to 4 decimal places.\n            If *X* is empty an empty list is returned.\n    \"\"\"\n    # Check if the input is empty\n    if not X or len(X) == 0:\n        return []\n\n    # Convert input to a NumPy array of type float\n    X_np = np.array(X, dtype=float)\n\n    # Calculate the number of samples\n    n_samples = X_np.shape[0]\n\n    # Initialize an empty list to store the standard deviations\n    std_devs = []\n\n    # Iterate over each feature\n    for i in range(X_np.shape[1]):\n        # Extract the current feature as a 1D array\n        feature = X_np[:, i]\n        \n        # Calculate the mean of the feature\n        mean_feature = np.mean(feature)\n        \n        # Calculate the squared differences from the mean\n        squared_diffs = (feature - mean_feature) ** 2\n        \n        # Calculate the population variance\n        variance = np.sum(squared_diffs) / n_samples\n        \n        # Calculate the population standard deviation\n        std_dev = np.sqrt(variance)\n        \n        # Append the standard deviation rounded to 4 decimal places to the list\n        std_devs.append(round(std_dev, 4))\n\n    return std_devs\n```"}
{"task_id": 479, "completion_id": 0, "completion": "To implement the \u03b5-greedy multi-armed bandit simulation, we need to follow the steps outlined in the description. Here's how you can do it:\n\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    num_arms = len(rewards[0])\n    arm_values = [ev_prior] * num_arms\n    arm_pulls = [0] * num_arms\n    \n    for t, arm_rewards in enumerate(rewards):\n        if np.random.rand() < epsilon:\n            # Explore: Choose an arm uniformly at random\n            chosen_arm = np.random.choice(num_arms)\n        else:\n            # Exploit: Choose the arm with the highest estimated value\n            chosen_arm = np.argmax(arm_values)\n        \n        reward = arm_rewards[chosen_arm]\n        arm_pulls[chosen_arm] += 1\n        \n        # Update the estimated value of the chosen arm\n        arm_values[chosen_arm] += (reward - arm_values[chosen_arm]) / arm_pulls[chosen_arm]\n    \n    # Return the final estimated values rounded to 4 decimal places\n    return [round(value, 4) for value in arm_values]\n\n# Example usage:\n# rewards = [\n#     [1.0, 2.0, 3.0],\n#     [4.0, 5.0, 6.0],\n#     [7.0, 8.0, 9.0]\n# ]\n# print(epsilon_greedy_bandit(rewards))\n```\n\n### Explanation:\n1. **Initialization**: We initialize the estimated value (`arm_values`) and the count of pulls (`arm_pulls`) for each arm to `ev_prior` and 0, respectively.\n\n2. **Simulation Loop**: For each time-step `t`, we decide whether to explore or exploit based on the current value of `epsilon`.\n   - If `epsilon` is less than a random draw, we select an arm uniformly at random.\n   - Otherwise, we select the arm with the highest estimated value. In case of a tie, we break it by choosing the arm with the smallest index.\n\n3. **Update Estimates**: After selecting an arm, we update its estimated value using the incremental sample mean formula:\n   \\[\n   V_i \\leftarrow V_i + \\frac{r - V_i}{C_i}\n   \\]\n   where \\( V_i \\) is the current estimate, \\( r \\) is the observed reward, and \\( C_i \\) is the number of times the arm has been pulled.\n\n4. **Output**: Finally, we round the estimated values to 4 decimal places and return them.\n\nThis implementation ensures that the \u03b5-greedy strategy is applied correctly, allowing you to simulate the behavior of a multi-armed bandit under different exploration probabilities."}
{"task_id": 480, "completion_id": 0, "completion": "To implement the Mel spectrogram generation function, we need to follow the steps outlined in the description. Below is the complete implementation:\n\n```python\nimport numpy as np\n\ndef mel_spectrogram(\n    x: \"np.ndarray\",\n    window_duration: float = 0.025,\n    stride_duration: float = 0.01,\n    mean_normalize: bool = True,\n    window: str = \"hamming\",\n    n_filters: int = 20,\n    center: bool = True,\n    alpha: float = 0.95,\n    fs: int = 44000,\n):\n    \"\"\"Return the Mel spectrogram of *x* together with the energy of every frame.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional array that contains the audio samples.\n    window_duration : float, optional\n        Frame length in seconds (default 0.025).\n    stride_duration : float, optional\n        Hop length in seconds (default 0.01).\n    mean_normalize : bool, optional\n        If *True* subtract the mean of every filter column.\n    window : str, optional\n        Window function name: \"hamming\", \"hann\" or \"blackman_harris\" (default \"hamming\").\n    n_filters : int, optional\n        Number of triangular filters in the Mel bank (default 20).\n    center : bool, optional\n        If *True* the signal is padded so that frames are centred on their time stamps.\n    alpha : float, optional\n        Pre-emphasis coefficient (default 0.95).\n    fs : int, optional\n        Sampling rate of the signal in Hertz (default 44000).\n\n    Returns\n    -------\n    filter_energies : list[list[float]]\n        2-D list containing the Mel-spectrogram (G \u00d7 n_filters).\n    energy_per_frame : list[float]\n        1-D list of length G with the sum of power per frame.\n    \"\"\"\n    # Step 1: Pre-emphasis if required\n    if alpha != 1.0:\n        x = np.append(x[0], x[1:] - alpha * x[:-1])\n\n    # Step 2: Frame the signal\n    frame_width = round(window_duration * fs)\n    stride = round(stride_duration * fs)\n    \n    if center:\n        padding = frame_width // 2\n        x = np.pad(x, (padding, padding), mode='symmetric')\n\n    num_frames = (len(x) - frame_width) // stride + 1\n    frames = []\n    for i in range(num_frames):\n        start = i * stride\n        end = start + frame_width\n        frames.append(x[start:end])\n\n    # Step 3: Apply window function\n    window_func = {\n        \"hamming\": lambda x: np.hamming(len(x)),\n        \"hann\": lambda x: np.hann(len(x)),\n        \"blackman_harris\": lambda x: np.blackman(len(x))\n    }\n    window_func_name = window.lower()\n    if window_func_name not in window_func:\n        raise ValueError(f\"Unsupported window function: {window}\")\n    window_func = window_func[window_func_name]\n\n    windowed_frames = [frame * window_func(frame) for frame in frames]\n\n    # Step 4: Compute power spectrum\n    fft_size = 2 ** np.ceil(np.log2(max(frame_width, 2)))\n    fft = np.fft.fft(windowed_frames, n=fft_size, axis=-1)\n    power_spectrum = np.abs(fft) ** 2 / fft_size\n\n    # Step 5: Build the Mel filter bank\n    def hz_to_mel(hz):\n        return 2595.0 * np.log10(1.0 + hz / 700.0)\n\n    def mel_to_hz(mel):\n        return 700.0 * (10.0 ** (mel / 2595.0) - 1.0)\n\n    low_freq_mel = hz_to_mel(0.0)\n    high_freq_mel = hz_to_mel(fs / 2.0)\n    mel_points = np.linspace(low_freq_mel, high_freq_mel, n_filters + 2)\n    hz_points = mel_to_hz(mel_points)\n    bin_edges = np.floor((fft_size / 2 + 1) * hz_points / fs).astype(int)\n\n    fbank = np.zeros((n_filters, len(bin_edges) - 1))\n\n    for j in range(n_filters):\n        left = bin_edges[j]\n        right = bin_edges[j + 1]\n        width = right - left\n        slope = width / (hz_points[j + 1] - hz_points[j])\n        for k in range(left, right):\n            fbank[j, k - left] = slope * (hz_points[j + 1] - k / fs) + (1 - slope) * (hz_points[j] - k / fs)\n\n    # Step 6: Project the power spectra onto the filter bank\n    filter_energies = np.dot(power_spectrum[:, :len(bin_edges)], fbank.T)\n\n    # Step 7: Mean normalization if required\n    if mean_normalize:\n        filter_energies -= filter_energies.mean(axis=1, keepdims=True)\n\n    # Step 8: Replace any exact zeros with np.finfo(float).eps\n    filter_energies[np.isclose(filter_energies, 0)] = np.finfo(float).eps\n\n    # Round to 4 decimal places and convert to Python lists\n    filter_energies = np.round(filter_energies, 4).tolist()\n    energy_per_frame = np.sum(power_spectrum, axis=1).round(4).tolist()\n\n    return filter_energies, energy_per_frame\n\n# Example usage\n# x = np.random.rand(10000)\n# filter_energies, energy_per_frame = mel_spectrogram(x)\n# print(\"Filter Energies:\", filter_energies)\n# print(\"Energy Per Frame:\", energy_per_frame)\n```\n\nThis code defines the `mel_spectrogram` function, which performs all the necessary steps to generate the Mel spectrogram and energy per frame from a given input signal. It includes pre-emphasis, windowing, Fourier transform, Mel filter bank construction, projection, and normalization. The results are rounded to four decimal places and returned as Python lists."}
{"task_id": 481, "completion_id": 0, "completion": "To implement the DBSCAN clustering algorithm from scratch, we need to follow these steps:\n\n1. **Compute Pairwise Distances**: Calculate the Euclidean distances between all pairs of points in the dataset.\n2. **Identify Core Points**: Determine which points are core points based on the given `eps` and `min_samples`.\n3. **Expand Clusters**: Starting from core points, expand clusters by connecting them through their neighbors.\n4. **Label Noise Points**: Identify points that are not part of any cluster and label them as noise.\n\nHere's the implementation of the DBSCAN algorithm from scratch:\n\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n\n    num_points = len(data)\n    visited = np.zeros(num_points, dtype=bool)\n    labels = np.full(num_points, -1)\n\n    def get_neighbors(point_index):\n        return np.where(np.linalg.norm(data - data[point_index], axis=1) <= eps)[0]\n\n    def expand_cluster(cluster_id, point_index):\n        queue = [point_index]\n        visited[point_index] = True\n\n        while queue:\n            current_point_index = queue.pop(0)\n            labels[current_point_index] = cluster_id\n\n            neighbors = get_neighbors(current_point_index)\n            for neighbor_index in neighbors:\n                if not visited[neighbor_index]:\n                    visited[neighbor_index] = True\n                    if len(get_neighbors(neighbor_index)) >= min_samples:\n                        queue.append(neighbor_index)\n\n    cluster_id = 0\n    for i in range(num_points):\n        if visited[i]:\n            continue\n\n        neighbors = get_neighbors(i)\n        if len(neighbors) < min_samples:\n            labels[i] = -1  # Noise point\n        else:\n            cluster_id += 1\n            expand_cluster(cluster_id, i)\n\n    return list(labels)\n\n# Example usage:\n# data = np.array([[1, 2], [1, 8], [2, 3], [8, 9]])\n# eps = 1.5\n# min_samples = 2\n# print(dbscan(data, eps, min_samples))\n```\n\n### Explanation:\n\n- **Pairwise Distance Calculation**: We compute the Euclidean distance between each pair of points using `np.linalg.norm`.\n- **Core Point Identification**: A point is considered a core point if it has at least `min_samples` neighbors within `eps`.\n- **Cluster Expansion**: For each core point, we expand the cluster by adding its neighbors recursively until no more neighbors can be added.\n- **Noise Point Detection**: If a point does not meet the criteria to be a core point, it is labeled as noise (`-1`).\n\nThis implementation ensures that the clusters are labeled according to the specified rules and handles edge cases such as an empty input dataset."}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    if num_classes is None:\n        num_classes = max(y) + 1\n    \n    if not isinstance(num_classes, int) or num_classes < 1:\n        raise ValueError(\"num_classes must be a positive integer.\")\n    \n    if not all(isinstance(label, int) and label >= 0 for label in y):\n        raise ValueError(\"All labels must be non-negative integers.\")\n    \n    if any(label >= num_classes for label in y):\n        raise ValueError(\"No label can be greater than or equal to num_classes.\")\n    \n    y_encoded = np.eye(num_classes)[y]\n    return y_encoded.astype(int)\n\n# Example usage:\n# y = [1, 2, 3, 4]\n# print(to_categorical(y))\n```\n\nThis implementation checks for valid inputs, calculates the number of classes if not specified, ensures all labels are within the valid range, and then constructs the one-hot encoded matrix using NumPy's `eye` function. The resulting array is converted to integers before being returned."}
{"task_id": 483, "completion_id": 0, "completion": "To implement the `to_categorical` function according to the specified requirements, we can follow these steps:\n\n1. Determine the number of classes (`num_classes`). If not provided, it should be calculated as `max(labels) + 1`.\n2. Create a list of zeros for each label, with the length equal to `num_classes`.\n3. Set the appropriate element in the list to 1 for each label.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef to_categorical(labels, num_classes=None):\n    \"\"\"Convert integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    labels : sequence of int or 1-D np.ndarray\n        The class labels to transform.\n    num_classes : int, optional\n        Desired length of the one-hot vectors; if omitted the function should\n        use ``max(labels)+1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of ``labels``.\n    \"\"\"\n    if num_classes is None:\n        num_classes = max(labels) + 1\n    \n    # Convert labels to a numpy array if they are not already\n    labels_array = np.array(labels)\n    \n    # Initialize the result list\n    result = []\n    \n    # Iterate over each label\n    for label in labels_array:\n        # Create a zero-filled list of size num_classes\n        one_hot_vector = [0] * num_classes\n        \n        # Set the element at the index corresponding to the label to 1\n        one_hot_vector[label] = 1\n        \n        # Append the one-hot vector to the result list\n        result.append(one_hot_vector)\n    \n    return result\n\n# Example usage:\nlabels = [1, 2, 0, 3]\nencoded_labels = to_categorical(labels)\nprint(encoded_labels)\n```\n\n### Explanation:\n- **Determine `num_classes`:** If `num_classes` is not provided, it is calculated as `max(labels) + 1`.\n- **Convert labels to array:** We ensure that `labels` is a NumPy array for efficient processing.\n- **Initialize result:** An empty list `result` is initialized to store the one-hot encoded vectors.\n- **Iterate over labels:** For each label, a zero-filled list of length `num_classes` is created. The element at the index corresponding to the label is set to 1, and the resulting one-hot vector is appended to the `result` list.\n- **Return result:** Finally, the list of one-hot encoded vectors is returned.\n\nThis function meets all the specified constraints and behaves similarly to Keras' `to_categorical`."}
{"task_id": 484, "completion_id": 0, "completion": "To implement polynomial Lasso regression using coordinate descent, we need to follow these steps:\n\n1. Expand the input data into polynomial features.\n2. Initialize the weight vector.\n3. Perform coordinate descent to update each weight iteratively.\n4. Evaluate the model on the test set and return the predictions.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef lasso_regression_predict(\n    X_train: list[float | int],\n    y_train: list[float | int],\n    X_test: list[float | int],\n    degree: int,\n    reg_factor: float = 0.1,\n    n_iterations: int = 1000,\n) -> list[float]:\n    \"\"\"Fit a one-dimensional Lasso regression model and predict on new data.\n\n    Args:\n        X_train: List of scalar training inputs.\n        y_train: Target values corresponding to `X_train`.\n        X_test:  Inputs on which the trained model is evaluated.\n        degree:  Highest polynomial degree used for the feature expansion.\n        reg_factor: Non-negative regularisation strength (\u03bb).\n        n_iterations: Number of coordinate-descent sweeps.\n\n    Returns:\n        List of predictions for every element in `X_test`, each rounded to\n        the 4th decimal place.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Add a bias term (column of ones) to the feature matrix\n    X_train_poly = np.c_[np.ones(len(X_train)), np.power(X_train, range(1, degree+1))]\n    X_test_poly = np.c_[np.ones(len(X_test)), np.power(X_test, range(1, degree+1))]\n    \n    # Initialize weights\n    w = np.zeros(degree + 1)\n    \n    # Perform coordinate descent\n    for _ in range(n_iterations):\n        for j in range(degree + 1):\n            # Compute z_j\n            z_j = np.sum(X_train_poly[:, j]**2)\n            \n            # Compute rho_j\n            rho_j = np.sum((y_train - np.dot(X_train_poly, w)) * X_train_poly[:, j])\n            \n            # Apply soft-thresholding\n            if j == 0:\n                w[j] = rho_j / z_j  # No regularization for the bias term\n            else:\n                w[j] = max(0, abs(rho_j) - reg_factor) * np.sign(rho_j) / z_j\n    \n    # Predict on the test set\n    y_pred = np.dot(X_test_poly, w)\n    \n    # Round predictions to 4 decimal places\n    y_pred_rounded = [round(pred, 4) for pred in y_pred]\n    \n    return y_pred_rounded\n\n# Example usage:\n# X_train = [1, 2, 3, 4]\n# y_train = [2, 4, 6, 8]\n# X_test = [5, 6, 7]\n# print(lasso_regression_predict(X_train, y_train, X_test, degree=2))\n```\n\n### Explanation:\n\n- **Feature Expansion**: We use `np.power(X_train, range(1, degree+1))` to create polynomial features up to the specified degree. This includes the constant term (bias).\n\n- **Coordinate Descent**: In each iteration, we update each weight \\(w_j\\) while keeping the other weights fixed. The update rule involves computing \\(z_j\\) and \\(\\rho_j\\), then applying the soft-thresholding operator.\n\n- **Regularization**: The soft-thresholding operator ensures that when \\(|\\rho_j|\\) is less than or equal to \\(reg_factor\\), the weight \\(w_j\\) becomes zero, effectively performing Lasso regularization.\n\n- **Prediction**: After convergence, we compute predictions on the test set using the learned weights and round them to four decimal places.\n\nThis implementation efficiently handles the polynomial expansion and coordinate descent updates, providing accurate predictions for the given problem."}
{"task_id": 485, "completion_id": 0, "completion": "Here's a implementation of a very small version of the CART regression tree algorithm:\n\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    if len(X_train) == 0:\n        return []\n\n    # Convert lists to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    def find_best_split(x, y):\n        best_sse = float('inf')\n        best_split_point = None\n        best_left_y = None\n        best_right_y = None\n\n        for i in range(1, len(x)):\n            left_x = x[:i]\n            right_x = x[i:]\n            left_y = y[:i]\n            right_y = y[i:]\n\n            if len(left_x) > 0 and len(right_x) > 0:\n                left_mean = np.mean(left_y)\n                right_mean = np.mean(right_y)\n                sse = np.sum((left_y - left_mean) ** 2) + np.sum((right_y - right_mean) ** 2)\n                if sse < best_sse:\n                    best_sse = sse\n                    best_split_point = x[i]\n                    best_left_y = left_y\n                    best_right_y = right_y\n\n        return best_split_point, best_left_y, best_right_y\n\n    def build_tree(x, y, depth=0):\n        if depth >= max_depth or len(x) <= 1:\n            return {\n                'is_leaf': True,\n                'value': np.mean(y)\n            }\n\n        split_point, left_y, right_y = find_best_split(x, y)\n\n        left_x = x[x < split_point]\n        right_x = x[x >= split_point]\n\n        left_node = build_tree(left_x, left_y, depth + 1)\n        right_node = build_tree(right_x, right_y, depth + 1)\n\n        return {\n            'is_leaf': False,\n            'split_point': split_point,\n            'left': left_node,\n            'right': right_node\n        }\n\n    root = build_tree(X_train, y_train)\n\n    def traverse_tree(node, x):\n        if node['is_leaf']:\n            return node['value']\n        else:\n            if x < node['split_point']:\n                return traverse_tree(node['left'], x)\n            else:\n                return traverse_tree(node['right'], x)\n\n    predictions = [traverse_tree(root, x) for x in X_test]\n    return predictions\n\n# Example usage:\n# X_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# y_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# X_test = [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5]\n# print(regression_tree_predict(X_train, y_train, X_test))\n```\n\n### Explanation:\n1. **Data Preparation**: The input data `X_train` and `y_train` are converted to NumPy arrays for efficient computation.\n2. **Splitting Function**: The `find_best_split` function calculates the SSE for each possible split point and selects the one that minimizes it.\n3. **Tree Construction**: The `build_tree` function recursively constructs the decision tree. It stops recursion when the maximum depth is reached or when a node contains fewer than two samples.\n4. **Prediction Function**: The `traverse_tree` function uses the constructed tree to make predictions on new data points.\n5. **Example Usage**: An example is provided to demonstrate how the function can be used."}
{"task_id": 486, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Compute the mean squared error (MSE) between two numeric sequences.\n\n    The function should return -1 if the two inputs do not have the same\n    non-zero length. Otherwise, it returns the MSE rounded to 4 decimal\n    places.\n\n    Args:\n        y_true: Ground-truth values (list, tuple or NumPy array).\n        y_pred: Predicted values (list, tuple or NumPy array).\n\n    Returns:\n        A float rounded to 4 decimals representing the MSE, or -1 on error.\n    \"\"\"\n    # Convert inputs to NumPy arrays for easier manipulation\n    try:\n        y_true_array = np.array(y_true)\n        y_pred_array = np.array(y_pred)\n    except ValueError:\n        # If conversion fails, return -1\n        return -1\n\n    # Check if both arrays have the same shape and non-zero length\n    if len(y_true_array) != len(y_pred_array) or len(y_true_array) == 0:\n        return -1\n\n    # Calculate the Mean Squared Error\n    mse = np.mean((y_true_array - y_pred_array) ** 2)\n\n    # Return the MSE rounded to 4 decimal places\n    return round(mse, 4)\n\n# Example usage:\nprint(mean_squared_error([1, 2, 3], [1, 2, 4]))  # Output: 0.6667\nprint(mean_squared_error([1, 2, 3], [1, 2, 3]))  # Output: 0.0000\nprint(mean_squared_error([1, 2, 3], [1, 2]))     # Output: -1\nprint(mean_squared_error([], []))                # Output: -1\n```\n\nThis solution includes the necessary imports, defines the `mean_squared_error` function, and handles potential errors such as mismatched lengths or non-numeric data types. The function converts input lists or tuples into NumPy arrays for efficient computation and checks for valid input before calculating the MSE."}
{"task_id": 487, "completion_id": 0, "completion": "To implement the K-Means clustering algorithm from scratch, we need to follow the specified steps:\n\n1. **Initialization**: Start by selecting the first `k` samples as the initial centroids.\n2. **Iterative Assignment and Update**: Repeat the following process until convergence or the maximum number of iterations is reached:\n   - Assign each sample to the nearest centroid based on Euclidean distance.\n   - Update each centroid to be the mean of the samples assigned to it.\n3. **Convergence Check**: Stop the algorithm if the Euclidean distance between the old and new centroids is below the specified tolerance.\n4. **Handling Empty Centroids**: If a centroid becomes empty during an iteration, keep it unchanged.\n\nHere's the implementation of the `k_means` function:\n\n```python\nimport numpy as np\n\ndef k_means(X: np.ndarray, k: int, max_iters: int = 300, tol: float = 1e-4) -> list[int]:\n    \"\"\"Cluster the data contained in *X* into *k* groups using the K-Means algorithm.\n\n    Args:\n        X: NumPy array of shape (n_samples, n_features) containing the data points.\n        k: The desired number of clusters (1 \u2264 k \u2264 n_samples).\n        max_iters: Maximum number of iterations (default 300).\n        tol: Convergence tolerance. The algorithm stops when the L2-norm\n            between old and new centroids is smaller than *tol* (default 1e-4).\n\n    Returns:\n        A Python list containing the cluster index (0 \u2026 k-1) for every sample\n        in the original order of *X*.\n    \"\"\"\n    # Step 1: Initialize centroids\n    centroids = X[:k]\n    \n    # Step 2: Iterate until convergence or max_iters is reached\n    for _ in range(max_iters):\n        # Step 2.1: Assign samples to the nearest centroid\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids[np.newaxis], axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 2.2: Update centroids\n        new_centroids = []\n        for i in range(k):\n            # Collect all samples assigned to centroid i\n            cluster_samples = X[labels == i]\n            \n            # If the cluster is not empty, update the centroid\n            if cluster_samples.shape[0] > 0:\n                new_centroid = np.mean(cluster_samples, axis=0)\n                new_centroids.append(new_centroid)\n            else:\n                # If the cluster is empty, keep the old centroid\n                new_centroids.append(centroids[i])\n        \n        # Convert list to NumPy array for easier manipulation\n        new_centroids = np.array(new_centroids)\n        \n        # Step 2.3: Check for convergence\n        if np.allclose(centroids, new_centroids, atol=tol):\n            break\n        \n        # Update centroids\n        centroids = new_centroids\n    \n    # Return the cluster labels\n    return labels.tolist()\n\n# Example usage:\n# X = np.random.rand(100, 2)  # Example 2D dataset\n# k = 3\n# labels = k_means(X, k)\n# print(labels)\n```\n\n### Explanation:\n- **Initialization**: We start by selecting the first `k` samples from `X` as the initial centroids.\n- **Assignment**: For each sample, we calculate the Euclidean distance to each centroid and assign the sample to the centroid with the smallest distance.\n- **Update**: We recalculate the centroid of each cluster as the mean of all samples assigned to it.\n- **Convergence**: We check if the centroids have changed significantly (using `np.allclose` with a small tolerance) and stop if they haven't changed enough after a certain number of iterations.\n- **Empty Centroids**: If a centroid becomes empty, we keep it unchanged to prevent division by zero errors.\n\nThis implementation should work well for most cases where `k` is not too large relative to the number of samples, and it provides a basic understanding of how K-Means works from scratch."}
{"task_id": 488, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_variance(X):\n    \"\"\"Return the population variance of every feature (column) in X.\n\n    Parameters\n    ----------\n    X : list[list[int | float]] | np.ndarray\n        A 2-D data set where each row is a sample and each column is a feature.\n\n    Returns\n    -------\n    list[float] | int\n        A list containing the variance of each feature rounded to four\n        decimals.  Returns -1 if the input is empty or not 2-D.\n    \"\"\"\n    # Check if the input is empty or not 2-D\n    if not isinstance(X, (list, np.ndarray)) or len(X) == 0 or len(X[0]) == 0:\n        return -1\n    \n    # Convert to NumPy array for easier manipulation\n    X = np.array(X)\n    \n    # Check if the input is 2-D\n    if X.ndim != 2:\n        return -1\n    \n    # Calculate the variance for each feature\n    variances = np.var(X, axis=0)\n    \n    # Round the variances to 4 decimal places\n    variances_rounded = [round(var, 4) for var in variances]\n    \n    return variances_rounded\n```\n\nThis function first checks if the input is valid (non-empty and 2-D). It then calculates the population variance for each feature using NumPy's `var` function with `axis=0` to compute the variance across columns. Finally, it rounds each variance to four decimal places and returns the results as a list."}
{"task_id": 489, "completion_id": 0, "completion": "To solve this problem, we need to implement a function `to_nominal` that processes a batch of one-hot encoded vectors and returns their corresponding nominal class labels. Here's how you can achieve this:\n\n```python\nimport numpy as np\n\ndef to_nominal(one_hot):\n    \"\"\"Convert a batch of one-hot encoded vectors to nominal class labels.\n\n    Parameters\n    ----------\n    one_hot : list[list[int]] | numpy.ndarray\n        A 2-D structure where each row is expected to be a one-hot encoded\n        vector (all zeros except for a single 1).\n\n    Returns\n    -------\n    list[int] | int\n        A list of integer class labels when the input is valid; otherwise the\n        integer -1.\n    \"\"\"\n    if isinstance(one_hot, list):\n        one_hot = np.array(one_hot)\n\n    if not isinstance(one_hot, np.ndarray) or one_hot.ndim != 2:\n        raise ValueError(\"Input must be a 2D NumPy array.\")\n\n    rows, cols = one_hot.shape\n    nominal_labels = []\n\n    for row_idx in range(rows):\n        # Check if the row is a valid one-hot vector\n        if np.sum(one_hot[row_idx]) != 1 or np.any(one_hot[row_idx] != 0):\n            return -1\n        \n        # Find the index of the 1 in the current row\n        nominal_label = np.argmax(one_hot[row_idx])\n        nominal_labels.append(nominal_label)\n\n    return nominal_labels\n\n# Example usage:\none_hot_data = [\n    [0, 1, 0],\n    [1, 0, 0],\n    [0, 0, 1]\n]\n\nprint(to_nominal(one_hot_data))  # Output: [1, 0, 2]\n```\n\n### Explanation:\n\n1. **Input Validation**: \n   - The function first checks if the input is a list or a NumPy array. It then ensures that the input is a 2-dimensional array using `np.ndim`.\n\n2. **One-Hot Vector Validation**:\n   - For each row, the function checks two conditions:\n     - The sum of the row should be exactly 1, indicating that there is only one non-zero element.\n     - All elements in the row should be either 0 or 1.\n\n3. **Nominal Label Extraction**:\n   - If the row is valid, the function uses `np.argmax()` to find the index of the first occurrence of 1 in the row, which corresponds to the nominal label.\n\n4. **Return Result**:\n   - If any row is invalid, the function returns `-1`. Otherwise, it returns a list of nominal labels.\n\nThis implementation efficiently handles both list and NumPy array inputs and provides clear error handling for invalid data."}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if isinstance(x, (list, tuple)):\n        x = np.array(x)\n    \n    n = len(x)\n    diagonal_matrix = [[0] * n for _ in range(n)]\n    \n    for i in range(n):\n        diagonal_matrix[i][i] = x[i]\n    \n    return diagonal_matrix.tolist()\n```\n\nThis function first checks if the input is a list or tuple and converts it to a NumPy array if necessary. It then creates a zero-filled square matrix of size `n x n`, where `n` is the length of the input vector. Finally, it places each element of the vector on its corresponding position on the diagonal and returns the resulting matrix as a list of lists."}
{"task_id": 491, "completion_id": 0, "completion": "To implement the FP-Growth algorithm, we need to follow the two main stages as described: constructing the FP-tree and recursively mining for frequent item-sets. Here's how you can implement this in Python:\n\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    def find_frequent_itemsets():\n        # Step 1: Construct the FP-tree\n        root = Node(None)\n        header_table = defaultdict(list)\n        \n        for transaction in transactions:\n            sorted_transaction = sorted(transaction, key=lambda x: -frequencies[x])\n            insert_into_fp_tree(root, sorted_transaction, header_table)\n        \n        # Step 2: Generate frequent item-sets\n        frequent_itemsets = []\n        for i in range(1, len(frequencies) + 1):\n            candidate_items = get_candidate_items(header_table, i)\n            for itemset in generate_frequent_itemsets(candidate_items, min_sup, root):\n                if len(itemset) > 1:\n                    frequent_itemsets.append(tuple(sorted(itemset)))\n        \n        return frequent_itemsets\n    \n    def insert_into_fp_tree(node, transaction, header_table):\n        current_node = node\n        for item in transaction:\n            if item not in current_node.children:\n                current_node.children[item] = Node(item)\n            current_node = current_node.children[item]\n            header_table[item].append(current_node)\n    \n    def get_candidate_items(header_table, k):\n        candidates = set()\n        for i, nodes in header_table.items():\n            if len(nodes) >= k:\n                candidates.add(i)\n        return candidates\n    \n    def generate_frequent_itemsets(candidate_items, min_sup, root):\n        for item in candidate_items:\n            new_root = Node(item)\n            for node in header_table[item]:\n                new_root.children[node.item] = node\n                new_root.size += node.size\n            if new_root.size >= min_sup:\n                yield frozenset([item]) | frozenset(generate_frequent_itemsets(new_root.children.values(), min_sup, new_root))\n    \n    # Count item frequencies\n    item_counts = Counter(itertools.chain.from_iterable(transactions))\n    total_transactions = len(transactions)\n    frequencies = {item: count / total_transactions for item, count in item_counts.items()}\n    \n    return sorted(find_frequent_itemsets())\n\nclass Node:\n    def __init__(self, item=None):\n        self.item = item\n        self.children = {}\n        self.size = 0\n\n# Example usage:\ntransactions = [\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"A\", \"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"C\", \"D\", \"E\"],\n    [\"B\", \"C\", \"D\"]\n]\nmin_sup = 3\nprint(fp_growth(transactions, min_sup))\n```\n\n### Explanation:\n1. **Node Class**: Represents a node in the FP-tree. Each node has a value (`item`), children, and a size representing the frequency of the path leading to it.\n\n2. **find_frequent_itemsets Function**: Main function to execute the FP-Growth algorithm.\n   - **Step 1**: Construct the FP-tree.\n     - Traverse each transaction and sort the items by their frequency in descending order.\n     - Insert the sorted transaction into the FP-tree.\n     - Update the header table to keep track of the last node for each item.\n\n   - **Step 2**: Generate frequent item-sets.\n     - Identify candidate items that meet the minimum support threshold.\n     - Recursively generate frequent item-sets starting from these candidate items.\n\n3. **insert_into_fp_tree Function**: Inserts a transaction into the FP-tree and updates the header table.\n\n4. **get_candidate_items Function**: Generates candidate items based on the header table.\n\n5. **generate_frequent_itemsets Function**: Recursively generates frequent item-sets by exploring paths in the FP-tree.\n\n6. **Counter and Sorting**: Used to count item frequencies and sort the results.\n\nThis implementation ensures that the frequent item-sets are found and returned in the specified format."}
{"task_id": 492, "completion_id": 0, "completion": "To solve this problem, we need to generate a polynomial feature matrix where each row corresponds to a sample from the input data, and each column corresponds to a polynomial term. The polynomial terms are constructed by taking non-negative integer powers of the original features, ensuring that the sum of these powers does not exceed the specified degree `d`. Here's how you can implement this:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    if degree < 0:\n        return -1\n    \n    # Convert the input list to a NumPy array for easier manipulation\n    X_np = np.array(X)\n    \n    # Initialize the result matrix with ones (degree-0 term)\n    result = [[1] * len(X[0])]\n    \n    # Generate combinations with replacement for each possible degree up to 'degree'\n    for d in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(len(X[0])), d):\n            # Calculate the product of the features raised to the power of their combination indices\n            term = 1\n            for i, exp in enumerate(combo):\n                term *= X_np[:, i] ** exp\n            # Append the term to the result matrix\n            result.append(list(term))\n    \n    # Convert the result list back to a list of lists\n    return result.tolist()\n\n# Example usage:\nX = [\n    [1, 2],\n    [3, 4]\n]\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\n### Explanation:\n\n1. **Input Validation**: First, check if the `degree` is negative. If so, return `-1`.\n\n2. **Convert Input**: Convert the input list `X` into a NumPy array for efficient computation.\n\n3. **Initialize Result Matrix**: Start with a result matrix containing only ones, representing the constant term (degree 0).\n\n4. **Generate Combinations**: Use `combinations_with_replacement` to generate all possible combinations of feature indices up to the specified degree `d`.\n\n5. **Compute Terms**: For each combination, compute the product of the corresponding features raised to the power of their indices.\n\n6. **Append Terms**: Append each computed term to the result matrix as a new row.\n\n7. **Return Result**: Finally, convert the result list to a list of lists and return it.\n\nThis approach ensures that the polynomial terms are generated in the correct order, starting with degree 0 and then moving to higher degrees while maintaining lexicographical order within each degree."}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if not y:\n        return -1\n    \n    # Convert to numpy array if it's not already\n    y = np.array(y)\n    \n    # Calculate the mean of the sample\n    mean_y = np.mean(y)\n    \n    # Calculate the squared differences\n    squared_diffs = (y - mean_y) ** 2\n    \n    # Calculate the mean of the squared differences\n    mse_value = np.mean(squared_diffs)\n    \n    # Round to 4 decimal places\n    return round(mse_value, 4)\n\n# Example usage:\n# print(mse([1, 2, 3, 4]))  # Should output 1.2500\n# print(mse([]))              # Should output -1\n```\n\nThis function first checks if the input `y` is empty and returns `-1` if it is. It then calculates the mean of the sample using `np.mean()`, computes the squared differences between each element and the mean, and finally calculates the mean of these squared differences to get the MSE. The result is rounded to four decimal places before being returned."}
{"task_id": 494, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef aggregate_predictions(\n    preds: Union[List[List[Union[int, float]]], np.ndarray],\n    classifier: bool = True,\n) -> List[Union[int, float]]:\n    \"\"\"Aggregate ensemble predictions.\n\n    Parameters\n    ----------\n    preds : list[list[int | float]] | np.ndarray\n        A 2-D structure where each row holds the predictions from one model\n        and each column corresponds to a data sample.\n    classifier : bool, default True\n        If True the task is classification and majority voting is used.\n        If False the task is regression and simple averaging is used.\n\n    Returns\n    -------\n    list[int | float]\n        Final aggregated predictions for every sample. For regression values\n        are rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(preds, list):\n        preds = np.array(preds)\n\n    if classifier:\n        # Use majority voting for classification\n        return np.argmax(preds, axis=0)\n    else:\n        # Use averaging for regression\n        avg_preds = np.mean(preds, axis=0)\n        return [round(val, 4) for val in avg_preds]\n\n# Example usage:\n# Classification example\nclassification_preds = [\n    [1, 2, 3],\n    [2, 1, 3],\n    [3, 3, 2]\n]\nprint(aggregate_predictions(classification_preds))  # Output: [2, 2, 3]\n\n# Regression example\nregression_preds = [\n    [1.56789, 2.34567, 3.14159],\n    [2.34567, 1.56789, 3.14159],\n    [3.14159, 3.14159, 2.34567]\n]\nprint(aggregate_predictions(regression_preds))  # Output: [2.3457, 2.3457, 2.3457]\n```\n\nThis solution defines the `aggregate_predictions` function, which takes a 2-D array of predictions and an optional boolean flag indicating whether the task is classification. It handles both cases by using either majority voting for classification or averaging for regression, rounding the average predictions to four decimal places when necessary."}
{"task_id": 495, "completion_id": 0, "completion": "Here's how you can implement the `standardize` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef standardize(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Standardize every column of a 2-D NumPy array.\n\n    Each feature (column) should have mean 0 and variance 1 after the\n    transformation.  Columns with zero variance must become all zeros.\n\n    The input array **must not** be modified in place; return a new result.\n\n    Args:\n        X: 2-D NumPy array (shape = n_samples \u00d7 n_features).\n\n    Returns:\n        Standardized dataset as a Python list rounded to 4 decimals.\n    \"\"\"\n    # Calculate the mean and standard deviation for each column\n    means = X.mean(axis=0)\n    stds = X.std(axis=0, ddof=0)\n    \n    # Create a mask for columns with zero standard deviation\n    zero_std_mask = stds == 0\n    \n    # Initialize the result array\n    standardized_X = np.zeros_like(X)\n    \n    # Fill the result array with standardized values\n    for i in range(X.shape[1]):\n        if not zero_std_mask[i]:\n            standardized_X[:, i] = (X[:, i] - means[i]) / stds[i]\n        else:\n            standardized_X[:, i] = 0\n    \n    # Round the result to 4 decimal places and convert to a list\n    standardized_list = standardized_X.round(4).tolist()\n    \n    return standardized_list\n\n# Example usage:\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nresult = standardize(X)\nprint(result)\n```\n\n### Explanation:\n1. **Mean Calculation**: We calculate the mean of each column using `np.mean(X, axis=0)`.\n2. **Standard Deviation Calculation**: We calculate the population standard deviation of each column using `np.std(X, axis=0, ddof=0)`. Note that `ddof=0` ensures we use the population standard deviation.\n3. **Zero Std Mask**: We create a boolean mask to identify columns where the standard deviation is zero.\n4. **Standardization**: We iterate over each column, applying the standardization formula `(x - mean_i) / std_i` for non-zero standard deviations. If a column has zero standard deviation, we set all entries to zero.\n5. **Rounding and Conversion**: We round the resulting array to four decimal places using `round(4)` and convert it to a list using `tolist()`.\n\nThis function ensures that the original input array `X` is not modified and returns a new standardized dataset as a list of lists."}
{"task_id": 496, "completion_id": 0, "completion": "Here's the implementation of the `apply_affine` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert x to a NumPy array if it's not already one\n    x_array = np.array(x)\n    \n    # Calculate the activation output\n    y = slope * x_array + intercept\n    \n    # Calculate the first derivative\n    grad = [slope] * len(x_array)\n    \n    # Calculate the second derivative\n    grad2 = [0] * len(x_array)\n    \n    # Round all elements to 4 decimal places\n    y_rounded = np.round(y, decimals=4).tolist()\n    grad_rounded = np.round(grad, decimals=4).tolist()\n    grad2_rounded = np.round(grad2, decimals=4).tolist()\n    \n    return y_rounded, grad_rounded, grad2_rounded\n\n# Example usage:\nx = [1, 2, 3]\nresult = apply_affine(x)\nprint(\"Activation:\", result[0])\nprint(\"First Derivative:\", result[1])\nprint(\"Second Derivative:\", result[2])\n```\n\n### Explanation:\n- The function first converts the input `x` into a NumPy array to facilitate vectorized operations.\n- It calculates the activation output using the formula \\( f(x) = \\text{slope} \\cdot x + \\text{intercept} \\).\n- The first derivative is constant for each element in the input, so we create a list of `[slope]` repeated for the number of elements in `x`.\n- The second derivative is zero for each element, so we create a list of `[0]` repeated for the number of elements in `x`.\n- Finally, all calculated values are rounded to four decimal places before being returned as a tuple of lists."}
{"task_id": 497, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef exponential_activation(x, order: int = 0):\n    \"\"\"Compute the exponential activation or one of its first two derivatives.\n\n    Parameters\n    ----------\n    x : list | numpy.ndarray\n        A (possibly nested) list or NumPy array containing real numbers.\n    order : int, optional\n        Indicates what to compute:\n        * 0 \u2013 activation itself (default)\n        * 1 \u2013 first derivative\n        * 2 \u2013 second derivative\n\n    Returns\n    -------\n    list | int\n        Python list mirroring the shape of *x* with every entry replaced by\n        the corresponding result rounded to 6 decimals, or -1 for invalid\n        *order*.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    x_array = np.array(x)\n    \n    # Check if the order is valid\n    if order not in [0, 1, 2]:\n        return -1\n    \n    # Compute the exponential function\n    exp_x = np.exp(x_array)\n    \n    # Handle different orders\n    if order == 0:\n        return exp_x.tolist()\n    elif order == 1:\n        return (exp_x * np.exp(x_array)).tolist()  # First derivative\n    else:\n        return ((exp_x * np.exp(x_array)) * np.exp(x_array)).tolist()  # Second derivative\n\n# Example usage:\nprint(exponential_activation([[0, 1], [2, -1]], order=0))\nprint(exponential_activation([[0, 1], [2, -1]], order=1))\nprint(exponential_activation([[0, 1], [2, -1]], order=2))\nprint(exponential_activation([[0, 1], [2, -1]], order=3))  # Invalid order\n```\n\nThis code defines the `exponential_activation` function, which computes the exponential activation or one of its first two derivatives based on the specified order. It handles both single values and nested lists by converting them to NumPy arrays for efficient computation. The function returns the results rounded to six decimal places and maintains the original nesting structure of the input."}
{"task_id": 498, "completion_id": 0, "completion": "Here's the implementation of the `leaky_relu` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef leaky_relu(x: np.ndarray | list | int | float, alpha: float = 0.3, order: int = 0) -> list:\n    \"\"\"Compute Leaky ReLU, its first derivative, or its second derivative.\n\n    Args:\n        x: Values to be transformed. Accepts NumPy array, list, or scalar.\n        alpha: Slope used when an element is non-positive (default 0.3).\n        order: 0 \u2013 return the function value; 1 \u2013 return the first derivative; 2 \u2013 return the second derivative.\n\n    Returns:\n        Python list with the same shape as *x* that contains the requested quantity.\n    \"\"\"\n    # Convert inputs to NumPy arrays for consistent processing\n    x_array = np.array(x)\n    \n    # Check if the order is valid\n    if order not in [0, 1, 2]:\n        return []\n    \n    # Apply Leaky ReLU based on the given order\n    if order == 0:\n        # Function value\n        result = np.where(x_array > 0, x_array, alpha * x_array).tolist()\n    elif order == 1:\n        # First derivative\n        result = np.where(x_array > 0, 1, alpha).tolist()\n    else:\n        # Second derivative (identically zero)\n        result = [0] * len(x_array)\n    \n    return result\n\n# Example usage:\nprint(leaky_relu([1, -2, 3, -4], alpha=0.1, order=0))  # Output: [1, 0.2, 3, 0.4]\nprint(leaky_relu([1, -2, 3, -4], alpha=0.1, order=1))  # Output: [1, 0.1, 1, 0.1]\nprint(leaky_relu([1, -2, 3, -4], alpha=0.1, order=2))  # Output: [0, 0, 0, 0]\n```\n\n### Explanation:\n- **Input Handling**: The function accepts a variety of input types (NumPy array, list, or scalar) and converts them into a NumPy array for uniform processing.\n- **Order Validation**: It checks if the `order` parameter is valid (0, 1, or 2). If not, it returns an empty list.\n- **Leaky ReLU Calculation**:\n  - For `order = 0`, it computes the Leaky ReLU function value using `np.where`.\n  - For `order = 1`, it computes the first derivative using `np.where`.\n  - For `order = 2`, it returns a list of zeros since the second derivative is identically zero.\n- **Output Conversion**: The results are converted to a list using `.tolist()` before being returned."}
{"task_id": 499, "completion_id": 0, "completion": "Here's a complete implementation of the PCA dimensionality reduction routine:\n\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Check if k is within the valid range\n    if not (1 <= k <= data.shape[1]):\n        return -1\n\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n\n    # Step 2: Compute the sample covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Step 3: Perform an eigen-decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Step 4: Sort eigenvalues in descending order and arrange eigenvectors accordingly\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Fix the sign of every eigenvector so that its entry with the largest absolute value is positive\n    for i in range(eigenvectors.shape[1]):\n        if np.abs(eigenvectors[i, 0]) < 1e-8:\n            # If the first element is very close to zero, choose any other element\n            eigenvectors[:, i] = eigenvectors[:, (i + 1) % eigenvectors.shape[1]]\n\n    # Step 6: Project the standardised data on the first k eigenvectors\n    projected_data = np.dot(standardized_data, eigenvectors[:, :k])\n\n    # Round the projected data to 4 decimal places and convert to a regular list\n    return round(projected_data, 4).tolist()\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [3, 4]])\nk = 1\nresult = pca_transform(data, k)\nprint(result)\n```\n\n### Explanation:\n1. **Standardization**: We subtract the mean and divide by the standard deviation of each feature to ensure that each feature has zero mean and unit variance.\n2. **Covariance Matrix**: We compute the covariance matrix of the standardized data using Bessel's correction.\n3. **Eigen-Decomposition**: We perform an eigen-decomposition of the covariance matrix to find the principal components.\n4. **Sorting Eigenvalues and Eigenvectors**: We sort the eigenvalues in descending order and rearrange the corresponding eigenvectors.\n5. **Fixing Sign Ambiguity**: We ensure that each eigenvector has a positive leading component by rotating it if necessary.\n6. **Projection**: We project the standardized data onto the first `k` principal components.\n7. **Output**: The result is rounded to four decimal places and returned as a list of lists."}
{"task_id": 500, "completion_id": 0, "completion": "To implement the Tiny Gradient Boosting Binary Classifier using decision stumps, we need to follow these steps:\n\n1. Initialize the base learner (`h_m`) as a simple decision stump that makes a one-dimensional split.\n2. For each iteration `m` from 1 to `n_estimators`:\n   - Compute the residuals `r_i = y_i - F(x_i)`, where `F(x_i)` is the current prediction made by the ensemble up to step `m-1`.\n   - Fit a new decision stump `h_m` to minimize the squared error on the residuals.\n   - Update the ensemble's prediction by adding `\u03b7 \u00b7 h_m(x_i)` to the previous prediction.\n3. Return the predicted class labels for the test set by thresholding the final score `F(x)` at 0.5.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    # Step 1: Initialize the ensemble with the average of the training labels\n    c = np.mean(y_train)\n    F = [c] * len(X_train)\n    \n    # Step 2: Perform n_estimators boosting iterations\n    for m in range(n_estimators):\n        # Step 2.1: Compute residuals\n        r = [y - F[i] for i, y in enumerate(y_train)]\n        \n        # Step 2.2: Fit a new decision stump to minimize squared error on residuals\n        best_split = None\n        min_error = float('inf')\n        num_features = len(X_train[0])\n        \n        for feature_index in range(num_features):\n            thresholds = sorted(set([x[feature_index] for x in X_train]))\n            \n            for i in range(len(thresholds) - 1):\n                left_threshold = thresholds[i]\n                right_threshold = thresholds[i + 1]\n                \n                # Calculate the left and right residuals\n                left_residuals = [r[j] if X_train[j][feature_index] <= left_threshold else 0 for j in range(len(X_train))]\n                right_residuals = [r[j] if X_train[j][feature_index] > right_threshold else 0 for j in range(len(X_train))]\n                \n                # Calculate the squared error\n                error_left = sum(left_residuals) ** 2\n                error_right = sum(right_residuals) ** 2\n                \n                # Total squared error\n                total_error = error_left + error_right\n                \n                if total_error < min_error:\n                    min_error = total_error\n                    best_split = (feature_index, left_threshold, right_threshold)\n        \n        # Step 2.3: Update the ensemble's prediction\n        feature_index, left_threshold, right_threshold = best_split\n        h_m = [1 if x[feature_index] <= left_threshold else 0 for x in X_train]\n        F = [f + learning_rate * h for f, h in zip(F, h_m)]\n    \n    # Step 3: Predict class labels for the test set\n    predictions = [(1 if f >= 0.5 else 0) for f in F]\n    return predictions\n\n# Example usage:\n# X_train = [[1, 2], [3, 4], [5, 6]]\n# y_train = [0, 1, 0]\n# X_test = [[2, 3], [4, 5]]\n# print(gradient_boosting_classifier(X_train, y_train, X_test))\n```\n\nThis implementation defines a function `gradient_boosting_classifier` that trains a Gradient Boosting model using decision stumps and returns the predicted class labels for the test set. The function iteratively fits decision stumps to the residuals and updates the ensemble's prediction based on the learned weights. The final scores are thresholded at 0.5 to produce binary class labels."}
{"task_id": 501, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        Current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    # Since we need to return the initial_lr regardless of the step,\n    # we can simply return it without any modifications.\n    return initial_lr\n\n# Example usage:\nprint(constant_scheduler(0.001, 100))  # Output: 0.001\nprint(constant_scheduler(-0.01, 50))   # Output: -0.01\n```"}
{"task_id": 502, "completion_id": 0, "completion": "Here's the implementation of the Hard-Sigmoid activation function along with its first and second derivatives:\n\n```python\nimport numpy as np\n\ndef hard_sigmoid(z: np.ndarray, order: int = 0) -> list[float]:\n    \"\"\"Compute Hard-Sigmoid activation or its derivatives.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array whose elements the activation (or its derivatives) will be\n        applied to.\n    order : int, optional (default = 0)\n        \u2022 0 \u2013 return Hard-Sigmoid values.\n        \u2022 1 \u2013 return first derivative.\n        \u2022 2 \u2013 return second derivative.\n\n    Returns\n    -------\n    list[float]\n        A list containing the computed values, rounded to 4 decimal places.\n        If an unsupported *order* is provided the function should return an\n        empty list.\n    \"\"\"\n    # Ensure the input is a numpy array\n    z = np.asarray(z)\n    \n    # Check for unsupported order\n    if order not in [0, 1, 2]:\n        return []\n    \n    # Apply Hard-Sigmoid function based on the order\n    if order == 0:\n        # Hard-Sigmoid activation\n        return list(np.round((z >= -2.5) & (z <= 2.5) * (0.2 * z + 0.5), decimals=4))\n    elif order == 1:\n        # First derivative of Hard-Sigmoid\n        return list(np.round(((z >= -2.5) & (z <= 2.5)) * 0.2, decimals=4))\n    else:\n        # Second derivative of Hard-Sigmoid (always zero)\n        return [0] * len(z)\n\n# Example usage:\nz = np.array([-3, -2, -1, 0, 1, 2, 3])\nprint(hard_sigmoid(z))  # Output: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.0]\nprint(hard_sigmoid(z, order=1))  # Output: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\nprint(hard_sigmoid(z, order=2))  # Output: [0, 0, 0, 0, 0, 0, 0]\n```\n\n### Explanation:\n- **Hard-Sigmoid Activation**: The function checks each element of `z` against the boundaries `-2.5` and `2.5`. It applies the formula `(0.2 * z + 0.5)` where `z` is between these bounds, and returns `0` otherwise.\n  \n- **First Derivative**: For the first derivative, the function multiplies the output of the Hard-Sigmoid activation by `0.2` where `z` is between `-2.5` and `2.5`, and returns `0` elsewhere.\n\n- **Second Derivative**: Since the Hard-Sigmoid function is a piecewise linear function, its second derivative is always zero everywhere, so the function returns `[0] * len(z)`.\n\nThis implementation ensures that the results are rounded to four decimal places and handles invalid orders by returning an empty list."}
{"task_id": 503, "completion_id": 0, "completion": "To implement the `selective_update` function according to the specified requirements, we need to follow these steps:\n\n1. Check if `hparams` is `None`. If it is, return `-1`.\n2. Create a deep copy of `hparams` using `deepcopy` to ensure that the original dictionary remains unchanged.\n3. Iterate over each key-value pair in `updates`.\n4. For each key in `updates`, check if it exists in the copied `hparams` dictionary.\n5. If the key exists, update the value in the copied `hparams` dictionary with the value from `updates`.\n6. Return the updated copied dictionary.\n\nHere's how you can implement this:\n\n```python\nfrom copy import deepcopy\n\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    \"\"\"Update only existing keys in *hparams* with values from *updates*.\n\n    Parameters\n    ----------\n    hparams : dict | None\n        Original hyper-parameter dictionary.  If None, return -1.\n    updates : dict | None\n        Dictionary containing proposed updates.  Keys **not** present in\n        *hparams* are ignored.  If None, no updates are applied.\n\n    Returns\n    -------\n    dict | int\n        A deep-copied dictionary with valid updates applied, or -1 when\n        *hparams* is None.\n    \"\"\"\n    # Check if hparams is None\n    if hparams is None:\n        return -1\n    \n    # Create a deep copy of hparams\n    updated_hparams = deepcopy(hparams)\n    \n    # Iterate over each key-value pair in updates\n    for key, value in updates.items():\n        # Check if the key exists in the copied hparams\n        if key in updated_hparams:\n            # Update the value in the copied hparams\n            updated_hparams[key] = value\n    \n    # Return the updated copied dictionary\n    return updated_hparams\n\n# Example usage:\nhparams = {'lr': 0.1, 'momentum': 0.9}\nupdates = {'lr': 0.01, 'weight_decay': 0.0001}\nresult = selective_update(hparams, updates)\nprint(result)  # Output: {'lr': 0.01, 'momentum': 0.9}\n```\n\nThis implementation ensures that only existing keys in `hparams` are updated with values from `updates`, while ignoring any keys that do not exist in `hparams`. If `hparams` is `None`, the function returns `-1`."}
{"task_id": 505, "completion_id": 0, "completion": "To implement the Contrastive Divergence (CD-k) update for a Binary Restricted Boltzmann Machine (RBM), we need to follow these steps:\n\n1. Compute the positive phase hidden probabilities.\n2. Run `k` full Gibbs sampling steps without stochastic sampling.\n3. Calculate the positive and negative gradients.\n4. Update the weight matrix using the computed gradients.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    if X.size == 0:\n        return []\n\n    # Step 1: Compute the positive phase hidden probabilities\n    p_h_given_v = sigmoid(X @ W + hbias)\n\n    # Initialize hidden states for the first Gibbs step\n    h = p_h_given_v\n\n    # Step 2: Run k full Gibbs sampling steps\n    for _ in range(k):\n        v = sigmoid(h @ W.T + vbias)\n        h = sigmoid(v @ W + hbias)\n\n    # Step 3: Calculate positive and negative gradients\n    pos_grad = X.T @ h\n    v_k_prob = sigmoid(h @ W.T + vbias)\n    h_k_prob = sigmoid(v_k_prob @ W + hbias)\n    neg_grad = v_k_prob.T @ h_k_prob\n\n    # Step 4: Update the weight matrix\n    W_new = W + learning_rate * (pos_grad - neg_grad) / X.shape[0]\n\n    # Return the updated weight matrix rounded to 4 decimal places\n    return np.round(W_new, decimals=4).tolist()\n\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n```\n\n### Explanation:\n\n- **Sigmoid Function**: We define a helper function `sigmoid` to compute the sigmoid of a given input `x`.\n- **Positive Phase Hidden Probabilities**: In the positive phase, we calculate the probability of each hidden unit being active given the visible units using the sigmoid function.\n- **Gibbs Sampling**: We perform `k` full Gibbs sampling steps. For each step, we update the visible units based on the current hidden state and then update the hidden units based on the new visible state.\n- **Gradients Calculation**: We compute the positive gradient using the observed data (`X`) and the probabilities from the positive phase. The negative gradient is computed using the probabilities after `k` Gibbs steps.\n- **Weight Update**: We update the weight matrix using the calculated gradients and the learning rate.\n- **Output**: The updated weight matrix is returned as a list of lists, rounded to four decimal places. If the input batch is empty, an empty list is returned."}
{"task_id": 508, "completion_id": 0, "completion": "Here's how you could implement the `params_with_name` function:\n\n```python\nfrom typing import Any, Dict, List\n\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    \"\"\"Return all values whose keys contain a given substring.\n\n    The function must preserve the insertion order of the dictionary.\n\n    Args:\n        params: Dictionary mapping parameter names (strings) to parameter values (any type).\n        name: Substring to be searched for inside the dictionary keys.\n\n    Returns:\n        List of values whose corresponding keys contain *name* (order preserved).\n    \"\"\"\n    # Use a list comprehension to filter keys based on the presence of the substring\n    filtered_values = [value for key, value in params.items() if name in key]\n    \n    return filtered_values\n\n# Example usage:\nparams = {\n    'learning_rate': 0.001,\n    'batch_size': 32,\n    'weight1': 0.5,\n    'bias': -0.2,\n    'weight2': 0.8\n}\n\nprint(params_with_name(params, 'weight'))  # Output: [0.5, 0.8]\nprint(params_with_name(params, 'bias'))   # Output: [-0.2]\nprint(params_with_name(params, 'gamma'))  # Output: []\n```\n\n### Explanation:\n- We use a list comprehension to iterate over each key-value pair in the dictionary.\n- For each pair, we check if the `name` substring is present in the key using the `in` keyword.\n- If the condition is true, the value associated with that key is included in the resulting list.\n- The list comprehension maintains the order of elements as they appear in the original dictionary due to the nature of list comprehensions."}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Check if both inputs are either lists or NumPy arrays\n    if not isinstance(x, (list, np.ndarray)) or not isinstance(y, (list, np.ndarray)):\n        raise ValueError(\"Both inputs must be either lists or NumPy arrays.\")\n    \n    # Convert inputs to NumPy arrays for easier manipulation\n    x_array = np.array(x)\n    y_array = np.array(y)\n    \n    # Check if the lengths of the vectors are equal\n    if len(x_array) != len(y_array):\n        return -1\n    \n    # Calculate the component-wise absolute differences\n    differences = np.abs(x_array - y_array)\n    \n    # Compute the maximum of these differences\n    max_difference = np.max(differences)\n    \n    # Round the result to four decimal places\n    return round(max_difference, 4)\n\n# Example usage:\nx = [1, 2, 3]\ny = [2, 4, 6]\nprint(chebyshev(x, y))  # Output: 3.0\n```\n\nThis function first checks if the inputs are valid lists or NumPy arrays. It then converts them to NumPy arrays for efficient computation. It verifies that the vectors have the same length and calculates the Chebyshev distance by finding the maximum of the absolute differences between corresponding elements of the vectors. Finally, it rounds the result to four decimal places before returning it."}
{"task_id": 510, "completion_id": 0, "completion": "To implement the VAE loss for Bernoulli visible units, we need to calculate both the reconstruction loss and the KL divergence, and then combine them using their respective weights. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # Define the epsilon value for numerical stability\n    epsilon = np.finfo(float).eps\n    \n    # Clip the predictions to ensure they are within the open interval (epsilon, 1-epsilon)\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Calculate the reconstruction loss using binary cross-entropy\n    reconstruction_loss = -np.sum(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n    \n    # Calculate the KL divergence\n    kl_divergence = -0.5 * np.sum(1 + t_log_var - t_mean**2 - np.exp(t_log_var))\n    \n    # Combine the losses with appropriate weights\n    total_loss = reconstruction_loss + kl_divergence\n    \n    # Return the average loss rounded to 6 decimal places\n    return round(total_loss / y.shape[0], 6)\n\n# Example usage:\n# y = np.array([[0, 1], [1, 0]])\n# y_pred = np.array([[0.9, 0.1], [0.8, 0.2]])\n# t_mean = np.array([[0.5, 0.5], [0.5, 0.5]])\n# t_log_var = np.array([[-1, -1], [-1, -1]])\n\n# print(vae_loss(y, y_pred, t_mean, t_log_var))\n```\n\n### Explanation:\n1. **Numerical Stability**: We clip the predicted values `y_pred` to ensure they are within the open interval `(\u03b5, 1-\u03b5)` to avoid taking the logarithm of zero or one, which would lead to NaNs.\n\n2. **Reconstruction Loss**: This is calculated using the binary cross-entropy formula:\n   \\[\n   L_{\\text{recon}} = -\\sum y_i \\log(y_{i,\\text{pred}}) - (1-y_i) \\log(1-y_{i,\\text{pred}})\n   \\]\n\n3. **KL Divergence**: For a standard normal distribution \\( p(t) = \\mathcal{N}(0, I) \\), the KL divergence between the approximate posterior \\( q(t|x) = \\mathcal{N}(\\mu, \\operatorname{diag}(\\sigma^2)) \\) and the prior is:\n   \\[\n   L_{\\text{KL}} = -0.5 \\sum \\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)\n   \\]\n\n4. **Total Loss**: The total loss is the sum of the reconstruction loss and the KL divergence, weighted appropriately.\n\n5. **Average Calculation**: The average loss is computed by dividing the total loss by the batch size and rounding to 6 decimal places."}
{"task_id": 511, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_bernoulli_mab(n_arms: int = 10) -> list[float]:\n    \"\"\"\n    Generates a random Bernoulli multi-armed bandit environment.\n\n    Parameters:\n    n_arms (int): The number of arms in the bandit. Default is 10.\n\n    Returns:\n    list[float]: A list of success probabilities for all arms, rounded to 4 decimal places.\n    \"\"\"\n    if n_arms < 1:\n        return []\n\n    # Step 1: Draw n_arms independent samples from a continuous uniform distribution on [0, 1)\n    samples = np.random.uniform(0, 1, n_arms)\n\n    # Step 2: Normalize the drawn numbers by their sum\n    probabilities = samples / samples.sum()\n\n    # Step 3: Round every probability to 4 decimal places and return the list\n    return [round(prob, 4) for prob in probabilities]\n```\n\nThis function generates a list of `n_arms` random success probabilities for a Bernoulli multi-armed bandit. It ensures that the probabilities are normalized such that they sum to 1 and rounds each probability to four decimal places before returning the list. If `n_arms` is less than 1, it returns an empty list."}
{"task_id": 512, "completion_id": 0, "completion": "Here's the implementation of the `pooling_shape` function according to the given specifications:\n\n```python\ndef pooling_shape(pool_shape: tuple[int, int], image_shape: tuple[int, int, int, int], stride: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Calculate the spatial output dimensions of a 2-D pooling layer.\n\n    The function must implement the *no-padding* formula\n        out_h = (h - k_h) / s_h + 1\n        out_w = (w - k_w) / s_w + 1\n    and return ``(out_h, out_w)`` **only** when the result is an integer for\n    both dimensions *and* the kernel fits inside the input.  Otherwise return\n    ``-1``.\n\n    Args:\n        pool_shape: Tuple ``(k_h, k_w)`` pooling window size.\n        image_shape: Tuple ``(n, c, h, w)`` original tensor shape.\n        stride: Tuple ``(s_h, s_w)`` strides.\n\n    Returns:\n        Tuple of integers with the output height and width, or the integer\n        ``-1`` when the configuration is impossible.\n    \"\"\"\n    k_h, k_w = pool_shape\n    n, c, h, w = image_shape\n    s_h, s_w = stride\n\n    # Check if the kernel fits within the input\n    if k_h > h or k_w > w:\n        return -1\n\n    # Calculate the output dimensions\n    out_h = (h - k_h) // s_h + 1\n    out_w = (w - k_w) // s_w + 1\n\n    # Check if both dimensions are integers\n    if out_h.is_integer() and out_w.is_integer():\n        return (int(out_h), int(out_w))\n    else:\n        return -1\n```\n\n### Explanation:\n1. **Input Validation**: \n   - The function first checks if the kernel size (`k_h` and `k_w`) is larger than the image size (`h` and `w`). If so, it returns `-1` because the kernel cannot fit within the input.\n\n2. **Output Calculation**:\n   - It calculates the output height (`out_h`) using the formula `(h - k_h) // s_h + 1`.\n   - Similarly, it calculates the output width (`out_w`) using the formula `(w - k_w) // s_w + 1`.\n\n3. **Integer Check**:\n   - After calculating the output dimensions, the function checks if both dimensions are integers. This is done using the `is_integer()` method.\n   - If both dimensions are integers, it returns them as a tuple `(out_h, out_w)`.\n   - If either dimension is not an integer, it returns `-1`."}
{"task_id": 513, "completion_id": 0, "completion": "Here's how you can implement the `fm_predict` function using NumPy:\n\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays for efficient computation\n    X_np = np.array(X)\n    w_np = np.array(w)\n    V_np = np.array(V)\n    \n    # Calculate the dot product of X and w\n    linear_part = np.dot(X_np, w_np)\n    \n    # Initialize the sum of squared interactions\n    squared_interactions_sum = 0\n    \n    # Compute the sum of squared interactions\n    for f in range(V_np.shape[1]):\n        interaction_term = np.sum(V_np[:, f] * X_np, axis=1)\n        squared_interaction_term = interaction_term ** 2 - (V_np[:, f] ** 2).dot(X_np ** 2)\n        squared_interactions_sum += squared_interaction_term\n    \n    # Combine all parts to get the final prediction\n    predictions = w0 + linear_part + 0.5 * squared_interactions_sum\n    \n    # Round predictions to 4 decimal places\n    return [round(pred, 4) for pred in predictions]\n\n# Example usage:\nX = [[1, 2], [3, 4]]\nw0 = 0.5\nw = [1, 2]\nV = [[0.1, 0.2], [0.3, 0.4]]\n\npredictions = fm_predict(X, w0, w, V)\nprint(predictions)\n```\n\n### Explanation:\n- **Input Conversion**: The input matrices `X`, `w`, and `V` are converted to NumPy arrays for efficient computation.\n- **Linear Part**: The linear part of the prediction is calculated as the dot product of the feature matrix `X` and the weight vector `w`.\n- **Squared Interactions**: The sum of squared interactions is computed for each latent factor `f`. This involves calculating the interaction terms between each feature and the latent factor, squaring them, and subtracting the squared norms of the latent factors from these products.\n- **Final Prediction**: The global bias `w0` is added to the linear part and the sum of squared interactions to obtain the final prediction for each sample.\n- **Rounding**: The predictions are rounded to four decimal places before being returned."}
{"task_id": 514, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef switch_phase(current_phase: str,\n                 set_train: bool | None = None,\n                 set_test: bool | None = None) -> str:\n    \"\"\"Update the training/testing phase.\n\n    Parameters\n    ----------\n    current_phase : str\n        The current phase, must be either \"training\" or \"testing\".\n    set_train : bool | None, optional\n        If given, forces the phase according to its boolean value.\n        ``True``  \u2192 \"training\".\n        ``False`` \u2192 \"testing\".\n    set_test : bool | None, optional\n        If given, forces the phase according to its boolean value.\n        ``True``  \u2192 \"testing\".\n        ``False`` \u2192 \"training\".\n\n    Returns\n    -------\n    str\n        The updated phase (\"training\" or \"testing\").  Returns -1 for\n        invalid inputs.\n    \"\"\"\n    valid_phases = [\"training\", \"testing\"]\n    \n    # Check if the current phase is valid\n    if current_phase not in valid_phases:\n        return -1\n    \n    # If both set_train and set_test are provided, return -1\n    if set_train is not None and set_test is not None:\n        return -1\n    \n    # If only one of set_train or set_test is provided, use it to determine the new phase\n    if set_train is not None:\n        return \"training\" if set_train else \"testing\"\n    elif set_test is not None:\n        return \"testing\" if set_test else \"training\"\n    \n    # If no switches are provided, toggle the current phase\n    return \"training\" if current_phase == \"testing\" else \"testing\"\n\n# Example usage:\nprint(switch_phase(\"training\"))          # Output: \"testing\"\nprint(switch_phase(\"testing\", set_train=True))  # Output: \"training\"\nprint(switch_phase(\"invalid_phase\"))     # Output: -1\nprint(switch_phase(\"training\", set_train=True, set_test=False))  # Output: -1\nprint(switch_phase(\"training\", set_train=True))  # Output: \"training\"\nprint(switch_phase(\"testing\", set_test=True))  # Output: \"testing\"\n```"}
{"task_id": 515, "completion_id": 0, "completion": "To solve the problem of flattening a nested parameter dictionary, we can use a recursive approach. The idea is to traverse each key-value pair in the dictionary. If the value is another dictionary, we recursively flatten it. Otherwise, we add the key-value pair to the result dictionary with the appropriate path constructed using dots.\n\nHere's how you can implement the `flatten_parameters` function:\n\n```python\nfrom typing import Any, Dict\n\ndef flatten_parameters(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Flatten a nested parameter dictionary.\n\n    A dot (`.`) is used to join nested keys.  The input dictionary must not be\n    mutated.\n\n    Args:\n        params: A potentially nested dictionary where some values can be other\n                dictionaries.\n\n    Returns:\n        A new dictionary without nested structures.\n    \"\"\"\n    def _flatten(current_path: str, current_dict: Dict[str, Any], result: Dict[str, Any]):\n        for key, value in current_dict.items():\n            if isinstance(value, dict):\n                # Recursively flatten the nested dictionary\n                _flatten(f\"{current_path}.{key}\", value, result)\n            else:\n                # Add the key-value pair to the result dictionary\n                result[f\"{current_path}.{key}\"] = value\n\n    result = {}\n    _flatten(\"\", params, result)\n    return result\n\n# Example usage:\nparams = {\n    \"optimizer\": {\n        \"type\": \"SGD\",\n        \"config\": {\n            \"lr\": 0.01,\n            \"momentum\": 0.9\n        }\n    },\n    \"epochs\": 25\n}\n\nflattened_params = flatten_parameters(params)\nprint(flattened_params)\n```\n\n### Explanation:\n1. **Recursive Function**: We define a helper function `_flatten` that takes three arguments: the current path (used to construct the flattened key), the current dictionary being processed, and the result dictionary.\n2. **Base Case**: If the value is not a dictionary, we add the key-value pair to the result dictionary with the current path concatenated to the key.\n3. **Recursive Case**: If the value is a dictionary, we recursively call `_flatten` with the updated path and the nested dictionary.\n4. **Result Construction**: We initialize an empty dictionary `result` and call `_flatten` starting from an empty path (`\"\"`) and the input dictionary (`params`).\n5. **Return Result**: Finally, we return the `result` dictionary which contains the flattened parameters.\n\nThis implementation ensures that the input dictionary remains unmodified and that the resulting dictionary is correctly structured with paths joined by dots."}
{"task_id": 516, "completion_id": 0, "completion": "Here's how you can implement the `euclidean_distance` function in Python:\n\n```python\nimport math\nimport numpy as np\n\ndef euclidean_distance(a, b):\n    \"\"\"Compute the Euclidean distance between two numeric vectors.\n\n    The vectors may be provided as Python lists or NumPy arrays and may contain\n    integers or floats.  If the vectors have different numbers of total\n    elements, the function must return -1.\n\n    Args:\n        a (list[int | float] | np.ndarray): First vector.\n        b (list[int | float] | np.ndarray): Second vector.\n\n    Returns:\n        float: Euclidean distance rounded to 4 decimal places, or -1 if the\n               vectors are of unequal length.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    try:\n        arr_a = np.array(a)\n        arr_b = np.array(b)\n    except ValueError:\n        raise ValueError(\"Both inputs must be lists or NumPy arrays.\")\n    \n    # Check if both arrays have the same shape\n    if arr_a.shape != arr_b.shape:\n        return -1\n    \n    # Calculate the Euclidean distance\n    distance_squared = np.sum((arr_a - arr_b) ** 2)\n    distance = math.sqrt(distance_squared)\n    \n    # Round to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\nprint(euclidean_distance([1, 2, 3], [4, 5, 6]))  # Output: 5.1962\nprint(euclidean_distance([1, 2], [3, 4, 5]))      # Output: -1\n```\n\n### Explanation:\n1. **Input Validation**: The function first attempts to convert the input vectors into NumPy arrays. If this fails due to type mismatch, it raises a `ValueError`.\n\n2. **Shape Check**: It checks if the two arrays have the same shape. If not, it returns `-1`, indicating that the vectors cannot be compared for distance calculation.\n\n3. **Distance Calculation**: If the shapes match, it calculates the squared differences between corresponding elements of the arrays using `(arr_a - arr_b) ** 2`. It then sums these squared differences and takes the square root to get the Euclidean distance.\n\n4. **Rounding**: Finally, it rounds the calculated distance to four decimal places before returning it.\n\nThis implementation ensures that the function handles both lists and NumPy arrays correctly and provides the correct output based on the specified requirements."}
{"task_id": 517, "completion_id": 0, "completion": "To implement the sigmoid activation function in Python, we need to ensure numerical stability, especially for large values of \\( z \\). One way to achieve this is by using the `logaddexp2` function from NumPy, which is designed to handle such cases efficiently.\n\nHere's how you can implement the `sigmoid` function:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    z = np.array(z)\n    \n    # Use logaddexp2 for numerical stability\n    exp_z = np.exp(-z)\n    log_add_exp = np.logaddexp(0, -exp_z)\n    \n    # Compute the sigmoid\n    s = 1 / (1 + np.exp(log_add_exp))\n    \n    # Round the result to four decimal places\n    s_rounded = np.round(s, decimals=4)\n    \n    # Return the result based on whether z was a scalar or an array\n    if z.ndim == 0:\n        return float(s_rounded.item())\n    else:\n        return s_rounded.tolist()\n\n# Test the function\nprint(sigmoid([-1, 0, 1]))  # Should return [0.2689, 0.5, 0.7311]\n```\n\n### Explanation:\n\n1. **Input Handling**: The function first converts the input `z` into a NumPy array. This allows us to use vectorized operations, which are efficient and handle large arrays well.\n\n2. **Numerical Stability**: We use `np.logaddexp(0, -exp_z)` instead of directly computing `np.log(1 + exp(-z))`. This approach helps prevent overflow when `z` is very large because it avoids calculating `e^z` directly, which could lead to overflow errors.\n\n3. **Sigmoid Calculation**: After obtaining the log of the sum of exponentials, we compute the sigmoid using the formula \\( g(z) = \\frac{1}{1 + e^{-z}} \\).\n\n4. **Rounding**: The result is rounded to four decimal places using `np.round`.\n\n5. **Output**: The function returns a float if the input was a scalar and a list if the input was an array. This ensures that the output matches the expected structure.\n\nThis implementation should handle various input types and provide accurate results while maintaining numerical stability."}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> any:\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        \n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        \n        return function(actual, predicted)\n    \n    return wrapper\n```\n\nThis implementation defines the `unhot` decorator, which checks if the input arrays are one-hot encoded and converts them to their corresponding class labels before calling the wrapped metric function. The decorator handles both 2D and 1D arrays appropriately, ensuring that only one-hot encoded arrays undergo conversion."}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already one\n    z = np.array(z)\n    \n    # Compute the derivative using element-wise multiplication\n    grad = z * (1 - z)\n    \n    # Round the result to 4 decimal places\n    grad_rounded = np.round(grad, 4)\n    \n    # Return the result as a Python object of the same rank as the input\n    return grad_rounded.tolist()\n\n# Example usage:\nprint(sigmoid_grad([0.5]))  # Output: [0.25]\nprint(sigmoid_grad([[0.3, 0.7], [0.1, 0.9]]))  # Output: [[0.2449, 0.4898], [0.1875, 0.75]]\nprint(sigmoid_grad([]))  # Output: []\n```"}
{"task_id": 521, "completion_id": 0, "completion": "To implement the gradient of the Rectified Linear Unit (ReLU) activation function in Python, we need to define a function `relu_grad` that processes the input according to the given rules. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef relu_grad(z):\n    \"\"\"Compute the element-wise derivative of the ReLU activation.\n\n    The function should return 0 for every entry that is less than or equal to\n    zero and 1 for every entry that is strictly greater than zero.  Scalars\n    must yield a single integer; arrays/lists must yield a list of identical\n    shape.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # Handle scalar case\n        return 1 if z > 0 else 0\n    \n    # Handle numpy array or list case\n    if isinstance(z, np.ndarray):\n        # Use numpy's where function to apply the condition\n        return np.where(z > 0, 1, 0)\n    \n    # Handle nested lists\n    if isinstance(z, list):\n        # Recursively process each element\n        return [relu_grad(sub_z) for sub_z in z]\n    \n    raise ValueError(\"Unsupported input type\")\n\n# Example usage:\nprint(relu_grad(3))          # Output: 1\nprint(relu_grad(-2))         # Output: 0\nprint(relu_grad([1, -2, 3]))   # Output: [1, 0, 1]\nprint(relu_grad(np.array([1, -2, 3])))  # Output: [1, 0, 1]\n```\n\n### Explanation:\n- **Scalar Case**: If the input is a scalar (`int` or `float`), we simply check if it is greater than zero and return `1` or `0` accordingly.\n- **NumPy Array**: We use NumPy's `where` function to create an array where elements greater than zero are set to `1` and others are set to `0`.\n- **List**: For nested lists, we recursively apply the `relu_grad` function to each element and collect the results into a new list.\n- **Error Handling**: If the input type is not supported, a `ValueError` is raised.\n\nThis implementation ensures that the function handles both scalar and multi-dimensional inputs correctly."}
{"task_id": 522, "completion_id": 0, "completion": "Here's the implementation of the `standard_normal` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef standard_normal(weight_shape):\n    \"\"\"Return a tensor of the given shape filled with samples from N(0,1).\n\n    The function *must* reset the NumPy random seed to 42 every time it is\n    called so that the output is reproducible.  It should then draw numbers\n    from the standard normal distribution, round every value to four decimal\n    places and finally convert the NumPy array to a Python list before\n    returning it.\n\n    Args:\n        weight_shape (int | tuple[int, ...] | list[int]): Shape of the tensor\n            to be returned.\n\n    Returns:\n        list: A Python list containing the generated (rounded) values.\n    \"\"\"\n    # Reset the NumPy random seed to ensure reproducibility\n    np.random.seed(42)\n    \n    # Convert the input shape to a tuple if it's not already one\n    if isinstance(weight_shape, int):\n        weight_shape = (weight_shape,)\n    \n    # Generate a NumPy array of the specified shape filled with random numbers from N(0,1)\n    weights = np.random.randn(*weight_shape)\n    \n    # Round each element to four decimal places\n    rounded_weights = np.round(weights, decimals=4)\n    \n    # Convert the NumPy array to a Python list and return\n    return rounded_weights.tolist()\n\n# Example usage:\nprint(standard_normal(3))\nprint(standard_normal((2, 3)))\nprint(standard_normal([4, 5]))\n```\n\n### Explanation:\n1. **Random Seed Reset**: The `np.random.seed(42)` ensures that the random numbers generated are reproducible each time the function is called.\n\n2. **Shape Handling**: The function first checks if the input `weight_shape` is an integer and converts it to a tuple if necessary. This allows for flexibility in handling different types of input shapes.\n\n3. **Generating Random Numbers**: `np.random.randn(*weight_shape)` generates an array of random numbers following a standard normal distribution (mean = 0, variance = 1) with the specified shape.\n\n4. **Rounding**: `np.round(weights, decimals=4)` rounds each element of the array to four decimal places.\n\n5. **Conversion to List**: Finally, `weights.tolist()` converts the NumPy array to a Python list, which is the required format for the function's output."}
{"task_id": 523, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_normal(weight_shape: tuple[int, ...]) -> list:\n    \"\"\"Initialise a weight tensor using the He normal distribution.\n\n    Args:\n        weight_shape: Shape of the desired weight tensor. For dense layers this\n            is usually (fan_in, fan_out). For 2-D convolutions it is\n            (kernel_h, kernel_w, in_channels, out_channels).\n\n    Returns:\n        Nested Python lists containing the initialised weights, rounded to four\n        decimal places.\n    \"\"\"\n    # Determine fan-in based on the shape\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError(\"Unsupported weight shape. Expected 2D or 4D.\")\n\n    # Generate He normal distribution\n    std_dev = np.sqrt(2 / fan_in)\n    rng = np.random.default_rng(seed=0)\n    weights = rng.normal(loc=0, scale=std_dev, size=weight_shape)\n\n    # Round the weights to four decimal places\n    rounded_weights = np.round(weights, decimals=4)\n\n    # Convert the NumPy array to a list of lists\n    return rounded_weights.tolist()\n\n# Example usage:\nprint(he_normal((2, 2)))\n```\n\nThis code defines a function `he_normal` that initializes a weight tensor using the He normal distribution, given the shape of the tensor. The function handles both 2D and 4D shapes and rounds the resulting weights to four decimal places before returning them as a nested Python list."}
{"task_id": 525, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(\n        X: np.ndarray,\n        y: np.ndarray,\n        lambda_: float = 0.0) -> tuple[list[float], float]:\n    \"\"\"Fit a Ridge (L2-regularised) linear regression model.\n\n    The function must compute the optimal weights **analytically** (closed form)\n    and return them together with the intercept term, both rounded to 4\n    decimal places.\n\n    Args:\n        X: 2-D array of shape (m, n) with the input features.\n        y: 1-D or 2-D array of shape (m,) or (m, 1) with the targets.\n        lambda_: Non-negative regularisation strength.\n\n    Returns:\n        (weights, bias)\n        weights \u2013 list of length *n* with the model weights.\n        bias    \u2013 scalar intercept term.\n    \"\"\"\n    # Step 1: Mean-center the data\n    mean_X = np.mean(X, axis=0)\n    mean_y = np.mean(y)\n\n    X_centered = X - mean_X\n    y_centered = y - mean_y\n\n    # Step 2: Solve for the optimal weights\n    X_transpose = X_centered.T\n    I = np.eye(X.shape[1])\n    w = np.linalg.inv(X_transpose @ X + lambda_ * I) @ X_transpose @ y_centered\n\n    # Step 3: Recover the bias\n    bias = mean_y - mean_X @ w\n\n    # Round the results to 4 decimal places\n    weights_rounded = [round(weight, 4) for weight in w]\n    bias_rounded = round(bias, 4)\n\n    return weights_rounded, bias_rounded\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([7, 8, 9])\n# lambda_ = 0.1\n# print(ridge_regression(X, y, lambda_))\n```\n\nThis implementation follows the specified steps to fit a Ridge regression model in closed form without using any third-party libraries. It includes mean centering the data, solving for the weights, and recovering the bias. The results are then rounded to four decimal places before being returned."}
{"task_id": 527, "completion_id": 0, "completion": "Here's a Python implementation of the `extract_archive` function based on the provided requirements:\n\n```python\nimport os\nfrom typing import List, Union\n\ndef extract_archive(file_path: str, path: str = '.', archive_format: Union[str, List[str], None] = 'auto') -> bool:\n    \"\"\"Checks whether *file_path* belongs to an allowed archive format.\n\n    The function MUST NOT raise any exception and MUST NOT perform real file\n    extraction.  Read the task description for the full specification.\n\n    Args:\n        file_path: Full path (or name) of the archive file.\n        path: Destination folder \u2013 ignored in this simplified version.\n        archive_format: See task description.\n\n    Returns:\n        True if the file name matches one of the allowed formats, False otherwise.\n    \"\"\"\n    # Define the supported archive formats\n    SUPPORTED_FORMATS = {\n        'tar': ['.tar', '.tar.gz', '.tgz', '.tar.bz', '.tar.bz2', '.tbz'],\n        'zip': ['.zip']\n    }\n    \n    # Convert archive_format to a set for quick lookup\n    valid_formats = set()\n    \n    if archive_format == 'auto':\n        valid_formats.update(SUPPORTED_FORMATS['tar'], SUPPORTED_FORMATS['zip'])\n    elif isinstance(archive_format, str):\n        valid_formats.add(archive_format.lower())\n    elif isinstance(archive_format, list):\n        valid_formats.update(format.lower() for format in archive_format)\n    elif archive_format is None or not archive_format:\n        return False\n    \n    # Extract the file extension from the file_path\n    _, file_extension = os.path.splitext(file_path)\n    \n    # Check if the file extension matches any of the valid formats\n    return file_extension.lower() in valid_formats\n\n# Example usage:\nprint(extract_archive('example.tar.gz'))  # True\nprint(extract_archive('example.zip'))    # True\nprint(extract_archive('example.docx'))  # False\nprint(extract_archive('example.tar.gz', archive_format='zip'))  # False\nprint(extract_archive('example.tar.gz', archive_format=['tar']))  # True\nprint(extract_archive('example.tar.gz', archive_format=[]))  # False\n```\n\n### Explanation:\n- **Supported Formats**: We define a dictionary `SUPPORTED_FORMATS` containing the supported archive types and their respective file extensions.\n- **Valid Formats Set**: We convert the `archive_format` parameter into a set for efficient lookup.\n- **Case Insensitivity**: Both the file extension and the archive format are converted to lowercase before comparison.\n- **File Extension Extraction**: We use `os.path.splitext` to separate the file name and its extension.\n- **Matching**: We check if the file extension exists in the set of valid formats and return `True` if it does, otherwise `False`."}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Extract the minimum and maximum values of each feature from X\n    min_x1, max_x1 = np.min(X[:, 0]), np.max(X[:, 0])\n    min_x2, max_x2 = np.min(X[:, 1]), np.max(X[:, 1])\n\n    # Create a grid of points spanning the bounding box of X\n    x1_plot = np.linspace(min_x1, max_x1, grid_n)\n    x2_plot = np.linspace(min_x2, max_x2, grid_n)\n\n    # Create a mesh-grid from the 1D arrays\n    X_grid, Y_grid = np.meshgrid(x1_plot, x2_plot)\n\n    # Flatten the mesh-grid to a 2D array for processing\n    X_flat = X_grid.flatten()\n    Y_flat = Y_grid.flatten()\n\n    # Compute the linear combination of weights and add the bias\n    Z_flat = W[0] * X_flat + W[1] * Y_flat + b\n\n    # Apply the sign function to get the classification labels\n    Z = np.sign(Z_flat).reshape(grid_n, grid_n)\n\n    # Convert the result to a list of lists\n    return Z.tolist()\n\n# Example usage:\n# X = [[-1, -1], [-1, 1], [1, -1], [1, 1]]\n# W = [1, 1]\n# b = 0\n# print(decision_boundary_grid(X, W, b))\n```\n\nThis function generates a grid of points within the bounding box of the input data `X` and classifies each point using the given linear classifier parameters `W` and `b`. The resulting grid of classifications is returned as a list of lists containing only the integers 1 and -1."}
{"task_id": 529, "completion_id": 0, "completion": "Here's how you can implement the `generate_progress_bar` function according to the specified requirements:\n\n```python\nimport math\n\ndef generate_progress_bar(current: int, target: int | None, width: int = 30) -> str:\n    \"\"\"Return an ASCII progress bar string similar to Keras' *Progbar*.\n\n    Parameters\n    ----------\n    current : int\n        The index of the most recently completed step.\n    target : int | None\n        The total number of steps, or ``None`` if it is unknown.\n    width : int, default=30\n        The length (in characters) of the bar itself (everything situated\n        between the two square brackets).\n\n    Returns\n    -------\n    str\n        A textual progress bar that fully complies with the specification\n        given in the task description (all spaces, brackets, arrows and dots\n        have to be placed exactly).\n    \"\"\"\n    if target is None:\n        # If target is unknown, return a string with the current count and \"Unknown\"\n        return f\"{current:>6}/Unknown\"\n\n    # Calculate the number of digits required for the target value\n    num_digits = len(str(target))\n\n    # Format the current step with the appropriate number of leading zeros\n    formatted_current = f\"{current:{num_digits}d}\"\n\n    # Calculate the progress width based on the current and target values\n    prog_width = int(width * current / target)\n\n    # Construct the progress bar string\n    bar = '[' + '=' * (prog_width - 1)\n    if current < target:\n        bar += '>'\n    else:\n        bar += '='\n    bar += '.' * (width - prog_width) + ']'\n\n    # Combine the formatted current, target, and progress bar into the final result\n    return f\"{formatted_current}/{target} {bar}\"\n\n# Example usage:\nprint(generate_progress_bar(5, 10))  # Output: \"05/10 [=====>......]\"\nprint(generate_progress_bar(10, 10)) # Output: \"10/10 [==================]\"\nprint(generate_progress_bar(0, None))  # Output: \"00/Unknown \"\n```\n\n### Explanation:\n- **Target Unknown**: When `target` is `None`, the function formats the output as `\"current/Unknown\"`, ensuring that the current step is displayed correctly but no progress bar is shown.\n- **Target Known**: \n  - The number of digits required for the `target` is calculated using `len(str(target))`.\n  - The current step is formatted with leading zeros to ensure it matches the required number of digits.\n  - The progress width is calculated as `int(width * current / target)`.\n  - The progress bar is constructed by appending the appropriate number of `=` characters, an optional `>`, and trailing dots to fill the remaining space.\n  - Finally, the formatted current, target, and progress bar are combined into the final result string."}
{"task_id": 530, "completion_id": 0, "completion": "To compute the mean binary cross-entropy loss, we need to follow these steps:\n\n1. Clip the predicted probabilities `y_pred` to ensure they fall within the valid range `[1e-7, 1 - 1e-7]`.\n2. Calculate the loss for each sample using the formula:\n   \\[\n   \\ell = -[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) ]\n   \\]\n3. Compute the mean of these losses across all samples.\n4. Round the result to four decimal places.\n\nHere's the complete implementation of the function:\n\n```python\nimport numpy as np\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates the mean Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same shape\n        as *y_true*.\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n    \"\"\"\n    # Ensure y_pred is clipped to avoid log(0) errors\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    \n    # Calculate the loss for each sample\n    loss = - (y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    \n    # Compute the mean loss\n    mean_loss = np.mean(loss)\n    \n    # Return the mean loss rounded to 4 decimal places\n    return round(mean_loss, 4)\n\n# Example usage:\n# y_true = np.array([0, 1, 0, 1])\n# y_pred = np.array([0.1, 0.9, 0.8, 0.2])\n# print(binary_cross_entropy(y_true, y_pred))  # Output should be 0.6931\n```\n\nThis function will correctly compute the mean binary cross-entropy loss for the given inputs, ensuring numerical stability by clipping the predicted probabilities."}
{"task_id": 531, "completion_id": 0, "completion": "To implement the generalized cosine window generator as described, we need to follow these steps:\n\n1. Determine the phase variable $\\theta_n$ based on whether the window is symmetric or periodic.\n2. Compute the window values using the given coefficients.\n3. Round the results to four decimal places and return them as a list.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef generalized_cosine(window_len: int, coefs: list[float], symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a generalized cosine window.\n\n    The function should create either a periodic or a symmetric generalized\n    cosine window according to the *symmetric* flag and return it rounded to\n    four decimal places.\n\n    Args:\n        window_len (int): Desired length *L* of the final window (must be\n            strictly positive).\n        coefs (list[float]): List of coefficients ``a_k``.\n        symmetric (bool, optional): If *True* produce a symmetric window of\n            length *L*; otherwise produce a periodic window suitable for the\n            FFT.  Defaults to *False*.\n\n    Returns:\n        list[float]: Window values rounded to four decimals.\n    \"\"\"\n    if not isinstance(window_len, int) or window_len <= 0:\n        raise ValueError(\"Window length must be a strictly positive integer.\")\n    \n    if len(coefs) != window_len - 1:\n        raise ValueError(\"The number of coefficients must be one less than the window length.\")\n    \n    # Determine the phase variable theta_n\n    if symmetric:\n        theta_n = np.linspace(-np.pi, np.pi, window_len)\n    else:\n        theta_n = np.linspace(-np.pi, np.pi - 2 * np.pi / window_len, window_len)\n    \n    # Compute the window values\n    window_values = np.sum([coef * np.cos(k * theta_n) for k, coef in enumerate(coefs)], axis=0)\n    \n    # Round the results to four decimal places\n    window_values_rounded = [round(val, 4) for val in window_values]\n    \n    return window_values_rounded\n\n# Example usage:\nwindow_len = 10\ncoefs = [0.5, -0.8660254037844386, 0.5]\nsymmetric = True\nprint(generalized_cosine(window_len, coefs, symmetric))\n```\n\n### Explanation:\n\n- **Phase Variable Calculation**: \n  - For a symmetric window, $\\theta_n$ is sampled from $-\\pi$ to $\\pi$.\n  - For a periodic window, $\\theta_n$ is sampled from $-\\pi$ to $\\pi - \\frac{2\\pi}{L}$ to ensure the last point is discarded.\n\n- **Window Value Computation**:\n  - We use a list comprehension to compute the weighted sum of cosines for each coefficient.\n\n- **Rounding**:\n  - The results are rounded to four decimal places using a list comprehension.\n\nThis function will generate the desired generalized cosine window based on the specified parameters."}
{"task_id": 532, "completion_id": 0, "completion": "```python\nfrom typing import *\nfrom collections import *\n\ndef count_fc_parameters(layers: list[int]) -> int:\n    \"\"\"Calculate total trainable parameters (weights + biases) in a fully-connected network.\n\n    Args:\n        layers: List of integers where each element represents the number of\n            neurons in the respective layer (input layer first).\n\n    Returns:\n        An integer \u2013 total count of trainable parameters. Returns 0 when fewer\n        than two layers are supplied.\n    \"\"\"\n    if len(layers) < 2:\n        return 0\n    \n    total_params = 0\n    for i in range(1, len(layers)):\n        n_in = layers[i - 1]\n        n_out = layers[i]\n        total_params += n_in * n_out  # Weights\n        total_params += n_out         # Biases\n    \n    return total_params\n```"}
{"task_id": 533, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `gaussian_likelihood` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef gaussian_likelihood(x, mean, sigma):\n    \"\"\"Compute the probability density of a normal distribution.\n\n    Parameters\n    ----------\n    x : float | list[float] | np.ndarray\n        Point(s) where the density will be evaluated.\n    mean : float\n        The mean (\u00b5) of the distribution.\n    sigma : float\n        The standard deviation (\u03c3) of the distribution.\n\n    Returns\n    -------\n    float | list[float]\n        Density value(s) rounded to 4 decimal places. A single float is\n        returned for a scalar x, otherwise a list is returned.\n    \"\"\"\n    if not isinstance(sigma, (int, float)):\n        raise ValueError(\"Sigma must be a float.\")\n    \n    if sigma <= 0:\n        raise ValueError(\"Sigma must be greater than 0.\")\n    \n    # Convert x to a NumPy array for vectorized operations\n    x = np.array(x)\n    \n    # Calculate the exponent term\n    exponent_term = -((x - mean) ** 2) / (2 * sigma ** 2)\n    \n    # Calculate the denominator term\n    denominator_term = np.sqrt(2 * np.pi) * sigma\n    \n    # Compute the PDF values\n    pdf_values = 1 / denominator_term * np.exp(exponent_term)\n    \n    # Round the results to 4 decimal places\n    pdf_values_rounded = np.round(pdf_values, decimals=4)\n    \n    # Return the result as a list\n    return pdf_values_rounded.tolist()\n\n# Example usage:\n# print(gaussian_likelihood(0, 0, 1))  # Single point\n# print(gaussian_likelihood([0, 1, 2], 0, 1))  # List of points\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if `sigma` is a valid float and greater than zero. If not, it raises a `ValueError`.\n- **Vectorization**: The input `x` is converted to a NumPy array to enable efficient computation using vectorized operations.\n- **Exponent and Denominator Terms**: The exponential term is calculated using `-((x - mean) ** 2) / (2 * sigma ** 2)` and the denominator term is computed as `np.sqrt(2 * np.pi) * sigma`.\n- **PDF Calculation**: The PDF values are calculated by dividing the exponent term by the denominator term.\n- **Rounding**: The resulting PDF values are rounded to four decimal places using `np.round`.\n- **Output**: The final result is returned as a list using `tolist()` from NumPy. This ensures compatibility with both scalar and list inputs."}
{"task_id": 534, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_(Z):\n    \"\"\"Compute the logistic sigmoid activation element-wise.\n\n    Args:\n        Z: A scalar, 1-D list/array, or 2-D list/array of real numbers.\n\n    Returns:\n        Sigmoid-transformed value(s) rounded to 4 decimals.  Returns a float for\n        scalar input; otherwise returns a Python list preserving the input\n        shape.\n    \"\"\"\n    # Convert input to a NumPy array if it's not already\n    Z = np.array(Z)\n    \n    # Compute the sigmoid using NumPy's exp function\n    sigmoid_values = 1 / (1 + np.exp(-Z))\n    \n    # Round the results to 4 decimal places\n    rounded_values = np.round(sigmoid_values, 4)\n    \n    # If the input was a scalar, return a float\n    if len(rounded_values.shape) == 0:\n        return rounded_values.item()\n    \n    # Otherwise, convert back to a Python list\n    return rounded_values.tolist()\n\n# Example usage:\nprint(sigmoid_(0))  # Should print 0.5000\nprint(sigmoid_([0, 1]))  # Should print [0.5000, 0.7311]\nprint(sigmoid([[0, 1], [-1, 0]]))  # Should print [[0.5000, 0.7311], [0.2689, 0.5000]]\n```\n\nThis solution defines a function `sigmoid_` that takes a single argument `Z`, which can be a scalar, a 1-dimensional list, or a 2-dimensional list. It converts the input into a NumPy array, computes the sigmoid using the exponential function, rounds the results to four decimal places, and then checks if the output is a scalar. If so, it returns the scalar value; otherwise, it converts the NumPy array back to a Python list and returns it."}
{"task_id": 536, "completion_id": 0, "completion": "Here's the implementation of the `linear_regression` function that supports both the Normal Equation and Batch Gradient Descent (BGD) for linear regression:\n\n```python\nimport numpy as np\n\ndef linear_regression(X_train, y_train, X_test, method=\"normal\", alpha=0.01, num_iter=1000):\n    \"\"\"Linear regression with two training modes (Normal Equation or BGD).\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Feature matrix for training data of shape (m, n).\n    y_train : np.ndarray\n        Target vector for training data of length m.\n    X_test : np.ndarray\n        Feature matrix for which predictions are required, shape (k, n).\n    method : str, optional\n        \"normal\" (default) for the closed-form solution; any other string\n        triggers batch gradient descent.\n    alpha : float, optional\n        Learning rate for gradient descent (ignored for the normal equation).\n    num_iter : int, optional\n        Number of gradient descent iterations (ignored for the normal equation).\n\n    Returns\n    -------\n    list[float]\n        Predictions for each row in X_test, rounded to 4 decimal places.\n    \"\"\"\n    # Add a column of ones to X_train for the bias term\n    X_train = np.column_stack((np.ones(len(X_train)), X_train))\n    \n    if method == \"normal\":\n        # Closed-form solution using Normal Equation\n        theta_normal = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n    else:\n        # Initialize theta with zeros\n        theta_bgd = np.zeros(X_train.shape[1])\n        \n        # Batch Gradient Descent\n        for _ in range(num_iter):\n            h = X_train @ theta_bgd\n            gradient = 2 / len(y_train) * X_train.T @ (h - y_train)\n            theta_bgd -= alpha * gradient\n    \n    # Add a column of ones to X_test for prediction\n    X_test = np.column_stack((np.ones(len(X_test)), X_test))\n    \n    # Predict using the learned parameters\n    predictions = X_test @ theta_bgd\n    \n    # Round predictions to 4 decimal places\n    rounded_predictions = np.round(predictions, 4)\n    \n    return rounded_predictions.tolist()\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4]])\n# y_train = np.array([3, 5, 7])\n# X_test = np.array([[4, 5], [5, 6]])\n# print(linear_regression(X_train, y_train, X_test, method=\"normal\"))\n# print(linear_regression(X_train, y_train, X_test, method=\"bgd\", alpha=0.01, num_iter=1000))\n```\n\n### Explanation:\n- **Feature Matrix**: The function first adds a column of ones to the input feature matrix `X_train` to account for the bias term.\n- **Normal Equation**: If the `method` is `\"normal\"`, it calculates the parameter vector `theta` using the formula derived from the Normal Equation.\n- **Batch Gradient Descent**: If the `method` is not `\"normal\"`, it initializes `theta` to zero and iteratively updates it using the gradient descent algorithm. The learning rate `alpha` and the number of iterations `num_iter` are used to control the optimization process.\n- **Prediction**: After training, the function predicts the target values for the test set `X_test` and rounds them to four decimal places before returning them."}
{"task_id": 537, "completion_id": 0, "completion": "To implement a Gaussian Naive Bayes classifier from scratch, we need to follow the steps outlined in the task description. Below is the complete implementation:\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    def calculate_mean_std(X, y):\n        means = {}\n        stds = {}\n        for i in range(2):  # Class 0 and Class 1\n            mask = y == i\n            if not mask.any():\n                continue\n            means[i] = X[mask].mean(axis=0)\n            stds[i] = X[mask].std(axis=0)\n        return means, stds\n\n    def log_likelihood(x, mu, sigma, epsilon=1e-9):\n        return -np.log(np.sqrt(2 * np.pi * sigma + epsilon)) - ((x - mu) ** 2) / (2 * sigma + epsilon)\n\n    # Calculate mean and standard deviation for each feature for both classes\n    means, stds = calculate_mean_std(X_train, y_train)\n\n    # Calculate class priors\n    class_priors = np.bincount(y_train) / len(y_train)\n\n    # Initialize predictions\n    predictions = []\n\n    # Predict for each test sample\n    for x in X_test:\n        posteriors = []\n        for i in range(2):\n            mu_i = means[i]\n            sigma_i = stds[i]\n            likelihood = np.prod([log_likelihood(xi, mu_i[j], sigma_i[j]) for j, xi in enumerate(x)])\n            prior = class_priors[i]\n            posterior = likelihood * prior\n            posteriors.append(posterior)\n        \n        # Predict the class with the higher posterior probability\n        prediction = np.argmax(posteriors)\n        predictions.append(prediction)\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y_train = np.array([0, 0, 1, 1])\n# X_test = np.array([[1.5, 2.5], [3.5, 4.5]])\n# print(gaussian_naive_bayes(X_train, y_train, X_test))\n```\n\n### Explanation:\n1. **Mean and Standard Deviation Calculation**: We first calculate the mean and standard deviation of each feature for both classes. This involves iterating over each class and computing the mean and standard deviation of the features that belong to that class.\n\n2. **Log Likelihood Function**: We define a helper function to compute the log likelihood of a feature value given its mean and standard deviation, incorporating a small epsilon to prevent division by zero.\n\n3. **Class Priors**: We calculate the class priors as the relative frequency of each class in the training set.\n\n4. **Prediction**: For each test sample, we compute the posterior probability for each class using the product of the likelihood of the features and the class prior. We then predict the class with the highest posterior probability.\n\nThis implementation assumes a binary classification problem, but it can be easily extended to multi-class classification by modifying the class handling logic."}
{"task_id": 539, "completion_id": 0, "completion": "Certainly! Below is the implementation of the binary cross-entropy cost function using NumPy:\n\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy cost.\n\n    Parameters:\n    AL (np.ndarray): The predicted probabilities from the output layer of shape (1, m) or (m,)\n    Y (np.ndarray): The true labels as binary vectors of shape (1, m) or (m,)\n\n    Returns:\n    float: The computed cost as a plain Python float rounded to 4 decimal places.\n    \"\"\"\n    # Clip probabilities to avoid numerical overflow\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    \n    # Compute the loss for each example\n    loss = -(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n    \n    # Calculate the average loss over all samples\n    cost = np.mean(loss)\n    \n    # Round the result to 4 decimal places\n    return round(cost, 4)\n\n# Example usage:\nAL = np.array([[0.99, 0.01], [0.85, 0.15]])\nY = np.array([[1, 0], [1, 0]])\n\nprint(compute_cost(AL, Y))  # Output will depend on the values of AL and Y\n```\n\n### Explanation:\n1. **Clipping Probabilities**: The function clips the probabilities `AL` to ensure they do not exceed the range `[1e-15, 1 - 1e-15]`. This prevents numerical overflow when computing the logarithms.\n\n2. **Loss Calculation**: For each element in the prediction array `AL`, the loss is calculated using the formula:\n   \\[\n   \\text{loss} = -[y \\cdot \\log(a) + (1-y) \\cdot \\log(1-a)]\n   \\]\n   where \\( y \\) is the true label and \\( a \\) is the predicted probability.\n\n3. **Average Loss**: The average loss is computed by dividing the sum of losses by the number of samples `m`.\n\n4. **Rounding**: The final cost is rounded to four decimal places before being returned."}
{"task_id": 540, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `derivative_sigmoid` function, which calculates the element-wise derivative of the sigmoid function for a given NumPy array `Z`. The result is rounded to four decimal places and returned as a Python list.\n\n```python\nimport numpy as np\n\ndef derivative_sigmoid(Z: np.ndarray) -> list:\n    \"\"\"Return the element-wise derivative of the sigmoid function.\n\n    Args:\n        Z: NumPy array (any shape) containing numeric values.\n\n    Returns:\n        A Python list with the same shape as *Z*, containing the\n        derivative of the sigmoid evaluated at each element, rounded to\n        4 decimal places.\n    \"\"\"\n    # Calculate the sigmoid values\n    sigmoid_values = 1 / (1 + np.exp(-Z))\n    \n    # Calculate the derivative of the sigmoid\n    derivative_values = sigmoid_values * (1 - sigmoid_values)\n    \n    # Round the derivative values to 4 decimal places\n    rounded_derivative_values = np.round(derivative_values, decimals=4)\n    \n    # Convert the resulting array to a Python list\n    return rounded_derivative_values.tolist()\n\n# Example usage:\nZ = np.array([0, 1, 2, -1])\nresult = derivative_sigmoid(Z)\nprint(result)\n```\n\nThis function first computes the sigmoid values using the formula `\u03c3(z) = 1 / (1 + e^(-z))`. It then calculates the derivative using the formula `\u03c3'(z) = \u03c3(z) * (1 - \u03c3(z))`. Finally, it rounds the derivative values to four decimal places and converts the resulting NumPy array to a Python list before returning it."}
{"task_id": 541, "completion_id": 0, "completion": "To implement the function that computes the gradient of the Binary Cross-Entropy (BCE) loss with respect to the discriminator's inputs (logits), we need to follow these steps:\n\n1. Ensure both `y` and `y_hat` have the same shape.\n2. Calculate the element-wise difference between `y_hat` and `y`.\n3. Round each element of the resulting gradient to four decimal places.\n4. Convert the final gradient array to a Python list using `tolist()`.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef return_input_grads(y, y_hat):\n    \"\"\"Compute the gradient of Binary Cross-Entropy loss with respect to the\n    network inputs (logits).\n\n    Parameters\n    ----------\n    y : array-like\n        Ground-truth binary labels (0 or 1).  Can be a scalar, 1-D list/array\n        or 2-D nested list/array.\n    y_hat : array-like\n        Predicted probabilities (after sigmoid, values in the range 0-1) with\n        the same shape as *y*.\n\n    Returns\n    -------\n    list\n        Gradient of the BCE loss w.r.t. each logit, rounded to 4 decimal\n        places, returned as a regular Python list (with the original shape).\n    \"\"\"\n    # Ensure y and y_hat are numpy arrays\n    y = np.array(y)\n    y_hat = np.array(y_hat)\n    \n    # Check if shapes match\n    if y.shape != y_hat.shape:\n        raise ValueError(\"The shapes of y and y_hat must be the same.\")\n    \n    # Compute the gradient\n    grad = y_hat - y\n    \n    # Round the gradient to 4 decimal places\n    grad_rounded = np.round(grad, decimals=4)\n    \n    # Convert the result to a regular Python list\n    return grad_rounded.tolist()\n\n# Example usage:\ny = [0, 1, 0]\ny_hat = [0.2, 0.7, 0.9]\nprint(return_input_grads(y, y_hat))\n```\n\n### Explanation:\n- We first convert `y` and `y_hat` to numpy arrays to handle numerical operations efficiently.\n- We check if the shapes of `y` and `y_hat` are the same. If not, we raise a `ValueError`.\n- We compute the element-wise difference `y_hat - y` to get the gradient.\n- We use `np.round()` to round each element of the gradient to four decimal places.\n- Finally, we convert the rounded gradient array to a Python list using `tolist()` and return it.\n\nThis function will correctly compute the desired gradient for any input shapes where the dimensions match."}
{"task_id": 542, "completion_id": 0, "completion": "Here's the implementation of the RMSProp optimizer:\n\n```python\nimport numpy as np\n\ndef rmsprop_update(\n    params: list[np.ndarray],\n    grads: list[np.ndarray],\n    s: list[np.ndarray] | None = None,\n    alpha: float = 0.01,\n    beta: float = 0.9,\n    epsilon: float = 1e-8,\n) -> tuple[list[np.ndarray], list[np.ndarray]]:\n    \"\"\"Implements a single optimization step of the RMSProp algorithm.\"\"\"\n    \n    # Initialize running averages if they are not provided\n    if s is None:\n        s = [np.zeros_like(g) for g in grads]\n    \n    # Update the running averages\n    new_s = []\n    for param, grad, avg in zip(params, grads, s):\n        avg = beta * avg + (1 - beta) * grad**2\n        new_s.append(avg)\n    \n    # Perform the parameter update\n    new_params = []\n    for param, grad, avg in zip(params, grads, new_s):\n        new_param = param - alpha * grad / (np.sqrt(avg) + epsilon)\n        new_params.append(np.round(new_param, 6))\n    \n    return new_params, new_s\n```\n\n### Explanation:\n1. **Initialization**: \n   - If the `s` list is `None`, we initialize it with zero arrays of the same shape as each gradient.\n\n2. **Running Average Update**:\n   - For each pair of parameter and gradient, we update the running average \\( s \\) using the formula:\n     \\[\n     s \\leftarrow \\beta \\cdot s + (1 - \\beta) \\cdot g^2\n     \\]\n\n3. **Parameter Update**:\n   - We compute the update for each parameter using the formula:\n     \\[\n     \\theta \\leftarrow \\theta - \\alpha \\cdot \\frac{g}{\\sqrt{s} + \\epsilon}\n     \\]\n   - The result is rounded to six decimal places using `np.round`.\n\n4. **Return Values**:\n   - The function returns a tuple containing the updated parameters and the updated running averages."}
{"task_id": 543, "completion_id": 0, "completion": "To calculate the Gini index for a given list of class labels, we need to follow these steps:\n\n1. Count the frequency of each unique label in the list using `collections.Counter`.\n2. Calculate the probability of each label by dividing its count by the total number of samples.\n3. Compute the Gini index using the formula provided.\n\nHere's the implementation of the function `cal_gini_index`:\n\n```python\nfrom collections import Counter\n\ndef cal_gini_index(data: list) -> float:\n    \"\"\"Calculate the Gini index of a list of class labels.\n\n    Args:\n        data: A list of hashable values representing class labels.\n\n    Returns:\n        The Gini index rounded to 4 decimal places. If the input list is empty\n        return 0.0.\n    \"\"\"\n    if not data:\n        return 0.0\n    \n    # Count the frequency of each unique label\n    label_counts = Counter(data)\n    \n    # Calculate the total number of samples\n    total_samples = len(data)\n    \n    # Calculate the Gini index\n    gini_index = 1.0 - sum((count / total_samples) ** 2 for count in label_counts.values())\n    \n    # Return the Gini index rounded to 4 decimal places\n    return round(gini_index, 4)\n\n# Example usage:\nprint(cal_gini_index([1, 2, 2, 3, 3, 3]))  # Output: 0.3750\n```\n\n### Explanation:\n\n- **Empty List**: If the input list `data` is empty, the function immediately returns `0.0`.\n- **Label Counts**: We use `Counter` to count the occurrences of each label in the list.\n- **Total Samples**: The total number of samples is determined by the length of the list.\n- **Gini Index Calculation**: For each label, we calculate its probability and then square it. The Gini index is computed by subtracting the sum of these squared probabilities from 1.\n- **Rounding**: The result is rounded to four decimal places before being returned.\n\nThis function efficiently computes the Gini index using only the standard library and handles edge cases like an empty list gracefully."}
{"task_id": 544, "completion_id": 0, "completion": "To implement the binary threshold split as described, we can iterate over the input data and separate the samples based on whether their feature value at the specified index is greater than or equal to the given threshold. Here's how you can achieve this:\n\n```python\ndef split_tree(data: list, fea: int, value: float):\n    \"\"\"Split a data set into two ordered subsets according to a threshold.\n\n    Args:\n        data (list): List of samples. Each sample is a list or tuple of feature values.\n        fea (int): Index of the feature used for the split.\n        value (float): Threshold for the split.\n\n    Returns:\n        tuple[list, list]: Two lists (set1, set2) where set1 contains the samples\n        with feature value >= threshold and set2 contains the rest.\n    \"\"\"\n    if not data:\n        return ([], [])\n    \n    set1 = []\n    set2 = []\n    \n    for sample in data:\n        if sample[fea] >= value:\n            set1.append(sample)\n        else:\n            set2.append(sample)\n    \n    return (set1, set2)\n\n# Example usage:\ndata = [\n    [1, 5],\n    [2, 3],\n    [3, 7],\n    [4, 6]\n]\nfea = 0\nvalue = 2\n\nset1, set2 = split_tree(data, fea, value)\nprint(\"Set 1:\", set1)  # Output: [[2, 3], [3, 7]]\nprint(\"Set 2:\", set2)  # Output: [[1, 5], [4, 6]]\n```\n\n### Explanation:\n- We first check if the `data` list is empty. If it is, we return two empty lists immediately.\n- We initialize two empty lists, `set1` and `set2`, to store the samples that meet and do not meet the threshold condition, respectively.\n- We iterate over each sample in the `data` list.\n- For each sample, we check its value at the specified `fea` index. If the value is greater than or equal to the `value`, we add the sample to `set1`; otherwise, we add it to `set2`.\n- Finally, we return the two lists as a tuple `(set1, set2)`. This ensures that the order of samples in each subset matches their original order in the `data` list."}
{"task_id": 545, "completion_id": 0, "completion": "To solve the problem of calculating the number of leaf nodes and the maximum depth of a binary decision tree using a single recursive traversal, we can define a helper function that performs both calculations simultaneously. Here's how you can implement the `tree_stats` function:\n\n```python\ndef tree_stats(root: \"node | None\") -> tuple[int, int]:\n    \"\"\"Return the number of leaf nodes and the maximum depth of the tree.\n\n    A **leaf** is defined as a node whose ``results`` attribute is not ``None``.\n\n    Parameters\n    ----------\n    root : node | None\n        Root of the decision tree.  If *None*, the tree is considered empty.\n\n    Returns\n    -------\n    tuple[int, int]\n        ``(leaf_count, max_depth)`` where\n        * *leaf_count* is the total number of leaves in the tree, and\n        * *max_depth* is the length of the longest path from *root* to a leaf.\n    \"\"\"\n    if root is None:\n        return 0, 0\n\n    def traverse(node, current_depth):\n        # Base case: if it's a leaf, increment the leaf count\n        if node.results is not None:\n            leaf_count += 1\n        \n        # Recursively calculate the max depth for the left and right subtrees\n        left_depth, right_depth = traverse(node.left, current_depth + 1), traverse(node.right, current_depth + 1)\n        \n        # Update the maximum depth found so far\n        max_depth = max(current_depth, left_depth, right_depth)\n        \n        return leaf_count, max_depth\n\n    leaf_count, max_depth = traverse(root, 1)\n    return leaf_count, max_depth\n```\n\n### Explanation:\n\n1. **Base Case**: If the current node is `None`, return `(0, 0)` because there are no nodes to process.\n\n2. **Leaf Check**: If the current node is a leaf (i.e., its `results` attribute is not `None`), increment the `leaf_count`.\n\n3. **Recursive Traversal**: For each node, recursively call the `traverse` function on its left and right children, increasing the `current_depth` by 1 for each step down the tree.\n\n4. **Update Maximum Depth**: During the recursion, keep track of the maximum depth encountered so far. This is done by comparing the current depth with the depths of the left and right subtrees.\n\n5. **Return Results**: After processing all nodes, return the total `leaf_count` and the `max_depth`.\n\nThis approach ensures that we compute both statistics in a single traversal of the tree, adhering to the constraints provided."}
{"task_id": 546, "completion_id": 0, "completion": "To implement the CART decision tree construction algorithm using the Gini impurity criterion, we need to follow these steps:\n\n1. **Compute the current node's Gini impurity**: This involves calculating the weighted sum of the Gini impurities of the child nodes.\n\n2. **Iterate over features and possible split values**: For each feature, find all unique values it can take. For each unique value, split the data into two subsets: those where the feature matches the value and those where it does not.\n\n3. **Calculate the Gini gain for each split**: The Gini gain is calculated as the difference between the original Gini impurity and the weighted sum of the Gini impurities of the child nodes.\n\n4. **Select the best split**: Choose the split that results in the highest Gini gain. If there is a tie, select the first split encountered.\n\n5. **Create a leaf if no positive gain is obtained**: If no split results in a positive Gini gain, create a leaf node with the majority class of the current subset.\n\n6. **Recursive building of the tree**: Recursively apply the above steps to the left and right subsets until all leaves are reached.\n\nHere's the implementation of the `build_tree` function:\n\n```python\nfrom collections import Counter\nfrom typing import Any, Dict, List\n\ndef gini_impurity(labels: List[Any]) -> float:\n    \"\"\"Calculate the Gini impurity of a set of labels.\"\"\"\n    total_count = len(labels)\n    if total_count == 0:\n        return 0\n    \n    counts = Counter(labels)\n    gini = 1.0\n    for count in counts.values():\n        probability = count / total_count\n        gini -= probability ** 2\n    return gini\n\ndef build_tree(data: List[List[Any]]) -> Dict[str, Any]:\n    \"\"\"Build a CART decision tree using the Gini impurity criterion.\"\"\"\n    \n    def get_unique_values(feature_index: int, data: List[List[Any]]) -> List[Any]:\n        \"\"\"Get all unique values for a given feature from the data.\"\"\"\n        return list(set(sample[feature_index] for sample in data))\n    \n    def get_best_split(data: List[List[Any]], feature_index: int) -> Tuple[int, Any, float]:\n        \"\"\"Find the best split for a given feature.\"\"\"\n        unique_values = get_unique_values(feature_index, data)\n        \n        best_gain = -float('inf')\n        best_feature_value = None\n        \n        for value in unique_values:\n            left_subset = [sample for sample in data if sample[feature_index] == value]\n            right_subset = [sample for sample in data if sample[feature_index] != value]\n            \n            if not left_subset or not right_subset:\n                continue\n            \n            left_gini = gini_impurity([sample[-1] for sample in left_subset])\n            right_gini = gini_impurity([sample[-1] for sample in right_subset])\n            total_samples = len(left_subset) + len(right_subset)\n            weight_left = len(left_subset) / total_samples\n            weight_right = len(right_subset) / total_samples\n            \n            gain = gini_impurity([sample[-1] for sample in data]) - (\n                weight_left * left_gini + weight_right * right_gini\n            )\n            \n            if gain > best_gain:\n                best_gain = gain\n                best_feature_value = value\n        \n        return best_feature_index, best_feature_value, best_gain\n    \n    def build_node(data: List[List[Any]]) -> Dict[str, Any]:\n        \"\"\"Recursively build a tree node.\"\"\"\n        if not data:\n            majority_class = Counter([sample[-1] for sample in data]).most_common(1)[0][0]\n            return {\"label\": majority_class}\n        \n        feature_index, split_value, _ = get_best_split(data, feature_index=0)\n        \n        if feature_index is None:\n            majority_class = Counter([sample[-1] for sample in data]).most_common(1)[0][0]\n            return {\"label\": majority_class}\n        \n        left_subset = [sample for sample in data if sample[feature_index] == split_value]\n        right_subset = [sample for sample in data if sample[feature_index] != split_value]\n        \n        return {\n            \"feature\": feature_index,\n            \"value\": split_value,\n            \"left\": build_node(left_subset),\n            \"right\": build_node(right_subset)\n        }\n    \n    return build_node(data)\n\n# Example usage\ndata = [[1, \"Yes\"], [1, \"Yes\"], [0, \"No\"], [0, \"No\"]]\nprint(build_tree(data))\n```\n\nThis implementation uses recursion to build the decision tree, considering each feature and its unique values to determine the best split. It calculates the Gini impurity to evaluate the quality of each split and selects the one that provides the most significant reduction in impurity."}
{"task_id": 547, "completion_id": 0, "completion": "To implement the `forward_step` function according to the given description, we need to follow these steps:\n\n1. Check if the input dimensions are consistent.\n2. Validate the value of `t`.\n3. Initialize the forward probability vector \\(\\alpha_0\\) using the initial state distribution and the emission probabilities.\n4. Use the recurrence relation to compute \\(\\alpha_t\\) for \\(t \\geq 1\\).\n5. Return the result rounded to four decimal places.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef forward_step(A: list[list[float]],\n                 B: list[list[float]],\n                 S: list[float],\n                 obs: list[int],\n                 t: int) -> list[float]:\n    \"\"\"Compute the forward probability vector \u03b1_t for a Hidden Markov Model.\n\n    Implement the forward recursion and return \u03b1_t rounded to 4 decimals.\n    If the inputs are invalid (shape mismatch, illegal t, etc.) return -1.\n\n    Args:\n        A (list[list[float]]): State-transition matrix (n\u00d7n).\n        B (list[list[float]]): Emission probability matrix (n\u00d7m).\n        S (list[float]):       Initial state distribution (length n).\n        obs (list[int]):       Observation sequence (symbol indices).\n        t (int):               Desired time step (0-based).\n\n    Returns:\n        list[float] | int: \u03b1_t as a list or -1 on error.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    B = np.array(B)\n    S = np.array(S)\n    \n    # Get the number of states and observations\n    n, m = B.shape\n    \n    # Check if the shapes are consistent\n    if A.shape != (n, n):\n        return -1\n    if B.shape != (n, m):\n        return -1\n    if len(S) != n:\n        return -1\n    if not all(0 <= obs[i] < m for i in range(len(obs))):\n        return -1\n    \n    # Check if t is within the valid range\n    if t < 0 or t >= len(obs):\n        return -1\n    \n    # Initialize alpha_0\n    alpha_0 = S * B[:, obs[0]]\n    \n    # Compute alpha_t for t >= 1\n    alpha_t = []\n    for i in range(t + 1):\n        if i == 0:\n            alpha_t.append(alpha_0)\n        else:\n            alpha_t_prev = alpha_t[-1]\n            alpha_t_current = (alpha_t_prev.T @ A) * B[:, obs[i]]\n            alpha_t.append(np.round(alpha_t_current, 4))\n    \n    return alpha_t[-1]\n\n# Example usage:\n# A = [[0.7, 0.3], [0.2, 0.8]]\n# B = [[0.9, 0.1], [0.6, 0.4]]\n# S = [0.5, 0.5]\n# obs = [0, 1]\n# t = 1\n# print(forward_step(A, B, S, obs, t))  # Output: [0.575, 0.425]\n```\n\n### Explanation:\n- We convert the input lists to numpy arrays for efficient computation.\n- We check if the dimensions of the matrices and vectors are consistent.\n- We validate that `t` is within the valid range `[0, len(obs) - 1]`.\n- We initialize \\(\\alpha_0\\) using the initial state distribution and the emission probabilities for the first observation.\n- We use a loop to compute \\(\\alpha_t\\) for each step from 1 to `t`, updating the previous values using the recurrence relation.\n- Finally, we return the last computed \\(\\alpha_t\\) vector, rounded to four decimal places."}
{"task_id": 548, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward-backward algorithm for Hidden Markov Models. This involves computing the forward probabilities (`\u03b1`), backward probabilities (`\u03b2`), and then using these to calculate the `\u03be` matrix. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef compute_xi(A: list[list[float]],\n               B: list[list[float]],\n               pi: list[float],\n               obs: list[int],\n               t: int) -> list[list[float]]:\n    \"\"\"Return the \u03be (xi) matrix for a single time step in a discrete HMM.\n\n    The function must implement the forward\u2013backward equations internally and\n    compute\n        \u03be_t(i,j) = P(q_t=i, q_{t+1}=j | obs, \u03bb)\n    for the requested time index t.\n\n    Args:\n        A: Transition matrix (N\u00d7N).\n        B: Emission matrix (N\u00d7M).\n        pi: Initial distribution (length N).\n        obs: Observation sequence encoded as integers.\n        t: Time index (0 \u2264 t < len(obs)\u22121).\n\n    Returns:\n        The \u03be matrix as a list of lists rounded to 4 decimals, or \u22121 for an\n        invalid t.\n    \"\"\"\n    if t < 0 or t >= len(obs) - 1:\n        return -1\n    \n    N = len(pi)  # Number of states\n    M = len(B[0])  # Number of possible observations\n    \n    # Initialize \u03b1 array\n    alpha = np.zeros((N, 1))\n    alpha[:, 0] = pi * B[:, obs[t]]\n    \n    # Compute forward probabilities up to time t\n    for i in range(1, t + 1):\n        alpha[:, i] = np.dot(alpha[:, i - 1], A) * B[:, obs[i - 1]]\n    \n    # Initialize \u03b2 array\n    beta = np.ones((N, 1))\n    \n    # Compute backward probabilities from time t+1 onwards\n    for i in range(t, -1, -1):\n        beta[:, i] = np.dot(beta[:, i + 1], A.T) * B[:, obs[i + 1]]\n    \n    # Compute \u03be matrix\n    xi = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            xi[i, j] = alpha[i, t] * A[i, j] * B[j, obs[t + 1]] * beta[j, t + 1]\n    \n    # Normalize \u03be matrix along columns\n    xi /= np.sum(xi, axis=1, keepdims=True)\n    \n    # Round to 4 decimal places\n    xi_rounded = np.round(xi, 4)\n    \n    return xi_rounded.tolist()\n\n# Example usage:\nA = [[0.7, 0.3],\n     [0.4, 0.6]]\n\nB = [[0.1, 0.4, 0.5],\n     [0.6, 0.3, 0.1]]\n\npi = [0.6, 0.4]\n\nobs = [0, 1, 2]   # three observations\n\nt = 1\n\nprint(compute_xi(A, B, pi, obs, t))\n```\n\n### Explanation:\n1. **Initialization**: We initialize the forward (`\u03b1`) and backward (`\u03b2`) probability arrays. The forward probabilities represent the probability of being in each state at each time step given the observations up to that point. The backward probabilities represent the probability of observing the remaining part of the sequence starting from each state.\n\n2. **Forward Calculation**: We iterate over each time step from 1 to `t`, updating the forward probabilities using the transition matrix `A` and emission matrix `B`.\n\n3. **Backward Calculation**: Similarly, we iterate over each time step from `t` to 0, updating the backward probabilities.\n\n4. **\u03be Calculation**: For each pair of states `(i, j)` at time `t`, we compute the `\u03be` value using the forward and backward probabilities and the emission matrix.\n\n5. **Normalization**: We normalize the `\u03be` matrix so that the sum of probabilities for each state at time `t` is 1.\n\n6. **Rounding**: Finally, we round the `\u03be` matrix to 4 decimal places and return it as a list of lists. If the input time index `t` is out of bounds, we return `-1`."}
{"task_id": 549, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cal_dis(x, y):\n    \"\"\"Compute the squared Euclidean distance between two vectors.\n\n    Parameters\n    ----------\n    x : list, tuple, or np.ndarray\n        First vector of numbers.\n    y : list, tuple, or np.ndarray\n        Second vector of numbers.\n\n    Returns\n    -------\n    float\n        Squared Euclidean distance rounded to 4 decimal places, or -1 if the\n        vectors are of different lengths.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    x_array = np.array(x)\n    y_array = np.array(y)\n\n    # Check if vectors have the same length\n    if len(x_array) != len(y_array):\n        return -1\n\n    # Compute squared Euclidean distance\n    squared_distance = np.sum((x_array - y_array) ** 2)\n\n    # Round the result to 4 decimal places\n    return round(squared_distance, 4)\n\n# Example usage:\nprint(cal_dis([1, 2, 3], [4, 5, 6]))  # Output: 29.0000\nprint(cal_dis([1, 2], [3, 4, 5]))      # Output: -1\n```"}
{"task_id": 551, "completion_id": 0, "completion": "To implement the `best_gini_split` function, we need to follow these steps:\n\n1. Iterate over all possible feature indices and split values.\n2. For each split, calculate the weighted Gini impurity.\n3. Compare the weighted Gini impurity of the split with the parent node's Gini impurity.\n4. Keep track of the best split that reduces the impurity by at least `epsilon`.\n5. Return the best feature index, split value, and Gini impurity value.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef best_gini_split(X: np.ndarray,\n                    y: np.ndarray,\n                    epsilon: float = 1e-3) -> tuple[int, object, float]:\n    \"\"\"Find the best (feature, value) equality split based on Gini impurity.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the feature\n        values.  The dtype may be numeric or object (strings, etc.).\n    y : np.ndarray\n        1-D array of length n_samples that contains the class labels.\n    epsilon : float, optional (default=1e-3)\n        Minimum required decrease in Gini impurity for a split to be\n        considered valid.\n\n    Returns\n    -------\n    tuple\n        (best_feature_index, best_split_value, best_gini_value). If no valid\n        split is found the function must return (-1, None, parent_gini).\n    \"\"\"\n    n_samples = len(y)\n    parent_gini = gini_impurity(y)\n\n    best_feature = -1\n    best_split_value = None\n    best_gini = parent_gini\n\n    # Iterate over all features\n    for feature_index in range(X.shape[1]):\n        unique_values = np.unique(X[:, feature_index])\n        \n        # Iterate over all possible split values\n        for split_value in unique_values:\n            left_mask = X[:, feature_index] == split_value\n            right_mask = ~left_mask\n            \n            if not left_mask.any() or not right_mask.any():\n                continue\n            \n            left_y = y[left_mask]\n            right_y = y[right_mask]\n            \n            left_gini = gini_impurity(left_y)\n            right_gini = gini_impurity(right_y)\n            \n            current_gini = (len(left_y) / n_samples) * left_gini + \\\n                           (len(right_y) / n_samples) * right_gini\n            \n            if current_gini < best_gini - epsilon:\n                best_feature = feature_index\n                best_split_value = split_value\n                best_gini = current_gini\n\n    return best_feature, best_split_value, best_gini\n\ndef gini_impurity(y):\n    \"\"\"Calculate the Gini impurity for a given label vector.\"\"\"\n    counts = Counter(y)\n    n_samples = len(y)\n    \n    gini = 1.0\n    for count in counts.values():\n        p = count / n_samples\n        gini -= p * p\n    \n    return gini\n```\n\n### Explanation:\n\n- **Gini Impurity Calculation**: The `gini_impurity` function calculates the Gini impurity of a label vector using the formula \\( G(S) = 1 - \\sum_{c} p_c^2 \\), where \\( p_c \\) is the proportion of class \\( c \\) in the set \\( S \\).\n\n- **Split Evaluation**: For each feature and its unique values, we calculate the Gini impurity of the left and right subsets formed by the split. We then compute the weighted Gini impurity of this split.\n\n- **Best Split Selection**: We keep track of the split that results in the greatest reduction in Gini impurity compared to the parent node's impurity. If no valid split reduces the impurity by at least `epsilon`, we return `-1`.\n\nThis implementation efficiently finds the best split while adhering to the constraints specified."}
{"task_id": 552, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward-backward algorithm for Hidden Markov Models (HMMs). This involves computing the forward probabilities \\(\\alpha_t(i)\\) and backward probabilities \\(\\beta_t(i)\\), and then using these to calculate the posterior probabilities \\(\\gamma_t(i)\\).\n\nHere's how you can implement the `gamma_probabilities` function:\n\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n\n    pi = hmm['pi']  # Initial distribution\n    A = hmm['A']   # Transition matrix\n    B = hmm['B']   # Emission matrix\n\n    T = len(observations)\n\n    # Step 1: Compute alpha_t(i) - Forward probabilities\n    alpha = np.zeros((T, len(pi)))\n    alpha[0] = pi * B[:, observations[0]]\n\n    for i in range(1, T):\n        alpha[i] = B[:, observations[i]] @ (alpha[i-1].dot(A))\n\n    # Step 2: Compute beta_t(i) - Backward probabilities\n    beta = np.ones((T, len(pi)))\n    beta[-1] = np.ones(len(pi))  # Beta_t(T) is all ones\n\n    for i in range(T-2, -1, -1):\n        beta[i] = (A.dot(B[:, observations[i+1]]) * beta[i+1]).sum(axis=1)\n\n    # Step 3: Compute gamma_t(i) - Posterior probabilities\n    gamma = alpha[t] * beta[t]\n    gamma /= gamma.sum()\n\n    # Round the result to four decimal places\n    return [round(x, 4) for x in gamma]\n\n# Example usage:\nhmm_params = {\n    'pi': np.array([0.6, 0.4]),\n    'A': np.array([[0.7, 0.3],\n                   [0.2, 0.8]]),\n    'B': np.array([[0.9, 0.1],\n                   [0.2, 0.8]])\n}\nobservation_sequence = [0, 1]\nt = 1\n\nprint(gamma_probabilities(hmm_params, observation_sequence, t))\n```\n\n### Explanation:\n1. **Initialization**:\n   - We initialize the initial distribution \\(\\pi\\) and the matrices \\(A\\) and \\(B\\).\n   - We also initialize the first element of the forward probabilities array \\(\\alpha_0\\) based on the initial distribution and the emission probabilities for the first observation.\n\n2. **Forward Algorithm**:\n   - We iterate over each time step \\(i\\) from 1 to \\(T-1\\) to compute the forward probabilities \\(\\alpha_i\\).\n   - For each state \\(i\\), we update the forward probability by multiplying the previous forward probabilities with the transition matrix and the emission probabilities for the current observation.\n\n3. **Backward Algorithm**:\n   - We initialize the last element of the backward probabilities array \\(\\beta_T\\) to be all ones.\n   - We iterate backwards from \\(T-2\\) to 0 to compute the backward probabilities \\(\\beta_i\\).\n   - For each state \\(i\\), we update the backward probability by summing over the next states weighted by their transition probabilities and the emission probabilities for the next observation.\n\n4. **Posterior Probabilities**:\n   - Finally, we compute the posterior probabilities \\(\\gamma_t(i)\\) by multiplying the forward and backward probabilities at time \\(t\\) and dividing by their sum.\n\n5. **Rounding**:\n   - We round each component of the \\(\\gamma_t\\) vector to four decimal places before returning it.\n\nThis function will correctly compute the posterior probabilities for any valid time step \\(t\\) within the bounds of the observation sequence. If \\(t\\) is out of bounds, it returns an empty list."}
{"task_id": 553, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    alpha: float = 0.01,\n    max_iter: int = 1000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model with batch gradient descent and\n    predict class labels for a test set.\n\n    Args:\n        X_train: Training patterns of shape (n_samples, n_features).\n        y_train: Binary labels (0/1) of length n_samples.\n        X_test:  Patterns to be classified, shape (m_samples, n_features).\n        alpha:   Learning rate used in gradient ascent on the log-likelihood.\n        max_iter: Maximum number of gradient updates.\n\n    Returns:\n        A Python list containing the predicted class label (0 or 1) for every\n        row of X_test.\n    \"\"\"\n    # Step 1: Add a bias column of ones to all design matrices\n    n_samples, n_features = X_train.shape\n    X_train_with_bias = np.c_[np.ones(n_samples), X_train]\n    X_test_with_bias = np.c_[np.ones(len(X_test)), X_test]\n\n    # Step 2: Initialize the weight vector with zeros\n    w = np.zeros(n_features + 1)\n\n    # Step 3: Repeat max_iter times\n    for _ in range(max_iter):\n        z = np.dot(X_train_with_bias, w)\n        s = 1 / (1 + np.exp(-z))\n        dw = np.dot(X_train_with_bias.T, (s - y_train)) / n_samples\n        w -= alpha * dw\n\n    # Step 4: Compute the probability for each test example and output 1 if the probability \u2265 0.5, otherwise 0\n    probabilities = 1 / (1 + np.exp(-np.dot(X_test_with_bias, w)))\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n\n    return predictions\n```\n\nThis implementation follows the specified algorithm to train a logistic regression model using batch gradient descent and make predictions on a test set. The function returns a list of predicted class labels."}
{"task_id": 555, "completion_id": 0, "completion": "To implement the DBSCAN algorithm from scratch, we need to follow the steps outlined in the description. Below is the complete implementation:\n\n```python\nimport numpy as np\nfrom collections import deque\n\ndef euclidean_distance(p1, p2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.linalg.norm(p1 - p2)\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    if data.shape[0] == 0:\n        return []\n\n    # Step 1: Compute every pairwise Euclidean distance\n    distances = np.zeros((data.shape[0], data.shape[0]))\n    for i in range(data.shape[0]):\n        for j in range(i + 1, data.shape[0]):\n            distances[i, j] = euclidean_distance(data[i], data[j])\n            distances[j, i] = distances[i, j]\n\n    # Step 2: Build the neighborhood list for each sample using the supplied \u03b5\n    neighborhoods = []\n    for i in range(data.shape[0]):\n        neighborhood = [j for j in range(data.shape[0]) if distances[i, j] <= eps]\n        neighborhoods.append(neighborhood)\n\n    # Step 3: Identify core points (|neighborhood| \u2265 min_samples)\n    core_points = [i for i in range(data.shape[0]) if len(neighborhoods[i]) >= min_samples]\n\n    # Step 4: Initialize variables for BFS and labeling\n    visited = set()\n    cluster_id = 1\n    cluster_labels = [-1] * data.shape[0]\n\n    # Step 5: Expand clusters starting from core points\n    for core_point in core_points:\n        if core_point not in visited:\n            queue = deque([core_point])\n            visited.add(core_point)\n            cluster_labels[core_point] = cluster_id\n\n            while queue:\n                current_point = queue.popleft()\n\n                for neighbor in neighborhoods[current_point]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        cluster_labels[neighbor] = cluster_id\n\n                        # If the neighbor is a core point, add it to the queue\n                        if len(neighborhoods[neighbor]) >= min_samples:\n                            queue.append(neighbor)\n\n            cluster_id += 1\n\n    return cluster_labels\n\n# Example usage:\n# data = np.array([[1, 1], [2, 2], [3, 3], [6, 6], [7, 7], [8, 8]])\n# eps = 2\n# min_samples = 2\n# print(dbscan(data, eps, min_samples))\n```\n\n### Explanation:\n- **Distance Calculation**: We use the `euclidean_distance` function to compute the pairwise Euclidean distances between all points.\n- **Neighborhood Construction**: For each point, we create a list of indices of its neighbors within the specified radius `eps`.\n- **Core Point Identification**: A point is identified as a core point if it has at least `min_samples` neighbors.\n- **Breadth-First Search (BFS)**: Starting from each core point, we perform a BFS to expand the cluster. Each visited point is labeled with the current cluster ID.\n- **Noise Points**: Any point that remains unvisited after processing all core points is labeled as noise (`-1`).\n\nThis implementation efficiently groups points based on their spatial density and handles both core and non-core points according to the DBSCAN algorithm's rules."}
{"task_id": 559, "completion_id": 0, "completion": "To solve the problem of collecting leaf values from a binary tree, we can use a depth-first search (DFS) approach. We will traverse the tree recursively, checking each node to see if it is a leaf node (i.e., it has no children). If it is a leaf node, we add its value to our result list.\n\nHere's how you can implement the `leaf_values` function:\n\n```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary-tree node used by the task.\n\n    Attributes:\n        val:     The value stored in the node.  Only leaf nodes have a value.\n        right:   Reference to the right child or ``None``.\n        left:    Reference to the left  child or ``None``.\n    \"\"\"\n\n    def __init__(self, val: Optional[int] = None, right: 'Optional[Node]' = None,\n                 left: 'Optional[Node]' = None):\n        self.val = val\n        self.right = right\n        self.left = left\n\n\ndef leaf_values(root: Optional[Node]) -> List[int]:\n    \"\"\"Collect all leaf values from left to right.\n\n    Args:\n        root: The root of the binary tree or ``None``.\n\n    Returns:\n        A list containing the values found in every leaf node, ordered from\n        left-most leaf to right-most leaf.  If the tree is empty the list is\n        empty as well.\n    \"\"\"\n    def dfs(node: Optional[Node], result: List[int]) -> None:\n        if not node:\n            return\n        \n        # Check if the current node is a leaf node\n        if not node.left and not node.right:\n            result.append(node.val)\n        \n        # Recursively visit the left and right subtrees\n        dfs(node.left, result)\n        dfs(node.right, result)\n    \n    result = []\n    dfs(root, result)\n    return result\n\n# Example usage:\nroot = Node(\n    left=Node(val=1),\n    right=Node(\n        left=Node(val=2),\n        right=Node(val=3)\n    )\n)\n\nprint(leaf_values(root))  # Output: [1, 2, 3]\n```\n\n### Explanation:\n- We define a helper function `dfs` that takes a node and a result list as arguments.\n- If the current node is `None`, we simply return without doing anything.\n- If the current node is a leaf node (i.e., it has no left or right children), we append its value to the result list.\n- We then recursively call `dfs` on the left and right children of the current node.\n- Finally, we initialize an empty list `result` and call `dfs` starting from the root of the tree.\n- The result list contains all the leaf node values in left-to-right order, which we return at the end."}
{"task_id": 560, "completion_id": 0, "completion": "To solve the problem of finding the \\( k \\)-nearest neighbors using a k-d tree without employing classes or other OOP features, we need to follow these steps:\n\n1. **Build the k-d Tree**: We'll construct a binary tree where each node represents a split along a particular dimension. The tree will be balanced, ensuring that the depth difference between the left and right subtrees is at most 1.\n\n2. **Search the k-d Tree**: Starting from the root, we'll traverse the tree to find the \\( k \\) nearest neighbors to the query point.\n\n3. **Heapq for Efficient Selection**: We'll use a min-heap to keep track of the \\( k \\) closest points efficiently.\n\nHere's the implementation of the `k_nearest_neighbors` function:\n\n```python\nimport math\nimport heapq\n\ndef k_nearest_neighbors(points: list[list[float]], query: list[float], k: int = 3) -> list[int]:\n    \"\"\"Find the *k* nearest neighbours of *query* among *points* using a k-d tree.\n\n    Parameters\n    ----------\n    points : list[list[float]]\n        A list containing *n* points.  Each point itself is a list of floats\n        and all points have the same dimensionality.\n    query : list[float]\n        The query point whose neighbours are to be found.\n    k : int, default 3\n        The number of nearest neighbours that should be returned.  If *k* is\n        greater than the number of points, all points must be returned.\n\n    Returns\n    -------\n    list[int]\n        A list with the indices (0-based) of the *k* closest points, sorted by\n        increasing distance to *query* (ties are broken by the smaller index).\n    \"\"\"\n    \n    def _build_kdtree(points, depth=0):\n        n = len(points)\n        \n        # Choose the axis based on the current depth\n        axis = depth % len(points[0])\n        \n        # Sort points by the chosen axis\n        points.sort(key=lambda x: x[axis])\n        \n        # Select the median as the root of this subtree\n        mid = n // 2\n        \n        # Create the current node\n        node = {\n            'index': mid,\n            'axis': axis,\n            'left': None,\n            'right': None\n        }\n        \n        # Recursively build the left and right subtrees\n        if mid > 0:\n            node['left'] = _build_kdtree(points[:mid], depth + 1)\n        if mid < n - 1:\n            node['right'] = _build_kdtree(points[mid+1:], depth + 1)\n        \n        return node\n    \n    def _search(node, query, k, heap):\n        if not node:\n            return\n        \n        # Calculate the squared distance from the query to the current node's point\n        dist = sum((a - b) ** 2 for a, b in zip(query, points[node['index']]))\n        \n        # Push the current node into the heap\n        heapq.heappush(heap, (-dist, node['index']))\n        \n        # Determine which child to explore next\n        axis = node['axis']\n        next_node = node['left'] if query[axis] <= points[node['index']][axis] else node['right']\n        \n        # Explore the next node\n        _search(next_node, query, k, heap)\n        \n        # Check if there is space in the heap to insert another point\n        if len(heap) > k:\n            heapq.heappop(heap)\n        \n        # Check the opposite direction\n        if next_node is not None and abs(query[axis] - points[node['index']][axis]) < -heap[0][0]:\n            _search(next_node, query, k, heap)\n    \n    # Build the k-d tree\n    root = _build_kdtree(points)\n    \n    # Initialize the heap\n    heap = []\n    \n    # Perform the search\n    _search(root, query, k, heap)\n    \n    # Extract the indices from the heap\n    return [-idx for _, idx in sorted(heap)]\n\n# Example usage\npoints = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]\nquery = [9, 2]\nk = 2\nprint(k_nearest_neighbors(points, query, k))  # Output: [4, 5]\n```\n\n### Explanation:\n\n- **Building the k-d Tree**: \n  - We recursively select the median of the points along the current axis to create the root of each subtree.\n  - The `axis` is determined by the current depth modulo the dimensionality of the points.\n\n- **Searching the k-d Tree**:\n  - We use a recursive function `_search` to traverse the tree.\n  - For each node, we calculate the squared distance from the query point.\n  - We push the node into a min-heap to store the closest points found so far.\n  - We explore both children of the current node, prioritizing the child that could potentially contain closer points due to the splitting plane.\n  - If the heap size exceeds \\( k \\), we pop the farthest point from the heap.\n  - We also check the opposite direction to ensure we don't miss any potential candidates.\n\n- **Returning Results**:\n  - After searching, we extract the indices from the heap, sort them by distance and then by index, and return them."}
{"task_id": 561, "completion_id": 0, "completion": "Here's a complete implementation of the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model (GMM):\n\n```python\nimport numpy as np\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Implement the Expectation\u2013Maximization (EM) algorithm for a Gaussian Mixture Model (GMM).\n\n    Parameters:\n    - data: A 2-D NumPy array containing N samples with D features.\n    - k: An integer representing the number of Gaussian components.\n    - max_iter: The maximum number of iterations to perform.\n    - epsilon: The tolerance for stopping the EM algorithm when the change in mixture weights is small.\n\n    Returns:\n    - A list of length N containing the final cluster label of each sample.\n    \"\"\"\n\n    # Step 1: Initialize the parameters\n    n_samples, n_features = data.shape\n    if k == 1:\n        return [0] * n_samples  # If k=1, all samples belong to the single component\n\n    # Initial random initialization\n    weights = np.ones(k) / k\n    means = data[np.random.choice(n_samples, size=k)]\n    covariances = [np.cov(data, rowvar=False) for _ in range(k)]\n\n    # Initialize responsibilities\n    responsibilities = np.zeros((n_samples, k))\n\n    # Iterate until convergence\n    for i in range(max_iter):\n        # E-step: Compute responsibilities\n        for j in range(k):\n            responsibilities[:, j] = weights[j] * np.exp(-0.5 * np.sum((data - means[j]) ** 2 / covariances[j], axis=1))\n        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n\n        # Check for convergence\n        if np.max(np.abs(weights - np.mean(weights))) < epsilon:\n            break\n\n        # M-step: Update parameters\n        weights = responsibilities.sum(axis=0) / n_samples\n        new_means = np.dot(responsibilities.T, data) / responsibilities.sum(axis=0, keepdims=True)\n        new_covariances = []\n        for j in range(k):\n            diff = data - means[j]\n            new_covariances.append(np.dot(diff.T @ responsibilities[:, j].reshape(-1, 1), diff) / responsibilities[:, j].sum())\n\n        # Update parameters\n        weights = np.clip(weights, 1e-6, 1 - 1e-6)  # Ensure positive weights\n        means = new_means\n        covariances = new_covariances\n\n    # Assign clusters based on the highest responsibility\n    cluster_labels = np.argmax(responsibilities, axis=1)\n\n    # Sort components by the first coordinate of their mean in ascending order\n    sorted_indices = np.argsort(means[:, 0])\n    sorted_cluster_labels = [cluster_labels[i] for i in sorted_indices]\n\n    return sorted_cluster_labels\n\n# Example usage:\n# data = np.random.rand(100, 2)  # Example 2D data\n# k = 3\n# labels = gmm_em(data, k)\n# print(labels)\n```\n\n### Explanation:\n\n1. **Initialization**: \n   - We initialize the mixture weights randomly.\n   - We randomly select `k` samples from the dataset to serve as initial means.\n   - We compute the initial covariance matrices using the sample covariance matrix.\n\n2. **Expectation (E) Step**:\n   - For each sample, we calculate the responsibility of each Gaussian component using Bayes' theorem.\n   - The responsibility is proportional to the product of the weight of the component and the likelihood of the sample given the component.\n\n3. **Maximization (M) Step**:\n   - We update the mixture weights by averaging the responsibilities across samples.\n   - We update the means by taking the weighted average of the samples.\n   - We update the covariance matrices by computing the weighted sum of squared differences from the updated means.\n\n4. **Convergence Check**:\n   - We check if the change in mixture weights is less than the specified tolerance (`epsilon`). If it is, we stop the iteration.\n\n5. **Cluster Assignment**:\n   - We assign each sample to the component with the highest responsibility.\n\n6. **Sorting Components**:\n   - We sort the components by the first coordinate of their mean in ascending order and re-label the cluster indices accordingly.\n\nThis implementation ensures that the output is deterministic for identical inputs by maintaining a consistent order of components."}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    if n_cluster == 1:\n        return [0] * len(data)\n    \n    # Step 1: Build a fully-connected similarity graph using the Gaussian kernel\n    N = data.shape[0]\n    dist_matrix = np.sum((data[:, np.newaxis] - data[np.newaxis, :])**2, axis=-1)\n    similarity_matrix = np.exp(-gamma * dist_matrix)\n    \n    # Step 2: Construct the un-normalized Laplacian L = D - W\n    degree_vector = np.sum(similarity_matrix, axis=1)\n    if method == 'normalized':\n        D_sqrt_inv = np.diag(1 / np.sqrt(degree_vector))\n        L = D_sqrt_inv @ similarity_matrix @ D_sqrt_inv\n    else:\n        L = similarity_matrix - np.diag(degree_vector)\n    \n    # Step 3: Compute the eigenvectors corresponding to the n_cluster smallest eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    sorted_indices = np.argsort(eigenvalues)[:n_cluster]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Step 4: Run k-means in the eigenvector space\n    def kmeans_eigenvectors(X, k):\n        from scipy.cluster.vq import vq\n        \n        # Initialize centroids randomly from the first k samples\n        centroids = X[:k]\n        \n        while True:\n            labels, _ = vq(X, centroids)\n            new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n            \n            if np.allclose(new_centroids, centroids):\n                break\n            \n            centroids = new_centroids\n    \n    labels = kmeans_eigenvectors(eigenvectors_sorted, n_cluster)\n    \n    # Step 5: Relabel clusters based on the order of their original indices\n    original_indices = np.arange(len(data))\n    sorted_labels = sorted(zip(original_indices, labels), key=lambda x: x[0])\n    final_labels = [label for _, label in sorted_labels]\n    \n    return final_labels\n```"}
{"task_id": 563, "completion_id": 0, "completion": "Here's the implementation of the Dual-form Perceptron learning algorithm:\n\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N, d = X_data.shape\n    alpha = np.zeros(N)\n    b = 0\n    \n    # Step 2: Build the Gram matrix G\n    G = np.dot(X_data, X_data.T)\n    \n    # Step 3: Main training loop\n    converged = False\n    num_iterations = 0\n    \n    while not converged and num_iterations < max_iter:\n        updated = False\n        \n        for i in range(N):\n            # Compute g\n            g = np.sum(alpha * y_data * G[:, i])\n            \n            # Check if update is needed\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        \n        # Check convergence\n        if not updated:\n            converged = True\n        \n        num_iterations += 1\n    \n    # Step 4: Recover the weight vector w\n    w = np.dot(alpha, y_data * X_data)\n    \n    # Step 5: Return the result\n    return [round(w_i, 4) for w_i in w.tolist()], round(b, 4)\n\n# Example usage:\n# X_data = np.array([[1, 2], [-1, -2], [2, -1], [-2, 1]])\n# y_data = np.array([1, -1, 1, -1])\n# w, b = perceptron_dual_train(X_data, y_data)\n# print(\"Weight vector:\", w)\n# print(\"Bias term:\", b)\n```\n\n### Explanation:\n- **Initialization**: We initialize the coefficients `alpha` to zero and the bias `b` to zero.\n- **Gram Matrix**: We compute the Gram matrix `G` which contains the dot products between all pairs of input vectors.\n- **Training Loop**: We iterate over each sample, compute the value of `g`, and check if an update is necessary based on the sign condition. If an update is needed, we adjust `alpha` and `b`.\n- **Convergence Check**: We continue iterating until no more updates are made or the maximum number of iterations is reached.\n- **Recovery of Weight Vector**: Finally, we recover the weight vector `w` using the formula \\( w = \\sum_{i=1}^n \\alpha_i y_i x_i \\).\n- **Output**: The weights and bias are returned as lists with each element rounded to four decimal places."}
{"task_id": 564, "completion_id": 0, "completion": "To implement the `predict_sample` function according to the specified rules, we need to traverse the decision tree based on whether it's a classification or regression tree. Here's how you can achieve this:\n\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str) -> int | float:\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    # Start at the root node\n    current_node = root\n    \n    while True:\n        if task_type == 'classification':\n            # Check if the current node is a leaf\n            if 'y' in current_node.__dict__:\n                return current_node.y\n            \n            # Inspect the feature with index node.label\n            feature_index = current_node.label\n            feature_value = features[feature_index]\n            \n            # Choose the first child with matching x value\n            for child in current_node.child:\n                if child.x == feature_value:\n                    current_node = child\n                    break\n            else:\n                # Default to the second child\n                current_node = current_node.child[1]\n        \n        elif task_type == 'regression':\n            # Check if the current node is a leaf\n            if 'y' in current_node.__dict__:\n                return current_node.y\n            \n            # Inspect the feature with index node.label\n            feature_index = current_node.label\n            feature_value = features[feature_index]\n            \n            # Compare the feature value with node.s\n            if feature_value <= current_node.s:\n                current_node = current_node.child[0]\n            else:\n                current_node = current_node.child[1]\n    \n    # This line should never be reached due to the loop condition\n    raise ValueError(\"Unexpected state reached during prediction.\")\n\n# Example usage:\n# Define the Node class (assuming it's defined elsewhere)\n# root = Node(label=0, x=5, child=[Node(y=1), Node(y=2)])\n# features = [5]\n# print(predict_sample(root, features, 'classification'))  # Output: 1\n```\n\n### Explanation:\n1. **Initialization**: We start at the root node.\n2. **Traversal Loop**:\n   - For classification trees, we check if the current node is a leaf. If so, we return the stored value `y`. Otherwise, we inspect the feature at the index specified by `node.label` and select the first child with a matching `x` value. If no match is found, we default to the second child.\n   - For regression trees, we similarly check if the current node is a leaf. If so, we return the stored value `y`. Otherwise, we inspect the feature at the index specified by `node.label`. Based on whether the feature value is less than or equal to `node.s`, we select the appropriate child.\n3. **Termination**: The loop continues until a leaf node is reached. If the loop terminates without finding a leaf, it raises a `ValueError`, which should not happen under normal conditions given the problem constraints."}
{"task_id": 565, "completion_id": 0, "completion": "To implement the `choose_best_feature` function using the ID3 decision tree algorithm, we need to calculate the information gain for each feature and select the one with the highest gain. Here's how you can do it:\n\n1. Calculate the total number of samples (`n_samples`).\n2. For each feature, count the frequency of each unique value in the feature column.\n3. Calculate the entropy of the entire dataset (`H(y)`).\n4. For each unique value of the feature, split the dataset into subsets based on this value and calculate the entropy of these subsets (`H(y|A)`).\n5. Compute the information gain for the current feature as `IG(A) = H(y) - H(y|A)`.\n6. Track the feature with the maximum information gain and its corresponding gain.\n\nHere's the implementation:\n\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    def entropy(probabilities):\n        if not probabilities.any():\n            return 0.0\n        return -sum(p * math.log2(p) for p in probabilities)\n    \n    def information_gain(parent_entropy, child_entropies, weights):\n        weighted_sum = sum(w * e for w, e in zip(weights, child_entropies))\n        return parent_entropy - weighted_sum\n    \n    max_info_gain = -math.inf\n    best_feature_index = None\n    \n    for feature_idx in range(n_features):\n        # Count the frequency of each unique value in the feature column\n        unique_values, counts = np.unique(X[:, feature_idx], return_counts=True)\n        \n        # Calculate the probability of each unique value\n        probabilities = counts / n_samples\n        \n        # Calculate the entropy of the entire dataset\n        parent_entropy = entropy(probabilities)\n        \n        # Split the dataset into subsets based on the current feature\n        child_entropies = []\n        weights = []\n        \n        for value in unique_values:\n            subset_mask = X[:, feature_idx] == value\n            subset_y = y[subset_mask]\n            \n            # Calculate the entropy of the subset\n            subset_entropy = entropy(subset_y / subset_y.size)\n            \n            # Calculate the weight of the subset\n            weight = subset_y.size / n_samples\n            \n            child_entropies.append(subset_entropy)\n            weights.append(weight)\n        \n        # Calculate the information gain\n        info_gain = information_gain(parent_entropy, child_entropies, weights)\n        \n        # Update the best feature if this one has a higher gain\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_feature_index = feature_idx\n    \n    return best_feature_index, round(max_info_gain, 6)\n\n# Example usage:\nX = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\ny = np.array([0, 0, 1, 1])\n\nbest_feature, max_gain = choose_best_feature(X, y)\nprint(f\"Best feature index: {best_feature}, Max information gain: {max_gain}\")\n```\n\nThis code defines the `choose_best_feature` function, which calculates the information gain for each feature and returns the feature with the highest gain along with its rounded information gain. The example usage demonstrates how to use this function with a sample dataset."}
{"task_id": 566, "completion_id": 0, "completion": "To implement the Tiny Isolation Forest algorithm as described, we need to follow these steps:\n\n1. Set the global seed for reproducibility.\n2. Define helper functions to calculate the path length and correction factor `c(n)`.\n3. Implement the main function `detect_outliers` to grow multiple isolation trees and compute the average path length for each observation.\n4. Determine the top `\u03b5`% of outlier scores and return their indices.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    # Set the global seed for reproducibility\n    np.random.seed(42)\n    \n    def path_length(node):\n        if len(node['samples']) == 1:\n            return 0\n        else:\n            feature = np.random.randint(len(data[0]))\n            threshold = np.random.uniform(min(data[node['samples'], feature]), max(data[node['samples'], feature]))\n            left_mask = data[:, feature] <= threshold\n            right_mask = data[:, feature] > threshold\n            left_samples = node['samples'][left_mask]\n            right_samples = node['samples'][right_mask]\n            return 1 + min(path_length({'samples': left_samples}), path_length({'samples': right_samples}))\n    \n    def c(n):\n        if n == 1:\n            return 0\n        else:\n            return 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n    \n    def generate_tree():\n        root = {'samples': np.arange(len(data))}\n        height_limit = math.ceil(math.log2(sample_size))\n        while True:\n            if height_limit == 0 or len(root['samples']) <= 1:\n                break\n            feature = np.random.randint(len(data[0]))\n            threshold = np.random.uniform(min(data[root['samples'], feature]), max(data[root['samples'], feature]))\n            left_mask = data[:, feature] <= threshold\n            right_mask = data[:, feature] > threshold\n            left_samples = root['samples'][left_mask]\n            right_samples = root['samples'][right_mask]\n            root['children'] = [{'samples': left_samples}, {'samples': right_samples}]\n            height_limit -= 1\n        return root\n    \n    scores = []\n    for _ in range(n_trees):\n        tree = generate_tree()\n        scores.append(path_length(tree))\n    \n    avg_scores = np.mean(scores)\n    phi = 2 * math.log(len(data)) - 2 * (len(data) - 1) / len(data)\n    anomaly_scores = 2 ** (-avg_scores / phi)\n    \n    num_outliers = int(epsilon * len(anomaly_scores))\n    top_outliers_indices = np.argsort(anomaly_scores)[-num_outliers:]\n    return top_outliers_indices.tolist()\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\nn_trees = 100\nsample_size = 5\nepsilon = 0.1\noutliers = detect_outliers(data, n_trees, sample_size, epsilon)\nprint(outliers)\n```\n\n### Explanation:\n- **Path Length Calculation**: The `path_length` function recursively calculates the path length from a given node to a leaf.\n- **Tree Generation**: The `generate_tree` function builds a purely random binary tree using the specified rules.\n- **Score Computation**: For each tree, the path lengths are averaged, and the anomaly scores are computed using the formula \\(s(x) = 2^{-(\\bar{h}(x)/\\phi)}\\).\n- **Outlier Detection**: The top `\u03b5`% of anomaly scores are identified, and their indices are returned.\n\nThis implementation ensures that the algorithm is both simple and efficient, adhering to the constraints provided."}
{"task_id": 567, "completion_id": 0, "completion": "To implement the majority voting algorithm, we need to follow these steps:\n\n1. Count the occurrences of each label in each column.\n2. Determine the mode for each column, which is the most frequent label.\n3. In case of a tie, select the smallest label among the modes.\n\nHere's the implementation of the `majority_vote` function:\n\n```python\nimport numpy as np\n\ndef majority_vote(predictions: list[list[int | float]]) -> list[int]:\n    \"\"\"Ensemble majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float]]\n        A 2-D list where each row contains the predictions made by one tree\n        and each column groups the votes for a single sample.\n\n    Returns\n    -------\n    list[int]\n        The final class label for every sample after majority voting.  In a\n        tie, the smallest label must be chosen.\n    \"\"\"\n    # Convert the input list to a NumPy array for easier manipulation\n    predictions_array = np.array(predictions)\n    \n    # Find the mode along each column using argmax\n    # We use np.argmax to find the index of the maximum value in each column\n    # However, since there might be ties, we use np.unique to handle them\n    modes = []\n    for col in predictions_array.T:\n        unique_values, counts = np.unique(col, return_counts=True)\n        max_count = np.max(counts)\n        \n        # Filter out the modes that have the maximum count\n        candidates = unique_values[counts == max_count]\n        \n        # If there's a tie, choose the smallest label\n        if len(candidates) > 1:\n            modes.append(np.min(candidates))\n        else:\n            modes.append(int(candidates[0]))\n    \n    return modes\n\n# Example usage:\npredictions = [\n    [1, 2, 3],\n    [2, 2, 3],\n    [3, 3, 1],\n    [1, 2, 1]\n]\n\nprint(majority_vote(predictions))  # Output: [2, 2, 1, 1]\n```\n\n### Explanation:\n- We first convert the input list into a NumPy array for efficient computation.\n- We iterate over each column of the array using `T` to transpose it.\n- For each column, we use `np.unique` to get the unique labels and their counts.\n- We find the maximum count of any label.\n- If there are multiple labels with the same maximum count, we select the smallest one using `np.min`.\n- Finally, we collect all the modes into a list and return it.\n\nThis function handles ties by selecting the smallest label in case of a tie, as required."}
{"task_id": 568, "completion_id": 0, "completion": "To implement the Maximum Entropy Classifier using the Generalized Iterative Scaling (GIS) algorithm, we need to follow these steps:\n\n1. **Build Empirical Distributions**: Calculate the empirical frequency of each feature-value-label combination in the training data.\n2. **Initialize Weights**: Initialize the weights randomly.\n3. **Iteratively Update Weights**: Use the GIS algorithm to update the weights iteratively until convergence.\n4. **Predict Labels**: For each test sample, compute the conditional probabilities and predict the label based on the highest probability.\n\nHere's the complete implementation of the `maxent_predict` function:\n\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Fill in here.\"\"\"\n    \n    # Step 1: Build Empirical Distributions\n    num_features = X_train.shape[1]\n    num_labels = len(np.unique(y_train))\n    \n    # Dictionary to store empirical frequencies\n    empirical_frequencies = defaultdict(lambda: defaultdict(int))\n    \n    # Count occurrences of each feature-value-label combination\n    for i in range(len(X_train)):\n        x = X_train[i]\n        y = y_train[i]\n        for j in range(num_features):\n            feature_value = x[j]\n            empirical_frequencies[j][feature_value][y] += 1\n    \n    # Step 2: Initialize Weights\n    num_combinations = sum(empirical_frequencies.values())\n    weights = np.random.rand(num_features, num_labels)\n    \n    # Step 3: Iteratively Update Weights\n    for _ in range(n_iter):\n        converged = True\n        \n        for i in range(len(X_train)):\n            x = X_train[i]\n            y = y_train[i]\n            \n            # Compute the current prediction\n            log_probabilities = []\n            for j in range(num_features):\n                feature_value = x[j]\n                for k in range(num_labels):\n                    prob = empirical_frequencies[j][feature_value][k] / num_combinations\n                    log_probabilities.append(math.log(prob) + weights[j][k])\n            \n            # Find the maximum likelihood estimate\n            max_log_prob = max(log_probabilities)\n            predicted_label = np.argmax(log_probabilities)\n            \n            # Check if the prediction is correct\n            if predicted_label != y:\n                # Update the weights using GIS\n                for j in range(num_features):\n                    feature_value = x[j]\n                    for k in range(num_labels):\n                        empirical_frequencies[j][feature_value][k] -= 1\n                        new_prob = empirical_frequencies[j][feature_value][k] / num_combinations\n                        log_probabilities[j] = math.log(new_prob) + weights[j][k]\n                        new_max_log_prob = max(log_probabilities)\n                        if abs(new_max_log_prob - max_log_prob) > epsilon:\n                            converged = False\n                            break\n                    if not converged:\n                        break\n                if not converged:\n                    break\n        \n        if converged:\n            break\n    \n    # Step 4: Predict Labels for Test Data\n    predictions = []\n    for x in X_test:\n        log_probabilities = []\n        for j in range(num_features):\n            feature_value = x[j]\n            for k in range(num_labels):\n                prob = empirical_frequencies[j][feature_value][k] / num_combinations\n                log_probabilities.append(math.log(prob) + weights[j][k])\n        \n        predicted_label = np.argmax(log_probabilities)\n        predictions.append(predicted_label)\n    \n    return predictions\n\n# Example usage:\n# X_train, y_train, X_test = load_data()  # Replace with actual data loading\n# predictions = maxent_predict(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n\n1. **Empirical Frequencies**: We count how many times each feature-value-label combination appears in the training data.\n2. **Weight Initialization**: Randomly initialize the weights.\n3. **Iteration**: For each training example, calculate the log probabilities of all possible labels given the features. Update the weights based on the difference between the old and new maximum log probabilities if they exceed the threshold `epsilon`.\n4. **Prediction**: For each test example, compute the log probabilities similarly and choose the label with the highest probability.\n\nThis implementation assumes that the input data is in a format where each sample is represented as a NumPy array of feature values, and the labels are integers. Adjust the data loading part as needed for your specific dataset."}
{"task_id": 569, "completion_id": 0, "completion": "To complete the task of calculating the document-frequency keyword statistics, we need to follow these steps:\n\n1. Count the occurrences of each word across all documents.\n2. Calculate the document-frequency ratio for each word.\n3. Sort the words based on their document-frequency ratio in descending order.\n4. Return the sorted list of words along with the set of the most common words up to the specified `cut_off`.\n\nHere's the implementation of the `statistic_key_word` function:\n\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    # Step 1: Count the occurrences of each word across all documents\n    word_counts = Counter()\n    \n    for _, words in data:\n        word_counts.update(words)\n    \n    # Step 2: Calculate the document-frequency ratio for each word\n    total_documents = len(data)\n    word_ratios = [(word, count / total_documents) for word, count in word_counts.items()]\n    \n    # Step 3: Sort the words based on their document-frequency ratio in descending order\n    word_ratios.sort(key=lambda x: x[1], reverse=True)\n    \n    # Step 4: Return the sorted list of words along with the set of the most common words\n    if cut_off is None:\n        return word_ratios, set(word_counts.keys())\n    else:\n        return word_ratios[:cut_off], {word for word, _ in word_ratios[:cut_off]}\n\n# Example usage:\ndata = [\n    (\"doc1\", [\"apple\", \"banana\", \"cherry\"]),\n    (\"doc2\", [\"banana\", \"date\", \"fig\"]),\n    (\"doc3\", [\"grape\", \"fig\", \"kiwi\"])\n]\n\nresult = statistic_key_word(data, cut_off=2)\nprint(result)\n```\n\n### Explanation:\n\n- **Step 1**: We use `Counter` to count the occurrences of each word across all documents.\n- **Step 2**: We calculate the document-frequency ratio for each word by dividing its count by the total number of documents.\n- **Step 3**: We sort the list of word-ratio pairs in descending order based on the ratio.\n- **Step 4**: Depending on whether `cut_off` is `None`, we return either the entire sorted list of words or only the top `cut_off` words along with their frequencies.\n\nThis implementation ensures that the frequency ratios are rounded to four decimal places and handles edge cases such as when `cut_off` is larger than the number of distinct words or when it is `0`."}
{"task_id": 570, "completion_id": 0, "completion": "To solve the problem of implementing a keyword-based Bernoulli Naive Bayes classifier, we need to follow these steps:\n\n1. Count the occurrences of each keyword in documents belonging to each class.\n2. Calculate the conditional probability \\( P(w|c) \\) for each keyword in each class.\n3. For each test sample, compute the likelihood of each class using the conditional probabilities.\n4. Predict the class with the highest likelihood for each test sample.\n\nHere's the implementation of the `naive_bayes_predict` function:\n\n```python\nfrom collections import defaultdict\n\ndef naive_bayes_predict(train_data: list[tuple[str, list[str]]],\n                        keywords: set[str],\n                        test_samples: list[list[str]]) -> list[str]:\n    \"\"\"Build an unsmoothed Bernoulli Naive Bayes text classifier and predict labels.\n\n    The classifier treats every word in *keywords* as a binary feature that is\n    either present or absent in a document.  Words outside *keywords* are\n    ignored.  Training follows the standard Bernoulli model *without* any form\n    of smoothing, and *no* class prior is used when computing the likelihoods.\n\n    Args:\n        train_data: List of ``(label, tokens)`` pairs forming the training set.\n        keywords:   Set of words that act as binary features.\n        test_samples: List of token lists (one per unseen document).\n\n    Returns:\n        List of predicted labels for the documents in *test_samples* (same\n        order as input).\n    \"\"\"\n    \n    # Step 1: Initialize dictionaries to store counts and probabilities\n    class_counts = defaultdict(int)\n    keyword_counts = defaultdict(lambda: defaultdict(int))\n    \n    # Step 2: Train the classifier\n    for label, tokens in train_data:\n        class_counts[label] += 1\n        for token in tokens:\n            if token in keywords:\n                keyword_counts[label][token] += 1\n    \n    # Step 3: Calculate conditional probabilities\n    total_documents = sum(class_counts.values())\n    keyword_probabilities = {}\n    \n    for label, counts in keyword_counts.items():\n        for keyword, count in counts.items():\n            keyword_probabilities[(label, keyword)] = count / class_counts[label]\n    \n    # Step 4: Predict labels for test samples\n    predictions = []\n    for sample in test_samples:\n        max_likelihood = -float('inf')\n        best_label = None\n        \n        for label in class_counts.keys():\n            likelihood = 1\n            \n            for keyword in keywords:\n                if keyword in sample:\n                    likelihood *= keyword_probabilities[(label, keyword)]\n                else:\n                    likelihood *= (1 - keyword_probabilities.get((label, keyword), 0))\n            \n            if likelihood > max_likelihood:\n                max_likelihood = likelihood\n                best_label = label\n        \n        predictions.append(best_label)\n    \n    return predictions\n\n# Example usage\ntrain_data = [\n    ('spam', ['cheap', 'offer']),\n    ('ham',  ['hello', 'friend']),\n    ('spam', ['buy', 'cheap'])\n]\nkeywords     = {'cheap', 'buy', 'hello'}\ntest_samples = [['cheap'], ['hello']]\n\nprint(naive_bayes_predict(train_data, keywords, test_samples))  # Output: ['spam', 'ham']\n```\n\n### Explanation:\n\n1. **Counting Occurrences**: We use two dictionaries: `class_counts` to keep track of how many documents belong to each class, and `keyword_counts` to count how many times each keyword appears in documents of each class.\n\n2. **Calculating Conditional Probabilities**: For each keyword in each class, we calculate its conditional probability \\( P(w|c) \\) using the formula:\n   \\[\n   P(w|c) = \\frac{\\text{count}_c(w)}{\\text{N}_c}\n   \\]\n   where \\(\\text{count}_c(w)\\) is the number of times keyword \\(w\\) appears in documents of class \\(c\\), and \\(\\text{N}_c\\) is the total number of documents in class \\(c\\).\n\n3. **Predicting Labels**: For each test sample, we compute the likelihood of each class using the conditional probabilities. The likelihood is calculated as:\n   \\[\n   \\prod_{w \\in \\text{keywords} \\cap \\text{T}} P(w|c) \\times \\prod_{w \\in \\text{keywords} \\setminus \\text{T}} (1 - P(w|c))\n   \\]\n   where \\(T\\) is the set of tokens in the test sample. The label with the highest likelihood is chosen as the prediction.\n\nThis implementation efficiently computes the required probabilities and makes predictions based on the learned model."}
{"task_id": 571, "completion_id": 0, "completion": "To complete the `compute_svm_parameters` function, we need to follow the steps outlined in the problem description. Here's how you can implement it:\n\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector w\n    w = np.dot(X.T, np.multiply(alpha, y))\n    \n    # Round the weight vector to 4 decimal places\n    w_rounded = np.round(w, decimals=4)\n    \n    # If X has only one feature, convert w to a one-dimensional list\n    if X.shape[1] == 1:\n        w_list = w_rounded.tolist()\n    else:\n        w_list = w_rounded.tolist()\n    \n    # Compute the bias term b\n    b = np.mean(np.multiply(y, (1 - np.dot(X, w))))\n    \n    # Round the bias term to 4 decimal places\n    b_rounded = round(b, 4)\n    \n    return w_list, b_rounded\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [-1, -2]])\n# y = np.array([1, 1, -1])\n# alpha = np.array([0.5, 0.5, 0])\n# print(compute_svm_parameters(X, y, alpha))\n```\n\n### Explanation:\n1. **Weight Vector Calculation**: We use the formula \\( \\mathbf{w} = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y}) \\) to compute the weight vector \\( \\mathbf{w} \\). The operation \\( \\boldsymbol{\\alpha} \\odot \\mathbf{y} \\) performs element-wise multiplication between the alpha vector and the label vector.\n\n2. **Rounding**: We round each element of the weight vector to 4 decimal places using `np.round`.\n\n3. **Bias Term Calculation**: The bias term \\( b \\) is calculated using the formula \\( b = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i) \\).\n\n4. **Rounding the Bias**: We round the bias term to 4 decimal places using `round`.\n\n5. **Handling Single Feature Case**: If the input matrix \\( X \\) has only one feature, we convert the weight vector to a one-dimensional list before returning.\n\nThis implementation ensures that both the weight vector and the bias term are correctly computed and formatted according to the specified requirements."}
{"task_id": 572, "completion_id": 0, "completion": "To implement the Local Outlier Factor (LOF) algorithm from scratch, we need to follow the steps outlined in the problem description. Below is the complete implementation of the `local_outlier_factor` function:\n\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Compute the Local Outlier Factor for each point in the dataset.\"\"\"\n    n = len(data)\n    \n    # Step 1: Compute the full pairwise Euclidean distance matrix\n    dist_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            dist_matrix[i, j] = np.linalg.norm(data[i] - data[j])\n            dist_matrix[j, i] = dist_matrix[i, j]\n    \n    # Step 2: For every sample p, get its k-distance and the index list of the k-nearest neighbors N_k(p)\n    lof_scores = []\n    outlier_indices = []\n    \n    for p in range(n):\n        distances = dist_matrix[p]\n        sorted_indices = np.argsort(distances)\n        \n        k_distances = distances[sorted_indices[:k]]\n        N_k_p = sorted_indices[:k]\n        \n        # Step 3: Define the reachability distance between two points\n        def reach_dist(q):\n            return max(k_distances[q], dist_matrix[p, q])\n        \n        # Step 4: Calculate the Local Reachability Density (LRD) of p\n        lrd_p = k / sum(reach_dist(q) for q in N_k_p)\n        \n        # Step 5: Calculate the Local Outlier Factor (LOF) of p\n        lof_score = sum(lrd_q / k for q in N_k_p) / lrd_p\n        \n        # Round the LOF score to 4 decimal places\n        lof_score_rounded = round(lof_score, 4)\n        \n        lof_scores.append(lof_score_rounded)\n        \n        # Check if the LOF score is greater than epsilon\n        if lof_score_rounded > epsilon:\n            outlier_indices.append(p)\n    \n    # Return the list of LOF scores and the list of outlier indices\n    return lof_scores, outlier_indices\n\n# Example usage:\ndata = np.array([[1, 1], [1.5, 1.5], [0, 0], [2, 2], [3, 3]])\nk = 2\nepsilon = 1.5\nlof_scores, outlier_indices = local_outlier_factor(data, k, epsilon)\nprint(\"LOF Scores:\", lof_scores)\nprint(\"Outlier Indices:\", outlier_indices)\n```\n\n### Explanation:\n1. **Distance Matrix**: We first compute the full pairwise Euclidean distance matrix using NumPy's `linalg.norm`.\n\n2. **k-Distance Calculation**: For each sample `p`, we sort the distances and select the `k` smallest distances to find the `k`-th nearest neighbors (`N_k(p)`).\n\n3. **Reachability Distance**: For each neighbor `q` in `N_k(p)`, we calculate the reachability distance as the maximum of the `k`-distance of `q` or the distance between `p` and `q`.\n\n4. **Local Reachability Density (LRD)**: The LRD of `p` is calculated as `k / \u03a3_{q\u2208N_k(p)} reach-dist_k(p,q)`.\n\n5. **Local Outlier Factor (LOF)**: The LOF of `p` is calculated as `(\u03a3_{q\u2208N_k(p)} LRD_k(q) / k) / LRD_k(p)`.\n\n6. **Rounding and Filtering**: Each LOF score is rounded to 4 decimal places, and any sample with a LOF score greater than `epsilon` is marked as an outlier.\n\nThis implementation should correctly compute the LOF scores and identify outliers based on the given constraints."}
{"task_id": 573, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_train(X_train: np.ndarray,\n                     y_train: np.ndarray,\n                     eta: float = 0.1,\n                     n_iter: int = 1000) -> tuple[list[float], float, list[int]]:\n    \"\"\"Implement the Perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training patterns of shape (m, n).\n    y_train : np.ndarray\n        Target labels (+1 or -1) of length m.\n    eta : float, optional\n        Learning rate, by default 0.1.\n    n_iter : int, optional\n        Maximum number of passes over the data, by default 1000.\n\n    Returns\n    -------\n    tuple[list[float], float, list[int]]\n        A tuple containing the learned weight vector (as list), the bias term,\n        and a list with the number of errors in each epoch.\n    \"\"\"\n    m, n = X_train.shape\n    weights = np.zeros(n)\n    bias = 0\n    error_count_history = []\n\n    for _ in range(n_iter):\n        error_count = 0\n        for i in range(m):\n            prediction = np.dot(weights, X_train[i]) + bias\n            if y_train[i] * prediction <= 0:\n                weights += eta * y_train[i] * X_train[i]\n                bias += eta * y_train[i]\n                error_count += 1\n        error_count_history.append(error_count)\n\n        if error_count == 0:\n            break\n\n    # Round the weights and bias to the nearest 4th decimal place\n    weights_rounded = [round(w, 4) for w in weights]\n    bias_rounded = round(bias, 4)\n\n    return weights_rounded, bias_rounded, error_count_history\n```\n\nThis implementation of the Perceptron learning algorithm follows the specified requirements, including initializing the weight vector and bias, iterating through the training data, updating the parameters when necessary, and returning the final weights, bias, and error count history. The weights and bias are rounded to the nearest 4th decimal place before being returned."}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * np.floor(c / 2)\n        x_train = np.random.uniform(base + 1, base + 9, size=(n_train,))\n        y_train = np.random.uniform(base + 1, base + 9, size=(n_train,))\n        x_val = np.random.uniform(base + 1, base + 9, size=(n_val,))\n        y_val = np.random.uniform(base + 1, base + 9, size=(n_val,))\n        \n        X_train.extend(np.column_stack((x_train, y_train)))\n        X_val.extend(np.column_stack((x_val, y_val)))\n        y_train.extend([c] * n_train)\n        y_val.extend([c] * n_val)\n    \n    # Convert lists to NumPy arrays and round to 4 decimal places\n    X_train = np.array(X_train).round(4)\n    X_val = np.array(X_val).round(4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\nThis function generates synthetic 2D data according to the specified requirements. It ensures reproducibility using a seed if provided, samples points uniformly within the specified ranges, rounds the coordinates to 4 decimal places, and returns the data in the required format."}
{"task_id": 577, "completion_id": 0, "completion": "To solve the task of generating a synthetic 2-D data set for a binary or multi-class classification problem, we need to carefully follow the rules provided. Here's how you can implement the `generate_data` function in Python:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a tiny 2-D synthetic data set for classification.\n\n    The function must create two NumPy arrays with coordinates and two with\n    labels using the rules explained in the task description.\n\n    Args:\n        m:        Number of different classes.\n        n_train:  Number of training samples per class.\n        n_val:    Number of validation samples per class.\n        seed:     Optional seed so that results are reproducible.\n\n    Returns:\n        A tuple ``(X_train, X_val, y_train, y_val)`` consisting of four\n        NumPy arrays.\n    \"\"\"\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Initialize lists to store the data\n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    # Generate data for each class\n    for i in range(m):\n        # Calculate mean and standard deviation for X\n        mean_x = (i // 2 + 0.1) * 10\n        std_x = 0.4 * 10\n        \n        # Calculate mean and standard deviation for Y\n        mean_y = (i % 2 * 0.5 + 0.1) * 10\n        std_y = 0.4 * 10\n        \n        # Sample training data\n        for _ in range(n_train):\n            x = round(random.normalvariate(mean_x, std_x), 4)\n            y = round(random.normalvariate(mean_y, std_y), 4)\n            X_train.append([x, y])\n            y_train.append((i - 0.5) * 2)\n        \n        # Sample validation data\n        for _ in range(n_val):\n            x = round(random.normalvariate(mean_x, std_x), 4)\n            y = round(random.normalvariate(mean_y, std_y), 4)\n            X_val.append([x, y])\n            y_val.append((i - 0.5) * 2)\n    \n    # Convert lists to NumPy arrays\n    X_train = np.array(X_train)\n    X_val = np.array(X_val)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    \n    return X_train, X_val, y_train, y_val\n\n# Example usage\nm = 2\nn_train = 2\nn_val = 1\nseed = 42\nX_train, X_val, y_train, y_val = generate_data(m, n_train, n_val, seed)\n\nprint(\"X_train:\")\nprint(X_train)\nprint(\"\\ny_train:\")\nprint(y_train)\nprint(\"\\nX_val:\")\nprint(X_val)\nprint(\"\\ny_val:\")\nprint(y_val)\n```\n\n### Explanation:\n\n1. **Random Seed**: We use the `random.seed(seed)` to ensure that the results are reproducible.\n\n2. **Data Generation Loop**: We iterate over each class `i` from `0` to `m-1`.\n\n3. **Parameters Calculation**:\n   - For `X` coordinates, the mean is calculated as `(i // 2 + 0.1) * 10` and the standard deviation as `0.4 * 10`.\n   - For `Y` coordinates, the mean is calculated as `(i % 2 * 0.5 + 0.1) * 10` and the standard deviation as `0.4 * 10`.\n\n4. **Sampling**:\n   - For both training and validation sets, we sample `n_train` and `n_val` points respectively using `random.normalvariate` to get normally distributed values around the calculated means and standard deviations.\n   - These points are rounded to four decimal places using `round(x, 4)`.\n\n5. **Appending to Lists**: Each sampled point and its corresponding label are appended to their respective lists.\n\n6. **Conversion to NumPy Arrays**: Finally, the lists of coordinates and labels are converted to NumPy arrays.\n\nThis implementation ensures that the data is generated according to the specified rules and constraints."}
{"task_id": 578, "completion_id": 0, "completion": "Here's how you can implement the `make_pairs` function according to the specified requirements:\n\n```python\nfrom typing import List, Tuple\n\ndef make_pairs(corpus: list) -> list[Tuple]:\n    \"\"\"Return every consecutive pair (bigram) found in *corpus*.\n\n    Args:\n        corpus: A list containing any kind of elements.\n\n    Returns:\n        A list of tuples where each tuple is a pair of consecutive\n        elements from *corpus*.\n    \"\"\"\n    # Initialize an empty list to store the pairs\n    pairs = []\n    \n    # Iterate over the corpus up to the second-to-last element\n    for i in range(len(corpus) - 1):\n        # Create a tuple of the current element and the next element\n        pair = (corpus[i], corpus[i + 1])\n        # Append the pair to the list\n        pairs.append(pair)\n    \n    # Return the list of pairs\n    return pairs\n\n# Example usage:\ncorpus = [\"apple\", \"banana\", \"cherry\", \"date\"]\nresult = make_pairs(corpus)\nprint(result)  # Output: [('apple', 'banana'), ('banana', 'cherry'), ('cherry', 'date')]\n```\n\n### Explanation:\n- We define a function `make_pairs` that takes a list `corpus` as input.\n- We initialize an empty list `pairs` to store the resulting bigrams.\n- We iterate over the `corpus` using a `for` loop, stopping one element before the end to avoid index out-of-range errors when accessing the next element.\n- Inside the loop, we create a tuple `(corpus[i], corpus[i + 1])` representing a bigram and append it to the `pairs` list.\n- Finally, we return the list of bigrams."}
{"task_id": 579, "completion_id": 0, "completion": "Here's a complete implementation of the classical (Lloyd-style) K-Means clustering algorithm using only base Python and NumPy:\n\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"Input 'X' must be a 2D NumPy array.\")\n    if not isinstance(k, int) or k <= 0 or k > X.shape[0]:\n        raise ValueError(\"Number of clusters 'k' must be a positive integer less than or equal to the number of data points.\")\n    if not isinstance(max_iters, int) or max_iters < 0:\n        raise ValueError(\"Maximum number of iterations 'max_iters' must be a non-negative integer.\")\n\n    m, d = X.shape\n    if k > m:\n        raise ValueError(\"The number of clusters cannot be greater than the number of data points.\")\n\n    # Initialisation: Take the first k points as the initial centers\n    centres = X[:k].tolist()\n\n    # Round the initial centers to 4 decimal places\n    centres_rounded = [round(coord, 4) for coord in centres]\n\n    # Assignment step\n    labels = []\n    for i in range(m):\n        distances = np.linalg.norm(X[i] - np.array(centres_rounded), axis=1)\n        min_index = np.argmin(distances)\n        labels.append(min_index)\n\n    # Update step\n    for _ in range(max_iters):\n        new_centres = []\n        for j in range(k):\n            points_in_cluster = X[np.array(labels) == j]\n            if points_in_cluster.size > 0:\n                new_centre = np.mean(points_in_cluster, axis=0)\n                new_centres.append(new_centre.tolist())\n            else:\n                new_centres.append(centres_rounded[j])\n        \n        # Round the new centres to 4 decimal places\n        new_centres_rounded = [round(coord, 4) for coord in new_centres]\n\n        # Check for convergence\n        if np.allclose(np.array(centres_rounded), np.array(new_centres_rounded)):\n            break\n\n        # Update the current centres\n        centres_rounded = new_centres_rounded\n\n    return labels, centres_rounded\n\n# Example usage:\n# X = np.random.rand(100, 2)  # Generate random 2D data points\n# k = 5\n# labels, centres = kmeans(X, k)\n# print(\"Labels:\", labels)\n# print(\"Centres:\", centres)\n```\n\n### Explanation:\n1. **Initialization**: The first `k` points from the dataset are chosen as the initial cluster centers.\n2. **Assignment Step**: Each data point is assigned to the closest center based on Euclidean distance.\n3. **Update Step**: The center of each cluster is updated to be the mean of all points in that cluster.\n4. **Convergence Check**: The process repeats until the cluster centers do not change significantly between iterations or the maximum number of iterations is reached.\n5. **Output**: The function returns the final cluster labels and the rounded coordinates of the cluster centers."}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * X.shape[1], 0.0\n\n    m, n = X.shape\n    weights = np.zeros(n)\n    bias = 0.0\n\n    for _ in range(iterations):\n        # Compute predictions\n        predictions = np.dot(X, weights) + bias\n        \n        # Compute residuals\n        residuals = y - predictions\n        \n        # Compute gradients\n        gradient_w = -2 * np.dot(X.T, residuals) + l1_penalty * np.sign(weights) + 2 * l2_penalty * weights\n        gradient_b = -2 * np.sum(residuals)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_w / m\n        bias -= learning_rate * gradient_b / m\n    \n    # Round the results to 4 decimal places\n    weights_rounded = [round(weight, 4) for weight in weights]\n    bias_rounded = round(bias, 4)\n    \n    return weights_rounded, bias_rounded\n```\n\nThis implementation defines the `elastic_net_regression` function which implements batch gradient descent for Elastic-Net regularization. It calculates the predictions, residuals, gradients, and updates the weights and bias accordingly. Finally, it rounds the results to four decimal places before returning them."}
{"task_id": 582, "completion_id": 0, "completion": "To solve the problem of constructing the undirected k-Nearest-Neighbour (k-NN) graph, we need to follow these steps:\n\n1. Determine the value of \\( k \\).\n2. Compute the Euclidean distances between all pairs of points.\n3. Identify the k nearest distinct neighbors for each point.\n4. Create an undirected adjacency matrix from the directed edges identified in step 3.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef nearest_neighbor_graph(X):\n    \"\"\"Build the undirected k-nearest-neighbour adjacency matrix.\n\n    Parameters\n    ----------\n    X : Sequence[Sequence[float]]\n        2-D array-like object of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix W where W[i][j] is 1 iff an undirected edge exists\n        between sample i and sample j, otherwise 0.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Step 1: Determine the value of k\n    k = min(int(np.sqrt(n_samples)), 10)\n    \n    # Step 2: Compute the Euclidean distances between all pairs of points\n    dist_matrix = np.linalg.norm(X[:, np.newaxis] - X[np.newaxis, :], axis=-1)\n    \n    # Step 3: Identify the k nearest distinct neighbors for each point\n    indices_sorted = np.argsort(dist_matrix)\n    \n    # Step 4: Create an undirected adjacency matrix from the directed edges\n    W = np.zeros((n_samples, n_samples))\n    \n    for i in range(n_samples):\n        # Get the k smallest indices excluding the current point i\n        k_nearest_indices = indices_sorted[i][1:k+1]\n        \n        # Add the undirected edges\n        for j in k_nearest_indices:\n            W[i, j] = 1\n            W[j, i] = 1\n    \n    # Convert the result to a list of lists\n    W_list = W.tolist()\n    \n    return W_list\n\n# Example usage:\nX = [[0, 0], [1, 0], [0, 1], [10, 10]]\nprint(nearest_neighbor_graph(X))\n```\n\n### Explanation:\n\n1. **Determine \\( k \\)**:\n   - We calculate \\( k = \\min\\left(\\lfloor\\sqrt{n}\\rfloor, 10\\right) \\).\n\n2. **Compute Distances**:\n   - We use `np.linalg.norm` to compute the Euclidean distance matrix. This gives us a symmetric matrix where `dist_matrix[i][j]` is the distance between point `i` and point `j`.\n\n3. **Identify Neighbors**:\n   - We sort the indices of each row of the distance matrix using `np.argsort`. The first element in each sorted list is the point itself, so we take the next \\( k \\) elements to get the k nearest distinct neighbors.\n\n4. **Create the Adjacency Matrix**:\n   - We initialize an empty matrix `W` of zeros. Then, for each point `i`, we add edges to its k nearest neighbors in both directions.\n\n5. **Convert to List of Lists**:\n   - Finally, we convert the adjacency matrix `W` to a list of lists for the desired output format.\n\nThis implementation ensures that the resulting adjacency matrix is undirected and contains only binary values (0s and 1s)."}
{"task_id": 584, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_initial_state(state: list[list[int | float]]) -> list[list[list[int | float]]]:\n    \"\"\"Stack a single 2-D Atari frame four times to form the initial state.\n\n    Args:\n        state: 2-D list representing the grayscale frame (height \u00d7 width).\n\n    Returns:\n        3-D list with shape (height \u00d7 width \u00d7 4) where the frame is repeated\n        along the last axis.\n    \"\"\"\n    # Convert the input list to a NumPy array\n    state_array = np.array(state)\n    \n    # Stack the array along the new last axis\n    stacked_state = np.stack([state_array] * 4, axis=-1).tolist()\n    \n    return stacked_state\n```"}
{"task_id": 585, "completion_id": 0, "completion": "To replicate the behavior of Python 2's `cmp` function in Python 3, we need to define a function that compares two objects using the standard comparison operators (`<`, `>`, and `==`). Here's how you can implement it:\n\n```python\ndef cmp(a: object, b: object) -> int:\n    \"\"\"Compares two values and returns their ordering.\n\n    The function mimics Python 2's ``cmp``. It must return:\n    \u2022  1  if ``a`` is greater than ``b``\n    \u2022  0  if ``a`` equals ``b``\n    \u2022  -1 if ``a`` is less than ``b``\n\n    Args:\n        a: Any Python object supporting comparison operations.\n        b: Any Python object supporting comparison operations.\n\n    Returns:\n        int: One of -1, 0, or 1 indicating the comparison result.\n    \"\"\"\n    if a > b:\n        return 1\n    elif a < b:\n        return -1\n    else:\n        return 0\n\n# Example usage:\nprint(cmp(5, 3))      # Output: 1\nprint(cmp(3, 5))      # Output: -1\nprint(cmp(3, 3))      # Output: 0\n```\n\n### Explanation:\n- The function `cmp` takes two arguments, `a` and `b`.\n- It uses the comparison operators `>` and `<` to determine the relationship between `a` and `b`.\n- If `a` is greater than `b`, it returns `1`.\n- If `a` is less than `b`, it returns `-1`.\n- If `a` is equal to `b`, it returns `0`.\n\nThis implementation ensures that the function behaves similarly to Python 2's `cmp` function, providing the same ordering results for comparable objects."}
{"task_id": 586, "completion_id": 0, "completion": "To implement the LOWESS single-point prediction in Python, we need to follow the steps outlined in the task description. Below is the complete implementation of the `lowess_predict` function:\n\n```python\nimport numpy as np\n\ndef lowess_predict(x: np.ndarray, y: np.ndarray, x_query: float, window: int) -> float:\n    \"\"\"Predict a single y value using LOWESS with linear (degree-1) local fitting.\n\n    The function should perform the following steps:\n    1. Normalise x and y separately to [0, 1].\n    2. Choose *window* nearest neighbours of x_query (on the normalised x axis).\n    3. Compute tricubic weights for those neighbours.\n    4. Fit a weighted straight line through the selected points.\n    5. Predict the normalised \u0177 at the normalised x_query, convert it back to the\n       original scale and round to 4 decimal places.\n\n    Args:\n        x (np.ndarray): 1-D array of training inputs.\n        y (np.ndarray): 1-D array of training targets (same length as x).\n        x_query (float): The x value at which to make the prediction.\n        window (int): Number of neighbours to use (2 \u2264 window \u2264 len(x)).\n\n    Returns:\n        float: The LOWESS prediction for x_query rounded to four decimals.\n    \"\"\"\n    # Validate input\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise ValueError(\"x and y must be numpy arrays.\")\n    if x.shape != y.shape:\n        raise ValueError(\"|x| must equal |y|.\")\n    if not (2 <= window <= len(x)):\n        raise ValueError(\"2 \u2264 window \u2264 |x|.\")\n\n    # Normalize x and y\n    min_x, max_x = np.min(x), np.max(x)\n    min_y, max_y = np.min(y), np.max(y)\n    x_norm = (x - min_x) / (max_x - min_x)\n    y_norm = (y - min_y) / (max_y - min_y)\n\n    # Find nearest neighbors\n    dists = np.abs(x_norm - x_query)\n    idx_sorted = np.argsort(dists)\n    idx_neighbors = idx_sorted[:window]\n\n    # Compute tricubic weights\n    d_max = dists[idx_neighbors].max()\n    weights = np.zeros_like(dists)\n    for i, idx in enumerate(idx_neighbors):\n        d = dists[idx]\n        if d <= d_max:\n            w = (1 - abs(d / d_max)**3)**3\n        else:\n            w = 0\n        weights[i] = w\n\n    # Calculate weighted sum and weight sum\n    weighted_sum_y = np.sum(weights * y_norm[idx_neighbors])\n    weighted_sum_weights = np.sum(weights)\n\n    # Calculate slope and intercept\n    if weighted_sum_weights == 0:\n        # Handle case where all weights are zero\n        a = np.mean(y_norm[idx_neighbors])\n    else:\n        a = weighted_sum_y / weighted_sum_weights\n        b = np.sum(weights * (x_norm[idx_neighbors] - x_query) * y_norm[idx_neighbors]) / weighted_sum_weights\n\n    # Predict the normalised \u0177 at the normalised query point\n    y_pred_norm = a + b * (x_query - x_query)\n\n    # Convert back to original scale\n    y_pred_original = y_pred_norm * (max_y - min_y) + min_y\n\n    # Round to 4 decimal places\n    return round(y_pred_original, 4)\n\n# Example usage:\n# x = np.array([1, 2, 3, 4, 5])\n# y = np.array([2, 3, 5, 7, 11])\n# x_query = 3.5\n# window = 3\n# print(lowess_predict(x, y, x_query, window))\n```\n\n### Explanation:\n1. **Normalization**: Both `x` and `y` are normalized to the interval `[0, 1]`.\n2. **Nearest Neighbors**: We find the `window` nearest neighbors to `x_query` based on their normalized values.\n3. **Tricubic Weights**: We compute tricubic weights for these neighbors, ensuring that they sum to 1.\n4. **Weighted Least Squares**: We fit a weighted straight line using the tricubic weights.\n5. **Prediction**: We predict the normalised value at `x_query`, convert it back to the original scale, and round the result to four decimal places.\n6. **Validation**: Basic checks are performed to ensure the input data meets the requirements."}
{"task_id": 587, "completion_id": 0, "completion": "Here's how you can implement the `manage_replay_memory` function based on the given requirements:\n\n```python\nimport random\nfrom typing import Any, List, Tuple\n\ndef manage_replay_memory(operations: List[Tuple]) -> List[Any]:\n    \"\"\"Perform operations on an in-memory replay buffer.\n\n    The buffer starts out empty and is manipulated according to *operations*.\n\n    Args:\n        operations: A list of tuples describing the commands to execute. The\n            first element of each tuple is a string (\"append\", \"pop\", \"sample\",\n            or \"size\").  Additional elements contain the arguments of that\n            command.\n\n    Returns:\n        A list containing the outputs of all commands that return a value, in\n        the same order in which those commands appear in *operations*.\n    \"\"\"\n    # Set the seed for reproducibility\n    random.seed(0)\n    \n    # Initialize the replay memory as an empty list\n    replay_buffer = []\n    \n    # Initialize a list to collect the outputs of operations that produce a result\n    outputs = []\n    \n    # Iterate over each operation in the input list\n    for operation in operations:\n        command, *args = operation\n        \n        if command == 'append':\n            # Append the transition to the replay buffer\n            replay_buffer.append(args[0])\n        elif command == 'pop':\n            # Pop and return the most recently added transition\n            if replay_buffer:\n                outputs.append(replay_buffer.pop())\n            else:\n                outputs.append(None)\n        elif command == 'sample':\n            # Sample a batch of transitions\n            batch_size = args[0]\n            if batch_size > len(replay_buffer):\n                outputs.append([])\n            else:\n                sampled_indices = random.sample(range(len(replay_buffer)), batch_size)\n                sampled_transitions = [replay_buffer[i] for i in sampled_indices]\n                outputs.append(sampled_transitions)\n        elif command == 'size':\n            # Return the current number of elements in the replay buffer\n            outputs.append(len(replay_buffer))\n    \n    return outputs\n\n# Example usage:\noperations = [('append', 1), ('append', 2), ('sample', 1), ('pop',), ('size',)]\nresult = manage_replay_memory(operations)\nprint(result)  # Output: [[2], 2, 1]\n```\n\n### Explanation:\n- **Seed Setting**: We set the seed to 0 using `random.seed(0)` to ensure that the behavior is deterministic.\n- **Buffer Initialization**: We initialize an empty list `replay_buffer` to store the transitions.\n- **Outputs Collection**: We maintain a separate list `outputs` to store the results of operations that produce a value.\n- **Operation Execution**:\n  - `'append'`: Adds a transition to the buffer.\n  - `'pop'`: Removes and returns the last transition if the buffer is not empty; otherwise, returns `None`.\n  - `'sample'`: Samples a specified number of transitions from the buffer. If the request exceeds the buffer size, it returns an empty list.\n  - `'size'`: Returns the current number of elements in the buffer.\n- **Return Value**: Finally, we return the list of collected outputs."}
{"task_id": 588, "completion_id": 0, "completion": "To solve the problem of finding the maximum depth of a binary tree, we need to traverse the tree recursively and calculate the depth of each subtree. Here's how you can implement the `tree_max_depth` function:\n\n```python\nfrom typing import Optional\n\nclass Node:\n    \"\"\"A simple binary tree node.\"\"\"\n    \n    def __init__(self,\n                 data=None,\n                 split_variable=None,\n                 split_variable_value=None,\n                 left: Optional['Node'] = None,\n                 right: Optional['Node'] = None,\n                 depth: int = 0,\n                 criterion_value=None):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.criterion_value = criterion_value\n\n\ndef tree_max_depth(root: Optional[Node]) -> int:\n    \"\"\"Computes the maximum depth of a binary tree.\n\n    The depth of the root node is defined to be 0.  The depth of an empty\n    tree is defined to be -1.\n\n    Args:\n        root: The root of the binary tree (Node or None).\n\n    Returns:\n        The maximum depth as an integer.\n    \"\"\"\n    if root is None:\n        return -1\n    \n    # Recursively find the maximum depth of the left and right subtrees\n    left_depth = tree_max_depth(root.left)\n    right_depth = tree_max_depth(root.right)\n    \n    # The maximum depth of the current tree is the greater of the depths of the subtrees plus one for the current node\n    return max(left_depth, right_depth) + 1\n\n# Example usage:\n# Constructing a balanced binary tree\nroot = Node(10)\nroot.left = Node(5)\nroot.right = Node(20)\nroot.right.left = Node(15)\nroot.right.right = Node(25)\n\nprint(tree_max_depth(root))  # Output: 2\n```\n\n### Explanation:\n1. **Base Case**: If the `root` is `None`, the tree is empty, so the depth is `-1`.\n2. **Recursive Case**: \n   - Calculate the depth of the left subtree using `tree_max_depth(root.left)`.\n   - Calculate the depth of the right subtree using `tree_max_depth(root.right)`.\n   - The depth of the current node is the maximum of the depths of its left and right subtrees, plus one to account for the current node itself.\n3. **Return the Result**: The function returns the computed depth of the entire tree.\n\nThis approach ensures that we correctly compute the maximum depth of the binary tree without relying on the `depth` attribute, which may not be accurate."}
{"task_id": 590, "completion_id": 0, "completion": "To solve the problem of performing a single-pass Minesweeper logic step, we need to follow these steps:\n\n1. Iterate through each revealed square in the `ground` matrix.\n2. For each revealed square, determine the number of unflagged neighbors.\n3. Apply the Minesweeper rules based on the value of the revealed square:\n   - If the value is 0, reveal all unflagged neighbors.\n   - If the value is 8, flag all unflagged neighbors.\n   - Otherwise, check if the number of unflagged neighbors matches the value.\n4. Collect the coordinates of squares that can be safely revealed (`clicks`) and those that must be flagged (`new_flags`).\n5. Return both lists sorted in ascending lexicographical order.\n\nHere's the implementation of the `minesweeper_step` function:\n\n```python\nimport numpy as np\n\ndef minesweeper_step(ground: list[list[float | int]], \n                     flags: list[list[bool]]) -> tuple[list[tuple[int, int]], \n                                                  list[tuple[int, int]]]:\n    \"\"\"Perform one logical deduction step on a Minesweeper board.\n\n    The function receives the currently visible part of the board (```ground```)\n    and the already placed flags (```flags```). It must return two *sorted* lists:\n\n    1. **clicks** \u2013 every square that can be safely revealed.\n    2. **new_flags** \u2013 every square that must be a bomb according to the rules\n       described in the task description.\n\n    Notes:\n        * ``ground`` and ``flags`` have identical dimensions.\n        * Hidden squares are represented by ``np.nan`` inside ``ground``.\n        * Neighbourhood is the 8-connected Moore neighbourhood.\n    \"\"\"\n    def get_neighbors(r, c):\n        directions = [(-1, -1), (-1, 0), (-1, 1),\n                      (0, -1),           (0, 1),\n                      (1, -1), (1, 0), (1, 1)]\n        neighbors = [(r + dr, c + dc) for dr, dc in directions]\n        return [neighbor for neighbor in neighbors if 0 <= neighbor[0] < len(ground) and 0 <= neighbor[1] < len(ground[0])]\n    \n    clicks = []\n    new_flags = []\n    \n    for r in range(len(ground)):\n        for c in range(len(ground[0])):\n            if not np.isnan(ground[r][c]) and not flags[r][c]:\n                value = ground[r][c]\n                neighbors = get_neighbors(r, c)\n                \n                if value == 0:\n                    # All unflagged neighbors can be revealed\n                    for nr, nc in neighbors:\n                        if not np.isnan(ground[nr][nc]) and not flags[nr][nc]:\n                            clicks.append((nr, nc))\n                elif value == 8:\n                    # All unflagged neighbors must be flagged\n                    for nr, nc in neighbors:\n                        if not np.isnan(ground[nr][nc]) and not flags[nr][nc]:\n                            new_flags.append((nr, nc))\n                else:\n                    # Check if the number of unflagged neighbors matches the value\n                    bombs = sum(1 for nr, nc in neighbors if np.isnan(ground[nr][nc]) and not flags[nr][nc])\n                    unexplored = sum(1 for nr, nc in neighbors if np.isnan(ground[nr][nc]) and not flags[nr][nc])\n                    \n                    if bombs == value:\n                        # All unflagged neighbors can be revealed\n                        for nr, nc in neighbors:\n                            if not np.isnan(ground[nr][nc]) and not flags[nr][nc]:\n                                clicks.append((nr, nc))\n                    elif unexplored == value:\n                        # All unflagged neighbors must be flagged\n                        for nr, nc in neighbors:\n                            if not np.isnan(ground[nr][nc]) and not flags[nr][nc]:\n                                new_flags.append((nr, nc))\n    \n    # Sort the results\n    clicks.sort()\n    new_flags.sort()\n    \n    return clicks, new_flags\n\n# Example usage\nif __name__ == \"__main__\":\n    import numpy as np\n    \n    ground = [[1, np.nan],\n              [np.nan, np.nan]]\n    flags = [[False, True],\n              [False, False]]\n    \n    result = minesweeper_step(ground, flags)\n    print(result)  # Output: ([], [])\n```\n\nThis function iterates through each square in the `ground` matrix, checks the conditions, and updates the `clicks` and `new_flags` lists accordingly. Finally, it sorts the results and returns them."}
{"task_id": 591, "completion_id": 0, "completion": "To implement the `neural_net_predict` function, we need to follow the steps outlined in the problem description. We'll iterate over each layer of the network, apply the necessary transformations, and accumulate the results until we reach the output layer. Here's how you can implement this:\n\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Add a bias term to the input vector\n    x_with_bias = [1] + x\n    \n    # Initialize the output with the bias term\n    output = x_with_bias\n    \n    # Iterate over each layer\n    for layer_weights in weights:\n        # Compute the weighted sum for each neuron\n        weighted_sum = np.dot(output, layer_weights)\n        \n        # Apply ReLU activation to all layers except the last one\n        if len(layer_weights) > 1:  # Check if there is more than one neuron\n            output = np.maximum(0, weighted_sum)\n        else:\n            output = weighted_sum\n    \n    # Round the output to four decimal places\n    if len(output) == 1:\n        return round(output[0], 4)\n    else:\n        return [round(val, 4) for val in output]\n\n# Example usage:\n# weights = [\n#     [[1, 0.5], [0.5, 1]],  # Layer 0\n#     [[1, 0.3], [0.3, 1]],  # Layer 1\n#     [[1, 0.2], [0.2, 1]]   # Layer 2\n# ]\n# x = [0.5, 0.5]\n# print(neural_net_predict(weights, x))  # Output will depend on the actual weights\n```\n\n### Explanation:\n1. **Bias Addition**: We add a bias term `1` to the input vector `x` to account for the bias weights in the first layer.\n2. **Weighted Sum Calculation**: For each layer, we calculate the weighted sum of the inputs using `np.dot`.\n3. **ReLU Activation**: If the layer has more than one neuron, we apply the ReLU activation function using `np.maximum`. If it's the last layer, we simply take the weighted sum since the last layer is linear.\n4. **Rounding**: Finally, we round the output to four decimal places and return it as a list or a single float based on whether the output layer has one neuron or multiple neurons."}
{"task_id": 592, "completion_id": 0, "completion": "To solve this problem, we need to identify the neighboring cells of a given cell `(row, col)` on a Minesweeper board. We will then determine which of these neighbors are unopened (i.e., `clicked` is `False`) and how many of them are flagged as mines (`flags` is `True`). Finally, we will return both the list of unopened neighbors and the count of flagged mines.\n\nHere's the implementation of the `analyze_cell` function:\n\n```python\nimport numpy as np\n\ndef analyze_cell(clicked: np.ndarray, flags: np.ndarray, row: int, col: int) -> tuple[list[tuple[int, int]], int]:\n    \"\"\"Return unopened neighbouring cells and flagged-mine count for a board position.\n\n    Args:\n        clicked: 2-D NumPy boolean array where True means the cell is already opened.\n        flags:   2-D NumPy boolean array of the same shape \u2013 True means the cell is\n                  already marked as a mine by the player.\n        row:     Row index of the cell to analyse.\n        col:     Column index of the cell to analyse.\n\n    Returns:\n        A tuple (unopened_neighbours, flagged_count):\n        * unopened_neighbours \u2013 list with coordinates (row, col) of all neighbouring\n          cells that are not opened yet.  The list is sorted ascending first by row\n          then by column.\n        * flagged_count \u2013 integer with the number of neighbouring cells that are\n          currently flagged as mines.\n    \"\"\"\n    # Define possible directions for neighboring cells\n    directions = [\n        (-1, -1), (-1, 0), (-1, 1),\n        (0, -1),           (0, 1),\n        (1, -1), (1, 0), (1, 1)\n    ]\n    \n    # Initialize lists to store unopened neighbors and flagged mines\n    unopened_neighbors = []\n    flagged_count = 0\n    \n    # Iterate over each direction\n    for dr, dc in directions:\n        new_row, new_col = row + dr, col + dc\n        \n        # Check if the new position is within bounds\n        if 0 <= new_row < clicked.shape[0] and 0 <= new_col < clicked.shape[1]:\n            # Check if the cell is unopened and flagged as a mine\n            if not clicked[new_row, new_col] and flags[new_row, new_col]:\n                flagged_count += 1\n            \n            # If the cell is unopened, add its coordinates to the list\n            elif not clicked[new_row, new_col]:\n                unopened_neighbors.append((new_row, new_col))\n    \n    # Sort the list of unopened neighbors by row, then by column\n    unopened_neighbors.sort()\n    \n    return unopened_neighbors, flagged_count\n\n# Example usage:\nclicked = np.array([\n    [True, False, False],\n    [False, False, True],\n    [False, False, False]\n])\n\nflags = np.array([\n    [False, True, False],\n    [False, False, False],\n    [False, False, False]\n])\n\nresult = analyze_cell(clicked, flags, 1, 1)\nprint(result)  # Output: ([], 1)\n```\n\n### Explanation:\n- **Directions**: We define possible movement directions for neighboring cells: up, down, left, right, and their diagonals.\n- **Iteration**: For each direction, we calculate the new coordinates `(new_row, new_col)`.\n- **Bounds Checking**: We ensure that the new coordinates are within the bounds of the board.\n- **Condition Checks**: \n  - If the cell is unopened (`clicked[new_row, new_col] == False`) and flagged as a mine (`flags[new_row, new_col] == True`), we increment the `flagged_count`.\n  - If the cell is unopened but not flagged, we add its coordinates to the `unopened_neighbors` list.\n- **Sorting**: After collecting all unopened neighbors, we sort them by row and then by column using the `sort()` method.\n- **Return**: Finally, we return the sorted list of unopened neighbors and the count of flagged mines."}
{"task_id": 593, "completion_id": 0, "completion": "To solve the problem of deducing mines in Minesweeper using logical deduction, we need to implement a function that applies several rules iteratively until no further deductions can be made. Here's how you can achieve this:\n\n1. **Solved Constraints**: If the value of a constraint is 0, all variables in the constraint are safe. If the value is equal to the number of variables, all variables in the constraint are mines.\n\n2. **Propagation of Knowledge**: Once a variable is determined to be a mine or safe, it must be removed from all other constraints, and the values of those constraints must be adjusted accordingly.\n\n3. **Subset Rule**: If one constraint is a subset of another, the subset constraint can be reduced by removing the common variables and adjusting the value.\n\n4. **Duplicate Removal**: Remove any duplicate constraints.\n\nLet's implement this step-by-step in the provided code:\n\n```python\nfrom copy import deepcopy\n\ndef deduce_mines(constraints: list[list]) -> tuple[list[str], list[str]]:\n    \"\"\"Simplify Minesweeper constraints.\n\n    You receive *constraints* as a list of 2-element lists *[variables, value]* \n    where *variables* is a list of identifiers and *value* is an int telling how\n    many of those variables hide a mine.\n\n    The function must apply the logical rules described in the task statement\n    (solved constraints, propagation, subset rule, duplicate removal) until no\n    new information can be obtained.\n\n    The result is a tuple (safe, mines) of two **alphabetically sorted** lists.\n    The first element contains all variables that are certainly safe, the second\n    one all variables that are certainly mines.  If nothing can be deduced an\n    empty list must be returned instead.\n    \"\"\"\n    while True:\n        safe = []\n        mines = []\n\n        # Apply solved constraints\n        for i, (vars_, val_) in enumerate(constraints):\n            if val_ == 0:\n                safe.extend(vars_)\n                constraints.pop(i)\n            elif val_ == len(vars_):\n                mines.extend(vars_)\n                constraints.pop(i)\n\n        # Propagate knowledge\n        for i, (vars_, val_) in enumerate(deepcopy(constraints)):\n            if val_ == 0:\n                continue\n            for j, (vars_j, val_j) in enumerate(deepcopy(constraints)):\n                if i != j and vars_j.issubset(vars_):\n                    new_vars = vars_.difference(vars_j)\n                    new_val = val_j - val_\n                    if new_val < 0:\n                        raise ValueError(\"Invalid constraint\")\n                    constraints[i] = [new_vars, new_val]\n                    break\n            else:\n                continue\n            break\n\n        # Remove duplicates\n        constraints = list(dict.fromkeys(constraints))\n\n        # Check if no new information was gained\n        if not safe and not mines:\n            break\n\n    # Sort results\n    safe.sort()\n    mines.sort()\n\n    return safe, mines\n\n# Example usage:\nconstraints = [\n    ['A', 'B', 'C'], 5,\n    ['D', 'E'], 1,\n    ['F', 'G'], 3,\n    ['H', 'I', 'J'], 2\n]\n\nsafe, mines = deduce_mines(constraints)\nprint(\"Safe:\", safe)\nprint(\"Mines:\", mines)\n```\n\n### Explanation:\n- **Solved Constraints**: We iterate over each constraint and check if the value is 0 or equal to the number of variables. If so, we add the variables to the respective list (`safe` or `mines`) and remove the constraint.\n  \n- **Propagation of Knowledge**: For each constraint, we check if it can be a subset of any other constraint. If it is, we create a new constraint with the subset of variables and adjust the value. This process continues until no more subsets can be found.\n\n- **Duplicate Removal**: We use a dictionary to remove duplicate constraints based on their variables and value.\n\n- **Break Condition**: The loop breaks if no new information is gained, indicating that the constraints have been fully processed.\n\nThis implementation efficiently deduces the positions of mines and safes in the Minesweeper grid."}
{"task_id": 594, "completion_id": 0, "completion": "To solve the problem of converting a maze into an adjacency list, we need to iterate through each cell in the maze and check for neighboring cells that are also open. Here's how you can implement the `maze_to_graph` function:\n\n```python\ndef maze_to_graph(maze: list[list[int]]) -> dict[tuple[int, int], list[tuple[int, int]]]:\n    \"\"\"Convert a binary maze into an adjacency-list graph.\n\n    A 0 in the maze denotes an open cell, while 1 denotes a wall.  Two open\n    cells are adjacent if they share an edge (up, down, left, right).\n\n    Args:\n        maze: Rectangular 2-D list containing only 0s and 1s.\n\n    Returns:\n        A dictionary mapping each open cell coordinate (row, col) to a sorted\n        list of neighbouring open cell coordinates.  Walls are not included in\n        the dictionary.  If there are no open cells the function returns an\n        empty dictionary.\n    \"\"\"\n    rows = len(maze)\n    cols = len(maze[0])\n    \n    # Dictionary to store the adjacency list\n    graph = {}\n    \n    # Iterate over each cell in the maze\n    for row in range(rows):\n        for col in range(cols):\n            if maze[row][col] == 0:  # Check if the cell is open\n                neighbors = []\n                \n                # Check north\n                if row > 0 and maze[row - 1][col] == 0:\n                    neighbors.append((row - 1, col))\n                # Check south\n                if row < rows - 1 and maze[row + 1][col] == 0:\n                    neighbors.append((row + 1, col))\n                # Check west\n                if col > 0 and maze[row][col - 1] == 0:\n                    neighbors.append((row, col - 1))\n                # Check east\n                if col < cols - 1 and maze[row][col + 1] == 0:\n                    neighbors.append((row, col + 1))\n                \n                # Add the sorted list of neighbors to the graph\n                graph[(row, col)] = sorted(neighbors)\n    \n    return graph\n\n# Example usage:\nmaze = [\n    [0, 1, 0],\n    [0, 0, 0],\n    [1, 0, 0]\n]\n\nprint(maze_to_graph(maze))\n```\n\n### Explanation:\n- We first determine the dimensions of the maze (`rows` and `cols`).\n- We initialize an empty dictionary `graph` to store the adjacency list.\n- We iterate over each cell in the maze using nested loops.\n- For each open cell `(row, col)`, we check its four possible neighbors (north, south, west, east).\n- If a neighbor is also open (`maze[row][col] == 0`), we add it to the list of neighbors.\n- After collecting all neighbors for an open cell, we sort them in ascending lexicographic order and add them to the graph under the key `(row, col)`.\n- Finally, we return the constructed graph.\n\nThis function will correctly convert any valid maze into an undirected graph representation, ensuring that all edges are bidirectional and the neighbors are listed in sorted order."}
{"task_id": 595, "completion_id": 0, "completion": "To implement the K-Means clustering algorithm from scratch in Python, we need to follow the steps outlined in the task description. Here's how you can do it:\n\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Implements the K-Means clustering algorithm from scratch.\"\"\"\n    \n    # Step 1: Initialize centroids with the first k samples\n    centroids = X[:k]\n    \n    # Step 2: Iterate until convergence or max iterations\n    for _ in range(max_iters):\n        # Step 2.1: Assign samples to the nearest centroid\n        distances = np.linalg.norm(X[:, None] - centroids[None], axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 2.2: Calculate new centroids\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Step 2.3: Check for convergence\n        if np.allclose(new_centroids, centroids, atol=1e-4):\n            break\n        \n        # Step 2.4: Update centroids\n        centroids = new_centroids\n    \n    # Step 3: Sort centroids and round to 4 decimal places\n    sorted_centroids = [round(x, 4) for x in centroids]\n    sorted_centroids.sort()\n    \n    return sorted_centroids\n\n# Example usage:\nX = np.array([\n    [1.0, 2.0],\n    [1.5, 1.8],\n    [5.0, 8.0],\n    [8.0, 8.0],\n    [1.0, 7.0],\n    [9.0, 6.0],\n    [0.0, 9.0]\n])\nk = 2\ncentroids = kmeans_centroids(X, k)\nprint(centroids)\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the centroids with the first `k` samples from the input data `X`.\n\n2. **Iteration**: We repeat the following steps until convergence or the maximum number of iterations (`max_iters`) is reached:\n   - **Assignment**: For each sample, calculate the squared Euclidean distance to each centroid and assign the sample to the closest centroid.\n   - **Update**: Compute the new centroid for each cluster as the average of the samples assigned to that cluster.\n   - **Convergence Check**: If the change in centroids between two consecutive iterations is less than `1e-4`, we stop the algorithm.\n\n3. **Sorting and Rounding**: After convergence, we sort the centroids in ascending order and round each coordinate to four decimal places before returning them.\n\nThis implementation ensures that the centroids are initialized deterministically and that the algorithm converges efficiently."}
{"task_id": 596, "completion_id": 0, "completion": "To solve this problem, we need to iterate through each cell in the maze and check for adjacent free cells. We will then build an adjacency list where each key is a tuple representing a free cell's coordinates, and the value is a sorted list of tuples representing the coordinates of its adjacent free neighbors.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef convert_maze_to_graph(maze: np.ndarray) -> dict:\n    \"\"\"Convert a binary maze to an adjacency-list representation.\n\n    You are given a 2-D NumPy array in which 1 represents a free cell and 0\n    represents a wall.  Treat every free cell as a graph vertex.  Connect two\n    vertices with an (undirected) edge if and only if the corresponding cells\n    are adjacent in the maze via the four-neighbourhood (up, down, left, right).\n\n    The function must return a dictionary that maps each free cell\u2019s coordinate\n    `(row, col)` to a **sorted** list of its adjacent free neighbours.  If the\n    maze contains no free cells, return an empty dictionary.\n\n    Args:\n        maze: A 2-D NumPy array of 0s and 1s.\n\n    Returns:\n        A dictionary representing the adjacency list of the maze-graph.\n    \"\"\"\n    # Initialize an empty dictionary to store the adjacency list\n    adj_list = {}\n\n    # Get the dimensions of the maze\n    rows, cols = maze.shape\n\n    # Iterate over each cell in the maze\n    for row in range(rows):\n        for col in range(cols):\n            # Check if the current cell is a free cell (value 1)\n            if maze[row][col] == 1:\n                # List to store the adjacent free cells\n                neighbors = []\n\n                # Check up direction\n                if row > 0 and maze[row - 1][col] == 1:\n                    neighbors.append((row - 1, col))\n                \n                # Check down direction\n                if row < rows - 1 and maze[row + 1][col] == 1:\n                    neighbors.append((row + 1, col))\n                \n                # Check left direction\n                if col > 0 and maze[row][col - 1] == 1:\n                    neighbors.append((row, col - 1))\n                \n                # Check right direction\n                if col < cols - 1 and maze[row][col + 1] == 1:\n                    neighbors.append((row, col + 1))\n\n                # Add the sorted list of neighbors to the adjacency list\n                adj_list[(row, col)] = sorted(neighbors)\n\n    return adj_list\n\n# Example usage:\nmaze = np.array([\n    [1, 0, 1],\n    [1, 1, 0],\n    [0, 1, 1]\n])\n\nprint(convert_maze_to_graph(maze))\n```\n\n### Explanation:\n- We initialize an empty dictionary `adj_list` to store the adjacency information.\n- We get the dimensions of the maze using `maze.shape`.\n- We iterate over each cell in the maze using nested loops.\n- For each free cell (cell with value 1), we check its four possible directions (up, down, left, right) to see if they are also free cells.\n- If a neighboring cell is free, we add its coordinates to the `neighbors` list.\n- After collecting all neighbors, we sort the list and add it to the `adj_list` with the current cell's coordinates as the key.\n- Finally, we return the `adj_list`.\n\nThis function will correctly convert the maze into an adjacency list according to the specified requirements."}
{"task_id": 597, "completion_id": 0, "completion": "To solve the problem of finding the best univariate split for a regression tree using the Mean Squared Error (MSE), we need to follow these steps:\n\n1. Iterate over each feature and calculate the weighted MSE for all possible thresholds within that feature.\n2. Keep track of the minimum weighted MSE found and the corresponding feature and threshold.\n3. Return the feature index, threshold, and the minimum MSE, rounded to four decimal places.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef best_split_mse(X: list | np.ndarray, y: list | np.ndarray) -> list:\n    \"\"\"Return the best (feature, threshold) split for a regression tree.\n\n    A split is evaluated with the weighted mean-squared error (population\n    variance) of the two resulting groups.  Only thresholds that coincide\n    with an existing feature value are examined.\n\n    The function has to return a list `[best_feature_index, best_threshold,\n    best_mse]` where `best_mse` is rounded to 4 decimal places.\n\n    Parameters\n    ----------\n    X : list | np.ndarray\n        Feature matrix with shape (n_samples, n_features). Only numerical\n        values are allowed.\n    y : list | np.ndarray\n        Target vector with shape (n_samples,).\n\n    Returns\n    -------\n    list\n        [index_of_best_feature, value_of_best_threshold, minimal_weighted_mse]\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n    \n    n_samples, n_features = X.shape\n    \n    # Initialize variables to store the best split information\n    best_feature_index = -1\n    best_threshold = None\n    best_mse = float('inf')\n    \n    # Iterate over each feature\n    for feature_idx in range(n_features):\n        # Sort the feature values to find potential thresholds\n        unique_values = np.unique(X[:, feature_idx])\n        \n        # Iterate over each unique threshold\n        for i in range(len(unique_values) - 1):\n            # Calculate the left and right subsets\n            left_indices = X[:, feature_idx] <= unique_values[i]\n            right_indices = X[:, feature_idx] > unique_values[i]\n            \n            # Check if both subsets are non-empty\n            if left_indices.any() and right_indices.any():\n                # Calculate the number of samples in each subset\n                left_count = np.sum(left_indices)\n                right_count = np.sum(right_indices)\n                \n                # Calculate the means and variances of the left and right subsets\n                left_mean = np.mean(y[left_indices])\n                right_mean = np.mean(y[right_indices])\n                left_variance = np.var(y[left_indices], ddof=0)\n                right_variance = np.var(y[right_indices], ddof=0)\n                \n                # Calculate the weighted MSE\n                weighted_mse = (left_count / n_samples) * left_variance + \\\n                                (right_count / n_samples) * right_variance\n                \n                # Update the best split if this MSE is smaller\n                if weighted_mse < best_mse:\n                    best_mse = weighted_mse\n                    best_feature_index = feature_idx\n                    best_threshold = unique_values[i]\n    \n    # Round the best MSE to 4 decimal places\n    best_mse_rounded = round(best_mse, 4)\n    \n    # Return the result as a list\n    return [best_feature_index, best_threshold, best_mse_rounded]\n\n# Example usage:\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny = [1, 2, 3, 4]\nprint(best_split_mse(X, y))\n```\n\n### Explanation:\n- **Feature Iteration**: We iterate over each feature in the dataset.\n- **Threshold Selection**: For each feature, we sort the unique values and consider each pair of consecutive values as potential thresholds.\n- **Subset Calculation**: For each threshold, we split the data into two subsets based on whether the feature value is less than or greater than the threshold.\n- **Weighted MSE Calculation**: We calculate the weighted MSE for the current split using the formula provided.\n- **Update Best Split**: If the calculated MSE is lower than the current best MSE, we update the best split information.\n- **Return Result**: Finally, we return the best feature index, threshold, and the rounded MSE."}
{"task_id": 598, "completion_id": 0, "completion": "Here's the implementation of the `standardize` function in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef standardize(data: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Standardise a numeric data matrix feature-wise.\n\n    The function must transform every column so that it has zero mean and unit\n    variance (z-score).  Constant columns (standard deviation 0) must be filled\n    with zeros.  Any NaN or infinite values occurring during the computation\n    have to be replaced by 0.  The result has to be rounded to 4 decimals and\n    returned as a plain Python list.\n\n    Args:\n        data: Two-dimensional list of numbers (size n_samples \u00d7 n_features).\n\n    Returns:\n        list[list[float]]: Standardised data with the same shape as *data*.\n    \"\"\"\n    # Convert the input list to a NumPy array for efficient computation\n    np_data = np.array(data)\n    \n    # Compute the mean and standard deviation along each column\n    means = np.mean(np_data, axis=0)\n    stds = np.std(np_data, ddof=0, axis=0)\n    \n    # Handle cases where stds are zero (constant columns)\n    stds[stds == 0] = 1\n    \n    # Replace NaN or infinite values with 0\n    np_data[np.isnan(np_data)] = 0\n    np_data[np.isinf(np_data)] = 0\n    \n    # Standardize the data\n    standardized_data = (np_data - means) / stds\n    \n    # Round the results to 4 decimal places\n    standardized_data = standardized_data.round(4)\n    \n    # Convert the result back to a list of lists\n    return standardized_data.tolist()\n\n# Example usage:\ndata = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nresult = standardize(data)\nprint(result)\n```\n\n### Explanation:\n1. **Convert Input to NumPy Array**: We convert the input list to a NumPy array for efficient computation.\n2. **Compute Means and Standard Deviations**: We calculate the mean and standard deviation for each feature across all samples.\n3. **Handle Zero Standard Deviation**: If any standard deviation is zero, we set it to 1 to avoid division by zero.\n4. **Replace NaN and Infinite Values**: We replace any NaN or infinite values with 0 to ensure they do not affect the calculation.\n5. **Standardize the Data**: We apply the z-score formula to standardize each feature.\n6. **Round Results**: We round the standardized values to 4 decimal places.\n7. **Convert Back to List of Lists**: Finally, we convert the NumPy array back to a list of lists and return it.\n\nThis function ensures that the data is standardized according to the specified requirements."}
