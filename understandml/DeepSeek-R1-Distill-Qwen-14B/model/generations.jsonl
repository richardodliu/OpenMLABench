{"task_id": 0, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Ridge Regression using its closed-form solution. Ridge Regression is a technique used to prevent overfitting in linear regression models by adding a regularization term to the cost function. This regularization term helps to constrain the size of the coefficients, making the model more robust.\n\n### Approach\n1. **Data Separation**: Split the input data into features (X) and the target variable (y). The last column of the input array is the target variable, and all preceding columns are the features.\n2. **Add Bias Term**: Add a bias term (intercept) to the features matrix to account for the intercept in the linear regression model.\n3. **Compute Key Matrices**: Calculate the matrix product of the transpose of the features matrix with the features matrix itself (X^T X). Add the regularization term (alpha times the identity matrix) to this product to form the matrix for inversion.\n4. **Matrix Inversion**: Check if the matrix formed in the previous step is invertible. If it is not, return -1. If it is invertible, compute its inverse.\n5. **Weight Calculation**: Multiply the inverse matrix with the transpose of the features matrix and the target variable vector to get the weight vector.\n6. **Rounding**: Round the computed weights to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Implements Ridge (L2-regularized) Linear Regression using the closed-form solution.\n    \n    Given an ndarray where the last column is the target y and the preceding columns are the features X,\n    this function adds a bias term to X, computes the weight vector using the ridge regression formula,\n    and returns the weights rounded to 4 decimal places. If the matrix is not invertible, returns -1.\n    \n    Parameters:\n        data (np.ndarray): Input data array with the last column as target y.\n        alpha (float): Regularization strength, default is 0.0.\n    \n    Returns:\n        list[float]: Weight vector rounded to 4 decimal places, or -1 if the matrix is not invertible.\n    \"\"\"\n    # Split the data into features X and target y\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add a bias term to X\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Compute X^T X\n    XtX = X.T.dot(X)\n    \n    # Compute the matrix for inversion: X^T X + alpha * I\n    n = X.shape[1]\n    I = np.identity(n)\n    matrix = XtX + alpha * I\n    \n    # Check if the matrix is invertible\n    try:\n        inv_matrix = np.linalg.inv(matrix)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Compute X^T y\n    Xty = X.T.dot(y)\n    \n    # Compute the weight vector\n    w_hat = inv_matrix.dot(Xty)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_weights = np.round(w_hat, 4).tolist()\n    \n    return rounded_weights\n```\n\n### Explanation\n1. **Data Separation**: The input data is split into features (X) and the target variable (y). This is done using array slicing where the last column is taken as y and the rest as X.\n2. **Bias Term Addition**: A column of ones is added to X to account for the intercept in the linear regression model.\n3. **Matrix Computation**: The matrix product X^T X is computed, which is a key component in the closed-form solution for linear regression. The regularization term (alpha times the identity matrix) is added to this product to form the matrix for inversion.\n4. **Matrix Inversion**: The matrix is checked for invertibility. If it is not invertible, the function returns -1. If it is invertible, the inverse is computed.\n5. **Weight Calculation**: The weight vector is computed by multiplying the inverse matrix with X^T y.\n6. **Rounding**: The computed weights are rounded to four decimal places and returned as a list.\n\nThis approach ensures that we handle both the linear regression and regularization efficiently using matrix operations, providing a robust solution to prevent overfitting."}
{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the dual-form perceptron learning algorithm. This algorithm is used to learn a linear classifier of the form \\( f(x) = \\text{sign}(w \\cdot x + b) \\). The dual formulation expresses the weight vector \\( w \\) as a linear combination of training samples, which is a precursor to kernel methods.\n\n### Approach\nThe dual-form perceptron algorithm involves the following steps:\n\n1. **Initialization**: Start with zero dual parameters \\( \\alpha \\) and bias \\( b \\).\n2. **Iterative Learning**: For each sample, compute the activation and check if the sample is misclassified. If it is, update the dual parameters and bias.\n3. **Stopping Condition**: The algorithm stops when an entire pass over the data finishes with no updates or after a specified number of iterations to prevent infinite loops on inseparable data.\n4. **Weight Vector Calculation**: After training, compute the primal weight vector \\( w \\) from the final dual parameters \\( \\alpha \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or -1) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples = X_train.shape[0]\n    n_features = X_train.shape[1]\n    \n    w = np.zeros(n_features)\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    count = 0\n    \n    while count < n_iter:\n        updated = False\n        for i in range(n_samples):\n            activation = np.dot(w, X_train[i]) + b\n            margin = y_train[i] * activation\n            if margin <= 0:\n                alpha[i] += eta\n                w += eta * y_train[i] * X_train[i]\n                b += eta * y_train[i]\n                count += 1\n                updated = True\n                break\n        if not updated:\n            break\n    \n    rounded_w = [round(wi, 4) for wi in w]\n    rounded_b = round(b, 4)\n    \n    return (rounded_w, rounded_b)\n```\n\n### Explanation\n1. **Initialization**: The weight vector \\( w \\) and dual parameters \\( \\alpha \\) are initialized to zero vectors, and the bias \\( b \\) is initialized to zero.\n2. **Iterative Learning**: For each sample, the activation is computed as the dot product of the current weight vector \\( w \\) and the sample, plus the bias \\( b \\). If the margin (activation multiplied by the label) is non-positive, the sample is misclassified, and the dual parameters and bias are updated.\n3. **Stopping Condition**: The loop continues until either all samples are processed without updates (indicating convergence) or the maximum number of iterations is reached.\n4. **Weight Vector Calculation**: After training, the weight vector \\( w \\) is computed from the dual parameters \\( \\alpha \\) and the training samples. The results are rounded to four decimal places for precision.\n\nThis approach efficiently handles the learning process by maintaining and updating the weight vector incrementally, ensuring that the algorithm converges when the data is linearly separable."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of standard Generalized Linear Model (GLM) link functions. Each link function is used to transform the expected value of the response variable, \u03bc, to the linear predictor, \u03b7. The dictionary should include three common link functions: identity, log, and logit, each with their respective inverse and derivative functions.\n\n### Approach\n1. **Identity Link Function**: This function simply returns the input value as it is. The inverse function is the same as the link function, and the derivative is 1.\n2. **Log Link Function**: This function computes the natural logarithm of the input. The inverse function is the exponential function, and the derivative is the reciprocal of the input.\n3. **Logit Link Function**: This function computes the logarithm of the odds of the input. The inverse function is the logistic function, and the derivative is the reciprocal of the product of the input and its complement.\n\nEach link function, its inverse, and its derivative are implemented as numpy-aware callables to handle both scalar and array inputs efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: 1\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1.0 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1.0 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1.0 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\n- **Identity Link**: The identity function is straightforward, returning the input as is. Its inverse is the same function, and the derivative is always 1.\n- **Log Link**: The log function transforms the input using the natural logarithm. The inverse transformation is the exponential function, and the derivative is the reciprocal of the input.\n- **Logit Link**: The logit function transforms the input using the logarithm of the odds. The inverse transformation is the logistic function, which maps any real number to a value between 0 and 1. The derivative of the logit function is the reciprocal of the product of the input and its complement.\n\nThis implementation ensures that each function works efficiently with both scalar values and numpy arrays, leveraging element-wise operations for array inputs."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to build an isolation tree for a given dataset and compute the depth of the leaf node for each sample. The isolation tree is built by recursively partitioning the data with random splits, and the depth of each sample is the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Set Random Seed**: To ensure reproducibility, we set the random seed to 0 before any random operations.\n2. **Recursive Tree Construction**: We use a helper function to recursively build the tree. Each node processes a subset of indices and determines if it should be a leaf or split further.\n3. **Leaf Node Check**: If a node contains fewer than three samples, it becomes a leaf, and all samples in this node are assigned the current depth.\n4. **Splitting Nodes**: If a node has three or more samples, it splits the data by choosing a random feature, determining the range of values for that feature, and selecting a split value uniformly within this range.\n5. **Depth Calculation**: The helper function recursively processes the left and right children of each node, incrementing the depth for each level of recursion. The results from the left and right children are combined to form the final depth map.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    indices = list(range(n_samples))\n    \n    def helper(indices, current_depth):\n        if len(indices) < 3:\n            return {i: current_depth for i in indices}\n        else:\n            f = np.random.choice(len(data[0]))\n            feature_values = data[indices, f]\n            down = np.min(feature_values)\n            up = np.max(feature_values)\n            v = np.random.uniform(down, up)\n            left = []\n            right = []\n            for i in indices:\n                if data[i, f] <= v:\n                    left.append(i)\n                else:\n                    right.append(i)\n            left_depths = helper(left, current_depth + 1)\n            right_depths = helper(right, current_depth + 1)\n            combined = {}\n            combined.update(left_depths)\n            combined.update(right_depths)\n            return combined\n    \n    depths_dict = helper(indices, 0)\n    result = [0] * n_samples\n    for i in range(n_samples):\n        result[i] = depths_dict[i]\n    return result\n```\n\n### Explanation\n- **Random Seed**: The random seed is set to ensure that the results are reproducible.\n- **Helper Function**: The helper function recursively processes each node, determining if it should be a leaf or split further. It splits the data based on a randomly chosen feature and a uniformly selected split value.\n- **Depth Calculation**: The depth of each sample is tracked through recursive calls, with each level of recursion increasing the depth. The results are combined to form a dictionary mapping each sample index to its depth.\n- **Result Construction**: The final result is constructed by mapping each sample index to its computed depth, ensuring the output is in the order of the input data.\n\nThis approach efficiently builds the isolation tree and computes the required depths using recursion and random splitting, ensuring correctness and reproducibility."}
{"task_id": 7, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to identify all frequent itemsets in a given list of transactions. An itemset is considered frequent if its support (the number of transactions containing the itemset) meets or exceeds a specified minimum support threshold. The solution involves using the Apriori algorithm, which efficiently finds frequent itemsets by iteratively generating candidate itemsets and pruning those that do not meet the support threshold.\n\n### Approach\n1. **Calculate the Threshold**: Compute the minimum support threshold as the ceiling of the product of the minimum support fraction and the number of transactions.\n2. **Preprocess Transactions**: Convert each transaction into a set for efficient subset operations.\n3. **Generate Initial Candidates**: Start with single-item candidates and compute their support.\n4. **Iterative Candidate Generation**: For each level of candidate size, generate new candidates by combining existing frequent candidates. Prune candidates that do not meet the support threshold.\n5. **Store Results**: Collect all frequent itemsets and their support counts, ensuring each itemset is sorted and stored as a tuple.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    N = len(transactions)\n    if N == 0:\n        return {}\n    threshold = math.ceil(minsup * N)\n    if threshold == 0:\n        threshold = 1  # Ensure at least 1 if minsup is 0, though minsup >0 per problem statement\n\n    # Preprocess transactions into sets for faster subset checking\n    transactions_sets = [set(t) for t in transactions]\n\n    # Collect all unique items\n    all_items = set()\n    for t in transactions:\n        all_items.update(t)\n    all_items = sorted(all_items)\n\n    result = {}\n\n    # Generate initial candidates (1-itemsets)\n    current_candidates = [tuple([item]) for item in all_items]\n\n    while current_candidates:\n        # Compute support for each candidate\n        candidate_support = {}\n        for candidate in current_candidates:\n            count = 0\n            for t_set in transactions_sets:\n                if all(item in t_set for item in candidate):\n                    count += 1\n            if count >= threshold:\n                candidate_support[candidate] = count\n                # Add to result\n                result[candidate] = count\n\n        if not candidate_support:\n            break  # No more candidates to process\n\n        # Generate next level candidates\n        next_candidates = set()\n        k = len(current_candidates[0])\n        for c1 in candidate_support:\n            for c2 in candidate_support:\n                if c1 < c2:  # To avoid duplicates\n                    # Check if the first k-1 items are the same\n                    if c1[:-1] == c2[:-1]:\n                        # Union of the two candidates to form the next level\n                        new_candidate = tuple(sorted(c1 + c2))\n                        next_candidates.add(new_candidate)\n        current_candidates = list(next_candidates)\n\n    return result\n```\n\n### Explanation\n1. **Threshold Calculation**: The threshold is calculated using the ceiling of the product of the minimum support and the number of transactions.\n2. **Transaction Preprocessing**: Each transaction is converted into a set to facilitate efficient subset checking.\n3. **Initial Candidates**: Single-item candidates are generated from the unique items in the transactions.\n4. **Iterative Candidate Generation**: For each level, new candidates are generated by combining existing candidates. Only those candidates that meet the support threshold are kept for the next level.\n5. **Result Collection**: Each frequent itemset is stored as a sorted tuple with its support count.\n\nThis approach efficiently narrows down the candidate itemsets, ensuring that only those meeting the support threshold are considered, thus optimizing the process for larger datasets."}
{"task_id": 11, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm from scratch without relying on any external machine learning libraries. The goal is to cluster a given dataset into `k` groups based on the squared Euclidean distance, ensuring the algorithm stops when the maximum change in cluster centers is below a specified threshold or when the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start by selecting the first `k` samples of the input array as the initial cluster centers.\n2. **Assignment**: For each sample, compute the squared Euclidean distance to each cluster center and assign the sample to the nearest center.\n3. **Recomputation**: For each cluster, compute the new center as the arithmetic mean of all samples assigned to that cluster. If a cluster becomes empty, re-initialize its center with a random sample from the dataset.\n4. **Convergence Check**: Calculate the maximum change in the position of any cluster center between consecutive iterations. If this change is smaller than the specified threshold `epsilon`, the algorithm stops. If the maximum number of iterations is reached, the algorithm also stops.\n5. **Final Assignment**: After convergence, reassign each sample to the nearest cluster based on the final centers to ensure the cluster labels are correct.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    # Initialize the centers with the first k samples\n    centers = data[:k].copy()\n    \n    for iter_num in range(max_iter):\n        # Assign each sample to the nearest center\n        cluster_labels = []\n        for point in data:\n            distances = np.sum((point - centers) ** 2, axis=1)\n            cluster_idx = np.argmin(distances)\n            cluster_labels.append(cluster_idx)\n        cluster_labels = np.array(cluster_labels)\n        \n        # Recompute the centers\n        new_centers = []\n        for i in range(k):\n            points_in_cluster = data[cluster_labels == i]\n            if len(points_in_cluster) == 0:\n                # Re-initialize with a random sample\n                new_center = data[np.random.randint(len(data))]\n            else:\n                new_center = np.mean(points_in_cluster, axis=0)\n            new_centers.append(new_center)\n        \n        # Convert new_centers to a numpy array for easier handling\n        new_centers = np.array(new_centers)\n        \n        # Compute the maximum change in centers\n        max_change = 0.0\n        for i in range(k):\n            change = np.sum((centers[i] - new_centers[i]) ** 2)\n            if change > max_change:\n                max_change = change\n        \n        # Check if we should stop\n        if max_change < epsilon:\n            break\n        else:\n            centers = new_centers.copy()\n    \n    # After convergence, compute the final cluster labels\n    cluster_labels = []\n    for point in data:\n        distances = np.sum((point - centers) ** 2, axis=1)\n        cluster_idx = np.argmin(distances)\n        cluster_labels.append(cluster_idx)\n    cluster_labels = np.array(cluster_labels).tolist()\n    \n    # Round the centers to 4 decimal places\n    rounded_centers = [center.round(4).tolist() for center in centers]\n    \n    return (rounded_centers, cluster_labels)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first `k` samples of the input data.\n2. **Assignment**: Each sample is assigned to the nearest cluster center based on the squared Euclidean distance.\n3. **Recomputation**: Each cluster's center is updated to the mean of all samples assigned to it. If a cluster has no samples, it is re-initialized with a random sample.\n4. **Convergence Check**: The algorithm stops if the maximum change in cluster centers is below `epsilon` or if the maximum number of iterations is reached.\n5. **Final Assignment**: After convergence, each sample is reassigned to the nearest cluster based on the final centers to ensure accurate cluster labels.\n\nThis approach ensures that the algorithm efficiently clusters the data while handling edge cases such as empty clusters and ensuring convergence within the specified constraints."}
{"task_id": 13, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an item-based k-Nearest Neighbour (k-NN) collaborative filtering recommender system. This system will predict the most attractive items for an active user based on their past ratings and the ratings of similar users.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Build an Item-Item Similarity Matrix**: For each pair of items, compute their similarity based on the ratings given by users who have rated both items. The similarity can be computed using either the cosine similarity or Pearson correlation, depending on the specified criterion.\n\n2. **Identify Rated and Unrated Items**: For the active user, identify the items they have rated and those they have not rated.\n\n3. **Compute Predicted Scores for Unrated Items**: For each unrated item, compute a predicted score based on the similarity of the unrated item with the items the user has rated. The score is calculated using the sum of the products of the user's ratings and the similarity scores, divided by the sum of the absolute similarity scores.\n\n4. **Sort and Select Top k Items**: Sort the unrated items by their predicted scores in descending order. If two items have the same score, the item with the smaller index comes first. Return the top k items.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_user, n_item = data.shape\n    if n_item == 0:\n        return []\n    \n    # Precompute users per item\n    users_per_item = {item: set() for item in range(n_item)}\n    for user in range(n_user):\n        for item in range(n_item):\n            if data[user, item] > 0:\n                users_per_item[item].add(user)\n    \n    # Build the item-item similarity matrix S\n    S = np.zeros((n_item, n_item), dtype=np.float32)\n    for i in range(n_item):\n        for j in range(i + 1, n_item):\n            common_users = users_per_item[i].intersection(users_per_item[j])\n            if not common_users:\n                S[i, j] = 0.0\n                S[j, i] = 0.0\n                continue\n            \n            v1 = np.array([data[u, i] for u in common_users])\n            v2 = np.array([data[u, j] for u in common_users])\n            \n            if criterion == 'cosine':\n                for v in [v1, v2]:\n                    if np.std(v) > 1e-3:\n                        v -= np.mean(v)\n            elif criterion == 'pearson':\n                v1 = v1 - np.mean(v1)\n                v2 = v2 - np.mean(v2)\n            else:\n                raise ValueError(\"Invalid criterion\")\n            \n            numerator = np.dot(v1, v2)\n            denominator = np.linalg.norm(v1) * np.linalg.norm(v2)\n            if denominator == 0:\n                similarity = 0.0\n            else:\n                similarity = numerator / denominator\n            \n            S[i, j] = similarity\n            S[j, i] = similarity\n    \n    # Get rated and unrated items for the active user\n    rated_items = [item for item in range(n_item) if data[user_ind, item] > 0]\n    unrated = [item for item in range(n_item) if data[user_ind, item] == 0]\n    \n    if not unrated:\n        return []\n    \n    # Extract the ratings for the rated items\n    r = data[user_ind, rated_items]\n    \n    # Compute scores for each unrated item\n    scores = {}\n    for t in unrated:\n        S_t = S[t, :]\n        S_t_rated = S_t[rated_items]\n        sum_num = np.dot(S_t_rated, r)\n        sum_den = np.sum(np.abs(S_t_rated))\n        if sum_den == 0:\n            score = 0.0\n        else:\n            score = sum_num / sum_den\n        scores[t] = score\n    \n    # Sort the unrated items by score and then by index\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    \n    # Extract the top k items\n    result = [t for t, score in sorted_items[:k]]\n    \n    return result\n```\n\n### Explanation\n1. **Building the Similarity Matrix**: The code first constructs a matrix where each element represents the similarity between two items. This is done by finding common users who have rated both items and computing the similarity based on their ratings.\n\n2. **Identifying Rated and Unrated Items**: For the active user, the code identifies which items they have rated and which they have not.\n\n3. **Computing Predicted Scores**: For each unrated item, the code calculates a score based on the similarity of the item with the items the user has rated. This score is used to predict how attractive the unrated item is to the user.\n\n4. **Sorting and Selecting Top k Items**: The unrated items are sorted by their predicted scores, and the top k items are selected and returned.\n\nThis approach ensures that the recommendations are both personalized and efficient, leveraging collaborative filtering to provide meaningful suggestions based on user behavior."}
{"task_id": 19, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the best split for a gradient-boosting tree node by maximizing the gain. The gain is calculated based on the reduction in the regularized loss function when splitting the node into two children. The goal is to find the feature and threshold that yield the highest positive gain.\n\n### Approach\n1. **Problem Analysis**: The task involves evaluating each feature's unique values as potential thresholds to split the data. For each possible split, we compute the gain, which is the reduction in the loss function. If no split yields a positive gain, we return `None`.\n\n2. **Intuition**: The gain is determined by the difference in the loss of the parent node and the sum of the losses of the left and right children. A positive gain indicates that the split reduces the overall loss.\n\n3. **Algorithm**:\n   - For each feature, sort the data based on the feature values.\n   - Compute prefix sums for the first-order (gradient) and second-order (Hessian) statistics to efficiently calculate the sums for potential splits.\n   - For each unique value of the feature, consider it as a potential threshold and compute the sums for the left and right children.\n   - Check if the split is valid (each child has at least two samples).\n   - Calculate the gain for each valid split and keep track of the maximum gain and corresponding feature and threshold.\n\n4. **Complexity Analysis**: The approach involves sorting the data for each feature, which takes O(n log n) time. For each unique value, a binary search is performed, which takes O(log n) time. Thus, the overall complexity is O(n log n) per feature, which is efficient for the given problem constraints.\n\n### Solution Code\n```python\nimport numpy as np\nimport bisect\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    total_g = np.sum(g)\n    total_h = np.sum(h)\n    current_max_gain = -float('inf')\n    best_feature = -1\n    best_threshold = None\n\n    for j in range(n_features):\n        feature_values = X[:, j]\n        # Create a list of tuples (feature_value, g_i, h_i) for each sample\n        sorted_samples = sorted(zip(feature_values, g, h), key=lambda x: x[0])\n        feature_values_sorted = [x[0] for x in sorted_samples]\n        g_sorted = [x[1] for x in sorted_samples]\n        h_sorted = [x[2] for x in sorted_samples]\n\n        # Compute prefix sums for g and h\n        prefix_g = [0.0] * (n_samples + 1)\n        prefix_h = [0.0] * (n_samples + 1)\n        for i in range(n_samples):\n            prefix_g[i + 1] = prefix_g[i] + g_sorted[i]\n            prefix_h[i + 1] = prefix_h[i] + h_sorted[i]\n\n        # Get unique values of feature j\n        unique_vals = np.sort(np.unique(feature_values))\n\n        for v in unique_vals:\n            # Find the index where feature_values_sorted <= v\n            k = bisect.bisect_right(feature_values_sorted, v)\n            count_left = k\n            if count_left < 2 or (n_samples - count_left) < 2:\n                continue\n\n            sum_g_left = prefix_g[k]\n            sum_h_left = prefix_h[k]\n\n            sum_g_right = total_g - sum_g_left\n            sum_h_right = total_h - sum_h_left\n\n            # Compute L_parent\n            denominator_parent = total_h + lam\n            L_parent = -0.5 * (total_g ** 2) / denominator_parent + gamma\n\n            # Compute L_left\n            denominator_left = sum_h_left + lam\n            L_left = -0.5 * (sum_g_left ** 2) / denominator_left + gamma\n\n            # Compute L_right\n            denominator_right = sum_h_right + lam\n            L_right = -0.5 * (sum_g_right ** 2) / denominator_right + gamma\n\n            gain = L_parent - (L_left + L_right)\n\n            if gain > current_max_gain:\n                current_max_gain = gain\n                best_feature = j\n                best_threshold = v\n\n    if current_max_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None\n```\n\n### Explanation\nThe function `best_split` iterates over each feature and each unique value of that feature to evaluate potential splits. For each split, it calculates the gain by comparing the loss of the parent node with the combined loss of the left and right children. The function keeps track of the maximum gain and returns the corresponding feature and threshold if a valid split is found. If no valid split yields a positive gain, it returns `None`. This approach ensures that the best possible split is chosen to minimize the loss in a gradient-boosting tree."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation and its gradient for a given input. The input can be a scalar, a list, or a NumPy array. The function should return a tuple containing the sigmoid values and their gradients, both rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should handle different types of input, including integers, floats, lists, and NumPy arrays. If the input is a scalar (int, float, or 0D NumPy array), the function processes it as a single value. For array-like inputs (lists or higher-dimensional NumPy arrays), the function processes each element individually.\n2. **Conversion to NumPy Array**: For array-like inputs, the function converts the input to a NumPy array to facilitate element-wise operations.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Gradient Calculation**: The gradient of the sigmoid function is computed using the formula \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n5. **Rounding**: Both the sigmoid values and their gradients are rounded to four decimal places.\n6. **Return Type**: If the input is a scalar, the function returns two floats. If the input is an array-like structure, the function returns two lists of the same shape, each element rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Handle scalar case, including 0D arrays\n        if isinstance(x, (int, float)):\n            x_scalar = x\n        else:\n            x_scalar = x.item()  # Extract scalar from 0D array\n        sigmoid = 1 / (1 + np.exp(-x_scalar))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Handle array-like case\n        if isinstance(x, list):\n            x_array = np.array(x)\n        else:\n            x_array = x\n        sigmoid = 1 / (1 + np.exp(-x_array))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = np.round(sigmoid, 4)\n        gradient_rounded = np.round(gradient, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a scalar or a 0D array. If so, it processes the input as a single value. Otherwise, it converts the input to a NumPy array.\n- **Sigmoid Calculation**: Using NumPy's exponential function, the sigmoid values are computed element-wise.\n- **Gradient Calculation**: The gradient is derived from the sigmoid function and computed using the derived formula.\n- **Rounding**: Both the sigmoid values and gradients are rounded to four decimal places using NumPy's rounding function.\n- **Return Type**: Depending on whether the input was a scalar or an array-like structure, the function returns the results as floats or lists, respectively.\n\nThis approach ensures that the function works efficiently for various input types and returns the results in the required format."}
{"task_id": 21, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using one-dimensional decision stumps as weak classifiers. The goal is to train an ensemble of these weak classifiers to achieve a desired training error, and then use this ensemble to make predictions on a test set.\n\n### Approach\n1. **Sort the Training Data**: First, we sort the training data based on the feature values. This helps in generating the decision thresholds efficiently.\n2. **Generate Decision Stumps**: For each consecutive pair of feature values in the sorted data, we generate two possible decision stumps. Each stump is defined by a threshold and a direction (d=0 or d=1).\n3. **Initialize Weights**: We start with uniform weights for all training samples.\n4. **Iterative Training**: In each iteration, we:\n   - Compute the weighted error for each possible decision stump.\n   - Select the stump with the minimum weighted error.\n   - Compute the coefficient (alpha) for this stump using the formula: \u03b1 = \u00bd\u00b7ln((1\u2013err)/err).\n   - Update the sample weights based on the selected stump.\n   - Check the training error of the current ensemble. If it is within the allowed error (epsilon), we stop training.\n5. **Make Predictions**: After training, we use the ensemble of stumps to make predictions on the test set by combining their decisions weighted by their coefficients.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Sort the training data\n    sorted_data = sorted(zip(x_train, y_train), key=lambda x: x[0])\n    x_train_sorted = [x for x, y in sorted_data]\n    y_train_sorted = [y for x, y in sorted_data]\n    n = len(sorted_data)\n    \n    # Handle the case where there are no samples\n    if n == 0:\n        return []\n    \n    # Generate all possible stumps\n    stumps = []\n    for i in range(n - 1):\n        x_i = sorted_data[i][0]\n        x_j = sorted_data[i + 1][0]\n        theta = (x_i + x_j) / 2\n        stumps.append((0, theta))\n        stumps.append((1, theta))\n    \n    # Compute initial training error\n    initial_error = sum(1 for y in y_train_sorted if y == -1) / n\n    if initial_error <= epsilon:\n        return [1] * len(x_test)\n    \n    # Initialize weights\n    current_weights = [1.0 / n] * n\n    ensemble = []  # List to hold (d, theta, alpha)\n    \n    while True:\n        # Compute errors for all stumps\n        errors = []\n        for (d, theta) in stumps:\n            error = 0.0\n            for i in range(n):\n                x_i, y_i = sorted_data[i]\n                # Compute h_i\n                if d == 0:\n                    h_i = 1 if x_i <= theta else -1\n                else:\n                    h_i = 1 if x_i > theta else -1\n                if h_i != y_i:\n                    error += current_weights[i]\n            errors.append((error, (d, theta)))\n        \n        # Find the best stump with minimal error\n        min_error = min([e for e, s in errors])\n        # Find all stumps with this error\n        candidates = [s for e, s in errors if e == min_error]\n        # Select the first one (any)\n        best_stump = candidates[0]\n        \n        # Compute alpha\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n        \n        # Update weights\n        new_weights = []\n        for i in range(n):\n            x_i, y_i = sorted_data[i]\n            d, theta = best_stump\n            # Compute h_i\n            if d == 0:\n                h_i = 1 if x_i <= theta else -1\n            else:\n                h_i = 1 if x_i > theta else -1\n            if h_i == y_i:\n                new_w = current_weights[i] * math.exp(-alpha)\n            else:\n                new_w = current_weights[i] * math.exp(alpha)\n            new_weights.append(new_w)\n        \n        # Normalize the weights\n        sum_weights = sum(new_weights)\n        current_weights = [w / sum_weights for w in new_weights]\n        \n        # Add the best stump to the ensemble\n        ensemble.append((best_stump[0], best_stump[1], alpha))\n        \n        # Compute the current training error\n        training_error = 0.0\n        for i in range(n):\n            x_i, y_i = sorted_data[i]\n            sum_h = 0.0\n            for (d, theta, alpha) in ensemble:\n                if d == 0:\n                    h_i = 1 if x_i <= theta else -1\n                else:\n                    h_i = 1 if x_i > theta else -1\n                sum_h += alpha * h_i\n            if np.sign(sum_h) != y_i:\n                training_error += 1\n        training_error /= n\n        \n        if training_error <= epsilon:\n            break\n    \n    # Make predictions for the test set\n    predictions = []\n    for x in x_test:\n        sum_h = 0.0\n        for (d, theta, alpha) in ensemble:\n            if d == 0:\n                h = 1 if x <= theta else -1\n            else:\n                h = 1 if x > theta else -1\n            sum_h += alpha * h\n        if sum_h >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Sorting and Stump Generation**: The training data is sorted, and all possible decision stumps are generated based on the midpoints between consecutive feature values.\n2. **Weight Initialization**: Each sample starts with equal weight.\n3. **Iterative Training**: In each iteration, the algorithm evaluates all possible stumps, selects the one with the minimum error, updates the weights, and checks the training error. This process continues until the training error is within the specified bound.\n4. **Prediction**: The trained ensemble of stumps is used to make predictions on the test set by combining their weighted decisions.\n\nThis approach ensures that we efficiently train an ensemble of weak classifiers to achieve the desired performance, leveraging the power of AdaBoost to iteratively improve the model."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian kernel. The SVM decision function is given, and we need to compute the predicted labels efficiently using vectorized operations.\n\n### Approach\n1. **Compute Kernel Matrix**: For each test sample, compute the Gaussian kernel values with all training samples. This is done efficiently using vectorized operations to avoid loops.\n2. **Sum Contributions**: Multiply each kernel value by the corresponding Lagrange multiplier and class label, then sum these contributions for each test sample.\n3. **Add Bias Term**: Add the bias term to the computed sum for each test sample.\n4. **Determine Predictions**: Use the sign of the resulting values to determine the predicted class labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Reshape the training and test data for broadcasting\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    \n    X_train_reshaped = X_train[:, np.newaxis, :]  # Shape: (n, 1, d)\n    X_test_reshaped = X_test[np.newaxis, :, :]    # Shape: (1, m, d)\n    \n    # Compute the squared differences\n    diff = X_test_reshaped - X_train_reshaped      # Shape: (n, m, d)\n    squared_norms = np.sum(diff ** 2, axis=2)     # Shape: (n, m)\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * squared_norms)            # Shape: (n, m)\n    \n    # Compute the sum of alpha_i * y_i * K[i,j] for each test sample j\n    sum_values = (alpha * y_train).dot(K.T)       # Shape: (m,)\n    \n    # Add the bias term\n    g_z = sum_values + b\n    \n    # Determine the predicted labels\n    predictions = np.where(g_z >= 0, 1, -1)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Reshape Data**: The training and test data are reshaped to facilitate broadcasting, allowing efficient computation of pairwise differences.\n2. **Compute Differences**: The differences between each test sample and each training sample are computed, and the squared Euclidean norms are summed.\n3. **Gaussian Kernel**: The Gaussian kernel values are computed using the squared norms.\n4. **Sum Contributions**: The contributions from each training sample are summed for each test sample using vectorized operations.\n5. **Bias and Sign**: The bias term is added, and the sign of the result determines the predicted class label.\n\nThis approach ensures that the solution is efficient and scalable, handling large datasets using vectorized operations without relying on external libraries."}
{"task_id": 28, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the optimal rank-\\(k\\) reconstruction of a given matrix \\(X\\) using truncated Singular Value Decomposition (SVD). This is equivalent to the best linear autoencoder with \\(k\\) latent units. The goal is to reconstruct the matrix \\(X\\) and compute the mean squared error (MSE) of the reconstruction.\n\n### Approach\n1. **Check Validity of \\(k\\)**: First, we check if \\(k\\) is within the valid range, which is from 1 to \\(\\min(m, n)\\), where \\(m\\) is the number of rows and \\(n\\) is the number of columns in \\(X\\). If \\(k\\) is invalid, we return -1.\n2. **Convert Input to Numpy Array**: Convert the input matrix \\(X\\) into a numpy array for efficient numerical operations.\n3. **Compute SVD**: Perform the Singular Value Decomposition (SVD) on the matrix \\(X\\) to obtain the matrices \\(U\\), \\(S\\), and \\(V^H\\) (where \\(V^H\\) is the conjugate transpose of \\(V\\)).\n4. **Truncate SVD Components**: Retain only the top \\(k\\) components from \\(U\\), \\(S\\), and \\(V^H\\).\n5. **Reconstruct Matrix**: Use the truncated components to reconstruct the matrix \\(\\hat{X}\\) as \\(U_{\\text{truncated}} \\times S_{\\text{truncated}} \\times V_{\\text{truncated}}^H\\).\n6. **Compute MSE**: Calculate the mean squared error between the original matrix \\(X\\) and the reconstructed matrix \\(\\hat{X}\\).\n7. **Round Results**: Round the reconstructed matrix and the MSE to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    m = len(X)\n    if m == 0:\n        return -1\n    n = len(X[0]) if m > 0 else 0\n    if k < 1 or k > min(m, n):\n        return -1\n    \n    X_np = np.array(X)\n    U, S, Vh = np.linalg.svd(X_np, full_matrices=False)\n    \n    U_truncated = U[:, :k]\n    S_truncated = S[:k]\n    Vh_truncated = Vh[:k, :]\n    \n    diag_S = np.diag(S_truncated)\n    X_hat_np = U_truncated @ diag_S @ Vh_truncated\n    \n    X_hat = np.round(X_hat_np, 4)\n    X_hat_list = X_hat.tolist()\n    \n    error = X_np - X_hat_np\n    squared_error = error ** 2\n    mse = np.mean(squared_error)\n    mse_rounded = np.round(mse, 4)\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if \\(k\\) is within the valid range. If not, it returns -1.\n2. **Matrix Conversion**: The input matrix \\(X\\) is converted into a numpy array to leverage efficient numerical operations.\n3. **SVD Computation**: The SVD of \\(X\\) is computed, decomposing it into \\(U\\), \\(S\\), and \\(V^H\\).\n4. **Truncation**: Only the top \\(k\\) components from each of \\(U\\), \\(S\\), and \\(V^H\\) are retained.\n5. **Reconstruction**: The matrix \\(\\hat{X}\\) is reconstructed using the truncated components.\n6. **MSE Calculation**: The mean squared error between the original and reconstructed matrices is computed.\n7. **Rounding**: The reconstructed matrix and MSE are rounded to four decimal places for the final result.\n\nThis approach ensures that we efficiently compute the optimal rank-\\(k\\) reconstruction using truncated SVD, providing both the reconstructed matrix and the MSE."}
{"task_id": 29, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor that uses decision stumps as weak learners. The goal is to predict the target value for a given query point based on the training data.\n\n### Approach\n1. **Sort the Training Data**: The training data is sorted by the feature values to facilitate the identification of candidate split points.\n2. **Generate Candidate Splits**: Midpoints between consecutive feature values are considered as potential split points.\n3. **Iterative Learning**: The algorithm iteratively adds the best decision stump that minimizes the residual sum of squares (RSS). This continues until the RSS is below a specified tolerance.\n4. **Update Residuals**: After adding each decision stump, the residuals are updated to reflect the new errors.\n5. **Prediction**: For a given query point, the prediction is the sum of the contributions from all learned stumps.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert to numpy arrays for easier handling\n    x_train = np.asarray(x_train)\n    y_train = np.asarray(y_train)\n    \n    # Sort the data by x\n    sorted_data = sorted(zip(x_train, y_train), key=lambda pair: pair[0])\n    x_sorted = np.array([pair[0] for pair in sorted_data])\n    y_sorted = np.array([pair[1] for pair in sorted_data])\n    \n    # Generate candidate split points\n    n = len(x_sorted)\n    if n == 0:\n        # No data, return 0.0\n        return 0.0\n    candidates = []\n    for i in range(n-1):\n        s = (x_sorted[i] + x_sorted[i+1]) / 2\n        candidates.append(s)\n    \n    # Initialize residuals\n    residuals = y_sorted.copy()\n    \n    # List to hold the stumps\n    stumps = []\n    \n    while True:\n        # Compute current RSS\n        current_rss = np.sum(residuals**2)\n        if current_rss <= epsilon:\n            break\n        \n        best_new_rss = float('inf')\n        best_s = None\n        best_c1 = 0.0\n        best_c2 = 0.0\n        \n        for s in candidates:\n            # Split the data\n            left = x_sorted <= s\n            right = x_sorted > s\n            \n            # Check if either group is empty\n            if np.sum(left) == 0 or np.sum(right) == 0:\n                continue\n            \n            # Compute c1 and c2\n            c1 = np.mean(residuals[left])\n            c2 = np.mean(residuals[right])\n            \n            # Compute new RSS\n            rss_left = np.sum( (residuals[left] - c1)**2 )\n            rss_right = np.sum( (residuals[right] - c2)**2 )\n            total_rss = rss_left + rss_right\n            \n            # Update best\n            if total_rss < best_new_rss:\n                best_new_rss = total_rss\n                best_s = s\n                best_c1 = c1\n                best_c2 = c2\n        \n        # If no valid split found, break\n        if best_s is None:\n            break\n        \n        # Add the best stump to the list\n        stumps.append( (best_s, best_c1, best_c2) )\n        \n        # Update residuals\n        mask_left = x_sorted <= best_s\n        mask_right = x_sorted > best_s\n        residuals[mask_left] -= best_c1\n        residuals[mask_right] -= best_c2\n    \n    # Make the prediction\n    prediction = 0.0\n    for s, c1, c2 in stumps:\n        if x_query <= s:\n            prediction += c1\n        else:\n            prediction += c2\n    \n    # Round to 4 decimal places\n    return round(prediction, 4)\n```\n\n### Explanation\n1. **Sorting and Preparation**: The input data is sorted by the feature values, and candidate split points are generated as midpoints between consecutive feature values.\n2. **Residual Calculation**: The initial residuals are set to the target values. In each iteration, the algorithm evaluates each candidate split point to find the one that minimizes the RSS.\n3. **Stump Addition**: The best split point is added to the ensemble, and the residuals are updated accordingly.\n4. **Prediction**: For the query point, the prediction is computed by summing the contributions from all learned stumps based on whether the query point falls to the left or right of each split.\n\nThis approach ensures that the model iteratively improves its predictions by focusing on the most significant splits, leading to a robust and accurate prediction for the query point."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a NumPy tensor filled with 0.0s and 1.0s based on the specified shape, sparsity, and an optional seed for reproducibility. The sparsity parameter determines the probability of an element being 1.0, and the seed ensures that the results are reproducible.\n\n### Approach\n1. **Check Sparsity Validity**: The sparsity value must be within the range [0, 1]. If it is outside this range, the function returns -1.\n2. **Set Seed**: If a seed is provided, it is used to initialize the random number generator to ensure reproducibility.\n3. **Generate Random Tensor**: A tensor of the specified shape is generated with random values between 0 and 1.\n4. **Convert to Binary Tensor**: Each element in the generated tensor is compared against a threshold (1 - sparsity). If an element is greater than or equal to this threshold, it is set to 1.0; otherwise, it is set to 0.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray:\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: The shape of the tensor to be generated.\n        sparsity: The probability of an element being 1.0. Must be in [0, 1].\n        seed: Optional seed for random number generation.\n        \n    Returns:\n        A numpy.ndarray containing 0.0s and 1.0s, or -1 if sparsity is invalid.\n    \"\"\"\n    if sparsity < 0 or sparsity > 1:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_tensor = np.random.rand(*shape)\n    binary_tensor = (random_tensor >= (1 - sparsity)).astype(float)\n    return binary_tensor\n```\n\n### Explanation\n- **Sparsity Check**: The function first checks if the sparsity value is within the valid range [0, 1]. If not, it returns -1.\n- **Seed Setting**: If a seed is provided, it is used to initialize the random number generator, ensuring that the results are reproducible.\n- **Random Tensor Generation**: A tensor of the specified shape is generated using `np.random.rand()`, which fills the tensor with random values between 0 and 1.\n- **Binary Conversion**: Each element in the generated tensor is compared against the threshold (1 - sparsity). Elements greater than or equal to this threshold are set to 1.0, and others are set to 0.0, resulting in a binary tensor.\n\nThis approach ensures that the generated tensor meets the specified sparsity requirement and is reproducible when a seed is provided."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs a median split on a dataset using NumPy operations. The goal is to split the dataset along a specified column such that the median value is used as the pivot, and the dataset is divided into two parts: one with values strictly less than the median and the other with values strictly greater than the median.\n\n### Approach\n1. **Extract the Column**: Extract the column from the dataset that we will use for the split.\n2. **Find the Median**: Determine the median value in the extracted column. For an even number of samples, the median is the value at the position `n//2`.\n3. **Partition the Data**: Use NumPy's `argpartition` to efficiently find the median without sorting the entire column. This operation rearranges the data such that the median is at the correct position.\n4. **Identify Pivot**: The pivot is the row index of the median value in the original dataset.\n5. **Split the Data**: Create two lists of indices: one for values strictly less than the median and another for values strictly greater than the median.\n6. **Sort the Indices**: Ensure the resulting lists of indices are sorted to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    vals = data[:, d]\n    n = len(vals)\n    m = n // 2\n    indices = np.argpartition(vals, m)\n    pivot = indices[m]\n    median_val = vals[pivot]\n    left_mask = vals < median_val\n    right_mask = vals > median_val\n    left = np.where(left_mask)[0].tolist()\n    right = np.where(right_mask)[0].tolist()\n    left.sort()\n    right.sort()\n    return (pivot, left, right)\n```\n\n### Explanation\n1. **Extract the Column**: The column `d` is extracted from the dataset to form an array `vals`.\n2. **Find the Median**: The median position `m` is calculated as `n // 2`, where `n` is the number of samples.\n3. **Partition the Data**: Using `np.argpartition`, the array `vals` is partitioned such that the element at position `m` becomes the median. This operation is efficient and runs in linear time.\n4. **Identify Pivot**: The pivot is the index of the median value in the original dataset, obtained from the partitioned indices.\n5. **Split the Data**: Two boolean masks are created to identify indices where values are less than and greater than the median. These masks are used to extract the corresponding row indices.\n6. **Sort the Indices**: The extracted indices are sorted to ensure the output is deterministic and meets the problem requirements.\n\nThis approach ensures that the split is performed efficiently in linear time, and the resulting lists of indices are sorted as required."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that breaks a one-dimensional signal into overlapping frames using NumPy. The function should return a view on the original signal, not a copy, and handle various edge cases and pre-conditions.\n\n### Approach\n1. **Pre-Conditions Check**: Ensure the input signal is one-dimensional, the stride is positive, the frame width is positive, and the signal length is at least as long as the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Create View Using Stride Tricks**: Use `numpy.lib.stride_tricks.as_strided` to create a view of the original array with the desired shape and strides. The strides are calculated based on the frame width and stride.\n4. **Set Writeable Flag**: Adjust the writeable flag of the result based on the input parameter to ensure the result is either read-only or inherits the writeability of the original array.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    assert x.ndim == 1, \"x must be 1D\"\n    assert stride >= 1, \"stride must be positive\"\n    assert frame_width >= 1, \"frame_width must be positive\"\n    n = x.size\n    assert n >= frame_width, \"x must be at least frame_width long\"\n\n    # Calculate number of frames\n    n_frames = (n - frame_width) // stride + 1\n\n    # Calculate shape and strides\n    shape = (n_frames, frame_width)\n    strides = (stride * x.itemsize, x.itemsize)\n\n    # Create the view using as_strided\n    result = as_strided(x, shape=shape, strides=strides)\n\n    # Set writeable flag\n    if writeable:\n        result.flags.writeable = x.flags.writeable\n    else:\n        result.flags.writeable = False\n\n    return result\n```\n\n### Explanation\n1. **Pre-Conditions Check**: The function starts by verifying that the input signal is one-dimensional, the stride and frame width are positive, and the signal length is sufficient to form at least one frame.\n2. **Number of Frames Calculation**: The number of frames is computed using integer division to ensure we only include complete frames.\n3. **View Creation**: Using `as_strided`, the function creates a view of the original array with the specified shape and strides. This ensures the result is a view rather than a copy, optimizing memory usage.\n4. **Writeable Flag Adjustment**: The writeable flag of the result is set based on the input parameter, ensuring the result is either read-only or writeable as required.\n\nThis approach efficiently handles the problem requirements and constraints, providing a solution that is both correct and optimized for performance."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given matrix is row-stochastic. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\n1. **Input Validation**: Check if the input is a list of lists or a numpy array. If it's a list, ensure all elements are lists and all rows have the same length. If it's a numpy array, ensure it is two-dimensional.\n2. **Element Check**: Verify that every element in the matrix is between 0 and 1 inclusive.\n3. **Row Sum Check**: Ensure that each row sums to 1 within a numerical tolerance of \\(1 \\times 10^{-8}\\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Check if X is a list of lists or a numpy array\n    if isinstance(X, list):\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Check if X is empty\n        if not X:\n            return False\n        # Check if all rows have the same length\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        # Convert to numpy array\n        X_np = np.array(X)\n    else:\n        # Check if it's a numpy array and 2D\n        if X.ndim != 2:\n            return False\n        X_np = X\n\n    # Check if all elements are between 0 and 1\n    if not ((X_np >= 0).all() and (X_np <= 1).all()):\n        return False\n\n    # Check if each row sums to 1 within a tolerance\n    row_sums = np.sum(X_np, axis=1)\n    if not np.allclose(row_sums, 1, atol=1e-8):\n        return False\n\n    # All conditions met\n    return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a list of lists or a numpy array. For a list, it ensures all elements are lists and each row has the same length. For a numpy array, it checks if it is two-dimensional.\n2. **Element Check**: The function converts the input to a numpy array and checks if all elements are between 0 and 1.\n3. **Row Sum Check**: The function calculates the sum of each row and checks if each sum is approximately 1 within a tolerance of \\(1 \\times 10^{-8}\\) using `numpy.allclose`.\n\nThis approach ensures that the matrix meets all the criteria for being row-stochastic efficiently and correctly."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least-squares (OLS) linear regression. The goal is to find a weight vector that minimizes the mean-squared error using the RMSprop update rule.\n\n### Approach\n1. **Problem Understanding**: The task is to minimize the mean-squared error using the RMSprop algorithm. The RMSprop algorithm adapts the learning rate for each parameter based on the magnitude of the gradient, which helps in efficient optimization.\n\n2. **RMSprop Update Rule**: The algorithm maintains a running average of the squared gradients (s) and updates the weights (w) using the formula:\n   - \\( s \\leftarrow \\rho \\cdot s + (1 - \\rho) \\cdot g^2 \\)\n   - \\( w \\leftarrow w - \\eta \\cdot g / (\\sqrt{s} + \\epsilon) \\)\n   where \\( g \\) is the gradient, \\( \\rho \\) is the decay rate, \\( \\eta \\) is the learning rate, and \\( \\epsilon \\) is a small constant to avoid division by zero.\n\n3. **Batch Processing**: The data is processed in mini-batches. If the number of samples is less than the batch size, the entire dataset is used as one batch. Otherwise, mini-batches are created by slicing successive blocks of rows, wrapping around when the end of the matrix is reached.\n\n4. **Stopping Condition**: The optimization stops early when the L2-norm of the gradient becomes smaller than a specified threshold (epsilon) or when the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n = X.shape[0]\n    d = X.shape[1]\n    w = np.zeros((d, 1))\n    s = np.zeros((d, 1))\n    current_index = 0\n    \n    for iter in range(max_iter):\n        # Compute batch indices\n        indices = (current_index + np.arange(batch_size)) % n\n        X_batch = X[indices]\n        y_batch = y[indices]\n        \n        # Compute gradient for the batch\n        Xw = np.dot(X_batch, w)\n        diff = Xw - y_batch\n        X_t_diff = np.dot(X_batch.T, diff)\n        g = X_t_diff / batch_size\n        \n        # Update s\n        s = rho * s + (1 - rho) * (g ** 2)\n        \n        # Compute step\n        step = eta * g / (np.sqrt(s) + eps_station)\n        \n        # Update weights\n        w = w - step\n        \n        # Check stopping condition\n        norm_g = np.linalg.norm(g)\n        if norm_g < epsilon:\n            break\n        \n        # Update current index\n        current_index += batch_size\n        if current_index >= n:\n            current_index %= n\n    \n    # Round the weights to four decimal places\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The weight vector (w) and the running average of squared gradients (s) are initialized to zero vectors. The current index is initialized to start from the beginning of the dataset.\n\n2. **Batch Processing**: For each iteration, a mini-batch of data is selected. If the current index exceeds the number of samples, it wraps around to the beginning.\n\n3. **Gradient Calculation**: The gradient for the current batch is computed using matrix multiplication and element-wise operations. This gradient is an average over the batch.\n\n4. **Update Rule**: The running average of squared gradients (s) is updated, followed by the computation of the step size for updating the weights. The weights are updated using the computed step.\n\n5. **Stopping Condition**: The optimization stops early if the norm of the gradient falls below the specified threshold or the maximum number of iterations is reached.\n\n6. **Result**: The final weight vector is rounded to four decimal places and returned as a list.\n\nThis approach efficiently minimizes the mean-squared error using the RMSprop algorithm, handling both full-batch and mini-batch scenarios while ensuring numerical stability."}
{"task_id": 56, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the SoftPlus activation function and its derivative in Python. The SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\), and its derivative is the sigmoid function \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). The implementation must handle scalar, list, and NumPy array inputs, ensure numerical stability, and return results rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Convert the input to a NumPy array to handle element-wise operations uniformly. If the input is a scalar (including a 0D NumPy array), reshape it to a 1D array for processing.\n2. **Numerical Stability**: For very large positive values of \\( x \\), compute \\( \\text{SoftPlus}(x) \\) directly as \\( x \\) to avoid overflow. For other values, use \\( \\log(1 + e^x) \\) computed using `np.log1p` for numerical stability.\n3. **Derivative Handling**: When the derivative flag is set, compute the sigmoid function \\( \\sigma(x) \\) as \\( 1 / (1 + e^{-x}) \\).\n4. **Rounding**: Round the results to four decimal places.\n5. **Output Handling**: Return the result as a Python list if the input was a list or higher-dimensional array, or as a float if the input was a scalar.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function or its derivative.\"\"\"\n    x = np.asarray(x)\n    original_scalar = x.ndim == 0\n    if original_scalar:\n        x = x.reshape(1)\n    \n    if deriv:\n        y = 1 / (1 + np.exp(-x))\n    else:\n        y = np.where(x > 20, x, np.log1p(np.exp(x)))\n    \n    y_rounded = np.round(y, 4)\n    \n    if original_scalar:\n        return float(y_rounded[0])\n    else:\n        return y_rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The input is converted to a NumPy array to ensure uniform processing. If the input is a scalar, it is reshaped to a 1D array for consistent element-wise operations.\n2. **Numerical Stability**: For \\( x > 20 \\), \\( \\text{SoftPlus}(x) \\) is approximated as \\( x \\) to avoid overflow. For other values, `np.log1p(np.exp(x))` is used to compute \\( \\log(1 + e^x) \\) accurately.\n3. **Derivative Handling**: When the derivative flag is set, the sigmoid function is computed using \\( 1 / (1 + e^{-x}) \\), which is numerically stable for all values of \\( x \\).\n4. **Rounding**: The results are rounded to four decimal places to meet the problem's requirements.\n5. **Output Handling**: The result is returned as a Python list if the input was a list or higher-dimensional array, or as a float if the input was a scalar, ensuring the output format matches the input type."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an unsmoothed Maximum-Likelihood Estimation (MLE) N-gram language model. The goal is to compute the total log-probability of a given sequence under the N-gram model trained on a provided corpus.\n\n### Approach\n1. **Problem Analysis**:\n   - For N=1 (unigram), the probability of a word is its count divided by the total number of words in the corpus.\n   - For N>1, the probability of an N-gram is the count of the N-gram divided by the count of its (N-1)-gram prefix.\n   - The log-probability of the entire sequence is the sum of the log-probabilities of each N-gram window in the sequence.\n\n2. **Intuitions and Insights**:\n   - Use Counters to efficiently count occurrences of N-grams and their prefixes.\n   - Handle edge cases such as empty sequences, sequences shorter than N, and zero counts appropriately.\n\n3. **Algorithm Selection**:\n   - Use the `Counter` class from the `collections` module to count occurrences of N-grams and their prefixes.\n   - For each N-gram in the sequence, compute the probability and accumulate the log-probability. If any count is zero, return negative infinity.\n\n4. **Complexity Analysis**:\n   - Building the Counters for N-grams and (N-1)-grams takes O(M) time where M is the length of the corpus.\n   - Processing the sequence to compute log-probabilities takes O(K) time where K is the length of the sequence.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\"\"\"\n    if N == 1:\n        total_tokens = len(corpus)\n        if total_tokens == 0:\n            if len(sequence) > 0:\n                return float('-inf')\n            else:\n                return 0.0\n        ngram_counts = Counter(corpus)\n        sum_log_prob = 0.0\n        for word in sequence:\n            count = ngram_counts.get(word, 0)\n            if count == 0:\n                return float('-inf')\n            prob = count / total_tokens\n            sum_log_prob += math.log(prob)\n    else:\n        ngram_counts = Counter()\n        ngram_minus_1_counts = Counter()\n        for i in range(len(corpus) - N + 1):\n            ngram = tuple(corpus[i:i+N])\n            ngram_counts[ngram] += 1\n            prefix = tuple(corpus[i:i+N-1])\n            ngram_minus_1_counts[prefix] += 1\n        sum_log_prob = 0.0\n        for i in range(len(sequence) - N + 1):\n            current_ngram = tuple(sequence[i:i+N])\n            count_ngram = ngram_counts.get(current_ngram, 0)\n            if count_ngram == 0:\n                return float('-inf')\n            prefix = tuple(sequence[i:i+N-1])\n            count_prefix = ngram_minus_1_counts.get(prefix, 0)\n            if count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n            sum_log_prob += math.log(prob)\n    return round(sum_log_prob, 4)\n```\n\n### Explanation\n1. **Building Counters**:\n   - For N=1, count the occurrences of each word in the corpus.\n   - For N>1, count occurrences of each N-gram and their (N-1)-gram prefixes.\n\n2. **Processing the Sequence**:\n   - For each N-gram window in the sequence, compute the probability using the counts from the Counters.\n   - If any count is zero, return negative infinity immediately.\n\n3. **Log-Probability Calculation**:\n   - Sum the log-probabilities of all valid N-gram windows.\n   - Return the result rounded to 4 decimal places.\n\nThis approach ensures that we efficiently compute the log-probability while handling edge cases and zero counts appropriately."}
{"task_id": 62, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a univariate Lasso regression model using coordinate descent. The goal is to minimize the objective function given by the sum of squared errors and an L1 regularization term. The implementation should support polynomial feature expansion up to a specified degree.\n\n### Approach\n1. **Polynomial Feature Expansion**: Convert the input feature vector into a design matrix that includes polynomial terms up to the specified degree. This matrix will have a column of ones for the bias term followed by columns for x, x\u00b2, ..., x^degree.\n\n2. **Coordinate Descent**: Use coordinate descent to iteratively update each weight. The bias term (w\u2080) is updated exactly in each iteration, while other weights (w\u2c7c) are updated using a soft thresholding step. The soft thresholding ensures that weights shrink towards zero, promoting sparsity.\n\n3. **Convergence Check**: The algorithm stops when the maximum absolute change in any weight between iterations falls below a specified tolerance or after a maximum number of iterations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Create the design matrix\n    m = len(X)\n    n_features = degree + 1\n    design_matrix = np.ones((m, n_features))\n    for j in range(1, n_features):\n        design_matrix[:, j] = X ** j\n    \n    # Initialize weights\n    w = np.zeros(n_features)\n    \n    for _ in range(n_iterations):\n        prev_w = w.copy()\n        \n        # Update w0\n        X_not0 = design_matrix[:, 1:]\n        w_not0 = w[1:]\n        residual = y - np.dot(X_not0, w_not0)\n        w0 = np.mean(residual)\n        w[0] = w0\n        \n        # Update other coefficients\n        for j in range(1, n_features):\n            x_j = design_matrix[:, j]\n            Xw = np.dot(design_matrix, w)\n            term = y - Xw + w[j] * x_j[:, None]\n            rho = np.dot(x_j, term)\n            denominator = np.dot(x_j, x_j)\n            \n            if rho > reg_factor:\n                wj = (rho - reg_factor) / denominator\n            elif rho < -reg_factor:\n                wj = (-rho - reg_factor) / denominator\n            else:\n                wj = 0.0\n            \n            w[j] = wj\n        \n        # Check for convergence\n        max_change = np.max(np.abs(w - prev_w))\n        if max_change < tol:\n            break\n    \n    # Round the weights to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n```\n\n### Explanation\n1. **Polynomial Feature Expansion**: The input feature vector `X` is expanded into a design matrix that includes polynomial terms up to the specified degree. This matrix is used to fit the regression model.\n\n2. **Coordinate Descent**: The algorithm iteratively updates each weight. The bias term is updated using the mean of the residuals, while other weights are updated using a soft thresholding step that incorporates the regularization strength. This step ensures that weights are shrunk towards zero, which helps in feature selection.\n\n3. **Convergence Check**: The algorithm stops early if the maximum change in any weight between iterations is below the specified tolerance, ensuring efficient computation.\n\nThis approach efficiently minimizes the objective function using coordinate descent, providing a sparse solution that balances fit and complexity through regularization."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The backward probability \\(\\beta_t(i)\\) represents the probability of seeing the remaining observations from time \\(t+1\\) onward given that the system is in state \\(i\\) at time \\(t\\).\n\n### Approach\n1. **Initialization**: Start by initializing the backward probability vector for the last time step \\(T-1\\) to 1 for all states, as there are no future observations beyond the last step.\n2. **Backward Calculation**: Compute the backward probabilities iteratively from the second last time step down to the first time step. For each time step \\(t\\), compute the probability for each state \\(i\\) by summing over all possible next states \\(j\\), using the transition probabilities, emission probabilities, and the previously computed backward probabilities.\n3. **Result Extraction**: After computing the backward probabilities for all relevant time steps, extract the backward probability vector for the given time index \\(t\\) and round the values to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n    \n    Args:\n        A: Transition probability matrix of shape (N, N)\n        B: Emission probability matrix of shape (N, M)\n        obs: List of observation indices (length T)\n        t: Integer time index (0 \u2264 t < T)\n        \n    Returns:\n        A list of N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)\n    T = len(obs)\n    \n    # Initialize beta vectors\n    beta = [[0.0 for _ in range(N)] for _ in range(T)]\n    \n    # Base case: t = T-1\n    for i in range(N):\n        beta[T-1][i] = 1.0\n    \n    # Compute beta for t from T-2 down to 0\n    for current_t in range(T-2, -1, -1):\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                a_ij = A[i][j]\n                b_j = B[j][obs[current_t + 1]]\n                beta_j = beta[current_t + 1][j]\n                total += a_ij * b_j * beta_j\n            beta[current_t][i] = total\n    \n    # Round the result to 4 decimal places\n    result = [round(beta[t][i], 4) for i in range(N)]\n    return result\n```\n\n### Explanation\n1. **Initialization**: The backward probability vector for the last time step \\(T-1\\) is initialized to 1 for all states because there are no future observations to consider.\n2. **Backward Calculation**: For each time step from \\(T-2\\) down to 0, the backward probability for each state \\(i\\) is computed by summing the product of the transition probability from state \\(i\\) to state \\(j\\), the emission probability of the next observation in state \\(j\\), and the previously computed backward probability for state \\(j\\).\n3. **Result Extraction**: The computed backward probabilities for the specified time index \\(t\\) are extracted and rounded to four decimal places for the final result.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that we only compute the necessary values and reuse them as needed."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by leveraging the concept of backward variables, which represent the probability of observing a sequence from a certain point onward given the current state.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the probability of an observation sequence given an HMM using the backward algorithm. The HMM is defined by transition matrix `A`, emission matrix `B`, and initial distribution `pi`. The backward variables are used to compute this probability efficiently.\n\n2. **Backward Variables**: The backward variable `beta_t(i)` represents the probability of observing the sequence from time `t+1` to the end given that the state at time `t` is `i`. These variables are computed recursively starting from the last observation and moving backward.\n\n3. **Initialization**: The backward variables for the last time step are initialized to 1, as there are no future observations to consider.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to `0`, compute the backward variables using the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution, emission probabilities, and the backward variables.\n\n6. **Edge Cases**: If any of the input lists are empty, the function returns 0.0 as no valid probability can be computed.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    N = len(A)\n    T = len(obs)\n\n    if T == 0:\n        return 0.0\n\n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                new_beta[i] += A[i][j] * B[j][obs[t+1]] * beta[j]\n        beta = new_beta\n\n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n\n    return round(prob, 6)\n```\n\n### Explanation\n1. **Initialization Check**: The function first checks if any of the input lists are empty. If so, it returns 0.0 immediately.\n\n2. **Initialization of Backward Variables**: The backward variables for the last time step are initialized to 1.0.\n\n3. **Recursive Calculation**: For each time step from `T-2` down to `0`, the function computes the backward variables using the transition matrix `A`, emission matrix `B`, and the previously computed backward variables.\n\n4. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution `pi`, emission probabilities, and the backward variables.\n\n5. **Rounding**: The result is rounded to six decimal places before returning.\n\nThis approach efficiently computes the probability using dynamic programming principles, ensuring that the solution is both optimal and easy to understand."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the likelihood of an observation sequence given a Hidden Markov Model (HMM) using the forward algorithm. The HMM is defined by the initial state probabilities, state-transition probabilities, and emission probabilities. The function must validate the input dimensions and return the probability rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: \n   - Check if the observations list is empty.\n   - Ensure the dimensions of the initial state probabilities (S), state-transition matrix (A), and emission matrix (B) are correct.\n   - Verify that each observation index is within the valid range.\n\n2. **Forward Algorithm**:\n   - Initialize a vector `alpha` where `alpha[i]` represents the probability of being in state `i` at the first observation.\n   - For each subsequent observation, update `alpha` using the state-transition and emission probabilities.\n   - The result is the sum of the probabilities in the `alpha` vector after processing all observations.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check if observations is empty\n    if not observations:\n        return -1\n    \n    n = len(S)\n    # Check dimensions of S\n    if len(S) != n:\n        return -1\n    \n    # Check dimensions of A\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check dimensions of B\n    if len(B) != n:\n        return -1\n    if not B:\n        return -1  # B is empty\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check each observation is within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Compute forward algorithm\n    T = len(observations)\n    alpha = [0.0] * n\n    k0 = observations[0]\n    for i in range(n):\n        alpha[i] = S[i] * B[i][k0]\n    \n    for t in range(1, T):\n        kt = observations[t]\n        new_alpha = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += alpha[j] * A[j][i]\n            new_alpha[i] = sum_val * B[i][kt]\n        alpha = new_alpha\n    \n    result = sum(alpha)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the observations list is empty and returns -1 if it is. It then verifies the dimensions of the matrices S, A, and B to ensure they are consistent with the model's requirements. Each observation is checked to ensure it falls within the valid range.\n\n2. **Initialization**: The `alpha` vector is initialized based on the initial state probabilities and the emission probabilities for the first observation.\n\n3. **Forward Algorithm**: For each subsequent observation, the `alpha` vector is updated by considering all possible transitions from the previous states. This involves summing the probabilities of transitioning to each state and multiplying by the emission probability for the current observation.\n\n4. **Result Calculation**: The final result is the sum of the probabilities in the `alpha` vector after processing all observations, which is then rounded to four decimal places and returned."}
{"task_id": 70, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the FP-Growth algorithm to discover all frequent item-sets in a transactional database. The algorithm efficiently finds all item-sets that meet a given minimum support threshold.\n\n### Approach\nThe FP-Growth algorithm consists of three main steps:\n\n1. **Preprocess the Data**: Count the support of each individual item and filter out those with support below the minimum threshold.\n2. **Build the FP-Tree**: Construct a prefix tree (FP-tree) where each node represents an item and stores the count of how many times the path to that node has been traversed.\n3. **Mine the FP-Tree**: Recursively mine the tree to find all frequent item-sets. This involves processing nodes in order of increasing path length and collecting suffixes from each node's subtree to form longer item-sets.\n\n### Solution Code\n```python\nfrom collections import Counter, deque\nfrom typing import List\n\nclass Node:\n    def __init__(self, item=None):\n        self.item = item\n        self.children = {}\n        self.count = 0\n        self.parent = None\n        self.link = None\n\ndef fp_growth(transactions: List[List[str]], min_support: int) -> List[List[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\"\"\"\n    if not transactions:\n        return []\n    \n    # Step 1: Preprocess to count support of each item\n    item_counts = Counter()\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] += 1\n    \n    # Filter items with support >= min_support\n    filtered_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    if not filtered_items:\n        return []\n    \n    # Step 2: Build the FP-tree\n    root = Node()\n    for transaction in transactions:\n        sorted_transaction = sorted(transaction)\n        current_node = root\n        for item in sorted_transaction:\n            if item not in current_node.children:\n                new_node = Node(item)\n                new_node.parent = current_node\n                current_node.children[item] = new_node\n                current_node = new_node\n            else:\n                current_node = current_node.children[item]\n                current_node.count += 1\n    \n    # Step 3: Mine the FP-tree\n    result = []\n    \n    # Collect all nodes in order of increasing path length\n    nodes = []\n    queue = deque()\n    for child in root.children.values():\n        queue.append(child)\n        nodes.append(child)\n    while queue:\n        node = queue.popleft()\n        for child in node.children.values():\n            nodes.append(child)\n            queue.append(child)\n    \n    # Sort nodes by path length and then by item\n    def get_path_length(node):\n        length = 0\n        current = node\n        while current.parent is not None:\n            length += 1\n            current = current.parent\n        return length\n    nodes.sort(key=lambda x: (get_path_length(x), x.item))\n    \n    # Function to get the path from root to node\n    def get_path(node):\n        path = []\n        current = node\n        while current.parent is not None:\n            path.append(current.item)\n            current = current.parent\n        return path[::-1]\n    \n    # Function to collect suffixes from a node's subtree\n    def collect_suffixes(node):\n        suffixes = []\n        for item, child in node.children.items():\n            for suffix, count in collect_suffixes(child):\n                suffixes.append(([item] + suffix, count))\n            suffixes.append(([item], child.count))\n        return suffixes\n    \n    # Process each node\n    for node in nodes:\n        if node.count >= min_support:\n            path = get_path(node)\n            result.append(path)\n            suffixes = collect_suffixes(node)\n            for suffix, count in suffixes:\n                if count >= min_support:\n                    combined = path + suffix\n                    result.append(combined)\n    \n    # Remove duplicates\n    unique_result = []\n    seen = set()\n    for itemset in result:\n        t = tuple(itemset)\n        if t not in seen:\n            seen.add(t)\n            unique_result.append(itemset)\n    \n    # Sort each itemset and the outer list\n    for itemset in unique_result:\n        itemset.sort()\n    unique_result.sort(key=lambda x: (len(x), x))\n    \n    return unique_result\n```\n\n### Explanation\n1. **Preprocessing**: We count the occurrences of each item across all transactions to determine which items meet the minimum support threshold.\n2. **Building the FP-Tree**: Each transaction is sorted lexicographically, and nodes are created in the tree to represent the items. Each node's count is incremented for each occurrence of the corresponding item in the transactions.\n3. **Mining the Tree**: Nodes are processed in order of increasing path length. For each node with sufficient support, the path to the node is added to the result. Suffixes from the node's subtree are collected to form longer item-sets, which are also added if they meet the support threshold.\n4. **Result Processing**: The results are deduplicated, each item-set is sorted, and the outer list is sorted by length and lexicographic order.\n\nThis approach ensures that all frequent item-sets are efficiently discovered and returned in the required format."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The classifier will predict the class label for each test sample based on the majority vote among its k closest training samples using the Euclidean distance metric. If there is a tie in the majority vote, the smallest class label among the tied ones will be chosen. If k is not a valid positive integer or larger than the number of training samples, the function should return -1.\n\n### Approach\n1. **Validation of k**: Check if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, return a list of -1s with the same length as the test set.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. This avoids the computational cost of taking square roots while maintaining the order of distances.\n3. **Sorting and Selection**: Sort the training samples based on the computed distances and select the top k nearest samples.\n4. **Majority Vote**: Among the k nearest samples, determine the class label with the highest frequency. In case of a tie, select the smallest class label.\n\n### Solution Code\n```python\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    \n    # Check if k is valid\n    if k < 1 or k > n_train:\n        return [-1] * n_test\n    \n    predictions = []\n    for x in X_test:\n        # Compute distances to all training samples\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            dist_sq = sum((x[i] - xi[i])**2 for i in range(len(x)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort()\n        \n        # Take the first k samples\n        selected = distances[:k]\n        \n        # Collect the class labels\n        ys = [y for (d, y) in selected]\n        \n        # Find the mode, breaking ties by choosing the smallest class label\n        freq = {}\n        for y in ys:\n            freq[y] = freq.get(y, 0) + 1\n        \n        max_freq = max(freq.values())\n        candidates = [y for y in freq if freq[y] == max_freq]\n        predicted = min(candidates)\n        \n        predictions.append(predicted)\n    \n    return predictions\n```\n\n### Explanation\n1. **Validation of k**: The function first checks if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, it returns a list of -1s.\n2. **Distance Calculation**: For each test sample, the squared Euclidean distance to each training sample is computed. This avoids the computational cost of taking square roots.\n3. **Sorting and Selection**: The computed distances are sorted, and the top k nearest samples are selected.\n4. **Majority Vote**: The class labels of the k nearest samples are counted. The class with the highest frequency is chosen. In case of a tie, the smallest class label is selected.\n\nThis approach ensures that the function efficiently computes the k-NN classification for each test sample while handling edge cases such as invalid k and ties in the majority vote."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the unnormalized categorical cross-entropy loss for a batch of one-hot encoded target labels. The cross-entropy loss is a common metric used in machine learning to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function accepts inputs as either lists or NumPy arrays. We first convert these inputs to NumPy arrays to facilitate element-wise operations.\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to all elements of the predicted probabilities.\n3. **Compute Logarithm of Predictions**: We compute the natural logarithm of the adjusted predicted probabilities.\n4. **Element-wise Multiplication**: Multiply the true labels (one-hot encoded) with the logarithm of the predicted probabilities. This step effectively extracts the log probability of the true class for each sample.\n5. **Sum and Negate**: Sum all the elements of the resulting matrix and negate the sum to get the cross-entropy loss.\n6. **Round the Result**: Finally, round the computed loss to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = 1e-10\n    y_pred += epsilon\n    log_y_pred = np.log(y_pred)\n    product = y * log_y_pred\n    sum_product = np.sum(product)\n    loss = -sum_product\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This ensures that we can perform element-wise operations efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in an undefined value.\n- **Logarithm and Multiplication**: By taking the logarithm of the predicted probabilities and multiplying by the one-hot encoded true labels, we isolate the log probability of the correct class for each sample.\n- **Summing and Negating**: The sum of these log probabilities is negated to compute the cross-entropy loss, which measures the dissimilarity between the predicted and true distributions.\n- **Rounding**: The result is rounded to four decimal places for precision and returned as a float.\n\nThis approach efficiently computes the cross-entropy loss using NumPy operations, ensuring numerical stability and correctness."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward propagation for an L-layer neural network. The network uses ReLU activation for all hidden layers and sigmoid activation for the output layer. The goal is to compute the activation of the last layer and store the necessary information for backpropagation in a cache.\n\n### Approach\n1. **Initialization**: Start with the input matrix X as the initial activation (A_prev).\n2. **Iterate through each layer**: For each layer from 1 to L:\n   - Compute the linear step: \\( Z = W \\cdot A_{prev} + b \\)\n   - Apply the activation function: ReLU for hidden layers and sigmoid for the output layer.\n   - Store the cache for each layer, which includes the previous activation (A_prev), the linear output (Z), and the activation output (A).\n3. **Final Activation**: After processing all layers, the activation of the last layer (AL) is rounded to 4 decimal places and converted to a Python list.\n4. **Return**: The function returns a tuple containing the final activation (AL) and the list of caches.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]\n        A tuple containing the final activation AL and the list of caches.\n    \"\"\"\n    caches = []\n    A_prev = X\n    # Determine the number of layers\n    W_layers = [int(key[1:]) for key in parameters.keys() if key.startswith('W')]\n    L = max(W_layers) if W_layers else 0\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))\n        else:\n            A = np.maximum(0, Z)\n        caches.append((A_prev, Z, A))\n        A_prev = A\n    AL = A\n    AL_rounded = np.round(AL, 4)\n    AL_list = AL_rounded.tolist()\n    return (AL_list, caches)\n```\n\n### Explanation\n- **Initialization**: The input matrix X is used as the initial activation (A_prev).\n- **Linear Step**: For each layer, the linear combination of the previous activation and the current weights plus bias is computed.\n- **Activation Step**: ReLU is applied to all hidden layers, and sigmoid is applied to the output layer.\n- **Cache Storage**: Each layer's cache stores the previous activation, the linear output, and the activation output for use in backpropagation.\n- **Final Activation**: The output of the last layer is rounded and converted to a list before being returned along with the caches.\n\nThis approach ensures that we correctly compute the forward pass through the network and store the necessary information for efficient backpropagation."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost (log-loss) between the predicted probabilities and the ground-truth labels for a binary classifier. The binary cross-entropy cost is a measure of how well the predicted probabilities match the actual labels.\n\n### Approach\n1. **Clip Predicted Probabilities**: To avoid taking the logarithm of zero or one, we clip the predicted probabilities to a small interval [\u03b5, 1-\u03b5], where \u03b5 is 1e-15. This ensures that the values are within a valid range for logarithmic functions.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements (1 - predicted probabilities).\n3. **Compute Terms**: For each element, compute the terms involving the logarithms of the predicted probabilities and their complements, weighted by the ground-truth labels.\n4. **Sum and Average**: Sum all the computed terms and then average them by dividing by the number of samples.\n5. **Round Result**: Finally, round the result to six decimal places to get the final cost.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    # Clip A2 to avoid log(0) and log(1)\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    # Compute the logarithms\n    log_a = np.log(A2_clipped)\n    log_1a = np.log(1 - A2_clipped)\n    # Calculate each term\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1a\n    # Sum the terms\n    sum_terms = term1 + term2\n    sum_total = np.sum(sum_terms)\n    m = Y.size\n    # Compute the cost\n    J = (-1 / m) * sum_total\n    # Round to six decimal places\n    return round(J, 6)\n```\n\n### Explanation\n1. **Clipping**: The predicted probabilities are clipped to ensure they lie within a small range around 0 and 1, preventing invalid logarithm operations.\n2. **Logarithms**: The natural logarithm of the clipped probabilities and their complements are computed to handle the log terms in the cost function.\n3. **Terms Calculation**: Each term in the cost function is computed by multiplying the logarithm values with the corresponding ground-truth labels and their complements.\n4. **Summation and Averaging**: The terms are summed up and then averaged over the number of samples to get the final cost.\n5. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be 0 or 1."}
{"task_id": 82, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy (best) action. The goal is to compute the probabilities of selecting each action based on their Q-values and a given exploration parameter \u03b5.\n\n### Approach\n1. **Identify the Greedy Action**: The first step is to determine the action with the highest Q-value. If there are multiple actions with the same highest Q-value, we select the first occurrence (smallest index).\n2. **Calculate Probabilities**: Using the identified greedy action, we compute the probabilities for each action. The probability for the greedy action is given by \\(1 - \\epsilon + \\frac{\\epsilon}{n}\\), where \\(n\\) is the number of actions. For all other actions, the probability is \\(\\frac{\\epsilon}{n}\\).\n3. **Ensure Precision and Sum Constraints**: The computed probabilities are rounded to 4 decimal places. The sum of these probabilities must be exactly 1, within a small numerical tolerance (1 \u00d7 10\u207b\u00b9\u00b2).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    # Convert Q to a numpy array if it's a list\n    if isinstance(Q, list):\n        Q = np.array(Q)\n    n = len(Q)\n    if n == 0:\n        return []\n    greedy_index = np.argmax(Q)\n    epsilon_div_n = epsilon / n\n    # Initialize probabilities with epsilon/n for all actions\n    probs = np.full(n, epsilon_div_n)\n    # Set the probability for the greedy action\n    probs[greedy_index] = 1 - epsilon + epsilon_div_n\n    # Round each probability to 4 decimal places\n    probs = np.round(probs, 4)\n    # Convert to list and return\n    return probs.tolist()\n```\n\n### Explanation\n1. **Convert Input to Array**: The input Q-values are converted to a numpy array if they are provided as a list. This allows efficient computation of the maximum value and other operations.\n2. **Determine Greedy Action**: Using `np.argmax`, we find the index of the first occurrence of the maximum Q-value, which is our greedy action.\n3. **Compute Probabilities**: We initialize an array of probabilities where each action starts with \\(\\frac{\\epsilon}{n}\\). The probability for the greedy action is then updated to \\(1 - \\epsilon + \\frac{\\epsilon}{n}\\).\n4. **Round and Return**: Each probability is rounded to 4 decimal places to meet the precision requirement, and the result is returned as a list.\n\nThis approach ensures that the probabilities are computed efficiently and meet the specified constraints, providing a balance between exploration and exploitation in the reinforcement learning context."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the predictions from a Random Forest classifier using majority voting. Each decision tree in the forest makes its own prediction for every input sample, and the final prediction is determined by taking the majority vote across all trees. In case of a tie, the smallest label (lexicographically for strings, numerically for numbers) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions such that each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions together.\n2. **Count Votes**: For each sample, count the occurrences of each class label using a counter.\n3. **Determine the Winner**: Identify the class label with the highest count. If there is a tie, select the smallest label based on lexicographic order for strings or numerical order for numbers.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Transpose the predictions to process each sample's predictions together\n    transposed = zip(*predictions)\n    result = []\n    for sample_predictions in transposed:\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        # Collect all keys with the maximum count\n        candidates = [k for k, v in counts.items() if v == max_count]\n        # Select the smallest candidate\n        selected = min(candidates)\n        result.append(selected)\n    return result\n```\n\n### Explanation\n1. **Transposing Predictions**: The `zip(*predictions)` function is used to transpose the 2D list, converting rows of tree predictions into columns of sample predictions. This allows us to handle each sample's predictions together.\n2. **Counting Votes**: For each sample, we use `Counter` from the `collections` module to count how many times each class label appears across all trees.\n3. **Selecting the Winner**: After counting, we determine the class label with the highest count. If multiple labels have the same highest count, we use the `min` function to select the smallest label, ensuring the correct tie-breaking rule is applied.\n\nThis approach efficiently aggregates the predictions and handles ties correctly, ensuring the solution is both optimal and easy to understand."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function, which is a smooth approximation of the ReLU activation function. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation of this function can lead to numerical instability for very large or very small values of \\( z \\). To handle this, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept a scalar, a Python list, or a NumPy array. The output should be a float for scalar inputs and a Python list for array or list inputs.\n2. **Numerically Stable Computation**: Use `np.logaddexp(0.0, z)` to compute the softplus values. This function avoids overflow and underflow issues.\n3. **Rounding**: Each computed softplus value is rounded to 4 decimal places.\n4. **Output Formatting**: For scalar inputs, return a single float. For array or list inputs, return a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    sp = np.logaddexp(0.0, z)\n    sp_rounded = np.round(sp, 4)\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return float(sp_rounded)\n    else:\n        return sp_rounded.tolist()\n```\n\n### Explanation\n1. **Numerically Stable Calculation**: The function uses `np.logaddexp(0.0, z)` to compute the softplus values. This method is chosen because it avoids numerical issues that can arise from directly computing \\( e^z \\) for very large or very small values of \\( z \\).\n2. **Rounding**: The computed values are rounded to 4 decimal places using `np.round`.\n3. **Output Handling**: The function checks if the input is a scalar (int, float, or 0D array) and returns a float. For other inputs (lists or arrays), it returns a Python list using `tolist()`.\n\nThis approach ensures that the function handles all types of inputs correctly and efficiently, providing accurate results without numerical instability."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate how good a policy's current estimates are by computing the mean-squared error (MSE) between the true expected payouts of each arm in a bandit problem and the policy's estimates. The MSE is a measure of the average squared difference between the estimated values and the true values.\n\n### Approach\n1. **Input Handling**: The function takes two inputs, `bandit` and `policy`. The `bandit` object contains the true expected values of each arm, while the `policy` object contains the policy's current estimates of these values.\n2. **Check for Empty Estimates**: If the policy's estimates are missing or empty, the function returns `numpy.nan`.\n3. **Extract True Values**: The true expected values of each arm are extracted from the bandit object.\n4. **Compute Squared Errors**: For each arm, the function computes the squared error between the true value and the policy's estimate. If an arm's estimate is missing, it is treated as zero.\n5. **Calculate Mean Squared Error**: The average of these squared errors is computed to get the MSE.\n6. **Round the Result**: The result is rounded to four decimal places and returned.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Get the true arm expected values\n    true_arm_evs = getattr(bandit, 'arm_evs', None)\n    if true_arm_evs is None:\n        return np.nan\n    \n    # Get the policy's estimates\n    policy_estimates = getattr(policy, 'ev_estimates', None)\n    if policy_estimates is None:\n        return np.nan\n    if not policy_estimates:\n        return np.nan\n    \n    # Compute squared errors\n    squared_errors = []\n    for i in range(len(true_arm_evs)):\n        true_val = true_arm_evs[i]\n        estimate = policy_estimates.get(i, 0.0)\n        squared_error = (estimate - true_val) ** 2\n        squared_errors.append(squared_error)\n    \n    # Compute mean\n    mean_squared_error = np.mean(squared_errors)\n    \n    # Round to 4 decimal places\n    return round(mean_squared_error, 4)\n```\n\n### Explanation\n- **Input Handling**: The function uses `getattr` to safely access the `arm_evs` and `ev_estimates` attributes from the `bandit` and `policy` objects, respectively.\n- **Empty Estimates Check**: If the policy's estimates are missing or empty, the function immediately returns `numpy.nan`.\n- **Squared Errors Calculation**: For each arm, the function retrieves the true value and the policy's estimate. If the estimate is missing, it defaults to zero. The squared error for each arm is computed and stored.\n- **Mean Squared Error**: The average of the squared errors is computed using `numpy.mean`, and the result is rounded to four decimal places before being returned.\n\nThis approach ensures that the function correctly handles various edge cases, such as missing estimates and different data structures for the bandit and policy objects."}
{"task_id": 96, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the additive-smoothed log-probability of a given sentence using an N-gram language model. The approach involves building frequency tables for N-grams from a training corpus, handling unseen words with an explicit \"<UNK>\" token, and applying additive smoothing to estimate probabilities.\n\n### Approach\n1. **Build N-gram Frequency Tables**: For each sentence in the training corpus, extract all possible 1-grams, 2-grams, ..., N-grams and count their occurrences using a dictionary of Counters.\n2. **Vocabulary Construction**: Collect all unique words from the training corpus and add the \"<UNK>\" token to handle unseen words.\n3. **Preprocess Target Sentence**: Replace any word in the target sentence that is not in the vocabulary with \"<UNK>\".\n4. **Probability Calculation**: For each N-gram in the target sentence, compute the probability using additive smoothing. The formula used is:\n   \\[\n   P(w_i | \\text{context}) = \\frac{\\text{count}(\\text{context} + w_i) + K}{\\text{count}(\\text{context}) + K \\cdot |V|}\n   \\]\n   where \\(|V|\\) is the vocabulary size including \"<UNK>\".\n5. **Log Probability Summation**: Sum the log-probabilities of all N-grams in the target sentence and return the result rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    # Build n-gram frequency tables\n    ngram_counts = {n: Counter() for n in range(1, N + 1)}\n    for sentence in corpus:\n        words = sentence.split()\n        for n in range(1, N + 1):\n            if len(words) < n:\n                continue\n            for i in range(len(words) - n + 1):\n                ngram = tuple(words[i:i+n])\n                ngram_counts[n][ngram] += 1\n\n    # Build vocabulary\n    vocabulary = set()\n    for sentence in corpus:\n        words = sentence.split()\n        for word in words:\n            vocabulary.add(word)\n    vocabulary.add(\"<UNK>\")\n    vocabulary_size = len(vocabulary)\n\n    # Process target sequence\n    target_words = sequence.split()\n    processed_target = []\n    for word in target_words:\n        if word in vocabulary:\n            processed_target.append(word)\n        else:\n            processed_target.append(\"<UNK>\")\n    \n    if len(processed_target) < N:\n        return 0.0\n\n    # Calculate log probability\n    total_log_prob = 0.0\n    num_sentences = len(corpus)\n    for i in range(len(processed_target) - N + 1):\n        ngram = tuple(processed_target[i:i+N])\n        if N == 1:\n            context = ()\n            count_context = num_sentences\n        else:\n            context = ngram[:-1]\n            count_context = ngram_counts[N-1].get(context, 0)\n        \n        count_context_plus_word = ngram_counts[N].get(ngram, 0)\n        denominator = count_context + K * vocabulary_size\n        numerator = count_context_plus_word + K\n        \n        if denominator == 0:\n            prob = 0.0\n        else:\n            prob = numerator / denominator\n        \n        if prob <= 0:\n            log_prob = float('-inf')\n        else:\n            log_prob = math.log(prob)\n        \n        total_log_prob += log_prob\n\n    return round(total_log_prob, 4)\n```\n\n### Explanation\n1. **Building N-gram Frequency Tables**: The code iterates through each sentence in the corpus, splits it into words, and counts all possible N-grams for each n from 1 to N.\n2. **Vocabulary Construction**: The vocabulary is built by collecting all unique words from the corpus and adding the \"<UNK>\" token.\n3. **Target Sentence Preprocessing**: Each word in the target sentence is replaced with \"<UNK>\" if it is not found in the vocabulary.\n4. **Probability Calculation**: For each N-gram in the target sentence, the code calculates the probability using additive smoothing. The log-probability is computed and summed up.\n5. **Edge Cases Handling**: If the target sentence is shorter than N, the function returns 0.0. If the probability calculation results in a zero or negative value, it is handled appropriately to avoid errors.\n\nThis approach ensures that we handle unseen words, apply additive smoothing, and compute the log-probability efficiently."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to factorize a given real-valued matrix using the regularized Alternating Least Squares (ALS) algorithm. The goal is to find two low-rank factor matrices that minimize a regularized Frobenius reconstruction loss. The algorithm alternates between optimizing these matrices until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with random initial values for the factor matrices W and H using a fixed random seed for reproducibility.\n2. **Iterative Optimization**: In each iteration, update one matrix while keeping the other fixed. Specifically, solve a regularized least squares problem for each matrix using the current estimate of the other matrix.\n3. **Loss Calculation**: Compute the regularized Frobenius loss after each update to check for convergence.\n4. **Stopping Condition**: Stop the iterations when the loss drops below a specified tolerance or the maximum number of iterations is reached.\n5. **Reconstruction**: After convergence, compute the reconstructed matrix using the final estimates of W and H, round it to four decimal places, and return it as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K) * 0.1\n    H = np.random.randn(K, M) * 0.1\n\n    def compute_loss(X, W, H, alpha):\n        recon = W @ H\n        loss = np.linalg.norm(X - recon)**2 + alpha * (np.linalg.norm(W)**2 + np.linalg.norm(H)**2)\n        return loss\n\n    current_loss = compute_loss(X, W, H, alpha)\n    if current_loss < tol:\n        pass  # Already converged\n    else:\n        for _ in range(max_iter):\n            # Update W\n            Ht = H.T\n            H_Ht = H @ Ht\n            A = H_Ht + alpha * np.eye(K)\n            b = X @ Ht\n            new_W = np.linalg.solve(A, b)\n\n            # Update H\n            Wt = new_W.T\n            Wt_W = Wt @ new_W\n            A = Wt_W + alpha * np.eye(K)\n            b = Wt @ X\n            new_H = np.linalg.solve(A, b)\n\n            # Update W and H\n            W, H = new_W, new_H\n\n            # Compute loss\n            current_loss = compute_loss(X, W, H, alpha)\n            if current_loss < tol:\n                break\n\n    # Compute the reconstructed matrix\n    X_hat = W @ H\n    X_hat_rounded = np.round(X_hat, 4)\n    return X_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrices W and H are initialized with small random values using a fixed seed to ensure reproducibility.\n2. **Loss Function**: The loss function combines the Frobenius norm of the difference between the original matrix and the product of the factor matrices, plus a regularization term.\n3. **Matrix Updates**: In each iteration, the matrices W and H are updated by solving regularized least squares problems. This involves computing the necessary matrix inverses to find the optimal updates.\n4. **Convergence Check**: After each update, the loss is computed and checked against the tolerance. If the loss is sufficiently small, the algorithm stops early.\n5. **Reconstruction**: The final reconstructed matrix is computed, rounded to four decimal places, and returned as a list of lists.\n\nThis approach ensures that the factorization is both efficient and numerically stable, leveraging matrix operations to solve the least squares problems at each step."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm with K-Means++ initialization. The goal is to cluster a given dataset into K clusters and return the centroids of these clusters, rounded to four decimal places and sorted by their first coordinate.\n\n### Approach\n1. **Set Random Seeds**: Ensure reproducibility by setting the random seeds for both Python's `random` module and NumPy's random generator using the provided `random_state`.\n\n2. **K-Means++ Initialization**: \n   - Select the first centroid randomly from the dataset.\n   - For each subsequent centroid, compute the distance from each point to the nearest existing centroid, then select the next centroid with a probability proportional to the squared distance from these points.\n\n3. **Lloyd's Algorithm**: \n   - Assign each point to the nearest centroid.\n   - Update each centroid to be the mean of the points assigned to it.\n   - Repeat until the centroids no longer change or the maximum number of iterations is reached.\n\n4. **Return Centroids**: Round the centroids to four decimal places and sort them by their first coordinate, using the entire centroid as a secondary key for tie-breaking.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    \n    m, n = X.shape\n    if K == 0:\n        return []\n    \n    centroids = []\n    \n    # Initialize centroids using K-Means++ algorithm\n    first_centroid_idx = np.random.choice(m)\n    centroids.append(X[first_centroid_idx].copy())\n    \n    for i in range(1, K):\n        if not centroids:\n            first_centroid_idx = np.random.choice(m)\n            centroids.append(X[first_centroid_idx].copy())\n            continue\n        \n        centroids_arr = np.array(centroids)\n        dist_sq = np.sum((X[:, np.newaxis] - centroids_arr) ** 2, axis=2)\n        min_dist_sq = np.min(dist_sq, axis=1)\n        sum_min_dist_sq = np.sum(min_dist_sq)\n        \n        if sum_min_dist_sq == 0:\n            break  # All points are already centroids, can't select new ones\n        \n        probabilities = min_dist_sq / sum_min_dist_sq\n        next_centroid_idx = np.random.choice(m, p=probabilities)\n        next_centroid = X[next_centroid_idx].copy()\n        centroids.append(next_centroid)\n    \n    # Convert to NumPy array for easier handling\n    current_centroids = np.array(centroids)\n    \n    for iter in range(max_iter):\n        # Assign each point to the nearest centroid\n        dist_sq = np.sum((X[:, np.newaxis] - current_centroids) ** 2, axis=2)\n        assignments = np.argmin(dist_sq, axis=1)\n        \n        # Compute new centroids\n        new_centroids = []\n        for c in range(K):\n            points_in_cluster = X[assignments == c]\n            if len(points_in_cluster) == 0:\n                new_centroids.append(current_centroids[c])\n            else:\n                new_centroid = np.mean(points_in_cluster, axis=0)\n                new_centroids.append(new_centroid)\n        \n        new_centroids_arr = np.array(new_centroids)\n        \n        # Check if centroids have converged\n        if np.allclose(current_centroids, new_centroids_arr, atol=1e-6):\n            break\n        current_centroids = new_centroids_arr\n    \n    # Round centroids to 4 decimal places and sort\n    rounded_centroids = [list(np.round(c, 4)) for c in current_centroids]\n    rounded_centroids.sort()\n    \n    return rounded_centroids\n```\n\n### Explanation\n1. **Random Seed Setting**: The function starts by setting the random seeds to ensure reproducibility.\n2. **K-Means++ Initialization**: The first centroid is chosen randomly. Subsequent centroids are selected based on the squared distances to the nearest existing centroid, ensuring a more efficient and effective initialization.\n3. **Lloyd's Algorithm**: The algorithm iteratively assigns points to the nearest centroid and updates the centroids until convergence or the maximum number of iterations is reached.\n4. **Result Processing**: The final centroids are rounded to four decimal places and sorted to meet the problem's requirements.\n\nThis approach ensures that the clustering is both efficient and reproducible, providing accurate and consistent results."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions to handle character-level sequence encoding and decoding. These functions will be used in a neural model to convert raw text into a fixed-length sequence of integer tokens, process the sequence through the model, and then decode the model's predictions back into human-readable text.\n\n### Approach\n1. **string_to_int**: This function converts each character of the input text into an integer using a provided vocabulary. The resulting list of integers is adjusted to a fixed length of 20, padding with zeros if the text is shorter and truncating if it is longer. Characters not found in the vocabulary are encoded as zeros.\n\n2. **int_to_string**: This function converts a list of integer tokens back into text using an inverse vocabulary. Padding tokens (zeros) are ignored during this conversion.\n\n3. **run_example**: This function orchestrates the entire process by encoding the input text, feeding it to the model, processing the model's predictions, and decoding the results back into a string.\n\n### Solution Code\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a text string into a fixed-length list of integer token ids.\"\"\"\n    # Convert each character to its corresponding integer, defaulting to 0 for unknown characters\n    encoded = [vocabulary.get(c, 0) for c in text]\n    # Adjust the length to match time_steps, padding with zeros or truncating as needed\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    # Filter out padding tokens and convert to characters\n    filtered = []\n    for idx in indices:\n        if idx != 0:\n            char = inverse_vocab.get(idx, '')\n            filtered.append(char)\n    return ''.join(filtered)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on encoded text and decodes the prediction.\"\"\"\n    # Encode the input text into a fixed-length sequence\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to a numpy array with shape (1, TIME_STEPS)\n    encoded_np = np.array(encoded, dtype=np.int32).reshape(1, -1)\n    # Get model predictions\n    predictions = model.predict(encoded_np)\n    # Use argmax to get the predicted token ids\n    predicted_ids = np.argmax(predictions, axis=-1).flatten()\n    # Decode the predicted ids into a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n1. **string_to_int**:\n   - Converts each character in the input text to an integer using the provided vocabulary.\n   - Ensures the resulting list is exactly 20 elements long by padding with zeros if the text is shorter or truncating if it is longer.\n\n2. **int_to_string**:\n   - Converts a list of integer tokens back into text using the inverse vocabulary.\n   - Ignores any padding tokens (zeros) during the conversion process.\n\n3. **run_example**:\n   - Encodes the input text into a fixed-length sequence.\n   - Feeds this sequence to the model to get predictions.\n   - Processes the model's predictions to extract the most probable token ids.\n   - Decodes these token ids back into a human-readable string.\n\nThis approach ensures that the text is properly encoded and decoded, handling both padding and truncation as needed, and efficiently processes the model's predictions to produce the final output."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the input examples and the output formatting, making it easier to quickly evaluate the model's performance on various test cases.\n\n### Approach\n1. **Function Parameters Handling**: The function `run_examples` takes four parameters: a model (a callable function), an input vocabulary dictionary, an inverse output vocabulary dictionary, and an optional examples iterable. If the examples iterable is not provided, the function falls back to a global constant `EXAMPLES`.\n2. **Example Processing**: For each input string in the examples, the function calls another helper function `run_example` to get the model's predictions. This helper function returns a list of characters, which are then concatenated into a single string.\n3. **Output Formatting and Collection**: Each input string and its corresponding prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    results = []\n    for example in examples:\n        output = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output)\n        print(f\"input: {example}\")\n        print(f\"output: {output_str}\")\n        results.append(output_str)\n    return results\n```\n\n### Explanation\n- **Function Parameters**: The function is designed to accept the model, input vocabulary, inverse output vocabulary, and an optional examples iterable. If examples are not provided, it uses a global constant `EXAMPLES`.\n- **Processing Each Example**: For each input string, the function calls `run_example` to get the model's predictions. The predictions are returned as a list of characters, which are then joined into a single string.\n- **Output Handling**: Each input string and its prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\nThis approach ensures that the function is both efficient and easy to use, allowing for quick evaluation of the model on multiple test cases."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for a logistic regression model. The function should handle numerical stability issues by ensuring predicted probabilities are within a safe range for logarithmic operations.\n\n### Approach\n1. **Compute the Linear Combination (z):** For each sample, compute the linear combination of features and weights, which is given by \\( z = X \\cdot w \\).\n2. **Compute the Sigmoid Function (p):** Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities:** To avoid numerical issues when taking logarithms, clip the probabilities to a small range around 0 and 1.\n4. **Compute the Loss:** Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p)] \\).\n5. **Compute the Gradient:** Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Return Results:** Return the loss rounded to 4 decimal places and the gradient as a list of lists, each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    y = y.ravel()\n    \n    loss = (-1 / m) * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    gradient = (1 / m) * X.T.dot(p - y)\n    gradient = gradient.reshape(-1, 1)\n    \n    loss_rounded = round(loss, 4)\n    gradient_rounded = np.round(gradient, 4).tolist()\n    \n    return (loss_rounded, gradient_rounded)\n```\n\n### Explanation\n1. **Linear Combination (z):** The linear combination of features and weights is computed using matrix multiplication, resulting in a vector of scores for each sample.\n2. **Sigmoid Function (p):** The scores are converted into probabilities using the sigmoid function, ensuring values are between 0 and 1.\n3. **Clipping Probabilities:** To avoid taking the logarithm of 0 or 1, probabilities are clipped to a small range around these values.\n4. **Loss Calculation:** The binary cross-entropy loss is computed by averaging the negative log-likelihood over all samples.\n5. **Gradient Calculation:** The gradient is computed using the formula involving the transpose of the feature matrix and the difference between predicted and actual probabilities.\n6. **Result Formatting:** The loss is rounded to 4 decimal places, and the gradient is converted to a list of lists with each element rounded to 4 decimal places for the final output."}
{"task_id": 118, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to iteratively re-weight training examples and combine the weak classifiers to form a strong classifier. The final prediction is made by aggregating the votes from all weak classifiers.\n\n### Approach\n1. **Initialization**: Start with equal weights for all training examples.\n2. **Iterative Training**: For each iteration, find the best decision stump (one-level decision tree) that minimizes the weighted classification error. Update the weights of the training examples based on the performance of the current weak classifier.\n3. **Weight Update**: After selecting the best weak classifier, compute its weight using the formula provided. Update the weights of the training examples to focus more on the misclassified examples.\n4. **Prediction**: For each test sample, compute the weighted sum of predictions from all weak classifiers. The sign of this sum determines the final prediction.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    n_clf = max(n_clf, 1)\n    \n    weights = np.ones(m) / m\n    classifiers = []\n    \n    for _ in range(n_clf):\n        best_error = 1.0\n        best_feature = -1\n        best_threshold = 0.0\n        \n        for j in range(n):\n            feature_data = list(zip(X_train[:, j], y_train, weights))\n            feature_data.sort(key=lambda x: x[0])\n            \n            y_list = [y for (x, y, w) in feature_data]\n            prefix_plus = np.zeros(m + 1)\n            prefix_minus = np.zeros(m + 1)\n            \n            for i in range(m):\n                prefix_plus[i+1] = prefix_plus[i] + (1 if y_list[i] == 1 else 0) * feature_data[i][2]\n                prefix_minus[i+1] = prefix_minus[i] + (1 if y_list[i] == -1 else 0) * feature_data[i][2]\n            \n            min_error = 1.0\n            best_k = 0\n            \n            for k in range(m + 1):\n                error_left = prefix_plus[k]\n                error_right = prefix_minus[m] - prefix_minus[k]\n                total_error = error_left + error_right\n                \n                if total_error < min_error:\n                    min_error = total_error\n                    best_k = k\n            \n            if best_k == 0:\n                threshold = -np.inf\n            elif best_k == m:\n                threshold = np.inf\n            else:\n                threshold = (feature_data[best_k-1][0] + feature_data[best_k][0]) / 2\n            \n            if min_error < best_error:\n                best_error = min_error\n                best_feature = j\n                best_threshold = threshold\n        \n        epsilon_t = best_error\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        \n        for i in range(m):\n            x_i = X_train[i][best_feature]\n            if x_i > best_threshold:\n                h = 1\n            else:\n                h = -1\n            \n            if h == y_train[i]:\n                weights[i] *= np.exp(-alpha_t)\n            else:\n                weights[i] *= np.exp(alpha_t)\n        \n        classifiers.append((best_feature, best_threshold, alpha_t))\n    \n    predictions = []\n    for x in X_test:\n        sum_vote = 0.0\n        for (j, t, alpha) in classifiers:\n            if x[j] > t:\n                h = 1\n            else:\n                h = -1\n            sum_vote += alpha * h\n        \n        if sum_vote > 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for all training examples are initialized to be equal.\n2. **Feature Evaluation**: For each feature, the data is sorted, and the best threshold is found by evaluating all possible split points. The error for each split point is computed, and the threshold with the minimum error is selected.\n3. **Weight Update**: After selecting the best weak classifier, the weights of the training examples are updated based on whether they were correctly classified. Misclassified examples receive higher weights to focus more attention on them in subsequent iterations.\n4. **Prediction**: For each test sample, the weighted sum of predictions from all weak classifiers is computed. The sign of this sum determines the final prediction, ensuring that the majority vote from the weak classifiers is considered.\n\nThis approach efficiently combines weak classifiers to form a strong classifier, leveraging the power of adaptive boosting to improve performance."}
{"task_id": 128, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the multi-class Linear Discriminant Analysis (LDA) to project the data onto the most discriminant directions. The goal is to find a linear projection that maximizes the between-class scatter while minimizing the within-class scatter.\n\n### Approach\n1. **Compute Within-class Scatter Matrix (S_W):** This matrix captures the variability within each class. For each class, compute the mean vector, then for each sample in the class, compute the outer product of the difference between the sample and the class mean, and sum these products.\n\n2. **Compute Between-class Scatter Matrix (S_B):** This matrix captures the variability between classes. Compute the global mean of all samples, then for each class, compute the difference between the class mean and the global mean, multiply by the number of samples in the class, and sum these outer products.\n\n3. **Form Matrix A:** Compute the matrix A as the product of the pseudo-inverse of S_W and S_B. This matrix is used to find the discriminant directions.\n\n4. **Eigen-decomposition of A:** Use numpy's `eigh` function to compute the eigenvalues and eigenvectors of A. Sort these in descending order of eigenvalues.\n\n5. **Normalize and Adjust Eigenvectors:** Normalize each eigenvector to unit length and adjust the sign to ensure deterministic results.\n\n6. **Project Data:** Project the original data onto the selected eigenvectors and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components == 0:\n        return []\n    \n    # Step 1: Compute within-class scatter matrix S_W\n    unique_labels = np.unique(y)\n    class_dict = {label: X[y == label] for label in unique_labels}\n    \n    d = X.shape[1]\n    S_W = np.zeros((d, d))\n    for label in unique_labels:\n        samples = class_dict[label]\n        n_c = samples.shape[0]\n        mu_c = np.mean(samples, axis=0)\n        for x in samples:\n            x_minus_mu = x - mu_c\n            S_W += np.outer(x_minus_mu, x_minus_mu)\n    \n    # Step 2: Compute between-class scatter matrix S_B\n    mu = np.mean(X, axis=0)\n    S_B = np.zeros((d, d))\n    for label in unique_labels:\n        samples = class_dict[label]\n        n_c = samples.shape[0]\n        mu_c = np.mean(samples, axis=0)\n        term = (mu_c - mu) * n_c\n        S_B += np.outer(term, term)\n    \n    # Step 3: Compute matrix A = pinv(S_W) * S_B\n    pinv_S_W = np.linalg.pinv(S_W)\n    A = pinv_S_W @ S_B\n    \n    # Step 4: Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    \n    # Reverse to get descending order\n    eigenvalues = eigenvalues[::-1]\n    eigenvectors = eigenvectors[:, ::-1]\n    \n    # Select top n_components\n    selected_eigenvalues = eigenvalues[:n_components]\n    selected_eigenvectors = eigenvectors[:, :n_components]\n    \n    # Normalize each eigenvector to unit length\n    for i in range(selected_eigenvectors.shape[1]):\n        vec = selected_eigenvectors[:, i]\n        norm = np.linalg.norm(vec)\n        if norm != 0:\n            selected_eigenvectors[:, i] = vec / norm\n    \n    # Fix the sign of each eigenvector\n    for i in range(selected_eigenvectors.shape[1]):\n        vec = selected_eigenvectors[:, i]\n        for j in range(len(vec)):\n            if vec[j] != 0:\n                if vec[j] < 0:\n                    selected_eigenvectors[:, i] = -vec\n                break  # No need to check further\n    \n    # Project X onto selected eigenvectors\n    projected_X = X @ selected_eigenvectors\n    \n    # Round to four decimal places\n    projected_X_rounded = np.round(projected_X, 4)\n    \n    # Convert to list of lists\n    result = projected_X_rounded.tolist()\n    \n    return result\n```\n\n### Explanation\n1. **Within-class Scatter Matrix (S_W):** This matrix is computed by summing the outer products of the differences between each sample and the mean of its class. It captures the variability within each class.\n\n2. **Between-class Scatter Matrix (S_B):** This matrix is computed by summing the outer products of the differences between the mean of each class and the global mean, scaled by the number of samples in the class. It captures the variability between classes.\n\n3. **Matrix A:** This matrix is formed by multiplying the pseudo-inverse of S_W with S_B. It is used to find the discriminant directions.\n\n4. **Eigen-decomposition:** The eigenvalues and eigenvectors of A are computed. The eigenvectors corresponding to the largest eigenvalues are selected as the discriminant directions.\n\n5. **Normalization and Sign Adjustment:** Each eigenvector is normalized to unit length and adjusted for sign consistency to ensure deterministic results.\n\n6. **Projection:** The original data is projected onto the selected eigenvectors, and the results are rounded to four decimal places for the final output."}
{"task_id": 140, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to navigate an agent through a maze while avoiding spreading fire. The maze is represented as an \\(n \\times n\\) grid where each cell can be a free cell, a wall, or on fire. The agent starts at the top-left corner and aims to reach the bottom-right corner. The fire spreads to adjacent cells each minute before the agent moves. The goal is to determine the minimum time required for the agent to reach the destination safely or determine if it's impossible.\n\n### Approach\n1. **Initial Checks**: \n   - If the starting or ending cell is on fire, return -1 immediately.\n   - If the grid is 1x1 and the cell is free, return 0 as the agent is already at the goal.\n\n2. **Precompute Fire Spread**:\n   - Use a multi-source BFS to compute the earliest time each cell catches fire. This helps in determining if a cell is safe for the agent to move into at a given time.\n\n3. **BFS for Agent Movement**:\n   - Use BFS to explore the agent's movement level by level, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival.\n\n### Solution Code\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    # Check if start or end is on fire at t=0\n    if grid[0][0] == 'F' or grid[-1][-1] == 'F':\n        return -1\n    # Check 1x1 case\n    if n == 1:\n        return 0\n    \n    # Precompute fire_time for each cell\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        i, j = q.popleft()\n        for dx, dy in dirs:\n            ni, nj = i + dx, j + dy\n            if 0 <= ni < n and 0 <= nj < n:\n                if fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    \n    # BFS for the agent's movement\n    visited = [[math.inf for _ in range(n)] for _ in range(n)]\n    visited[0][0] = 0\n    q = deque()\n    q.append((0, 0))\n    agent_dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    while q:\n        x, y = q.popleft()\n        current_time = visited[x][y]\n        # Check if reached the end\n        if x == n - 1 and y == n - 1:\n            return current_time\n        for dx, dy in agent_dirs:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] == '#':\n                    continue\n                if fire_time[nx][ny] <= current_time + 1:\n                    continue\n                if visited[nx][ny] > current_time + 1:\n                    visited[nx][ny] = current_time + 1\n                    q.append((nx, ny))\n    return -1\n```\n\n### Explanation\n1. **Initial Checks**: The function first checks if the starting or ending cell is on fire, returning -1 if true. It also handles the 1x1 grid case directly.\n\n2. **Fire Spread Calculation**: Using a multi-source BFS, the function calculates the earliest time each cell catches fire. This helps in determining the safety of each cell at any given time.\n\n3. **Agent Movement**: The agent's movement is explored using BFS, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival. The BFS ensures that the earliest time to reach the goal is found efficiently.\n\nThis approach ensures that the agent navigates the maze safely while avoiding the spreading fire, providing the minimum time required or determining if it's impossible to reach the goal."}
{"task_id": 141, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier. The goal is to predict the class of each test sample based on the majority vote of its k nearest neighbors in the training set. The distance between samples can be computed using one of three metrics: Euclidean, Manhattan, or Cosine. If an unknown metric is provided, we default to Euclidean.\n\n### Approach\n1. **Compute Distances**: For each test sample, compute the distance to every training sample using the specified metric. The distance metrics are:\n   - **Euclidean**: L2 distance, computed as the square root of the sum of squared differences.\n   - **Manhattan**: L1 distance, computed as the sum of absolute differences.\n   - **Cosine**: 1 minus the cosine similarity, computed as 1 minus the dot product of the vectors divided by the product of their magnitudes.\n\n2. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances. This is done efficiently using NumPy's `argpartition` to find the indices of the k smallest distances without fully sorting the distances.\n\n3. **Majority Vote with Tie-breaking**: For each test sample, collect the labels of the k nearest neighbors. Determine the most frequent label, and in case of a tie, select the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement the k-Nearest Neighbors classifier.\"\"\"\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    m = X_test.shape[0]\n    n = X_train.shape[0]\n    \n    # Compute the distance matrix based on the metric\n    if metric == 'euclidean':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        squared_diff = differences ** 2  # m x n x d\n        sum_squared = squared_diff.sum(axis=2)  # m x n\n        distances = np.sqrt(sum_squared)  # m x n\n    elif metric == 'manhattan':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        abs_diff = np.abs(differences)  # m x n x d\n        sum_abs = abs_diff.sum(axis=2)  # m x n\n        distances = sum_abs  # m x n\n    elif metric == 'cosine':\n        dot_products = X_test @ X_train.T  # m x n\n        norms_test = np.linalg.norm(X_test, axis=1)  # m x 1\n        norms_train = np.linalg.norm(X_train, axis=1)  # n x 1\n        cosine_similarities = dot_products / (norms_test[:, np.newaxis] * norms_train[np.newaxis, :])\n        distances = 1 - cosine_similarities  # m x n\n    \n    # Find the indices of the k nearest neighbors for each test sample\n    knn_indices = np.zeros((m, k), dtype=int)\n    for i in range(m):\n        dist_row = distances[i]\n        # Use argpartition to find the k smallest indices\n        k_smallest = np.argpartition(dist_row, k-1)[:k]\n        knn_indices[i] = k_smallest\n    \n    # Determine the predicted labels using majority vote with tie-breaking\n    y_knn = np.zeros(m, dtype=y_train.dtype)\n    for i in range(m):\n        indices = knn_indices[i]\n        y_group = y_train[indices]\n        unique_labels, label_counts = np.unique(y_group, return_counts=True)\n        max_count = np.max(label_counts)\n        candidates = unique_labels[label_counts == max_count]\n        predicted_label = np.min(candidates)\n        y_knn[i] = predicted_label\n    \n    return y_knn\n```\n\n### Explanation\n1. **Distance Calculation**: The function first computes the distance matrix between each test sample and training sample using the specified metric. This is done efficiently using NumPy's vectorized operations.\n\n2. **Finding Nearest Neighbors**: For each test sample, the function identifies the k nearest neighbors by finding the indices of the smallest distances using `argpartition`, which is more efficient than sorting for large datasets.\n\n3. **Majority Vote**: For each test sample, the function collects the labels of the k nearest neighbors and determines the most frequent label. In case of a tie, the smallest label is chosen.\n\nThis approach ensures that the function efficiently computes the predictions while handling different distance metrics and tie-breaking scenarios correctly."}
{"task_id": 146, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbours (k-NN) classifier from scratch. The function will take a training set, a test set, a neighborhood size k, and a distance metric, and it will return the predicted labels for all test samples.\n\n### Approach\nThe k-NN algorithm works by finding the k closest training samples to each test sample based on a specified distance metric. The predicted label for each test sample is determined by the majority vote of the labels of these k nearest neighbors. In case of a tie, the smallest label is chosen.\n\nThe steps involved in the solution are as follows:\n1. **Compute Distances**: For each test sample, compute its distance to every training sample using the specified metric (Euclidean, Manhattan, or Cosine).\n2. **Find Nearest Neighbors**: Sort the training samples based on the computed distances and select the top k samples.\n3. **Majority Vote**: Determine the most common label among the k nearest neighbors. In case of a tie, select the smallest label.\n4. **Return Predictions**: Collect the predicted labels for all test samples and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    n_samples, n_features = X.shape\n    m_samples = X_test.shape[0]\n    predicted_labels = []\n    \n    for x_test in X_test:\n        distances = []\n        \n        if metric == 'euclidean':\n            diff = X - x_test\n            squared_diff = diff ** 2\n            sum_squared = np.sum(squared_diff, axis=1)\n            distances = np.sqrt(sum_squared)\n        elif metric == 'manhattan':\n            diff = np.abs(X - x_test)\n            sum_diff = np.sum(diff, axis=1)\n            distances = sum_diff\n        elif metric == 'cosine':\n            dot = np.dot(X, x_test)\n            mag_x = np.linalg.norm(x_test)\n            mag_X = np.linalg.norm(X, axis=1)\n            denominator = mag_x * mag_X + 1e-12\n            cosine_similarity = dot / denominator\n            distances = 1 - cosine_similarity\n        \n        neighbors = list(zip(distances, y))\n        neighbors.sort(key=lambda x: x[0])\n        \n        k = min(k, len(neighbors))\n        top_k = neighbors[:k]\n        \n        labels = [label for (d, label) in top_k]\n        \n        counts = defaultdict(int)\n        for label in labels:\n            counts[label] += 1\n        \n        max_count = max(counts.values())\n        candidates = [label for label, cnt in counts.items() if cnt == max_count]\n        predicted_label = min(candidates)\n        \n        predicted_labels.append(predicted_label)\n    \n    return predicted_labels\n```\n\n### Explanation\n1. **Distance Calculation**: For each test sample, the distance to each training sample is computed using the specified metric. This is done efficiently using NumPy operations to handle vectorized calculations.\n2. **Nearest Neighbors Selection**: The computed distances are sorted, and the top k neighbors are selected. If k exceeds the number of training samples, all samples are used.\n3. **Majority Vote**: The labels of the selected neighbors are counted, and the most frequent label is chosen. In case of a tie, the smallest label is selected.\n4. **Efficiency**: The use of NumPy for vectorized operations ensures that the distance calculations are efficient, even for larger datasets.\n\nThis approach ensures that the k-NN algorithm is implemented efficiently and correctly, adhering to the specified constraints and requirements."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a two-hidden-layer actor network in a reinforcement learning context. The network takes a state vector as input and outputs an action vector after passing through three layers with specific activation functions.\n\n### Approach\nThe network architecture consists of three layers:\n1. The first layer is a linear transformation followed by a ReLU activation.\n2. The second layer is another linear transformation followed by a ReLU activation.\n3. The third layer is a linear transformation followed by a tanh activation, which ensures the output values are within the range [-1, 1].\n\nThe steps to compute the forward pass are as follows:\n1. Convert the input state vector into a NumPy array to handle both list and array inputs uniformly.\n2. Compute the output of the first hidden layer by applying the linear transformation using the first weight matrix and bias vector, followed by the ReLU activation.\n3. Compute the output of the second hidden layer similarly using the second weight matrix and bias vector, followed by another ReLU activation.\n4. Compute the final action vector using the third weight matrix and bias vector, followed by the tanh activation.\n5. Round the resulting action vector to four decimal places and convert it to a list for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.asarray(state)\n    \n    # First hidden layer\n    h1 = np.dot(state, weights['W1']) + weights['b1']\n    h1_relu = np.maximum(0, h1)\n    \n    # Second hidden layer\n    h2 = np.dot(h1_relu, weights['W2']) + weights['b2']\n    h2_relu = np.maximum(0, h2)\n    \n    # Output layer\n    action = np.dot(h2_relu, weights['W3']) + weights['b3']\n    action_tanh = np.tanh(action)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_action = np.round(action_tanh, 4).tolist()\n    \n    return rounded_action\n```\n\n### Explanation\n1. **Input Handling**: The input state is converted to a NumPy array to ensure consistent handling of both list and array inputs.\n2. **First Hidden Layer**: The state vector is transformed using the first weight matrix and bias vector. The ReLU activation function is applied to introduce non-linearity.\n3. **Second Hidden Layer**: The output from the first hidden layer is transformed using the second weight matrix and bias vector, followed by another ReLU activation.\n4. **Output Layer**: The output from the second hidden layer is transformed using the third weight matrix and bias vector. The tanh activation function is applied to ensure the output values are within the range [-1, 1].\n5. **Rounding and Conversion**: The resulting action vector is rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the forward pass is computed efficiently using NumPy operations, adhering to the specified network architecture and activation functions."}
{"task_id": 160, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Lasso regression using the coordinate descent optimization strategy. Lasso regression is a type of linear regression that includes an L1 regularization term, which helps in feature selection by shrinking less important feature coefficients to zero.\n\n### Approach\n1. **Add Intercept Column**: If `fit_intercept` is True, add an all-ones column to the feature matrix X. Otherwise, add an all-zeros column. This ensures that the intercept is always the first column and is not included in the L1 penalty.\n2. **Initialize Weights**: Initialize the weight vector to zeros. The weight vector includes the intercept term if `fit_intercept` is True.\n3. **Precompute Sum of Squares**: Precompute the sum of squares for each feature column to avoid redundant calculations during the coordinate descent iterations.\n4. **Coordinate Descent Loop**: For each iteration, update the intercept (if `fit_intercept` is True) by setting it to the mean of the residuals. Then, for each feature, update its weight using the soft-thresholding operator.\n5. **Soft-Thresholding Operator**: This operator is used to update each weight by considering the partial residual and the regularization strength. It ensures that weights are shrunk towards zero, which helps in feature selection.\n6. **Return Results**: After completing the iterations, return the weights and intercept, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    # Add intercept column\n    if fit_intercept:\n        intercept = np.ones(X.shape[0])\n    else:\n        intercept = np.zeros(X.shape[0])\n    X = np.hstack((intercept[:, np.newaxis], X))\n    \n    m, n_features = X.shape\n    w = np.zeros(n_features)\n    \n    # Precompute sum of squares for each feature column (excluding intercept)\n    sum_squares = np.sum(X[:, 1:] ** 2, axis=0)\n    \n    for _ in range(max_iters):\n        if fit_intercept:\n            residuals = y - X @ w\n            w[0] = np.mean(residuals)\n        \n        for j in range(1, n_features):\n            # Compute partial residual r_j\n            r_j = y - (X @ w) + X[:, j] * w[j]\n            \n            # Compute xj . rj\n            xj_dot_rj = X[:, j].T.dot(r_j)\n            \n            # Get denominator\n            denominator = sum_squares[j - 1]\n            \n            if denominator == 0:\n                w[j] = 0.0\n                continue\n            \n            # Compute tau\n            tau = lambda_param * m\n            \n            # Soft thresholding\n            s = np.sign(xj_dot_rj) * np.maximum(np.abs(xj_dot_rj) - tau, 0)\n            \n            # Update weight\n            w[j] = s / denominator\n    \n    # Round the results to 4 decimal places\n    weights = w[1:].round(4).tolist()\n    bias = w[0].round(4)\n    \n    return (weights, bias)\n```\n\n### Explanation\n- **Intercept Handling**: The intercept is added as a column to the feature matrix. If `fit_intercept` is True, it is initialized to ones and updated in each iteration. If False, it is initialized to zeros and remains fixed.\n- **Weight Initialization**: The weight vector is initialized to zeros, including the intercept term if applicable.\n- **Sum of Squares Precomputation**: This step optimizes the computation by precalculating the sum of squares for each feature column, which is used in the soft-thresholding update step.\n- **Coordinate Descent**: For each feature, the partial residual is computed, and the weight is updated using the soft-thresholding operator, which helps in shrinking the weights towards zero, thus performing feature selection.\n- **Result Rounding**: The final weights and intercept are rounded to four decimal places for the output.\n\nThis approach ensures that the Lasso regression is efficiently computed using coordinate descent, providing a balance between model fit and feature selection."}
{"task_id": 165, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a measure of how many positions two vectors differ, normalized by the length of the vectors. This distance is widely used in various fields such as information theory and data science.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input vectors are the same. If they are not, we return -1 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty, the Hamming distance is 0.0 because there are no elements to compare.\n3. **Convert to NumPy Arrays**: If the inputs are lists, we convert them to NumPy arrays for efficient element-wise operations.\n4. **Compute Differences**: Using NumPy, we compute a boolean array where each element is True if the corresponding elements in the input vectors differ.\n5. **Sum Differences**: We sum the True values in the boolean array to get the count of differing elements.\n6. **Normalize and Round**: We normalize the count by dividing by the length of the vectors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    N = len(x)\n    \n    if N == 0:\n        return 0.0\n    \n    # Convert to NumPy arrays if they are lists\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Compute element-wise differences\n    diff = x != y\n    \n    # Sum the Trues (count of differing elements)\n    sum_diff = np.sum(diff)\n    \n    # Calculate the normalized Hamming distance\n    distance = sum_diff / N\n    \n    # Round to 4 decimal places\n    return round(distance, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the input vectors have the same length. If not, it returns -1.\n- **Empty Vectors**: If both vectors are empty, the function returns 0.0 since there are no differing elements.\n- **Conversion to Arrays**: The function converts lists to NumPy arrays to leverage efficient element-wise operations.\n- **Element-wise Comparison**: Using NumPy, the function creates a boolean array indicating where elements differ.\n- **Summing Differences**: The sum of the boolean array gives the count of differing elements.\n- **Normalization and Rounding**: The count is divided by the vector length to get the normalized Hamming distance, which is then rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently computes the normalized Hamming distance while handling various input types and edge cases appropriately."}
{"task_id": 169, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate coefficients for several common symmetric window functions used in digital signal processing. The function should handle different window types and return the coefficients as a list of floats, each rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: Check if the input parameters are valid. Specifically, ensure that `N` is a positive integer and that the window type is one of the supported types.\n2. **Special Case Handling**: If `N` is 1, return a single value of 1.0 as the window coefficient.\n3. **Window Function Calculation**: Depending on the specified window type, compute the coefficients using the respective mathematical formula:\n   - **Hamming Window**: Uses the formula \\( w[n] = 0.54 - 0.46 \\cos(2\\pi n / (N-1)) \\).\n   - **Hann Window**: Uses the formula \\( w[n] = 0.5 - 0.5 \\cos(2\\pi n / (N-1)) \\).\n   - **Blackman-Harris Window**: Uses the formula \\( w[n] = a_0 - a_1 \\cos(2\\pi n / (N-1)) + a_2 \\cos(4\\pi n / (N-1)) - a_3 \\cos(6\\pi n / (N-1)) \\) with specific coefficients.\n   - **Generalized Cosine Window**: Uses a series of cosine terms with coefficients provided by the user.\n4. **Rounding**: Round each computed coefficient to four decimal places before returning the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Check if N is a positive integer\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer\")\n    \n    # Check if window is valid\n    if window not in [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]:\n        raise ValueError(\"Unknown window type\")\n    \n    # Handle N=1 case\n    if N == 1:\n        return [1.0]\n    \n    window_coeffs = []\n    \n    if window == \"hamming\":\n        for n in range(N):\n            coeff = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n            window_coeffs.append(coeff)\n    elif window == \"hann\":\n        for n in range(N):\n            coeff = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n            window_coeffs.append(coeff)\n    elif window == \"blackman_harris\":\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        for n in range(N):\n            term1 = a0\n            term2 = -a1 * np.cos(2 * np.pi * n / (N - 1))\n            term3 = a2 * np.cos(4 * np.pi * n / (N - 1))\n            term4 = -a3 * np.cos(6 * np.pi * n / (N - 1))\n            coeff = term1 + term2 + term3 + term4\n            window_coeffs.append(coeff)\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Coefficients must be provided for generalized cosine window\")\n        M = len(coefficients)\n        for n in range(N):\n            total = 0.0\n            for k in range(M):\n                a_k = coefficients[k]\n                term = a_k * np.cos(2 * np.pi * k * n / (N - 1))\n                total += term\n            window_coeffs.append(total)\n    \n    # Round all coefficients to 4 decimal places\n    rounded_coeffs = [round(c, 4) for c in window_coeffs]\n    \n    return rounded_coeffs\n```\n\n### Explanation\nThe function `generate_window` starts by validating the input parameters. It then handles the special case where `N` is 1 by returning a single coefficient of 1.0. For other values of `N`, it computes the window coefficients based on the specified window type using the respective mathematical formulas. Each coefficient is rounded to four decimal places before being returned as a list. This approach ensures that the function efficiently generates the required window coefficients for various types, adhering to the specified precision and constraints."}
{"task_id": 171, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression classifier using mini-batch gradient descent. The goal is to train the model on a given training set and use it to predict labels for a test set. The model should handle any two distinct numeric labels and return predictions in the same format as the training labels.\n\n### Approach\n1. **Data Preprocessing**: Convert the input data into a format suitable for training. This includes adding an intercept term to the feature matrix to account for the bias in the model.\n2. **Label Mapping**: Map the given labels to binary values (0 and 1) for internal processing. This allows the model to work with any two distinct numeric labels.\n3. **Special Case Handling**: If all training labels are the same, the model can trivially predict that label for all test samples without training.\n4. **Model Initialization**: Initialize the weight vector to zeros. This vector will be updated during the training process.\n5. **Training with Mini-Batch Gradient Descent**: Divide the training data into mini-batches and update the weights iteratively using the gradient of the loss function. This approach helps in efficiently finding the minimum of the loss function.\n6. **Prediction**: After training, use the model to predict labels for the test set by applying the logistic function and using a decision threshold of 0.5. Convert the predictions back to the original label format before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Handle the case where all labels are the same\n    unique_labels = sorted(list(set(y_train)))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Create a mapping from original labels to 0 and 1\n    label_to_idx = {unique_labels[0]: 0, unique_labels[1]: 1}\n    y_train_mapped = [label_to_idx[label] for label in y_train]\n    \n    # Convert to numpy arrays and add intercept\n    X_train_np = np.array(X_train)\n    X_train_np = np.hstack([X_train_np, np.ones((X_train_np.shape[0], 1))])\n    \n    X_test_np = np.array(X_test)\n    X_test_np = np.hstack([X_test_np, np.ones((X_test_np.shape[0], 1))])\n    \n    # Initialize weights\n    n_features = X_train_np.shape[1]\n    w = np.zeros((n_features, 1))\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Shuffle the data\n        indices = np.random.permutation(len(X_train_np))\n        X_shuffled = X_train_np[indices]\n        y_shuffled = np.array(y_train_mapped)[indices]\n        \n        # Process in batches\n        for i in range(0, len(X_shuffled), batch_size):\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n            \n            # Compute the activation\n            z = np.dot(X_batch, w)\n            a = 1 / (1 + np.exp(-z))\n            \n            # Compute the gradient\n            gradient = np.dot(X_batch.T, (a - y_batch)) / len(X_batch)\n            \n            # Update the weights\n            w = w - learning_rate * gradient\n    \n    # Make predictions on the test set\n    z = np.dot(X_test_np, w)\n    a = 1 / (1 + np.exp(-z))\n    predicted_classes = (a >= 0.5).astype(int)\n    \n    # Map back to original labels\n    original_predictions = [unique_labels[pred] for pred in predicted_classes]\n    \n    return original_predictions\n```\n\n### Explanation\n1. **Data Preprocessing**: The feature matrices `X_train` and `X_test` are converted to numpy arrays and an intercept term is added to each row. This allows the model to learn the bias term.\n2. **Label Mapping**: The labels are mapped to binary values (0 and 1) to simplify the training process. After training, the predictions are converted back to the original label format.\n3. **Special Case Handling**: If all training labels are the same, the model returns that label for all test samples without training.\n4. **Model Initialization**: The weight vector is initialized to zeros, providing a starting point for the optimization process.\n5. **Training with Mini-Batch Gradient Descent**: The training data is shuffled and processed in mini-batches. For each batch, the gradient of the loss function is computed and used to update the weights. This process is repeated for the specified number of epochs.\n6. **Prediction**: After training, the model predicts labels for the test set by applying the logistic function and using a decision threshold of 0.5. The predictions are then converted back to the original label format and returned."}
{"task_id": 176, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to train an ensemble of decision stumps and use them to make predictions on a test dataset.\n\n### Approach\n1. **Convert Labels**: Convert the given labels from {0, 1} to {-1, 1} for easier computation.\n2. **Initialize Weights**: Start with equal weights for all training samples.\n3. **Train Weak Learners**: For each boosting round, find the best decision stump by minimizing the weighted classification error. Update the sample weights based on the performance of the selected stump.\n4. **Early Stopping**: If a perfect stump (with zero error) is found, stop training early.\n5. **Make Predictions**: For each test sample, aggregate the weighted votes from all trained stumps and convert the result to class labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"Implement AdaBoost with decision stumps as weak learners.\"\"\"\n    # Convert y_train to -1 and 1\n    y_train = [1 if label == 1 else -1 for label in y_train]\n    m = len(X_train)\n    if m == 0:\n        return []\n    \n    # Initialize weights\n    weights = np.ones(m) / m\n    \n    stumps = []  # List to store (feature, threshold, polarity, alpha)\n    \n    for round in range(n_estimators):\n        best_error = 1.0\n        best_stump = None\n        \n        for j in range(len(X_train[0])):\n            # Collect feature data for current feature j\n            feature_data = []\n            for i in range(m):\n                x = X_train[i][j]\n                y = y_train[i]\n                w = weights[i]\n                feature_data.append((x, y, w))\n            \n            # Sort by x value\n            feature_data.sort(key=lambda x: x[0])\n            x_list = [fd[0] for fd in feature_data]\n            y_list = [fd[1] for fd in feature_data]\n            w_list = [fd[2] for fd in feature_data]\n            \n            # Compute prefix sums for y=1 and y=-1\n            prefix_1 = []\n            prefix_neg1 = []\n            sum_1 = 0.0\n            sum_neg1 = 0.0\n            for i in range(len(feature_data)):\n                if y_list[i] == 1:\n                    sum_1 += w_list[i]\n                else:\n                    sum_neg1 += w_list[i]\n                prefix_1.append(sum_1)\n                prefix_neg1.append(sum_neg1)\n            total_1 = sum_1\n            total_neg1 = sum_neg1\n            \n            # Find minimal error for p=1\n            min_error_p1 = 1.0\n            best_t_p1 = None\n            for i in range(len(x_list)):\n                t = x_list[i]\n                sum_neg1_less = prefix_neg1[i]\n                sum_1_ge = total_1 - prefix_1[i]\n                error = sum_neg1_less + sum_1_ge\n                if error < min_error_p1:\n                    min_error_p1 = error\n                    best_t_p1 = t\n            \n            # Find minimal error for p=-1\n            min_error_p_neg1 = 1.0\n            best_t_p_neg1 = None\n            for i in range(len(x_list)):\n                t = x_list[i]\n                sum_1_less = prefix_1[i]\n                sum_neg1_ge = total_neg1 - prefix_neg1[i]\n                error = sum_1_less + sum_neg1_ge\n                if error < min_error_p_neg1:\n                    min_error_p_neg1 = error\n                    best_t_p_neg1 = t\n            \n            # Choose better polarity for this feature\n            if min_error_p1 < min_error_p_neg1:\n                current_error = min_error_p1\n                current_t = best_t_p1\n                current_p = 1\n            else:\n                current_error = min_error_p_neg1\n                current_t = best_t_p_neg1\n                current_p = -1\n            \n            # Update best_stump if current is better\n            if current_error < best_error:\n                best_error = current_error\n                best_stump = (j, current_t, current_p)\n            elif current_error == best_error:\n                # Tie-breaker: feature index, threshold, polarity\n                if j < best_stump[0]:\n                    best_error = current_error\n                    best_stump = (j, current_t, current_p)\n                elif j == best_stump[0]:\n                    if current_t < best_stump[1]:\n                        best_error = current_error\n                        best_stump = (j, current_t, current_p)\n                    elif current_t == best_stump[1]:\n                        if current_p == 1 and best_stump[2] == -1:\n                            best_error = current_error\n                            best_stump = (j, current_t, current_p)\n        \n        if best_stump is None:\n            break  # No stumps found, though this shouldn't happen\n        \n        # Compute alpha_t\n        if best_error == 0:\n            alpha_t = 0.5 * np.log((1 - best_error) / (best_error + 1e-10))\n        else:\n            alpha_t = 0.5 * np.log((1 - best_error) / (best_error + 1e-10))\n        \n        # Update weights\n        for i in range(m):\n            x = X_train[i][best_stump[0]]\n            t = best_stump[1]\n            p = best_stump[2]\n            if (x < t and p == 1) or (x >= t and p == -1):\n                h = 1\n            else:\n                h = -1\n            weights[i] *= np.exp(-alpha_t * y_train[i] * h)\n        \n        # Normalize weights\n        sum_weights = sum(weights)\n        if sum_weights == 0:\n            weights = np.ones(m) / m\n        else:\n            weights = [w / sum_weights for w in weights]\n        \n        # Store the stump and alpha\n        stumps.append((best_stump[0], best_stump[1], best_stump[2], alpha_t))\n        \n        # Early stopping if perfect stump found\n        if best_error == 0:\n            break\n    \n    # Make predictions on X_test\n    predictions = []\n    for x in X_test:\n        sum_vote = 0.0\n        for (j, t, p, alpha) in stumps:\n            x_j = x[j]\n            if (x_j < t and p == 1) or (x_j >= t and p == -1):\n                h = 1\n            else:\n                h = -1\n            sum_vote += alpha * h\n        if sum_vote >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    return predictions\n```\n\n### Explanation\n1. **Convert Labels**: The labels are converted from {0, 1} to {-1, 1} to simplify the computation of errors.\n2. **Initialize Weights**: Each sample starts with equal weight, ensuring that all samples are considered equally important initially.\n3. **Train Weak Learners**: For each boosting round, the algorithm evaluates all possible decision stumps (each feature, threshold, and polarity) to find the one with the smallest weighted error. The weights are then updated based on the performance of the selected stump.\n4. **Early Stopping**: If a perfect stump is found (error = 0), the training stops early to save computation time.\n5. **Make Predictions**: For each test sample, the algorithm aggregates the weighted votes from all trained stumps and converts the result to class labels {0, 1} based on the sign of the aggregated votes.\n\nThis approach ensures that the algorithm efficiently trains a strong classifier by combining weak learners, each contributing to the final prediction based on their performance."}
{"task_id": 178, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as \\( f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2 \\) where each \\( x_i \\) is within the range \\([-1, 1]\\). The goal is to find the minimum value of this function using PSO.\n\n### Approach\n1. **Problem Analysis**: The Sphere function is a simple, continuous, and differentiable function used to test optimization algorithms. The PSO algorithm is a population-based optimization technique that simulates the social behavior of birds or fish. Each particle in the swarm has a position and velocity, and it updates its position based on its own best position and the best position of the entire swarm.\n\n2. **Algorithm Setup**: \n   - **Initialization**: Each particle starts with a random position within the specified bounds and a velocity of zero.\n   - **Velocity Update**: The velocity of each particle is updated using the inertia weight, cognitive weight, and social weight, along with random numbers.\n   - **Position Update**: The position of each particle is updated by adding the new velocity, ensuring it stays within the bounds.\n   - **Personal Best and Global Best**: Each particle keeps track of its best position (pbest), and the swarm keeps track of the best position found so far (gbest).\n\n3. **Iteration Process**: For each iteration, all particles update their velocities and positions. After updating, each particle checks if its new position is better than its personal best. The global best is updated if any particle's personal best is better than the current global best.\n\n4. **Termination**: The algorithm runs for a specified number of iterations, and the best value found is returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for invalid inputs\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize particles' positions, velocities, and personal bests\n    positions = rng.uniform(-1, 1, (num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    pbest = positions.copy()\n\n    # Calculate initial global best\n    initial_spheres = np.array([np.sum(p**2) for p in pbest])\n    initial_best_idx = np.argmin(initial_spheres)\n    gbest = pbest[initial_best_idx].copy()\n\n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Generate random numbers\n            r1 = rng.random()\n            r2 = rng.random()\n            \n            # Update velocity\n            velocities[i] = w * velocities[i] + c1 * r1 * (pbest[i] - positions[i]) + c2 * r2 * (gbest - positions[i])\n            \n            # Update position\n            new_position = positions[i] + velocities[i]\n            new_position_clipped = np.clip(new_position, -1, 1)\n            positions[i] = new_position_clipped\n            \n            # Update personal best if new position is better\n            new_sphere = np.sum(new_position_clipped**2)\n            current_pbest_sphere = np.sum(pbest[i]**2)\n            if new_sphere < current_pbest_sphere:\n                pbest[i] = new_position_clipped\n\n        # Update global best\n        pbest_spheres = np.array([np.sum(p**2) for p in pbest])\n        best_pbest_idx = np.argmin(pbest_spheres)\n        new_gbest = pbest[best_pbest_idx]\n        new_gbest_sphere = np.sum(new_gbest**2)\n        current_gbest_sphere = np.sum(gbest**2)\n        if new_gbest_sphere < current_gbest_sphere:\n            gbest = new_gbest\n\n    # Return the best value rounded to four decimal places\n    best_value = np.sum(gbest**2)\n    return round(best_value, 4)\n```\n\n### Explanation\n- **Initialization**: The particles' initial positions are randomly set within the bounds \\([-1, 1]\\). Velocities are initialized to zero.\n- **Velocity Update**: Each particle's velocity is updated using the inertia weight, cognitive weight, and social weight, along with random numbers to ensure exploration.\n- **Position Update**: The new position is calculated by adding the updated velocity to the current position, ensuring it stays within the bounds.\n- **Personal Best Update**: If a particle's new position yields a lower Sphere function value, it updates its personal best.\n- **Global Best Update**: After each iteration, the global best is updated if any particle's personal best is better than the current global best.\n- **Termination**: The algorithm runs for the specified number of iterations, and the best value found is returned, rounded to four decimal places."}
{"task_id": 180, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbours (k-NN) algorithm that can work both as a classifier and as a regressor. The function should take training feature vectors, their corresponding target values, test feature vectors, the number of neighbours to consider (k), and the task type (classification or regression). The goal is to predict the target values for the test feature vectors based on the k nearest neighbours in the training set.\n\n### Approach\n1. **Compute Distances**: For each test feature vector, compute the Euclidean distance to each training feature vector. Since we only need the relative distances, we can use the squared Euclidean distance to save computation time.\n2. **Sort and Select Neighbours**: For each test feature vector, sort the training vectors based on their computed distances and select the top k nearest vectors. If k is 0 or larger than the number of training vectors, use all training vectors.\n3. **Predict Based on Task**:\n   - **Classification**: Determine the most frequent label among the k nearest neighbours. In case of a tie, return the smallest label.\n   - **Regression**: Compute the arithmetic mean of the target values of the k nearest neighbours and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    predictions = []\n    n_train = len(X_train)\n    \n    # Determine the effective k\n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    \n    for test_point in X_test:\n        # Compute distances to all training points\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            # Calculate squared Euclidean distance\n            dist_sq = sum((test_point[i] - xi[i])**2 for i in range(len(test_point)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort(key=lambda x: x[0])\n        \n        # Select the first 'effective_k' neighbours\n        selected = distances[:effective_k]\n        y_neighbors = [y for (d, y) in selected]\n        \n        if task == \"classification\":\n            # Count the frequency of each label\n            counter = Counter(y_neighbors)\n            # Find the most common label(s)\n            max_count = max(counter.values())\n            # Get all labels with the maximum count\n            candidates = [label for label, count in counter.items() if count == max_count]\n            # Choose the smallest label in case of a tie\n            predicted_label = min(candidates)\n            predictions.append(predicted_label)\n        elif task == \"regression\":\n            # Compute the average and round to 4 decimal places\n            average = sum(y_neighbors) / len(y_neighbors)\n            predictions.append(round(average, 4))\n        else:\n            raise ValueError(\"Task must be 'classification' or 'regression'.\")\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: For each test point, the squared Euclidean distance to each training point is computed. This avoids the computational cost of taking square roots while maintaining the order of distances.\n2. **Neighbour Selection**: The distances are sorted, and the top k (or all if k is 0 or larger than the number of training points) nearest neighbours are selected.\n3. **Classification**: The most frequent label among the selected neighbours is determined. In case of a tie, the smallest label is chosen.\n4. **Regression**: The average of the target values of the selected neighbours is computed and rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently handles both classification and regression tasks while preserving the order of the test samples in the output."}
{"task_id": 184, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary decision tree classifier using the CART algorithm from scratch. The decision tree will use Gini impurity and recursive binary splitting to make predictions. The goal is to build a tree on the training data and then use it to predict labels for unseen test data.\n\n### Approach\n1. **Gini Impurity Calculation**: The Gini impurity measures the probability of misclassifying an element. For a node, it is calculated as 1 minus the sum of the squares of the probabilities of each class.\n2. **Tree Construction**: The tree is built recursively. Each node will either be a leaf or have left and right children. The best feature and threshold for splitting are determined by the maximum Gini gain.\n3. **Splitting Rule**: For each feature, sort the values and evaluate every midpoint between consecutive different values. The feature and threshold with the highest Gini gain are chosen for the split.\n4. **Stopping Criteria**: The tree stops splitting if all labels are identical, the maximum depth is reached, or a split produces an empty child.\n5. **Prediction**: Once the tree is built, each test sample is traversed through the tree to find the predicted class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    def gini(y):\n        count_0 = np.sum(y == 0)\n        count_1 = np.sum(y == 1)\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p0 = count_0 / n\n        p1 = count_1 / n\n        return 1 - (p0 ** 2 + p1 ** 2)\n\n    def build_tree(X, y, current_depth, max_depth):\n        if len(X) == 0:\n            return {'is_leaf': True, 'prediction': 0}\n        if len(np.unique(y)) == 1:\n            return {'is_leaf': True, 'prediction': y[0]}\n        if max_depth is not None and current_depth >= max_depth:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n        \n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_X = None\n        best_left_y = None\n        best_right_X = None\n        best_right_y = None\n        \n        n_features = X.shape[1]\n        for feature in range(n_features):\n            column = X[:, feature]\n            sorted_col = np.sort(column)\n            unique = np.unique(sorted_col)\n            if len(unique) == 1:\n                continue\n            for i in range(len(unique) - 1):\n                threshold = (unique[i] + unique[i + 1]) / 2\n                left_mask = column <= threshold\n                left_X = X[left_mask]\n                left_y = y[left_mask]\n                right_X = X[~left_mask]\n                right_y = y[~left_mask]\n                \n                G_parent = gini(y)\n                G_left = gini(left_y) if len(left_y) > 0 else 0\n                G_right = gini(right_y) if len(right_y) > 0 else 0\n                \n                n_total = len(y)\n                n_left = len(left_y)\n                n_right = len(right_y)\n                if n_total == 0:\n                    continue\n                weighted_G = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n                gain = G_parent - weighted_G\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_left_X = left_X\n                    best_left_y = left_y\n                    best_right_X = right_X\n                    best_right_y = right_y\n        \n        if best_gain > 0:\n            left_child = None\n            right_child = None\n            if len(best_left_X) == 0:\n                majority = np.bincount(y).argmax()\n                left_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                left_child = build_tree(best_left_X, best_left_y, current_depth + 1, max_depth)\n            \n            if len(best_right_X) == 0:\n                majority = np.bincount(y).argmax()\n                right_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                right_child = build_tree(best_right_X, best_right_y, current_depth + 1, max_depth)\n            \n            return {\n                'is_leaf': False,\n                'feature': best_feature,\n                'threshold': best_threshold,\n                'left': left_child,\n                'right': right_child\n            }\n        else:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n\n    def predict_sample(sample, tree):\n        if tree['is_leaf']:\n            return tree['prediction']\n        else:\n            feature = tree['feature']\n            threshold = tree['threshold']\n            if sample[feature] <= threshold:\n                return predict_sample(sample, tree['left'])\n            else:\n                return predict_sample(sample, tree['right'])\n\n    tree = build_tree(X_train, y_train, 0, max_depth)\n    predictions = []\n    for sample in X_test:\n        predictions.append(predict_sample(sample, tree))\n    return predictions\n```\n\n### Explanation\n1. **Gini Impurity Calculation**: The `gini` function calculates the Gini impurity for a given set of labels.\n2. **Tree Construction**: The `build_tree` function recursively constructs the tree. It checks for stopping criteria and finds the best feature and threshold to split on based on Gini gain.\n3. **Prediction**: The `predict_sample` function traverses the tree for a given sample to determine its predicted class label.\n4. **Main Function**: The `decision_tree_predict` function builds the tree using the training data and then uses it to predict labels for the test data.\n\nThis approach ensures that the decision tree is built deterministically based on the given rules and efficiently makes predictions for unseen data."}
{"task_id": 190, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the best binary split of a given dataset based on the Gini impurity criterion. The goal is to identify the feature and threshold that minimize the weighted Gini impurity of the split. If no valid split exists, we return the Gini impurity of the entire dataset.\n\n### Approach\n1. **Compute Overall Gini Impurity**: Calculate the Gini impurity of the entire dataset. This is used as a fallback if no valid splits are found.\n2. **Iterate Over Each Feature**: For each feature, consider every unique value as a potential threshold for splitting the data.\n3. **Split Data and Compute Gini**: For each unique value, split the data into left and right subsets. Compute the Gini impurity for each subset and the weighted Gini impurity of the split.\n4. **Track the Best Split**: Keep track of the split with the minimum Gini impurity. In case of ties, prefer the split with the smaller feature index, and if those are the same, prefer the smaller threshold value.\n5. **Return Result**: If no valid splits are found, return the overall Gini impurity. Otherwise, return the best feature, threshold, and Gini impurity.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    total = len(y)\n    if total == 0:\n        return (-1, None, 0.0)\n    \n    # Compute overall Gini impurity\n    counts = np.bincount(y)\n    p = counts / total\n    g_overall = 1 - np.sum(p**2)\n    \n    best = (float('inf'), -1, None)  # (g_split, feature, threshold)\n    \n    for j in range(X.shape[1]):\n        column = X[:, j]\n        unique_values = np.unique(column)\n        \n        for v in unique_values:\n            left = column <= v\n            right = column > v\n            \n            n_left = np.sum(left)\n            n_right = np.sum(right)\n            \n            if n_left == 0 or n_right == 0:\n                continue\n            \n            # Compute Gini for left subset\n            left_y = y[left]\n            counts_left = np.bincount(left_y)\n            p_left = counts_left / n_left\n            g_left = 1 - np.sum(p_left**2)\n            \n            # Compute Gini for right subset\n            right_y = y[right]\n            counts_right = np.bincount(right_y)\n            p_right = counts_right / n_right\n            g_right = 1 - np.sum(p_right**2)\n            \n            # Compute weighted Gini for the split\n            g_split = (n_left / total) * g_left + (n_right / total) * g_right\n            \n            candidate = (g_split, j, v)\n            \n            if candidate < best:\n                best = candidate\n    \n    if best[0] == float('inf'):\n        # No valid splits found\n        return (-1, None, round(g_overall, 4))\n    else:\n        best_gini = best[0]\n        best_feature = best[1]\n        best_threshold = best[2]\n        return (best_feature, best_threshold, round(best_gini, 4))\n```\n\n### Explanation\n1. **Overall Gini Calculation**: The Gini impurity of the entire dataset is computed to use as a fallback if no valid splits are found.\n2. **Feature and Threshold Iteration**: For each feature, every unique value is considered as a potential threshold. The data is split into two subsets based on this threshold.\n3. **Gini Impurity Calculation**: For each split, the Gini impurity of both subsets is calculated. The weighted Gini impurity of the split is then computed.\n4. **Best Split Tracking**: The best split is tracked by comparing the weighted Gini impurity of each potential split. In case of ties, the feature index and threshold value are used to break the tie.\n5. **Result Handling**: If no valid splits are found, the overall Gini impurity is returned. Otherwise, the best feature, threshold, and Gini impurity are returned."}
{"task_id": 191, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a factory function that generates a callable function to compute one of several classical distance metrics between two one-dimensional numeric vectors. The function must validate the inputs and compute the distance, returning the result rounded to four decimal places or -1 if validation fails.\n\n### Approach\n1. **Factory Function**: The `metric` function takes a string `name` specifying the distance metric and returns a callable function `d`.\n2. **Validation**: The callable function `d` must validate the input vectors:\n   - Both vectors must be convertible to NumPy arrays.\n   - Both vectors must be one-dimensional.\n   - Both vectors must be of the same non-zero length.\n   - For the cosine metric, the norms of both vectors must be non-zero.\n3. **Distance Calculation**: Based on the specified metric, the callable function computes the distance using the appropriate formula:\n   - **Euclidean**: Sum of squared differences, then square root.\n   - **Manhattan**: Sum of absolute differences.\n   - **Chebyshev**: Maximum absolute difference.\n   - **Cosine**: 1 minus the cosine similarity.\n4. **Rounding**: The result is rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    supported = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n    if name not in supported:\n        def d(x, y):\n            return -1\n        return d\n\n    def d(x, y):\n        try:\n            x_arr = np.asarray(x)\n            y_arr = np.asarray(y)\n        except:\n            return -1\n\n        if x_arr.ndim != 1 or y_arr.ndim != 1:\n            return -1\n\n        if len(x_arr) != len(y_arr) or len(x_arr) == 0:\n            return -1\n\n        if name == 'cosine':\n            norm_x = np.linalg.norm(x_arr)\n            norm_y = np.linalg.norm(y_arr)\n            if norm_x == 0 or norm_y == 0:\n                return -1\n\n        if name == 'euclidean':\n            distance = np.sqrt(np.sum((x_arr - y_arr)**2))\n        elif name == 'manhattan':\n            distance = np.sum(np.abs(x_arr - y_arr))\n        elif name == 'chebyshev':\n            distance = np.max(np.abs(x_arr - y_arr))\n        elif name == 'cosine':\n            dot = np.dot(x_arr, y_arr)\n            norm_x = np.linalg.norm(x_arr)\n            norm_y = np.linalg.norm(y_arr)\n            distance = 1 - (dot / (norm_x * norm_y))\n        else:\n            return -1\n\n        return round(distance, 4)\n\n    return d\n```\n\n### Explanation\n- **Factory Function**: The `metric` function checks if the provided `name` is a supported distance metric. If not, it returns a function that always returns -1.\n- **Validation**: The callable function `d` converts inputs to NumPy arrays, checks their dimensions and lengths, and ensures they are valid for the specified metric.\n- **Distance Calculation**: Depending on the metric, the appropriate distance formula is applied. For cosine similarity, additional checks ensure non-zero norms.\n- **Rounding**: The computed distance is rounded to four decimal places before being returned, ensuring the result meets the specified precision."}
{"task_id": 197, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function `action_stats` that analyzes the action space of a given reinforcement learning environment and returns basic statistics about it. The function should handle both continuous and discrete action spaces, as well as single and multi-dimensional cases.\n\n### Approach\n1. **Identify the Action Space Type**: The function first determines whether the action space is multi-dimensional or single-dimensional by checking the presence of the `spaces` attribute. If present, it indicates a multi-dimensional space.\n2. **Determine the Number of Dimensions**: For multi-dimensional spaces, the number of dimensions is the length of the `spaces` list. For continuous spaces, it is the length of the `shape` attribute. For discrete spaces, it is 1.\n3. **Calculate Actions per Dimension**: For each dimension, the function checks if the action space is discrete or continuous. Discrete spaces contribute their number of actions, while continuous spaces contribute infinity.\n4. **Generate Action IDs**: If all dimensions are discrete, the function generates all possible action IDs using the Cartesian product of the ranges of each dimension. If any dimension is continuous, action IDs are set to `None`.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    action_space = env.action_space\n\n    # Determine the number of action dimensions\n    if hasattr(action_space, 'spaces'):\n        action_dim = len(action_space.spaces)\n    elif hasattr(action_space, 'shape'):\n        action_dim = len(action_space.shape)\n    else:\n        action_dim = 1\n\n    # Determine the number of actions per dimension\n    n_actions_per_dim = []\n    if hasattr(action_space, 'spaces'):\n        for space in action_space.spaces:\n            if hasattr(space, 'n'):\n                n_actions_per_dim.append(space.n)\n            else:\n                n_actions_per_dim.append(np.inf)\n    elif hasattr(action_space, 'n'):\n        n_actions_per_dim.append(action_space.n)\n    elif hasattr(action_space, 'shape'):\n        n_actions_per_dim = [np.inf] * action_dim\n    else:\n        n_actions_per_dim.append(1)\n\n    # Determine if all dimensions are discrete to generate action_ids\n    is_continuous = any(np.isinf(x) for x in n_actions_per_dim)\n    if is_continuous:\n        action_ids = None\n    else:\n        ranges = [range(n) for n in n_actions_per_dim]\n        action_ids = list(product(*ranges))\n\n    return (n_actions_per_dim, action_ids, action_dim)\n```\n\n### Explanation\n- **Action Space Type**: The function checks if the action space is multi-dimensional by looking for the `spaces` attribute. If found, it processes each sub-space individually.\n- **Dimensions Calculation**: The number of dimensions is determined based on whether the action space is multi-dimensional, continuous, or discrete.\n- **Actions per Dimension**: For each dimension, the function checks if it is discrete or continuous and records the number of actions accordingly.\n- **Action IDs Generation**: If all dimensions are discrete, the function generates all possible action IDs using the Cartesian product. If any dimension is continuous, it returns `None` for action IDs.\n\nThis approach ensures that the function correctly handles various types of action spaces and provides the required statistics efficiently."}
{"task_id": 198, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to update the word\u2013topic matrix \u03b2 in Latent Dirichlet Allocation (LDA) based on the current variational parameters and the corpus. The goal is to compute the maximum-likelihood estimate of \u03b2 using the given variational parameters and then normalize each column to sum to one.\n\n### Approach\n1. **Initialization**: Create a matrix \u03b2 initialized to zero with dimensions (V, T), where V is the vocabulary size and T is the number of topics.\n2. **Accumulate Sums**: For each document and each token within the document, accumulate the contributions to \u03b2 using the current variational parameters. Specifically, for each token, if it corresponds to word v, add the variational parameter value for that token and topic to \u03b2[v][t].\n3. **Normalization**: After accumulating the sums, normalize each column of \u03b2 so that the sum of each column equals one. This ensures that each column represents a valid probability distribution over words for each topic.\n4. **Rounding**: Round each entry of the normalized matrix \u03b2 to four decimal places.\n5. **Conversion**: Convert the resulting matrix from a NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    if not phi:\n        return [[0.0 for _ in range(phi[0].shape[1])] for _ in range(V)]\n    \n    T = phi[0].shape[1]\n    beta = np.zeros((V, T), dtype=np.float64)\n    \n    for d in range(len(phi)):\n        current_phi = phi[d]\n        current_corpus = corpus[d]\n        for n in range(current_phi.shape[0]):\n            v = current_corpus[n]\n            beta[v] += current_phi[n, :]\n    \n    for t in range(T):\n        sum_t = beta[:, t].sum()\n        if sum_t == 0:\n            beta[:, t] = 0.0\n        else:\n            beta[:, t] /= sum_t\n    \n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrix \u03b2 is initialized to zero to store the accumulated sums for each word-topic pair.\n2. **Accumulation**: For each document and each token, the corresponding word index is used to update the \u03b2 matrix with the variational parameter values. This step aggregates the contributions of each token to the word-topic probabilities.\n3. **Normalization**: Each column of \u03b2 is normalized to ensure that the sum of probabilities for each topic equals one, which is necessary for a valid probability distribution.\n4. **Rounding**: The values in \u03b2 are rounded to four decimal places to meet the problem's requirements.\n5. **Conversion**: The resulting matrix is converted from a NumPy array to a list of lists for the final output, ensuring the correct format is returned.\n\nThis approach efficiently computes the updated word-topic distribution matrix \u03b2, ensuring it adheres to the constraints and requirements of the LDA model."}
{"task_id": 202, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Partitioning Around Medoids (PAM) clustering algorithm. PAM is a clustering technique that uses medoids, which are actual data points, as cluster centers. The algorithm starts with an initial set of medoids and iteratively improves the clustering by swapping medoids with non-medoid points if it reduces the total clustering cost.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples as the initial medoids.\n2. **Cost Calculation**: Compute the total clustering cost, which is the sum of the Euclidean distances from each sample to its nearest medoid.\n3. **Iterative Improvement**: Repeatedly attempt to reduce the total cost by considering all possible swaps between a medoid and a non-medoid. For each swap, compute the new cost and track the swap that provides the maximum reduction. If a swap reduces the cost, update the medoids and continue; otherwise, stop.\n4. **Label Assignment**: After convergence, assign each sample to the closest medoid, breaking ties by choosing the medoid that appears first in the list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return []\n    if k == 0:\n        return []\n    \n    # Precompute the distance matrix\n    distance = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            distance[i][j] = np.linalg.norm(X[i] - X[j])\n    \n    # Function to compute the total cost for given medoids\n    def compute_cost(medoids):\n        cost = 0.0\n        for x in range(n):\n            min_dist = np.min(distance[x][m] for m in medoids)\n            cost += min_dist\n        return cost\n    \n    # Initialize medoids and set\n    medoids = list(range(k))\n    medoid_set = set(medoids)\n    current_cost = compute_cost(medoids)\n    \n    while True:\n        best_swap = None\n        max_reduction = 0.0\n        \n        for m in medoids:\n            for i in range(n):\n                if i in medoid_set:\n                    continue\n                # Create new_medoids by replacing m with i, preserving order\n                new_medoids = []\n                for x in medoids:\n                    if x == m:\n                        new_medoids.append(i)\n                    else:\n                        new_medoids.append(x)\n                # Compute new cost\n                new_cost = compute_cost(new_medoids)\n                reduction = current_cost - new_cost\n                if reduction > max_reduction:\n                    max_reduction = reduction\n                    best_swap = (m, i, new_medoids, new_cost)\n        \n        if max_reduction > 0:\n            # Perform the best swap\n            medoids = best_swap[2]\n            medoid_set = set(medoids)\n            current_cost = best_swap[3]\n        else:\n            break\n    \n    # Assign labels based on closest medoid, considering order for ties\n    labels = []\n    for x in range(n):\n        min_dist = np.inf\n        best_m = medoids[0]  # default to the first medoid\n        for m in medoids:\n            d = distance[x][m]\n            if d < min_dist:\n                min_dist = d\n                best_m = m\n            elif d == min_dist:\n                # Keep the first occurrence\n                pass\n        labels.append(best_m)\n    \n    return labels\n```\n\n### Explanation\n1. **Initialization**: The first `k` samples are chosen as the initial medoids.\n2. **Distance Matrix**: A matrix is precomputed to store the Euclidean distances between all pairs of samples, which helps in efficiently calculating the cost.\n3. **Cost Calculation**: The cost is computed as the sum of the minimum distances from each sample to its nearest medoid.\n4. **Swapping Medoids**: For each medoid, every non-medoid is considered as a potential swap. The new cost is computed for each swap, and the swap that reduces the cost the most is performed.\n5. **Convergence**: The process stops when no further swap reduces the cost.\n6. **Label Assignment**: Each sample is assigned to the closest medoid, with ties broken by the order of medoids.\n\nThis approach ensures that the clustering is done deterministically and efficiently, adhering to the PAM algorithm's rules."}
{"task_id": 216, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic Net Regression from scratch using batch gradient descent. The model should be able to generate polynomial features, standardize the features, learn the weight vector using Elastic Net regularization, and make predictions on unseen data.\n\n### Approach\n1. **Input Validation**: Check if the input matrices `x_train` and `x_test` have the same number of features. If not, return `-1`. Also, check if the number of iterations is non-positive, returning `-1` if true.\n2. **Polynomial Feature Expansion**: Expand the features of both `x_train` and `x_test` to the specified degree. This involves creating polynomial terms up to the given degree for each feature.\n3. **Bias Term Addition**: Add a bias term (column of ones) to both the training and test feature matrices.\n4. **Feature Standardization**: Standardize the features (excluding the bias term) to have zero mean and unit variance using the training data's statistics.\n5. **Weight Initialization**: Initialize the weight vector to zeros.\n6. **Batch Gradient Descent**: Perform gradient descent for the specified number of iterations, updating the weights to minimize the loss function which includes both the mean squared error and the Elastic Net regularization term.\n7. **Prediction**: Use the trained weights to make predictions on the test data and return the results rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n\n    # Check if x_train and x_test have the same number of features\n    if x_train.shape[1] != x_test.shape[1]:\n        return -1\n\n    # Check if n_iterations is valid\n    if n_iterations < 1:\n        return -1\n\n    # Function to expand features to the given degree\n    def polynomial_features(x, degree):\n        if degree == 0:\n            return x\n        expanded = []\n        for feature in x.T:\n            powers = [feature**i for i in range(1, degree + 1)]\n            feature_expanded = np.vstack(powers).T\n            expanded.append(feature_expanded)\n        expanded = np.hstack(expanded)\n        return expanded\n\n    # Expand features\n    x_train_expanded = polynomial_features(x_train, degree)\n    x_test_expanded = polynomial_features(x_test, degree)\n\n    # Check if expanded features have the same number of features\n    if x_train_expanded.shape[1] != x_test_expanded.shape[1]:\n        return -1\n\n    # Add bias term\n    x_train_expanded = np.hstack([np.ones((x_train_expanded.shape[0], 1)), x_train_expanded])\n    x_test_expanded = np.hstack([np.ones((x_test_expanded.shape[0], 1)), x_test_expanded])\n\n    # Standardize features (excluding bias)\n    # Compute mean and std for features (excluding bias)\n    mean = np.mean(x_train_expanded[:, 1:], axis=0)\n    std = np.std(x_train_expanded[:, 1:], axis=0)\n\n    # Standardize training and test features\n    x_train_standard = x_train_expanded.copy()\n    x_train_standard[:, 1:] = (x_train_standard[:, 1:] - mean) / std\n\n    x_test_standard = x_test_expanded.copy()\n    x_test_standard[:, 1:] = (x_test_standard[:, 1:] - mean) / std\n\n    # Initialize weights\n    n_features = x_train_standard.shape[1]\n    w = np.zeros(n_features)\n\n    # Gradient descent\n    m = x_train_standard.shape[0]\n    for _ in range(n_iterations):\n        y_pred = x_train_standard @ w\n        error = y_train - y_pred\n        gradient_mse = (x_train_standard.T @ error) / m\n\n        # Compute regularization gradient\n        reg_gradient = np.zeros_like(w)\n        for j in range(1, len(w)):  # skip the bias term (j=0)\n            if w[j] == 0:\n                sign = 0\n            else:\n                sign = np.sign(w[j])\n            reg_term = reg_factor * (l1_ratio * sign + (1 - l1_ratio) * 2 * w[j])\n            reg_gradient[j] = reg_term\n\n        # Update weights\n        w = w - learning_rate * (gradient_mse + reg_gradient)\n\n    # Make predictions\n    y_pred_test = x_test_standard @ w\n    rounded_predictions = np.round(y_pred_test, 4).tolist()\n\n    return rounded_predictions\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices have compatible shapes and if the number of iterations is valid.\n2. **Polynomial Expansion**: Features are expanded to the specified degree using a helper function.\n3. **Bias Term**: A bias term is added to both the training and test data.\n4. **Standardization**: Features are standardized to ensure zero mean and unit variance, which helps in faster convergence during gradient descent.\n5. **Weight Initialization**: Weights are initialized to zero, and gradient descent is performed to minimize the loss function, which includes both the mean squared error and Elastic Net regularization.\n6. **Prediction**: After training, the model makes predictions on the test data and returns the results rounded to four decimal places.\n\nThis approach ensures that the model is both regularized and capable of handling polynomial features, providing a robust solution to regression problems."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input containers `actual` and `predicted` into NumPy arrays to facilitate vectorized computations.\n2. **Compute Probability**: Using the sigmoid function, we compute the probability for each predicted value. The sigmoid function transforms any real number into a value between 0 and 1.\n3. **Compute Gradient**: The gradient of the logistic loss is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the element-wise product of `actual` and the sigmoid of the negative product of `actual` and `predicted`.\n4. **Compute Hessian**: The Hessian is the second derivative of the logistic loss and is computed as `sigmoid(predicted_i) * (1 - sigmoid(predicted_i))`.\n5. **Round Results**: Each of the computed gradient, Hessian, and probability values is rounded to six decimal places to meet the problem's requirements.\n6. **Convert to Lists**: Finally, the NumPy arrays are converted back to Python lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute the probability using the sigmoid function\n    prob = 1 / (1 + np.exp(-predicted))\n    \n    # Compute the gradient of the logistic loss\n    z = -actual * predicted\n    sigmoid_z = 1 / (1 + np.exp(z))\n    gradient = actual * sigmoid_z\n    \n    # Compute the Hessian of the logistic loss\n    hessian = prob * (1 - prob)\n    \n    # Round each component to six decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert numpy arrays to lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Probability Calculation**: The sigmoid function is used to transform the predicted values into probabilities. This is done using the formula `1 / (1 + exp(-predicted))`.\n- **Gradient Calculation**: The gradient is derived from the logistic loss function and is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the sigmoid function applied to the negative product of `actual` and `predicted`.\n- **Hessian Calculation**: The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Rounding and Conversion**: The results are rounded to six decimal places and converted from NumPy arrays to Python lists for the final output.\n\nThis approach ensures that the computations are efficient and accurate, leveraging vectorized operations provided by NumPy for optimal performance."}
{"task_id": 218, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the outcome of a finished round of Blackjack based on the final hands of the player and the dealer. The function should return the reward for the player according to specific rules.\n\n### Approach\n1. **Calculate Hand Value**: The value of a hand is determined by summing the card values, with Aces (represented as 1) counted as 11 initially. If the sum exceeds 21, Aces are counted as 1 until the sum is 21 or less.\n2. **Determine Bust**: A hand is considered a bust if its value exceeds 21.\n3. **Check for Natural Blackjack**: A natural blackjack is a two-card hand consisting of an Ace and a 10-value card (10, Jack, Queen, King).\n4. **Evaluate Outcome**: Based on the hand values and whether either hand is a bust or a natural blackjack, determine the reward for the player.\n\n### Solution Code\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function must follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    # Calculate player's hand value and bust status\n    sum_p = 0\n    aces_p = 0\n    for card in player:\n        if card == 1:\n            aces_p += 1\n            sum_p += 11\n        else:\n            sum_p += card\n    while sum_p > 21 and aces_p > 0:\n        sum_p -= 10\n        aces_p -= 1\n    player_bust = sum_p > 21\n\n    # Calculate dealer's hand value and bust status\n    sum_d = 0\n    aces_d = 0\n    for card in dealer:\n        if card == 1:\n            aces_d += 1\n            sum_d += 11\n        else:\n            sum_d += card\n    while sum_d > 21 and aces_d > 0:\n        sum_d -= 10\n        aces_d -= 1\n    dealer_bust = sum_d > 21\n\n    # Check for natural blackjack\n    player_natural = len(player) == 2 and (1 in player) and (10 in player)\n    dealer_natural = len(dealer) == 2 and (1 in dealer) and (10 in dealer)\n\n    # Determine outcome based on rules\n    if player_bust:\n        return -1.0\n    elif dealer_bust:\n        return 1.0\n    else:\n        if player_natural and dealer_natural:\n            return 0.0\n        elif player_natural:\n            return 1.5\n        elif dealer_natural:\n            return -1.0\n        else:\n            if sum_p > sum_d:\n                return 1.0\n            elif sum_p < sum_d:\n                return -1.0\n            else:\n                return 0.0\n```\n\n### Explanation\n1. **Hand Value Calculation**: The function calculates the value of each hand by summing the card values, treating Aces as 11 initially. If the sum exceeds 21, Aces are converted to 1 one at a time until the sum is 21 or less.\n2. **Bust Determination**: A hand is marked as a bust if its value exceeds 21.\n3. **Natural Blackjack Check**: The function checks if a hand is a natural blackjack by verifying it has exactly two cards, one Ace, and one 10-value card.\n4. **Outcome Evaluation**: The function evaluates the outcome based on the rules:\n   - If the player busts, the reward is -1.\n   - If the dealer busts and the player does not, the reward is +1.\n   - If both hands are natural blackjacks, it's a draw.\n   - If only the player has a natural blackjack, the reward is +1.5.\n   - Otherwise, the higher hand value wins, or it's a draw if the values are equal."}
{"task_id": 221, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement two functions to calculate the Mean Squared Logarithmic Error (MSLE) for regression models. The first function, `squared_log_error`, computes the element-wise squared logarithmic errors between actual and predicted values. The second function, `mean_squared_log_error`, computes the average of these squared errors and returns it rounded to four decimal places. Input validation is also performed to ensure the inputs are valid.\n\n### Approach\n1. **Helper Function (`squared_log_error`)**:\n   - Convert the input sequences to NumPy arrays.\n   - Compute the logarithm of (1 + actual values) and (1 + predicted values).\n   - Calculate the squared differences between these logarithms for each element.\n   - Return the array of squared logarithmic errors.\n\n2. **Main Function (`mean_squared_log_error`)**:\n   - Validate the inputs to ensure they have the same shape and all values are non-negative.\n   - If any validation fails, return -1.\n   - Use the helper function to compute the squared logarithmic errors.\n   - Calculate the mean of these errors and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"TODO: implement helper that returns element-wise squared log errors.\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    log_actual = np.log(1 + actual)\n    log_predicted = np.log(1 + predicted)\n    squared_diff = (log_actual - log_predicted) ** 2\n    return squared_diff\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns -1.\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Check if the shapes are the same\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Check for non-negative values\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    \n    # Compute squared log errors\n    errors = squared_log_error(actual, predicted)\n    \n    # Calculate the mean and round to 4 decimal places\n    mean_error = np.mean(errors)\n    return round(mean_error, 4)\n```\n\n### Explanation\n- **Helper Function**: The `squared_log_error` function converts the inputs to NumPy arrays, computes the logarithm of (1 + actual) and (1 + predicted), and then calculates the squared differences between these logarithms. This results in an array of squared logarithmic errors.\n- **Main Function**: The `mean_squared_log_error` function first validates the inputs to ensure they are of the same shape and contain non-negative values. If any validation fails, it returns -1. If valid, it computes the squared logarithmic errors using the helper function, calculates their mean, and returns the result rounded to four decimal places. This approach ensures that the MSLE is computed efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 222, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the sigmoid (logistic) activation function, which can handle various types of inputs, including scalars, lists, and NumPy arrays. The function must compute the sigmoid transformation element-wise and ensure numerical stability for very large positive or negative values.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nTo ensure numerical stability for very large values of \\( z \\), we compute the function differently for positive and negative values:\n- For \\( z \\geq 0 \\), we use \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n- For \\( z < 0 \\), we use \\( \\sigma(z) = \\frac{e^{z}}{1 + e^{z}} \\).\n\nThe function must handle different input types:\n1. **Scalar Input**: If the input is a single number (int, float, or NumPy scalar), the function returns a single float rounded to 4 decimal places.\n2. **Array-like Input**: If the input is a list or NumPy array, the function processes each element, computes the sigmoid, and returns a Python list with the same structure, each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        # Handle scalar case\n        if z >= 0:\n            s = 1 / (1 + np.exp(-z))\n        else:\n            s = np.exp(z) / (1 + np.exp(z))\n        return round(s, 4)\n    else:\n        # Handle array-like case\n        arr = np.array(z)\n        mask = arr >= 0\n        positive_part = 1 / (1 + np.exp(-arr[mask]))\n        negative_part = np.exp(arr[~mask]) / (1 + np.exp(arr[~mask]))\n        result = np.zeros_like(arr)\n        result[mask] = positive_part\n        result[~mask] = negative_part\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Input Handling**: The function first checks if the input is a scalar. If so, it computes the sigmoid using the appropriate formula based on whether \\( z \\) is non-negative or negative. The result is rounded to 4 decimal places and returned as a float.\n\n2. **Array-like Input Handling**: For array-like inputs, the function converts the input to a NumPy array. It then creates a boolean mask to identify non-negative elements. Using vectorized operations, it computes the sigmoid for non-negative elements using one formula and for negative elements using another. The results are combined into an output array, which is then rounded and converted back to a Python list.\n\nThis approach ensures that the function is both efficient and numerically stable, handling all specified input types correctly."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is used in neural networks to avoid the \"dying\" neurons issue. The function should handle inputs that are scalars, nested lists, or numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For a given input value \\( x \\), if \\( x \\) is non-negative, the output is \\( x \\).\n- If \\( x \\) is negative, the output is \\( a \\times x \\), where \\( a \\) is a small positive float (default value is 0.01).\n\nThe approach to solve this problem involves:\n1. **Handling Numpy Arrays**: If the input is a numpy array, apply the Leaky ReLU activation element-wise using vectorized operations for efficiency.\n2. **Handling Lists**: If the input is a list (which can be nested), recursively process each element. This ensures that the output maintains the same nested structure as the input.\n3. **Handling Scalars**: If the input is a scalar, directly apply the Leaky ReLU activation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(e, a) for e in z]\n    else:\n        if z >= 0:\n            return z\n        else:\n            return a * z\n```\n\n### Explanation\n1. **Numpy Array Handling**: The function first checks if the input `z` is a numpy array. If it is, the Leaky ReLU activation is applied element-wise using `np.where`, which efficiently processes the entire array. The result is then converted to a list using `tolist()` to maintain the required output format.\n2. **List Handling**: If `z` is a list, the function processes each element recursively. This ensures that each element, whether it is a scalar, another list, or a numpy array, is processed correctly, maintaining the nested structure of the input.\n3. **Scalar Handling**: If `z` is a scalar, the function directly applies the Leaky ReLU activation. If the scalar is non-negative, it is returned as is. If it is negative, it is multiplied by `a` before being returned.\n\nThis approach ensures that the function works efficiently for all types of inputs and maintains the structure of the input, providing the correct Leaky ReLU activation for each element."}
{"task_id": 226, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to train an ensemble of decision stumps on a binary labeled training set and use this ensemble to predict labels for unseen test data.\n\n### Approach\n1. **Initialization**: Start by initializing the weights of all training samples to be equal, i.e., each weight is set to 1 divided by the number of samples.\n2. **Training the Ensemble**:\n   - For each boosting round, iterate over each feature to find the best decision stump. A decision stump is defined by a feature index, a threshold, and a polarity.\n   - For each feature, sort the samples based on the feature values. Compute prefix sums for the weights of samples classified as 1 and -1.\n   - For each possible split point in the sorted feature values, compute the classification error for both polarities (1 and -1). Track the split point and polarity that yield the minimum error.\n   - Compute the weight (alpha) for the best decision stump using the formula provided.\n   - Update the weights of the training samples. Misclassified samples receive higher weights, while correctly classified samples retain their weights. Normalize the weights after each update.\n3. **Prediction**:\n   - For each test sample, compute the weighted sum of the decisions from all trained decision stumps. The prediction is determined by the sign of this weighted sum.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test: 2-D numpy array of unseen samples to classify.\n        n_clf: Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    weights = np.full(n_samples, 1.0 / n_samples)\n    stumps = []\n\n    for _ in range(n_clf):\n        best_error = 1.0\n        best_clf = None\n\n        for j in range(n_features):\n            feature = X_train[:, j]\n            sorted_samples = sorted(zip(feature, y_train, weights), key=lambda x: x[0])\n\n            prefix_1 = [0.0]\n            prefix_neg1 = [0.0]\n\n            for x, y, w in sorted_samples:\n                prefix_1.append(prefix_1[-1] + (y == 1) * w)\n                prefix_neg1.append(prefix_neg1[-1] + (y == -1) * w)\n\n            total_1 = prefix_1[-1]\n            total_neg1 = prefix_neg1[-1]\n\n            min_error = 1.0\n            best_split = None\n            best_p = None\n\n            for i in range(len(sorted_samples) - 1):\n                t = (sorted_samples[i][0] + sorted_samples[i+1][0]) / 2\n\n                error_p1 = prefix_1[i+1] + (total_neg1 - prefix_neg1[i+1])\n                error_p_neg1 = prefix_neg1[i+1] + (total_1 - prefix_1[i+1])\n\n                current_min = min(error_p1, error_p_neg1)\n\n                if current_min < min_error:\n                    min_error = current_min\n                    if error_p1 < error_p_neg1:\n                        best_split = t\n                        best_p = 1\n                    else:\n                        best_split = t\n                        best_p = -1\n\n            if min_error < best_error:\n                best_error = min_error\n                best_clf = (j, best_split, best_p)\n\n        if best_clf is None:\n            break  # No better classifier found, break early\n\n        j, t, p = best_clf\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + 1e-10))\n\n        for i in range(n_samples):\n            x = X_train[i][j]\n            y = y_train[i]\n\n            if (x < t and p == 1 and y != -1) or (x < t and p == -1 and y != 1) or (x >= t and p == 1 and y != 1) or (x >= t and p == -1 and y != -1):\n                weights[i] *= math.exp(alpha)\n\n        total_weight = sum(weights)\n        weights = [w / total_weight for w in weights]\n\n        stumps.append((j, t, p, alpha))\n\n    predictions = []\n    for x in X_test:\n        sum_vote = 0.0\n        for (j, t, p, alpha) in stumps:\n            x_j = x[j]\n            if x_j < t:\n                vote = -1 if p == 1 else 1\n            else:\n                vote = 1 if p == 1 else -1\n            sum_vote += alpha * vote\n        predictions.append(1 if sum_vote >= 0 else -1)\n\n    return predictions\n```\n\n### Explanation\n- **Initialization**: The weights are initialized to ensure each sample starts with equal influence.\n- **Training**: For each boosting round, the algorithm evaluates each feature to find the best decision stump. The best stump is determined by minimizing the classification error. The weights of misclassified samples are increased, and the weights are normalized to maintain a valid probability distribution.\n- **Prediction**: Each test sample's prediction is determined by the weighted sum of decisions from all trained stumps, ensuring the ensemble's decision is robust and accurate.\n\nThis approach efficiently combines weak classifiers into a strong classifier, leveraging the power of ensemble methods to improve predictive performance."}
{"task_id": 241, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a tensor with an orthogonal matrix using the method popularized by Saxe et al. (2014). The goal is to ensure that the weights of a layer in a deep learning model do not cause the activations to shrink or explode during training by maintaining orthogonality in the weight matrix.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Check for Valid Shape**: If the input shape has fewer than two dimensions, return -1 as the problem requires at least two dimensions for an orthogonal matrix.\n\n2. **Create a 2D Matrix**: Construct a 2D matrix `A` from the given shape. This matrix is filled with independent and identically distributed (i.i.d.) samples from a standard normal distribution.\n\n3. **Singular Value Decomposition (SVD)**: Compute the SVD of matrix `A`, which decomposes `A` into three matrices: `U`, `\u03a3`, and `V^T`. Here, `U` and `V^T` are orthogonal matrices, and `\u03a3` is a diagonal matrix containing the singular values.\n\n4. **Select Orthogonal Factor**: Choose the factor matrix `Q` from the SVD such that it has the same shape as `A`. If `U` has the same shape as `A`, use `U`; otherwise, use `V^T`.\n\n5. **Reshape and Scale**: Reshape the selected matrix `Q` to the desired tensor shape and scale it by the given factor.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    \n    d0 = shape[0]\n    M = 1\n    for dim in shape[1:]:\n        M *= dim\n    \n    # Create the initial matrix A\n    A = np.random.randn(d0, M)\n    \n    # Compute SVD\n    U, S, V = np.linalg.svd(A)\n    \n    # Determine Q based on the shape of U\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = V.T\n    \n    # Reshape Q to the desired shape and scale\n    Q_reshaped = Q.reshape(shape) * scale\n    \n    # Convert to list of lists\n    return Q_reshaped.tolist()\n```\n\n### Explanation\n1. **Check for Valid Shape**: The function first checks if the input shape has fewer than two dimensions. If so, it returns -1 as orthogonal initialization is not possible.\n\n2. **Matrix Construction**: The matrix `A` is constructed with dimensions `(d0, M)`, where `d0` is the first dimension of the shape, and `M` is the product of the remaining dimensions.\n\n3. **SVD Decomposition**: The matrix `A` is decomposed into `U`, `\u03a3`, and `V^T` using SVD. This decomposition ensures that `U` and `V^T` are orthogonal matrices.\n\n4. **Selecting the Orthogonal Factor**: The factor matrix `Q` is chosen based on the shape of `U`. If `U` matches the shape of `A`, `Q` is set to `U`; otherwise, `Q` is set to `V^T`.\n\n5. **Reshaping and Scaling**: The matrix `Q` is reshaped to the desired tensor shape and scaled by the given factor to ensure the weights are appropriately scaled.\n\nThis approach ensures that the resulting tensor maintains the orthogonality property, which helps in preventing the issues of exploding or shrinking activations during the initial training phase of a deep learning model."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using basic NumPy operations. The network architecture consists of three fully-connected layers followed by two separate output heads for the actor (policy) and the critic (state-value function). The weights and biases are fixed and identical to one, which simplifies the computation.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers (each with ReLU activation) followed by two output heads. The actor head uses a softmax activation to produce action probabilities, while the critic head uses a linear activation to produce a scalar state value.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0. This simplifies the computation as each layer's output can be derived directly from the input without complex weight matrices.\n3. **Computation Steps**:\n   - Compute the sum of the input state.\n   - Use this sum to compute the outputs of the hidden layers using ReLU activation.\n   - Compute the action probabilities using the actor head, which results in equal probabilities for each action.\n   - Compute the state value using the critic head, which is zero if the sum of the state is non-positive, otherwise, it is 64 times the sum of the state.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_s = sum(state)\n    action_probs = [round(1/3, 4), round(1/3, 4), round(1/3, 4)]\n    state_value = 64.0 * sum_s if sum_s > 0 else 0.0\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)\n```\n\n### Explanation\n1. **Sum of State**: The input state is a list of three floats. The sum of these floats is computed to determine the subsequent values.\n2. **Action Probabilities**: The actor head computes logits which are all equal, leading to equal probabilities for each action when applying the softmax function. These probabilities are rounded to four decimal places.\n3. **State Value**: The critic head computes the state value based on the sum of the state. If the sum is positive, the state value is 64 times the sum; otherwise, it is zero. The state value is rounded to four decimal places.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, given the fixed weights and biases."}
{"task_id": 249, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an Actor-Critic network using NumPy. The network consists of three shared fully connected layers with ReLU activations, followed by two independent output heads: one for the actor (policy) and one for the critic (value estimation).\n\n### Approach\n1. **Convert State and Parameters**: Convert the input state and network parameters into NumPy arrays for efficient computation.\n2. **Shared Layers Processing**: Pass the state through three fully connected layers with ReLU activation functions. Each layer consists of an affine transformation followed by ReLU.\n3. **Actor Head Processing**: Use the output from the shared layers to compute the action scores, then apply the softmax function to convert these scores into a probability distribution.\n4. **Critic Head Processing**: Use the same output from the shared layers to compute the scalar state value.\n5. **Rounding**: Round the resulting probabilities and the state value to four decimal places, ensuring that exact values of 0 or 1 are represented with one decimal place.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to a numpy array\n    x = np.array(state)\n    \n    # Convert all parameters to numpy arrays\n    W1 = np.array(params['W1'])\n    b1 = np.array(params['b1'])\n    W2 = np.array(params['W2'])\n    b2 = np.array(params['b2'])\n    W3 = np.array(params['W3'])\n    b3 = np.array(params['b3'])\n    Wa = np.array(params['Wa'])\n    ba = np.array(params['ba'])\n    Wc = np.array(params['Wc'])\n    bc = np.array(params['bc'])\n    \n    # Process through the shared layers\n    x = np.maximum(0, W1 @ x + b1)\n    x = np.maximum(0, W2 @ x + b2)\n    x = np.maximum(0, W3 @ x + b3)\n    \n    # Actor head processing\n    a = Wa @ x + ba\n    exp_a = np.exp(a)\n    probabilities = exp_a / exp_a.sum()\n    rounded_probs = np.round(probabilities, 4).tolist()\n    \n    # Critic head processing\n    v = Wc @ x + bc\n    value = np.round(v, 4)\n    \n    return (rounded_probs, value)\n```\n\n### Explanation\n1. **State and Parameters Conversion**: The input state and network parameters are converted into NumPy arrays to facilitate efficient matrix operations.\n2. **Shared Layers**: The state is passed through three fully connected layers with ReLU activation. Each layer transforms the input using its weights and biases, followed by an element-wise ReLU activation.\n3. **Actor Head**: The output from the shared layers is used to compute action scores. These scores are converted into probabilities using the softmax function, ensuring they sum to 1.\n4. **Critic Head**: The same output from the shared layers is used to compute the scalar state value, which represents the estimated value of the current state.\n5. **Rounding**: The resulting probabilities and state value are rounded to four decimal places. If the result is exactly 0 or 1, it is represented with one decimal place for clarity.\n\nThis approach efficiently computes the forward pass of the Actor-Critic network, providing both the action probabilities and the state value in a structured manner."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Elastic-Net penalty and its gradient for a given weight vector. Elastic-Net is a convex combination of L1 and L2 regularization, which helps in reducing model complexity and preventing over-fitting.\n\n### Approach\nThe Elastic-Net penalty is defined as:\n\\[ R(w) = \\alpha \\cdot [\\lambda \\cdot ||w||_2 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w] \\]\nwhere:\n- \\( w \\) is the weight vector,\n- \\( \\alpha \\) is the regularization strength,\n- \\( \\lambda \\) is the L1 ratio.\n\nThe gradient of the penalty with respect to \\( w \\) is:\n\\[ \\nabla R(w) = \\alpha \\cdot [\\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w] \\]\n\nThe function `elastic_net_regularization` computes either the penalty value or its gradient based on the input flag `gradient`. The steps are as follows:\n1. Convert the input weight vector to a NumPy array for easier computation.\n2. If computing the gradient, calculate each element using the given formula and round the result.\n3. If computing the penalty, calculate the two terms (L2 norm and squared L2 norm) and combine them, then round the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.array(w)\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        grad_rounded = np.round(grad, 4).tolist()\n        return grad_rounded\n    else:\n        term1 = l1_ratio * np.linalg.norm(w)\n        term2 = (1 - l1_ratio) * 0.5 * np.sum(w ** 2)\n        penalty = alpha * (term1 + term2)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**: The penalty is computed by combining the L2 norm and the squared L2 norm terms, scaled by the given parameters. The result is then rounded to four decimal places.\n- **Gradient Calculation**: The gradient is computed element-wise using the sign function for the L1 term and the weight vector for the L2 term. Each element is then rounded to four decimal places and returned as a list.\n\nThis approach ensures that we efficiently compute either the penalty or its gradient, adhering to the constraints and requirements specified."}
{"task_id": 256, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a numerical gradient checker for a vanilla Recurrent Neural Network (RNN) parameter using the centered finite-difference formula. The goal is to verify the analytical gradients computed during backpropagation by approximating the partial derivatives using numerical methods.\n\n### Approach\n1. **Parameter Handling**: The function first checks if the given parameter name is \"X\" or \"y\", which are not trainable parameters, and returns `None` in such cases. If the parameter name is \"Ba\" or \"Bx\", it is converted to lowercase. The function then checks if the parameter exists in the model's parameters dictionary.\n\n2. **Perturbation and Forward Pass**: For each element in the parameter matrix, the function creates two perturbed versions of the parameter: one with a small positive perturbation (epsilon) and another with a small negative perturbation. The model's parameter is updated with these perturbed values, and the forward pass is run for all time steps to collect predictions.\n\n3. **Loss Calculation**: The loss function is evaluated using the predictions obtained from the forward pass for both perturbed parameter values. The numerical gradient for each element is then approximated using the centered finite-difference formula.\n\n4. **Gradient Collection and Transpose**: The numerical gradients for all elements are collected, reshaped to match the original parameter's shape, and then transposed before being returned.\n\n### Solution Code\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    # Handle non-trainable parameters\n    if param_name in ('X', 'y'):\n        return None\n    # Convert \"Ba\" or \"Bx\" to lowercase\n    if param_name in ('Ba', 'Bx'):\n        param_name = param_name.lower()\n    # Check if the parameter exists\n    if param_name not in model.parameters:\n        return None\n    \n    # Get the original parameter\n    theta = model.parameters[param_name]\n    original_theta = theta.copy()\n    gradients = []\n    \n    # Iterate over each element in the parameter\n    for i in np.ndindex(theta.shape):\n        # Perturb the parameter with +epsilon\n        theta_plus = original_theta.copy()\n        theta_plus[i] += epsilon\n        model.parameters[param_name] = theta_plus\n        model.flush_gradients()\n        # Collect predictions for theta_plus\n        predictions_plus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            y_t = model.forward(x_t)\n            predictions_plus.append(y_t)\n        loss_plus = loss_func(predictions_plus)\n        \n        # Perturb the parameter with -epsilon\n        theta_minus = original_theta.copy()\n        theta_minus[i] -= epsilon\n        model.parameters[param_name] = theta_minus\n        model.flush_gradients()\n        # Collect predictions for theta_minus\n        predictions_minus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            y_t = model.forward(x_t)\n            predictions_minus.append(y_t)\n        loss_minus = loss_func(predictions_minus)\n        \n        # Compute the numerical gradient\n        gradient_i = (loss_plus - loss_minus) / (2 * epsilon)\n        gradients.append(gradient_i)\n        \n        # Restore the original parameter\n        model.parameters[param_name] = original_theta\n    \n    # Reshape the gradients and transpose\n    theta_shape = theta.shape\n    grads = np.array(gradients, dtype=np.float64).reshape(theta_shape)\n    return grads.T\n```\n\n### Explanation\n- **Parameter Handling**: The function first checks if the parameter is non-trainable or needs name conversion. It then retrieves the parameter from the model.\n- **Perturbation**: For each element in the parameter, two perturbed versions are created. The model's parameter is updated with these values, and the forward pass is run to collect predictions.\n- **Loss Calculation**: The loss is computed for both perturbed values, and the numerical gradient is approximated using the centered finite-difference formula.\n- **Gradient Collection**: The gradients are collected, reshaped to match the original parameter's dimensions, and transposed before being returned.\n\nThis approach ensures that the numerical gradients are accurately computed and verified, providing a robust debugging tool for the RNN model."}
{"task_id": 257, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to iteratively re-weight training examples and combine the weak classifiers to form a strong classifier. The final prediction is made by aggregating the votes from all weak classifiers.\n\n### Approach\n1. **Initialization**: Start with equal weights for all training examples.\n2. **Iterative Training**: For each iteration, find the best decision stump (one-level decision tree) that minimizes the weighted classification error. Update the weights of the training examples based on the performance of the current weak classifier.\n3. **Weight Update**: After selecting the best decision stump, compute its weight using the formula provided. Update the weights of the training examples to increase the weight of misclassified examples and decrease the weight of correctly classified ones.\n4. **Prediction**: For each test example, compute the weighted sum of predictions from all weak classifiers. The sign of this sum determines the final prediction.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf: Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in X_test.\n    \"\"\"\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    n_clf = max(n_clf, 1)\n    weak_classifiers = []\n\n    # Initialize weights\n    w = np.ones(m) / m\n\n    for _ in range(n_clf):\n        min_error = 1.0\n        best_feature = -1\n        best_split = -1\n\n        for j in range(n):\n            # Create a list of tuples (x_j, y_i, w_i)\n            data = list(zip(X_train[:, j], y_train, w))\n            # Sort the data by x_j\n            data.sort(key=lambda x: x[0])\n\n            # Compute prefix sums for y=1 and y=-1\n            prefix_y1 = [0.0] * (m + 1)\n            prefix_y_neg = [0.0] * (m + 1)\n            for i in range(m):\n                prefix_y1[i + 1] = prefix_y1[i] + (data[i][1] == 1) * data[i][2]\n                prefix_y_neg[i + 1] = prefix_y_neg[i] + (data[i][1] == -1) * data[i][2]\n\n            total_neg = prefix_y_neg[m]\n            min_error_j = 1.0\n            best_k = 0\n\n            for k in range(m + 1):\n                error = prefix_y1[k] + (total_neg - prefix_y_neg[k])\n                if error < min_error_j:\n                    min_error_j = error\n                    best_k = k\n\n            if min_error_j < min_error:\n                min_error = min_error_j\n                best_feature = j\n                best_split = best_k\n\n        j = best_feature\n        k = best_split\n\n        if k == 0:\n            t = -np.inf\n        else:\n            t = data[k - 1][0]\n\n        epsilon_t = min_error\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        weak_classifiers.append((j, t, alpha_t))\n\n        h = np.zeros(m)\n        for i in range(m):\n            if X_train[i][j] <= t:\n                h[i] = -1\n            else:\n                h[i] = 1\n\n        factor = np.zeros(m)\n        for i in range(m):\n            if h[i] == y_train[i]:\n                factor[i] = np.exp(-alpha_t)\n            else:\n                factor[i] = np.exp(alpha_t)\n\n        w = w * factor\n        w /= np.sum(w)\n\n    predictions = []\n    for x in X_test:\n        sum_vote = 0.0\n        for (j, t, alpha) in weak_classifiers:\n            if x[j] <= t:\n                h = -1\n            else:\n                h = 1\n            sum_vote += alpha * h\n        if sum_vote > 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n\n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for all training examples are initialized to be equal.\n2. **Training Loop**: For each weak classifier, the algorithm iterates over each feature to find the best decision stump. The best stump is determined by minimizing the weighted classification error.\n3. **Weight Update**: After selecting the best stump, the weights of the training examples are updated. Misclassified examples receive higher weights, while correctly classified examples receive lower weights.\n4. **Prediction**: For each test example, the algorithm aggregates the predictions from all weak classifiers, weighted by their respective weights. The final prediction is determined by the sign of the weighted sum.\n\nThis approach ensures that the algorithm focuses on difficult examples in subsequent iterations, leading to a strong classifier that combines the strengths of multiple weak classifiers."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (also called Xavier) normal weight-initialisation function for neural networks. This function initializes weights in a way that helps prevent the vanishing or exploding gradient problems during training.\n\n### Approach\nThe approach to implement the Glorot normal initialization involves the following steps:\n\n1. **Compute Fan In and Fan Out**: \n   - For a 2-D shape `(fan_in, fan_out)`, the values are directly taken from the two dimensions.\n   - For shapes with more than two dimensions, such as convolutional kernels, the fan in and fan out are computed using the product of all dimensions after the first two. Specifically, `fan_in` is the product of the second dimension and the product of the remaining dimensions, and `fan_out` is the product of the first dimension and the product of the remaining dimensions.\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation `s` is calculated using the formula `s = sqrt(2 / (fan_in + fan_out))`.\n\n3. **Generate Random Weights**: \n   - A NumPy array is generated with the specified shape, filled with values drawn from a normal distribution with mean 0 and standard deviation `s`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least two dimensions.\")\n    \n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    else:\n        product = 1\n        for dim in shape[2:]:\n            product *= dim\n        fan_in = shape[1] * product\n        fan_out = shape[0] * product\n    \n    s = np.sqrt(2 / (fan_in + fan_out))\n    return np.random.normal(0, s, shape)\n```\n\n### Explanation\n- **Fan In and Fan Out Calculation**: For a 2D shape, `fan_in` and `fan_out` are directly taken from the first and second dimensions, respectively. For higher-dimensional shapes, the product of the dimensions beyond the first two is used to compute `fan_in` and `fan_out` by multiplying with the respective channel dimensions.\n- **Standard Deviation Calculation**: The standard deviation is derived from the formula to ensure the variance is appropriate for the given fan in and fan out values.\n- **Random Weight Generation**: Using NumPy's `random.normal` function, we generate weights that follow a normal distribution with the computed mean and standard deviation, ensuring the weights are initialized in a way that helps maintain stable gradients during training."}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency-list representation. The adjacency list should be a list of lists where each inner list contains the vertices adjacent to the corresponding vertex in `V`, sorted by their order of appearance in `V`.\n\n### Approach\n1. **Create a Vertex Index Mapping**: First, we create a dictionary to map each vertex to its index in the vertex list `V`. This helps in sorting the neighbors based on their order of appearance in `V`.\n2. **Initialize Adjacency List**: Use a dictionary where each key is a vertex and the value is a set to store its neighbors. This helps in efficiently handling duplicates and ensures that each edge is processed only once.\n3. **Process Each Edge**: For each edge in the edge list `E`, add each vertex of the edge to the other vertex's set of neighbors. This ensures that both vertices are updated correctly for an undirected graph.\n4. **Sort Neighbors**: For each vertex in `V`, convert its set of neighbors into a list and sort this list based on the order of their appearance in `V` using the vertex index mapping.\n5. **Construct the Result**: Construct the final adjacency list by iterating through each vertex in `V` and appending the sorted list of its neighbors.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\nfrom collections import defaultdict\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    vertex_index = {v: i for i, v in enumerate(V)}\n    adj = defaultdict(set)\n    for u, v in E:\n        adj[u].add(v)\n        adj[v].add(u)\n    G = []\n    for v in V:\n        neighbors = list(adj[v])\n        neighbors_sorted = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(neighbors_sorted)\n    return G\n```\n\n### Explanation\n- **Vertex Index Mapping**: This step ensures that we can quickly determine the order of each vertex in `V` when sorting the neighbors.\n- **Adjacency List Initialization**: Using a set for each vertex's neighbors ensures that duplicates are automatically handled, and the use of a dictionary allows efficient updates.\n- **Edge Processing**: Each edge is processed to update both vertices' neighbor sets, ensuring the graph is undirected.\n- **Sorting Neighbors**: By sorting the neighbors based on their indices in `V`, we ensure the adjacency list is ordered correctly and deterministically.\n\nThis approach efficiently constructs the adjacency list while handling duplicates and ensuring the correct order of neighbors, providing a clear and correct solution to the problem."}
{"task_id": 267, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the optimal weighted decision stump for a binary classification task. A decision stump is the simplest decision tree, making a prediction based on a single feature and a threshold. The goal is to minimize the weighted classification error, considering the given weights for each sample.\n\n### Approach\n1. **Iterate Over Each Feature**: For each feature, we consider it as a potential candidate for the decision stump.\n2. **Sort and Preprocess**: For each feature, we sort the samples based on the feature values. We also precompute prefix sums for the weights of the positive (1) and negative (-1) labels to efficiently calculate the classification error.\n3. **Evaluate Thresholds**: For each unique value of the feature (considered as a potential threshold), we compute the classification error for both possible polarities (1 and -1). The error is calculated using the precomputed prefix sums.\n4. **Track Minimum Error**: We keep track of the minimum error encountered and the corresponding feature, threshold, and polarity. If multiple stumps yield the same minimal error, any one can be chosen.\n\n### Solution Code\n```python\nimport numpy as np\nimport bisect\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n = X.shape[0]\n    d = X.shape[1]\n    best_error = float('inf')\n    best_feature = -1\n    best_theta = -1\n    best_p = 1\n\n    for j in range(d):\n        feature = X[:, j]\n        samples = list(zip(feature, y, sample_weights))\n        samples.sort(key=lambda x: x[0])\n        sorted_x = [s[0] for s in samples]\n        y_list = [s[1] for s in samples]\n        w_list = [s[2] for s in samples]\n        \n        sum_pos = [0.0]\n        sum_neg = [0.0]\n        for y_i, w_i in zip(y_list, w_list):\n            sum_pos.append(sum_pos[-1] + (w_i if y_i == 1 else 0))\n            sum_neg.append(sum_neg[-1] + (w_i if y_i == -1 else 0))\n        \n        total_pos = sum_pos[-1]\n        total_neg = sum_neg[-1]\n        unique_thetas = np.unique(sorted_x)\n        \n        for theta in unique_thetas:\n            k = bisect.bisect_right(sorted_x, theta)\n            error_p1 = sum_neg[k] + (total_pos - sum_pos[k])\n            error_p_neg = sum_pos[k] + (total_neg - sum_neg[k])\n            current_min = min(error_p1, error_p_neg)\n            \n            if current_min < best_error:\n                best_error = current_min\n                best_p = 1 if error_p1 < error_p_neg else -1\n                best_feature = j\n                best_theta = theta\n            elif current_min == best_error:\n                if error_p1 < error_p_neg:\n                    best_p = 1\n                else:\n                    best_p = -1\n                best_feature = j\n                best_theta = theta\n\n    best_theta_rounded = round(best_theta, 4)\n    best_error_rounded = round(best_error, 4)\n    \n    return {\n        \"feature_index\": best_feature,\n        \"threshold\": best_theta_rounded,\n        \"polarity\": best_p,\n        \"weighted_error\": best_error_rounded\n    }\n```\n\n### Explanation\n1. **Feature Extraction and Sorting**: For each feature, we extract the feature values along with their corresponding labels and weights. These are then sorted based on the feature values.\n2. **Prefix Sums Calculation**: We compute prefix sums for the weights of positive and negative labels. This helps in efficiently calculating the classification error for different thresholds.\n3. **Threshold Evaluation**: For each unique feature value (threshold), we compute the classification error for both polarities. The error is calculated using the prefix sums, allowing for efficient computation.\n4. **Track Best Stump**: We keep track of the minimum error encountered and update the best feature, threshold, and polarity accordingly. The results are then rounded to four decimal places for the final output.\n\nThis approach ensures that we efficiently find the optimal decision stump by considering all possible features and thresholds, while minimizing the weighted classification error."}
{"task_id": 273, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the optimal numerical threshold for splitting data in a decision tree to maximize information gain. The threshold should be chosen such that it splits the data into two parts, and the information gain from this split is maximized. If multiple thresholds yield the same maximum gain, the smallest threshold is selected. If no useful split exists, we return (None, 0.0).\n\n### Approach\n1. **Check for Valid Data**: Ensure the feature and target lists are non-empty and of the same length.\n2. **Sort and Extract Unique Values**: Sort the feature values and extract unique values to generate candidate thresholds.\n3. **Generate Candidate Thresholds**: Compute midpoints between consecutive unique values to form candidate thresholds.\n4. **Compute Parent Entropy**: Calculate the entropy of the entire dataset to use as the parent entropy.\n5. **Evaluate Each Threshold**: For each candidate threshold, split the data into left and right subsets, compute their entropies, and determine the information gain.\n6. **Select Optimal Threshold**: Track the threshold that provides the maximum information gain, choosing the smallest threshold in case of ties.\n7. **Return Result**: Return the optimal threshold and information gain, rounded to four decimal places. If no useful split exists, return (None, 0.0).\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert to list if they are NumPy arrays\n    feature = list(feature)\n    target = list(target)\n    \n    # Check if feature or target is empty\n    if not feature or not target:\n        return (None, 0.0)\n    \n    n = len(feature)\n    if n != len(target):\n        raise ValueError(\"Feature and target must be of the same length.\")\n    \n    # Sort the feature and extract unique sorted values\n    sorted_feature = sorted(feature)\n    unique_sorted = []\n    prev = None\n    for val in sorted_feature:\n        if val != prev:\n            unique_sorted.append(val)\n            prev = val\n    \n    # If less than two unique values, no split possible\n    if len(unique_sorted) < 2:\n        return (None, 0.0)\n    \n    # Generate candidate thresholds\n    candidate_thresholds = []\n    for i in range(len(unique_sorted) - 1):\n        t = (unique_sorted[i] + unique_sorted[i + 1]) / 2\n        candidate_thresholds.append(t)\n    \n    # Compute H_parent\n    counts_parent = Counter(target)\n    H_parent = 0.0\n    for class_label, count in counts_parent.items():\n        p = count / n\n        H_parent -= p * math.log2(p)\n    \n    # If all labels are the same, no gain\n    if H_parent == 0.0:\n        return (None, 0.0)\n    \n    max_ig = -1.0\n    best_t = None\n    \n    for t in candidate_thresholds:\n        left_counts = Counter()\n        right_counts = Counter()\n        \n        for val, label in zip(feature, target):\n            if val < t:\n                left_counts[label] += 1\n            else:\n                right_counts[label] += 1\n        \n        left_size = sum(left_counts.values())\n        right_size = n - left_size\n        \n        # Compute H_L\n        H_L = 0.0\n        for count in left_counts.values():\n            p = count / left_size\n            H_L -= p * math.log2(p)\n        \n        # Compute H_R\n        H_R = 0.0\n        for count in right_counts.values():\n            p = count / right_size\n            H_R -= p * math.log2(p)\n        \n        # Calculate Information Gain\n        IG = H_parent - (left_size / n) * H_L - (right_size / n) * H_R\n        \n        # Update best threshold and max IG\n        if IG > max_ig:\n            max_ig = IG\n            best_t = t\n        elif IG == max_ig:\n            if best_t is None or t < best_t:\n                best_t = t\n    \n    if max_ig <= 0.0:\n        return (None, 0.0)\n    else:\n        best_t_rounded = round(best_t, 4)\n        max_ig_rounded = round(max_ig, 4)\n        return (best_t_rounded, max_ig_rounded)\n```\n\n### Explanation\n1. **Data Handling**: The function starts by converting input arrays to lists and checking for valid data lengths.\n2. **Unique Values Extraction**: The feature values are sorted, and unique values are extracted to form candidate thresholds.\n3. **Entropy Calculation**: The entropy of the entire dataset (parent entropy) is computed to measure the impurity.\n4. **Threshold Evaluation**: For each candidate threshold, the dataset is split, and the entropies of the resulting subsets are calculated. The information gain is then determined.\n5. **Optimal Threshold Selection**: The threshold with the highest information gain is selected, with ties broken by choosing the smallest threshold.\n6. **Result Formatting**: The results are rounded to four decimal places and returned, ensuring clarity and precision."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function `get_initializer` that retrieves a weight-initialization function based on a given name. The function should look up the specified initialization function in the module's global namespace, verify that it is callable, and return it. If the function is not found or is not callable, it should raise a `ValueError`.\n\n### Approach\n1. **Lookup the Function**: Use `globals().get(name)` to find the function with the given name in the module's global namespace.\n2. **Check Callability**: Verify if the retrieved function is callable using the `callable()` function.\n3. **Return or Raise Error**: If the function is callable, return it. Otherwise, raise a `ValueError` indicating an invalid initialization function.\n\nThe solution includes three initialization functions:\n- `zeros_init`: Returns a matrix filled with zeros.\n- `ones_init`: Returns a matrix filled with ones.\n- `random_uniform_init`: Returns a matrix with elements uniformly distributed between specified low and high values, using a fixed seed for reproducibility.\n\n### Solution Code\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    rows, cols = shape\n    matrix = []\n    for _ in range(rows):\n        row = []\n        for _ in range(cols):\n            row.append(low + (high - low) * random.random())\n        matrix.append(row)\n    return matrix\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n\n    Returns:\n        A callable initializer.\n\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    initializer = globals().get(name)\n    if callable(initializer):\n        return initializer\n    else:\n        raise ValueError(\"Invalid initialization function.\")\n```\n\n### Explanation\n- **Initialization Functions**: Each function (`zeros_init`, `ones_init`, `random_uniform_init`) creates a matrix of the specified shape and initializes it according to its specific method.\n- **get_initializer Function**: This function retrieves the specified initialization function from the global namespace, checks if it is callable, and returns it. If the function is not found or is not callable, it raises an appropriate error.\n\nThis approach ensures that the correct initialization function is retrieved and used dynamically based on the provided name, making the code flexible and easy to extend with additional initialization methods if needed."}
{"task_id": 287, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement both the forward and backward passes of a 2-D average pooling layer in a convolutional neural network. The forward pass computes the pooled output by sliding a fixed-size window over the input feature map and replacing each window with the arithmetic mean of its elements. The backward pass propagates the gradient back to the input, distributing the gradient equally to every element that contributed to each average.\n\n### Approach\n1. **Forward Pass**:\n   - For each batch and channel, extract the feature map.\n   - Create a windowed view of the feature map using strides to slide the window over the feature map.\n   - Compute the sum of elements in each window and then take the average by dividing by the number of elements in the window.\n   - Store the result in the output array.\n\n2. **Backward Pass**:\n   - Initialize the gradient input array with zeros, having the same shape as the input.\n   - For each batch and channel, process the gradient for the feature map.\n   - For each output position, compute the corresponding window in the feature map and distribute the gradient equally to each element in the window.\n   - Store the computed gradients in the gradient input array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    # Compute output dimensions\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n\n    # Initialize output\n    output = np.zeros((N, C, H_out, W_out), dtype=X.dtype)\n\n    # Initialize gradient input\n    grad_input = np.zeros_like(X)\n\n    for batch in range(N):\n        for channel in range(C):\n            # Forward pass\n            feature_map = X[batch, channel, :, :]\n            # Create windowed view\n            windowed = np.lib.stride_tricks.as_strided(feature_map, shape=(H_out, W_out, p_h, p_w), strides=(s_h, s_w, 1, 1))\n            sum_window = windowed.sum(axis=(-2, -1))\n            avg = sum_window / (p_h * p_w)\n            output[batch, channel, :, :] = avg\n\n            # Backward pass\n            grad_feature = grad_input[batch, channel, :, :]\n            for i in range(H_out):\n                for j in range(W_out):\n                    g = accum_grad[batch, channel, i, j]\n                    row_start = i * s_h\n                    row_end = row_start + p_h\n                    col_start = j * s_w\n                    col_end = col_start + p_w\n                    grad_feature[row_start:row_end, col_start:col_end] += g / (p_h * p_w)\n            grad_input[batch, channel, :, :] = grad_feature\n\n    # Round to 4 decimals\n    output_rounded = np.round(output * 10000) / 10000\n    grad_input_rounded = np.round(grad_input * 10000) / 10000\n\n    return output_rounded.tolist(), grad_input_rounded.tolist()\n```\n\n### Explanation\n- **Forward Pass**:\n  - The input feature map is processed by sliding a window of size `pool_shape` over it with a stride of `stride`.\n  - For each window, the sum of the elements is computed, and the average is taken by dividing by the number of elements in the window.\n  - The result is stored in the output array.\n\n- **Backward Pass**:\n  - The gradient from the loss is distributed equally to each element in the window that contributed to the corresponding output element.\n  - For each output position, the gradient is added to each element in the corresponding window in the input feature map, scaled by the reciprocal of the number of elements in the window.\n\nThis approach ensures that both the forward and backward passes are computed efficiently, leveraging NumPy's array operations for performance. The results are rounded to four decimal places and converted to lists for the final output."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same feature indices at each node, have thresholds that are numerically equal up to a tolerance of 1 \u00d7 10\u207b\u2078, and produce the same predictions at the leaves within the same tolerance.\n\n### Approach\nThe approach to solve this problem involves recursively comparing each node and its children in both trees. Here are the key steps:\n\n1. **Check Node Types**: First, check if both trees are either leaves or nodes. If one is a leaf and the other is a node, they are not equivalent.\n2. **Compare Leaf Values**: If both trees are leaves, compare their values using `numpy.allclose` with a tolerance of 1 \u00d7 10\u207b\u2078. This handles various data types including integers, floats, lists, and numpy arrays.\n3. **Compare Node Features and Thresholds**: If both trees are nodes, compare their feature indices and thresholds. The thresholds are compared using `numpy.isclose` with the same tolerance.\n4. **Recursive Comparison of Children**: Recursively compare the left and right children of the nodes. If any comparison fails, the trees are not equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    value: object\n\n@dataclass\nclass Node:\n    feature: int\n    threshold: float\n    left: object\n    right: object\n\ndef compare_trees(tree_a, tree_b):\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        a_val = tree_a.value\n        b_val = tree_b.value\n        if not isinstance(a_val, np.ndarray):\n            a_val = np.array(a_val)\n        if not isinstance(b_val, np.ndarray):\n            b_val = np.array(b_val)\n        return np.allclose(a_val, b_val, atol=1e-8)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        left_equal = compare_trees(tree_a.left, tree_b.left)\n        right_equal = compare_trees(tree_a.right, tree_b.right)\n        return left_equal and right_equal\n    else:\n        return False\n```\n\n### Explanation\n- **Leaf Comparison**: When both trees are leaves, their values are converted to numpy arrays if they aren't already. This ensures that values of different types (int, float, list, array) are compared correctly using `numpy.allclose`.\n- **Node Comparison**: For nodes, the feature indices and thresholds are compared directly. The thresholds are checked for numerical equality within a small tolerance to handle floating-point precision issues.\n- **Recursive Structure Check**: The function recursively checks the left and right children of each node, ensuring that the structure of both trees is identical and that all corresponding nodes and leaves are equivalent.\n\nThis approach ensures that all aspects of the trees are checked, including their structure, feature usage, threshold values, and leaf predictions, making it robust and accurate for determining equivalence."}
{"task_id": 292, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a single-point column crossover on two parent neural network weight matrices to generate two new child matrices. This operation is commonly used in evolutionary algorithms to mix the weights of two parent networks.\n\n### Approach\n1. **Validation**: First, we check if the two parent matrices have the same shape. If they do not, we return -1 as specified.\n2. **Conversion to NumPy Arrays**: Convert the parent matrices into NumPy arrays for efficient slicing and manipulation.\n3. **Crossover Operation**: Create two new child matrices by:\n   - Taking columns before the cutoff from their respective parent.\n   - Taking columns from the cutoff (inclusive) to the end from the other parent.\n4. **Conversion Back to Lists**: Convert the resulting NumPy arrays back to Python lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Check if parents have the same shape\n    if len(parent1) != len(parent2):\n        return -1\n    for r1, r2 in zip(parent1, parent2):\n        if len(r1) != len(r2):\n            return -1\n\n    # Convert to numpy arrays for efficient slicing\n    parent1_np = np.array(parent1)\n    parent2_np = np.array(parent2)\n\n    # Compute the children\n    child1_np = np.hstack((parent1_np[:, :cutoff], parent2_np[:, cutoff:]))\n    child2_np = np.hstack((parent2_np[:, :cutoff], parent1_np[:, cutoff:]))\n\n    # Convert back to lists\n    child1 = child1_np.tolist()\n    child2 = child2_np.tolist()\n\n    return (child1, child2)\n```\n\n### Explanation\n1. **Validation**: The function first checks if the two parent matrices have the same dimensions. If not, it returns -1.\n2. **Conversion**: The parent matrices are converted into NumPy arrays to leverage efficient matrix operations.\n3. **Crossover**: Using NumPy's slicing capabilities, the function creates two new matrices. For each child, columns before the cutoff are taken from the respective parent, and columns from the cutoff onwards are taken from the other parent.\n4. **Conversion Back**: The resulting NumPy arrays are converted back to Python lists to meet the problem's requirements.\n\nThis approach ensures that the crossover operation is performed efficiently and correctly, adhering to the constraints and requirements specified."}
{"task_id": 294, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a custom graph representation into a plain adjacency dictionary. The custom graph is represented by a dictionary with specific keys, and the goal is to transform it into a format that is easier to serialize or inspect.\n\n### Approach\n1. **Understand the Input Structure**: The input graph `G` is a dictionary with three main components:\n   - `G['is_directed']`: A boolean indicating if the graph is directed.\n   - `G['_V2I']`: A dictionary mapping vertex labels to unique integer indices.\n   - `G['_G']`: A list where each element is a list of outgoing edges for the corresponding vertex. Each edge is a tuple `(source_label, target_label, weight)`.\n\n2. **Initialize the Adjacency Dictionary**: Create an empty adjacency dictionary `adj` where each key is a vertex label, and the value is an empty list.\n\n3. **Process Each Vertex's Edges**: For each vertex, iterate through its outgoing edges. For each edge, add the target vertex and weight to the adjacency list of the source vertex. If the graph is undirected, also add the source vertex and weight to the adjacency list of the target vertex.\n\n4. **Sort the Adjacency Lists**: After processing all edges, sort each adjacency list lexicographically by the neighbor label to ensure a deterministic order.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # Initialize adjacency dictionary with all vertex labels\n    adj = {u: [] for u in G['_V2I'].keys()}\n\n    # Process each vertex's edges\n    for u in G['_V2I']:\n        i = G['_V2I'][u]\n        for edge in G['_G'][i]:\n            v = edge[1]\n            w = edge[2]\n            adj[u].append((v, w))\n            if not G['is_directed']:\n                adj[v].append((u, w))\n\n    # Sort each adjacency list by the neighbor label\n    for u in adj:\n        adj[u].sort(key=lambda x: x[0])\n\n    return adj\n```\n\n### Explanation\n1. **Initialization**: The adjacency dictionary `adj` is initialized with all vertex labels as keys, each mapping to an empty list.\n\n2. **Edge Processing**: For each vertex, we iterate through its outgoing edges. Each edge is processed to add the target vertex and weight to the source vertex's adjacency list. If the graph is undirected, the source vertex and weight are also added to the target vertex's adjacency list.\n\n3. **Sorting**: After processing all edges, each adjacency list is sorted lexicographically by the neighbor label to ensure a consistent and deterministic order.\n\nThis approach ensures that the resulting adjacency dictionary is correctly formatted, with edges appearing in both directions for undirected graphs and sorted lists for each vertex."}
{"task_id": 296, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if all elements in a given NumPy array are binary values, i.e., either 0 or 1. The function should handle arrays of any shape and type, including integer, float, and boolean arrays.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Input to NumPy Array**: Ensure the input is converted into a NumPy array to handle all array-like inputs uniformly.\n2. **Check Binary Values**: For each element in the array, check if it is either 0 or 1. This is done using vectorized operations to ensure efficiency.\n3. **Return Result**: Use the `np.all` function to check if all elements satisfy the condition of being either 0 or 1.\n\nThis approach efficiently handles different data types and ensures that the function works for arrays of any shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    return np.all((x == 0) | (x == 1))\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array using `np.asarray(x)`. This ensures that the function works uniformly for all array-like inputs.\n2. **Binary Check**: The condition `(x == 0) | (x == 1)` is used to check if each element is either 0 or 1. This condition is applied element-wise across the entire array.\n3. **Efficient Check**: The `np.all` function is used to verify if all elements in the array satisfy the binary condition. This function efficiently checks the condition across the entire array in a vectorized manner, ensuring optimal performance.\n\nThis solution efficiently handles various data types and array shapes, ensuring that the function correctly identifies whether all elements are binary values."}
{"task_id": 298, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the log-probability of a query sentence under an N-gram Maximum Likelihood Estimation (MLE) model. The function processes a given corpus to build the N-gram model and then evaluates the log-probability of the query sentence.\n\n### Approach\n1. **Preprocess the Corpus**: \n   - Split each sentence in the corpus into tokens.\n   - For N > 1, pad each token sequence with N-1 special tokens \"<bol>\" at the beginning and one \"<eol>\" at the end.\n   - Count all N-grams and their (N-1)-gram prefixes across the entire corpus.\n\n2. **Preprocess the Query**:\n   - Split the query sentence into tokens.\n   - For N > 1, pad the query tokens with N-1 \"<bol>\" at the beginning and one \"<eol>\" at the end.\n   - Check if the length of the padded query is sufficient for the N-gram model. If not, return negative infinity.\n\n3. **Calculate Log-Probability**:\n   - For each consecutive N-gram in the padded query, check if it exists in the N-gram counts. If any N-gram is unseen, return negative infinity.\n   - Compute the conditional probability for each N-gram using the counts of the N-gram and its prefix.\n   - Sum the natural logarithms of these probabilities and return the result rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\"\"\"\n    if N < 1:\n        return float('-inf')\n    \n    ngram_counts = defaultdict(int)\n    prefix_counts = defaultdict(int)\n    \n    # Process the corpus to build ngram and prefix counts\n    for sentence in corpus:\n        tokens = sentence.split()\n        if N > 1:\n            padded_tokens = ['<bol>'] * (N-1) + tokens + ['<eol>']\n        else:\n            padded_tokens = tokens\n        \n        for i in range(len(padded_tokens) - N + 1):\n            ngram = tuple(padded_tokens[i:i+N])\n            prefix = tuple(padded_tokens[i:i+N-1])\n            ngram_counts[ngram] += 1\n            prefix_counts[prefix] += 1\n    \n    # Process the query\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N-1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n    \n    if len(padded_query) < N:\n        return float('-inf')\n    \n    log_prob = 0.0\n    for i in range(len(padded_query) - N + 1):\n        ngram = tuple(padded_query[i:i+N])\n        if ngram not in ngram_counts:\n            return float('-inf')\n        prefix = tuple(padded_query[i:i+N-1])\n        if prefix not in prefix_counts:\n            return float('-inf')\n        \n        count_ngram = ngram_counts[ngram]\n        count_prefix = prefix_counts[prefix]\n        \n        if count_prefix == 0:\n            return float('-inf')\n        \n        prob = count_ngram / count_prefix\n        log_prob += math.log(prob)\n    \n    return round(log_prob, 4)\n```\n\n### Explanation\n- **Corpus Processing**: Each sentence is split into tokens and padded with special tokens for N > 1. This helps in capturing the context around each token.\n- **Counting N-grams and Prefixes**: We count occurrences of each N-gram and their prefixes to build the probability distribution.\n- **Query Processing**: The query is similarly padded and checked for sufficient length. Each N-gram in the query is evaluated for its probability using the precomputed counts.\n- **Log-Probability Calculation**: The log-probability of the query is computed by summing the log of each N-gram's conditional probability. If any N-gram is unseen, the result is negative infinity.\n\nThis approach ensures that we efficiently build the N-gram model and accurately compute the log-probability of the query sentence."}
{"task_id": 302, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Spectral Clustering algorithm from scratch. The algorithm involves several steps, including building a weighted adjacency matrix, computing the graph Laplacian, finding the eigenvectors, and running k-means clustering. Let's break down the approach and the solution.\n\n### Approach\n1. **Build the Weighted Adjacency Matrix (W):** \n   - Compute the pairwise Euclidean distances between all points.\n   - Construct the weighted adjacency matrix where the weight between two points is inversely proportional to their distance.\n\n2. **Compute the Graph Laplacian (L):**\n   - Calculate the degree matrix (D) from the weighted adjacency matrix.\n   - Subtract the weighted adjacency matrix from the degree matrix to get the Laplacian matrix.\n\n3. **Find the Eigenvectors:**\n   - Compute the eigenvalues and eigenvectors of the Laplacian matrix.\n   - Select the first k eigenvectors corresponding to the smallest eigenvalues to form the projection matrix.\n\n4. **Run k-means Clustering:**\n   - Initialize the centroids using the first k rows of the projection matrix.\n   - Iteratively assign points to the nearest centroid and update the centroids until convergence or maximum iterations are reached.\n   - Handle empty clusters by re-initializing their centroids with random rows from the projection matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n, d = X.shape\n    \n    # Step 1: Compute the weighted adjacency matrix W\n    G = X @ X.T\n    norms = np.sum(X ** 2, axis=1)\n    D_sq = norms[:, np.newaxis] + norms[np.newaxis, :] - 2 * G\n    D = np.sqrt(D_sq)\n    W = 1 / (1 + D)\n    W = np.where(np.eye(n), 0, W)\n    \n    # Step 2: Compute the graph Laplacian L\n    D_diag = np.diag(np.sum(W, axis=1))\n    L = D_diag - W\n    \n    # Step 3: Compute the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n    \n    # Step 4: Run k-means on the rows of E\n    n_points = E.shape[0]\n    centroids = E[:k, :]\n    assignments = np.zeros(n_points, dtype=int)\n    \n    for iter in range(100):\n        # Compute distances from each point to each centroid\n        distances = np.sum((E[:, np.newaxis] - centroids[np.newaxis, :]) ** 2, axis=2)\n        assignments = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = np.zeros((k, E.shape[1]))\n        for c in range(k):\n            cluster_points = E[assignments == c]\n            if len(cluster_points) == 0:\n                # Re-initialize with a random row from E\n                random_row = np.random.randint(n_points)\n                new_centroids[c] = E[random_row]\n            else:\n                new_centroids[c] = np.mean(cluster_points, axis=0)\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        centroids = new_centroids\n    \n    return assignments.tolist()\n```\n\n### Explanation\n1. **Weighted Adjacency Matrix (W):** This matrix is built by computing the inverse of the Euclidean distances between all pairs of points. The diagonal elements are set to zero to avoid self-connections.\n\n2. **Graph Laplacian (L):** This matrix is derived from the degree matrix (D) and the weighted adjacency matrix (W). It is used to capture the structure of the data graph.\n\n3. **Eigenvectors:** The first k eigenvectors of the Laplacian matrix are used to project the data into a lower-dimensional space, which helps in clustering.\n\n4. **k-means Clustering:** The projected data points are clustered using k-means, starting with the first k points as initial centroids. The algorithm iteratively refines the clusters until convergence or a maximum number of iterations is reached, ensuring that empty clusters are re-initialized with random points from the projection matrix.\n\nThis approach ensures that the data is clustered effectively using spectral methods, leveraging linear algebra operations and iterative optimization."}
{"task_id": 303, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that formats a detailed debugging string comparing the current output of a program to the expected (gold-standard) output. The function should follow a specific layout and include optional warning messages.\n\n### Approach\n1. **Identify Previous and Current Elements**: Given an index `ix`, determine the previous element in the list `params` using `max(ix - 1, 0)` to handle cases where `ix` is 0.\n2. **Extract Values**: Extract the current and previous values from `params` and their corresponding expected values from `golds`.\n3. **Construct Lines**: Format each line of the debugging string according to the specified layout. This includes lines for the previous and current elements, their respective outputs, and any additional warning message.\n4. **Assemble the String**: Combine all formatted lines into a single string with the correct number of dashes and line breaks.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    line1 = '-' * 25 + 'DEBUG' + '-' * 25\n    line2 = f'Mine (prev) [{prev_label}]:'\n    line3 = prev_mine\n    line4 = f'Theirs (prev) [{prev_label}]:'\n    line5 = prev_gold\n    line6 = f'Mine [{curr_label}]:'\n    line7 = curr_mine\n    line8 = f'Theirs [{curr_label}]:'\n    line9 = curr_gold + warn_str\n    line10 = '-' * 23 + 'END DEBUG' + '-' * 23\n    \n    lines = [line1, line2, line3, line4, line5, line6, line7, line8, line9, line10]\n    return '\\n'.join(lines)\n```\n\n### Explanation\n1. **Previous and Current Elements**: The function calculates the previous index using `max(ix - 1, 0)` to ensure it does not go out of bounds. It then retrieves the corresponding elements from `params`.\n2. **Expected Values**: The function fetches the expected values from the `golds` dictionary using the labels from the previous and current elements.\n3. **Line Construction**: Each line of the debugging string is constructed using formatted strings. The lines include the previous and current elements' outputs, their expected values, and any warning message.\n4. **String Assembly**: The lines are combined into a single string with newline characters, ensuring the correct layout with the specified number of dashes and section headers.\n\nThis approach ensures that the debugging string is formatted exactly as required, providing a clear and structured comparison between the program's output and the expected results."}
{"task_id": 304, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the posterior mean (Maximum-A-Posteriori estimate) of the regression coefficients in a Bayesian linear regression model with an unknown noise variance. The model uses a normal-inverse-gamma conjugate prior on the coefficients and the noise variance. The function should handle different prior specifications and return the posterior mean rounded to four decimal places.\n\n### Approach\n1. **Preprocess the Design Matrix**: If `fit_intercept` is True, prepend a column of ones to the design matrix X to account for the intercept term.\n2. **Determine the Number of Features**: After preprocessing, determine the number of features, M, which is the number of columns in the design matrix X.\n3. **Process the Prior Mean (mu)**: If mu is a scalar, broadcast it to a vector of length M. If mu is already a vector, ensure its length matches M.\n4. **Process the Prior Scale (V)**: Handle different cases for V:\n   - If V is None, use an identity matrix of size M.\n   - If V is a scalar, create a diagonal matrix with the scalar on the diagonal.\n   - If V is a list or tuple, treat it as the diagonal elements of the matrix.\n5. **Compute the Inverse of V**: Calculate the inverse of the matrix V.\n6. **Compute the Posterior Covariance**: Calculate the posterior covariance matrix using the formula provided.\n7. **Compute the Posterior Mean**: Use the posterior covariance and the given terms to compute the posterior mean of the coefficients.\n8. **Round the Result**: Round the resulting posterior mean to four decimal places and return it as a list of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Prepend intercept if needed\n    if fit_intercept:\n        X = np.hstack([X, np.ones((X.shape[0], 1))])\n    M = X.shape[1]\n    \n    # Process mu\n    if isinstance(mu, (int, float)):\n        mu = np.full(M, mu)\n    else:\n        mu = np.asarray(mu)\n        if len(mu) != M:\n            raise ValueError(\"Length of mu must match the number of features.\")\n    \n    # Process V\n    if V is None:\n        V = np.eye(M)\n    elif isinstance(V, (int, float)):\n        V = V * np.eye(M)\n    elif isinstance(V, (list, tuple)):\n        if len(V) != M:\n            raise ValueError(\"Length of V must match the number of features.\")\n        V = np.diag(V)\n    else:\n        if V.shape != (M, M):\n            raise ValueError(\"V must be a square matrix of size (M, M).\")\n    \n    # Compute V inverse\n    V_inv = np.linalg.inv(V)\n    \n    # Compute X^T X\n    XTX = X.T @ X\n    \n    # Compute sum of inverses\n    sum_inv = V_inv + XTX\n    \n    # Compute posterior covariance\n    Sigma_b = np.linalg.inv(sum_inv)\n    \n    # Compute V_inv * mu\n    V_inv_mu = V_inv @ mu\n    \n    # Compute X^T y\n    XTy = X.T @ y\n    \n    # Compute the term\n    term = V_inv_mu + XTy\n    \n    # Compute posterior mean\n    mu_b = Sigma_b @ term\n    \n    # Round to 4 decimal places\n    mu_b = np.round(mu_b, 4)\n    \n    return mu_b.tolist()\n```\n\n### Explanation\nThe function begins by preprocessing the design matrix X to include an intercept if required. It then processes the prior mean (mu) and scale (V) to ensure they are in the correct format. The inverse of the prior scale matrix V is computed, followed by the posterior covariance matrix. Using these matrices, the posterior mean of the coefficients is calculated and rounded to four decimal places before being returned as a list of floats. This approach efficiently handles different prior specifications and ensures accurate computation of the posterior mean."}
{"task_id": 308, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Mel filterbank matrix that can convert a power spectrum vector into Mel band energies. The Mel filterbank is a set of triangular filters laid out on the Mel scale, which is a perceptual scale that provides high resolution to low frequencies and low resolution to high frequencies.\n\n### Approach\n1. **Convert Frequency Limits to Mel Scale**: Convert the given minimum and maximum frequencies to the Mel scale using the provided helper functions.\n2. **Generate Mel Points**: Create `n_filters + 2` equally spaced points on the Mel scale between the converted minimum and maximum Mel values.\n3. **Convert Mel Points to Hz**: Convert these equally spaced Mel points back to Hz to get the corner frequencies of the triangular filters.\n4. **Compute Filter Weights**: For each DFT bin and each filter, compute the weight of the filter at that bin. The weight is determined by the triangular shape of the filter, which increases linearly from the lower corner to the midpoint and then decreases linearly to the upper corner.\n5. **Normalize Filters**: If normalization is required, scale each filter by a factor to ensure the area under each filter in Mel space equals 1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    # Step 1: Compute min and max Mel values\n    if max_freq is None:\n        max_freq = fs / 2\n    # Compute min_mel and max_mel\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n    \n    # Step 2: Generate n_filters + 2 equally spaced Mel points\n    mel_step = (max_mel - min_mel) / (n_filters + 1)\n    mel_points = [min_mel + i * mel_step for i in range(n_filters + 2)]\n    \n    # Step 3: Convert Mel points back to Hz\n    f = []\n    for p in mel_points:\n        freq = 700 * (10 ** (p / 2595) - 1)\n        f.append(freq)\n    \n    # Step 4: Compute the filter weights\n    n_bins = N // 2 + 1\n    F = []\n    for i in range(n_filters):\n        filter_weights = []\n        for k in range(n_bins):\n            f_k = k * fs / N\n            f_left = f[i]\n            f_right = f[i + 1]\n            if f_k < f_left or f_k > f_right:\n                weight = 0.0\n            else:\n                f_center = (f_left + f_right) / 2\n                if f_k <= f_center:\n                    left = (f_k - f_left) / (f_center - f_left)\n                    weight = left\n                else:\n                    right = (f_right - f_k) / (f_right - f_center)\n                    weight = right\n            filter_weights.append(weight)\n        F.append(filter_weights)\n    \n    # Step 5: Normalize if required\n    if normalize:\n        for i in range(n_filters):\n            denominator = f[i + 2] - f[i]\n            scaling_factor = 2.0 / denominator\n            F[i] = [w * scaling_factor for w in F[i]]\n    \n    # Round each element to 4 decimal places\n    F = [[round(w, 4) for w in row] for row in F]\n    \n    return F\n```\n\n### Explanation\n1. **Convert Frequencies to Mel Scale**: The minimum and maximum frequencies are converted to the Mel scale using the formula `mel = 2595 * log10(1 + f/700)`.\n2. **Generate Mel Points**: These points are equally spaced between the minimum and maximum Mel values, creating `n_filters + 2` points.\n3. **Convert Back to Hz**: Each Mel point is converted back to Hz to determine the corner frequencies of the triangular filters.\n4. **Compute Weights**: For each filter and each DFT bin, the weight is calculated based on the triangular shape of the filter. The weight is zero outside the filter's range, increases linearly to the midpoint, and then decreases linearly.\n5. **Normalization**: If normalization is enabled, each filter is scaled to ensure the area under the filter in Mel space is 1, using the formula `w_i = 2 / (f_{i+2} - f_i) * w_i`.\n\nThis approach ensures that the resulting matrix can be used to convert a power spectrum vector into Mel band energies efficiently."}
{"task_id": 312, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Blackman-Harris window of a given length, which can be either symmetric or periodic. The Blackman-Harris window is a popular window function used in signal processing to minimize spectral leakage when performing Fourier transforms.\n\n### Approach\n1. **Problem Analysis**: The Blackman-Harris window is defined by a specific formula involving cosine terms with fixed coefficients. The window can be generated in two forms: symmetric and periodic. The symmetric form is suitable for Finite Impulse Response (FIR) filter design, while the periodic form is used for FFT-based spectral analysis.\n\n2. **Special Cases Handling**: \n   - If the window length is less than or equal to 0, return an empty list.\n   - If the window length is 1, return a list containing a single element [1.0].\n\n3. **Window Generation**:\n   - For the symmetric form, the window length is `L`, and the formula uses `L-1` as the denominator for the cosine terms.\n   - For the periodic form, the window is generated for `L+1` points, but the last point is discarded to ensure the window can tile seamlessly for an FFT of length `L`.\n\n4. **Cosine Terms Calculation**: Using NumPy, we compute the cosine terms for each point in the window and combine them according to the Blackman-Harris formula.\n\n5. **Rounding**: The resulting window values are rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    if symmetric:\n        N = window_len - 1\n        n = np.arange(window_len)\n    else:\n        N = window_len\n        n = np.arange(window_len + 1)\n    \n    # Compute each cosine term\n    cos1 = np.cos(2 * np.pi * n / N)\n    cos2 = np.cos(4 * np.pi * n / N)\n    cos3 = np.cos(6 * np.pi * n / N)\n    \n    # Compute BH window\n    bh = a0 - a1 * cos1 + a2 * cos2 - a3 * cos3\n    \n    # For periodic case, take first window_len points\n    if not symmetric:\n        bh = bh[:window_len]\n    \n    # Round to four decimal places\n    bh_rounded = np.round(bh, 4)\n    \n    # Convert to list of floats\n    return bh_rounded.tolist()\n```\n\n### Explanation\n- **Special Cases Handling**: The function immediately returns an empty list for non-positive window lengths and a single-element list for window length 1.\n- **Window Generation**: Depending on whether the window is symmetric or periodic, the function computes the appropriate values using the Blackman-Harris formula. For the periodic form, it generates one extra point and then discards the last point to ensure the window can tile seamlessly.\n- **Cosine Terms Calculation**: Using NumPy's `cos` function, the necessary cosine terms are computed efficiently.\n- **Rounding**: The resulting window values are rounded to four decimal places to meet the precision requirement.\n\nThis approach ensures that the generated window is both accurate and efficient, adhering to the specific requirements of the Blackman-Harris window function."}
{"task_id": 313, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the expectation of the logarithm of a component of a Dirichlet-distributed random vector. This involves using the digamma function, which is the first derivative of the logarithm of the gamma function. Since we cannot use external scientific libraries, we will implement the digamma function ourselves using a combination of recursion and an asymptotic expansion.\n\n### Approach\n1. **Understanding the Digamma Function**: The digamma function, \u03c8(x), is the first derivative of the logarithm of the gamma function. For small values of x, we use a recursive approach to shift x up to a moderate value. For larger values, we use an asymptotic expansion to approximate \u03c8(x).\n\n2. **Recursive Approach for Small x**: For values of x less than 6, we recursively apply the relation \u03c8(x) = \u03c8(x+1) - 1/x until x reaches 6. We then use the asymptotic expansion to compute \u03c8(6).\n\n3. **Asymptotic Expansion for Large x**: For values of x 6 or greater, we use the asymptotic expansion:\n   \\[\n   \u03c8(x) \u2248 \\ln x \u2212 \\frac{1}{2x} \u2212 \\frac{1}{12x^2} + \\frac{1}{120x^4} \u2212 \\frac{1}{252x^6}\n   \\]\n   This expansion provides a good approximation for larger values of x.\n\n4. **Compute the Expectation**: Using the digamma function, we compute the expectation of the logarithm of the t-th component of the Dirichlet distribution as \u03c8(\u03b3[d,t]) \u2212 \u03c8(\u2211\u2096 \u03b3[d,k]).\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    \n    def compute_expansion(x):\n        ln_x = math.log(x)\n        term1 = 1.0 / (2 * x)\n        term2 = 1.0 / (12 * x**2)\n        term3 = 1.0 / (120 * x**4)\n        term4 = 1.0 / (252 * x**6)\n        return ln_x - term1 - term2 + term3 - term4\n    \n    def digamma(x):\n        if x < 6:\n            sum_reciprocal = 0.0\n            current = x\n            while current < 6:\n                sum_reciprocal += 1.0 / current\n                current += 1\n            psi_6 = compute_expansion(6)\n            return psi_6 - sum_reciprocal\n        else:\n            return compute_expansion(x)\n    \n    gamma_d = gamma[d]\n    sum_gamma_d = sum(gamma_d)\n    x = gamma_d[t]\n    \n    psi_x = digamma(x)\n    psi_sum = digamma(sum_gamma_d)\n    \n    result = psi_x - psi_sum\n    return round(result, 4)\n```\n\n### Explanation\n1. **Helper Functions**: The `compute_expansion` function calculates the asymptotic expansion of the digamma function for large values of x. The `digamma` function uses this expansion for x >= 6 and shifts smaller values up to 6 using recursion.\n\n2. **Main Function**: The main function `expected_log_dirichlet` extracts the relevant parameters from the gamma matrix, computes the necessary digamma values, and returns the result rounded to four decimal places.\n\nThis approach ensures that we accurately compute the expectation using the digamma function without relying on external libraries, providing a solution that is both efficient and self-contained."}
{"task_id": 317, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a formatted debug error report based on given inputs. The report should compare predictions made by a program with expected results and include optional warnings. The report must follow a specific structure with headers, previous and current item details, and a footer.\n\n### Approach\n1. **Extract Previous and Current Items**: Determine the index of the previous item using `max(ix - 1, 0)` to ensure it doesn't go below zero. Extract the prediction and label for both the previous and current items from the `params` list.\n2. **Retrieve Gold Values**: Use the labels from the previous and current items to fetch their corresponding gold values from the `golds` dictionary.\n3. **Construct Report Components**: Build each part of the report string:\n   - **Header**: A line with 25 dashes, the word \"DEBUG\", and another 25 dashes.\n   - **Previous Item Details**: Two lines for the previous item's prediction and gold value.\n   - **Current Item Details**: Two lines for the current item's prediction and gold value, with an optional warning appended to the gold value line.\n   - **Footer**: A line with 23 dashes, the word \"END DEBUG\", and another 23 dashes.\n4. **Assemble the Report**: Combine all components into a single string with appropriate newlines and blank lines as specified.\n\n### Solution Code\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    header = '-' * 25 + ' DEBUG ' + '-' * 25 + '\\n'\n    prev_mine = f\"Mine (prev) [{prev_label}]:\\n{prev_pred}\\n\"\n    prev_theirs = f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\"\n    curr_mine = f\"Mine [{curr_label}]:\\n{curr_pred}\\n\"\n    curr_theirs = f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\"\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23 + '\\n'\n    \n    report = (\n        header\n        + prev_mine\n        + prev_theirs\n        + '\\n\\n'  # Two blank lines\n        + curr_mine\n        + curr_theirs\n        + footer\n    )\n    return report\n```\n\n### Explanation\n- **Header and Footer Construction**: The header and footer are constructed using fixed strings with the required number of dashes and the appropriate text.\n- **Previous and Current Items**: The previous item is determined using `max(ix - 1, 0)` to handle edge cases where `ix` is 0. The current item is directly accessed using `ix`.\n- **Report Assembly**: Each part of the report is constructed as a string and concatenated in the specified order, ensuring correct formatting with newlines and blank lines.\n\nThis approach ensures that the report is generated accurately and efficiently, adhering to the specified format."}
{"task_id": 318, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak learners. The goal is to train an ensemble of these weak learners and use them to predict the labels of a test dataset.\n\n### Approach\n1. **Initialization**: Start by initializing the weights of each training sample to be equal.\n2. **Training the Weak Learners**: For each boosting round, train a decision stump on the current weighted training data. Each decision stump is a one-level decision tree that splits the data based on a single feature and a threshold.\n3. **Error Calculation**: For each feature, compute the best possible threshold and polarity that minimizes the weighted classification error. This involves sorting the samples by the feature values and computing prefix sums of weights for both classes.\n4. **Update Weights**: After selecting the best decision stump for the current round, update the weights of the training samples. Samples that were misclassified have their weights increased, while those correctly classified have their weights decreased.\n5. **Prediction**: After training all the weak learners, use them to predict the labels of the test samples by computing the weighted sum of their predictions and taking the sign of this sum.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    n_samples = X_train.shape[0]\n    weights = np.ones(n_samples) / n_samples\n\n    stumps = []\n    alphas = []\n\n    for _ in range(n_estimators):\n        best_error = 1.0\n        best_j = -1\n        best_t = 0.0\n        best_p = 1\n\n        for j in range(X_train.shape[1]):\n            Xj = X_train[:, j]\n            current_weights = weights.copy()\n\n            samples = list(zip(Xj, y_train, current_weights))\n            samples.sort(key=lambda x: x[0])\n\n            y1 = []\n            y_neg1 = []\n            for x, yi, wi in samples:\n                if yi == 1:\n                    y1.append(wi)\n                else:\n                    y_neg1.append(wi)\n\n            prefix_1 = np.cumsum(y1)\n            prefix_neg1 = np.cumsum(y_neg1)\n\n            min_error = 1.0\n            best_i = 0\n            best_p_j = 1\n\n            for i in range(len(samples) - 1):\n                sum_neg1_ge_t = sum(y_neg1[i+1:])\n                sum_1_lt_t = prefix_1[i] if i >= 0 else 0\n                error_p1 = sum_neg1_ge_t + sum_1_lt_t\n\n                sum_1_ge_t = sum(y1[i+1:])\n                sum_neg1_lt_t = prefix_neg1[i] if i >= 0 else 0\n                error_p_neg1 = sum_1_ge_t + sum_neg1_lt_t\n\n                current_min = min(error_p1, error_p_neg1)\n                if current_min < min_error:\n                    min_error = current_min\n                    best_i = i\n                    if current_min == error_p1:\n                        best_p_j = 1\n                    else:\n                        best_p_j = -1\n\n            if len(samples) > 1:\n                t = (samples[best_i][0] + samples[best_i + 1][0]) / 2\n            else:\n                t = samples[best_i][0]\n\n            if min_error < best_error:\n                best_error = min_error\n                best_j = j\n                best_t = t\n                best_p = best_p_j\n\n        if best_error == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * math.log((1 - best_error) / best_error)\n\n        if best_error == 0:\n            factor = np.zeros(n_samples)\n            for i in range(n_samples):\n                if y_train[i] == best_p * np.sign(X_train[i, best_j] - best_t):\n                    factor[i] = 0\n                else:\n                    factor[i] = float('inf')\n        else:\n            h = best_p * np.sign(X_train[:, best_j] - best_t)\n            yh = y_train * h\n            factor = np.exp(-alpha * yh)\n\n        weights = weights * factor\n        sum_weights = np.sum(weights)\n        if sum_weights != 0:\n            weights = weights / sum_weights\n        else:\n            weights = np.zeros(n_samples)\n\n        stumps.append((best_j, best_t, best_p))\n        alphas.append(alpha)\n\n    predictions = []\n    for x in X_test:\n        total = 0.0\n        for m in range(n_estimators):\n            j, t, p = stumps[m]\n            h = p * np.sign(x[j] - t)\n            total += alphas[m] * h\n        pred = np.sign(total)\n        if pred == 0:\n            pred = 1\n        predictions.append(int(pred))\n\n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for each training sample are initialized to be equal, ensuring each sample starts with the same influence.\n2. **Training Loop**: For each boosting round, the algorithm iterates over each feature to find the best decision stump. This involves sorting the samples by the feature values and computing the prefix sums of weights for both classes to determine the optimal threshold and polarity.\n3. **Error Calculation**: For each possible split point, the error for both polarities (1 and -1) is computed. The split point and polarity with the minimal error are selected.\n4. **Weight Update**: After selecting the best decision stump, the weights of the training samples are updated based on whether they were correctly classified. This step ensures that subsequent rounds focus more on the samples that were previously misclassified.\n5. **Prediction**: For each test sample, the predictions from all weak learners are combined using their respective weights. The final prediction is determined by the sign of the weighted sum of these predictions.\n\nThis approach efficiently combines weak learners to form a strong classifier, leveraging the power of ensemble methods to improve predictive accuracy."}
{"task_id": 329, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the center frequencies of each DFT bin for a given number of DFT coefficients (N) and sampling frequency (fs). The function should handle two modes: returning only the non-negative frequencies or all frequencies, including negative ones.\n\n### Approach\n1. **Check for Valid Input**: If either N or fs is non-positive, return an empty list.\n2. **Calculate Frequency Step**: Compute the frequency step (df) as fs divided by N.\n3. **Compute Frequencies**:\n   - For positive-only mode, compute frequencies from 0 to N/2.\n   - For all frequencies mode, compute each frequency and adjust negative frequencies by subtracting N times the frequency step if the index exceeds N/2.\n4. **Round Frequencies**: Round each computed frequency to 4 decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    \n    df = fs / N\n    if positive_only:\n        K = N // 2\n        frequencies = [k * df for k in range(K + 1)]\n    else:\n        frequencies = []\n        for k in range(N):\n            f = k * df\n            if k > N / 2:\n                f -= N * df\n            frequencies.append(f)\n    \n    # Round each frequency to 4 decimal places\n    rounded_freqs = [round(f, 4) for f in frequencies]\n    \n    return rounded_freqs\n```\n\n### Explanation\n- **Input Validation**: The function first checks if N or fs is non-positive and returns an empty list if true.\n- **Frequency Step Calculation**: The frequency step (df) is calculated as the sampling frequency divided by the number of DFT coefficients.\n- **Positive-Only Mode**: For positive-only mode, the function computes frequencies from 0 to N/2, inclusive.\n- **All Frequencies Mode**: For all frequencies mode, the function computes each frequency and adjusts negative frequencies by subtracting N times the frequency step if the index exceeds N/2.\n- **Rounding**: Each computed frequency is rounded to 4 decimal places to ensure the result meets the specified precision.\n\nThis approach efficiently handles both modes and ensures accurate computation of DFT bin frequencies."}
{"task_id": 331, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the out-of-bag (OOB) mean squared error (MSE) for a Random Forest regressor. The OOB-MSE is an unbiased performance estimate obtained by using the predictions from trees that do not contain a given sample in their training data.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input arrays to NumPy arrays for efficient processing.\n2. **Initialize Variables**: Prepare to store squared errors for samples that have at least one OOB prediction.\n3. **Iterate Over Each Sample**: For each sample, determine which trees have OOB predictions for that sample.\n4. **Compute OOB Predictions**: For each sample, collect the predictions from the trees where the sample was OOB. If there are any such predictions, compute the average of these predictions.\n5. **Calculate Squared Error**: Compute the squared error between the average OOB prediction and the true value for the sample.\n6. **Handle Edge Cases**: If no sample has any OOB predictions, return -1. Otherwise, compute the mean of the squared errors and return it rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to the nearest 4th decimal place, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask)\n    \n    n_samples = y_true.shape[0]\n    squared_errors = []\n    \n    for i in range(n_samples):\n        oob_indices = np.where(oob_mask[i])[0]\n        if len(oob_indices) == 0:\n            continue\n        oob_preds = predictions[i, oob_indices]\n        avg = np.mean(oob_preds)\n        error = (avg - y_true[i])**2\n        squared_errors.append(error)\n    \n    if not squared_errors:\n        return -1.0\n    else:\n        oob_mse_value = np.mean(squared_errors)\n        return round(oob_mse_value, 4)\n```\n\n### Explanation\n- **Convert Inputs**: The function starts by converting the input lists or arrays into NumPy arrays to facilitate efficient array operations.\n- **Iterate Over Samples**: For each sample, it checks which trees have OOB predictions using the `oob_mask`.\n- **Compute OOB Predictions**: For each sample, it collects the predictions from the trees where the sample was OOB and computes the average of these predictions.\n- **Calculate Squared Error**: The squared error between the average OOB prediction and the true value is computed for each sample.\n- **Handle Edge Cases**: If no sample has any OOB predictions, the function returns -1. Otherwise, it computes the mean of the squared errors and returns it rounded to four decimal places.\n\nThis approach ensures that we efficiently compute the OOB-MSE while handling all edge cases appropriately."}
{"task_id": 332, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Frequent-Pattern Tree (FP-Tree) from a given set of transactions and a minimum support threshold. The FP-Tree is a data structure used in frequent pattern mining, where each node represents an item and the path from the root to a node represents a combination of items. The support of each node is the number of transactions that include that combination.\n\n### Approach\n1. **Compute Support Counts**: For each item in the transactions, count how many transactions it appears in. This gives us the support for each item.\n2. **Filter Items**: Discard items whose support is less than the given minimum support threshold.\n3. **Global Ordering**: Sort the remaining items first by decreasing support, then alphabetically to break ties.\n4. **Construct FP-Tree**: For each transaction, remove duplicates, keep only frequent items, reorder them according to the global ordering, and update the FP-Tree accordingly.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Compute support counts for each item\n    support_counts = {}\n    for trans in transactions:\n        unique_items = set(trans)\n        for item in unique_items:\n            support_counts[item] = support_counts.get(item, 0) + 1\n\n    # Step 2: Filter items with support >= min_support\n    frequent_items = {item: count for item, count in support_counts.items() if count >= min_support}\n\n    # If no frequent items, return root node\n    if not frequent_items:\n        return {'support': 0, 'children': {}}\n\n    # Step 3: Create global ordering of items\n    items = list(frequent_items.keys())\n    # Sort by support descending, then by item name ascending\n    items.sort(key=lambda x: (-frequent_items[x], x))\n\n    # Step 4: Build the FP-Tree\n    root = {'support': 0, 'children': {}}\n\n    for trans in transactions:\n        # Remove duplicates and filter non-frequent items\n        unique_items = set(trans)\n        filtered = [item for item in unique_items if item in frequent_items]\n\n        # Reorder the items according to the global ordering\n        filtered_sorted = sorted(filtered, key=lambda x: (-frequent_items[x], x))\n\n        current_node = root\n        for item in filtered_sorted:\n            if item in current_node['children']:\n                # Move to the child and increment support\n                current_node = current_node['children'][item]\n                current_node['support'] += 1\n            else:\n                # Create a new node\n                new_node = {'support': 1, 'children': {}}\n                current_node['children'][item] = new_node\n                current_node = new_node\n\n    return root\n```\n\n### Explanation\n1. **Compute Support Counts**: We iterate through each transaction and count how many times each item appears across all transactions. This gives us the support for each item.\n2. **Filter Items**: We discard items that do not meet the minimum support threshold.\n3. **Global Ordering**: We sort the remaining items by their support in descending order. If two items have the same support, they are sorted alphabetically.\n4. **Construct FP-Tree**: For each transaction, we remove duplicates and keep only the frequent items. We then reorder these items according to the global ordering. We then update the FP-Tree by traversing from the root and creating nodes as needed, incrementing the support of each node along the path.\n\nThis approach ensures that we efficiently build the FP-Tree while adhering to the constraints and requirements of the problem."}
{"task_id": 336, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel matrix for given input vectors X and Y. The linear kernel function is defined as \\( k(x, y) = x \\cdot y + c_0 \\), where \\( x \\cdot y \\) is the dot product of vectors x and y, and \\( c_0 \\) is an optional constant. The function should handle both lists and NumPy arrays as input and return the kernel matrix as a list of lists of floats, rounded to four decimal places.\n\n### Approach\n1. **Handle Y being None**: If Y is not provided (i.e., Y is None), set Y to be the same as X.\n2. **Convert Inputs to NumPy Arrays**: Convert X and Y to NumPy arrays if they are provided as lists. This allows efficient matrix operations.\n3. **Check for Empty X**: If X is empty (i.e., has zero rows), return an empty list immediately.\n4. **Validate Column Counts**: Ensure that X and Y have the same number of columns. If not, return -1.\n5. **Compute the Kernel Matrix**: Use matrix multiplication to compute the dot product of each pair of rows from X and Y. Add the constant \\( c_0 \\) to each element of the resulting matrix.\n6. **Round and Convert to List**: Round the resulting matrix to four decimal places and convert it to a list of lists of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # Handle Y being None\n    if Y is None:\n        Y = X\n\n    # Convert X and Y to NumPy arrays if they are lists\n    if isinstance(X, list):\n        X_np = np.array(X)\n    else:\n        X_np = X\n\n    if isinstance(Y, list):\n        Y_np = np.array(Y)\n    else:\n        Y_np = Y\n\n    # Check if X is empty\n    if X_np.shape[0] == 0:\n        return []\n\n    # Check if the number of columns are the same\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n\n    # Compute the dot product matrix\n    dot_product = X_np @ Y_np.T\n\n    # Add c0\n    kernel_matrix = dot_product + c0\n\n    # Round to 4 decimals\n    kernel_matrix_rounded = np.round(kernel_matrix, 4)\n\n    # Convert to list of lists\n    result = kernel_matrix_rounded.tolist()\n\n    # Return the result\n    return result\n```\n\n### Explanation\n- **Handling Y as None**: This ensures that if Y is not provided, the function computes a square matrix where both inputs are the same.\n- **Conversion to NumPy Arrays**: This step allows efficient matrix operations using NumPy's optimized routines.\n- **Empty X Check**: If X is empty, the function immediately returns an empty list, as there are no vectors to compute the kernel for.\n- **Column Validation**: Ensures that the inputs have compatible shapes for matrix multiplication.\n- **Matrix Computation**: Uses NumPy's matrix multiplication to efficiently compute the dot products for all pairs of vectors.\n- **Rounding and Conversion**: The result is rounded to four decimal places and converted to a list of lists for the required output format.\n\nThis approach ensures that the function handles various input types and edge cases efficiently and correctly."}
{"task_id": 340, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single step of the Adam (Adaptive Moment Estimation) optimization algorithm. The Adam optimizer combines the benefits of two different adaptive moment estimation techniques, making it suitable for training deep learning models. The task involves updating the parameters of a model based on the current gradient, time step, and running moment estimates.\n\n### Approach\nThe Adam optimizer updates parameters using the following steps:\n1. **First Moment Estimate (Mean of Gradients):** This is computed as a running average of the gradients.\n2. **Second Moment Estimate (Variance of Gradients):** This is computed as a running average of the squared gradients.\n3. **Bias Correction:** The first and second moment estimates are bias-corrected to get more accurate estimates.\n4. **Parameter Update:** The parameters are updated using the learning rate, the bias-corrected estimates, and the gradient.\n\nThe steps are as follows:\n1. Compute the new first moment estimate using the current gradient.\n2. Compute the new second moment estimate using the current gradient.\n3. Bias-correct the first and second moment estimates.\n4. Update the parameters using the learning rate, the bias-corrected estimates, and the gradient.\n5. If the gradient is exactly zero, the parameter remains unchanged, but the moment estimates are still updated.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad: Current gradient \u2207\u03b8 (same shape as *param*).\n        t: Time-step counter **starting at 1**.\n        m: First moment estimate from the previous step (same shape).\n        v: Second moment estimate from the previous step (same shape).\n        lr: Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps: Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) as\n        python lists, each rounded to 8 decimal places.\n    \"\"\"\n    # Compute new first and second moment estimates\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute denominators for bias correction\n    denom_m = 1 - (beta1 ** t)\n    denom_v = 1 - (beta2 ** t)\n    \n    # Compute bias-corrected estimates\n    m_hat = new_m / denom_m\n    v_hat = new_v / denom_v\n    \n    # Check if gradient is exactly zero\n    if np.all(grad == 0):\n        theta_new = param.copy()\n    else:\n        # Compute the parameter update\n        delta = (lr * m_hat) / np.sqrt(v_hat + eps)\n        theta_new = param - delta\n    \n    # Round the results to 8 decimal places and convert to lists\n    theta_new_rounded = np.round(theta_new, 8)\n    theta_new_list = theta_new_rounded.tolist()\n    \n    new_m_rounded = np.round(new_m, 8)\n    new_m_list = new_m_rounded.tolist()\n    \n    new_v_rounded = np.round(new_v, 8)\n    new_v_list = new_v_rounded.tolist()\n    \n    return (theta_new_list, new_m_list, new_v_list)\n```\n\n### Explanation\n1. **First Moment Estimate (new_m):** This is computed as a weighted average of the previous first moment and the current gradient.\n2. **Second Moment Estimate (new_v):** This is computed as a weighted average of the previous second moment and the square of the current gradient.\n3. **Bias Correction:** The first and second moment estimates are adjusted to remove the bias from their running averages.\n4. **Parameter Update:** The parameters are updated using the learning rate, the bias-corrected first moment estimate, and the square root of the bias-corrected second moment estimate plus a small constant to avoid division by zero.\n5. **Gradient Check:** If the gradient is exactly zero, the parameter remains unchanged, but the moment estimates are still updated.\n\nThis approach ensures that the parameters are updated efficiently while handling edge cases such as zero gradients appropriately. The solution is vectorized to handle multi-dimensional parameters and gradients, making it suitable for large-scale deep learning models."}
{"task_id": 343, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) used in Reinforcement Learning to update policy parameters. The goal is to retain the top-performing parameter vectors based on their rewards, compute their mean and variance, and return these values rounded to four decimal places.\n\n### Approach\n1. **Convert Rewards to NumPy Array**: Ensure the rewards are in a NumPy array format for efficient sorting and indexing.\n2. **Determine Retained Samples**: Calculate the number of top-performing samples to retain using the given fraction `retain_prcnt`.\n3. **Sort Samples by Reward**: Sort the parameter vectors based on their corresponding rewards in descending order.\n4. **Select Top Samples**: Retain the top `k` parameter vectors where `k` is the number of samples to retain.\n5. **Compute Mean and Variance**: Calculate the mean and variance for each dimension of the retained parameter vectors.\n6. **Round Results**: Convert the mean and variance to lists and round each value to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to a NumPy array if it's a list\n    rewards = np.array(rewards)\n    N = theta_samples.shape[0]\n    k = int(retain_prcnt * N)\n    \n    # Get the indices sorted by descending rewards\n    sorted_indices = np.argsort(-rewards)\n    \n    # Select the top k thetas\n    retained_thetas = theta_samples[sorted_indices[:k]]\n    \n    # Compute mean and variance\n    mean = np.mean(retained_thetas, axis=0)\n    variance = np.var(retained_thetas, axis=0, ddof=0)\n    \n    # Round to four decimals\n    mean_list = [round(m, 4) for m in mean]\n    variance_list = [round(v, 4) for v in variance]\n    \n    return (mean_list, variance_list)\n```\n\n### Explanation\n1. **Convert Rewards**: The rewards are converted to a NumPy array to facilitate efficient sorting and indexing.\n2. **Calculate Retained Samples**: The number of samples to retain is determined by taking the floor of the product of `retain_prcnt` and the number of samples `N`.\n3. **Sort by Reward**: The indices of the rewards are sorted in descending order to prioritize the highest rewards.\n4. **Select Top Samples**: The top `k` parameter vectors are selected based on the sorted indices.\n5. **Compute Statistics**: The mean and variance for each dimension of the retained parameter vectors are computed. The variance is calculated using the population variance formula (ddof=0).\n6. **Round Results**: The computed mean and variance values are rounded to four decimal places and returned as lists.\n\nThis approach ensures that we efficiently and correctly update the policy parameters using the Cross-Entropy Method, focusing on the top-performing samples to guide the next iteration of parameter updates."}
{"task_id": 353, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the base-2 Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of impurity or disorder in a set of labels, and it is defined using the probabilities of each unique label in the set.\n\n### Approach\n1. **Check for Empty Input**: If the input sequence is empty, the entropy is 0.0 by definition.\n2. **Count Frequencies**: Iterate through the input sequence to count the frequency of each label. This helps in determining the probability of each label.\n3. **Calculate Probabilities**: For each unique label, compute its probability as the ratio of its count to the total number of labels.\n4. **Compute Entropy**: Using the formula for Shannon entropy, compute the sum of the product of each probability and the logarithm (base 2) of that probability. The entropy is the negative of this sum.\n5. **Handle Edge Cases**: If all labels are identical, the entropy will be 0.0, which is correctly handled by the formula.\n\n### Solution Code\n```python\nimport math\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    counts = {}\n    total = 0\n    for label in labels:\n        counts[label] = counts.get(label, 0) + 1\n        total += 1\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy += p * math.log2(p)\n    entropy = -entropy\n    return round(entropy, 5)\n```\n\n### Explanation\n1. **Empty Input Handling**: The function first checks if the input is empty by attempting to iterate through it. If no elements are found, it returns 0.0.\n2. **Frequency Counting**: Using a dictionary, the function counts how many times each label appears in the input sequence.\n3. **Probability Calculation**: For each unique label, the probability is calculated as the count of the label divided by the total number of labels.\n4. **Entropy Calculation**: The entropy is computed using the formula \\( H = -\\sum (p_i \\log_2 p_i) \\), where \\( p_i \\) is the probability of each label. The result is rounded to five decimal places before being returned.\n\nThis approach ensures that the function efficiently handles any iterable input, including lists, NumPy arrays, and generators, while correctly computing the Shannon entropy."}
{"task_id": 354, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a high-level routine that performs 2-D convolution (cross-correlation) between a batch of images and a bank of kernels using the im2col and GEMM strategy. This approach allows efficient matrix multiplication by transforming the image into columns, enabling the use of optimized matrix multiplication routines.\n\n### Approach\n1. **Effective Kernel Size Calculation**: Compute the effective size of the kernel considering dilation. This is done by adding the necessary zeros between kernel elements based on the dilation factor.\n2. **Padding**: Depending on the padding specification, add zeros around the input images. The padding can be an integer, a tuple, a 4-tuple, or 'same'. For 'same' padding, the padding is calculated to ensure the output size is the same as the input size.\n3. **Output Size Calculation**: Determine the dimensions of the output based on the padded input size, kernel size, and stride.\n4. **Im2col Conversion**: Convert the input image into a matrix of columns, where each column represents a patch of the image. This allows the convolution operation to be performed as a matrix multiplication.\n5. **Matrix Multiplication**: Reshape the kernels into a matrix and perform matrix multiplication with the im2col matrix to get the output.\n6. **Reshape Output**: Convert the resulting matrix back into the desired 4-D shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple, or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    # Extract kernel dimensions and compute effective size considering dilation\n    kr = W.shape[0]\n    kc = W.shape[1]\n    d = dilation\n    kr_eff = kr + (kr - 1) * d\n    kc_eff = kc + (kc - 1) * d\n\n    # Determine padding based on the pad parameter\n    if pad == 'same':\n        pr1 = (kr_eff - 1) // 2\n        pr2 = (kr_eff - 1) // 2\n        pc1 = (kc_eff - 1) // 2\n        pc2 = (kc_eff - 1) // 2\n    else:\n        if isinstance(pad, int):\n            pr1 = pr2 = pc1 = pc2 = pad\n        elif isinstance(pad, tuple):\n            if len(pad) == 2:\n                pr1 = pr2 = pad[0]\n                pc1 = pc2 = pad[1]\n            elif len(pad) == 4:\n                pr1, pr2, pc1, pc2 = pad\n            else:\n                # Invalid padding specification, behavior undefined\n                pass\n        else:\n            # Invalid padding specification, behavior undefined\n            pass\n\n    # Pad the input array\n    X_padded = np.pad(X, ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n\n    # Compute the dimensions after padding\n    in_rows_padded = X_padded.shape[1]\n    in_cols_padded = X_padded.shape[2]\n\n    # Calculate output dimensions\n    out_rows = (in_rows_padded - kr_eff) // stride + 1\n    out_cols = (in_cols_padded - kc_eff) // stride + 1\n\n    # Number of examples and channels\n    n_ex = X.shape[0]\n    in_ch = X.shape[3]\n\n    # Total number of patches\n    num_patches = out_rows * out_cols\n\n    # Convert images to im2col format\n    im2col = []\n    for batch in range(n_ex):\n        image = X_padded[batch, :, :, :]\n        patches = []\n        for i in range(0, in_rows_padded - kr_eff + 1, stride):\n            for j in range(0, in_cols_padded - kc_eff + 1, stride):\n                patch = image[i:i + kr_eff, j:j + kc_eff, :]\n                patch_flat = patch.reshape(kr_eff * kc_eff * in_ch)\n                patches.append(patch_flat)\n        im2col.append(patches)\n    im2col = np.array(im2col)\n    im2col = im2col.reshape(n_ex * num_patches, kr_eff * kc_eff * in_ch)\n\n    # Reshape the kernels\n    W_reshaped = W.reshape(kr_eff * kc_eff * in_ch, W.shape[3])\n\n    # Perform matrix multiplication\n    output = im2col @ W_reshaped\n\n    # Reshape the output to the desired dimensions\n    output = output.reshape(n_ex, out_rows, out_cols, W.shape[3])\n\n    return output.tolist()\n```\n\n### Explanation\n1. **Effective Kernel Size**: The kernel size is adjusted to account for dilation by adding zeros between kernel elements.\n2. **Padding**: The input is padded according to the specified padding, ensuring the output size is correctly calculated.\n3. **Output Dimensions**: The dimensions of the output are computed based on the padded input, kernel size, and stride.\n4. **Im2col Conversion**: The input image is converted into a matrix of columns, each representing a patch of the image.\n5. **Matrix Multiplication**: The kernels are reshaped into a matrix, and matrix multiplication is performed with the im2col matrix to produce the output.\n6. **Reshape Output**: The resulting matrix is reshaped back into the desired 4-D format and returned as a Python list.\n\nThis approach efficiently handles the convolution operation by leveraging matrix multiplication, making it suitable for large input sizes and multiple kernels."}
{"task_id": 355, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Principal Component Analysis (PCA) with two possible solvers: singular value decomposition (SVD) and eigen-decomposition of the covariance matrix. The goal is to transform the data into a lower-dimensional space while preserving the maximum variance.\n\n### Approach\n1. **Standardize the Data**: Subtract the feature-wise mean from the data to center it at the origin.\n2. **Compute Principal Directions**:\n   - **SVD Solver**: Use `numpy.linalg.svd` to decompose the centered data. The right singular vectors (from the decomposition) are the principal directions.\n   - **Eigen Solver**: Compute the sample covariance matrix and use `numpy.linalg.eigh` to get the eigenvalues and eigenvectors. Sort these in descending order of eigenvalues to get the principal directions.\n3. **Sort Directions**: Ensure the directions are sorted by the variance they explain, from highest to lowest.\n4. **Determine Sign**: Adjust the sign of each direction so that the first non-zero element is non-negative, ensuring deterministic results.\n5. **Project Data**: Project the centered data onto the selected principal directions to get the scores matrix.\n6. **Return Results**: Return the scores and explained variance ratios, both rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Step 1: Standardize the data\n    data_centered = data - data.mean(axis=0)\n    \n    # Step 2: Compute principal directions based on solver\n    if solver == 'svd':\n        U, S, Vh = np.linalg.svd(data_centered, full_matrices=False)\n        directions = Vh.T  # Transpose to get V as columns\n        total_variance = np.sum(S**2)\n        explained_variance = (S**2) / total_variance\n    elif solver == 'eigen':\n        cov_matrix = np.cov(data_centered, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        directions = eigenvectors\n        total_variance = np.sum(eigenvalues)\n        explained_variance = eigenvalues / total_variance\n    else:\n        raise ValueError(\"Invalid solver. Use 'svd' or 'eigen'.\")\n    \n    # Step 3: Select top n_components directions\n    n_components = min(n_components, directions.shape[1])\n    selected_directions = directions[:, :n_components]\n    \n    # Step 4: Compute scores\n    scores = data_centered @ selected_directions\n    \n    # Step 5: Make the sign deterministic\n    for i in range(n_components):\n        direction = selected_directions[:, i]\n        # Find first non-zero element\n        first_non_zero = None\n        for idx in range(len(direction)):\n            if direction[idx] != 0:\n                first_non_zero = idx\n                break\n        if first_non_zero is not None and direction[first_non_zero] < 0:\n            selected_directions[:, i] *= -1\n            scores[:, i] *= -1\n    \n    # Step 6: Prepare the output\n    # Round scores to 4 decimals\n    scores_rounded = np.round(scores, 4).tolist()\n    # Round explained variance ratio to 4 decimals\n    explained_variance_ratio = explained_variance[:n_components]\n    explained_variance_ratio_rounded = np.round(explained_variance_ratio, 4).tolist()\n    \n    return (scores_rounded, explained_variance_ratio_rounded)\n```\n\n### Explanation\n1. **Standardization**: The data is centered by subtracting the mean of each feature, ensuring that the analysis is not biased by the scale of the features.\n2. **SVD and Eigen Solvers**: Depending on the solver, the data is decomposed either using SVD or by computing the covariance matrix and its eigenvalues. This step identifies the principal components.\n3. **Sorting and Sign Adjustment**: The principal components are sorted by their explained variance. The sign of each component is adjusted to ensure the first non-zero element is positive, ensuring deterministic results.\n4. **Projection**: The centered data is projected onto the selected principal components to form the scores matrix.\n5. **Output**: The scores and explained variance ratios are rounded and converted to lists for the final output.\n\nThis approach ensures that the PCA transformation is both efficient and deterministic, providing consistent results across different runs."}
{"task_id": 356, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that predicts the outcome stored in a leaf node of a decision tree. The function should handle both classification and regression tasks differently, returning the appropriate prediction based on the type of task.\n\n### Approach\n1. **Problem Analysis**:\n   - For a classification tree, each leaf node stores a list of class probabilities. The prediction is the index of the class with the highest probability. In case of ties, the smaller index is chosen.\n   - For a regression tree, each leaf node stores the mean of the target values for the region it represents. The prediction is simply this mean value.\n\n2. **Intuition**:\n   - If the task is classification, we need to find the index of the maximum value in the list of probabilities. This can be done by iterating through the list and keeping track of the highest value and its index.\n   - If the task is regression, we directly return the scalar value stored in the leaf node.\n\n3. **Algorithm Selection**:\n   - For classification, we use a loop to iterate through the list of probabilities to find the maximum value and its index.\n   - For regression, we simply return the scalar value.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        max_val = leaf.value[0]\n        max_index = 0\n        for index, val in enumerate(leaf.value):\n            if val > max_val:\n                max_val = val\n                max_index = index\n        return max_index\n    else:\n        return leaf.value\n```\n\n### Explanation\n- **Classification Case**: The function iterates through the list of probabilities stored in the leaf node. It keeps track of the highest probability and its corresponding index. The index of the highest probability is returned as the prediction.\n- **Regression Case**: The function directly returns the scalar value stored in the leaf node, which represents the mean of the target values for that region.\n\nThis approach ensures that the function correctly handles both classification and regression tasks, providing the appropriate prediction based on the type of problem."}
{"task_id": 357, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-means clustering algorithm from scratch. The goal is to partition a given dataset into K clusters such that each sample belongs to the cluster with the nearest centroid. The algorithm is deterministic and easy to test as it initializes the centroids with the first K samples.\n\n### Approach\n1. **Initialization**: Start by initializing the centroids using the first K samples from the dataset. This makes the algorithm deterministic.\n2. **Iteration**: Repeat the following steps up to a maximum of 100 iterations:\n   - **Assignment**: Compute the Euclidean distance from each sample to each centroid and assign each sample to the nearest centroid.\n   - **Update**: Recompute each centroid as the mean of the samples assigned to it. If a centroid loses all its samples, it remains unchanged for that iteration.\n   - **Convergence Check**: Stop early if none of the centroids change by more than a tolerance of 1 \u00d7 10\u207b\u2076.\n3. **Sorting and Rounding**: Once the algorithm converges, sort the centroids lexicographically and round each coordinate to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    # Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n    \n    for _ in range(max_iters):\n        # Calculate distances from each sample to each centroid\n        distances = np.sqrt(np.sum((data[:, np.newaxis] - centroids) ** 2, axis=2))\n        assignments = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = []\n        for c in range(K):\n            mask = (assignments == c)\n            if np.sum(mask) == 0:\n                new_centroids.append(centroids[c])\n            else:\n                new_centroids.append(np.mean(data[mask], axis=0))\n        new_centroids = np.array(new_centroids)\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        centroids = new_centroids\n    \n    # Sort centroids lexicographically\n    sorted_centroids = sorted(centroids.tolist())\n    \n    # Round each coordinate to four decimals\n    rounded = [[round(c, 4) for c in centroid] for centroid in sorted_centroids]\n    \n    return rounded\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first K samples of the dataset. This ensures the algorithm is deterministic.\n2. **Distance Calculation**: For each sample, the Euclidean distance to each centroid is computed. This is done efficiently using NumPy's vectorized operations.\n3. **Assignment**: Each sample is assigned to the nearest centroid based on the computed distances.\n4. **Centroid Update**: Each centroid is updated to the mean of the samples assigned to it. If a centroid has no samples, it remains unchanged.\n5. **Convergence Check**: The algorithm stops early if the centroids do not change significantly between iterations, ensuring efficiency.\n6. **Sorting and Rounding**: The final centroids are sorted lexicographically and each coordinate is rounded to four decimal places for the output.\n\nThis approach ensures that the K-means algorithm is implemented efficiently and correctly, handling various edge cases and ensuring convergence within a reasonable number of iterations."}
{"task_id": 362, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an arbitrary 2-D list or NumPy array of non-negative numbers into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. The function should handle both lists and NumPy arrays, check for rows that sum to zero (which makes normalization impossible), and return the normalized matrix rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a list of lists or a NumPy array. We first convert it into a NumPy array of floats for easier manipulation.\n2. **Compute Row Sums**: Calculate the sum of each row using NumPy's `sum` function with `axis=1`.\n3. **Check for Zero Sums**: If any row sum is zero, return -1 as normalization is impossible.\n4. **Normalize Rows**: For each row, divide each element by the row's sum to make the row sum to 1.\n5. **Round and Convert to List**: Round each element to four decimal places and convert the resulting NumPy array back to a native Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert the input to a NumPy array of floats\n    data_array = np.asarray(data, dtype=np.float64)\n    \n    # Compute the sum of each row\n    row_sums = np.sum(data_array, axis=1)\n    \n    # Check if any row sum is zero\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row by dividing by its sum\n    normalized = data_array / row_sums[:, np.newaxis]\n    \n    # Round each element to 4 decimal places\n    rounded = np.round(normalized, 4)\n    \n    # Convert the NumPy array to a native Python list of lists\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage efficient numerical operations.\n2. **Row Sums Calculation**: Using `np.sum` with `axis=1` computes the sum of each row efficiently.\n3. **Zero Sum Check**: If any row sum is zero, the function immediately returns -1, as normalizing such a row is impossible.\n4. **Row Normalization**: Each element in a row is divided by the row's sum, ensuring each row sums to 1.\n5. **Rounding and Conversion**: The normalized matrix is rounded to four decimal places and converted back to a Python list of lists for the final output.\n\nThis approach ensures that the function handles both lists and NumPy arrays, performs necessary checks, and returns the result in the required format efficiently."}
{"task_id": 363, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether the action and observation spaces of a given reinforcement learning environment are continuous. The environment's spaces can be of different types, and we need to check each type to see if it meets the criteria for being continuous.\n\n### Approach\n1. **Problem Analysis**: \n   - A space is continuous if it is an instance of `Box`.\n   - For composite spaces (like `Tuple` or `Dict`), the space is continuous only if all its subspaces are `Box`.\n   - We are given two flags, `tuple_action` and `tuple_obs`, which indicate whether the action and observation spaces are composite types.\n\n2. **Helper Function**:\n   - We create a helper function `is_space_continuous` that checks if a given space is continuous based on whether it is a composite type.\n   - If the space is a composite (indicated by the flag), we check each subspace to ensure they are all `Box`.\n   - If the space is not a composite, we simply check if it is an instance of `Box`.\n\n3. **Main Function**:\n   - Use the helper function to check both the action and observation spaces separately.\n   - Return a tuple indicating whether each space is continuous.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool) -> (bool, bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def is_space_continuous(space, is_composite):\n        if is_composite:\n            if isinstance(space, Tuple):\n                subs = space.spaces\n            elif isinstance(space, Dict):\n                subs = space.spaces.values()\n            else:\n                return False\n            for sub in subs:\n                if not isinstance(sub, Box):\n                    return False\n            return True\n        else:\n            return isinstance(space, Box)\n    \n    action_space = env.action_space\n    obs_space = env.observation_space\n    \n    cont_action = is_space_continuous(action_space, tuple_action)\n    cont_obs = is_space_continuous(obs_space, tuple_obs)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Helper Function**: The helper function `is_space_continuous` checks if a given space is continuous. It handles both composite and non-composite spaces. For composite spaces, it ensures all subspaces are `Box`.\n- **Main Function**: The main function `is_continuous` uses the helper function to check both the action and observation spaces. It returns a tuple indicating the continuity of each space based on the given flags.\n\nThis approach efficiently checks each space type and ensures that composite spaces meet the criteria for continuity by verifying all their subspaces."}
{"task_id": 369, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the outputs of Gradient Boosting trees to produce a final prediction. The aggregation process differs slightly between regression and classification tasks. \n\n### Approach\n1. **Initialization**: Start with a running prediction initialized to zero. The shape of this running prediction depends on whether the task is regression or classification.\n2. **Update Aggregation**: For each tree's output, update the running prediction by subtracting the scaled output (using the learning rate) from the current running prediction.\n3. **Post-Processing**:\n   - For regression, the running prediction is directly used as the final output, rounded to four decimal places.\n   - For classification, convert the running prediction (which represents log-probabilities) into probabilities using the softmax function. The predicted class labels are then obtained by taking the argmax of these probabilities.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        if regression:\n            return np.array([], dtype=np.float64)\n        else:\n            return np.array([], dtype=np.int64)\n    \n    first_update = updates[0]\n    if regression:\n        n_samples = first_update.shape[0]\n        running_pred = np.zeros(n_samples)\n    else:\n        n_samples, n_classes = first_update.shape\n        running_pred = np.zeros((n_samples, n_classes))\n    \n    for u in updates:\n        running_pred -= learning_rate * u\n    \n    if regression:\n        return np.round(running_pred, 4)\n    else:\n        # Compute softmax\n        max_logit = np.max(running_pred, axis=1, keepdims=True)\n        exp_running_pred = np.exp(running_pred - max_logit)\n        sum_exp = np.sum(exp_running_pred, axis=1, keepdims=True)\n        probabilities = exp_running_pred / sum_exp\n        predicted_classes = np.argmax(probabilities, axis=1)\n        return predicted_classes\n```\n\n### Explanation\n1. **Initialization**: The running prediction is initialized to a zero array with the same shape as the first update. For regression, this is a 1D array, and for classification, it is a 2D array.\n2. **Update Aggregation**: Each tree's output is scaled by the learning rate and subtracted from the running prediction. This is done in a vectorized manner for efficiency.\n3. **Post-Processing**:\n   - For regression, the running prediction is simply rounded to four decimal places.\n   - For classification, the running prediction (log-probabilities) is converted to probabilities using the softmax function. The argmax of these probabilities gives the predicted class labels.\n\nThis approach ensures that the aggregation is efficient and correctly handles both regression and classification tasks."}
{"task_id": 371, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a 1-D cross-correlation (convolution) function that supports various features such as batch processing, multiple input and output channels, strides, padding, and dilation. The function should return the result as a nested Python list.\n\n### Approach\n1. **Effective Kernel Calculation**: Compute the effective width of the kernel considering dilation. This is done by inserting zeros between kernel elements based on the dilation value.\n2. **Padding Calculation**: Determine the padding required based on the input type (integer, tuple, or \"same\"). The padding ensures that the output length is consistent with the input length when using \"same\" padding.\n3. **Input Padding**: Apply the computed padding to the input signals to handle edge cases and ensure proper convolution.\n4. **Kernel Dilation**: Dilate each kernel to create a larger kernel with zeros inserted between elements. This is done for each input and output channel.\n5. **Sliding Window and Matrix Multiplication**: Use a sliding window approach to apply the dilated kernels to the padded input signals. This involves creating a view of the input signal using strides and then performing matrix multiplication to compute the output efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Compute effective kernel width\n    kernel_width = W.shape[0]\n    effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n\n    # Compute padding\n    if pad == \"same\":\n        l_in = X.shape[1]\n        out_len = (l_in + stride - 1) // stride  # Ceiling division\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - l_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = pad\n        p_right = pad\n    elif isinstance(pad, tuple):\n        p_left, p_right = pad\n    else:\n        raise ValueError(\"Invalid padding value\")\n\n    # Pad the input\n    X_padded = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n\n    # Dilate the kernels\n    in_channels = W.shape[1]\n    out_channels = W.shape[2]\n    W_dilated = np.zeros((effective_kernel, in_channels, out_channels), dtype=W.dtype)\n    for c_in in range(in_channels):\n        for c_out in range(out_channels):\n            kernel = W[:, c_in, c_out]\n            dilated = []\n            for i in range(len(kernel)):\n                dilated.append(kernel[i])\n                if i < len(kernel) - 1:\n                    dilated += [0] * dilation\n            W_dilated[:, c_in, c_out] = dilated\n\n    # Compute output dimensions\n    L_padded = X_padded.shape[1]\n    num_windows = (L_padded - effective_kernel) // stride + 1\n    n_examples = X.shape[0]\n\n    # Initialize output\n    output = np.zeros((n_examples, num_windows, out_channels), dtype=X.dtype)\n\n    # Process each example\n    for example in range(n_examples):\n        padded_signal = X_padded[example, :, :]\n        # Create windowed view\n        itemsize = padded_signal.itemsize\n        windowed = np.lib.stride_tricks.as_strided(\n            padded_signal,\n            shape=(num_windows, effective_kernel, in_channels),\n            strides=(stride * in_channels * itemsize, in_channels * itemsize, itemsize)\n        )\n        # Reshape for matrix multiplication\n        windowed_reshaped = windowed.reshape(num_windows, -1)\n        kernels_reshaped = W_dilated.reshape(-1, out_channels)\n        output_example = np.dot(windowed_reshaped, kernels_reshaped)\n        output[example, :, :] = output_example\n\n    return output.tolist()\n```\n\n### Explanation\n1. **Effective Kernel Calculation**: The effective kernel width is computed by considering the dilation, which increases the kernel size by inserting zeros between elements.\n2. **Padding Calculation**: Depending on the padding type, the left and right padding values are determined to ensure the output length is consistent with the input length when using \"same\" padding.\n3. **Input Padding**: The input signals are padded with zeros on both sides to handle the edges and ensure the kernel can slide over the entire signal.\n4. **Kernel Dilation**: Each kernel is dilated by inserting zeros between its elements based on the dilation value, creating a larger kernel.\n5. **Sliding Window and Matrix Multiplication**: The input signal is processed using a sliding window approach to apply the dilated kernels. The windowed view of the input is reshaped and matrix-multiplied with the dilated kernels to efficiently compute the output.\n\nThis approach ensures that the function handles various input configurations and efficiently computes the 1-D cross-correlation with the given parameters."}
{"task_id": 373, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Gini impurity of a given sequence of integer labels. The Gini impurity is a measure used in decision-tree learning to quantify the likelihood of mislabeling a randomly chosen element from the set based on the distribution of labels in the subset.\n\n### Approach\nThe approach to compute the Gini impurity involves the following steps:\n\n1. **Check for Empty Input**: If the input sequence is empty, return a Gini impurity of 0.0 by convention.\n2. **Count Label Frequencies**: Use a counter to determine the frequency of each distinct label in the input sequence.\n3. **Calculate Total Samples**: Determine the total number of samples in the input sequence.\n4. **Compute Probabilities**: For each distinct label, compute the probability of the label occurring, which is the ratio of the count of the label to the total number of samples.\n5. **Sum of Squared Probabilities**: Sum the squares of these probabilities.\n6. **Calculate Gini Impurity**: Subtract the sum of squared probabilities from 1 to get the Gini impurity.\n7. **Round the Result**: Return the result rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    N = len(y)\n    sum_p_sq = 0.0\n    for c in counts:\n        n_c = counts[c]\n        p_c = n_c / N\n        sum_p_sq += p_c ** 2\n    gini_imp = 1 - sum_p_sq\n    return round(gini_imp, 4)\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input sequence is empty and returns 0.0 immediately if true.\n- **Frequency Counting**: Using `Counter` from the `collections` module, the function counts the occurrences of each label in the input sequence.\n- **Probability Calculation**: For each unique label, the probability is calculated as the count of the label divided by the total number of samples.\n- **Sum of Squares**: The sum of the squares of these probabilities is computed.\n- **Gini Impurity Calculation**: The Gini impurity is derived by subtracting this sum from 1.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently handles both lists and NumPy arrays, ensuring accurate and efficient computation of the Gini impurity."}
{"task_id": 374, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that trains a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) by iteratively updating the model parameters.\n\n### Approach\n1. **Input Consistency Check**: First, we check if the number of samples in the feature matrix `X` and the target vector `y` are the same. If not, we return -1 as the data is inconsistent.\n2. **Initialization**: We initialize the weight vector `theta` to zeros and the bias `b` to zero.\n3. **Gradient Descent**: For a specified number of iterations, we compute the predictions using the current parameters, calculate the error, and then update the parameters using the gradients. The gradients are computed as:\n   - `dw = (2/m) * X.T.dot(\u0177 - y)`\n   - `db = (2/m) * (\u0177 - y).sum()`\n   where `\u0177` is the predicted values, `m` is the number of samples, and `X.T` is the transpose of the feature matrix.\n4. **Parameter Update**: We update the weights and bias by subtracting the product of the learning rate and the respective gradients.\n5. **Rounding**: After completing the iterations, we round the weights and bias to four decimal places and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    # Check if the number of samples in X and y are the same\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    # Reshape y to ensure it's a 1D array\n    y = y.reshape(-1)\n    \n    m = X.shape[0]\n    n = X.shape[1]\n    \n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(n_iterations):\n        y_pred = X.dot(theta) + b\n        error = y_pred - y\n        dw = (2.0 / m) * X.T.dot(error)\n        db = (2.0 / m) * error.sum()\n        theta = theta - learning_rate * dw\n        b = b - learning_rate * db\n    \n    # Round the parameters to 4 decimal places\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = np.round(b, 4)\n    \n    return (theta_rounded, b_rounded)\n```\n\n### Explanation\n- **Input Consistency Check**: Ensures that the feature matrix `X` and target vector `y` have the same number of samples, which is crucial for correct matrix operations.\n- **Initialization**: Starts with zero weights and bias, which is a common practice in regression problems.\n- **Gradient Descent**: Uses batch gradient descent to update parameters in each iteration. The gradients are computed using matrix operations for efficiency.\n- **Parameter Update**: Adjusts the weights and bias in the direction that minimizes the MSE, scaled by the learning rate.\n- **Rounding**: Ensures the output meets the precision requirement of four decimal places for both weights and bias.\n\nThis approach efficiently trains the model using batch gradient descent, ensuring that the parameters are optimized to minimize the MSE."}
{"task_id": 376, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is considered symmetric if it is square and equal to its transpose. This means that for every valid index pair (i, j), the element at (i, j) must be equal to the element at (j, i).\n\n### Approach\n1. **Check if the Matrix is Square**: First, we need to ensure that the given matrix is square. A matrix is square if the number of rows is equal to the number of columns. This is done by checking if each row has the same length as the number of rows in the matrix.\n2. **Convert to NumPy Array**: Convert the given list of lists into a NumPy array to facilitate easy transposition and comparison.\n3. **Compare with Transpose**: Compute the transpose of the matrix and compare it with the original matrix using NumPy's `allclose` function. This function checks if two arrays are element-wise equal within a tolerance, which is suitable for handling floating-point precision issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Check if the matrix is square\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n    \n    # Convert to NumPy array and check if it's symmetric\n    X_np = np.array(X)\n    return np.allclose(X_np, X_np.T)\n```\n\n### Explanation\n1. **Square Check**: The function first checks if the matrix is square by comparing the length of each row with the total number of rows. If any row does not match the required length, the function returns `False`.\n2. **Conversion and Comparison**: The matrix is converted into a NumPy array to leverage efficient matrix operations. The transpose of the matrix is computed, and `allclose` is used to compare the original matrix with its transpose. This method ensures that we handle floating-point precision issues correctly by allowing a small tolerance for differences.\n\nThis approach efficiently checks for symmetry by leveraging NumPy's capabilities, ensuring both correctness and performance."}
{"task_id": 377, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the negative gradient for the logistic loss used in Gradient Boosting for binary classification. The goal is to compute the negative gradient of the logistic loss function with respect to the model's predictions, which is then used to update the model in the next iteration of boosting.\n\n### Approach\n1. **Input Validation**: First, we check if the input labels `y` contain only binary values (0 or 1). If any value in `y` is not 0 or 1, we return a list of -1.0s of the same length as `y`.\n2. **Convert Labels**: Convert the binary labels `y` to the set {-1, 1} using the transformation `y' = 2y - 1`.\n3. **Compute Gradients**: For each sample, compute the negative gradient using the formula:\n   \\[\n   g_i = \\frac{y'_i}{1 + \\exp(y'_i \\cdot f_i)}\n   \\]\n   where \\( y'_i \\) is the transformed label and \\( f_i \\) is the current model's prediction.\n4. **Round Results**: Round each computed gradient to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient for the logistic loss in Gradient Boosting.\n    \n    Args:\n        y: Binary class labels (0 or 1)\n        f: Current prediction scores (floats)\n        \n    Returns:\n        A list of negative gradients, each rounded to four decimal places.\n        If y contains values other than 0 or 1, returns a list of -1.0s.\n    \"\"\"\n    # Check if any element in y is not 0 or 1\n    if any(yi not in (0, 1) for yi in y):\n        return [-1.0] * len(y)\n    \n    # Convert y to y' = 2y - 1\n    y_prime = [2 * yi - 1 for yi in y]\n    \n    # Compute the negative gradient for each sample\n    gradients = []\n    for yp, fi in zip(y_prime, f):\n        z = yp * fi\n        denominator = 1 + np.exp(z)\n        g = yp / denominator\n        gradients.append(round(g, 4))\n    \n    return gradients\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if all elements in `y` are either 0 or 1. If not, it returns a list of -1.0s of the same length as `y`.\n2. **Label Transformation**: The labels `y` are converted to `y'` using the transformation \\( y' = 2y - 1 \\), which maps 0 to -1 and 1 to 1.\n3. **Gradient Calculation**: For each sample, the negative gradient is computed using the formula derived from the logistic loss function. This involves computing the exponential of the product of the transformed label and the prediction, then using this to compute the gradient.\n4. **Rounding**: Each computed gradient is rounded to four decimal places to ensure the result meets the specified precision.\n\nThis approach ensures that the negative gradient is correctly computed and formatted for use in the Gradient Boosting algorithm, providing the necessary updates to the model for subsequent iterations."}
{"task_id": 380, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The SELU function is defined using specific constants and has different expressions depending on whether the input is positive or negative. The task is to compute the function or its derivatives based on the given order and return the result in a specified format.\n\n### Approach\n1. **Understanding SELU Function**: The SELU function is defined as `scale * ELU(x, \u03b1)`, where ELU is a piecewise function. For positive inputs, ELU returns the input value, and for non-positive inputs, it returns `\u03b1 * (e^x - 1)`. The constants `\u03b1` and `scale` are given as 1.6732632423543772848170429916717 and 1.0507009873554804934193349852946, respectively.\n\n2. **Derivatives of SELU**: \n   - The first derivative of SELU is `scale` for positive inputs and `scale * \u03b1 * e^x` for non-positive inputs.\n   - The second derivative is 0 for positive inputs and `scale * \u03b1 * e^x` for non-positive inputs.\n\n3. **Handling Different Input Types**: The function should handle scalar, 1-D, and multi-D array inputs. The result should be rounded to 4 decimal places and returned as a list or scalar depending on the input type.\n\n4. **Efficient Computation**: Using NumPy's `where` function allows efficient element-wise computation for arrays, ensuring the solution is both concise and efficient.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2SELU/\u2202x\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    if isinstance(x, (int, float, np.number)):\n        # Handle scalar case\n        if order == 0:\n            if x > 0:\n                res = scale * x\n            else:\n                res = scale * alpha * (np.exp(x) - 1)\n        elif order == 1:\n            if x > 0:\n                res = scale\n            else:\n                res = scale * alpha * np.exp(x)\n        elif order == 2:\n            if x > 0:\n                res = 0.0\n            else:\n                res = scale * alpha * np.exp(x)\n        else:\n            raise ValueError(\"Order must be 0, 1, or 2.\")\n        res_rounded = round(res, 4)\n        return res_rounded\n    else:\n        # Handle array case\n        x_arr = np.array(x)\n        if order == 0:\n            result = np.where(x_arr > 0, scale * x_arr, scale * alpha * (np.exp(x_arr) - 1))\n        elif order == 1:\n            result = np.where(x_arr > 0, scale, scale * alpha * np.exp(x_arr))\n        elif order == 2:\n            result = np.where(x_arr > 0, 0, scale * alpha * np.exp(x_arr))\n        else:\n            raise ValueError(\"Order must be 0, 1, or 2.\")\n        result_rounded = np.round(result, 4)\n        return result_rounded.tolist()\n```\n\n### Explanation\n- **Scalar Handling**: If the input `x` is a scalar, the function computes the SELU or its derivative directly using conditional checks.\n- **Array Handling**: For array inputs, NumPy's `where` function is used to efficiently compute the result element-wise.\n- **Rounding and Conversion**: The results are rounded to 4 decimal places and converted to a list for array inputs or returned as a scalar for scalar inputs.\n\nThis approach ensures that the function is both efficient and easy to understand, handling all specified cases correctly."}
{"task_id": 387, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor that uses one-dimensional stumps. The goal is to build a model that, for each boosting round, adds a decision stump, which is a simple split on the x-axis. The stump is chosen to minimize the sum of squared residuals on both sides of the split.\n\n### Approach\n1. **Initialization**: Start with the initial prediction as the mean of the target values.\n2. **Iterative Boosting**: For each boosting round:\n   - Compute the residuals, which are the negative gradients of the squared error loss.\n   - Sort the data by the feature values and compute prefix sums and squared sums of the residuals.\n   - Evaluate all possible splits between consecutive feature values. For each split, compute the sum of squared errors and choose the split that minimizes this sum.\n   - If no valid splits are possible (all feature values are identical), use the global mean of the residuals.\n   - Update the predictions using the chosen split and the learning rate.\n3. **Prediction**: After all boosting rounds, return the predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n = len(x)\n    y_pred = np.mean(y)\n    \n    for _ in range(n_estimators):\n        r = y - y_pred\n        \n        # Sort the data by x, keeping track of original indices\n        sorted_with_indices = sorted(enumerate(x), key=lambda pair: pair[1])\n        sorted_indices = [idx for idx, xi in sorted_with_indices]\n        sorted_x = np.array([xi for idx, xi in sorted_with_indices])\n        sorted_r = r[sorted_indices]\n        \n        # Compute prefix sums and squared sums\n        prefix_sum = np.zeros(n)\n        prefix_sq_sum = np.zeros(n)\n        prefix_sum[0] = sorted_r[0]\n        prefix_sq_sum[0] = sorted_r[0] ** 2\n        for i in range(1, n):\n            prefix_sum[i] = prefix_sum[i-1] + sorted_r[i]\n            prefix_sq_sum[i] = prefix_sq_sum[i-1] + sorted_r[i] ** 2\n        \n        # Find the best split\n        min_sum = float('inf')\n        best_split = None\n        best_t = None\n        best_mean_left = 0\n        best_mean_right = 0\n        \n        for i in range(n-1):\n            if sorted_x[i] == sorted_x[i+1]:\n                continue\n            t = (sorted_x[i] + sorted_x[i+1]) / 2\n            \n            # Calculate sum of squared errors for this split\n            sum_left = prefix_sq_sum[i] - (prefix_sum[i] ** 2) / (i + 1)\n            sum_right = (prefix_sq_sum[-1] - prefix_sq_sum[i]) - ((prefix_sum[-1] - prefix_sum[i]) ** 2) / (n - (i + 1))\n            total = sum_left + sum_right\n            \n            if total < min_sum:\n                min_sum = total\n                best_split = i\n                best_t = t\n                best_mean_left = prefix_sum[i] / (i + 1)\n                best_mean_right = (prefix_sum[-1] - prefix_sum[i]) / (n - (i + 1))\n        \n        if best_split is not None:\n            # Compute r_hat for each sample in the sorted order\n            r_hat_sorted = np.zeros(n)\n            t = best_t\n            mean_left = best_mean_left\n            mean_right = best_mean_right\n            for i in range(n):\n                if sorted_x[i] <= t:\n                    r_hat_sorted[i] = mean_left\n                else:\n                    r_hat_sorted[i] = mean_right\n            \n            # Map back to original indices\n            r_hat = np.zeros(n)\n            for i in range(n):\n                original_idx = sorted_indices[i]\n                r_hat[original_idx] = r_hat_sorted[i]\n            \n            # Update y_pred\n            y_pred += learning_rate * r_hat\n        else:\n            # All x are the same; use global mean\n            mean_r = np.mean(r)\n            y_pred += learning_rate * mean_r\n    \n    # Round the predictions to four decimal places\n    y_pred = np.round(y_pred, 4)\n    return y_pred.tolist()\n```\n\n### Explanation\n- **Initialization**: The initial prediction is set to the mean of the target values.\n- **Residuals Calculation**: For each boosting round, residuals are computed as the difference between the target values and the current predictions.\n- **Sorting and Prefix Sums**: The data is sorted by the feature values, and prefix sums and squared sums of residuals are computed to efficiently evaluate splits.\n- **Split Evaluation**: Each possible split is evaluated to find the one that minimizes the sum of squared errors. The best split is used to update the predictions.\n- **Prediction Update**: The predictions are updated using the best split and the learning rate. If no valid splits are found, the global mean of residuals is used.\n- **Final Predictions**: After all boosting rounds, the predictions are rounded to four decimal places and returned as a list."}
{"task_id": 394, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimization algorithm, which is used to update the parameters of a neural network. RMSprop adjusts the learning rate dynamically based on the magnitude of the gradients, which helps in faster convergence and stability during training.\n\n### Approach\nThe RMSprop algorithm maintains a running average of the squared gradients, which is used to scale the learning rate for each parameter update. The key steps involved in the RMSprop update are:\n\n1. **Initialization**: If the running average of squared gradients (`Eg`) is not provided, initialize it with zeros of the same shape as the gradient.\n2. **Update Running Average**: Compute the new running average using the formula \\( E_g^{(t)} = \\rho \\cdot E_g^{(t-1)} + (1 - \\rho) \\cdot g^2 \\), where \\( \\rho \\) is the decay rate.\n3. **Compute Denominator**: Calculate the denominator for the parameter update step as the square root of the running average plus a small constant \\( \\varepsilon \\) (to avoid division by zero).\n4. **Update Parameters**: Adjust the parameters using the formula \\( w^{(t)} = w^{(t-1)} - \\frac{\\alpha \\cdot g}{\\sqrt{E_g^{(t)} + \\varepsilon}} \\), where \\( \\alpha \\) is the learning rate.\n5. **Rounding and Conversion**: Round the updated parameters and the new running average to four decimal places and convert them to regular Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # Initialize E_g if it is None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    # Compute the new running average of squared gradients\n    Eg_new = rho * Eg + (1 - rho) * (grad ** 2)\n    \n    # Compute the denominator with a small epsilon for numerical stability\n    epsilon = 1e-8\n    denominator = np.sqrt(Eg_new + epsilon)\n    \n    # Update the parameters\n    w_new = w - (learning_rate * grad) / denominator\n    \n    # Round both the new parameters and the new running average to 4 decimal places\n    w_rounded = np.round(w_new, 4)\n    Eg_rounded = np.round(Eg_new, 4)\n    \n    # Convert to regular Python lists\n    w_list = w_rounded.tolist()\n    Eg_list = Eg_rounded.tolist()\n    \n    return (w_list, Eg_list)\n```\n\n### Explanation\n1. **Initialization**: The function first checks if the running average `Eg` is provided. If not, it initializes `Eg` with zeros of the same shape as the gradient.\n2. **Update Running Average**: Using the provided decay rate \\( \\rho \\), the function computes the new running average of the squared gradients.\n3. **Compute Denominator**: The denominator is computed to avoid division by zero by adding a small constant \\( \\varepsilon \\) (1e-8) to the running average.\n4. **Update Parameters**: The parameters are updated using the computed running average and denominator, scaled by the learning rate.\n5. **Rounding and Conversion**: The updated parameters and running average are rounded to four decimal places and converted to Python lists for the final output.\n\nThis approach ensures that the RMSprop update step is efficiently computed and formatted as required."}
{"task_id": 398, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is widely used in deep learning due to its ability to help networks converge faster and reduce the vanishing gradient problem. The function and its gradient are defined differently for non-negative and negative inputs, which we will handle efficiently using vectorized operations in NumPy.\n\n### Approach\n1. **Problem Analysis**: The ELU function is defined as:\n   - ELU(x, \u03b1) = x for x \u2265 0\n   - ELU(x, \u03b1) = \u03b1(e^x - 1) for x < 0\n   The gradient (derivative) of ELU is:\n   - ELU\u2019(x, \u03b1) = 1 for x \u2265 0\n   - ELU\u2019(x, \u03b1) = \u03b1(e^x) for x < 0\n\n2. **Intuition**: The function and its gradient can be computed efficiently using vectorized operations. We will use NumPy to handle these operations, which allows us to process arrays of inputs in a computationally efficient manner.\n\n3. **Algorithm**:\n   - Convert the input list to a NumPy array for efficient processing.\n   - Use a boolean mask to identify non-negative values.\n   - Compute the ELU values or their gradients based on the mask.\n   - Round the results to four decimal places and convert them to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x = np.array(x)\n    mask = x >= 0\n    if not derivative:\n        result = np.where(mask, x, alpha * (np.exp(x) - 1))\n    else:\n        result = np.where(mask, 1.0, alpha * np.exp(x))\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to leverage vectorized operations, which are more efficient than element-wise loops.\n- **Boolean Mask**: A boolean mask is created to identify where the elements of `x` are non-negative. This mask is used to apply the appropriate computation for ELU or its gradient.\n- **ELU Computation**: For non-negative values, the ELU value is the input itself. For negative values, it is computed as \u03b1(e^x - 1).\n- **Gradient Computation**: For non-negative values, the gradient is 1. For negative values, it is computed as \u03b1(e^x).\n- **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's capabilities for vectorized operations to handle potentially large input arrays efficiently."}
{"task_id": 411, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute statistics for collections of actions and observations in a reinforcement learning context. The goal is to determine various properties such as dimensionality, data type (discrete or continuous), and the unique values appearing in each dimension.\n\n### Approach\n1. **Check for Iterables**: Determine if any element in the actions or observations list is an iterable (list, tuple, or numpy array). This helps in identifying if the data is multi-dimensional.\n2. **Determine Dimensionality**: Calculate the dimensionality of actions and observations. If any element is an iterable, the dimensionality is the length of the first element. If all elements are scalars, the dimensionality is 1.\n3. **Check for Continuity**: Identify if the values are continuous by checking if any value is a non-integer float.\n4. **Collect Unique Values**: For each dimension, collect the unique values, count them, and sort them. This helps in understanding the range and type of values in each dimension.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    # Process actions\n    is_action_iterable = any(isinstance(a, (list, tuple, np.ndarray)) for a in actions)\n    if actions:\n        if is_action_iterable:\n            action_dim = len(actions[0])\n        else:\n            action_dim = 1\n    else:\n        action_dim = 0\n\n    all_action_values = []\n    for a in actions:\n        if isinstance(a, (list, tuple, np.ndarray)):\n            all_action_values.extend(a)\n        else:\n            all_action_values.append(a)\n\n    continuous_actions = any(isinstance(v, float) and not v.is_integer() for v in all_action_values)\n\n    n_actions_per_dim = []\n    action_ids = []\n    if action_dim > 0:\n        if action_dim == 1:\n            unique = sorted(list(set(all_action_values)))\n            n_actions_per_dim.append(len(unique))\n            action_ids.append(unique)\n        else:\n            for d in range(action_dim):\n                values = [a[d] for a in actions]\n                unique = sorted(list(set(values)))\n                n_actions_per_dim.append(len(unique))\n                action_ids.append(unique)\n\n    # Process observations\n    is_obs_iterable = any(isinstance(o, (list, tuple, np.ndarray)) for o in observations)\n    if observations:\n        if is_obs_iterable:\n            obs_dim = len(observations[0])\n        else:\n            obs_dim = 1\n    else:\n        obs_dim = 0\n\n    all_obs_values = []\n    for o in observations:\n        if isinstance(o, (list, tuple, np.ndarray)):\n            all_obs_values.extend(o)\n        else:\n            all_obs_values.append(o)\n\n    continuous_observations = any(isinstance(v, float) and not v.is_integer() for v in all_obs_values)\n\n    n_obs_per_dim = []\n    obs_ids = []\n    if obs_dim > 0:\n        if obs_dim == 1:\n            unique = sorted(list(set(all_obs_values)))\n            n_obs_per_dim.append(len(unique))\n            obs_ids.append(unique)\n        else:\n            for d in range(obs_dim):\n                values = [o[d] for o in observations]\n                unique = sorted(list(set(values)))\n                n_obs_per_dim.append(len(unique))\n                obs_ids.append(unique)\n\n    # Prepare the result dictionary\n    result = {\n        'tuple_actions': is_action_iterable,\n        'tuple_observations': is_obs_iterable,\n        'multidim_actions': action_dim > 1 if is_action_iterable else False,\n        'multidim_observations': obs_dim > 1 if is_obs_iterable else False,\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': n_actions_per_dim,\n        'action_dim': action_dim,\n        'n_obs_per_dim': n_obs_per_dim,\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids,\n    }\n\n    return result\n```\n\n### Explanation\n1. **Check for Iterables**: The function first checks if any element in the actions or observations list is an iterable. This helps determine if the data is multi-dimensional.\n2. **Determine Dimensionality**: The dimensionality is calculated based on whether the elements are iterables. If they are, the dimensionality is the length of the first element. If not, it is 1.\n3. **Check for Continuity**: The function checks if any value is a non-integer float to determine if the data is continuous.\n4. **Collect Unique Values**: For each dimension, the function collects all values, finds the unique ones, counts them, and sorts them. This provides insights into the range and type of values in each dimension.\n\nThis approach ensures that we comprehensively analyze the actions and observations to provide detailed statistics, which are essential for understanding the environment in reinforcement learning tasks."}
{"task_id": 413, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch. The goal is to predict the final values for the test dataset after training the model using the provided training data.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Initialization**: Start with an initial prediction equal to the mean of the training targets.\n2. **Boosting Iterations**: For each iteration, compute the residuals (negative gradients of the squared error loss), fit a CART regression tree to these residuals, and update the running prediction by adding the tree's output multiplied by the learning rate.\n3. **Tree Construction**: Implement a simple CART regression tree that uses mean squared error as the split criterion and stops growing when the maximum depth is reached or no further reduction in error is possible.\n4. **Prediction**: After all iterations, use the accumulated predictions to make final predictions on the test dataset.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    \n    class Node:\n        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value\n\n        def is_leaf(self):\n            return self.feature is None\n\n    class DecisionTreeRegressor:\n        def __init__(self):\n            self.root = None\n\n        def fit(self, X, y, max_depth):\n            self.root = self._build_tree(X, y, max_depth)\n\n        def _build_tree(self, X, y, current_depth):\n            if np.all(y == y[0]):\n                return Node(value=np.mean(y))\n            if current_depth == max_depth:\n                return Node(value=np.mean(y))\n            best_feature = None\n            best_threshold = None\n            min_mse = float('inf')\n            n_samples = len(y)\n            n_features = X.shape[1]\n            for feature in range(n_features):\n                x_col = X[:, feature]\n                sorted_indices = np.argsort(x_col)\n                x_col_sorted = x_col[sorted_indices]\n                y_sorted = y[sorted_indices]\n                for i in range(1, len(x_col_sorted)):\n                    threshold = (x_col_sorted[i-1] + x_col_sorted[i]) / 2\n                    left_indices = np.where(x_col <= threshold)\n                    right_indices = np.where(x_col > threshold)\n                    y_left = y[left_indices]\n                    y_right = y[right_indices]\n                    mse_left = np.mean((y_left - np.mean(y_left))**2) if len(y_left) > 0 else 0\n                    mse_right = np.mean((y_right - np.mean(y_right))**2) if len(y_right) > 0 else 0\n                    current_mse = (len(y_left) * mse_left + len(y_right) * mse_right) / n_samples\n                    if current_mse < min_mse:\n                        min_mse = current_mse\n                        best_feature = feature\n                        best_threshold = threshold\n            if best_feature is None:\n                return Node(value=np.mean(y))\n            left_indices = np.where(X[:, best_feature] <= best_threshold)\n            right_indices = np.where(X[:, best_feature] > best_threshold)\n            left_X = X[left_indices]\n            left_y = y[left_indices]\n            right_X = X[right_indices]\n            right_y = y[right_indices]\n            left_node = self._build_tree(left_X, left_y, current_depth + 1)\n            right_node = self._build_tree(right_X, right_y, current_depth + 1)\n            return Node(feature=best_feature, threshold=best_threshold, left=left_node, right=right_node, value=np.mean(y))\n\n        def predict(self, X):\n            return np.array([self._predict_row(row) for row in X])\n\n        def _predict_row(self, row):\n            node = self.root\n            while True:\n                if node.is_leaf():\n                    return node.value\n                if row[node.feature] <= node.threshold:\n                    node = node.left\n                else:\n                    node = node.right\n\n    current_train_pred = np.mean(y_train)\n    current_test_pred = np.mean(y_train)\n\n    for _ in range(n_estimators):\n        residual_train = y_train - current_train_pred\n        tree = DecisionTreeRegressor()\n        tree.fit(X_train, residual_train, max_depth)\n        tree_pred_train = tree.predict(X_train)\n        tree_pred_test = tree.predict(X_test)\n        current_train_pred += learning_rate * tree_pred_train\n        current_test_pred += learning_rate * tree_pred_test\n\n    final_predictions = np.round(current_test_pred, 4).tolist()\n    return final_predictions\n```\n\n### Explanation\n1. **Initialization**: The initial prediction is set to the mean of the training targets.\n2. **Boosting Iterations**: For each iteration, the residual (negative gradient) is computed. A decision tree is fit to this residual, and the tree's predictions are used to update the current predictions.\n3. **Tree Construction**: The decision tree is built using a recursive approach, splitting nodes based on the feature that minimizes the mean squared error. The tree grows until the maximum depth is reached or no further improvement is possible.\n4. **Prediction**: After training, the accumulated predictions from all trees are used to make final predictions on the test dataset, which are then rounded to four decimal places and returned as a list.\n\nThis approach efficiently combines the predictions of multiple weak learners (decision trees) to create a strong regressor, leveraging the power of gradient boosting."}
{"task_id": 416, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the probability density function (PDF) of a multivariate Gaussian distribution. The task is to compute the PDF for each data point in a given dataset without using any third-party libraries, relying solely on NumPy for matrix operations.\n\n### Approach\nThe multivariate Gaussian PDF is defined by the formula:\n\n\\[ p(x) = \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)\\right) \\]\n\nwhere:\n- \\( d \\) is the dimensionality of the data.\n- \\( \\mu \\) is the mean vector.\n- \\( \\Sigma \\) is the covariance matrix.\n\nThe steps to compute the PDF for each data point are as follows:\n1. **Compute the Inverse of the Covariance Matrix**: This is necessary to compute the quadratic form in the exponent.\n2. **Compute the Determinant of the Covariance Matrix**: This is used in the normalization factor of the PDF.\n3. **Normalization Factor**: This factor is derived from the determinant and the dimensionality of the data.\n4. **Quadratic Form**: For each data point, compute the quadratic form using the inverse covariance matrix.\n5. **Exponent and PDF Calculation**: Using the quadratic form, compute the exponent and then the PDF value for each data point.\n6. **Rounding**: Each computed PDF value is rounded to four decimal places before being added to the result list.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Computes the multivariate Gaussian probability density function for each sample in X.\"\"\"\n    # Compute the inverse of the covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    # Compute the determinant of the covariance matrix\n    det_cov = np.linalg.det(cov)\n    # Get the dimensionality of the data\n    d = len(mean)\n    # Calculate the normalization factor\n    normalization = 1.0 / np.sqrt((2 * np.pi) ** d * det_cov)\n    # Initialize the result list\n    result = []\n    # Iterate over each sample in X\n    for x in X:\n        # Subtract the mean from the sample\n        x_minus_mu = x - mean\n        # Compute the quadratic form (x - \u03bc)^T \u03a3^{-1} (x - \u03bc)\n        quadratic = x_minus_mu.dot(inv_cov).dot(x_minus_mu)\n        # Compute the exponent\n        exponent = -0.5 * quadratic\n        # Calculate the PDF value\n        pdf = normalization * np.exp(exponent)\n        # Append the rounded value to the result list\n        result.append(round(pdf, 4))\n    return result\n```\n\n### Explanation\n1. **Inverse and Determinant Calculation**: The inverse of the covariance matrix is computed using `np.linalg.inv`, and the determinant is computed using `np.linalg.det`. These operations are essential for the quadratic form and normalization factor.\n2. **Normalization Factor**: This factor is derived from the determinant of the covariance matrix and the dimensionality of the data, ensuring the PDF is correctly scaled.\n3. **Quadratic Form**: For each data point, the quadratic form is computed using matrix multiplication, which involves the inverse covariance matrix. This form is crucial for the exponent in the PDF formula.\n4. **Exponent and PDF Calculation**: The exponent is derived from the quadratic form, and the PDF is computed by multiplying the normalization factor with the exponential of the exponent.\n5. **Rounding**: Each computed PDF value is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the multivariate Gaussian PDF for each data point, ensuring accuracy and correctness while adhering to the constraints of the problem."}
{"task_id": 419, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Bayesian linear regression model with a conjugate Gaussian prior and known observation variance. The goal is to compute the MAP (Maximum A Posteriori) predictive mean for new data points.\n\n### Approach\n1. **Add Intercept**: If `fit_intercept` is True, add an intercept column (a column of ones) to both the training data `X` and the new data `X_new`.\n2. **Prior Covariance Matrix**: Construct the prior covariance matrix `V` based on the input. If `V` is None, use the identity matrix. If `V` is a scalar, treat it as a multiple of the identity matrix. If `V` is a 1D array, treat it as the diagonal of the matrix. If `V` is a 2D array, use it directly as it is assumed to be symmetric and positive-definite.\n3. **Compute Posterior**: Calculate the posterior distribution parameters. This involves computing the posterior covariance matrix `Sigma_p` and the posterior mean `mu_p`.\n4. **Predictive Mean**: For each row in `X_new`, compute the MAP predictive mean using the posterior mean `mu_p`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    if fit_intercept:\n        # Add intercept term to X and X_new\n        X = np.hstack([np.ones((X.shape[0], 1)), X])\n        X_new = np.hstack([np.ones((X_new.shape[0], 1)), X_new])\n    \n    M = X.shape[1]\n    \n    # Handle V: construct the prior covariance matrix\n    if V is None:\n        V = np.eye(M)\n    elif np.isscalar(V):\n        V = V * np.eye(M)\n    elif V.ndim == 1:\n        V = np.diag(V)\n    else:\n        # Assume V is already a square matrix\n        pass\n    \n    # Compute V inverse\n    V_inv = np.linalg.inv(V)\n    \n    # Compute X^T X\n    X_T_X = X.T @ X\n    \n    # Sum of V inverse and X^T X\n    sum_V_inv_XTX = V_inv + X_T_X\n    \n    # Compute posterior covariance Sigma_p\n    Sigma_p = np.linalg.inv(sum_V_inv_XTX)\n    \n    # Compute mu_vec\n    if np.isscalar(mu):\n        mu_vec = np.full(M, mu)\n    else:\n        mu_vec = mu\n    \n    # Compute term1 = V_inv @ mu_vec\n    term1 = V_inv @ mu_vec\n    \n    # Compute term2 = X^T y\n    term2 = X.T @ y\n    \n    # Compute posterior mean mu_p\n    mu_p = Sigma_p @ (term1 + term2)\n    \n    # Compute predictive mean for X_new\n    y_pred = X_new @ mu_p\n    \n    # Round to 4 decimal places and convert to list\n    rounded_pred = [round(v, 4) for v in y_pred]\n    \n    return rounded_pred\n```\n\n### Explanation\n1. **Intercept Addition**: The intercept is added to both the training data and the new data to account for the bias term in the linear regression model.\n2. **Prior Covariance Handling**: The prior covariance matrix `V` is constructed based on its input type (scalar, 1D array, or 2D matrix). This matrix is then inverted to compute the prior precision matrix.\n3. **Posterior Computation**: The posterior covariance matrix `Sigma_p` is computed by inverting the sum of the prior precision matrix and the matrix product of the training data transpose with itself. The posterior mean `mu_p` is derived using the posterior covariance matrix and the data.\n4. **Prediction**: The predictive mean for each row in `X_new` is computed using the posterior mean, and the results are rounded to four decimal places for the final output.\n\nThis approach efficiently combines the prior information with the observed data to make predictions, leveraging the properties of Gaussian distributions and linear algebra."}
{"task_id": 423, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) between two lists of non-negative numbers, which represent the ground-truth values and the model predictions. The RMSLE is a metric that penalizes the ratio between the predicted and actual values, making it suitable for regression tasks.\n\n### Approach\n1. **Input Validation**: \n   - Check if the lengths of the two input lists are equal. If not, return -1.\n   - Check if either list is empty. If so, return -1.\n   - Ensure all elements in both lists are non-negative. If any element is negative, return -1.\n\n2. **Compute RMSLE**:\n   - Convert the lists into numpy arrays for efficient vectorized operations.\n   - Compute the natural logarithm of (1 + actual) and (1 + predicted) for each element.\n   - Calculate the differences between these logarithms, square them, and compute the mean of these squared differences.\n   - Take the square root of the mean to get the RMSLE.\n   - Round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    if any(a < 0 or p < 0 for a, p in zip(actual, predicted)):\n        return -1\n    a = np.array(actual)\n    p = np.array(predicted)\n    log_a = np.log(1 + a)\n    log_p = np.log(1 + p)\n    diffs = log_p - log_a\n    squared_diffs = diffs ** 2\n    mean_squared = np.mean(squared_diffs)\n    rmsle = np.sqrt(mean_squared)\n    return round(rmsle, 4)\n```\n\n### Explanation\n- **Input Validation**: The function first ensures that the inputs are valid by checking their lengths and non-negativity. This step is crucial to handle invalid inputs gracefully.\n- **Logarithmic Transformation**: By converting the actual and predicted values to their logarithms, we transform the problem into one that penalizes relative errors rather than absolute errors.\n- **Vectorized Operations**: Using numpy arrays allows efficient computation of logarithms, differences, and means, which is particularly useful for large datasets.\n- **Rounding**: The result is rounded to four decimal places to meet the specified output format, ensuring precision and readability.\n\nThis approach efficiently handles the computation of RMSLE while ensuring robustness against invalid inputs, making it suitable for various regression tasks."}
{"task_id": 428, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Hann window of a specified length. The Hann window is widely used in digital signal processing for tasks like spectral analysis because it smoothly reaches zero at the endpoints, reducing spectral leakage. The function should handle both symmetric and periodic windows, with the periodic window being slightly adjusted to ensure continuity between successive windows.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length (`window_len`) is less than or equal to 0, return an empty list.\n   - If the window length is 1, return a list containing a single element [1.0].\n\n2. **Window Generation**:\n   - For a symmetric window, compute the window directly using the given formula for each point from 0 to `window_len - 1`.\n   - For a periodic window, compute a symmetric window of length `window_len + 1` and then discard the last element to ensure the window is suitable for periodic use.\n\n3. **Formula Application**:\n   - The Hann window values are computed using the formula: \\( hann(n) = 0.5 - 0.5 \\times \\cos(2 \\times \\pi \\times n / (N - 1)) \\), where \\( N \\) is the effective window length (either `window_len` for symmetric or `window_len + 1` for periodic).\n\n4. **Rounding and Conversion**:\n   - Each computed value is rounded to 4 decimal places and converted to a list of floats before being returned.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    if symmetric:\n        N = window_len\n    else:\n        N = window_len + 1\n    \n    n = np.arange(N)\n    hann_values = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n    \n    if not symmetric:\n        hann_values = hann_values[:window_len]\n    \n    rounded = np.round(hann_values, 4).tolist()\n    return rounded\n```\n\n### Explanation\n- **Special Cases**: The function immediately handles cases where the window length is non-positive or exactly 1, returning appropriate results without further computation.\n- **Window Length Calculation**: Depending on whether the window is symmetric or periodic, the effective window length \\( N \\) is determined. For periodic windows, \\( N \\) is one more than the specified window length.\n- **Hann Window Formula**: The formula is applied to each point in the range from 0 to \\( N-1 \\) to compute the window values.\n- **Periodic Adjustment**: For periodic windows, the computed values are truncated to the specified window length after the full symmetric window is generated.\n- **Rounding and Conversion**: The computed values are rounded to four decimal places and converted to a list of floats for the final result.\n\nThis approach ensures that the generated Hann window is both correct and efficient, handling all edge cases and providing the required precision."}
{"task_id": 433, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the fan-in and fan-out values for a given weight tensor shape according to specific rules. The fan-in and fan-out values are crucial for initializing neural network weights properly, especially in schemes like Xavier/Glorot initialization.\n\n### Approach\nThe approach to solve this problem involves two main cases based on the dimensionality of the input tensor:\n\n1. **4-Dimensional Tensor (Convolutional Kernel)**:\n   - For a 4-dimensional tensor, the shape is assumed to be `(out_channels, in_channels, kernel_height, kernel_width)`.\n   - The fan-in is calculated as the product of the number of input channels and the receptive field size (kernel_height * kernel_width).\n   - The fan-out is calculated as the product of the number of output channels and the same receptive field size.\n\n2. **All Other Tensors**:\n   - For tensors with dimensions other than 4, the fan-in and fan-out are simply the first two elements of the shape tuple. This applies to dense layers, embeddings, and higher-dimensional tensors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        out_channels = shape[0]\n        in_channels = shape[1]\n        kernel_height = shape[2]\n        kernel_width = shape[3]\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **4-Dimensional Case**: When the tensor is a 4-dimensional convolutional kernel, the function calculates the fan-in and fan-out by considering the number of input and output channels and the size of the kernel. The receptive field size is the product of the kernel's height and width.\n- **Other Cases**: For tensors that are not 4-dimensional, the function directly uses the first two dimensions of the shape to determine the fan-in and fan-out values.\n\nThis approach ensures that the function correctly handles both convolutional kernels and other types of tensors, providing the required fan-in and fan-out values as floats."}
{"task_id": 435, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a tiny version of the Gradient Boosting Regressor that uses ordinary least-squares (OLS) linear regression as the weak learner and the squared error as the loss function. The goal is to make predictions based on the given training data and return the results rounded to four decimal places.\n\n### Approach\n1. **Convert Input Data**: Convert the input features matrix `X` and target vector `y` into NumPy arrays of float type.\n2. **Handle Special Cases**: If the number of estimators (`n_estimators`) is less than or equal to zero or the learning rate (`learning_rate`) is zero, return a vector filled with the mean of the target vector `y`.\n3. **Initialize Predictions**: Start with the initial prediction as the mean of the target vector `y`.\n4. **Iterative Boosting**: For each estimator, compute the residuals, fit an OLS linear model on these residuals, and update the predictions using the weak learner's output scaled by the learning rate.\n5. **Return Results**: After all iterations, round the final predictions to four decimal places and convert them to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y, dtype=np.float64)\n    m = len(y)\n    \n    if m == 0:\n        return []\n    \n    mean_y = np.mean(y)\n    \n    if n_estimators <= 0 or learning_rate == 0:\n        predictions = np.full(m, mean_y)\n    else:\n        y_hat = np.full(m, mean_y)\n        for _ in range(n_estimators):\n            residuals = y - y_hat\n            # Add intercept term to X\n            X_intercept = np.c_[X, np.ones(m)]\n            # Fit OLS model\n            beta = np.linalg.lstsq(X_intercept, residuals, rcond=None)[0]\n            h_t = X_intercept.dot(beta)\n            # Update the prediction\n            y_hat += learning_rate * h_t\n        predictions = y_hat\n    \n    # Round to 4 decimal places and convert to list\n    rounded_predictions = np.round(predictions, 4).tolist()\n    return rounded_predictions\n```\n\n### Explanation\n1. **Data Conversion**: The input lists `X` and `y` are converted to NumPy arrays to facilitate efficient numerical operations.\n2. **Special Cases Handling**: If the number of estimators or the learning rate is non-positive, the function immediately returns a vector of the mean value of `y`.\n3. **Initialization**: The initial prediction is set to the mean of `y` to start the boosting process.\n4. **Boosting Loop**: For each estimator, the residuals (errors) between the current predictions and the actual target values are computed. An OLS model is then fit on these residuals, and the model's predictions are used to update the overall predictions.\n5. **Final Predictions**: After all iterations, the predictions are rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach ensures that the model iteratively improves its predictions by focusing on the errors from the previous step, leveraging the simplicity and efficiency of OLS regression."}
{"task_id": 437, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the logistic sigmoid function and its first and second derivatives based on the given order parameter. The function should handle scalar, list, and NumPy array inputs and return the result in the appropriate format.\n\n### Approach\n1. **Check Order Validity**: The function first checks if the provided order is valid (0, 1, or 2). If not, it returns -1.\n2. **Convert Input to NumPy Array**: The input `x` is converted to a NumPy array to handle both scalar and array inputs uniformly.\n3. **Determine Scalar or Array**: The function checks if the input is a scalar (0-dimensional array) or an array (higher-dimensional).\n4. **Compute Sigmoid Function**: Depending on the order, the function computes the sigmoid function or its derivatives:\n   - Order 0: Sigmoid function value.\n   - Order 1: First derivative of the sigmoid function.\n   - Order 2: Second derivative of the sigmoid function.\n5. **Round Results**: The computed results are rounded to four decimal places.\n6. **Return Appropriate Format**: If the input was a scalar, the result is returned as a float. If the input was an array, the result is returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in [0, 1, 2]:\n        return -1\n\n    x_arr = np.asarray(x)\n    is_scalar = x_arr.ndim == 0\n\n    sigma = 1 / (1 + np.exp(-x_arr))\n\n    if order == 0:\n        result = sigma\n    elif order == 1:\n        result = sigma * (1 - sigma)\n    elif order == 2:\n        result = sigma * (1 - sigma) * (1 - 2 * sigma)\n\n    rounded = np.round(result, 4)\n\n    if is_scalar:\n        return rounded.item()\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Order Check**: The function immediately returns -1 if the order is invalid.\n- **Input Handling**: The input is converted to a NumPy array to handle both scalar and array inputs uniformly. This allows vectorized operations for efficient computation.\n- **Sigmoid Calculation**: The sigmoid function and its derivatives are computed using NumPy operations, ensuring efficiency and correctness.\n- **Result Formatting**: The results are rounded to four decimal places and returned as a float for scalar inputs or a list for array inputs, preserving the original shape of the input.\n\nThis approach ensures that the function is both efficient and versatile, handling various input types and orders correctly."}
{"task_id": 438, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a list of class labels into a one-hot encoded matrix. This matrix is a 2D array where each row corresponds to a class label from the input list, and each column represents a class, with a 1 indicating the presence of that class and 0 otherwise.\n\n### Approach\n1. **Input Handling**: The function accepts a 1-D list or NumPy array `y` containing non-negative integer class indices and an optional `num_classes` argument.\n2. **Determine Number of Classes**: If `num_classes` is `None`, it is automatically determined as `max(y) + 1`. If `num_classes` is provided but is smaller than `max(y) + 1`, the function returns -1 to indicate that one-hot encoding is impossible.\n3. **Create One-Hot Matrix**: Using NumPy's `eye` function, we create an identity matrix of size `num_classes`. Each row of this matrix corresponds to a class label in `y`, resulting in a one-hot encoded matrix.\n4. **Convert to List**: The resulting NumPy array is converted to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y = np.asarray(y)\n    if len(y) == 0:\n        return []\n    if num_classes is None:\n        num_classes = y.max() + 1\n    else:\n        if num_classes < y.max() + 1:\n            return -1\n    one_hot = np.eye(num_classes, dtype=int)[y]\n    return one_hot.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input `y` is converted to a NumPy array to facilitate array operations.\n2. **Empty Check**: If `y` is empty, the function immediately returns an empty list.\n3. **Determine Classes**: The number of classes is determined either from the input or by calculating it based on the maximum value in `y`.\n4. **Validation**: If the provided `num_classes` is insufficient, the function returns -1.\n5. **One-Hot Encoding**: Using NumPy's `eye` function, an identity matrix is created and indexed by `y` to form the one-hot encoded matrix.\n6. **Conversion**: The resulting matrix is converted to a list of lists for the final output.\n\nThis approach ensures that the function efficiently handles both list and NumPy array inputs, correctly computes the one-hot encoding, and returns the appropriate result or error code."}
{"task_id": 439, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Apriori algorithm to discover all frequent itemsets in a transactional database. The goal is to find all itemsets whose empirical support is at least a given minimum support threshold. The solution must follow the classical Apriori breadth-first strategy and return the itemsets in a specific order.\n\n### Approach\nThe Apriori algorithm is a popular method for mining frequent itemsets from a transactional database. The algorithm works in a breadth-first manner, starting with single-item candidates and progressively building larger itemsets. Here's a detailed breakdown of the approach:\n\n1. **Preprocessing**: Collect all unique items from the transactions and sort them. This helps in efficiently generating candidate itemsets.\n\n2. **Initial Step (1-itemsets)**: Compute the support for each individual item. Items with support greater than or equal to the minimum support threshold are considered frequent and form the initial set of itemsets.\n\n3. **Iterative Generation of Larger Itemsets**: For each size `k` starting from 2, generate candidate itemsets by combining frequent itemsets of size `k-1`. Each candidate is formed by adding an item to a frequent itemset of size `k-1`, ensuring that the new item is larger than the last item in the current set to avoid duplicates.\n\n4. **Support Calculation**: For each candidate itemset, compute its support by checking how many transactions contain all items in the candidate. Keep only those candidates whose support meets or exceeds the minimum threshold.\n\n5. **Termination**: The process continues until no new frequent itemsets are found.\n\n6. **Sorting**: The resulting frequent itemsets are sorted first by their length and then lexicographically.\n\n### Solution Code\n```python\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # Preprocess: collect all unique items and sort them\n    unique_items = sorted({item for transaction in transactions for item in transaction})\n    total_transactions = len(transactions)\n\n    # Compute initial 1-item frequent itemsets\n    item_counts = {}\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] = item_counts.get(item, 0) + 1\n\n    current_level = []\n    freq_itemsets = []\n    for item in unique_items:\n        support = item_counts.get(item, 0) / total_transactions\n        if support >= min_sup:\n            current_level.append((item,))\n            freq_itemsets.append((item,))\n\n    k = 2\n    while current_level:\n        # Group current_level by their prefix (k-2) items\n        groups = {}\n        for itemset in current_level:\n            prefix = tuple(itemset[:-1]) if len(itemset) > 1 else ()\n            if prefix not in groups:\n                groups[prefix] = []\n            groups[prefix].append(itemset)\n\n        # Generate next level candidates\n        next_level = []\n        seen = set()  # To avoid duplicate candidates\n        for prefix, group in groups.items():\n            # Sort the group by last item\n            group_sorted = sorted(group, key=lambda x: x[-1])\n            # Iterate through all pairs in the group\n            for i in range(len(group_sorted)):\n                for j in range(i + 1, len(group_sorted)):\n                    a = group_sorted[i]\n                    b = group_sorted[j]\n                    # If the last item of b is larger than a's\n                    if b[-1] > a[-1]:\n                        # Form the new candidate\n                        new_candidate = tuple(sorted(prefix + (a[-1], b[-1])))\n                        if new_candidate not in seen:\n                            seen.add(new_candidate)\n                            next_level.append(new_candidate)\n\n        # Now, compute support for each candidate in next_level\n        candidate_support = {}\n        for candidate in next_level:\n            count = 0\n            for transaction in transactions:\n                if all(item in transaction for item in candidate):\n                    count += 1\n            support = count / total_transactions\n            if support >= min_sup:\n                candidate_support[candidate] = True\n            else:\n                candidate_support[candidate] = False\n\n        # Keep the candidates that are frequent\n        current_level = [candidate for candidate in next_level if candidate_support.get(candidate, False)]\n\n        # Add to freq_itemsets\n        freq_itemsets.extend(current_level)\n\n        # Check if any candidates were found\n        if not current_level:\n            break\n        k += 1\n\n    # Sort the freq_itemsets as per the requirements\n    # First by length, then lexicographically\n    freq_itemsets.sort(key=lambda x: (len(x), x))\n    return freq_itemsets\n```\n\n### Explanation\n1. **Preprocessing**: The code starts by collecting all unique items from the transactions and sorting them. This helps in efficiently generating candidate itemsets.\n\n2. **Initial Step**: The support for each individual item is computed. Items with support greater than or equal to the minimum threshold are added to the initial set of frequent itemsets.\n\n3. **Iterative Generation**: For each size `k`, the code generates candidate itemsets by combining frequent itemsets of size `k-1`. Each candidate is formed by adding an item to a frequent itemset, ensuring no duplicates.\n\n4. **Support Calculation**: For each candidate, the support is calculated by checking how many transactions contain all items in the candidate. Only candidates with sufficient support are kept.\n\n5. **Termination**: The process stops when no new frequent itemsets are found.\n\n6. **Sorting**: The resulting itemsets are sorted by length and then lexicographically to meet the output requirements.\n\nThis approach ensures that all frequent itemsets are discovered efficiently and returned in the required order."}
{"task_id": 440, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the probability predictions from multiple estimators (e.g., trees in a Random Forest) to form a final prediction. The goal is to average the probabilities for each class across all estimators for each sample and then determine the predicted class label based on the highest average probability.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is a three-level nested list. We convert this into a NumPy array to facilitate efficient computation.\n2. **Average Probabilities**: Compute the mean of the probabilities across all estimators for each sample and class. This is done by taking the mean along the first axis (estimators) of the NumPy array.\n3. **Round Probabilities**: Round each averaged probability to four decimal places.\n4. **Determine Predicted Classes**: For each sample, find the class with the highest probability. In case of ties, the smallest index is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the predictions to a NumPy array for efficient computation\n    np_predictions = np.array(predictions)\n    \n    # Compute the average probabilities across all estimators for each sample and class\n    averaged = np.mean(np_predictions, axis=0)\n    \n    # Round the averaged probabilities to four decimal places\n    rounded = np.round(averaged, 4)\n    \n    # Convert the NumPy array to a Python list of lists\n    averaged_probs = rounded.tolist()\n    \n    # Determine the predicted class for each sample\n    predicted_classes = np.argmax(rounded, axis=1).tolist()\n    \n    return (averaged_probs, predicted_classes)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted into a NumPy array to leverage efficient numerical operations.\n2. **Averaging Probabilities**: Using NumPy's `mean` function with `axis=0` computes the average probability for each class across all estimators for each sample.\n3. **Rounding**: The `round` function is used to round each averaged probability to four decimal places.\n4. **Predicted Classes**: The `argmax` function is used to find the index of the maximum probability for each sample, ensuring that the smallest index is chosen in case of ties.\n\nThis approach efficiently handles the aggregation and ensures that the results are returned in the required format."}
{"task_id": 444, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Radial Basis Function (RBF) kernel matrix, which is used in various machine learning algorithms such as Gaussian Processes and Support Vector Machines. The RBF kernel measures the similarity between two vectors in a high-dimensional space, which is useful for tasks like classification and regression.\n\n### Approach\n1. **Input Validation**: \n   - Ensure that the input vectors `X` and `Y` (if provided) have the same number of features.\n   - Validate the scale parameter `sigma` to ensure it is either a positive float, a list of positive floats with the same length as the number of features, or `None` (in which case a default value is used).\n\n2. **Kernel Matrix Computation**:\n   - Convert the input vectors into NumPy arrays for efficient computation.\n   - Compute the difference matrix between all pairs of vectors in `X` and `Y`.\n   - Normalize these differences by the scale parameter `sigma` and compute the squared differences.\n   - Sum these squared differences across all features and apply the exponential function to compute the kernel values.\n\n3. **Result Formatting**:\n   - Round the computed kernel values to four decimal places.\n   - Convert the resulting NumPy array into a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Validate X\n    if not isinstance(X, list) or not all(isinstance(row, list) for row in X):\n        return -1\n    if not X:\n        return -1\n    C = len(X[0])\n    for row in X:\n        if len(row) != C:\n            return -1\n\n    # Validate Y\n    if Y is not None:\n        if not isinstance(Y, list) or not all(isinstance(row, list) for row in Y):\n            return -1\n        if not Y:\n            return -1\n        if len(Y[0]) != C:\n            return -1\n\n    # Process sigma\n    if sigma is None:\n        sigma_val = np.sqrt(C / 2)\n    elif isinstance(sigma, (int, float)):\n        if sigma <= 0:\n            return -1\n        sigma_val = [sigma] * C\n    elif isinstance(sigma, list):\n        if len(sigma) != C:\n            return -1\n        for s in sigma:\n            if not isinstance(s, (int, float)) or s <= 0:\n                return -1\n        sigma_val = sigma\n    else:\n        return -1\n\n    # Convert to numpy arrays\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n\n    # Compute the difference matrix\n    diff = X_np[:, np.newaxis] - Y_np\n    sigma_np = np.array(sigma_val).reshape(1, 1, -1)\n    diff_scaled = diff / sigma_np\n    squared_diff = diff_scaled ** 2\n    sum_squared = np.sum(squared_diff, axis=2)\n    kernel = np.exp(-0.5 * sum_squared)\n\n    # Round to 4 decimal places\n    kernel_rounded = np.round(kernel, 4)\n\n    # Convert to nested list\n    result = kernel_rounded.tolist()\n\n    return result\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input vectors `X` and `Y` are valid. It ensures that all vectors have the same number of features and that the scale parameter `sigma` is valid.\n2. **Kernel Matrix Computation**: Using NumPy arrays, the function efficiently computes the differences between all pairs of vectors, normalizes these differences by `sigma`, and computes the squared differences. These are summed across features and exponentiated to get the kernel values.\n3. **Result Formatting**: The computed kernel matrix is rounded to four decimal places and converted into a nested Python list for the final output.\n\nThis approach ensures that the function handles various edge cases and efficiently computes the RBF kernel matrix while adhering to the problem constraints."}
{"task_id": 446, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the best single-level split (decision stump) for a decision tree based on information gain. The goal is to maximize the decrease in entropy when splitting the dataset into two subsets based on a feature and a threshold.\n\n### Approach\n1. **Compute Parent Entropy**: Calculate the entropy of the entire dataset. If the entropy is zero, all samples have the same label, and we return the default values.\n2. **Iterate Over Each Feature**: For each feature, determine the possible thresholds by finding midpoints between consecutive distinct values in the sorted column.\n3. **Evaluate Each Threshold**: For each threshold, split the dataset into left and right subsets. Compute the entropy of each subset and the information gain for the split.\n4. **Track Best Split**: Keep track of the feature and threshold that provide the highest information gain. In case of ties, select the smallest feature index and threshold.\n5. **Determine Majority Labels**: After identifying the best split, determine the majority labels for the left and right subsets.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Compute parent entropy\n    counts = Counter(y)\n    if len(counts) == 1:\n        majority_label = y[0]\n        return (-1, None, majority_label, majority_label)\n    \n    probs = [count / len(y) for count in counts.values()]\n    H_parent = sum(-p * np.log2(p) for p in probs) if probs else 0.0\n    \n    n_samples, n_features = X.shape\n    best_feature = -1\n    best_threshold = None\n    max_gain = -1\n    \n    for j in range(n_features):\n        column = X[:, j]\n        sorted_col = sorted(column)\n        unique_sorted = sorted(list(set(sorted_col)))\n        \n        if len(unique_sorted) < 2:\n            continue\n        \n        midpoints = [(unique_sorted[i] + unique_sorted[i+1])/2 for i in range(len(unique_sorted)-1)]\n        \n        current_max_gain = -1\n        current_best_threshold = None\n        \n        for t in midpoints:\n            mask = column <= t\n            y_left = y[mask]\n            y_right = y[~mask]\n            \n            # Compute H_left\n            if len(y_left) == 0:\n                H_left = 0.0\n            else:\n                counts_left = Counter(y_left)\n                probs_left = [cnt / len(y_left) for cnt in counts_left.values()]\n                H_left = sum(-p * np.log2(p) for p in probs_left) if probs_left else 0.0\n            \n            # Compute H_right\n            if len(y_right) == 0:\n                H_right = 0.0\n            else:\n                counts_right = Counter(y_right)\n                probs_right = [cnt / len(y_right) for cnt in counts_right.values()]\n                H_right = sum(-p * np.log2(p) for p in probs_right) if probs_right else 0.0\n            \n            p_left = len(y_left) / n_samples\n            p_right = len(y_right) / n_samples\n            \n            gain = H_parent - p_left * H_left - p_right * H_right\n            \n            if gain > current_max_gain:\n                current_max_gain = gain\n                current_best_threshold = t\n            elif gain == current_max_gain:\n                if t < current_best_threshold:\n                    current_best_threshold = t\n        \n        if current_max_gain > max_gain:\n            max_gain = current_max_gain\n            best_feature = j\n            best_threshold = current_best_threshold\n        elif current_max_gain == max_gain:\n            if j < best_feature:\n                best_feature = j\n                best_threshold = current_best_threshold\n            elif j == best_feature:\n                if current_best_threshold < best_threshold:\n                    best_threshold = current_best_threshold\n    \n    if max_gain <= 0:\n        counts = Counter(y)\n        max_count = max(counts.values())\n        candidates = [label for label, cnt in counts.items() if cnt == max_count]\n        majority_label = min(candidates)\n        return (-1, None, majority_label, majority_label)\n    else:\n        mask = X[:, best_feature] <= best_threshold\n        y_left = y[mask]\n        y_right = y[~mask]\n        \n        # Compute left majority\n        counts_left = Counter(y_left)\n        max_count_left = max(counts_left.values())\n        candidates_left = [label for label, cnt in counts_left.items() if cnt == max_count_left]\n        left_majority = min(candidates_left)\n        \n        # Compute right majority\n        counts_right = Counter(y_right)\n        max_count_right = max(counts_right.values())\n        candidates_right = [label for label, cnt in counts_right.items() if cnt == max_count_right]\n        right_majority = min(candidates_right)\n        \n        best_threshold = round(best_threshold, 4)\n        return (best_feature, best_threshold, left_majority, right_majority)\n```\n\n### Explanation\n1. **Parent Entropy Calculation**: The entropy of the entire dataset is computed to determine the initial disorder. If all labels are the same, the function returns the default values.\n2. **Feature and Threshold Evaluation**: For each feature, possible thresholds are determined by midpoints between consecutive distinct values. Each threshold is evaluated to compute the information gain.\n3. **Information Gain Calculation**: For each threshold, the dataset is split, and the entropy of each subset is computed. The information gain is used to determine the best split.\n4. **Best Split Selection**: The feature and threshold with the highest information gain are selected. In case of ties, the smallest feature index and threshold are chosen.\n5. **Majority Labels**: After identifying the best split, the majority labels for the left and right subsets are determined to provide the final result."}
{"task_id": 452, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a given dataset into two subsets based on a specified feature column and a threshold. The dataset can be either a list of lists or a NumPy 2D array. The split is determined by whether the threshold is numeric or categorical.\n\n### Approach\n1. **Convert Input to NumPy Array**: If the input dataset is a list, convert it to a NumPy array for easier manipulation.\n2. **Extract Feature Column**: Extract the values from the specified feature column.\n3. **Determine Threshold Type**: Check if the threshold is numeric (int or float) or categorical.\n4. **Create Condition**: Based on the threshold type, create a boolean condition to determine which samples belong to the first subset.\n5. **Split Dataset**: Use the condition to split the dataset into two subsets.\n6. **Convert to Lists**: Convert the resulting NumPy arrays to regular Python lists and return them.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Convert X to a NumPy array if it's a list\n    if isinstance(X, list):\n        X = np.array(X)\n    \n    # Extract the feature column\n    feature_values = X[:, feature_i]\n    \n    # Determine if the threshold is numeric\n    is_numeric = type(threshold) in (int, float)\n    \n    # Create the condition based on the threshold type\n    if is_numeric:\n        condition = feature_values >= threshold\n    else:\n        condition = feature_values == threshold\n    \n    # Split the data into two subsets\n    subset0 = X[condition]\n    subset1 = X[~condition]\n    \n    # Convert the subsets to lists\n    subset0_list = subset0.tolist()\n    subset1_list = subset1.tolist()\n    \n    return [subset0_list, subset1_list]\n```\n\n### Explanation\n1. **Convert Input**: The function first checks if the input dataset is a list and converts it to a NumPy array if necessary.\n2. **Extract Feature Column**: The values from the specified feature column are extracted to determine the split.\n3. **Determine Threshold Type**: The function checks if the threshold is numeric or categorical to decide the comparison condition.\n4. **Create Condition**: A boolean array is created where each element indicates whether the corresponding sample meets the threshold condition.\n5. **Split Dataset**: Using the boolean condition, the dataset is split into two subsets. The first subset contains samples that meet the condition, and the second subset contains the remaining samples.\n6. **Convert to Lists**: The resulting subsets are converted from NumPy arrays to Python lists and returned.\n\nThis approach ensures that the dataset is split efficiently while preserving the original order of samples."}
{"task_id": 453, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the target Q-values for training a Deep Q-Network (DQN). The target Q-values are used to update the neural network's weights during the training phase. The target values depend on the agent's current Q-value estimates and the next-state Q-value estimates.\n\n### Approach\n1. **Input Validation**: First, we check if all input arrays have consistent lengths and if the discount factor `gamma` is within the valid range [0, 1]. If any of these conditions are not met, we return -1.\n2. **Compute Targets**: For each sample in the batch:\n   - If the next state is terminal (indicated by `dones[i]`), the target value is simply the immediate reward.\n   - If the next state is not terminal, the target value is computed as the immediate reward plus the discounted maximum Q-value from the next state.\n3. **Update Q-values**: For each sample, we create a copy of the current Q-values, update the Q-value corresponding to the action taken with the computed target value, and round the result to four decimal places.\n4. **Return Result**: The updated Q-values are returned as a nested list of lists, where each inner list represents the target Q-values for a sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if all input lists have the same length as Q\n    batch_size = len(Q)\n    if (len(Q_next) != batch_size or \n        len(actions) != batch_size or \n        len(rewards) != batch_size or \n        len(dones) != batch_size):\n        return -1\n    \n    # Check if gamma is within the valid range\n    if not (0 <= gamma <= 1):\n        return -1\n    \n    result = []\n    for i in range(batch_size):\n        action = actions[i]\n        reward = rewards[i]\n        done = dones[i]\n        \n        if done:\n            target = reward\n        else:\n            max_q_next = np.max(Q_next[i])\n            target = reward + gamma * max_q_next\n        \n        # Create a copy of the current Q values for this state\n        y_i = Q[i].copy()\n        y_i[action] = target\n        \n        # Round to 4 decimal places and convert to a list\n        y_i = np.round(y_i, 4)\n        result.append(y_i.tolist())\n    \n    return result\n```\n\n### Explanation\n- **Input Validation**: The function first checks if all input arrays have the same length and if `gamma` is within the valid range. This ensures that the inputs are consistent and valid for further processing.\n- **Target Computation**: For each sample, the target Q-value is computed based on whether the next state is terminal or not. If terminal, the target is the immediate reward. If not, it is the sum of the immediate reward and the discounted maximum Q-value from the next state.\n- **Q-value Update**: The current Q-values are copied, and the Q-value corresponding to the action taken is updated with the computed target. The updated values are then rounded to four decimal places and converted to a list for the result.\n- **Result Construction**: The function constructs a list of updated Q-values for each sample and returns it as the final result."}
{"task_id": 458, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array of non-negative integer labels into a two-dimensional one-hot encoded matrix. This is a common requirement in machine learning workflows where class labels are represented as integers, and neural networks expect them in a categorical form.\n\n### Approach\n1. **Input Validation**: \n   - Check if the input labels are empty. If they are, return an empty list.\n   - Ensure all labels are non-negative. If any label is negative, return -1.\n   - If `num_classes` is provided, check if it is smaller than the maximum label value plus one. If it is, return -1. If `num_classes` is not provided, compute it as the maximum label value plus one.\n\n2. **One-Hot Encoding**:\n   - For each label in the input, create a row of zeros of length `num_classes`.\n   - Set the element at the index corresponding to the label to 1.\n\n3. **Return the Result**:\n   - Construct and return the resulting list of lists, where each sublist is a one-hot encoded version of the corresponding label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    if len(labels) == 0:\n        return []\n    \n    if min(labels) < 0:\n        return -1\n    \n    if num_classes is not None:\n        if num_classes < (max(labels) + 1):\n            return -1\n    else:\n        num_classes = max(labels) + 1\n    \n    result = []\n    for label in labels:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input labels are empty and returns an empty list if true. It then checks for negative labels and returns -1 if any are found. If `num_classes` is provided, it ensures it is sufficient to cover all possible labels; otherwise, it calculates the required number of classes.\n- **One-Hot Encoding**: For each label, a row of zeros is created, and the position corresponding to the label is set to 1. This is done using a loop that constructs each row individually.\n- **Efficiency**: The approach efficiently handles the conversion by leveraging list operations and ensures that the output is in the correct format, adhering to the constraints and requirements specified."}
{"task_id": 461, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor for one-dimensional data using decision stumps. The goal is to fit an ensemble of these stumps to the training data and return the final predictions.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Initialization**: Start with the initial prediction as the mean of the target values.\n2. **Iterative Boosting**: For each iteration, compute the residuals, find the best split point, and update the predictions using the best split found.\n3. **Residual Calculation**: For each iteration, compute the residuals as the difference between the actual target values and the current predictions.\n4. **Splitting and SSE Calculation**: For each possible split point, compute the sum of squared errors (SSE) for the resulting left and right subsets. The split with the smallest SSE is chosen.\n5. **Update Predictions**: Using the best split, update the predictions by adding the contribution of the current decision stump, scaled by the learning rate.\n6. **Edge Cases Handling**: Handle cases where no valid splits exist or the number of estimators is zero or negative.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n = len(x)\n    \n    if n == 0:\n        return []\n    \n    if n_estimators <= 0:\n        mean_y = np.mean(y)\n        return [np.round(mean_y, 4)] * n\n    \n    y_hat = np.mean(y)\n    \n    for _ in range(n_estimators):\n        r = y - y_hat\n        unique_x = np.unique(x)\n        possible_ts = unique_x[:-1] if len(unique_x) > 1 else []\n        \n        best_t = None\n        best_v_L = 0.0\n        best_v_R = 0.0\n        min_sse = float('inf')\n        \n        for t in possible_ts:\n            left_mask = x <= t\n            right_mask = x > t\n            \n            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n                continue\n            \n            r_left = r[left_mask]\n            r_right = r[right_mask]\n            \n            v_L = np.mean(r_left)\n            v_R = np.mean(r_right)\n            \n            sse = np.sum((r_left - v_L) ** 2) + np.sum((r_right - v_R) ** 2)\n            \n            if sse < min_sse:\n                min_sse = sse\n                best_t = t\n                best_v_L = v_L\n                best_v_R = v_R\n        \n        if best_t is not None:\n            left_mask = x <= best_t\n            right_mask = x > best_t\n            delta = np.zeros(n)\n            delta[left_mask] = best_v_L\n            delta[right_mask] = best_v_R\n        else:\n            delta = np.mean(r) * np.ones(n)\n        \n        y_hat += learning_rate * delta\n    \n    return np.round(y_hat, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial prediction is set to the mean of the target values to start with a baseline prediction.\n2. **Iterative Boosting**: For each of the specified number of iterations, the algorithm computes the residuals, finds the best split point, and updates the predictions.\n3. **Residual Calculation**: Residuals are calculated as the difference between the actual target values and the current predictions.\n4. **Splitting and SSE Calculation**: For each possible split point, the algorithm splits the data, computes the mean residuals for each subset, and calculates the SSE. The split with the smallest SSE is chosen to minimize the error.\n5. **Update Predictions**: The best split's contribution is scaled by the learning rate and added to the current predictions to form the new predictions.\n6. **Edge Cases Handling**: The algorithm handles cases where no valid splits exist by using the mean residual for all samples, ensuring the algorithm continues to make progress.\n\nThis approach efficiently combines weak learners (decision stumps) to form a strong regressor, iteratively improving the predictions by focusing on difficult examples."}
{"task_id": 471, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement one step of the on-policy TD(0) Expected\u2013SARSA algorithm for a tabular setting. The goal is to update the Q-table based on the immediate reward and the next state, considering the epsilon-soft policy derived from the current Q-table.\n\n### Approach\n1. **Problem Understanding**: The task involves updating the Q-table using the Expected\u2013SARSA algorithm. The update rule involves computing the target value based on the immediate reward and the expected future rewards under the current policy.\n2. **Target Calculation**: If the next state is terminal (None), the target is simply the immediate reward. Otherwise, compute the expected Q-value for the next state using the epsilon-soft policy.\n3. **Epsilon-Soft Policy**: For each state, the policy assigns a higher probability to the greedy action (the action with the highest Q-value) and equal lower probabilities to all other actions.\n4. **Q-Table Update**: Update the Q-value for the current state-action pair using the computed target value and the learning rate.\n5. **Rounding**: Ensure all values in the updated Q-table are rounded to four decimal places.\n\n### Solution Code\n```python\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\"\"\"\n    # Make a copy of the Q-table to avoid modifying the original\n    q_table_new = [row.copy() for row in q_table]\n    \n    if next_state is None:\n        target = reward\n    else:\n        s = next_state\n        n_actions = len(q_table_new[s])\n        max_q = max(q_table_new[s])\n        # Find the first action with the maximum Q-value\n        greedy_action = None\n        for a in range(n_actions):\n            if q_table_new[s][a] == max_q:\n                greedy_action = a\n                break\n        # Calculate the expected Q-value\n        expected_q = 0.0\n        for a in range(n_actions):\n            if a == greedy_action:\n                prob = (1 - epsilon) + epsilon / n_actions\n            else:\n                prob = epsilon / n_actions\n            expected_q += prob * q_table_new[s][a]\n        target = reward + gamma * expected_q\n    \n    # Update the Q-value\n    q_table_new[state][action] += lr * (target - q_table_new[state][action])\n    \n    # Round all values to four decimal places\n    rounded_q_table = []\n    for row in q_table_new:\n        rounded_row = [round(x, 4) for x in row]\n        rounded_q_table.append(rounded_row)\n    \n    return rounded_q_table\n```\n\n### Explanation\n1. **Copy Q-Table**: The function starts by creating a deep copy of the Q-table to avoid modifying the original table.\n2. **Target Calculation**: If the next state is terminal, the target is set to the immediate reward. Otherwise, the expected Q-value for the next state is computed using the epsilon-soft policy.\n3. **Epsilon-Soft Policy**: For each action in the next state, the probability is determined based on whether the action is the greedy action (highest Q-value) or not. The expected Q-value is the sum of the product of each action's probability and its Q-value.\n4. **Update Q-Value**: The Q-value for the current state-action pair is updated using the target value and the learning rate.\n5. **Rounding**: The updated Q-table is rounded to four decimal places to meet the output requirements.\n\nThis approach ensures that the Q-table is updated correctly according to the Expected\u2013SARSA algorithm, considering both immediate rewards and expected future rewards under the current policy."}
{"task_id": 474, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a 2-D toy dataset suitable for a binary or multi-class classification problem. The dataset consists of multiple classes, each arranged in a rectangular cluster. The first class is located in the square [1,9]\u00d7[1,9], and subsequent classes are shifted either horizontally or vertically based on the given parameters. The goal is to create a reusable function that generates training and validation sets with corresponding labels.\n\n### Approach\n1. **Problem Analysis**: The task requires generating a dataset where each class is separated into training and validation sets. Each class is positioned in a specific rectangular region, and the labels are determined by the class index. The function must be reusable and handle an optional random seed for reproducibility.\n\n2. **Intuition**: Each class is defined by a specific range for x and y coordinates. The x-coordinate range is determined by the integer division of the class index by 2, while the y-coordinate range is determined by the class index modulo 2 and the given interval. The labels are derived from the class index, ensuring each class has a unique label.\n\n3. **Algorithm**:\n   - For each class, generate x and y coordinates within specified ranges.\n   - Split the generated samples into training and validation sets.\n   - Ensure the coordinates are rounded to four decimal places for precision.\n   - Handle the optional random seed to make the dataset reproducible.\n\n4. **Complexity Analysis**: The algorithm efficiently generates the required samples using uniform distribution, ensuring the dataset is both separable and suitable for classification tasks.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    X_train = np.zeros((0, 2))\n    X_val = np.zeros((0, 2))\n    Y_train = np.zeros(0, dtype=int)\n    Y_val = np.zeros(0, dtype=int)\n    \n    for i in range(m):\n        i_div_2 = i // 2\n        x_low = (i_div_2 + 0.1) * 10\n        x_high = (i_div_2 + 0.9) * 10\n        \n        y_mod = i % 2\n        y_low = (y_mod * interval + 0.1) * 10\n        y_high = (y_mod * interval + 0.9) * 10\n        \n        n_samples = n_train + n_val\n        \n        x = np.random.uniform(x_low, x_high, size=n_samples)\n        y = np.random.uniform(y_low, y_high, size=n_samples)\n        \n        x = np.round(x, 4)\n        y = np.round(y, 4)\n        \n        points = np.column_stack((x, y))\n        \n        label = int((i - 0.5) * 2)\n        \n        train_points = points[:n_train]\n        val_points = points[n_train:]\n        \n        train_labels = np.full(n_train, label, dtype=int)\n        val_labels = np.full(n_val, label, dtype=int)\n        \n        X_train = np.vstack((X_train, train_points))\n        X_val = np.vstack((X_val, val_points))\n        Y_train = np.concatenate((Y_train, train_labels))\n        Y_val = np.concatenate((Y_val, val_labels))\n    \n    return (X_train, X_val, Y_train, Y_val)\n```\n\n### Explanation\n- **Initialization**: The function initializes arrays for training and validation sets.\n- **Loop Through Classes**: For each class, it calculates the x and y ranges based on the class index and interval.\n- **Generate Samples**: It generates the required number of samples for each class, splits them into training and validation sets, and assigns labels.\n- **Rounding**: Coordinates are rounded to four decimal places to ensure precision.\n- **Return**: The function returns the training and validation sets along with their corresponding labels.\n\nThis approach ensures that the generated dataset is both separable and suitable for various classification tasks, while being efficient and reusable."}
{"task_id": 475, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core mathematics of the Adam optimizer, which is used for training deep learning models. The Adam optimizer combines the benefits of two other optimization methods, AdaGrad and RMSProp, by maintaining two moving averages of the gradients: the first moment (mean) and the second moment (uncentered variance). These moments are bias-corrected and used to update the parameters.\n\n### Approach\n1. **Initialization of Moments**: If the previous first moment (`m_prev`) or the previous second moment (`v_prev`) is `None`, initialize them to zero arrays with the same shape as the gradient.\n2. **Compute New Moments**: Calculate the new first moment (`m_t`) and the new second moment (`v_t`) using the given gradient and the decay rates \u03b21 and \u03b22.\n3. **Bias Correction**: Apply bias correction to the new moments to get the bias-corrected estimates (`m_hat` and `v_hat`).\n4. **Parameter Update**: Use the bias-corrected moments to compute the denominator for the parameter update step, ensuring numerical stability by adding a small constant \u03b5. Update the parameters using the learning rate and the computed step size.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Handle None cases for previous moments\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Compute the new moments\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Apply bias correction\n    m_hat = m_t / (1 - (b1 ** t))\n    v_hat = v_t / (1 - (b2 ** t))\n    \n    # Compute the denominator with numerical stability\n    denom = np.sqrt(v_hat) + eps\n    \n    # Update the parameters\n    w_new = w - learning_rate * (m_hat / denom)\n    \n    return (w_new, m_t, v_t)\n```\n\n### Explanation\n1. **Initialization**: The function first checks if `m_prev` or `v_prev` are `None` and initializes them to zero arrays if necessary.\n2. **Moment Calculation**: The new first moment (`m_t`) and second moment (`v_t`) are computed using the exponential moving average formula with decay rates \u03b21 and \u03b22.\n3. **Bias Correction**: The bias-corrected moments (`m_hat` and `v_hat`) are calculated by dividing the new moments by the product of the decay rate and the time step.\n4. **Parameter Update**: The parameters are updated using the learning rate, the bias-corrected first moment, and the square root of the bias-corrected second moment, ensuring numerical stability by adding a small constant \u03b5.\n\nThis approach ensures that the Adam optimizer correctly updates the parameters while handling both scalar and array inputs efficiently."}
{"task_id": 479, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate an epsilon-greedy algorithm for the stationary multi-armed bandit problem. The goal is to determine the optimal strategy for selecting arms to maximize the expected reward over time, balancing between exploration and exploitation.\n\n### Approach\n1. **Initialization**: Start by initializing the estimated value of each arm to a given prior value (`ev_prior`) and set the pull counters for each arm to zero.\n2. **Epsilon-Greedy Strategy**: For each time step, decide whether to explore (randomly select an arm) or exploit (select the arm with the highest estimated value). The decision is based on a probability `epsilon`.\n3. **Update Estimates**: After selecting an arm, update the estimated value of the chosen arm using the incremental sample mean formula. This formula adjusts the estimate based on the reward received and the number of times the arm has been pulled.\n4. **Return Results**: After processing all time steps, return the final estimated values of each arm, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    if not rewards:\n        return []\n    \n    N = len(rewards[0])\n    V = [ev_prior] * N\n    C = [0] * N\n    \n    for t in range(len(rewards)):\n        current_rewards = rewards[t]\n        \n        # Choose arm\n        if np.random.random() < epsilon:\n            i = np.random.randint(N)\n        else:\n            max_v = max(V)\n            candidates = [idx for idx, val in enumerate(V) if val == max_v]\n            i = min(candidates)\n        \n        # Update count\n        C[i] += 1\n        \n        # Get reward\n        r = current_rewards[i]\n        \n        # Update estimate\n        V[i] = V[i] + (r - V[i]) / C[i]\n    \n    # Round to 4 decimals\n    return [round(v, 4) for v in V]\n```\n\n### Explanation\n1. **Initialization**: The function starts by setting the random seed if provided, initializing the estimated values (`V`) and pull counts (`C`) for each arm.\n2. **Epsilon-Greedy Strategy**: For each time step, the algorithm randomly selects an arm with probability `epsilon` or selects the arm with the highest estimated value otherwise. This ensures a balance between exploration and exploitation.\n3. **Update Estimates**: After selecting an arm, the pull counter for that arm is incremented, and the estimated value is updated using the reward received. This update is done using the incremental sample mean formula, which adjusts the estimate based on the new reward and the number of pulls.\n4. **Return Results**: Finally, the function returns the estimated values of each arm, rounded to four decimal places, providing the final results of the simulation.\n\nThis approach efficiently balances between exploring new options and exploiting known good options, making it suitable for the multi-armed bandit problem."}
{"task_id": 481, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm from scratch. DBSCAN is an unsupervised learning algorithm that groups data points into clusters based on their density. Points that are in dense regions (core points) form clusters, while points that are not dense enough are labeled as noise.\n\n### Approach\n1. **Precompute Neighbors**: For each point, compute all other points within a given Euclidean distance (eps). This helps in efficiently determining the reachable points for each point.\n2. **Identify Core Points**: A point is considered a core point if it has at least `min_samples` points within its epsilon neighborhood, including itself.\n3. **Cluster Expansion**: Using a breadth-first search (BFS) approach, expand the cluster from each core point. All points reachable from the core point, including both core and border points, are added to the same cluster.\n4. **Noise Points**: Points that are not reachable from any core point are labeled as noise (-1).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n    \n    n = data.shape[0]\n    # Precompute neighbors for each point\n    neighbors = [[] for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                dist = np.linalg.norm(data[i] - data[j])\n                if dist <= eps:\n                    neighbors[i].append(j)\n    \n    visited = [False] * n\n    cluster_labels = [-1] * n\n    current_cluster = 0\n    \n    for i in range(n):\n        if not visited[i]:\n            # Check if the point is a core point\n            if (len(neighbors[i]) + 1) < min_samples:\n                visited[i] = True\n                cluster_labels[i] = -1\n            else:\n                # Start a new cluster\n                cluster_labels[i] = current_cluster\n                visited[i] = True\n                queue = [i]\n                while queue:\n                    current = queue.pop(0)\n                    for neighbor in neighbors[current]:\n                        if not visited[neighbor]:\n                            visited[neighbor] = True\n                            cluster_labels[neighbor] = current_cluster\n                            # Check if the neighbor is a core point\n                            if (len(neighbors[neighbor]) + 1) >= min_samples:\n                                queue.append(neighbor)\n                current_cluster += 1\n    \n    return cluster_labels\n```\n\n### Explanation\n1. **Precompute Neighbors**: For each point, we calculate the Euclidean distance to every other point and store the indices of points within the given epsilon distance. This helps in quickly identifying reachable points during the clustering process.\n2. **Core Point Identification**: A point is identified as a core point if it has at least `min_samples` points within its epsilon neighborhood, including itself. This is checked by comparing the length of the precomputed neighbors list plus one (to include the point itself) against `min_samples`.\n3. **Cluster Expansion**: Using a BFS approach, we expand the cluster from each core point. This ensures that all reachable points, whether they are core points or border points, are included in the same cluster.\n4. **Noise Points**: Points that do not meet the core point criteria and are not reachable from any core point are labeled as noise (-1).\n\nThis approach efficiently groups points into clusters based on their density, ensuring that noise points are correctly identified and handled."}
{"task_id": 482, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a vector of integer class labels into a 2-D array where each row represents the one-hot encoding of the corresponding label. This is a common task in machine learning for converting categorical data into a format suitable for model training.\n\n### Approach\n1. **Input Handling**: Convert the input vector `y` into a NumPy array and ensure it is a 1-D array. If the input is not 1-D, raise a `ValueError`.\n2. **Empty Input Handling**: If the input vector `y` is empty, handle it appropriately based on whether `num_classes` is provided or not.\n3. **Determine Number of Classes**: If `num_classes` is not provided, compute it as `max(y) + 1`. If `num_classes` is provided, validate that all labels in `y` are within the range [0, num_classes).\n4. **One-Hot Encoding**: Use NumPy's `eye` function to create an identity matrix of size `num_classes` and index this matrix using the labels in `y` to get the one-hot encoded rows.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1D array.\")\n    \n    if len(y) == 0:\n        if num_classes is None:\n            raise ValueError(\"y cannot be empty when num_classes is None.\")\n        else:\n            return np.zeros((0, num_classes), dtype=int)\n    \n    if num_classes is None:\n        num_classes = max(y) + 1\n    else:\n        if np.any(y < 0) or np.any(y >= num_classes):\n            raise ValueError(\"All labels must be in [0, num_classes).\")\n    \n    result = np.eye(num_classes, dtype=int)[y]\n    return result\n```\n\n### Explanation\n1. **Input Conversion and Validation**: The input `y` is converted to a NumPy array and checked to ensure it is 1-D. If not, a `ValueError` is raised.\n2. **Empty Input Handling**: If `y` is empty, the function checks if `num_classes` is provided. If `num_classes` is provided, it returns an empty array with the appropriate number of columns. If `num_classes` is not provided, it raises a `ValueError` since the number of classes cannot be inferred from an empty array.\n3. **Number of Classes Calculation**: If `num_classes` is not provided, it is calculated as `max(y) + 1`. If `num_classes` is provided, the function checks that all labels in `y` are within the valid range.\n4. **One-Hot Encoding**: Using NumPy's `eye` function, an identity matrix of size `num_classes` is created. Each label in `y` is used to index into this matrix, resulting in a 2-D array where each row is the one-hot encoded version of the corresponding label.\n\nThis approach efficiently handles the conversion using NumPy operations, ensuring the result is a 2-D array of integers with the correct one-hot encoding."}
{"task_id": 485, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a univariate regression tree using the CART algorithm. The goal is to build a binary tree by recursively splitting on a single feature to minimize the sum of squared errors (SSE) and then use this tree to make predictions on test data.\n\n### Approach\n1. **Tree Construction**: The tree is built recursively. At each node, we consider splitting the data based on the feature values. The split is chosen to minimize the SSE of the resulting subsets.\n2. **Recursive Splitting**: For each node, if the current depth is less than the maximum allowed depth and the node contains at least two samples, we compute the best split point. The best split is determined by trying all possible split points and selecting the one that results in the smallest SSE.\n3. **Leaf Nodes**: If the current depth is equal to the maximum depth or the node contains fewer than two samples, the node is labeled as a leaf with the mean of the target values in that node.\n4. **Prediction**: For each test point, we traverse the tree from the root to a leaf node, using the feature value to decide the path at each internal node. The prediction is the mean value stored in the leaf node.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    if not X_train:\n        return []\n    \n    def build_tree(X, y, current_depth, max_depth):\n        if current_depth == max_depth or len(X) < 2:\n            return {'is_leaf': True, 'mean': np.mean(y)}\n        \n        current_mean = np.mean(y)\n        current_sse = sum((yi - current_mean)**2 for yi in y)\n        \n        sorted_data = sorted(zip(X, y), key=lambda x: x[0])\n        X_sorted = [x for x, _ in sorted_data]\n        y_sorted = [y for _, y in sorted_data]\n        \n        min_split_sse = float('inf')\n        best_split = None\n        best_left_X = []\n        best_left_y = []\n        best_right_X = []\n        best_right_y = []\n        \n        for i in range(len(X_sorted) - 1):\n            left_X = X_sorted[:i+1]\n            left_y = y_sorted[:i+1]\n            right_X = X_sorted[i+1:]\n            right_y = y_sorted[i+1:]\n            \n            mean_left = np.mean(left_y)\n            sse_left = sum((y - mean_left)**2 for y in left_y)\n            \n            mean_right = np.mean(right_y)\n            sse_right = sum((y - mean_right)**2 for y in right_y)\n            \n            total_sse = sse_left + sse_right\n            \n            if total_sse < min_split_sse:\n                min_split_sse = total_sse\n                best_split = X_sorted[i]\n                best_left_X = left_X\n                best_left_y = left_y\n                best_right_X = right_X\n                best_right_y = right_y\n        \n        if min_split_sse < current_sse:\n            left_child = build_tree(best_left_X, best_left_y, current_depth + 1, max_depth)\n            right_child = build_tree(best_right_X, best_right_y, current_depth + 1, max_depth)\n            return {\n                'is_leaf': False,\n                'split_value': best_split,\n                'left': left_child,\n                'right': right_child\n            }\n        else:\n            return {\n                'is_leaf': True,\n                'mean': current_mean\n            }\n    \n    tree = build_tree(X_train, y_train, 0, max_depth)\n    \n    predictions = []\n    for x in X_test:\n        current_node = tree\n        while not current_node['is_leaf']:\n            if x <= current_node['split_value']:\n                current_node = current_node['left']\n            else:\n                current_node = current_node['right']\n        predictions.append(current_node['mean'])\n    \n    return predictions\n```\n\n### Explanation\n1. **Tree Construction**: The `build_tree` function recursively constructs the tree. It starts by checking if the current node should be a leaf (based on depth or sample size). If not, it computes the best split point by evaluating all possible splits and selecting the one that minimizes the SSE.\n2. **Prediction**: The `regression_tree_predict` function first checks if the training set is empty. If not, it builds the tree and then uses it to predict the target values for each test point by traversing the tree from the root to a leaf node.\n\nThis approach ensures that the tree is built optimally to minimize the SSE at each split, and predictions are made efficiently by traversing the tree."}
{"task_id": 490, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional vector into a square diagonal matrix. The elements of the vector will be placed on the principal diagonal of the matrix, and all off-diagonal positions will be filled with zeros. The function should handle inputs that are lists, tuples, or NumPy arrays and return the resulting matrix as a list of lists.\n\n### Approach\n1. **Convert Input to List**: The input can be a list, tuple, or NumPy array. We first convert this input into a list of numeric values using NumPy's `tolist()` method.\n2. **Check for Empty Input**: If the input vector is empty, return an empty list immediately.\n3. **Determine Element Types**: Check if all elements in the input list are integers. This helps in determining whether the off-diagonal elements should be integers or floats.\n4. **Initialize Matrix**: Create an empty matrix. For each element in the input list, create a row where the element is placed on the diagonal and all other positions are zeros. The type of zeros (int or float) is determined based on the input elements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    x_list = np.asarray(x).tolist()\n    n = len(x_list)\n    if n == 0:\n        return []\n    \n    # Determine if all elements are integers\n    all_int = True\n    for e in x_list:\n        if not isinstance(e, int):\n            all_int = False\n            break\n    \n    # Initialize row with appropriate type\n    if all_int:\n        row = [0] * n\n    else:\n        row = [0.0] * n\n    \n    matrix = []\n    for i in range(n):\n        current_row = row.copy()\n        current_row[i] = x_list[i]\n        matrix.append(current_row)\n    \n    return matrix\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a list using `np.asarray(x).tolist()`, which handles lists, tuples, and NumPy arrays uniformly.\n2. **Empty Check**: If the input list is empty, the function returns an empty list immediately.\n3. **Type Check**: The function checks if all elements in the input list are integers. This determines whether the off-diagonal elements should be integers or floats.\n4. **Matrix Construction**: For each element in the input list, a new row is created where the element is placed on the diagonal, and the rest of the positions are filled with zeros. The type of zeros is chosen based on the input elements to ensure consistency.\n\nThis approach ensures that the resulting matrix is correctly formed with the input elements on the diagonal and zeros elsewhere, maintaining the correct data types for both diagonal and off-diagonal elements."}
{"task_id": 491, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the FP-Growth algorithm to mine all frequent item-sets from a collection of transactions. The algorithm consists of two main stages: constructing the FP-tree and recursively mining the tree to find frequent item-sets.\n\n### Approach\n1. **FP-Tree Construction**:\n   - **Count Frequencies**: First, we count the frequency of each item across all transactions.\n   - **Filter Items**: Remove items that do not meet the minimum support threshold.\n   - **Sort Transactions**: For each transaction, sort the items in descending order of their global frequency.\n   - **Build Tree**: Construct the FP-tree by inserting each item from the sorted transactions. Each node in the tree represents an item and maintains a count of how many times the path to that node appears.\n\n2. **Recursive Mining**:\n   - **Header Table Processing**: Use the header table to process each item. For each occurrence of an item in the header table, collect the path from the root to that node.\n   - **Conditional Pattern Base**: Build a conditional pattern base from these paths and construct a conditional FP-tree.\n   - **Generate Item-sets**: If the conditional tree has only one path, generate all non-empty subsets of that path. Otherwise, recursively mine the conditional tree.\n\n### Solution Code\n```python\nfrom collections import defaultdict\n\nclass FPNode:\n    def __init__(self, item, parent=None):\n        self.item = item\n        self.parent = parent\n        self.count = 0\n        self.children = defaultdict(FPNode)  # Key: item, Value: child node\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    # Step 1: Build the FP-tree\n    # Count the frequency of each item\n    item_counts = defaultdict(int)\n    for transaction in transactions:\n        for item in transaction:\n            item_counts[item] += 1\n\n    # Filter items that do not meet the minimum support\n    filtered_items = {item for item, count in item_counts.items() if count >= min_sup}\n\n    # Sort each transaction by the global frequency in descending order\n    sorted_transactions = []\n    for transaction in transactions:\n        # Filter items that are in filtered_items\n        filtered_transaction = [item for item in transaction if item in filtered_items]\n        # Sort by global frequency descending\n        sorted_transaction = sorted(filtered_transaction, key=lambda x: (-item_counts[x], x))\n        sorted_transactions.append(sorted_transaction)\n\n    # Build the FP-tree\n    root = FPNode(None)  # Root node has no item\n    header_table = defaultdict(list)  # Maps item to list of nodes\n\n    for transaction in sorted_transactions:\n        current_node = root\n        for item in transaction:\n            if item not in current_node.children:\n                # Create a new node\n                new_node = FPNode(item, parent=current_node)\n                current_node.children[item] = new_node\n                header_table[item].append(new_node)\n            # Move to the child node\n            current_node = current_node.children[item]\n            current_node.count += 1\n\n    # Step 2: Recursive mining\n    result = set()\n\n    def mine_tree(node, current_path):\n        # Check if current node has any children\n        if not node.children:\n            return\n        # Iterate through each child\n        for item, child in node.children.items():\n            new_path = current_path + [item]\n            if child.count >= min_sup:\n                # Add the new_path as a tuple\n                result.add(tuple(sorted(new_path)))\n            # If the child has children, recurse\n            if child.count >= min_sup or child.children:\n                mine_tree(child, new_path)\n\n    # Start mining from the root with an empty path\n    mine_tree(root, [])\n\n    # Convert the result to a sorted list of tuples\n    sorted_result = []\n    for item_set in result:\n        sorted_set = tuple(sorted(item_set))\n        sorted_result.append(sorted_set)\n    # Sort by length and then lex order\n    sorted_result.sort(key=lambda x: (len(x), x))\n    return sorted_result\n```\n\n### Explanation\n1. **FP-Tree Construction**:\n   - We first count the frequency of each item in the transactions. Items that do not meet the minimum support are filtered out.\n   - Each transaction is sorted by the frequency of its items in descending order.\n   - The FP-tree is built by inserting each item from the sorted transactions. Each node in the tree keeps track of the count of how many times the path to that node appears.\n\n2. **Recursive Mining**:\n   - Using the header table, we process each item. For each occurrence of an item, we collect the path from the root to that node.\n   - A conditional pattern base is built from these paths, and a conditional FP-tree is constructed.\n   - If the conditional tree has only one path, all non-empty subsets of that path are added to the result. Otherwise, the process is repeated recursively for each node in the tree.\n\nThis approach efficiently mines all frequent item-sets without generating all possible subsets, making it suitable for large datasets."}
{"task_id": 492, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a polynomial feature matrix from a given data matrix. The goal is to create a new matrix where each row contains all possible monomials of the original features with a total degree not exceeding a given degree. The monomials should be ordered first by their total degree and then lexicographically.\n\n### Approach\n1. **Check for Negative Degree**: If the given degree is negative, return -1 immediately.\n2. **Handle Empty Data**: If the input data matrix is empty, return an empty list.\n3. **Generate Exponent Tuples**: For each possible total degree from 0 to the given degree, generate all possible exponent tuples for the features such that the sum of the exponents equals the current total degree. These tuples are generated in lexicographic order.\n4. **Compute Monomials**: For each sample in the input data, compute the product of each feature raised to the corresponding exponent in each generated tuple. Collect these products to form the rows of the resulting matrix.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    if degree < 0:\n        return -1\n    if not X:\n        return []\n    \n    m = len(X[0])\n    \n    # Helper function to generate exponent tuples in lex order\n    def generate_exponents(m, t, current=None):\n        if current is None:\n            current = []\n        if len(current) == m - 1:\n            current.append(t)\n            yield tuple(current)\n            return\n        for k in range(t, -1, -1):\n            new_current = current + [k]\n            yield from generate_exponents(m, t - k, new_current)\n    \n    # Collect all exponent tuples in the required order\n    exponents = []\n    for t in range(degree + 1):\n        for exp in generate_exponents(m, t):\n            exponents.append(exp)\n    \n    # Compute the polynomial features for each sample\n    result = []\n    for sample in X:\n        row = []\n        for exp in exponents:\n            product = 1.0\n            for i in range(m):\n                product *= (sample[i] ** exp[i])\n            row.append(product)\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n1. **Negative Degree Check**: The function first checks if the degree is negative and returns -1 if true.\n2. **Empty Data Handling**: If the input data matrix is empty, the function returns an empty list.\n3. **Exponent Generation**: The helper function `generate_exponents` recursively generates all possible exponent tuples for each feature such that their sum equals the current total degree. These tuples are generated in lexicographic order.\n4. **Monomial Calculation**: For each sample, the function computes the product of each feature raised to the corresponding exponent in each generated tuple. These products are collected to form the rows of the resulting matrix.\n\nThis approach ensures that the resulting matrix contains all monomials up to the given degree, ordered correctly by their total degree and lexicographically."}
{"task_id": 493, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the mean-squared error (MSE) of a given 1-D numeric sample. The MSE is a measure of the average squared difference between each element in the sample and the mean of the sample. If the input sequence is empty, the function should return -1.\n\n### Approach\n1. **Check for Empty Input**: First, we check if the input sequence is empty. If it is, we return -1.\n2. **Convert to NumPy Array**: Convert the input sequence into a NumPy array to handle both lists and NumPy arrays uniformly.\n3. **Compute the Mean**: Calculate the mean of the elements in the array.\n4. **Compute Squared Errors**: For each element, compute the squared difference between the element and the mean.\n5. **Calculate MSE**: Compute the average of these squared errors to get the MSE.\n6. **Round the Result**: Round the computed MSE to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if not y:\n        return -1\n    y_np = np.array(y)\n    mean_y = np.mean(y_np)\n    squared_errors = (y_np - mean_y) ** 2\n    mse_value = np.mean(squared_errors)\n    return round(mse_value, 4)\n```\n\n### Explanation\n- **Empty Input Check**: The function first checks if the input is empty. If it is, the function returns -1 immediately.\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to facilitate vectorized operations, which are efficient and concise.\n- **Mean Calculation**: The mean of the array is computed using NumPy's `mean` function.\n- **Squared Errors Calculation**: Each element's difference from the mean is squared, and these squared differences are stored in an array.\n- **MSE Calculation**: The average of these squared differences is computed to get the MSE.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the problem's requirements.\n\nThis approach ensures that the function efficiently computes the MSE for both lists and NumPy arrays, handling edge cases such as empty inputs gracefully."}
{"task_id": 496, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies an affine (linear) activation function to an input vector, computes its first and second derivatives, and returns these values rounded to four decimal places. The function should handle both scalar and vector inputs gracefully.\n\n### Approach\n1. **Input Handling**: The function accepts a 1-D list or a NumPy array as input. If the input is a scalar, it is treated as a single-element vector.\n2. **Affine Activation Function**: The affine function is defined as \\( f(x) = \\text{slope} \\cdot x + \\text{intercept} \\). For each element in the input vector, this function is applied to compute the activation values.\n3. **First Derivative**: The first derivative of the affine function is a constant value equal to the slope.\n4. **Second Derivative**: The second derivative of the affine function is zero since the first derivative is a constant.\n5. **Rounding**: Each element of the resulting vectors (activation values, first derivatives, and second derivatives) is rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a NumPy array\n    x = np.asarray(x)\n    \n    # If x is a scalar, treat it as a 1-element array\n    if x.ndim == 0:\n        x = x.reshape(1)\n    \n    # Compute the affine activation\n    y = slope * x + intercept\n    \n    # Compute the first derivative (all elements are slope)\n    grad = np.full_like(x, slope)\n    \n    # Compute the second derivative (all elements are 0)\n    grad2 = np.zeros_like(x)\n    \n    # Convert to lists and round each element to 4 decimal places\n    y_list = [round(v, 4) for v in y.tolist()]\n    grad_list = [round(v, 4) for v in grad.tolist()]\n    grad2_list = [round(v, 4) for v in grad2.tolist()]\n    \n    return (y_list, grad_list, grad2_list)\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a NumPy array to handle both list and NumPy array inputs uniformly. If `x` is a scalar, it is reshaped into a 1-element array.\n2. **Activation Calculation**: The affine function is applied element-wise to compute the activation values.\n3. **Derivatives Calculation**: The first derivative is a constant array of the given slope, and the second derivative is an array of zeros.\n4. **Rounding**: Each element of the resulting arrays is converted to a list and rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function handles various input types correctly and efficiently computes the required values while adhering to the specified precision constraints."}
{"task_id": 499, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Principal Component Analysis (PCA) dimensionality reduction routine from scratch. The goal is to project the given data onto the first `k` principal components after standardizing the features, computing the covariance matrix, and performing eigen-decomposition.\n\n### Approach\n1. **Standardize Each Feature**: Each feature is standardized to have zero mean and unit population variance. If a feature has zero variance, it is left unchanged after centering.\n2. **Compute Covariance Matrix**: The sample covariance matrix of the standardized data is computed using Bessel\u2019s correction, which involves dividing by `n-1` where `n` is the number of samples.\n3. **Eigen-Decomposition**: The eigenvalues and eigenvectors of the covariance matrix are computed. The eigenvalues are sorted in descending order, and the corresponding eigenvectors are reordered accordingly.\n4. **Fix Eigenvector Signs**: Each eigenvector is adjusted so that the entry with the largest absolute value is positive to ensure deterministic output.\n5. **Project Data**: The standardized data is projected onto the first `k` eigenvectors.\n6. **Return Result**: The projected data is rounded to four decimal places and converted to a list of lists. If `k` is outside the valid range, return -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    n_samples, n_features = data.shape\n    if k < 1 or k > n_features:\n        return -1\n\n    # Standardize each feature\n    std_data = np.zeros_like(data)\n    for i in range(n_features):\n        col = data[:, i]\n        mean = np.mean(col)\n        centered = col - mean\n        var = np.var(centered, ddof=0)\n        if var == 0:\n            std_data[:, i] = centered\n        else:\n            std_data[:, i] = centered / np.sqrt(var)\n\n    # Compute covariance matrix\n    if n_samples == 1:\n        cov_matrix = np.zeros((n_features, n_features))\n    else:\n        cov_matrix = (std_data.T @ std_data) / (n_samples - 1)\n\n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Sort eigenvalues in descending order and adjust eigenvectors\n    eigenvalues = eigenvalues[::-1]\n    eigenvectors = eigenvectors[:, ::-1]\n\n    # Fix the sign of each eigenvector\n    for i in range(eigenvectors.shape[1]):\n        vec = eigenvectors[:, i]\n        max_abs_idx = np.argmax(np.abs(vec))\n        if vec[max_abs_idx] < 0:\n            eigenvectors[:, i] *= -1\n\n    # Select top k eigenvectors\n    selected_eigenvectors = eigenvectors[:, :k]\n\n    # Project the standardized data onto the selected eigenvectors\n    projection = std_data @ selected_eigenvectors\n\n    # Round to 4 decimal places and convert to list of lists\n    rounded = np.round(projection, 4).tolist()\n\n    return rounded\n```\n\n### Explanation\n1. **Standardization**: Each feature is centered and scaled to ensure zero mean and unit variance. This step is crucial for PCA as it makes the features comparable.\n2. **Covariance Matrix**: The covariance matrix captures the relationships between features. Using Bessel\u2019s correction ensures an unbiased estimate of the population covariance.\n3. **Eigen-Decomposition**: The eigenvalues and eigenvectors of the covariance matrix are computed. Eigenvalues represent the variance explained by each principal component, and eigenvectors are the directions of these components.\n4. **Sign Fixing**: Eigenvectors are adjusted to ensure consistency in their direction, which helps in making the output deterministic.\n5. **Projection**: The data is projected onto the principal components, reducing the dimensionality while retaining the most significant variance.\n\nThis approach ensures that the data is transformed into a lower-dimensional space while preserving the most important information, making it useful for various applications like visualization and machine learning."}
{"task_id": 500, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a tiny-scale Gradient Boosting binary classifier that uses decision stumps as weak learners and minimizes the squared error loss. The goal is to predict binary labels for a given test set based on an ensemble of decision stumps.\n\n### Approach\n1. **Initialization**: Compute the initial value `c` as the average of the training labels. This serves as the starting point for our model.\n2. **Iterative Training**: For each iteration, compute the residuals as the difference between the current predictions and the actual labels. Use these residuals to fit a decision stump that minimizes the squared error.\n3. **Decision Stump Fitting**: For each feature, consider all possible split points (including midpoints between unique values) to find the best split that minimizes the squared error. The optimal split is determined by computing the sum of squared errors for each possible split.\n4. **Update Model**: After finding the best split, update the model by adding the contribution of the decision stump, scaled by the learning rate.\n5. **Prediction**: For each test sample, compute the final score using the trained ensemble of decision stumps and threshold the result to get the predicted class labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the binary targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    # Compute initial average of y_train\n    c = np.mean(y_train)\n    num_samples = len(y_train)\n    F = np.array([c] * num_samples)\n    h_list = []\n\n    for _ in range(n_estimators):\n        r = y_train - F\n\n        best_SSE = float('inf')\n        best_j = -1\n        best_threshold = 0\n        best_left_mean = 0\n        best_right_mean = 0\n\n        for j in range(len(X_train[0])):\n            feature_values = [x[j] for x in X_train]\n            unique = sorted(list(set(feature_values)))\n\n            # Generate all possible split points including midpoints\n            split_points = unique.copy()\n            for i in range(len(unique) - 1):\n                split_points.append((unique[i] + unique[i + 1]) / 2)\n            split_points = sorted(list(set(split_points)))\n\n            for split_val in split_points:\n                left_r = []\n                right_r = []\n                for k in range(num_samples):\n                    if X_train[k][j] <= split_val:\n                        left_r.append(r[k])\n                    else:\n                        right_r.append(r[k])\n\n                # Compute means\n                if len(left_r) == 0:\n                    c_left = 0\n                else:\n                    c_left = np.mean(left_r)\n                if len(right_r) == 0:\n                    c_right = 0\n                else:\n                    c_right = np.mean(right_r)\n\n                # Compute SSE\n                sse = np.sum((np.array(left_r) - c_left) ** 2) + np.sum((np.array(right_r) - c_right) ** 2)\n\n                if sse < best_SSE:\n                    best_SSE = sse\n                    best_j = j\n                    best_threshold = split_val\n                    best_left_mean = c_left\n                    best_right_mean = c_right\n\n        # Update F with the best decision stump\n        h_pred = np.zeros(num_samples)\n        for k in range(num_samples):\n            if X_train[k][best_j] <= best_threshold:\n                h_pred[k] = best_left_mean\n            else:\n                h_pred[k] = best_right_mean\n        F += learning_rate * h_pred\n\n        h_list.append((best_j, best_threshold, best_left_mean, best_right_mean))\n\n    # Predict on X_test\n    y_pred = []\n    for x in X_test:\n        sum_h = 0\n        for (j, threshold, left_mean, right_mean) in h_list:\n            if x[j] <= threshold:\n                sum_h += learning_rate * left_mean\n            else:\n                sum_h += learning_rate * right_mean\n        F_x = c + sum_h\n        y_pred.append(1 if F_x >= 0.5 else 0)\n\n    return y_pred\n```\n\n### Explanation\n1. **Initialization**: The initial value `c` is computed as the mean of the training labels, which serves as the starting point for the model.\n2. **Iterative Training**: For each iteration, the residuals are computed as the difference between the current predictions and the actual labels. These residuals are used to fit a decision stump that minimizes the squared error.\n3. **Decision Stump Fitting**: For each feature, all possible split points (including midpoints between unique values) are considered. The best split is determined by minimizing the sum of squared errors.\n4. **Update Model**: The model is updated by adding the contribution of the best decision stump, scaled by the learning rate.\n5. **Prediction**: For each test sample, the final score is computed using the trained ensemble of decision stumps. The result is thresholded at 0.5 to get the predicted class labels.\n\nThis approach ensures that the model is built iteratively, with each decision stump improving the predictions based on the residuals from the previous iteration. The final predictions are made by combining the contributions of all decision stumps and thresholding the result."}
{"task_id": 505, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Contrastive Divergence (CD-k) algorithm for updating the weights of a binary Restricted Boltzmann Machine (RBM). The goal is to compute the weight update using the positive and negative gradients derived from the data and the model's distribution after k Gibbs sampling steps.\n\n### Approach\n1. **Compute Positive Phase Hidden Probabilities**: Calculate the hidden unit probabilities given the visible units using the current weights and biases.\n2. **Run Gibbs Sampling Steps**: Perform k Gibbs sampling steps to compute the visible and hidden probabilities iteratively without stochastic sampling.\n3. **Compute Gradients**: Calculate the positive gradient from the initial hidden probabilities and the negative gradient from the hidden probabilities after k Gibbs steps.\n4. **Update Weights**: Adjust the weights using the computed gradients, normalized by the batch size and learning rate.\n5. **Return Updated Weights**: Convert the updated weights to a list of lists, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    if X.size == 0:\n        return []\n    \n    m, n_v = X.shape\n    n_h = W.shape[1]\n    \n    # Compute initial hidden probabilities (h0_prob)\n    h0_prob = 1 / (1 + np.exp(- (W.dot(X) + hbias[:, np.newaxis])))\n    \n    # Initialize current visible and hidden probabilities\n    current_v = X.copy()\n    current_h = h0_prob.copy()\n    \n    for _ in range(k):\n        # Compute visible probabilities from current_h\n        v_prob = 1 / (1 + np.exp(- (current_h.dot(W.T) + vbias)))\n        # Compute next hidden probabilities from v_prob\n        current_h = 1 / (1 + np.exp(- (v_prob.dot(W) + hbias[:, np.newaxis])))\n        # Update current_v for the next iteration\n        current_v = v_prob\n    \n    # Calculate positive and negative gradients\n    pos_grad = X.T.dot(h0_prob)\n    neg_grad = current_v.T.dot(current_h)\n    \n    # Compute the weight update\n    delta = learning_rate * (pos_grad - neg_grad) / m\n    W_new = W + delta\n    \n    # Round to 4 decimal places and convert to list of lists\n    W_rounded = np.round(W_new, 4)\n    return W_rounded.tolist()\n```\n\n### Explanation\n1. **Positive Phase Calculation**: The hidden probabilities are computed using the sigmoid function applied to the linear combination of the visible units and weights, plus the hidden biases.\n2. **Gibbs Sampling**: For each of the k steps, the visible probabilities are computed from the current hidden probabilities, and then the next hidden probabilities are computed from these visible probabilities. This process is repeated iteratively.\n3. **Gradient Calculation**: The positive gradient is derived from the initial hidden probabilities, while the negative gradient is derived from the hidden probabilities after k Gibbs steps.\n4. **Weight Update**: The weights are updated using the difference between the positive and negative gradients, scaled by the learning rate and normalized by the batch size.\n5. **Result Conversion**: The updated weights are rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach ensures that the weights are updated efficiently using the CD-k algorithm, taking into account both the data distribution and the model's distribution after Gibbs sampling steps."}
{"task_id": 509, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Chebyshev distance between two real-valued vectors. The Chebyshev distance, also known as the maximum distance, is defined as the maximum absolute difference between corresponding elements of the two vectors. \n\n### Approach\n1. **Check Vector Lengths**: First, we verify if the two input vectors have the same length. If they do not, we return -1 as the distance.\n2. **Compute Absolute Differences**: For each corresponding pair of elements in the two vectors, compute the absolute difference.\n3. **Find Maximum Difference**: Track the maximum of these absolute differences.\n4. **Round the Result**: Finally, round the maximum difference to four decimal places and return it as a standard Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    max_diff = 0.0\n    for a, b in zip(x, y):\n        current_diff = abs(a - b)\n        if current_diff > max_diff:\n            max_diff = current_diff\n    return round(max_diff, 4)\n```\n\n### Explanation\n1. **Check Vector Lengths**: The function starts by checking if the lengths of the two input vectors are the same. If not, it returns -1.\n2. **Iterate Through Elements**: Using the `zip` function, the code iterates through each pair of corresponding elements from the two vectors.\n3. **Compute Differences**: For each pair, it computes the absolute difference and updates the maximum difference found so far.\n4. **Round Result**: After processing all elements, the maximum difference is rounded to four decimal places and returned as a float.\n\nThis approach ensures that the function efficiently computes the Chebyshev distance while handling both lists and NumPy arrays seamlessly."}
{"task_id": 510, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Variational Auto-Encoder (VAE) loss function for Bernoulli visible units. The loss function is composed of two parts: the reconstruction loss and the KL divergence. The goal is to compute the average of these two losses over a mini-batch and return the result rounded to six decimal places.\n\n### Approach\n1. **Reconstruction Loss**: This is computed using the binary cross-entropy (BCE) between the true input `y` and the reconstructed probabilities `y_pred`. To avoid numerical instability, we clip `y_pred` to ensure it lies within a small range around 0 and 1.\n2. **KL Divergence**: This measures the difference between the approximate posterior distribution and the unit Gaussian prior. The formula for KL divergence is derived from the properties of normal distributions and involves the mean and log-variance of the approximate posterior.\n3. **Summing and Averaging**: The total loss is the sum of the reconstruction loss and the KL divergence. This total is then averaged over all elements in both the input and latent spaces.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Compute Binary Cross-Entropy (BCE) loss\n    bce = -y * np.log(y_pred_clipped) - (1 - y) * np.log(1 - y_pred_clipped)\n    sum_bce = np.sum(bce)\n    \n    # Compute KL Divergence terms\n    kl_terms = -0.5 * (1 + t_log_var - t_mean**2 - np.exp(t_log_var))\n    sum_kl = np.sum(kl_terms)\n    \n    # Calculate total loss and average\n    total_loss = sum_bce + sum_kl\n    average_loss = total_loss / (y.size + t_mean.size)\n    \n    # Round to six decimal places\n    return round(average_loss, 6)\n```\n\n### Explanation\n1. **Reconstruction Loss Calculation**: The BCE loss is computed element-wise between `y` and `y_pred`. Each element of `y_pred` is clipped to avoid taking the logarithm of values too close to 0 or 1, which would be numerically unstable.\n2. **KL Divergence Calculation**: The KL divergence is computed using the mean and log-variance of the approximate posterior. The formula used ensures that we correctly measure the divergence from the unit Gaussian prior.\n3. **Summing and Averaging**: The total loss is the sum of the BCE and KL terms. This total is then divided by the sum of the sizes of the input and latent spaces to get the average loss per element, which is then rounded to six decimal places for the final result.\n\nThis approach ensures that we accurately compute the VAE loss while handling numerical stability and providing the result in the required format."}
{"task_id": 513, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Factorization Machine (FM) regression model to predict values for a given design matrix. The FM model combines linear regression with pairwise feature interactions, making it suitable for capturing complex relationships in the data.\n\n### Approach\nThe Factorization Machine prediction formula is given by:\n\n\\[ \\hat{y} = w_0 + \\sum_{j} w_j x_j + \\frac{1}{2} \\sum_{f=1}^{k} \\left[ \\left( \\sum_{j} V_{j,f} x_j \\right)^2 - \\sum_{j} V_{j,f}^2 x_j^2 \\right] \\]\n\nWhere:\n- \\( w_0 \\) is the bias term.\n- \\( w \\) is the vector of linear weights.\n- \\( V \\) is the matrix of latent factors.\n- \\( k \\) is the number of latent factors.\n\nThe approach involves the following steps:\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists into NumPy arrays for efficient numerical computations.\n2. **Compute Linear Part**: Calculate the linear part of the prediction using the dot product of the feature matrix \\( X \\) and the weight vector \\( w \\), then add the bias term \\( w_0 \\).\n3. **Compute Interaction Part**: For each latent factor, compute the pairwise interactions. This involves:\n   - Calculating the product of the feature matrix \\( X \\) and the latent factor matrix \\( V \\).\n   - Summing the squares of these products.\n   - Subtracting the element-wise squared products of the latent factors and features.\n4. **Combine Results**: Sum the linear and interaction parts, then round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n              w0: float,\n              w: list[float],\n              V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X = np.array(X)\n    w = np.array(w)\n    V = np.array(V)\n    \n    # Compute the linear part: w0 + X.dot(w)\n    linear_part = X.dot(w) + w0\n    \n    # Compute the interaction part\n    # First, compute X * V\n    XV = X.dot(V)\n    # Sum of squares of each row in XV\n    sum_squares = np.sum(XV ** 2, axis=1)\n    \n    # Compute V squared element-wise\n    V_squared = V ** 2\n    # Compute X * V_squared\n    XV_squared = X.dot(V_squared)\n    # Sum of each row in XV_squared\n    sum_t = np.sum(XV_squared, axis=1)\n    \n    # Interaction part is 0.5 * (sum_squares - sum_t)\n    interaction_part = 0.5 * (sum_squares - sum_t)\n    \n    # Total prediction\n    predictions = linear_part + interaction_part\n    \n    # Round to 4 decimal places\n    predictions = np.round(predictions, 4)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input lists into NumPy arrays to leverage efficient matrix operations.\n2. **Linear Part Calculation**: The linear part of the prediction is computed using the dot product of the feature matrix \\( X \\) and the weight vector \\( w \\), followed by the addition of the bias term \\( w_0 \\).\n3. **Interaction Part Calculation**: The interaction part involves two main steps:\n   - Compute the product of \\( X \\) and \\( V \\) to get intermediate results.\n   - Sum the squares of these products and subtract the element-wise squared products of \\( V \\) and \\( X \\).\n4. **Combining Results**: The linear and interaction parts are combined, and the result is rounded to four decimal places to meet the problem requirements.\n\nThis approach efficiently handles the computation using NumPy, ensuring that the solution is both concise and performant."}
{"task_id": 517, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation function for a given input. The sigmoid function is widely used in neural networks and is defined as:\n\n\\[ g(z) = \\frac{1}{1 + e^{-z}} \\]\n\nThe function must handle various input types, including scalars, lists, tuples, and NumPy arrays, and return the result in the same structure as the input. Additionally, the function must be numerically stable to avoid overflow or underflow issues for very large values of \\( z \\).\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (int, float, or NumPy number). If it is, the input is converted into a NumPy array for uniform processing. If the input is a list, tuple, or NumPy array, it is directly converted into a NumPy array.\n2. **Numerical Stability**: To avoid overflow or underflow issues, the function computes the sigmoid value differently based on whether \\( z \\) is positive or negative. For positive \\( z \\), it uses \\( \\frac{1}{1 + e^{-z}} \\). For negative \\( z \\), it uses \\( \\frac{e^z}{1 + e^z} \\).\n3. **Element-wise Computation**: The function processes each element of the input array individually, applying the appropriate computation based on the sign of each element.\n4. **Rounding**: The computed sigmoid values are rounded to four decimal places.\n5. **Output Structure**: The result is returned in the same structure as the input. If the input was a scalar, the result is returned as a float. Otherwise, the result is returned as a list obtained using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Check if z is a scalar\n    if isinstance(z, (int, float, np.number)):\n        is_scalar = True\n        z_array = np.array(z)\n    else:\n        is_scalar = False\n        z_array = np.array(z)\n    \n    # Compute the mask where z >= 0\n    mask = z_array >= 0\n    \n    # Initialize the result array\n    result = np.empty_like(z_array)\n    \n    # Compute the positive part\n    result[mask] = 1 / (1 + np.exp(-z_array[mask]))\n    \n    # Compute the negative part\n    result[~mask] = np.exp(z_array[~mask]) / (1 + np.exp(z_array[~mask]))\n    \n    # Round to four decimal places\n    rounded_result = np.round(result, 4)\n    \n    # Return the appropriate structure\n    if is_scalar:\n        return rounded_result.item()\n    else:\n        return rounded_result.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a scalar and converts it into a NumPy array for uniform processing. This ensures that the function can handle various input types seamlessly.\n- **Numerical Stability**: By using different computations for positive and negative values of \\( z \\), the function avoids numerical issues such as overflow or underflow, ensuring accurate results even for very large values of \\( z \\).\n- **Element-wise Computation**: The function processes each element of the input array individually, applying the appropriate computation based on the sign of each element. This ensures that the function works correctly for arrays of any shape.\n- **Rounding**: The computed sigmoid values are rounded to four decimal places to meet the problem's requirements.\n- **Output Structure**: The result is returned in the same structure as the input, ensuring compatibility with the input type whether it is a scalar, list, tuple, or NumPy array."}
{"task_id": 518, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a decorator called `unhot` that converts one-hot encoded NumPy arrays into their corresponding integer label representations before a metric function is called. This conversion is necessary because most machine learning metric functions expect labels in a one-dimensional integer array format.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Decorator Function**: The decorator function `unhot` takes a metric function as an argument and returns a wrapper function.\n2. **Wrapper Function**: Inside the wrapper function, we check if the `actual` and `predicted` arrays are two-dimensional and if their second dimension is greater than one. If so, we treat them as one-hot encoded arrays and convert them to 1-D arrays using `argmax` along the second axis.\n3. **Conversion Check**: If the arrays are already one-dimensional or have a shape of (n, 1), they are left unchanged.\n4. **Function Call**: After processing the arrays, the original metric function is called with the converted arrays, and its result is returned as is.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> Callable:\n        # Process the actual array if it's a 2D one-hot encoding\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        # Process the predicted array if it's a 2D one-hot encoding\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        # Call the original function with the processed arrays\n        return function(actual, predicted)\n    return wrapper\n```\n\n### Explanation\n- **Decorator Function**: The `unhot` function is a decorator that takes a metric function as input and returns a wrapper function.\n- **Wrapper Function**: The wrapper function processes the `actual` and `predicted` arrays. It checks if these arrays are two-dimensional and if their second dimension is greater than one. If so, it converts them from one-hot encoded format to 1-D integer labels using `argmax`.\n- **Conversion Check**: The arrays are only converted if they meet the criteria of being two-dimensional with a second dimension greater than one. Otherwise, they are left unchanged.\n- **Function Call**: The processed arrays are then passed to the original metric function, and the result is returned as is.\n\nThis approach ensures that the metric function receives the labels in the expected format, allowing it to compute the desired metrics correctly."}
{"task_id": 520, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the gradient of the sigmoid function given its output. The sigmoid function is commonly used in neural networks, and its derivative is essential for backpropagation. The function should handle various types of inputs, including scalars, lists, and NumPy arrays, and return the result in the same structure as the input.\n\n### Approach\n1. **Problem Analysis**: The sigmoid function is defined as \\( g(z) = \\frac{1}{1 + e^{-z}} \\). Its derivative is \\( g'(z) = g(z) \\cdot (1 - g(z)) \\). Given the output of the sigmoid function, we need to compute this derivative element-wise.\n2. **Input Handling**: The function should accept scalars, lists (including nested lists), or NumPy arrays. The output must match the input structure.\n3. **Recursive Processing**: For lists, each element is processed recursively to maintain the same structure. For NumPy arrays, element-wise operations are used to compute the gradient efficiently.\n4. **Rounding**: Each computed gradient value is rounded to four decimal places using NumPy's `round` function to ensure precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        gradient = z * (1 - z)\n        return np.round(gradient, 4)\n    elif isinstance(z, list):\n        return [sigmoid_grad(elem) for elem in z]\n    elif isinstance(z, np.ndarray):\n        gradient = z * (1 - z)\n        gradient = np.round(gradient, 4)\n        return gradient\n    else:\n        raise TypeError(\"Input must be scalar, list, or NumPy array.\")\n```\n\n### Explanation\n- **Scalar Input**: If the input is a scalar, the function computes the gradient using the formula \\( z \\cdot (1 - z) \\) and returns the result rounded to four decimal places.\n- **List Input**: For lists, each element is processed recursively. This ensures that nested lists are handled correctly, maintaining the same structure in the output.\n- **NumPy Array Input**: For NumPy arrays, the gradient is computed element-wise and then rounded. The result is returned as a NumPy array to preserve the input structure.\n- **Empty List Handling**: If the input is an empty list, the function returns an empty list, maintaining the structure.\n\nThis approach ensures that the function is versatile and can handle various input types while returning the correct gradient values in the same structure as the input."}
{"task_id": 528, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a grid of predictions for a simple linear classifier in two dimensions. The classifier is defined by a weight vector and a bias term, and it classifies points based on a sign function. The goal is to create a grid that spans the bounding box of the given data and classify each point in this grid.\n\n### Approach\n1. **Extract Min and Max Values**: Determine the minimum and maximum values for both features (x1 and x2) from the given data set. This helps in defining the range for the grid.\n2. **Create Grid Arrays**: Generate two 1D arrays, `x1_plot` and `x2_plot`, each of length `grid_n`, which are equally spaced between the minimum and maximum values of the respective features.\n3. **Generate Mesh Grid**: Use the two arrays to create a mesh grid, which will be used to evaluate the classifier at each point.\n4. **Compute Linear Combination**: For each point in the mesh grid, compute the linear combination using the given weights and bias.\n5. **Classify Points**: Apply the sign function to the computed values to classify each point into either 1 or -1.\n6. **Convert to List**: Convert the resulting numpy array of classifications into a Python list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Extract x1 and x2 values from the data\n    x1_values = [row[0] for row in X]\n    x2_values = [row[1] for row in X]\n    \n    # Determine the min and max for each feature\n    x1_min, x1_max = min(x1_values), max(x1_values)\n    x2_min, x2_max = min(x2_values), max(x2_values)\n    \n    # Create the grid points\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Generate meshgrid\n    X_mesh, Y_mesh = np.meshgrid(x1_plot, x2_plot)\n    \n    # Compute the linear combination for each point\n    z = W[0] * X_mesh + W[1] * Y_mesh + b\n    \n    # Classify each point\n    predictions = np.where(z >= 0, 1, -1)\n    \n    # Convert to Python list of lists\n    return predictions.tolist()\n```\n\n### Explanation\n1. **Extract Min and Max Values**: The minimum and maximum values for each feature are extracted from the data set to determine the range of the grid.\n2. **Create Grid Arrays**: Using `numpy.linspace`, we create arrays that span from the minimum to the maximum value for each feature, with `grid_n` points.\n3. **Generate Mesh Grid**: The mesh grid is created using `numpy.meshgrid`, which generates all combinations of the grid points.\n4. **Compute Linear Combination**: For each point in the mesh grid, the linear combination of the features, weights, and bias is computed.\n5. **Classify Points**: Using `numpy.where`, we classify each point based on whether the computed value is non-negative (1) or negative (-1).\n6. **Convert to List**: The resulting numpy array is converted to a Python list of lists to meet the output requirements.\n\nThis approach efficiently generates the grid and classifies each point, providing a clear visualization of the decision boundary for the linear classifier."}
{"task_id": 537, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Gaussian Naive Bayes classifier from scratch. The classifier will learn the parameters (mean and standard deviation) of each feature for every class using the training data, compute class priors, and then predict the class for each test sample based on the computed probabilities.\n\n### Approach\n1. **Separate Training Data by Class**: Split the training data into two classes based on the labels (0 and 1).\n2. **Compute Parameters**: For each class, compute the mean and standard deviation for each feature. Add a small constant to the standard deviation to avoid division by zero.\n3. **Compute Class Priors**: Calculate the relative frequency of each class in the training set.\n4. **Predict Class for Test Samples**: For each test sample, compute the log posterior probabilities for both classes using the Gaussian likelihood model. Compare these probabilities to determine the predicted class.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    # Separate training data into classes\n    mask0 = (y_train == 0)\n    mask1 = (y_train == 1)\n    X0 = X_train[mask0]\n    X1 = X_train[mask1]\n    \n    n_features = X_train.shape[1]\n    mu0 = np.zeros(n_features)\n    sigma0 = np.zeros(n_features)\n    for j in range(n_features):\n        if X0.size == 0:\n            mu0[j] = 0.0\n            sigma0[j] = 1e-9\n        else:\n            mu0[j] = X0[:, j].mean()\n            sigma0[j] = X0[:, j].std(ddof=0) + 1e-9\n    \n    mu1 = np.zeros(n_features)\n    sigma1 = np.zeros(n_features)\n    for j in range(n_features):\n        if X1.size == 0:\n            mu1[j] = 0.0\n            sigma1[j] = 1e-9\n        else:\n            mu1[j] = X1[:, j].mean()\n            sigma1[j] = X1[:, j].std(ddof=0) + 1e-9\n    \n    # Compute class priors\n    n0 = X0.shape[0]\n    n1 = X1.shape[0]\n    total = n0 + n1\n    prior0 = n0 / total if total != 0 else 0.0\n    prior1 = n1 / total if total != 0 else 0.0\n    \n    predictions = []\n    for x in X_test:\n        # Compute log posterior for class 0\n        if prior0 == 0:\n            log_p0 = -np.inf\n        else:\n            log_p0 = 0.0\n            for j in range(n_features):\n                xj = x[j]\n                mu = mu0[j]\n                sigma = sigma0[j]\n                term = (-0.5 * np.log(2 * np.pi)) - np.log(sigma) - ((xj - mu) ** 2) / (2 * sigma ** 2)\n                log_p0 += term\n            log_p0 += np.log(prior0)\n        \n        # Compute log posterior for class 1\n        if prior1 == 0:\n            log_p1 = -np.inf\n        else:\n            log_p1 = 0.0\n            for j in range(n_features):\n                xj = x[j]\n                mu = mu1[j]\n                sigma = sigma1[j]\n                term = (-0.5 * np.log(2 * np.pi)) - np.log(sigma) - ((xj - mu) ** 2) / (2 * sigma ** 2)\n                log_p1 += term\n            log_p1 += np.log(prior1)\n        \n        # Compare and predict\n        if log_p1 > log_p0:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    return predictions\n```\n\n### Explanation\n1. **Separate Training Data**: The training data is split into two classes based on the labels. This helps in computing the mean and standard deviation for each feature separately for each class.\n2. **Compute Parameters**: For each feature, the mean and standard deviation are computed for both classes. A small constant is added to the standard deviation to avoid division by zero.\n3. **Class Priors**: The prior probability of each class is calculated as the ratio of the number of samples in that class to the total number of samples.\n4. **Log Posterior Probabilities**: For each test sample, the log posterior probabilities for both classes are computed using the Gaussian likelihood model. The class with the higher log posterior probability is predicted as the result.\n\nThis approach ensures that we handle edge cases where the standard deviation might be zero and avoids numerical underflow by using log probabilities."}
{"task_id": 539, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost for a neural network model. This cost function is used to measure the performance of a binary classification model. The formula for binary cross-entropy cost is given, and we need to implement it efficiently while handling numerical stability issues.\n\n### Approach\n1. **Clipping Probabilities**: To avoid numerical overflow or underflow when taking logarithms, we clip the probabilities to a small range around 0 and 1. Specifically, we clip the values to the range [1e-15, 1 - 1e-15].\n2. **Logarithm Calculation**: After clipping, we compute the logarithm of the clipped probabilities and their complements.\n3. **Cost Calculation**: Using the given formula, we compute the cost by summing the terms for each example and then taking the average over all examples.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    m = Y.size\n    a_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    log_a = np.log(a_clipped)\n    log_1a = np.log(1 - a_clipped)\n    term = Y * log_a + (1 - Y) * log_1a\n    sum_terms = np.sum(term)\n    cost = (-1 / m) * sum_terms\n    return float(cost)\n```\n\n### Explanation\n1. **Clipping Probabilities**: The probabilities in `AL` are clipped to ensure they lie within a safe range for logarithm operations. This prevents issues like taking the logarithm of zero or one, which would be undefined or cause overflow.\n2. **Logarithm Calculation**: We compute the natural logarithm of the clipped probabilities and their complements (1 - clipped probabilities).\n3. **Term Calculation**: For each element, we compute the term using the formula `y * ln(a) + (1 - y) * ln(1 - a)`, where `y` is the true label and `a` is the predicted probability.\n4. **Sum and Average**: We sum all the terms and then compute the average by dividing by the number of examples `m`, applying the negative sign as per the formula.\n\nThis approach ensures numerical stability and correctly computes the binary cross-entropy cost, which is essential for training and evaluating binary classification models."}
{"task_id": 552, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the posterior state distribution \u03b3\u209c for a given Hidden Markov Model (HMM) at a specific time step t. The function should use the forward-backward algorithm to compute the probabilities and return the result rounded to four decimal places.\n\n### Approach\n1. **Check Validity of Time Step**: If the given time step t is outside the valid range [0, len(observations) - 1], return an empty list.\n2. **Extract HMM Parameters**: Extract the initial distribution (\u03c0), transition matrix (A), and emission matrix (B) from the given HMM dictionary.\n3. **Compute Forward Probabilities (\u03b1)**: The forward probability \u03b1\u209c(i) represents the probability of being in state i at time t given the observations up to time t. This is computed using dynamic programming starting from the initial distribution and moving forward through the observations.\n4. **Compute Backward Probabilities (\u03b2)**: The backward probability \u03b2\u209c(i) represents the probability of the remaining observations from time t+1 onwards given that we are in state i at time t. This is computed starting from the end of the observation sequence and moving backward.\n5. **Compute Posterior Probabilities (\u03b3)**: Using the forward and backward probabilities, compute the posterior probabilities \u03b3\u209c(i) for each state i at time t. This is done by normalizing the product of \u03b1\u209c(i) and \u03b2\u209c(i) by the sum of such products over all states.\n6. **Round the Results**: Round each component of the resulting vector \u03b3\u209c to four decimal places and return it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm to compute the\n    posterior probabilities and returns the result rounded to four decimal\n    places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A', and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n    \n    pi = np.array(hmm['pi'])\n    A = np.array(hmm['A'])\n    B = np.array(hmm['B'])\n    \n    N = len(pi)\n    T = len(observations)\n    \n    # Initialize alpha\n    alpha = np.zeros((T, N))\n    alpha[0] = pi * B[:, observations[0]]\n    \n    # Compute alpha for t >= 1\n    for t_step in range(1, T):\n        o = observations[t_step]\n        alpha[t_step] = (alpha[t_step-1] @ A) * B[:, o]\n    \n    # Initialize beta\n    beta = np.zeros((T, N))\n    beta[-1] = np.ones(N)\n    \n    # Compute beta for t < T-1\n    for t_step in range(T-2, -1, -1):\n        o = observations[t_step + 1]\n        beta[t_step] = (A.T * B[:, o]) @ beta[t_step + 1]\n    \n    # Compute gamma for the specific t\n    gamma = alpha[t] * beta[t]\n    denominator = np.sum(gamma)\n    if denominator == 0:\n        return [0.0 for _ in range(N)]\n    gamma /= denominator\n    \n    # Round to four decimal places\n    rounded_gamma = [round(g, 4) for g in gamma]\n    \n    return rounded_gamma\n```\n\n### Explanation\n1. **Forward Probabilities (\u03b1)**: These are computed starting from the initial distribution and moving forward through each observation. For each time step, the probability of being in each state is updated based on the previous probabilities and the transition and emission matrices.\n2. **Backward Probabilities (\u03b2)**: These are computed starting from the end of the observation sequence and moving backward. For each time step, the probability of the remaining observations is updated based on the transition and emission matrices.\n3. **Posterior Probabilities (\u03b3)**: These are computed by combining the forward and backward probabilities. The result is normalized to ensure it sums to 1, giving the posterior distribution of states at time t.\n\nThis approach efficiently computes the required probabilities using dynamic programming, ensuring that the solution is both optimal and scalable for larger inputs."}
{"task_id": 555, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. DBSCAN is a density-based clustering algorithm that groups points that are densely packed together and marks points in low-density regions as noise. The algorithm identifies core points, which are points with a sufficient number of nearby neighbors within a specified radius, and then expands clusters from these core points.\n\n### Approach\n1. **Compute Pairwise Distances**: For each point, compute the Euclidean distance to every other point to determine which points lie within a specified radius (epsilon).\n2. **Build Neighborhood List**: For each point, create a list of all points that are within the specified radius.\n3. **Identify Core Points**: A point is considered a core point if it has at least a specified minimum number of points within its neighborhood, including itself.\n4. **Expand Clusters Using BFS**: Starting with the first unlabeled core point, use a breadth-first search (BFS) to expand the cluster. Each point in the queue is processed to label all reachable points, including both core points and non-core points.\n5. **Label Noise Points**: After processing all core points, any remaining unlabeled points are marked as noise.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n = data.shape[0]\n    if n == 0:\n        return []\n    \n    # Compute the neighborhood for each point\n    neighborhood = [[] for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            distance = np.linalg.norm(data[i] - data[j])\n            if distance <= eps:\n                neighborhood[i].append(j)\n    \n    # Determine core points\n    core_points = [len(neighbors) >= min_samples for neighbors in neighborhood]\n    \n    # Initialize labels\n    labels = [0] * n\n    cluster_id = 1\n    \n    # Process each point to find clusters\n    for i in range(n):\n        if core_points[i] and labels[i] == 0:\n            # Start a new cluster\n            queue = deque()\n            queue.append(i)\n            labels[i] = cluster_id\n            while queue:\n                current = queue.popleft()\n                for neighbor in neighborhood[current]:\n                    if labels[neighbor] == 0:\n                        labels[neighbor] = cluster_id\n                        if core_points[neighbor]:\n                            queue.append(neighbor)\n            cluster_id += 1\n    \n    # Assign -1 to noise points\n    for i in range(n):\n        if labels[i] == 0:\n            labels[i] = -1\n    \n    return labels\n```\n\n### Explanation\n1. **Compute Pairwise Distances**: For each point, we compute the distance to every other point and store the indices of points within the specified radius (epsilon) in a neighborhood list.\n2. **Identify Core Points**: Using the neighborhood list, we determine which points are core points by checking if the number of points in their neighborhood meets or exceeds the minimum required.\n3. **Expand Clusters**: Starting with the first unlabeled core point, we use BFS to explore all reachable points. Each point is labeled with the current cluster ID, and if it is a core point, it is added to the queue to continue expanding the cluster.\n4. **Label Noise Points**: After all core points have been processed, any points that remain unlabeled are marked as noise with a label of -1.\n\nThis approach ensures that we efficiently group points into clusters based on their density and correctly identify noise points in low-density regions."}
{"task_id": 561, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model (GMM). The goal is to cluster the given 2D data into K components and return the cluster labels for each sample. The solution must be deterministic and handle convergence based on a prescribed tolerance.\n\n### Approach\n1. **Initialization**: \n   - Randomly initialize the mixture weights using a Dirichlet distribution to ensure they sum to 1.\n   - Initialize the means by sampling from a uniform distribution within the range of each feature in the data.\n   - Initialize the covariance matrices as scaled identity matrices to ensure they are positive definite.\n\n2. **EM Algorithm**:\n   - **E-step**: Compute the responsibilities (posterior probabilities) of each sample belonging to each component using log probabilities to avoid underflow.\n   - **M-step**: Update the parameters (weights, means, and covariances) based on the computed responsibilities.\n   - Check for convergence by comparing the change in weights to a prescribed tolerance or reaching the maximum number of iterations.\n\n3. **Determine Cluster Labels**:\n   - After convergence, assign each sample to the component with the highest responsibility.\n   - Sort the components by the first coordinate of their means and re-label them to ensure deterministic output.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\nfrom scipy.stats import multivariate_normal\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Implements the Expectation-Maximization algorithm for a Gaussian Mixture Model.\"\"\"\n    np.random.seed(0)  # Ensure deterministic behavior\n    \n    if k == 1:\n        return [0] * len(data)\n    \n    N, D = data.shape\n    \n    # Initialize parameters\n    weights = np.random.dirichlet([1.0] * k)\n    means = np.zeros((k, D))\n    for j in range(k):\n        for d in range(D):\n            means[j, d] = np.random.uniform(data[:, d].min(), data[:, d].max())\n    covariances = np.array([np.eye(D) * 0.1 for _ in range(k)])\n    \n    old_weights = np.zeros(k)\n    old_means = np.zeros((k, D))\n    old_covariances = np.zeros((k, D, D))\n    \n    for iter in range(max_iter):\n        # E-step: Compute responsibilities\n        r = np.zeros((N, k))\n        for i in range(N):\n            x = data[i]\n            log_p = np.zeros(k)\n            for j in range(k):\n                log_p[j] = np.log(weights[j]) + multivariate_normal.logpdf(x, means[j], covariances[j])\n            sum_log_p = np.logaddexp.reduce(log_p)\n            r[i] = np.exp(log_p - sum_log_p)\n        \n        # M-step: Update parameters\n        new_weights = np.zeros(k)\n        new_means = np.zeros((k, D))\n        new_covariances = np.zeros((k, D, D))\n        \n        for j in range(k):\n            sum_rj = r[:, j].sum()\n            new_weights[j] = sum_rj / N\n            \n            x_centered = data - means[j]\n            x_centered_weighted = x_centered * r[:, j][:, np.newaxis]\n            sum_x = x_centered_weighted.sum(axis=0)\n            new_means[j] = sum_x / sum_rj\n            \n            covariance = (x_centered_weighted.T @ x_centered_weighted) / sum_rj\n            new_covariances[j] = covariance + 1e-6 * np.eye(D)  # Ensure positive definite\n        \n        # Check for convergence\n        if iter == 0:\n            old_weights = new_weights.copy()\n            old_means = new_means.copy()\n            old_covariances = new_covariances.copy()\n            continue\n        \n        max_change = np.max(np.abs(new_weights - old_weights))\n        if max_change < epsilon:\n            break\n        \n        # Update parameters\n        weights = new_weights\n        means = new_means\n        covariances = new_covariances\n        old_weights = new_weights.copy()\n        old_means = new_means.copy()\n        old_covariances = new_covariances.copy()\n    \n    # Assign labels based on responsibilities\n    labels = np.argmax(r, axis=1)\n    \n    # Sort components by the first coordinate of their mean\n    sorted_order = np.argsort(means[:, 0])\n    \n    # Create mapping from old component index to new label\n    mapping = {original_j: new_idx for new_idx, original_j in enumerate(sorted_order)}\n    \n    # Re-label the samples\n    new_labels = [mapping[label] for label in labels]\n    \n    return new_labels\n```\n\n### Explanation\n- **Initialization**: The parameters are initialized to ensure a good starting point for the EM algorithm. Weights are initialized using a Dirichlet distribution, means are sampled from the data's feature ranges, and covariances are set to small diagonal matrices.\n- **EM Algorithm**: The E-step computes the responsibilities using log probabilities to avoid numerical underflow. The M-step updates the parameters using the responsibilities, ensuring the covariance matrices remain positive definite.\n- **Convergence Check**: The algorithm stops when the change in weights is below the tolerance or the maximum iterations are reached.\n- **Determine Labels**: After convergence, the sample with the highest responsibility for each component is assigned to that component. The components are sorted by their means' first coordinate to ensure deterministic labeling.\n\nThis approach ensures that the function is deterministic and produces consistent results for identical inputs."}
{"task_id": 562, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Spectral Clustering algorithm from scratch. The goal is to cluster a given set of points into a specified number of clusters using the Gaussian kernel to build a similarity graph and spectral methods to find the optimal clusters.\n\n### Approach\nThe approach involves several key steps:\n\n1. **Build a Similarity Graph**: Using the Gaussian kernel, we compute a fully-connected similarity graph where each node represents a data point and edges are weighted by the Gaussian similarity function.\n\n2. **Construct the Laplacian Matrix**: Depending on the method specified ('unnormalized' or 'normalized'), we construct the Laplacian matrix. This matrix is crucial for capturing the structure of the graph.\n\n3. **Compute Eigenvectors**: We find the eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix. These eigenvectors form a feature matrix that is used for clustering.\n\n4. **Row-normalize Eigenvectors**: For the normalized method, we normalize the rows of the eigenvector matrix to ensure each row has a unit length.\n\n5. **K-means Clustering**: Using the feature matrix derived from the eigenvectors, we apply a deterministic K-means algorithm to form clusters. The initial centroids are chosen from the first few points in the feature space.\n\n6. **Relabel Clusters**: After obtaining the initial cluster labels, we relabel the clusters based on the smallest original index to ensure a consistent and ordered labeling.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    if n_cluster == 1:\n        return [0] * data.shape[0]\n    \n    N, d = data.shape\n    \n    # Step 1: Compute similarity matrix W using Gaussian kernel\n    distances = np.sum((data[:, np.newaxis, :] - data[np.newaxis, :, :]) ** 2, axis=2)\n    W = np.exp(-gamma * distances)\n    \n    # Step 2: Compute Laplacian matrix\n    D = np.diag(np.sum(W, axis=1))\n    \n    if method == 'unnormalized':\n        L = D - W\n    else:\n        # Compute D inverse square root for normalized Laplacian\n        D_inv = np.diag(1.0 / np.sum(W, axis=1))\n        D_inv_sqrt = np.sqrt(D_inv)\n        L = D_inv_sqrt @ (D - W) @ D_inv_sqrt\n    \n    # Step 3: Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Select the first n_cluster eigenvectors\n    features = eigenvectors[:, :n_cluster]\n    \n    # For normalized method, row-normalize the features\n    if method == 'normalized':\n        features = features / np.linalg.norm(features, axis=1, keepdims=True)\n    \n    # Step 4: Run k-means clustering\n    initial_centroids = features[:n_cluster, :]\n    \n    def kmeans(features, n_cluster, initial_centroids):\n        N = features.shape[0]\n        centroids = initial_centroids.copy()\n        prev_centroids = np.zeros_like(centroids)\n        labels = np.zeros(N, dtype=int)\n        while not np.allclose(centroids, prev_centroids, atol=1e-6):\n            prev_centroids = centroids.copy()\n            # Compute distances from each point to each centroid\n            distances = np.sum((features[:, np.newaxis, :] - centroids) ** 2, axis=2)\n            # Assign each point to the nearest centroid\n            labels = np.argmin(distances, axis=1)\n            # Compute new centroids\n            new_centroids = np.zeros_like(centroids)\n            for j in range(n_cluster):\n                points_in_cluster = features[labels == j]\n                if len(points_in_cluster) == 0:\n                    new_centroids[j] = centroids[j]\n                else:\n                    new_centroids[j] = np.mean(points_in_cluster, axis=0)\n            centroids = new_centroids\n        return labels\n    \n    labels = kmeans(features, n_cluster, initial_centroids)\n    \n    # Step 5: Relabel clusters based on the smallest original index\n    cluster_info = []\n    for cluster_label in range(n_cluster):\n        points_in_cluster = [i for i in range(N) if labels[i] == cluster_label]\n        min_index = min(points_in_cluster)\n        cluster_info.append((min_index, cluster_label))\n    \n    # Sort clusters by the minimum index\n    cluster_info.sort()\n    \n    # Create a mapping from old cluster labels to new labels\n    new_label_map = {cluster_label: rank for rank, (_, cluster_label) in enumerate(cluster_info)}\n    \n    # Assign new labels\n    new_labels = [new_label_map[label] for label in labels]\n    \n    return new_labels\n```\n\n### Explanation\n1. **Similarity Graph Construction**: The Gaussian kernel is used to compute the similarity between each pair of points, forming a fully-connected graph.\n\n2. **Laplacian Matrix**: The Laplacian matrix is constructed to capture the structure of the graph. For the normalized method, it is adjusted to ensure each row sums to 1.\n\n3. **Eigenvalue Decomposition**: The smallest eigenvalues and their corresponding eigenvectors are computed to form a feature matrix that reduces the dimensionality of the data.\n\n4. **K-means Clustering**: The feature matrix is used to perform K-means clustering, starting with initial centroids from the first few points in the feature space.\n\n5. **Cluster Relabeling**: The clusters are relabeled based on the smallest original index to ensure a consistent and ordered labeling, providing a clear and interpretable result."}
{"task_id": 563, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Dual-form Perceptron learning algorithm. This algorithm operates in the feature-space inner-product and updates coefficients for each training example instead of directly updating the weight vector. The goal is to find the optimal weight vector and bias term that can classify the given data.\n\n### Approach\n1. **Initialization**: Start with zero coefficients (alpha) for each training example and a zero bias term (b).\n2. **Gram Matrix Construction**: Compute the Gram matrix where each element represents the inner product of the feature vectors of the training examples. This matrix helps in efficiently computing the inner products needed for the updates.\n3. **Training Loop**: Iterate over the training data multiple times (up to a specified maximum number of iterations). For each sample, compute the inner product sum using the current coefficients and the Gram matrix. If the current sample is misclassified, update the coefficients and bias term.\n4. **Weight Vector Recovery**: After the training loop completes, compute the weight vector using the final coefficients and the training data.\n5. **Rounding**: Round the components of the weight vector and the bias term to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N = X_data.shape[0]\n    G = X_data.dot(X_data.T)\n    \n    alpha = np.zeros(N)\n    b = 0.0\n    \n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.dot(alpha * y_data, G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    \n    w = np.sum(alpha[:, np.newaxis] * y_data[:, np.newaxis] * X_data, axis=0)\n    w = w.round(4).tolist()\n    b = round(b, 4)\n    \n    return (w, b)\n```\n\n### Explanation\n1. **Initialization**: The coefficients (alpha) and bias (b) are initialized to zero.\n2. **Gram Matrix**: The Gram matrix is computed using matrix multiplication, which efficiently calculates all pairwise inner products of the feature vectors.\n3. **Training Loop**: For each sample, the inner product sum (g) is computed using the current coefficients and the Gram matrix. If the sample is misclassified, the coefficients and bias are updated.\n4. **Weight Vector**: After training, the weight vector is computed by summing the contributions from each training example, weighted by their respective coefficients.\n5. **Rounding**: The final weight vector and bias term are rounded to four decimal places to meet the output requirements.\n\nThis approach ensures that we efficiently train the perceptron using the dual formulation, leveraging the Gram matrix for inner product calculations and updating coefficients to find the optimal solution."}
{"task_id": 564, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a decision tree to predict the outcome for a given sample based on its feature values. The decision tree is represented by a class `Node`, which can be either a classification or regression tree. The traversal rules differ slightly between the two types, and we need to handle each case appropriately.\n\n### Approach\n1. **Check for Leaf Node**: If the current node is a leaf node (i.e., it has a `y` attribute), return the value of `y` as the prediction.\n2. **Classification Tree**:\n   - If the current node is not a leaf, inspect the feature value specified by the node's `label`.\n   - Traverse to the first child whose `x` attribute matches the inspected feature value.\n   - If no child matches, default to the second child in the list.\n3. **Regression Tree**:\n   - If the current node is not a leaf, inspect the feature value specified by the node's `label`.\n   - Compare the feature value with the node's `s` attribute. If the feature value is less than or equal to `s`, traverse to the first child; otherwise, traverse to the second child.\n\n### Solution Code\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    if hasattr(root, 'y'):\n        return root.y\n    if task_type == 'classification':\n        feature_index = root.label\n        feature_value = features[feature_index]\n        for child in root.child:\n            if child.x == feature_value:\n                return predict_sample(child, features, task_type)\n        # Default to the second child if no match is found\n        return predict_sample(root.child[1], features, task_type)\n    else:  # regression\n        feature_index = root.label\n        feature_value = features[feature_index]\n        if feature_value <= root.s:\n            child = root.child[0]\n        else:\n            child = root.child[1]\n        return predict_sample(child, features, task_type)\n```\n\n### Explanation\n- **Leaf Node Check**: The function first checks if the current node is a leaf by looking for the `y` attribute. If found, it returns `y` immediately.\n- **Classification Tree Handling**: For classification trees, the function inspects the feature value at the index specified by the node's `label`. It then searches through the node's children to find the first one whose `x` attribute matches the feature value. If no match is found, it defaults to the second child.\n- **Regression Tree Handling**: For regression trees, the function compares the feature value with the node's `s` attribute. Depending on whether the feature value is less than or equal to `s`, it traverses to the first or second child, respectively.\n\nThis approach ensures that the decision tree is traversed correctly according to the specified rules, leading to the correct prediction from the leaf node."}
{"task_id": 565, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that selects the feature from a given dataset that maximizes the information gain. This is a key step in the ID3 decision-tree learning algorithm. The function should return the index of the best feature and the maximum information gain rounded to six decimal places.\n\n### Approach\n1. **Compute the Entropy of Labels (H(y))**: The entropy of the label distribution is calculated using the formula for entropy, which measures the impurity or randomness in the data.\n2. **Iterate Over Each Feature**: For each feature, compute the conditional entropy (H(y|A)) after splitting the data by the feature's values.\n3. **Calculate Information Gain**: For each feature, the information gain (IG) is computed as the difference between the entropy of the labels (H(y)) and the conditional entropy (H(y|A)).\n4. **Track the Best Feature**: Keep track of the feature that provides the maximum information gain. In case of a tie, the feature with the smallest index is chosen.\n5. **Return the Result**: The function returns the index of the best feature and the maximum information gain rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    # Compute the entropy of the entire label set H(y)\n    y_counts = np.bincount(y)\n    total_y = len(y)\n    H_y = 0.0\n    for count in y_counts:\n        if count == 0:\n            continue\n        prob = count / total_y\n        H_y -= prob * math.log2(prob)\n    \n    n_features = X.shape[1]\n    best_feature = 0\n    max_gain = 0.0  # Initialize with the first feature's gain\n    \n    for feature_idx in range(n_features):\n        feature_values = X[:, feature_idx]\n        unique_values = np.unique(feature_values)\n        H_A = 0.0\n        \n        for value in unique_values:\n            mask = (feature_values == value)\n            subset_y = y[mask]\n            subset_counts = np.bincount(subset_y)\n            total_subset = len(subset_y)\n            \n            entropy_subset = 0.0\n            for cnt in subset_counts:\n                if cnt == 0:\n                    continue\n                prob_subset = cnt / total_subset\n                entropy_subset -= prob_subset * math.log2(prob_subset)\n            \n            count = mask.sum()\n            prob = count / total_y\n            H_A += prob * entropy_subset\n        \n        # Calculate information gain\n        IG = H_y - H_A\n        \n        # Update best_feature and max_gain if current feature is better\n        if IG > max_gain or (IG == max_gain and feature_idx < best_feature):\n            max_gain = IG\n            best_feature = feature_idx\n    \n    # Round the maximum gain to 6 decimal places\n    max_gain_rounded = round(max_gain, 6)\n    return (best_feature, max_gain_rounded)\n```\n\n### Explanation\n1. **Entropy Calculation**: The entropy of the labels is computed by counting the occurrences of each label, converting these counts into probabilities, and then applying the entropy formula.\n2. **Feature Processing**: For each feature, the data is split based on each unique value of the feature. The entropy of the labels within each split is computed and weighted by the size of the split. The sum of these weighted entropies gives the conditional entropy.\n3. **Information Gain**: The information gain for each feature is the difference between the entropy of the labels and the conditional entropy. The feature with the highest information gain is selected, with ties broken by choosing the smallest index.\n4. **Efficiency Considerations**: The approach efficiently handles each feature and its values, ensuring that the solution is both correct and performs well within typical problem constraints."}
{"task_id": 566, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a mini version of the Isolation Forest algorithm for anomaly detection. The goal is to identify outliers in a dataset using a collection of random isolation trees. Each tree is built on a subset of the data, and the anomaly score for each data point is determined based on the average path length across all trees.\n\n### Approach\n1. **Global Seed Setting**: Ensure reproducibility by setting a global seed for random number generation.\n2. **Tree Construction**: For each tree, build a random binary tree using a subset of the data. The tree is constructed by recursively splitting the subset until the depth reaches a specified limit or the subset size is reduced to one.\n3. **Path Length Calculation**: For each data point, compute the path length in each tree. The path length is adjusted using a correction factor when a leaf node is reached.\n4. **Anomaly Score Calculation**: Convert the average path length into an anomaly score using a given formula.\n5. **Outlier Detection**: Identify the top epsilon percent of data points with the highest anomaly scores as outliers.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)\n    n_samples, n_features = data.shape\n    if epsilon == 0:\n        return []\n    if n_samples == 0:\n        return []\n    \n    # Compute phi\n    if n_samples == 1:\n        if epsilon > 0:\n            return [0]\n        else:\n            return []\n    phi = 2 * math.log(n_samples - 1) - 2 * (n_samples - 1) / n_samples\n    \n    # Initialize average path lengths\n    avg_path = np.zeros(n_samples)\n    \n    def compute_c(n):\n        if n == 1:\n            return 0.0\n        else:\n            return 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n    \n    def compute_path_length(point, node):\n        path_length = 0\n        current_node = node\n        while not current_node['is_leaf']:\n            feature = current_node['feature']\n            threshold = current_node['threshold']\n            if point[feature] <= threshold:\n                current_node = current_node['left']\n            else:\n                current_node = current_node['right']\n            path_length += 1\n        # Apply correction\n        n = current_node['count']\n        c_n = compute_c(n)\n        path_length += c_n\n        return path_length\n    \n    def build_tree(subset, depth, height_limit):\n        if depth >= height_limit or len(subset) <= 1:\n            return {\n                'is_leaf': True,\n                'count': len(subset),\n                'left': None,\n                'right': None\n            }\n        # Choose a random feature\n        n_features = subset.shape[1]\n        feature = np.random.randint(n_features)\n        # Get min and max of the feature in subset\n        current_feature = subset[:, feature]\n        min_val = np.min(current_feature)\n        max_val = np.max(current_feature)\n        # Choose a random threshold\n        threshold = np.random.uniform(min_val, max_val)\n        # Split the subset\n        left_subset = subset[current_feature <= threshold]\n        right_subset = subset[current_feature > threshold]\n        # Build left and right children\n        left_child = build_tree(left_subset, depth + 1, height_limit)\n        right_child = build_tree(right_subset, depth + 1, height_limit)\n        return {\n            'is_leaf': False,\n            'count': len(subset),\n            'feature': feature,\n            'threshold': threshold,\n            'left': left_child,\n            'right': right_child\n        }\n    \n    for _ in range(n_trees):\n        # Select subset\n        if sample_size >= n_samples:\n            subset = data\n        else:\n            subset = data[np.random.choice(n_samples, sample_size, replace=False)]\n        # Compute height limit\n        height_limit = math.ceil(math.log2(sample_size)) if sample_size > 0 else 0\n        # Build the tree\n        root = build_tree(subset, 0, height_limit)\n        # For each point in data, compute path length\n        for i, point in enumerate(data):\n            path = compute_path_length(point, root)\n            avg_path[i] += path\n    \n    # Compute average path\n    avg_path /= n_trees\n    # Compute scores\n    scores = 2 ** (- avg_path / phi)\n    # Find the top epsilon * n_samples scores\n    n_outliers = int(epsilon * n_samples)\n    if n_outliers == 0:\n        return []\n    # Get the indices of the top n_outliers scores\n    indices = np.argsort(-scores, kind='mergesort')[:n_outliers]\n    # Sort the indices increasingly\n    indices.sort()\n    return indices\n```\n\n### Explanation\n1. **Global Seed Setting**: The function starts by setting a global seed to ensure reproducibility.\n2. **Tree Construction**: For each tree, a subset of the data is randomly selected. The tree is built recursively, splitting the subset based on random features and thresholds until the depth limit is reached or the subset size is reduced to one.\n3. **Path Length Calculation**: For each data point, the path length in each tree is computed by traversing the tree. The path length is adjusted using a correction factor when a leaf node is reached.\n4. **Anomaly Score Calculation**: The average path length across all trees is converted into an anomaly score using the given formula.\n5. **Outlier Detection**: The top epsilon percent of data points with the highest anomaly scores are identified as outliers and their indices are returned in ascending order."}
{"task_id": 568, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm. The goal is to learn the weight vector for each feature-value and label pair from the training data and use it to predict the labels for the test data.\n\n### Approach\n1. **Empirical Distribution Calculation**: Compute the empirical distribution for each feature-value and label pair. This is done by counting how often each feature-value pair appears with each label in the training data.\n2. **Weight Initialization**: Initialize the weights for each feature-value and label pair to zero.\n3. **Iterative Weight Update**: Use the GIS algorithm to iteratively update the weights until the change is smaller than a specified threshold (epsilon) or the maximum number of iterations is reached. In each iteration:\n   - Compute the expected value of each feature under the current model.\n   - Calculate the scaling factor for each feature and update the weights.\n4. **Prediction**: For each test sample, compute the conditional probability for each label using the learned weights and predict the label with the highest probability.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Implement Maximum Entropy classifier with Generalized Iterative Scaling.\"\"\"\n    # Precompute possible labels\n    possible_labels = set(y_train)\n    N = len(X_train)\n    if N == 0:\n        return []\n    \n    # Step 1: Compute empirical counts E_j\n    counts = defaultdict(int)\n    for k in range(N):\n        x = X_train[k]\n        y = y_train[k]\n        for i in range(x.shape[1]):\n            v = x[i]\n            j = (i, v, y)\n            counts[j] += 1\n    E = {j: (counts[j] / N) for j in counts}\n    \n    # Initialize weights\n    weights = defaultdict(float)\n    \n    # Step 2: Generalized Iterative Scaling\n    for _ in range(n_iter):\n        # Compute M_j for all j\n        M = defaultdict(float)\n        for k in range(N):\n            x = X_train[k]\n            # Compute sum_wy for each y\n            sum_wy = defaultdict(float)\n            for i in range(x.shape[1]):\n                v = x[i]\n                for y in possible_labels:\n                    j = (i, v, y)\n                    sum_wy[y] += weights.get(j, 0.0)\n            # Compute Z\n            Z = sum(sum_wy.values())\n            # Compute P(y|x) for each y and accumulate M_j\n            for i in range(x.shape[1]):\n                v = x[i]\n                for y in possible_labels:\n                    j = (i, v, y)\n                    if Z == 0:\n                        p = 0.0\n                    else:\n                        p = sum_wy[y] / Z\n                    M[j] += p\n        # Normalize M_j by N\n        for j in M:\n            M[j] /= N\n        \n        # Compute changes and update weights\n        changes = []\n        for j in E:\n            if E[j] == 0 or M[j] == 0:\n                continue\n            s_j = E[j] / M[j]\n            delta = math.log(s_j)\n            changes.append(abs(delta))\n            weights[j] += delta\n        \n        # Check for convergence\n        if changes:\n            max_change = max(changes)\n        else:\n            max_change = 0.0\n        if max_change < epsilon:\n            break\n    \n    # Step 3: Make predictions on test data\n    predicted_labels = []\n    for x in X_test:\n        sum_wy = defaultdict(float)\n        for i in range(x.shape[1]):\n            v = x[i]\n            for y in possible_labels:\n                j = (i, v, y)\n                sum_wy[y] += weights.get(j, 0.0)\n        Z = sum(sum_wy.values())\n        if Z == 0:\n            # All probabilities are zero; choose the first possible label\n            predicted_labels.append(next(iter(possible_labels)))\n            continue\n        best_y = None\n        max_p = -1\n        for y in possible_labels:\n            p = sum_wy[y] / Z\n            if p > max_p:\n                max_p = p\n                best_y = y\n        predicted_labels.append(best_y)\n    \n    return predicted_labels\n```\n\n### Explanation\n1. **Empirical Distribution Calculation**: We count how often each feature-value pair appears with each label in the training data to build the empirical distribution.\n2. **Weight Initialization**: Weights are initialized to zero for each feature-value and label pair.\n3. **Iterative Weight Update**: Using the GIS algorithm, we iteratively update the weights by computing the expected values of each feature under the current model, scaling the weights to match the empirical distribution, and checking for convergence.\n4. **Prediction**: For each test sample, we compute the conditional probabilities for each label using the learned weights and predict the label with the highest probability.\n\nThis approach ensures that we learn the optimal weights efficiently and make accurate predictions on the test data."}
{"task_id": 569, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to analyze a collection of text documents and produce two results: (1) a list of all distinct words along with their document-frequency ratio, ordered from the most common to the least common, and (2) a set containing the top k most common words, where k is specified by the user.\n\n### Approach\n1. **Count Word Frequencies**: For each document, convert the list of words into a set to get unique words. For each unique word, increment its count in a frequency dictionary.\n2. **Calculate Document-Frequency Ratio**: For each word, compute the ratio of the number of documents it appears in to the total number of documents. Round this ratio to four decimal places.\n3. **Sort Words by Frequency**: Create a list of tuples (word, ratio) and sort it in descending order based on the ratio.\n4. **Determine Top Words**: Based on the provided cut_off value, determine the set of top words. If cut_off is None, include all words; if it is zero, return an empty set.\n\n### Solution Code\n```python\nfrom collections import defaultdict\nfrom typing import Iterable, Tuple, List, Set, Any\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    if not data:\n        return [], set()\n\n    # Count the frequency of each word across documents\n    frequency = defaultdict(int)\n    for label, words in data:\n        unique_words = set(words)\n        for word in unique_words:\n            frequency[word] += 1\n\n    total_documents = len(data)\n    # Calculate the ratio for each word and create a list of tuples\n    ratio_list = []\n    for word, count in frequency.items():\n        ratio = count / total_documents\n        ratio_rounded = round(ratio, 4)\n        ratio_list.append((word, ratio_rounded))\n\n    # Sort the list by ratio in descending order\n    ratio_list.sort(key=lambda x: (-x[1], x[0]))\n\n    # Determine the set of top words based on cut_off\n    if cut_off is None:\n        top_words = set(frequency.keys())\n    elif cut_off == 0:\n        top_words = set()\n    else:\n        top_words = {word for word, _ in ratio_list[:cut_off]}\n\n    return ratio_list, top_words\n```\n\n### Explanation\n1. **Counting Frequencies**: We use a dictionary to count how many documents each word appears in. For each document, we convert the list of words into a set to ensure each word is only counted once per document.\n2. **Calculating Ratios**: For each word, we compute the ratio of its document count to the total number of documents and round it to four decimal places.\n3. **Sorting**: The list of words is sorted by their document-frequency ratio in descending order. If two words have the same ratio, their order is determined by their alphabetical order.\n4. **Selecting Top Words**: Based on the cut_off value, we select the top k words. If cut_off is None, all words are included; if it is zero, no words are included.\n\nThis approach ensures that we efficiently count word frequencies, compute the required ratios, and return the results in the specified format."}
{"task_id": 571, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the weight vector and bias term for a linear Support Vector Machine (SVM) using the given training data and optimized Lagrange multipliers. The solution involves reconstructing the SVM parameters from the Lagrange multipliers obtained through the Sequential Minimal Optimization (SMO) algorithm.\n\n### Approach\n1. **Compute the Weight Vector (w):**\n   - The weight vector is computed using the formula: \n     \\[\n     \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y})\n     \\]\n     where \\(X\\) is the training data matrix, \\(\\mathbf{y}\\) is the label vector, and \\(\\boldsymbol{\\alpha}\\) is the vector of Lagrange multipliers. This is implemented using matrix multiplication in NumPy.\n\n2. **Compute the Bias Term (b):**\n   - The bias term is computed using the formula:\n     \\[\n     b = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\mathbf{w}^\\top \\mathbf{x}_i \\right)\n     \\]\n     This involves calculating the dot product of the weight vector with each sample, subtracting the result from the corresponding label, summing these values, and then taking the average.\n\n3. **Rounding:**\n   - Both the weight vector and the bias term are rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the element-wise product of alpha and y\n    alpha_y = alpha * y\n    \n    # Compute the weight vector w\n    w = X.T.dot(alpha_y)\n    \n    # Compute the numerator for the bias term b\n    temp = w.dot(X.T)\n    numerator = (y - temp).sum()\n    \n    # Calculate the bias term b\n    n = X.shape[0]\n    b = numerator / n\n    \n    # Round the weight vector and bias term to four decimal places\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    \n    # Convert the weight vector to a Python list\n    w_list = w_rounded.tolist()\n    \n    return (w_list, b_rounded)\n```\n\n### Explanation\n- **Weight Vector Calculation:** The weight vector is derived by transposing the input matrix \\(X\\) and performing a matrix multiplication with the element-wise product of the Lagrange multipliers and labels. This efficiently sums the contributions of each training sample weighted by their respective Lagrange multipliers.\n- **Bias Term Calculation:** The bias term is computed by first determining the dot product of the weight vector with each sample, subtracting these values from the corresponding labels, summing the results, and then averaging. This step ensures that the decision boundary is correctly positioned relative to the training data.\n- **Rounding:** Both the weight vector and bias term are rounded to four decimal places to ensure precision and meet the problem's requirements.\n\nThis approach efficiently reconstructs the SVM parameters using matrix operations, ensuring accuracy and performance."}
{"task_id": 572, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Local Outlier Factor (LOF) algorithm from scratch to detect anomalies in a dataset. The LOF algorithm is a density-based method that identifies outliers based on the local density of a sample compared to its neighbors.\n\n### Approach\nThe approach to implement the LOF algorithm involves the following steps:\n\n1. **Compute the Full Pairwise Euclidean Distance Matrix**: Calculate the Euclidean distance between every pair of samples in the dataset. This results in a square matrix where each element (i, j) represents the distance between the i-th and j-th samples.\n\n2. **Determine k-Distance and k-Nearest Neighbors**: For each sample, find its k-th nearest neighbor and the indices of the k nearest neighbors. The k-distance is the distance to the k-th nearest neighbor.\n\n3. **Calculate Reachability Distance**: For each pair of samples (p, q), the reachability distance is defined as the maximum of the k-distance of q and the Euclidean distance between p and q.\n\n4. **Compute Local Reachability Density (LRD)**: The LRD of a sample p is the ratio of k to the sum of reachability distances from p to its k nearest neighbors.\n\n5. **Compute Local Outlier Factor (LOF)**: The LOF of a sample p is the ratio of the average LRD of its k nearest neighbors to its own LRD.\n\n6. **Identify Outliers**: Round the LOF scores to four decimal places and identify samples with LOF scores strictly greater than a given threshold epsilon.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Implement the Local Outlier Factor (LOF) algorithm to detect anomalies.\"\"\"\n    n = data.shape[0]\n    \n    # Step 1: Compute the full pairwise Euclidean distance matrix\n    distance_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            distance_matrix[i][j] = np.linalg.norm(data[i] - data[j])\n    \n    # Step 2: For each sample p, compute k-distance and list of k nearest neighbors\n    k_distances = np.zeros(n)\n    N_k = [[] for _ in range(n)]\n    for p in range(n):\n        distances = distance_matrix[p]\n        non_self = np.where(distances != 0)[0]\n        distances_excl = distances[non_self]\n        sorted_indices = np.argsort(distances_excl)\n        k_neighbor_indices = non_self[sorted_indices[:k]]\n        N_k[p] = k_neighbor_indices\n        k_distance_p = distances_excl[sorted_indices[k-1]]\n        k_distances[p] = k_distance_p\n    \n    # Step 3: Compute Local Reachability Density (LRD) for each sample\n    LRD = np.zeros(n)\n    for p in range(n):\n        sum_reach = 0.0\n        for q in N_k[p]:\n            dist_pq = distance_matrix[p][q]\n            k_distance_q = k_distances[q]\n            reach_dist = max(k_distance_q, dist_pq)\n            sum_reach += reach_dist\n        LRD[p] = k / sum_reach\n    \n    # Step 4: Compute Local Outlier Factor (LOF) for each sample\n    LOF = np.zeros(n)\n    for p in range(n):\n        sum_LRD = 0.0\n        for q in N_k[p]:\n            sum_LRD += LRD[q]\n        average_LRD = sum_LRD / k\n        LOF[p] = average_LRD / LRD[p]\n    \n    # Round LOF scores to 4 decimal places\n    LOF_rounded = np.round(LOF, 4).tolist()\n    \n    # Find indices where LOF > epsilon\n    outlier_indices = [i for i in range(n) if LOF[i] > epsilon]\n    outlier_indices.sort()\n    \n    return (LOF_rounded, outlier_indices)\n```\n\n### Explanation\n1. **Distance Matrix Calculation**: The distance matrix is computed using nested loops, where each element (i, j) is the Euclidean distance between the i-th and j-th samples.\n\n2. **k-Distance and k-Nearest Neighbors**: For each sample, the distances to all other samples are sorted, and the indices of the k nearest neighbors are identified. The k-distance is the distance to the k-th nearest neighbor.\n\n3. **Reachability Distance**: For each pair of samples, the reachability distance is calculated as the maximum of the k-distance of one sample and the Euclidean distance between the two samples.\n\n4. **Local Reachability Density (LRD)**: The LRD is computed as the ratio of k to the sum of reachability distances from a sample to its k nearest neighbors.\n\n5. **Local Outlier Factor (LOF)**: The LOF is computed as the ratio of the average LRD of the k nearest neighbors to the LRD of the sample itself.\n\n6. **Outlier Detection**: The LOF scores are rounded to four decimal places, and samples with scores exceeding the threshold epsilon are identified as outliers.\n\nThis approach ensures that we accurately detect outliers based on the local density of each sample, providing a robust method for anomaly detection."}
{"task_id": 574, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a synthetic 2-D dataset for multiple classes, which can be used for quick experiments or visualizations. The dataset should include both training and validation points for each class, with each point's coordinates sampled from a uniform distribution on a specific square region.\n\n### Approach\n1. **Problem Analysis**: The task requires generating points for each class such that each class's points are uniformly distributed within a specific square region. The region for each class is determined based on the class label. The points are split into training and validation sets, and the coordinates must be rounded to 4 decimal places for consistency and determinism.\n\n2. **Intuition**: For each class, the points are generated within a square region centered at a specific base value. The base value is determined by the class label, ensuring that each class has a distinct region. Using a uniform distribution ensures that points are spread evenly within the specified region.\n\n3. **Algorithm Selection**: We use the `random.uniform` function from the `random` module to generate the coordinates for each point. This ensures that the points are uniformly distributed within the specified range. The `numpy` library is used to efficiently handle and concatenate the generated points into arrays.\n\n4. **Seed Handling**: If a seed is provided, it is used to initialize the random number generator, ensuring reproducibility of the results.\n\n5. **Efficiency Considerations**: The approach uses list comprehensions and `numpy` functions to generate and concatenate points efficiently. Although using `random.uniform` in a loop might be slower for very large datasets, it ensures correctness and simplicity.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    y_train = []\n    X_val = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        low = base + 1\n        high = base + 9\n        \n        # Generate training points for this class\n        x = [random.uniform(low, high) for _ in range(n_train)]\n        y = [random.uniform(low, high) for _ in range(n_train)]\n        x = np.array(x)\n        y = np.array(y)\n        points = np.column_stack((x, y))\n        X_train.append(points)\n        y_train.append(np.full(n_train, c))\n        \n        # Generate validation points for this class\n        x_val = [random.uniform(low, high) for _ in range(n_val)]\n        y_val = [random.uniform(low, high) for _ in range(n_val)]\n        x_val = np.array(x_val)\n        y_val = np.array(y_val)\n        points_val = np.column_stack((x_val, y_val))\n        X_val.append(points_val)\n        y_val.append(np.full(n_val, c))\n    \n    # Concatenate all points and labels\n    X_train = np.concatenate(X_train)\n    y_train = np.concatenate(y_train)\n    X_val = np.concatenate(X_val)\n    y_val = np.concatenate(y_val)\n    \n    # Round all coordinates to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\n### Explanation\n1. **Initialization**: The function initializes lists to store the training and validation points and their corresponding labels.\n\n2. **Loop Through Classes**: For each class, the base value is calculated to determine the range for the uniform distribution. Points are generated for both training and validation sets.\n\n3. **Generate Points**: Using `random.uniform`, points are generated within the specified range for each class. These points are converted into numpy arrays and stored in lists.\n\n4. **Concatenate Points**: After generating points for all classes, the lists are concatenated into numpy arrays to form the final datasets.\n\n5. **Rounding**: The coordinates are rounded to 4 decimal places to ensure compact and deterministic output.\n\nThis approach ensures that the generated dataset is both correct and reproducible, meeting the problem's requirements efficiently."}
{"task_id": 579, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the classical K-Means clustering algorithm from scratch. The goal is to cluster a given set of points into a specified number of groups, using a deterministic approach to ensure reproducibility.\n\n### Approach\nThe K-Means algorithm consists of the following steps:\n\n1. **Initialization**: The first `k` points from the input dataset are taken as the initial cluster centers. This makes the result deterministic.\n2. **Assignment Step**: Each point is assigned to the nearest cluster center based on Euclidean distance. In case of a tie, the point is assigned to the cluster with the smaller index.\n3. **Update Step**: Each cluster center is recomputed as the arithmetic mean of all points assigned to that cluster. If a cluster loses all its points, it remains unchanged.\n4. **Convergence Check**: The algorithm repeats the assignment and update steps until the assignments no longer change or the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m = X.shape[0]\n    d = X.shape[1]\n    centers = X[:k].copy()\n    labels = np.zeros(m, dtype=int)\n    prev_labels = None\n\n    for _ in range(max_iters):\n        # Compute squared distances from each point to each center\n        squared_distances = np.sum((X[:, np.newaxis] - centers[np.newaxis, :]) ** 2, axis=2)\n        # Assign each point to the nearest center\n        best_center = np.argmin(squared_distances, axis=1)\n        labels = best_center\n\n        # Check if labels have not changed since last iteration\n        if prev_labels is not None and np.all(labels == prev_labels):\n            break\n\n        prev_labels = labels.copy()\n\n        # Update the centers\n        new_centers = np.zeros_like(centers)\n        for c in range(k):\n            points_in_c = X[labels == c]\n            if len(points_in_c) == 0:\n                new_centers[c] = centers[c]\n            else:\n                new_centers[c] = np.mean(points_in_c, axis=0)\n        centers = new_centers\n\n    # Round the centers to 4 decimal places\n    rounded_centers = np.round(centers, 4).tolist()\n    # Convert labels to a list\n    labels_list = labels.tolist()\n\n    return (labels_list, rounded_centers)\n```\n\n### Explanation\n1. **Initialization**: The first `k` points from the input array `X` are used as initial cluster centers. This ensures the result is deterministic.\n2. **Distance Calculation**: For each point, the squared Euclidean distance to each cluster center is computed. This avoids the computational cost of taking square roots while maintaining the order of distances.\n3. **Assignment**: Each point is assigned to the nearest cluster center. In case of ties, the point is assigned to the cluster with the smaller index.\n4. **Update**: Each cluster center is updated to the mean of the points assigned to it. If a cluster has no points, it remains unchanged.\n5. **Convergence Check**: The algorithm stops early if the assignments do not change between iterations, ensuring efficiency.\n\nThis approach ensures that the algorithm efficiently converges to a stable clustering solution, providing both the cluster labels for each point and the final cluster centers rounded to four decimal places."}
{"task_id": 581, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic-Net regularized linear regression using batch gradient descent. Elastic-Net combines L1 and L2 regularization to help in feature selection and to prevent overfitting. The goal is to iteratively update the model parameters to minimize the loss function.\n\n### Approach\n1. **Initialization**: Start with all weights and the bias term initialized to zero.\n2. **Gradient Calculation**: For each iteration, compute the predictions using the current weights and bias. Calculate the residual (difference between actual and predicted values). Use these residuals to compute the gradients for each weight and the bias.\n3. **Regularization Terms**: Incorporate the L1 and L2 regularization terms in the gradient calculation. The L1 term uses the sign function, with a special case where the sign of zero is defined as -1.\n4. **Parameter Update**: Update the weights and bias using the computed gradients, scaled by the learning rate and the number of samples.\n5. **Termination**: Repeat the gradient computation and parameter update for the specified number of iterations. Return the learned weights and bias, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    \n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * n, 0.0\n    \n    w = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(iterations):\n        y_hat = X.dot(w) + b\n        r = y - y_hat\n        \n        # Compute gradient for weights\n        grad_w_part1 = (-2) * (X.T.dot(r))\n        sign = np.sign(w)\n        mask = (w == 0)\n        sign[mask] = -1\n        grad_w = grad_w_part1 + l1_penalty * sign + 2 * l2_penalty * w\n        mean_grad_w = grad_w / m\n        \n        # Compute gradient for bias\n        grad_b = (-2) * np.sum(r)\n        mean_grad_b = grad_b / m\n        \n        # Update parameters\n        w = w - learning_rate * mean_grad_w\n        b = b - learning_rate * mean_grad_b\n    \n    rounded_weights = [round(weight, 4) for weight in w.tolist()]\n    rounded_bias = round(b, 4)\n    \n    return (rounded_weights, rounded_bias)\n```\n\n### Explanation\n1. **Initialization**: The weights (`w`) and bias (`b`) are initialized to zero.\n2. **Gradient Calculation**: For each iteration, the predictions are computed using matrix multiplication of the input features (`X`) and weights, plus the bias. The residual is calculated as the difference between the actual target values (`y`) and the predictions.\n3. **Regularization Terms**: The gradients for the weights include terms from both L1 and L2 regularization. The L1 term uses the sign of the weights, adjusting zeros to -1. The L2 term is a scaled version of the weights.\n4. **Parameter Update**: The weights and bias are updated using the computed gradients, scaled by the learning rate and normalized by the number of samples to get the mean gradient.\n5. **Termination**: After completing the specified number of iterations, the weights and bias are rounded to four decimal places and returned as a tuple.\n\nThis approach ensures that the model parameters are optimized to minimize the loss function while incorporating the effects of both L1 and L2 regularization."}
{"task_id": 591, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the output of a fully-connected feed-forward neural network given its weight matrices and an input vector. The network uses ReLU activation for all hidden layers and a linear activation for the last layer. The goal is to perform a forward pass through the network and return the prediction rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Convert the input vector into a numpy array for efficient matrix operations.\n2. **Forward Pass**: Iterate through each layer of the network:\n   - For each layer, augment the current activation vector with a bias term (1).\n   - Compute the pre-activations by multiplying the augmented input vector with the weight matrix of the current layer.\n   - Apply the ReLU activation function to the pre-activations for all layers except the last one.\n3. **Output Handling**: After processing all layers, round the final activations to four decimal places. If the output layer has only one neuron, return the result as a float. Otherwise, return a list of the rounded values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    current_activation = np.array(x, dtype=np.float64)\n    for L in range(len(weights)):\n        weight_matrix = np.array(weights[L])\n        # Augment current activation with 1\n        augmented_input = np.append(current_activation, 1.0)\n        # Reshape to column vector\n        augmented_input_col = augmented_input.reshape(-1, 1)\n        # Compute pre-activations\n        pre_activations = weight_matrix.dot(augmented_input_col)\n        # Extract as 1D array\n        pre_activations = pre_activations.flatten()\n        # Apply activation function\n        if L == len(weights) - 1:\n            activations = pre_activations\n        else:\n            activations = np.maximum(0, pre_activations)\n        # Update current activation for next layer\n        current_activation = activations\n    # Round the output\n    rounded_output = np.round(current_activation, 4)\n    if len(rounded_output) == 1:\n        return rounded_output[0]\n    else:\n        return rounded_output.tolist()\n```\n\n### Explanation\n1. **Initialization**: The input vector `x` is converted into a numpy array to facilitate efficient matrix operations.\n2. **Forward Pass**:\n   - For each layer, the current activation vector is augmented with a bias term (1).\n   - The augmented input vector is reshaped into a column vector and multiplied by the weight matrix of the current layer to compute the pre-activations.\n   - ReLU activation is applied to the pre-activations for all layers except the last one, which uses a linear activation.\n3. **Output Handling**: The final activations are rounded to four decimal places. If the output layer has only one neuron, the result is returned as a float. Otherwise, a list of the rounded values is returned.\n\nThis approach ensures that we efficiently compute the network's output by leveraging numpy for matrix operations and correctly applying the appropriate activation functions for each layer."}
{"task_id": 595, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm from scratch to compute the centroids of clusters. The algorithm should be deterministic, efficient, and handle edge cases such as empty clusters.\n\n### Approach\n1. **Initialization**: Start by initializing the centroids using the first `k` samples from the input data. This ensures deterministic results.\n2. **Iteration**: Repeat the process for a maximum number of iterations or until convergence:\n   - **Assignment**: Compute the squared Euclidean distance from each sample to each centroid and assign each sample to the nearest centroid.\n   - **Update**: Compute the new centroids as the mean of the samples assigned to each centroid. If a centroid has no samples assigned, it remains unchanged.\n3. **Convergence Check**: After each iteration, check if all centroids have moved by less than `1e-4`. If so, stop early.\n4. **Sorting and Rounding**: Once the algorithm converges, sort the centroids lexicographically and round each coordinate to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Implement K-Means clustering to compute centroids.\"\"\"\n    # Initialize centroids with the first k samples\n    centroids = X[:k]\n    \n    for iter in range(max_iters):\n        # Calculate distances from each sample to each centroid\n        distances = np.sum((X[:, np.newaxis] - centroids) ** 2, axis=2)\n        assignments = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = np.zeros_like(centroids)\n        for j in range(k):\n            mask = (assignments == j)\n            if np.sum(mask) == 0:\n                new_centroids[j] = centroids[j]\n            else:\n                new_centroids[j] = np.mean(X[mask], axis=0)\n        \n        # Check for convergence\n        movement = np.max(np.abs(new_centroids - centroids))\n        if movement < 1e-4:\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    else:\n        # If loop completes without breaking, update centroids\n        centroids = new_centroids\n    \n    # Sort centroids lexicographically\n    sorted_centroids = sorted(centroids.tolist())\n    \n    # Round each coordinate to 4 decimal places\n    rounded = []\n    for centroid in sorted_centroids:\n        rounded_centroid = [round(c, 4) for c in centroid]\n        rounded.append(rounded_centroid)\n    \n    return rounded\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first `k` samples, ensuring the results are deterministic.\n2. **Distance Calculation**: For each sample, compute the squared Euclidean distance to each centroid using vectorized operations for efficiency.\n3. **Assignment**: Each sample is assigned to the nearest centroid using the `argmin` function.\n4. **Centroid Update**: New centroids are computed as the mean of the samples assigned to each cluster. If a cluster is empty, its centroid remains unchanged.\n5. **Convergence Check**: The algorithm stops early if the maximum movement of any centroid is less than `1e-4`.\n6. **Sorting and Rounding**: After convergence, centroids are sorted lexicographically and each coordinate is rounded to four decimal places for the final output.\n\nThis approach ensures that the K-Means algorithm is implemented efficiently and correctly, handling edge cases and providing the required output format."}
