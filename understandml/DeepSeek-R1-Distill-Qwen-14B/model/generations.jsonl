{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of standard Generalized Linear Model (GLM) link functions. Each link function is used to transform the expected value of the response variable, \u03bc, to the linear predictor, \u03b7. The dictionary should include three common link functions: identity, log, and logit, each with their respective inverse and derivative functions.\n\n### Approach\n1. **Identity Link Function**: This function simply returns the input value as it is. The inverse function is the same as the link function, and the derivative is 1.\n2. **Log Link Function**: This function computes the natural logarithm of the input. The inverse function is the exponential function, and the derivative is the reciprocal of the input.\n3. **Logit Link Function**: This function computes the logarithm of the odds of the input. The inverse function is the logistic function, and the derivative is the reciprocal of the product of the input and its complement.\n\nEach link function, its inverse, and its derivative are implemented as numpy-aware callables to handle both scalar and array inputs efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: 1\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1.0 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1.0 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1.0 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\n- **Identity Link**: The identity function is straightforward, returning the input as is. Its inverse is the same function, and the derivative is always 1.\n- **Log Link**: The log function transforms the input using the natural logarithm. The inverse transformation is the exponential function, and the derivative is the reciprocal of the input.\n- **Logit Link**: The logit function transforms the input using the logarithm of the odds. The inverse transformation is the logistic function, which maps any real number to a value between 0 and 1. The derivative of the logit function is the reciprocal of the product of the input and its complement.\n\nThis implementation ensures that each function works efficiently with both scalar values and numpy arrays, leveraging element-wise operations for array inputs."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation and its gradient for a given input. The input can be a scalar, a list, or a NumPy array. The function should return a tuple containing the sigmoid values and their gradients, both rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should handle different types of input, including integers, floats, lists, and NumPy arrays. If the input is a scalar (int, float, or 0D NumPy array), the function processes it as a single value. For array-like inputs (lists or higher-dimensional NumPy arrays), the function processes each element individually.\n2. **Conversion to NumPy Array**: For array-like inputs, the function converts the input to a NumPy array to facilitate element-wise operations.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Gradient Calculation**: The gradient of the sigmoid function is computed using the formula \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n5. **Rounding**: Both the sigmoid values and their gradients are rounded to four decimal places.\n6. **Return Type**: If the input is a scalar, the function returns two floats. If the input is an array-like structure, the function returns two lists of the same shape, each element rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Handle scalar case, including 0D arrays\n        if isinstance(x, (int, float)):\n            x_scalar = x\n        else:\n            x_scalar = x.item()  # Extract scalar from 0D array\n        sigmoid = 1 / (1 + np.exp(-x_scalar))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Handle array-like case\n        if isinstance(x, list):\n            x_array = np.array(x)\n        else:\n            x_array = x\n        sigmoid = 1 / (1 + np.exp(-x_array))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = np.round(sigmoid, 4)\n        gradient_rounded = np.round(gradient, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a scalar or a 0D array. If so, it processes the input as a single value. Otherwise, it converts the input to a NumPy array.\n- **Sigmoid Calculation**: Using NumPy's exponential function, the sigmoid values are computed element-wise.\n- **Gradient Calculation**: The gradient is derived from the sigmoid function and computed using the derived formula.\n- **Rounding**: Both the sigmoid values and gradients are rounded to four decimal places using NumPy's rounding function.\n- **Return Type**: Depending on whether the input was a scalar or an array-like structure, the function returns the results as floats or lists, respectively.\n\nThis approach ensures that the function works efficiently for various input types and returns the results in the required format."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian kernel. The SVM decision function is given, and we need to compute the predicted labels efficiently using vectorized operations.\n\n### Approach\n1. **Compute Kernel Matrix**: For each test sample, compute the Gaussian kernel values with all training samples. This is done efficiently using vectorized operations to avoid loops.\n2. **Sum Contributions**: Multiply each kernel value by the corresponding Lagrange multiplier and class label, then sum these contributions for each test sample.\n3. **Add Bias Term**: Add the bias term to the computed sum for each test sample.\n4. **Determine Predictions**: Use the sign of the resulting values to determine the predicted class labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Reshape the training and test data for broadcasting\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    \n    X_train_reshaped = X_train[:, np.newaxis, :]  # Shape: (n, 1, d)\n    X_test_reshaped = X_test[np.newaxis, :, :]    # Shape: (1, m, d)\n    \n    # Compute the squared differences\n    diff = X_test_reshaped - X_train_reshaped      # Shape: (n, m, d)\n    squared_norms = np.sum(diff ** 2, axis=2)     # Shape: (n, m)\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * squared_norms)            # Shape: (n, m)\n    \n    # Compute the sum of alpha_i * y_i * K[i,j] for each test sample j\n    sum_values = (alpha * y_train).dot(K.T)       # Shape: (m,)\n    \n    # Add the bias term\n    g_z = sum_values + b\n    \n    # Determine the predicted labels\n    predictions = np.where(g_z >= 0, 1, -1)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Reshape Data**: The training and test data are reshaped to facilitate broadcasting, allowing efficient computation of pairwise differences.\n2. **Compute Differences**: The differences between each test sample and each training sample are computed, and the squared Euclidean norms are summed.\n3. **Gaussian Kernel**: The Gaussian kernel values are computed using the squared norms.\n4. **Sum Contributions**: The contributions from each training sample are summed for each test sample using vectorized operations.\n5. **Bias and Sign**: The bias term is added, and the sign of the result determines the predicted class label.\n\nThis approach ensures that the solution is efficient and scalable, handling large datasets using vectorized operations without relying on external libraries."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that breaks a one-dimensional signal into overlapping frames using NumPy. The function should return a view on the original signal, not a copy, and handle various edge cases and pre-conditions.\n\n### Approach\n1. **Pre-Conditions Check**: Ensure the input signal is one-dimensional, the stride is positive, the frame width is positive, and the signal length is at least as long as the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Create View Using Stride Tricks**: Use `numpy.lib.stride_tricks.as_strided` to create a view of the original array with the desired shape and strides. The strides are calculated based on the frame width and stride.\n4. **Set Writeable Flag**: Adjust the writeable flag of the result based on the input parameter to ensure the result is either read-only or inherits the writeability of the original array.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    assert x.ndim == 1, \"x must be 1D\"\n    assert stride >= 1, \"stride must be positive\"\n    assert frame_width >= 1, \"frame_width must be positive\"\n    n = x.size\n    assert n >= frame_width, \"x must be at least frame_width long\"\n\n    # Calculate number of frames\n    n_frames = (n - frame_width) // stride + 1\n\n    # Calculate shape and strides\n    shape = (n_frames, frame_width)\n    strides = (stride * x.itemsize, x.itemsize)\n\n    # Create the view using as_strided\n    result = as_strided(x, shape=shape, strides=strides)\n\n    # Set writeable flag\n    if writeable:\n        result.flags.writeable = x.flags.writeable\n    else:\n        result.flags.writeable = False\n\n    return result\n```\n\n### Explanation\n1. **Pre-Conditions Check**: The function starts by verifying that the input signal is one-dimensional, the stride and frame width are positive, and the signal length is sufficient to form at least one frame.\n2. **Number of Frames Calculation**: The number of frames is computed using integer division to ensure we only include complete frames.\n3. **View Creation**: Using `as_strided`, the function creates a view of the original array with the specified shape and strides. This ensures the result is a view rather than a copy, optimizing memory usage.\n4. **Writeable Flag Adjustment**: The writeable flag of the result is set based on the input parameter, ensuring the result is either read-only or writeable as required.\n\nThis approach efficiently handles the problem requirements and constraints, providing a solution that is both correct and optimized for performance."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given matrix is row-stochastic. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\n1. **Input Validation**: Check if the input is a list of lists or a numpy array. If it's a list, ensure all elements are lists and all rows have the same length. If it's a numpy array, ensure it is two-dimensional.\n2. **Element Check**: Verify that every element in the matrix is between 0 and 1 inclusive.\n3. **Row Sum Check**: Ensure that each row sums to 1 within a numerical tolerance of \\(1 \\times 10^{-8}\\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Check if X is a list of lists or a numpy array\n    if isinstance(X, list):\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Check if X is empty\n        if not X:\n            return False\n        # Check if all rows have the same length\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        # Convert to numpy array\n        X_np = np.array(X)\n    else:\n        # Check if it's a numpy array and 2D\n        if X.ndim != 2:\n            return False\n        X_np = X\n\n    # Check if all elements are between 0 and 1\n    if not ((X_np >= 0).all() and (X_np <= 1).all()):\n        return False\n\n    # Check if each row sums to 1 within a tolerance\n    row_sums = np.sum(X_np, axis=1)\n    if not np.allclose(row_sums, 1, atol=1e-8):\n        return False\n\n    # All conditions met\n    return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a list of lists or a numpy array. For a list, it ensures all elements are lists and each row has the same length. For a numpy array, it checks if it is two-dimensional.\n2. **Element Check**: The function converts the input to a numpy array and checks if all elements are between 0 and 1.\n3. **Row Sum Check**: The function calculates the sum of each row and checks if each sum is approximately 1 within a tolerance of \\(1 \\times 10^{-8}\\) using `numpy.allclose`.\n\nThis approach ensures that the matrix meets all the criteria for being row-stochastic efficiently and correctly."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The backward probability \\(\\beta_t(i)\\) represents the probability of seeing the remaining observations from time \\(t+1\\) onward given that the system is in state \\(i\\) at time \\(t\\).\n\n### Approach\n1. **Initialization**: Start by initializing the backward probability vector for the last time step \\(T-1\\) to 1 for all states, as there are no future observations beyond the last step.\n2. **Backward Calculation**: Compute the backward probabilities iteratively from the second last time step down to the first time step. For each time step \\(t\\), compute the probability for each state \\(i\\) by summing over all possible next states \\(j\\), using the transition probabilities, emission probabilities, and the previously computed backward probabilities.\n3. **Result Extraction**: After computing the backward probabilities for all relevant time steps, extract the backward probability vector for the given time index \\(t\\) and round the values to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n    \n    Args:\n        A: Transition probability matrix of shape (N, N)\n        B: Emission probability matrix of shape (N, M)\n        obs: List of observation indices (length T)\n        t: Integer time index (0 \u2264 t < T)\n        \n    Returns:\n        A list of N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)\n    T = len(obs)\n    \n    # Initialize beta vectors\n    beta = [[0.0 for _ in range(N)] for _ in range(T)]\n    \n    # Base case: t = T-1\n    for i in range(N):\n        beta[T-1][i] = 1.0\n    \n    # Compute beta for t from T-2 down to 0\n    for current_t in range(T-2, -1, -1):\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                a_ij = A[i][j]\n                b_j = B[j][obs[current_t + 1]]\n                beta_j = beta[current_t + 1][j]\n                total += a_ij * b_j * beta_j\n            beta[current_t][i] = total\n    \n    # Round the result to 4 decimal places\n    result = [round(beta[t][i], 4) for i in range(N)]\n    return result\n```\n\n### Explanation\n1. **Initialization**: The backward probability vector for the last time step \\(T-1\\) is initialized to 1 for all states because there are no future observations to consider.\n2. **Backward Calculation**: For each time step from \\(T-2\\) down to 0, the backward probability for each state \\(i\\) is computed by summing the product of the transition probability from state \\(i\\) to state \\(j\\), the emission probability of the next observation in state \\(j\\), and the previously computed backward probability for state \\(j\\).\n3. **Result Extraction**: The computed backward probabilities for the specified time index \\(t\\) are extracted and rounded to four decimal places for the final result.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that we only compute the necessary values and reuse them as needed."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by leveraging the concept of backward variables, which represent the probability of observing a sequence from a certain point onward given the current state.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the probability of an observation sequence given an HMM using the backward algorithm. The HMM is defined by transition matrix `A`, emission matrix `B`, and initial distribution `pi`. The backward variables are used to compute this probability efficiently.\n\n2. **Backward Variables**: The backward variable `beta_t(i)` represents the probability of observing the sequence from time `t+1` to the end given that the state at time `t` is `i`. These variables are computed recursively starting from the last observation and moving backward.\n\n3. **Initialization**: The backward variables for the last time step are initialized to 1, as there are no future observations to consider.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to `0`, compute the backward variables using the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution, emission probabilities, and the backward variables.\n\n6. **Edge Cases**: If any of the input lists are empty, the function returns 0.0 as no valid probability can be computed.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    N = len(A)\n    T = len(obs)\n\n    if T == 0:\n        return 0.0\n\n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                new_beta[i] += A[i][j] * B[j][obs[t+1]] * beta[j]\n        beta = new_beta\n\n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n\n    return round(prob, 6)\n```\n\n### Explanation\n1. **Initialization Check**: The function first checks if any of the input lists are empty. If so, it returns 0.0 immediately.\n\n2. **Initialization of Backward Variables**: The backward variables for the last time step are initialized to 1.0.\n\n3. **Recursive Calculation**: For each time step from `T-2` down to `0`, the function computes the backward variables using the transition matrix `A`, emission matrix `B`, and the previously computed backward variables.\n\n4. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution `pi`, emission probabilities, and the backward variables.\n\n5. **Rounding**: The result is rounded to six decimal places before returning.\n\nThis approach efficiently computes the probability using dynamic programming principles, ensuring that the solution is both optimal and easy to understand."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the likelihood of an observation sequence given a Hidden Markov Model (HMM) using the forward algorithm. The HMM is defined by the initial state probabilities, state-transition probabilities, and emission probabilities. The function must validate the input dimensions and return the probability rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: \n   - Check if the observations list is empty.\n   - Ensure the dimensions of the initial state probabilities (S), state-transition matrix (A), and emission matrix (B) are correct.\n   - Verify that each observation index is within the valid range.\n\n2. **Forward Algorithm**:\n   - Initialize a vector `alpha` where `alpha[i]` represents the probability of being in state `i` at the first observation.\n   - For each subsequent observation, update `alpha` using the state-transition and emission probabilities.\n   - The result is the sum of the probabilities in the `alpha` vector after processing all observations.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check if observations is empty\n    if not observations:\n        return -1\n    \n    n = len(S)\n    # Check dimensions of S\n    if len(S) != n:\n        return -1\n    \n    # Check dimensions of A\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check dimensions of B\n    if len(B) != n:\n        return -1\n    if not B:\n        return -1  # B is empty\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check each observation is within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Compute forward algorithm\n    T = len(observations)\n    alpha = [0.0] * n\n    k0 = observations[0]\n    for i in range(n):\n        alpha[i] = S[i] * B[i][k0]\n    \n    for t in range(1, T):\n        kt = observations[t]\n        new_alpha = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += alpha[j] * A[j][i]\n            new_alpha[i] = sum_val * B[i][kt]\n        alpha = new_alpha\n    \n    result = sum(alpha)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the observations list is empty and returns -1 if it is. It then verifies the dimensions of the matrices S, A, and B to ensure they are consistent with the model's requirements. Each observation is checked to ensure it falls within the valid range.\n\n2. **Initialization**: The `alpha` vector is initialized based on the initial state probabilities and the emission probabilities for the first observation.\n\n3. **Forward Algorithm**: For each subsequent observation, the `alpha` vector is updated by considering all possible transitions from the previous states. This involves summing the probabilities of transitioning to each state and multiplying by the emission probability for the current observation.\n\n4. **Result Calculation**: The final result is the sum of the probabilities in the `alpha` vector after processing all observations, which is then rounded to four decimal places and returned."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The classifier will predict the class label for each test sample based on the majority vote among its k closest training samples using the Euclidean distance metric. If there is a tie in the majority vote, the smallest class label among the tied ones will be chosen. If k is not a valid positive integer or larger than the number of training samples, the function should return -1.\n\n### Approach\n1. **Validation of k**: Check if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, return a list of -1s with the same length as the test set.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. This avoids the computational cost of taking square roots while maintaining the order of distances.\n3. **Sorting and Selection**: Sort the training samples based on the computed distances and select the top k nearest samples.\n4. **Majority Vote**: Among the k nearest samples, determine the class label with the highest frequency. In case of a tie, select the smallest class label.\n\n### Solution Code\n```python\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    \n    # Check if k is valid\n    if k < 1 or k > n_train:\n        return [-1] * n_test\n    \n    predictions = []\n    for x in X_test:\n        # Compute distances to all training samples\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            dist_sq = sum((x[i] - xi[i])**2 for i in range(len(x)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort()\n        \n        # Take the first k samples\n        selected = distances[:k]\n        \n        # Collect the class labels\n        ys = [y for (d, y) in selected]\n        \n        # Find the mode, breaking ties by choosing the smallest class label\n        freq = {}\n        for y in ys:\n            freq[y] = freq.get(y, 0) + 1\n        \n        max_freq = max(freq.values())\n        candidates = [y for y in freq if freq[y] == max_freq]\n        predicted = min(candidates)\n        \n        predictions.append(predicted)\n    \n    return predictions\n```\n\n### Explanation\n1. **Validation of k**: The function first checks if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, it returns a list of -1s.\n2. **Distance Calculation**: For each test sample, the squared Euclidean distance to each training sample is computed. This avoids the computational cost of taking square roots.\n3. **Sorting and Selection**: The computed distances are sorted, and the top k nearest samples are selected.\n4. **Majority Vote**: The class labels of the k nearest samples are counted. The class with the highest frequency is chosen. In case of a tie, the smallest class label is selected.\n\nThis approach ensures that the function efficiently computes the k-NN classification for each test sample while handling edge cases such as invalid k and ties in the majority vote."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the unnormalized categorical cross-entropy loss for a batch of one-hot encoded target labels. The cross-entropy loss is a common metric used in machine learning to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function accepts inputs as either lists or NumPy arrays. We first convert these inputs to NumPy arrays to facilitate element-wise operations.\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to all elements of the predicted probabilities.\n3. **Compute Logarithm of Predictions**: We compute the natural logarithm of the adjusted predicted probabilities.\n4. **Element-wise Multiplication**: Multiply the true labels (one-hot encoded) with the logarithm of the predicted probabilities. This step effectively extracts the log probability of the true class for each sample.\n5. **Sum and Negate**: Sum all the elements of the resulting matrix and negate the sum to get the cross-entropy loss.\n6. **Round the Result**: Finally, round the computed loss to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = 1e-10\n    y_pred += epsilon\n    log_y_pred = np.log(y_pred)\n    product = y * log_y_pred\n    sum_product = np.sum(product)\n    loss = -sum_product\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This ensures that we can perform element-wise operations efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in an undefined value.\n- **Logarithm and Multiplication**: By taking the logarithm of the predicted probabilities and multiplying by the one-hot encoded true labels, we isolate the log probability of the correct class for each sample.\n- **Summing and Negating**: The sum of these log probabilities is negated to compute the cross-entropy loss, which measures the dissimilarity between the predicted and true distributions.\n- **Rounding**: The result is rounded to four decimal places for precision and returned as a float.\n\nThis approach efficiently computes the cross-entropy loss using NumPy operations, ensuring numerical stability and correctness."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward propagation for an L-layer neural network. The network uses ReLU activation for all hidden layers and sigmoid activation for the output layer. The goal is to compute the activation of the last layer and store the necessary information for backpropagation in a cache.\n\n### Approach\n1. **Initialization**: Start with the input matrix X as the initial activation (A_prev).\n2. **Iterate through each layer**: For each layer from 1 to L:\n   - Compute the linear step: \\( Z = W \\cdot A_{prev} + b \\)\n   - Apply the activation function: ReLU for hidden layers and sigmoid for the output layer.\n   - Store the cache for each layer, which includes the previous activation (A_prev), the linear output (Z), and the activation output (A).\n3. **Final Activation**: After processing all layers, the activation of the last layer (AL) is rounded to 4 decimal places and converted to a Python list.\n4. **Return**: The function returns a tuple containing the final activation (AL) and the list of caches.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]\n        A tuple containing the final activation AL and the list of caches.\n    \"\"\"\n    caches = []\n    A_prev = X\n    # Determine the number of layers\n    W_layers = [int(key[1:]) for key in parameters.keys() if key.startswith('W')]\n    L = max(W_layers) if W_layers else 0\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))\n        else:\n            A = np.maximum(0, Z)\n        caches.append((A_prev, Z, A))\n        A_prev = A\n    AL = A\n    AL_rounded = np.round(AL, 4)\n    AL_list = AL_rounded.tolist()\n    return (AL_list, caches)\n```\n\n### Explanation\n- **Initialization**: The input matrix X is used as the initial activation (A_prev).\n- **Linear Step**: For each layer, the linear combination of the previous activation and the current weights plus bias is computed.\n- **Activation Step**: ReLU is applied to all hidden layers, and sigmoid is applied to the output layer.\n- **Cache Storage**: Each layer's cache stores the previous activation, the linear output, and the activation output for use in backpropagation.\n- **Final Activation**: The output of the last layer is rounded and converted to a list before being returned along with the caches.\n\nThis approach ensures that we correctly compute the forward pass through the network and store the necessary information for efficient backpropagation."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost (log-loss) between the predicted probabilities and the ground-truth labels for a binary classifier. The binary cross-entropy cost is a measure of how well the predicted probabilities match the actual labels.\n\n### Approach\n1. **Clip Predicted Probabilities**: To avoid taking the logarithm of zero or one, we clip the predicted probabilities to a small interval [\u03b5, 1-\u03b5], where \u03b5 is 1e-15. This ensures that the values are within a valid range for logarithmic functions.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements (1 - predicted probabilities).\n3. **Compute Terms**: For each element, compute the terms involving the logarithms of the predicted probabilities and their complements, weighted by the ground-truth labels.\n4. **Sum and Average**: Sum all the computed terms and then average them by dividing by the number of samples.\n5. **Round Result**: Finally, round the result to six decimal places to get the final cost.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    # Clip A2 to avoid log(0) and log(1)\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    # Compute the logarithms\n    log_a = np.log(A2_clipped)\n    log_1a = np.log(1 - A2_clipped)\n    # Calculate each term\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1a\n    # Sum the terms\n    sum_terms = term1 + term2\n    sum_total = np.sum(sum_terms)\n    m = Y.size\n    # Compute the cost\n    J = (-1 / m) * sum_total\n    # Round to six decimal places\n    return round(J, 6)\n```\n\n### Explanation\n1. **Clipping**: The predicted probabilities are clipped to ensure they lie within a small range around 0 and 1, preventing invalid logarithm operations.\n2. **Logarithms**: The natural logarithm of the clipped probabilities and their complements are computed to handle the log terms in the cost function.\n3. **Terms Calculation**: Each term in the cost function is computed by multiplying the logarithm values with the corresponding ground-truth labels and their complements.\n4. **Summation and Averaging**: The terms are summed up and then averaged over the number of samples to get the final cost.\n5. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be 0 or 1."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the predictions from a Random Forest classifier using majority voting. Each decision tree in the forest makes its own prediction for every input sample, and the final prediction is determined by taking the majority vote across all trees. In case of a tie, the smallest label (lexicographically for strings, numerically for numbers) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions such that each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions together.\n2. **Count Votes**: For each sample, count the occurrences of each class label using a counter.\n3. **Determine the Winner**: Identify the class label with the highest count. If there is a tie, select the smallest label based on lexicographic order for strings or numerical order for numbers.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Transpose the predictions to process each sample's predictions together\n    transposed = zip(*predictions)\n    result = []\n    for sample_predictions in transposed:\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        # Collect all keys with the maximum count\n        candidates = [k for k, v in counts.items() if v == max_count]\n        # Select the smallest candidate\n        selected = min(candidates)\n        result.append(selected)\n    return result\n```\n\n### Explanation\n1. **Transposing Predictions**: The `zip(*predictions)` function is used to transpose the 2D list, converting rows of tree predictions into columns of sample predictions. This allows us to handle each sample's predictions together.\n2. **Counting Votes**: For each sample, we use `Counter` from the `collections` module to count how many times each class label appears across all trees.\n3. **Selecting the Winner**: After counting, we determine the class label with the highest count. If multiple labels have the same highest count, we use the `min` function to select the smallest label, ensuring the correct tie-breaking rule is applied.\n\nThis approach efficiently aggregates the predictions and handles ties correctly, ensuring the solution is both optimal and easy to understand."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function, which is a smooth approximation of the ReLU activation function. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation of this function can lead to numerical instability for very large or very small values of \\( z \\). To handle this, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept a scalar, a Python list, or a NumPy array. The output should be a float for scalar inputs and a Python list for array or list inputs.\n2. **Numerically Stable Computation**: Use `np.logaddexp(0.0, z)` to compute the softplus values. This function avoids overflow and underflow issues.\n3. **Rounding**: Each computed softplus value is rounded to 4 decimal places.\n4. **Output Formatting**: For scalar inputs, return a single float. For array or list inputs, return a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    sp = np.logaddexp(0.0, z)\n    sp_rounded = np.round(sp, 4)\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return float(sp_rounded)\n    else:\n        return sp_rounded.tolist()\n```\n\n### Explanation\n1. **Numerically Stable Calculation**: The function uses `np.logaddexp(0.0, z)` to compute the softplus values. This method is chosen because it avoids numerical issues that can arise from directly computing \\( e^z \\) for very large or very small values of \\( z \\).\n2. **Rounding**: The computed values are rounded to 4 decimal places using `np.round`.\n3. **Output Handling**: The function checks if the input is a scalar (int, float, or 0D array) and returns a float. For other inputs (lists or arrays), it returns a Python list using `tolist()`.\n\nThis approach ensures that the function handles all types of inputs correctly and efficiently, providing accurate results without numerical instability."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to factorize a given real-valued matrix using the regularized Alternating Least Squares (ALS) algorithm. The goal is to find two low-rank factor matrices that minimize a regularized Frobenius reconstruction loss. The algorithm alternates between optimizing these matrices until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with random initial values for the factor matrices W and H using a fixed random seed for reproducibility.\n2. **Iterative Optimization**: In each iteration, update one matrix while keeping the other fixed. Specifically, solve a regularized least squares problem for each matrix using the current estimate of the other matrix.\n3. **Loss Calculation**: Compute the regularized Frobenius loss after each update to check for convergence.\n4. **Stopping Condition**: Stop the iterations when the loss drops below a specified tolerance or the maximum number of iterations is reached.\n5. **Reconstruction**: After convergence, compute the reconstructed matrix using the final estimates of W and H, round it to four decimal places, and return it as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K) * 0.1\n    H = np.random.randn(K, M) * 0.1\n\n    def compute_loss(X, W, H, alpha):\n        recon = W @ H\n        loss = np.linalg.norm(X - recon)**2 + alpha * (np.linalg.norm(W)**2 + np.linalg.norm(H)**2)\n        return loss\n\n    current_loss = compute_loss(X, W, H, alpha)\n    if current_loss < tol:\n        pass  # Already converged\n    else:\n        for _ in range(max_iter):\n            # Update W\n            Ht = H.T\n            H_Ht = H @ Ht\n            A = H_Ht + alpha * np.eye(K)\n            b = X @ Ht\n            new_W = np.linalg.solve(A, b)\n\n            # Update H\n            Wt = new_W.T\n            Wt_W = Wt @ new_W\n            A = Wt_W + alpha * np.eye(K)\n            b = Wt @ X\n            new_H = np.linalg.solve(A, b)\n\n            # Update W and H\n            W, H = new_W, new_H\n\n            # Compute loss\n            current_loss = compute_loss(X, W, H, alpha)\n            if current_loss < tol:\n                break\n\n    # Compute the reconstructed matrix\n    X_hat = W @ H\n    X_hat_rounded = np.round(X_hat, 4)\n    return X_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrices W and H are initialized with small random values using a fixed seed to ensure reproducibility.\n2. **Loss Function**: The loss function combines the Frobenius norm of the difference between the original matrix and the product of the factor matrices, plus a regularization term.\n3. **Matrix Updates**: In each iteration, the matrices W and H are updated by solving regularized least squares problems. This involves computing the necessary matrix inverses to find the optimal updates.\n4. **Convergence Check**: After each update, the loss is computed and checked against the tolerance. If the loss is sufficiently small, the algorithm stops early.\n5. **Reconstruction**: The final reconstructed matrix is computed, rounded to four decimal places, and returned as a list of lists.\n\nThis approach ensures that the factorization is both efficient and numerically stable, leveraging matrix operations to solve the least squares problems at each step."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions to handle character-level sequence encoding and decoding. These functions will be used in a neural model to convert raw text into a fixed-length sequence of integer tokens, process the sequence through the model, and then decode the model's predictions back into human-readable text.\n\n### Approach\n1. **string_to_int**: This function converts each character of the input text into an integer using a provided vocabulary. The resulting list of integers is adjusted to a fixed length of 20, padding with zeros if the text is shorter and truncating if it is longer. Characters not found in the vocabulary are encoded as zeros.\n\n2. **int_to_string**: This function converts a list of integer tokens back into text using an inverse vocabulary. Padding tokens (zeros) are ignored during this conversion.\n\n3. **run_example**: This function orchestrates the entire process by encoding the input text, feeding it to the model, processing the model's predictions, and decoding the results back into a string.\n\n### Solution Code\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a text string into a fixed-length list of integer token ids.\"\"\"\n    # Convert each character to its corresponding integer, defaulting to 0 for unknown characters\n    encoded = [vocabulary.get(c, 0) for c in text]\n    # Adjust the length to match time_steps, padding with zeros or truncating as needed\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    # Filter out padding tokens and convert to characters\n    filtered = []\n    for idx in indices:\n        if idx != 0:\n            char = inverse_vocab.get(idx, '')\n            filtered.append(char)\n    return ''.join(filtered)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on encoded text and decodes the prediction.\"\"\"\n    # Encode the input text into a fixed-length sequence\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to a numpy array with shape (1, TIME_STEPS)\n    encoded_np = np.array(encoded, dtype=np.int32).reshape(1, -1)\n    # Get model predictions\n    predictions = model.predict(encoded_np)\n    # Use argmax to get the predicted token ids\n    predicted_ids = np.argmax(predictions, axis=-1).flatten()\n    # Decode the predicted ids into a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n1. **string_to_int**:\n   - Converts each character in the input text to an integer using the provided vocabulary.\n   - Ensures the resulting list is exactly 20 elements long by padding with zeros if the text is shorter or truncating if it is longer.\n\n2. **int_to_string**:\n   - Converts a list of integer tokens back into text using the inverse vocabulary.\n   - Ignores any padding tokens (zeros) during the conversion process.\n\n3. **run_example**:\n   - Encodes the input text into a fixed-length sequence.\n   - Feeds this sequence to the model to get predictions.\n   - Processes the model's predictions to extract the most probable token ids.\n   - Decodes these token ids back into a human-readable string.\n\nThis approach ensures that the text is properly encoded and decoded, handling both padding and truncation as needed, and efficiently processes the model's predictions to produce the final output."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the input examples and the output formatting, making it easier to quickly evaluate the model's performance on various test cases.\n\n### Approach\n1. **Function Parameters Handling**: The function `run_examples` takes four parameters: a model (a callable function), an input vocabulary dictionary, an inverse output vocabulary dictionary, and an optional examples iterable. If the examples iterable is not provided, the function falls back to a global constant `EXAMPLES`.\n2. **Example Processing**: For each input string in the examples, the function calls another helper function `run_example` to get the model's predictions. This helper function returns a list of characters, which are then concatenated into a single string.\n3. **Output Formatting and Collection**: Each input string and its corresponding prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    results = []\n    for example in examples:\n        output = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output)\n        print(f\"input: {example}\")\n        print(f\"output: {output_str}\")\n        results.append(output_str)\n    return results\n```\n\n### Explanation\n- **Function Parameters**: The function is designed to accept the model, input vocabulary, inverse output vocabulary, and an optional examples iterable. If examples are not provided, it uses a global constant `EXAMPLES`.\n- **Processing Each Example**: For each input string, the function calls `run_example` to get the model's predictions. The predictions are returned as a list of characters, which are then joined into a single string.\n- **Output Handling**: Each input string and its prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\nThis approach ensures that the function is both efficient and easy to use, allowing for quick evaluation of the model on multiple test cases."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for a logistic regression model. The function should handle numerical stability issues by ensuring predicted probabilities are within a safe range for logarithmic operations.\n\n### Approach\n1. **Compute the Linear Combination (z):** For each sample, compute the linear combination of features and weights, which is given by \\( z = X \\cdot w \\).\n2. **Compute the Sigmoid Function (p):** Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities:** To avoid numerical issues when taking logarithms, clip the probabilities to a small range around 0 and 1.\n4. **Compute the Loss:** Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p)] \\).\n5. **Compute the Gradient:** Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Return Results:** Return the loss rounded to 4 decimal places and the gradient as a list of lists, each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    y = y.ravel()\n    \n    loss = (-1 / m) * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    gradient = (1 / m) * X.T.dot(p - y)\n    gradient = gradient.reshape(-1, 1)\n    \n    loss_rounded = round(loss, 4)\n    gradient_rounded = np.round(gradient, 4).tolist()\n    \n    return (loss_rounded, gradient_rounded)\n```\n\n### Explanation\n1. **Linear Combination (z):** The linear combination of features and weights is computed using matrix multiplication, resulting in a vector of scores for each sample.\n2. **Sigmoid Function (p):** The scores are converted into probabilities using the sigmoid function, ensuring values are between 0 and 1.\n3. **Clipping Probabilities:** To avoid taking the logarithm of 0 or 1, probabilities are clipped to a small range around these values.\n4. **Loss Calculation:** The binary cross-entropy loss is computed by averaging the negative log-likelihood over all samples.\n5. **Gradient Calculation:** The gradient is computed using the formula involving the transpose of the feature matrix and the difference between predicted and actual probabilities.\n6. **Result Formatting:** The loss is rounded to 4 decimal places, and the gradient is converted to a list of lists with each element rounded to 4 decimal places for the final output."}
{"task_id": 140, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to navigate an agent through a maze while avoiding spreading fire. The maze is represented as an \\(n \\times n\\) grid where each cell can be a free cell, a wall, or on fire. The agent starts at the top-left corner and aims to reach the bottom-right corner. The fire spreads to adjacent cells each minute before the agent moves. The goal is to determine the minimum time required for the agent to reach the destination safely or determine if it's impossible.\n\n### Approach\n1. **Initial Checks**: \n   - If the starting or ending cell is on fire, return -1 immediately.\n   - If the grid is 1x1 and the cell is free, return 0 as the agent is already at the goal.\n\n2. **Precompute Fire Spread**:\n   - Use a multi-source BFS to compute the earliest time each cell catches fire. This helps in determining if a cell is safe for the agent to move into at a given time.\n\n3. **BFS for Agent Movement**:\n   - Use BFS to explore the agent's movement level by level, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival.\n\n### Solution Code\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    # Check if start or end is on fire at t=0\n    if grid[0][0] == 'F' or grid[-1][-1] == 'F':\n        return -1\n    # Check 1x1 case\n    if n == 1:\n        return 0\n    \n    # Precompute fire_time for each cell\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        i, j = q.popleft()\n        for dx, dy in dirs:\n            ni, nj = i + dx, j + dy\n            if 0 <= ni < n and 0 <= nj < n:\n                if fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    \n    # BFS for the agent's movement\n    visited = [[math.inf for _ in range(n)] for _ in range(n)]\n    visited[0][0] = 0\n    q = deque()\n    q.append((0, 0))\n    agent_dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    while q:\n        x, y = q.popleft()\n        current_time = visited[x][y]\n        # Check if reached the end\n        if x == n - 1 and y == n - 1:\n            return current_time\n        for dx, dy in agent_dirs:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] == '#':\n                    continue\n                if fire_time[nx][ny] <= current_time + 1:\n                    continue\n                if visited[nx][ny] > current_time + 1:\n                    visited[nx][ny] = current_time + 1\n                    q.append((nx, ny))\n    return -1\n```\n\n### Explanation\n1. **Initial Checks**: The function first checks if the starting or ending cell is on fire, returning -1 if true. It also handles the 1x1 grid case directly.\n\n2. **Fire Spread Calculation**: Using a multi-source BFS, the function calculates the earliest time each cell catches fire. This helps in determining the safety of each cell at any given time.\n\n3. **Agent Movement**: The agent's movement is explored using BFS, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival. The BFS ensures that the earliest time to reach the goal is found efficiently.\n\nThis approach ensures that the agent navigates the maze safely while avoiding the spreading fire, providing the minimum time required or determining if it's impossible to reach the goal."}
{"task_id": 141, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier. The goal is to predict the class of each test sample based on the majority vote of its k nearest neighbors in the training set. The distance between samples can be computed using one of three metrics: Euclidean, Manhattan, or Cosine. If an unknown metric is provided, we default to Euclidean.\n\n### Approach\n1. **Compute Distances**: For each test sample, compute the distance to every training sample using the specified metric. The distance metrics are:\n   - **Euclidean**: L2 distance, computed as the square root of the sum of squared differences.\n   - **Manhattan**: L1 distance, computed as the sum of absolute differences.\n   - **Cosine**: 1 minus the cosine similarity, computed as 1 minus the dot product of the vectors divided by the product of their magnitudes.\n\n2. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances. This is done efficiently using NumPy's `argpartition` to find the indices of the k smallest distances without fully sorting the distances.\n\n3. **Majority Vote with Tie-breaking**: For each test sample, collect the labels of the k nearest neighbors. Determine the most frequent label, and in case of a tie, select the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement the k-Nearest Neighbors classifier.\"\"\"\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    m = X_test.shape[0]\n    n = X_train.shape[0]\n    \n    # Compute the distance matrix based on the metric\n    if metric == 'euclidean':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        squared_diff = differences ** 2  # m x n x d\n        sum_squared = squared_diff.sum(axis=2)  # m x n\n        distances = np.sqrt(sum_squared)  # m x n\n    elif metric == 'manhattan':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        abs_diff = np.abs(differences)  # m x n x d\n        sum_abs = abs_diff.sum(axis=2)  # m x n\n        distances = sum_abs  # m x n\n    elif metric == 'cosine':\n        dot_products = X_test @ X_train.T  # m x n\n        norms_test = np.linalg.norm(X_test, axis=1)  # m x 1\n        norms_train = np.linalg.norm(X_train, axis=1)  # n x 1\n        cosine_similarities = dot_products / (norms_test[:, np.newaxis] * norms_train[np.newaxis, :])\n        distances = 1 - cosine_similarities  # m x n\n    \n    # Find the indices of the k nearest neighbors for each test sample\n    knn_indices = np.zeros((m, k), dtype=int)\n    for i in range(m):\n        dist_row = distances[i]\n        # Use argpartition to find the k smallest indices\n        k_smallest = np.argpartition(dist_row, k-1)[:k]\n        knn_indices[i] = k_smallest\n    \n    # Determine the predicted labels using majority vote with tie-breaking\n    y_knn = np.zeros(m, dtype=y_train.dtype)\n    for i in range(m):\n        indices = knn_indices[i]\n        y_group = y_train[indices]\n        unique_labels, label_counts = np.unique(y_group, return_counts=True)\n        max_count = np.max(label_counts)\n        candidates = unique_labels[label_counts == max_count]\n        predicted_label = np.min(candidates)\n        y_knn[i] = predicted_label\n    \n    return y_knn\n```\n\n### Explanation\n1. **Distance Calculation**: The function first computes the distance matrix between each test sample and training sample using the specified metric. This is done efficiently using NumPy's vectorized operations.\n\n2. **Finding Nearest Neighbors**: For each test sample, the function identifies the k nearest neighbors by finding the indices of the smallest distances using `argpartition`, which is more efficient than sorting for large datasets.\n\n3. **Majority Vote**: For each test sample, the function collects the labels of the k nearest neighbors and determines the most frequent label. In case of a tie, the smallest label is chosen.\n\nThis approach ensures that the function efficiently computes the predictions while handling different distance metrics and tie-breaking scenarios correctly."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a two-hidden-layer actor network in a reinforcement learning context. The network takes a state vector as input and outputs an action vector after passing through three layers with specific activation functions.\n\n### Approach\nThe network architecture consists of three layers:\n1. The first layer is a linear transformation followed by a ReLU activation.\n2. The second layer is another linear transformation followed by a ReLU activation.\n3. The third layer is a linear transformation followed by a tanh activation, which ensures the output values are within the range [-1, 1].\n\nThe steps to compute the forward pass are as follows:\n1. Convert the input state vector into a NumPy array to handle both list and array inputs uniformly.\n2. Compute the output of the first hidden layer by applying the linear transformation using the first weight matrix and bias vector, followed by the ReLU activation.\n3. Compute the output of the second hidden layer similarly using the second weight matrix and bias vector, followed by another ReLU activation.\n4. Compute the final action vector using the third weight matrix and bias vector, followed by the tanh activation.\n5. Round the resulting action vector to four decimal places and convert it to a list for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.asarray(state)\n    \n    # First hidden layer\n    h1 = np.dot(state, weights['W1']) + weights['b1']\n    h1_relu = np.maximum(0, h1)\n    \n    # Second hidden layer\n    h2 = np.dot(h1_relu, weights['W2']) + weights['b2']\n    h2_relu = np.maximum(0, h2)\n    \n    # Output layer\n    action = np.dot(h2_relu, weights['W3']) + weights['b3']\n    action_tanh = np.tanh(action)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_action = np.round(action_tanh, 4).tolist()\n    \n    return rounded_action\n```\n\n### Explanation\n1. **Input Handling**: The input state is converted to a NumPy array to ensure consistent handling of both list and array inputs.\n2. **First Hidden Layer**: The state vector is transformed using the first weight matrix and bias vector. The ReLU activation function is applied to introduce non-linearity.\n3. **Second Hidden Layer**: The output from the first hidden layer is transformed using the second weight matrix and bias vector, followed by another ReLU activation.\n4. **Output Layer**: The output from the second hidden layer is transformed using the third weight matrix and bias vector. The tanh activation function is applied to ensure the output values are within the range [-1, 1].\n5. **Rounding and Conversion**: The resulting action vector is rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the forward pass is computed efficiently using NumPy operations, adhering to the specified network architecture and activation functions."}
{"task_id": 165, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a measure of how many positions two vectors differ, normalized by the length of the vectors. This distance is widely used in various fields such as information theory and data science.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input vectors are the same. If they are not, we return -1 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty, the Hamming distance is 0.0 because there are no elements to compare.\n3. **Convert to NumPy Arrays**: If the inputs are lists, we convert them to NumPy arrays for efficient element-wise operations.\n4. **Compute Differences**: Using NumPy, we compute a boolean array where each element is True if the corresponding elements in the input vectors differ.\n5. **Sum Differences**: We sum the True values in the boolean array to get the count of differing elements.\n6. **Normalize and Round**: We normalize the count by dividing by the length of the vectors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    N = len(x)\n    \n    if N == 0:\n        return 0.0\n    \n    # Convert to NumPy arrays if they are lists\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Compute element-wise differences\n    diff = x != y\n    \n    # Sum the Trues (count of differing elements)\n    sum_diff = np.sum(diff)\n    \n    # Calculate the normalized Hamming distance\n    distance = sum_diff / N\n    \n    # Round to 4 decimal places\n    return round(distance, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the input vectors have the same length. If not, it returns -1.\n- **Empty Vectors**: If both vectors are empty, the function returns 0.0 since there are no differing elements.\n- **Conversion to Arrays**: The function converts lists to NumPy arrays to leverage efficient element-wise operations.\n- **Element-wise Comparison**: Using NumPy, the function creates a boolean array indicating where elements differ.\n- **Summing Differences**: The sum of the boolean array gives the count of differing elements.\n- **Normalization and Rounding**: The count is divided by the vector length to get the normalized Hamming distance, which is then rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently computes the normalized Hamming distance while handling various input types and edge cases appropriately."}
{"task_id": 171, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression classifier using mini-batch gradient descent. The goal is to train the model on a given training set and use it to predict labels for a test set. The model should handle any two distinct numeric labels and return predictions in the same format as the training labels.\n\n### Approach\n1. **Data Preprocessing**: Convert the input data into a format suitable for training. This includes adding an intercept term to the feature matrix to account for the bias in the model.\n2. **Label Mapping**: Map the given labels to binary values (0 and 1) for internal processing. This allows the model to work with any two distinct numeric labels.\n3. **Special Case Handling**: If all training labels are the same, the model can trivially predict that label for all test samples without training.\n4. **Model Initialization**: Initialize the weight vector to zeros. This vector will be updated during the training process.\n5. **Training with Mini-Batch Gradient Descent**: Divide the training data into mini-batches and update the weights iteratively using the gradient of the loss function. This approach helps in efficiently finding the minimum of the loss function.\n6. **Prediction**: After training, use the model to predict labels for the test set by applying the logistic function and using a decision threshold of 0.5. Convert the predictions back to the original label format before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Handle the case where all labels are the same\n    unique_labels = sorted(list(set(y_train)))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Create a mapping from original labels to 0 and 1\n    label_to_idx = {unique_labels[0]: 0, unique_labels[1]: 1}\n    y_train_mapped = [label_to_idx[label] for label in y_train]\n    \n    # Convert to numpy arrays and add intercept\n    X_train_np = np.array(X_train)\n    X_train_np = np.hstack([X_train_np, np.ones((X_train_np.shape[0], 1))])\n    \n    X_test_np = np.array(X_test)\n    X_test_np = np.hstack([X_test_np, np.ones((X_test_np.shape[0], 1))])\n    \n    # Initialize weights\n    n_features = X_train_np.shape[1]\n    w = np.zeros((n_features, 1))\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Shuffle the data\n        indices = np.random.permutation(len(X_train_np))\n        X_shuffled = X_train_np[indices]\n        y_shuffled = np.array(y_train_mapped)[indices]\n        \n        # Process in batches\n        for i in range(0, len(X_shuffled), batch_size):\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n            \n            # Compute the activation\n            z = np.dot(X_batch, w)\n            a = 1 / (1 + np.exp(-z))\n            \n            # Compute the gradient\n            gradient = np.dot(X_batch.T, (a - y_batch)) / len(X_batch)\n            \n            # Update the weights\n            w = w - learning_rate * gradient\n    \n    # Make predictions on the test set\n    z = np.dot(X_test_np, w)\n    a = 1 / (1 + np.exp(-z))\n    predicted_classes = (a >= 0.5).astype(int)\n    \n    # Map back to original labels\n    original_predictions = [unique_labels[pred] for pred in predicted_classes]\n    \n    return original_predictions\n```\n\n### Explanation\n1. **Data Preprocessing**: The feature matrices `X_train` and `X_test` are converted to numpy arrays and an intercept term is added to each row. This allows the model to learn the bias term.\n2. **Label Mapping**: The labels are mapped to binary values (0 and 1) to simplify the training process. After training, the predictions are converted back to the original label format.\n3. **Special Case Handling**: If all training labels are the same, the model returns that label for all test samples without training.\n4. **Model Initialization**: The weight vector is initialized to zeros, providing a starting point for the optimization process.\n5. **Training with Mini-Batch Gradient Descent**: The training data is shuffled and processed in mini-batches. For each batch, the gradient of the loss function is computed and used to update the weights. This process is repeated for the specified number of epochs.\n6. **Prediction**: After training, the model predicts labels for the test set by applying the logistic function and using a decision threshold of 0.5. The predictions are then converted back to the original label format and returned."}
{"task_id": 178, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as \\( f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2 \\) where each \\( x_i \\) is within the range \\([-1, 1]\\). The goal is to find the minimum value of this function using PSO.\n\n### Approach\n1. **Problem Analysis**: The Sphere function is a simple, continuous, and differentiable function used to test optimization algorithms. The PSO algorithm is a population-based optimization technique that simulates the social behavior of birds or fish. Each particle in the swarm has a position and velocity, and it updates its position based on its own best position and the best position of the entire swarm.\n\n2. **Algorithm Setup**: \n   - **Initialization**: Each particle starts with a random position within the specified bounds and a velocity of zero.\n   - **Velocity Update**: The velocity of each particle is updated using the inertia weight, cognitive weight, and social weight, along with random numbers.\n   - **Position Update**: The position of each particle is updated by adding the new velocity, ensuring it stays within the bounds.\n   - **Personal Best and Global Best**: Each particle keeps track of its best position (pbest), and the swarm keeps track of the best position found so far (gbest).\n\n3. **Iteration Process**: For each iteration, all particles update their velocities and positions. After updating, each particle checks if its new position is better than its personal best. The global best is updated if any particle's personal best is better than the current global best.\n\n4. **Termination**: The algorithm runs for a specified number of iterations, and the best value found is returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for invalid inputs\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize particles' positions, velocities, and personal bests\n    positions = rng.uniform(-1, 1, (num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    pbest = positions.copy()\n\n    # Calculate initial global best\n    initial_spheres = np.array([np.sum(p**2) for p in pbest])\n    initial_best_idx = np.argmin(initial_spheres)\n    gbest = pbest[initial_best_idx].copy()\n\n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Generate random numbers\n            r1 = rng.random()\n            r2 = rng.random()\n            \n            # Update velocity\n            velocities[i] = w * velocities[i] + c1 * r1 * (pbest[i] - positions[i]) + c2 * r2 * (gbest - positions[i])\n            \n            # Update position\n            new_position = positions[i] + velocities[i]\n            new_position_clipped = np.clip(new_position, -1, 1)\n            positions[i] = new_position_clipped\n            \n            # Update personal best if new position is better\n            new_sphere = np.sum(new_position_clipped**2)\n            current_pbest_sphere = np.sum(pbest[i]**2)\n            if new_sphere < current_pbest_sphere:\n                pbest[i] = new_position_clipped\n\n        # Update global best\n        pbest_spheres = np.array([np.sum(p**2) for p in pbest])\n        best_pbest_idx = np.argmin(pbest_spheres)\n        new_gbest = pbest[best_pbest_idx]\n        new_gbest_sphere = np.sum(new_gbest**2)\n        current_gbest_sphere = np.sum(gbest**2)\n        if new_gbest_sphere < current_gbest_sphere:\n            gbest = new_gbest\n\n    # Return the best value rounded to four decimal places\n    best_value = np.sum(gbest**2)\n    return round(best_value, 4)\n```\n\n### Explanation\n- **Initialization**: The particles' initial positions are randomly set within the bounds \\([-1, 1]\\). Velocities are initialized to zero.\n- **Velocity Update**: Each particle's velocity is updated using the inertia weight, cognitive weight, and social weight, along with random numbers to ensure exploration.\n- **Position Update**: The new position is calculated by adding the updated velocity to the current position, ensuring it stays within the bounds.\n- **Personal Best Update**: If a particle's new position yields a lower Sphere function value, it updates its personal best.\n- **Global Best Update**: After each iteration, the global best is updated if any particle's personal best is better than the current global best.\n- **Termination**: The algorithm runs for the specified number of iterations, and the best value found is returned, rounded to four decimal places."}
{"task_id": 180, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbours (k-NN) algorithm that can work both as a classifier and as a regressor. The function should take training feature vectors, their corresponding target values, test feature vectors, the number of neighbours to consider (k), and the task type (classification or regression). The goal is to predict the target values for the test feature vectors based on the k nearest neighbours in the training set.\n\n### Approach\n1. **Compute Distances**: For each test feature vector, compute the Euclidean distance to each training feature vector. Since we only need the relative distances, we can use the squared Euclidean distance to save computation time.\n2. **Sort and Select Neighbours**: For each test feature vector, sort the training vectors based on their computed distances and select the top k nearest vectors. If k is 0 or larger than the number of training vectors, use all training vectors.\n3. **Predict Based on Task**:\n   - **Classification**: Determine the most frequent label among the k nearest neighbours. In case of a tie, return the smallest label.\n   - **Regression**: Compute the arithmetic mean of the target values of the k nearest neighbours and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    predictions = []\n    n_train = len(X_train)\n    \n    # Determine the effective k\n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    \n    for test_point in X_test:\n        # Compute distances to all training points\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            # Calculate squared Euclidean distance\n            dist_sq = sum((test_point[i] - xi[i])**2 for i in range(len(test_point)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort(key=lambda x: x[0])\n        \n        # Select the first 'effective_k' neighbours\n        selected = distances[:effective_k]\n        y_neighbors = [y for (d, y) in selected]\n        \n        if task == \"classification\":\n            # Count the frequency of each label\n            counter = Counter(y_neighbors)\n            # Find the most common label(s)\n            max_count = max(counter.values())\n            # Get all labels with the maximum count\n            candidates = [label for label, count in counter.items() if count == max_count]\n            # Choose the smallest label in case of a tie\n            predicted_label = min(candidates)\n            predictions.append(predicted_label)\n        elif task == \"regression\":\n            # Compute the average and round to 4 decimal places\n            average = sum(y_neighbors) / len(y_neighbors)\n            predictions.append(round(average, 4))\n        else:\n            raise ValueError(\"Task must be 'classification' or 'regression'.\")\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: For each test point, the squared Euclidean distance to each training point is computed. This avoids the computational cost of taking square roots while maintaining the order of distances.\n2. **Neighbour Selection**: The distances are sorted, and the top k (or all if k is 0 or larger than the number of training points) nearest neighbours are selected.\n3. **Classification**: The most frequent label among the selected neighbours is determined. In case of a tie, the smallest label is chosen.\n4. **Regression**: The average of the target values of the selected neighbours is computed and rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently handles both classification and regression tasks while preserving the order of the test samples in the output."}
{"task_id": 184, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary decision tree classifier using the CART algorithm from scratch. The decision tree will use Gini impurity and recursive binary splitting to make predictions. The goal is to build a tree on the training data and then use it to predict labels for unseen test data.\n\n### Approach\n1. **Gini Impurity Calculation**: The Gini impurity measures the probability of misclassifying an element. For a node, it is calculated as 1 minus the sum of the squares of the probabilities of each class.\n2. **Tree Construction**: The tree is built recursively. Each node will either be a leaf or have left and right children. The best feature and threshold for splitting are determined by the maximum Gini gain.\n3. **Splitting Rule**: For each feature, sort the values and evaluate every midpoint between consecutive different values. The feature and threshold with the highest Gini gain are chosen for the split.\n4. **Stopping Criteria**: The tree stops splitting if all labels are identical, the maximum depth is reached, or a split produces an empty child.\n5. **Prediction**: Once the tree is built, each test sample is traversed through the tree to find the predicted class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    def gini(y):\n        count_0 = np.sum(y == 0)\n        count_1 = np.sum(y == 1)\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p0 = count_0 / n\n        p1 = count_1 / n\n        return 1 - (p0 ** 2 + p1 ** 2)\n\n    def build_tree(X, y, current_depth, max_depth):\n        if len(X) == 0:\n            return {'is_leaf': True, 'prediction': 0}\n        if len(np.unique(y)) == 1:\n            return {'is_leaf': True, 'prediction': y[0]}\n        if max_depth is not None and current_depth >= max_depth:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n        \n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_X = None\n        best_left_y = None\n        best_right_X = None\n        best_right_y = None\n        \n        n_features = X.shape[1]\n        for feature in range(n_features):\n            column = X[:, feature]\n            sorted_col = np.sort(column)\n            unique = np.unique(sorted_col)\n            if len(unique) == 1:\n                continue\n            for i in range(len(unique) - 1):\n                threshold = (unique[i] + unique[i + 1]) / 2\n                left_mask = column <= threshold\n                left_X = X[left_mask]\n                left_y = y[left_mask]\n                right_X = X[~left_mask]\n                right_y = y[~left_mask]\n                \n                G_parent = gini(y)\n                G_left = gini(left_y) if len(left_y) > 0 else 0\n                G_right = gini(right_y) if len(right_y) > 0 else 0\n                \n                n_total = len(y)\n                n_left = len(left_y)\n                n_right = len(right_y)\n                if n_total == 0:\n                    continue\n                weighted_G = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n                gain = G_parent - weighted_G\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_left_X = left_X\n                    best_left_y = left_y\n                    best_right_X = right_X\n                    best_right_y = right_y\n        \n        if best_gain > 0:\n            left_child = None\n            right_child = None\n            if len(best_left_X) == 0:\n                majority = np.bincount(y).argmax()\n                left_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                left_child = build_tree(best_left_X, best_left_y, current_depth + 1, max_depth)\n            \n            if len(best_right_X) == 0:\n                majority = np.bincount(y).argmax()\n                right_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                right_child = build_tree(best_right_X, best_right_y, current_depth + 1, max_depth)\n            \n            return {\n                'is_leaf': False,\n                'feature': best_feature,\n                'threshold': best_threshold,\n                'left': left_child,\n                'right': right_child\n            }\n        else:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n\n    def predict_sample(sample, tree):\n        if tree['is_leaf']:\n            return tree['prediction']\n        else:\n            feature = tree['feature']\n            threshold = tree['threshold']\n            if sample[feature] <= threshold:\n                return predict_sample(sample, tree['left'])\n            else:\n                return predict_sample(sample, tree['right'])\n\n    tree = build_tree(X_train, y_train, 0, max_depth)\n    predictions = []\n    for sample in X_test:\n        predictions.append(predict_sample(sample, tree))\n    return predictions\n```\n\n### Explanation\n1. **Gini Impurity Calculation**: The `gini` function calculates the Gini impurity for a given set of labels.\n2. **Tree Construction**: The `build_tree` function recursively constructs the tree. It checks for stopping criteria and finds the best feature and threshold to split on based on Gini gain.\n3. **Prediction**: The `predict_sample` function traverses the tree for a given sample to determine its predicted class label.\n4. **Main Function**: The `decision_tree_predict` function builds the tree using the training data and then uses it to predict labels for the test data.\n\nThis approach ensures that the decision tree is built deterministically based on the given rules and efficiently makes predictions for unseen data."}
{"task_id": 198, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to update the word\u2013topic matrix \u03b2 in Latent Dirichlet Allocation (LDA) based on the current variational parameters and the corpus. The goal is to compute the maximum-likelihood estimate of \u03b2 using the given variational parameters and then normalize each column to sum to one.\n\n### Approach\n1. **Initialization**: Create a matrix \u03b2 initialized to zero with dimensions (V, T), where V is the vocabulary size and T is the number of topics.\n2. **Accumulate Sums**: For each document and each token within the document, accumulate the contributions to \u03b2 using the current variational parameters. Specifically, for each token, if it corresponds to word v, add the variational parameter value for that token and topic to \u03b2[v][t].\n3. **Normalization**: After accumulating the sums, normalize each column of \u03b2 so that the sum of each column equals one. This ensures that each column represents a valid probability distribution over words for each topic.\n4. **Rounding**: Round each entry of the normalized matrix \u03b2 to four decimal places.\n5. **Conversion**: Convert the resulting matrix from a NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    if not phi:\n        return [[0.0 for _ in range(phi[0].shape[1])] for _ in range(V)]\n    \n    T = phi[0].shape[1]\n    beta = np.zeros((V, T), dtype=np.float64)\n    \n    for d in range(len(phi)):\n        current_phi = phi[d]\n        current_corpus = corpus[d]\n        for n in range(current_phi.shape[0]):\n            v = current_corpus[n]\n            beta[v] += current_phi[n, :]\n    \n    for t in range(T):\n        sum_t = beta[:, t].sum()\n        if sum_t == 0:\n            beta[:, t] = 0.0\n        else:\n            beta[:, t] /= sum_t\n    \n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrix \u03b2 is initialized to zero to store the accumulated sums for each word-topic pair.\n2. **Accumulation**: For each document and each token, the corresponding word index is used to update the \u03b2 matrix with the variational parameter values. This step aggregates the contributions of each token to the word-topic probabilities.\n3. **Normalization**: Each column of \u03b2 is normalized to ensure that the sum of probabilities for each topic equals one, which is necessary for a valid probability distribution.\n4. **Rounding**: The values in \u03b2 are rounded to four decimal places to meet the problem's requirements.\n5. **Conversion**: The resulting matrix is converted from a NumPy array to a list of lists for the final output, ensuring the correct format is returned.\n\nThis approach efficiently computes the updated word-topic distribution matrix \u03b2, ensuring it adheres to the constraints and requirements of the LDA model."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input containers `actual` and `predicted` into NumPy arrays to facilitate vectorized computations.\n2. **Compute Probability**: Using the sigmoid function, we compute the probability for each predicted value. The sigmoid function transforms any real number into a value between 0 and 1.\n3. **Compute Gradient**: The gradient of the logistic loss is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the element-wise product of `actual` and the sigmoid of the negative product of `actual` and `predicted`.\n4. **Compute Hessian**: The Hessian is the second derivative of the logistic loss and is computed as `sigmoid(predicted_i) * (1 - sigmoid(predicted_i))`.\n5. **Round Results**: Each of the computed gradient, Hessian, and probability values is rounded to six decimal places to meet the problem's requirements.\n6. **Convert to Lists**: Finally, the NumPy arrays are converted back to Python lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute the probability using the sigmoid function\n    prob = 1 / (1 + np.exp(-predicted))\n    \n    # Compute the gradient of the logistic loss\n    z = -actual * predicted\n    sigmoid_z = 1 / (1 + np.exp(z))\n    gradient = actual * sigmoid_z\n    \n    # Compute the Hessian of the logistic loss\n    hessian = prob * (1 - prob)\n    \n    # Round each component to six decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert numpy arrays to lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Probability Calculation**: The sigmoid function is used to transform the predicted values into probabilities. This is done using the formula `1 / (1 + exp(-predicted))`.\n- **Gradient Calculation**: The gradient is derived from the logistic loss function and is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the sigmoid function applied to the negative product of `actual` and `predicted`.\n- **Hessian Calculation**: The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Rounding and Conversion**: The results are rounded to six decimal places and converted from NumPy arrays to Python lists for the final output.\n\nThis approach ensures that the computations are efficient and accurate, leveraging vectorized operations provided by NumPy for optimal performance."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is used in neural networks to avoid the \"dying\" neurons issue. The function should handle inputs that are scalars, nested lists, or numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For a given input value \\( x \\), if \\( x \\) is non-negative, the output is \\( x \\).\n- If \\( x \\) is negative, the output is \\( a \\times x \\), where \\( a \\) is a small positive float (default value is 0.01).\n\nThe approach to solve this problem involves:\n1. **Handling Numpy Arrays**: If the input is a numpy array, apply the Leaky ReLU activation element-wise using vectorized operations for efficiency.\n2. **Handling Lists**: If the input is a list (which can be nested), recursively process each element. This ensures that the output maintains the same nested structure as the input.\n3. **Handling Scalars**: If the input is a scalar, directly apply the Leaky ReLU activation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(e, a) for e in z]\n    else:\n        if z >= 0:\n            return z\n        else:\n            return a * z\n```\n\n### Explanation\n1. **Numpy Array Handling**: The function first checks if the input `z` is a numpy array. If it is, the Leaky ReLU activation is applied element-wise using `np.where`, which efficiently processes the entire array. The result is then converted to a list using `tolist()` to maintain the required output format.\n2. **List Handling**: If `z` is a list, the function processes each element recursively. This ensures that each element, whether it is a scalar, another list, or a numpy array, is processed correctly, maintaining the nested structure of the input.\n3. **Scalar Handling**: If `z` is a scalar, the function directly applies the Leaky ReLU activation. If the scalar is non-negative, it is returned as is. If it is negative, it is multiplied by `a` before being returned.\n\nThis approach ensures that the function works efficiently for all types of inputs and maintains the structure of the input, providing the correct Leaky ReLU activation for each element."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using basic NumPy operations. The network architecture consists of three fully-connected layers followed by two separate output heads for the actor (policy) and the critic (state-value function). The weights and biases are fixed and identical to one, which simplifies the computation.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers (each with ReLU activation) followed by two output heads. The actor head uses a softmax activation to produce action probabilities, while the critic head uses a linear activation to produce a scalar state value.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0. This simplifies the computation as each layer's output can be derived directly from the input without complex weight matrices.\n3. **Computation Steps**:\n   - Compute the sum of the input state.\n   - Use this sum to compute the outputs of the hidden layers using ReLU activation.\n   - Compute the action probabilities using the actor head, which results in equal probabilities for each action.\n   - Compute the state value using the critic head, which is zero if the sum of the state is non-positive, otherwise, it is 64 times the sum of the state.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_s = sum(state)\n    action_probs = [round(1/3, 4), round(1/3, 4), round(1/3, 4)]\n    state_value = 64.0 * sum_s if sum_s > 0 else 0.0\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)\n```\n\n### Explanation\n1. **Sum of State**: The input state is a list of three floats. The sum of these floats is computed to determine the subsequent values.\n2. **Action Probabilities**: The actor head computes logits which are all equal, leading to equal probabilities for each action when applying the softmax function. These probabilities are rounded to four decimal places.\n3. **State Value**: The critic head computes the state value based on the sum of the state. If the sum is positive, the state value is 64 times the sum; otherwise, it is zero. The state value is rounded to four decimal places.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, given the fixed weights and biases."}
