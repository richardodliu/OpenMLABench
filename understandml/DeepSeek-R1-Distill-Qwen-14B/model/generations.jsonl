{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of standard Generalized Linear Model (GLM) link functions. Each link function is used to transform the expected value of the response variable, \u03bc, to the linear predictor, \u03b7. The dictionary should include three common link functions: identity, log, and logit, each with their respective inverse and derivative functions.\n\n### Approach\n1. **Identity Link Function**: This function simply returns the input value as it is. The inverse function is the same as the link function, and the derivative is 1.\n2. **Log Link Function**: This function computes the natural logarithm of the input. The inverse function is the exponential function, and the derivative is the reciprocal of the input.\n3. **Logit Link Function**: This function computes the logarithm of the odds of the input. The inverse function is the logistic function, and the derivative is the reciprocal of the product of the input and its complement.\n\nEach link function, its inverse, and its derivative are implemented as numpy-aware callables to handle both scalar and array inputs efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: 1\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1.0 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1.0 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1.0 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\n- **Identity Link**: The identity function is straightforward, returning the input as is. Its inverse is the same function, and the derivative is always 1.\n- **Log Link**: The log function transforms the input using the natural logarithm. The inverse transformation is the exponential function, and the derivative is the reciprocal of the input.\n- **Logit Link**: The logit function transforms the input using the logarithm of the odds. The inverse transformation is the logistic function, which maps any real number to a value between 0 and 1. The derivative of the logit function is the reciprocal of the product of the input and its complement.\n\nThis implementation ensures that each function works efficiently with both scalar values and numpy arrays, leveraging element-wise operations for array inputs."}
{"task_id": 4, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the pair-wise squared Euclidean distance matrix for a given set of samples. The goal is to return a matrix where each element represents the squared Euclidean distance between two samples, rounded to 4 decimal places.\n\n### Approach\nThe approach to solve this problem efficiently involves using vectorized operations to avoid explicit loops, ensuring the solution runs in O(n_samples\u00b2) time. Here's a detailed breakdown of the steps:\n\n1. **Check for Empty Input**: If the input array `X` is empty (i.e., it has zero samples), return an empty list immediately.\n\n2. **Compute Norms**: Calculate the squared norms of each sample. This is done by summing the squares of each element in the rows of `X`.\n\n3. **Compute Dot Product Matrix**: Using matrix multiplication, compute the dot product of each pair of samples. This results in a matrix where each element (i, j) is the dot product of the i-th and j-th samples.\n\n4. **Construct Distance Matrix**: Using the formula for squared Euclidean distance, construct the distance matrix `D` using the precomputed norms and dot product matrix. The formula used is:\n   \\[\n   D[i][j] = \\text{norms}[i] + \\text{norms}[j] - 2 \\times \\text{dot\\_product}[i][j]\n   \\]\n\n5. **Round and Convert to List**: Round each element of the distance matrix to 4 decimal places and convert the resulting matrix into a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    norms = np.sum(X ** 2, axis=1)\n    dot_product = X.dot(X.T)\n    D = norms[:, None] + norms[None, :] - 2 * dot_product\n    D = np.round(D, 4)\n    return D.tolist()\n```\n\n### Explanation\n- **Empty Input Check**: The function first checks if the input array `X` has zero samples. If so, it returns an empty list as there are no distances to compute.\n- **Norms Calculation**: The squared norms of each sample are computed using `np.sum(X ** 2, axis=1)`, which efficiently sums the squares of each element in the rows.\n- **Dot Product Matrix**: The dot product matrix is computed using `X.dot(X.T)`, which efficiently calculates the dot product of each pair of samples.\n- **Distance Matrix Construction**: Using the norms and dot product matrix, the distance matrix `D` is constructed using vectorized operations to ensure efficiency.\n- **Rounding and Conversion**: The distance matrix is rounded to 4 decimal places and converted into a list of lists for the final output.\n\nThis approach ensures that the solution is both efficient and concise, leveraging NumPy's vectorized operations to handle large datasets effectively."}
{"task_id": 6, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the outputs from multiple XGBoost trees and produce the final class predictions. The process involves summing the predictions from each tree, scaling them by the learning rate, applying the softmax function to convert these scores into probabilities, and then determining the predicted class label for each sample.\n\n### Approach\n1. **Sum Predictions**: Aggregate the predictions from all trees. Each tree's prediction is a 2D array, and we sum these arrays element-wise to get the total prediction for each class across all trees.\n2. **Scale by Learning Rate**: Multiply the summed predictions by the negative of the learning rate to get the raw score for each class.\n3. **Softmax Transformation**: Convert the raw scores into probability distributions using the softmax function. This function ensures that the probabilities sum to 1 for each sample.\n4. **Argmax for Class Labels**: For each sample, determine the class with the highest probability. In case of ties, the first occurrence is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        A list where each element is a 2-D array-like object of shape\n        (n_samples, n_classes) containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning rate used by the booster.  This value scales every tree\n        output before it is added to the ensemble score.\n\n    Returns\n    -------\n    list[int]\n        A list of length *n_samples* with the predicted class label for every\n        sample.\n    \"\"\"\n    # Sum all tree predictions\n    sum_predictions = np.sum(predictions, axis=0)\n    \n    # Compute raw scores by scaling with learning rate\n    raw_score = sum_predictions * (-learning_rate)\n    \n    # Apply softmax to get probabilities\n    exp = np.exp(raw_score)\n    sum_exp = exp.sum(axis=1, keepdims=True)\n    probabilities = exp / sum_exp\n    \n    # Find the predicted class for each sample\n    predicted_classes = np.argmax(probabilities, axis=1)\n    \n    return predicted_classes.tolist()\n```\n\n### Explanation\n1. **Summing Predictions**: The predictions from all trees are summed element-wise to get the total prediction for each class. This is done using `np.sum(predictions, axis=0)`.\n2. **Scaling by Learning Rate**: The summed predictions are scaled by the negative of the learning rate to get the raw score. This step is crucial as it adjusts the contribution of each tree's prediction according to the learning rate.\n3. **Softmax Transformation**: The raw scores are converted into probabilities using the softmax function. This is implemented by computing the exponential of each score, summing these exponentials for each sample, and then dividing each exponential by the sum to get the probabilities.\n4. **Determining Class Labels**: The class with the highest probability for each sample is determined using `np.argmax`. In case of ties, the first occurrence is chosen, ensuring consistent tie-breaking.\n\nThis approach efficiently aggregates the tree outputs and converts them into class predictions, following the steps used in the XGBoost algorithm."}
{"task_id": 8, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression model from scratch using batch gradient descent. The goal is to train the model on a given training dataset and then use the trained model to predict class labels for a test dataset.\n\n### Approach\n1. **Problem Analysis**: Binary logistic regression is a classification algorithm used to predict the probability of a binary outcome. The model uses a linear combination of features to compute a score, which is then passed through a sigmoid function to convert it into a probability. This probability is then rounded to the nearest integer to get the class label.\n\n2. **Initialization**: The model parameters (weights `w` and bias `b`) are initialized to zero. The weights are represented as a vector, and the bias is a scalar.\n\n3. **Gradient Descent**: The model is trained using batch gradient descent. For each iteration, the linear score `z` is computed using the current weights and bias. The sigmoid function is applied to `z` to get the activation values. The gradients for the weights and bias are computed based on the difference between the predicted activation values and the true labels. These gradients are used to update the weights and bias using the given learning rate.\n\n4. **Prediction**: After training the model, the learned weights and bias are used to compute the predictions for the test dataset. The linear score for each test sample is computed, followed by the sigmoid function to get probabilities, which are then rounded to get the final class labels.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        List with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n    # Get the number of samples and features\n    m = X_train.shape[0]\n    n_features = X_train.shape[1]\n    \n    # Initialize parameters\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    # Training loop\n    for _ in range(n_iterations):\n        # Compute linear scores\n        z = np.dot(X_train, w) + b\n        \n        # Apply sigmoid activation\n        A = 1 / (1 + np.exp(-z))\n        \n        # Compute gradients\n        gradient_w = (X_train.T.dot(A - y_train)) / m\n        gradient_b = (A - y_train).mean()\n        \n        # Update parameters\n        w = w - learning_rate * gradient_w\n        b = b - learning_rate * gradient_b\n    \n    # Predict on test data\n    z_test = np.dot(X_test, w) + b\n    A_test = 1 / (1 + np.exp(-z_test))\n    preds = np.round(A_test).astype(int).tolist()\n    \n    return preds\n```\n\n### Explanation\n- **Initialization**: The weights `w` are initialized to a zero vector of size equal to the number of features, and the bias `b` is initialized to zero.\n- **Training Loop**: For each iteration, the linear score `z` is computed using the current weights and bias. The sigmoid function is applied to `z` to get the activation values `A`. The gradients for the weights and bias are computed using the difference between `A` and the true labels `y_train`. These gradients are used to update the weights and bias.\n- **Prediction**: After training, the linear score for each test sample is computed using the learned weights and bias. The sigmoid function is applied to these scores to get probabilities, which are then rounded to the nearest integer to get the final class labels.\n\nThis approach ensures that we train a binary logistic regression model from scratch using batch gradient descent and use it to make predictions on a test dataset."}
{"task_id": 10, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies first-order exponential smoothing to a given set of inputs. This technique is used to smooth out time series data by giving more weight to recent observations and less weight to older ones. The function should handle both scalar and vector inputs and return the result rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function accepts three inputs: `prev`, `cur`, and `weight`. These can be scalars or one-dimensional arrays. We convert these inputs into numpy arrays to facilitate element-wise operations.\n2. **Broadcasting**: Using numpy's broadcasting capabilities, we ensure that the operations are performed element-wise even if the inputs have different shapes.\n3. **Smoothing Calculation**: Using the formula \\(\\tilde{X}_t = \\varepsilon\\,\\tilde{X}_{t-1} + (1-\\varepsilon)\\,X_t\\), we compute the smoothed values.\n4. **Rounding**: The result is rounded to four decimal places.\n5. **Return Type**: If the result is a scalar, it is returned as a float. If the result is a vector, it is returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (\\(\\tilde{X}_{t-1}\\)).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (\\(X_t\\)).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \\(\\varepsilon\\), 0 <= weight <= 1.  Scalars or one-\n        dimensional vectors are supported; vectors are used element-wise.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n    prev_arr = np.asarray(prev)\n    cur_arr = np.asarray(cur)\n    weight_arr = np.asarray(weight)\n    \n    smoothed = weight_arr * prev_arr + (1 - weight_arr) * cur_arr\n    rounded = np.round(smoothed, 4)\n    \n    if rounded.ndim == 0:\n        return rounded.item()\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The inputs `prev`, `cur`, and `weight` are converted into numpy arrays using `np.asarray()`. This handles both scalar and array-like inputs seamlessly.\n2. **Smoothing Calculation**: The formula for exponential smoothing is applied element-wise using numpy operations. This ensures that the function works efficiently even with vector inputs.\n3. **Rounding**: The result is rounded to four decimal places using `np.round()`.\n4. **Return Type Handling**: The function checks if the result is a scalar (0-dimensional array) or a vector (1-dimensional array). If it's a scalar, it returns a float; otherwise, it returns a list of floats.\n\nThis approach ensures that the function is both efficient and versatile, handling various input types and returning the result in the required format."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the binary logistic loss function along with its first and second derivatives (gradient and Hessian) with respect to the model logits. The goal is to compute the mean loss, gradient, and Hessian for given ground-truth labels and predicted logits.\n\n### Approach\n1. **Compute the Sigmoid Function**: The predicted probabilities are obtained using the sigmoid function applied to the logits. This ensures that the probabilities are between 0 and 1.\n2. **Clip Values to Avoid Log(0)**: To prevent taking the logarithm of zero, we clip the probabilities to a small range around 0 and 1.\n3. **Calculate the Loss**: The binary logistic loss is computed using the formula for cross-entropy loss, which involves the logarithm of the predicted probabilities.\n4. **Compute the Gradient**: The gradient of the loss with respect to the logits is given by the difference between the predicted probabilities and the ground-truth labels.\n5. **Compute the Hessian**: The Hessian matrix, which is the second derivative of the loss, is computed as the product of the predicted probabilities and their complements.\n6. **Round Results**: All results are rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimals.\n    \"\"\"\n    eps = 1e-15\n    p = 1 / (1 + np.exp(-y_pred))\n    p = np.clip(p, eps, 1 - eps)\n    \n    loss = - (y * np.log(p) + (1 - y) * np.log(1 - p))\n    mean_loss = np.mean(loss)\n    \n    gradient = p - y\n    hessian = p * (1 - p)\n    \n    mean_loss_rounded = round(mean_loss, 4)\n    gradient_rounded = [round(g, 4) for g in gradient]\n    hessian_rounded = [round(h, 4) for h in hessian]\n    \n    return (mean_loss_rounded, gradient_rounded, hessian_rounded)\n```\n\n### Explanation\n1. **Sigmoid Function**: The predicted probabilities `p` are computed using the sigmoid function, which transforms logits into probabilities.\n2. **Clipping Probabilities**: To avoid numerical instability when computing logarithms, we clip the probabilities to a small range around 0 and 1.\n3. **Loss Calculation**: The loss for each sample is computed using the cross-entropy formula, which combines the logarithm of the predicted probabilities with the ground-truth labels.\n4. **Gradient Calculation**: The gradient is the difference between the predicted probabilities and the ground-truth labels, indicating the direction of steepest ascent in the loss function.\n5. **Hessian Calculation**: The Hessian matrix, which is diagonal in this case, is computed as the product of the predicted probabilities and their complements, reflecting the curvature of the loss function.\n6. **Rounding**: All results are rounded to four decimal places to ensure precision and readability.\n\nThis approach efficiently computes the required metrics using vectorized operations, ensuring both accuracy and performance."}
{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate all contiguous N-grams from a given sequence. An N-gram is a sliding window of length N taken from consecutive elements of the sequence. The function should handle various types of sequences, including lists, tuples, and strings, and return the N-grams as a list of tuples.\n\n### Approach\n1. **Check Sequence Length**: If the length of the sequence is less than N, return an empty list because it's impossible to form any N-gram.\n2. **Iterate Through the Sequence**: For each possible starting index from 0 to the length of the sequence minus N, extract a slice of length N starting from that index.\n3. **Convert Slices to Tuples**: Each slice is converted into a tuple and added to the result list.\n4. **Return the Result**: After processing all possible starting indices, return the list of tuples.\n\nThis approach ensures that we efficiently generate all possible N-grams by leveraging Python's slicing capabilities and converting each slice into a tuple.\n\n### Solution Code\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    length = len(sequence)\n    if N > length:\n        return []\n    result = []\n    for i in range(length - N + 1):\n        gram = tuple(sequence[i:i+N])\n        result.append(gram)\n    return result\n```\n\n### Explanation\n- **Check Sequence Length**: The function first checks if the length of the sequence is less than N. If true, it returns an empty list immediately.\n- **Iterate Through Indices**: Using a loop, the function iterates from 0 to the length of the sequence minus N. This ensures that each slice of length N is taken from the sequence.\n- **Convert Slices to Tuples**: Each slice is converted into a tuple and added to the result list. This conversion handles different types of sequences uniformly, treating each element as an atomic token.\n- **Return Result**: Finally, the list of tuples is returned, containing all contiguous N-grams of the input sequence.\n\nThis solution efficiently handles various input types and ensures that the N-grams are generated correctly and returned in the order they appear in the original sequence."}
{"task_id": 17, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a tree from three parallel lists: `items`, `counts`, and `parents`. Each list corresponds to the nodes of a rooted tree, and we need to return the tree as a nested dictionary structure.\n\n### Approach\n1. **Identify the Root Node**: The root node is identified as the node whose parent index is -1. This is found by iterating through the `parents` list.\n2. **Build Children Structure**: Using the `parents` list, we construct a dictionary where each key is a parent index, and the value is a list of child indices. This helps in efficiently finding the children of each node.\n3. **Recursive Tree Construction**: Starting from the root node, we recursively build the tree structure. For each node, we create a dictionary with its item, count, and children. The children are added in the order they appear in the `parents` list.\n\n### Solution Code\n```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    if not items:\n        return {}\n    \n    # Find the root index\n    root_index = None\n    for i in range(len(parents)):\n        if parents[i] == -1:\n            root_index = i\n            break\n    \n    # Build the children dictionary\n    children_dict = {}\n    for i in range(len(parents)):\n        p = parents[i]\n        if p == -1:\n            continue\n        if p not in children_dict:\n            children_dict[p] = []\n        children_dict[p].append(i)\n    \n    # Function to build the tree recursively\n    def build_node(index):\n        node = {\n            'item': items[index],\n            'count': counts[index],\n            'children': []\n        }\n        if index in children_dict:\n            for child_index in children_dict[index]:\n                node['children'].append(build_node(child_index))\n        return node\n    \n    return build_node(root_index)\n```\n\n### Explanation\n1. **Root Identification**: The root node is identified by finding the index where the parent value is -1.\n2. **Children Dictionary**: This dictionary maps each parent index to a list of its child indices, ensuring the children are added in the order they appear in the `parents` list.\n3. **Recursive Construction**: Starting from the root, each node is constructed by recursively building its children. This ensures the tree structure is built correctly with the children in the required order.\n\nThis approach efficiently constructs the tree using a dictionary to map parent-child relationships and recursively builds the nested dictionary structure."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy array filled with zeros, with a specified shape and data type. The function should mimic the behavior of NumPy's `zeros` function but in a lightweight manner.\n\n### Approach\nThe function `zero` takes two arguments: `shape` and an optional keyword argument `dtype`. The `shape` can be an integer (for a 1-D array) or an iterable of integers (for a multi-dimensional array). The `dtype` specifies the data type of the resulting array, defaulting to `float` if not provided.\n\nThe approach involves:\n1. Extracting the `dtype` from the keyword arguments, defaulting to `float` if not provided.\n2. Converting the extracted `dtype` to a NumPy compatible type.\n3. Using NumPy's `zeros` function to create the array with the specified shape and dtype.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', None)\n    if dtype is None:\n        dtype = float\n    else:\n        dtype = np.dtype(dtype)\n    return np.zeros(shape, dtype=dtype)\n```\n\n### Explanation\n1. **Extracting `dtype`**: The function first checks if `dtype` is provided in the keyword arguments. If not, it defaults to `float`.\n2. **Converting `dtype`**: The extracted `dtype` is converted to a NumPy compatible type using `np.dtype`.\n3. **Creating the Array**: The function then calls `np.zeros` with the specified `shape` and `dtype`, returning the resulting array filled with zeros.\n\nThis approach ensures that the function handles both 1-D and multi-dimensional arrays correctly and efficiently, while also respecting the specified data type."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation and its gradient for a given input. The input can be a scalar, a list, or a NumPy array. The function should return a tuple containing the sigmoid values and their gradients, both rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should handle different types of input, including integers, floats, lists, and NumPy arrays. If the input is a scalar (int, float, or 0D NumPy array), the function processes it as a single value. For array-like inputs (lists or higher-dimensional NumPy arrays), the function processes each element individually.\n2. **Conversion to NumPy Array**: For array-like inputs, the function converts the input to a NumPy array to facilitate element-wise operations.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Gradient Calculation**: The gradient of the sigmoid function is computed using the formula \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n5. **Rounding**: Both the sigmoid values and their gradients are rounded to four decimal places.\n6. **Return Type**: If the input is a scalar, the function returns two floats. If the input is an array-like structure, the function returns two lists of the same shape, each element rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Handle scalar case, including 0D arrays\n        if isinstance(x, (int, float)):\n            x_scalar = x\n        else:\n            x_scalar = x.item()  # Extract scalar from 0D array\n        sigmoid = 1 / (1 + np.exp(-x_scalar))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Handle array-like case\n        if isinstance(x, list):\n            x_array = np.array(x)\n        else:\n            x_array = x\n        sigmoid = 1 / (1 + np.exp(-x_array))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = np.round(sigmoid, 4)\n        gradient_rounded = np.round(gradient, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a scalar or a 0D array. If so, it processes the input as a single value. Otherwise, it converts the input to a NumPy array.\n- **Sigmoid Calculation**: Using NumPy's exponential function, the sigmoid values are computed element-wise.\n- **Gradient Calculation**: The gradient is derived from the sigmoid function and computed using the derived formula.\n- **Rounding**: Both the sigmoid values and gradients are rounded to four decimal places using NumPy's rounding function.\n- **Return Type**: Depending on whether the input was a scalar or an array-like structure, the function returns the results as floats or lists, respectively.\n\nThis approach ensures that the function works efficiently for various input types and returns the results in the required format."}
{"task_id": 22, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a synthetic random tensor with specific properties and optionally standardize it. The tensor is created by adding an offset, drawn from a uniform distribution of integers, to a uniform random number. If standardization is requested, each column of the tensor is adjusted to have a mean of 0 and a standard deviation of 1.\n\n### Approach\n1. **Generate Offset and Uniform Values**: \n   - The offset for each element is generated using a uniform distribution of integers from -300 to 299.\n   - A uniform random number from the interval [0, 1) is added to each offset to form the initial tensor.\n\n2. **Standardization**:\n   - If the `standardize` flag is set to `True`, each column of the tensor is standardized. This involves computing the mean and standard deviation of each column, then adjusting each element in the column to have a mean of 0 and a standard deviation of 1. A tiny constant is added to the standard deviation to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    offset = np.random.randint(-300, 300, size=shape)\n    u = np.random.uniform(0.0, 1.0, size=shape)\n    tensor = offset + u\n    if standardize:\n        means = tensor.mean(axis=0)\n        stds = tensor.std(axis=0)\n        epsilon = np.finfo(float).eps\n        tensor = (tensor - means) / (stds + epsilon)\n    return tensor\n```\n\n### Explanation\n1. **Offset Generation**: The offset is generated using `np.random.randint(-300, 300, size=shape)`, which creates an array of integers uniformly distributed between -300 and 299.\n2. **Uniform Random Values**: The uniform random values are generated using `np.random.uniform(0.0, 1.0, size=shape)`, producing an array of values in the range [0, 1).\n3. **Tensor Formation**: The initial tensor is formed by adding the offset and uniform values.\n4. **Standardization**: If `standardize` is `True`, each column's mean and standard deviation are computed. The tensor is then adjusted by subtracting the mean and dividing by the standard deviation (plus a tiny constant to avoid division by zero), ensuring each column has a mean of 0 and a standard deviation of 1.\n\nThis approach efficiently generates the required tensor and ensures it meets the specified properties, whether or not standardization is applied."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single update step of the Nesterov Accelerated Gradient (NAG) optimization algorithm. The goal is to update the parameters and velocity of the model based on the given gradient function, learning rate, and momentum coefficient.\n\n### Approach\nThe Nesterov Accelerated Gradient (NAG) algorithm is an optimization technique that combines the ideas of gradient descent and momentum. The key steps involved in the NAG update are as follows:\n\n1. **Look-ahead Point Calculation**: Compute a look-ahead point which is used to evaluate the gradient. This point is calculated as the current parameter vector minus the momentum multiplied by the current velocity vector.\n2. **Gradient Evaluation**: Evaluate the gradient of the objective function at the look-ahead point. Each element of the gradient is then clipped to ensure it lies within the range [-1, 1].\n3. **Velocity Update**: Update the velocity vector using the momentum coefficient, the current velocity, and the clipped gradient.\n4. **Parameter Update**: Update the parameter vector using the new velocity vector.\n\nThe algorithm ensures that the updates are more efficient by incorporating the look-ahead point, which helps in reducing the oscillations during the optimization process.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    # Convert w and velocity to numpy arrays\n    w = np.array(w)\n    if not velocity:\n        velocity = np.zeros_like(w)\n    else:\n        velocity = np.array(velocity)\n    \n    # Compute look-ahead point\n    w_tilde = w - momentum * velocity\n    \n    # Compute gradient at w_tilde\n    g = grad_func(w_tilde)\n    \n    # Clip gradient\n    g_clipped = np.clip(g, -1, 1)\n    \n    # Update velocity\n    new_velocity = momentum * velocity + learning_rate * g_clipped\n    \n    # Update parameters\n    new_w = w - new_velocity\n    \n    # Round and convert to lists\n    new_w_rounded = np.round(new_w, 4).tolist()\n    new_velocity_rounded = np.round(new_velocity, 4).tolist()\n    \n    return (new_w_rounded, new_velocity_rounded)\n```\n\n### Explanation\n1. **Convert Parameters to Arrays**: The current parameter vector `w` and velocity vector `velocity` are converted to numpy arrays for easier manipulation. If the velocity is empty, it is initialized to a zero vector of the same shape as `w`.\n2. **Look-ahead Point Calculation**: The look-ahead point `w_tilde` is computed using the formula `w - momentum * velocity`.\n3. **Gradient Evaluation**: The gradient of the objective function at `w_tilde` is computed and then element-wise clipped to ensure it lies within the range [-1, 1].\n4. **Velocity Update**: The new velocity vector is computed using the formula `momentum * velocity + learning_rate * clipped_gradient`.\n5. **Parameter Update**: The new parameter vector is computed by subtracting the new velocity vector from the current parameter vector.\n6. **Rounding and Conversion**: The updated parameter and velocity vectors are rounded to four decimal places and converted to Python lists before being returned.\n\nThis approach ensures that the NAG update step is efficiently computed and handles both initial and subsequent calls correctly."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian kernel. The SVM decision function is given, and we need to compute the predicted labels efficiently using vectorized operations.\n\n### Approach\n1. **Compute Kernel Matrix**: For each test sample, compute the Gaussian kernel values with all training samples. This is done efficiently using vectorized operations to avoid loops.\n2. **Sum Contributions**: Multiply each kernel value by the corresponding Lagrange multiplier and class label, then sum these contributions for each test sample.\n3. **Add Bias Term**: Add the bias term to the computed sum for each test sample.\n4. **Determine Predictions**: Use the sign of the resulting values to determine the predicted class labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Reshape the training and test data for broadcasting\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    \n    X_train_reshaped = X_train[:, np.newaxis, :]  # Shape: (n, 1, d)\n    X_test_reshaped = X_test[np.newaxis, :, :]    # Shape: (1, m, d)\n    \n    # Compute the squared differences\n    diff = X_test_reshaped - X_train_reshaped      # Shape: (n, m, d)\n    squared_norms = np.sum(diff ** 2, axis=2)     # Shape: (n, m)\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * squared_norms)            # Shape: (n, m)\n    \n    # Compute the sum of alpha_i * y_i * K[i,j] for each test sample j\n    sum_values = (alpha * y_train).dot(K.T)       # Shape: (m,)\n    \n    # Add the bias term\n    g_z = sum_values + b\n    \n    # Determine the predicted labels\n    predictions = np.where(g_z >= 0, 1, -1)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Reshape Data**: The training and test data are reshaped to facilitate broadcasting, allowing efficient computation of pairwise differences.\n2. **Compute Differences**: The differences between each test sample and each training sample are computed, and the squared Euclidean norms are summed.\n3. **Gaussian Kernel**: The Gaussian kernel values are computed using the squared norms.\n4. **Sum Contributions**: The contributions from each training sample are summed for each test sample using vectorized operations.\n5. **Bias and Sign**: The bias term is added, and the sign of the result determines the predicted class label.\n\nThis approach ensures that the solution is efficient and scalable, handling large datasets using vectorized operations without relying on external libraries."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function along with its gradient. The ReLU function is defined as \\( \\text{ReLU}(x) = \\max(0, x) \\) and its gradient is 1 for non-negative values of \\( x \\) and 0 otherwise. The goal is to compute both the activation and the gradient efficiently and return them in the same shape as the input.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is a possibly nested list, which we convert into a NumPy array for efficient element-wise operations.\n2. **Compute Activation**: Using NumPy's `maximum` function, we apply the ReLU transformation element-wise.\n3. **Compute Gradient**: Using NumPy's `where` function, we create a mask where elements are 1 if the corresponding element in the input is non-negative and 0 otherwise.\n4. **Convert Back to Lists**: Convert the resulting NumPy arrays back to Python lists while preserving the original nested structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(0, x_array)\n    gradient = np.where(x_array >= 0, 1, 0)\n    return (activation.tolist(), gradient.tolist())\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted to a NumPy array to leverage efficient element-wise operations.\n2. **ReLU Activation**: The `np.maximum(0, x_array)` function is used to compute the ReLU activation, which sets all negative values to zero.\n3. **Gradient Calculation**: The `np.where` function is used to create a mask where elements are 1 if the corresponding element in the input is non-negative and 0 otherwise.\n4. **Conversion to Lists**: The resulting NumPy arrays are converted back to Python lists using the `tolist()` method, ensuring the output maintains the same nested structure as the input.\n\nThis approach efficiently handles the computation of both the activation and the gradient, ensuring the results are returned in the correct format."}
{"task_id": 32, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a decorator that sanitizes input for binary numeric operations. The goal is to ensure that the inputs are valid numbers (int, float, or numeric strings) and then perform the required operation, returning the result in the appropriate numeric type.\n\n### Approach\n1. **Input Sanitization**: Create a helper function `check_data` that converts input values to numeric types. This function will:\n   - Accept two arguments, which can be integers, floats, or numeric strings.\n   - Convert any string to a float.\n   - Leave integers and floats unchanged.\n   - Return the converted values as a tuple.\n\n2. **Decorator Function**: Implement a decorator `validate_input` that wraps a function to ensure its inputs are sanitized using `check_data`.\n\n3. **Sum Function**: Create a function `sum_numbers` that uses the decorator to sanitize inputs and then returns their sum. The result should be an integer if the sum is a whole number, otherwise a float.\n\n### Solution Code\n```python\ndef check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    The function must accept integers, floats, or numeric strings.  Any string\n    should be converted with `float()`; existing numeric values stay as they\n    are.  The pair `(a, b)` must then be returned.\n    \"\"\"\n    if isinstance(a, str):\n        a = float(a)\n    if isinstance(b, str):\n        b = float(b)\n    return (a, b)\n\ndef validate_input(function):\n    def wrapper(a, b):\n        a, b = check_data(a, b)\n        return function(a, b)\n    return wrapper\n\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\"\"\"\n    total = a + b\n    if isinstance(total, float) and total.is_integer():\n        return int(total)\n    else:\n        return total\n```\n\n### Explanation\n1. **check_data Function**:\n   - This function takes two arguments, `a` and `b`.\n   - If either argument is a string, it converts it to a float using `float()`.\n   - It returns the converted values as a tuple, ensuring both values are numeric.\n\n2. **validate_input Decorator**:\n   - This decorator wraps a function, ensuring its inputs are validated and converted using `check_data`.\n   - The wrapper function calls `check_data` on the inputs and then passes the results to the original function.\n\n3. **sum_numbers Function**:\n   - This function takes two sanitized inputs, computes their sum, and returns the result.\n   - If the sum is a whole number, it returns an integer; otherwise, it returns a float.\n\nThis approach ensures that the inputs are correctly sanitized and the result is returned in the appropriate numeric type, meeting the problem requirements efficiently."}
{"task_id": 35, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that traverses a decision tree and makes a prediction based on the given sample. The decision tree is represented using a lightweight node class, and the traversal must follow specific rules to determine the prediction or identify if the sample cannot be classified.\n\n### Approach\n1. **Start at the Root Node**: Begin the traversal from the root node of the decision tree.\n2. **Traverse Non-Leaf Nodes**: While the current node is not a leaf (i.e., it has a feature index to split on):\n   - Check if the feature index is valid within the bounds of the sample list. If not, return -1.\n   - Extract the feature value from the sample using the feature index.\n   - Check if this feature value exists in the current node's children. If not, return -1.\n   - Move to the corresponding child node based on the feature value.\n3. **Leaf Node Handling**: When a leaf node is reached, return its result. If the traversal fails at any step, return -1.\n\n### Solution Code\n```python\nfrom typing import Any\n\nclass node:\n    \"\"\"A minimal tree node for decision-tree-like structures.\n\n    Attributes:\n        fea: int \u2013 index of the feature to split on. A value of -1 marks a leaf.\n        res: Any \u2013 label stored in the leaf. None for internal nodes.\n        child: dict \u2013 mapping from feature value to the corresponding sub-tree.\n    \"\"\"\n\n    def __init__(self, fea: int = -1, res: Any | None = None, child: dict | None = None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\n\n\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\n\n    Args:\n        root: The root node of the decision tree.\n        sample: A list of feature values representing one data point.\n\n    Returns:\n        The label stored in the reached leaf, or -1 if the sample cannot be\n        classified.\n    \"\"\"\n    current = root\n    while current.fea != -1:\n        fea = current.fea\n        if fea < 0 or fea >= len(sample):\n            return -1\n        val = sample[fea]\n        if val not in current.child:\n            return -1\n        current = current.child[val]\n    return current.res\n```\n\n### Explanation\n- **Initialization**: The function starts at the root node and initializes the current node to the root.\n- **Loop Through Non-Leaf Nodes**: The loop continues as long as the current node is not a leaf. For each non-leaf node, it checks the validity of the feature index, retrieves the feature value, and checks if the value exists in the current node's children.\n- **Check Validity**: If any check fails (invalid feature index or missing feature value), the function returns -1, indicating the sample cannot be classified.\n- **Leaf Node**: When a leaf node is reached, the function returns the result stored in the leaf node.\n\nThis approach ensures that the function efficiently traverses the decision tree and correctly handles both successful classifications and cases where the sample cannot be classified."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the outcome for a batch of samples using a binary decision tree. The decision tree is represented using a `Node` class where each node either splits the data based on a feature and a threshold or predicts a value if it is a leaf node.\n\n### Approach\n1. **Check for an Empty Tree**: If the root node is `None`, return an empty list as there are no predictions to make.\n2. **Iterate Through Each Sample**: For each sample in the input list, traverse the decision tree starting from the root node.\n3. **Traverse the Tree**: For each node, check if it is a leaf node. If it is, record the prediction value. If it is not, use the feature and threshold to decide whether to move to the left or right child.\n4. **Leaf Node Check**: A node is considered a leaf if both its left and right children are `None`. This can be efficiently checked by verifying if the left child is `None`.\n5. **Collect Predictions**: For each sample, after reaching a leaf node, collect the prediction value and append it to the result list.\n\n### Solution Code\n```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node:   rule = (feature_index, threshold)\n    Leaf node:       rule = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\n\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"Predicts outcomes for a batch of samples using a binary decision tree.\"\"\"\n    if root is None:\n        return []\n    predictions = []\n    for sample in X:\n        current = root\n        while current.left is not None:\n            feature = current.feature\n            threshold = current.threshold\n            if sample[feature] <= threshold:\n                current = current.left\n            else:\n                current = current.right\n        predictions.append(current.threshold)\n    return predictions\n```\n\n### Explanation\n- **Tree Structure**: Each node in the tree either splits the data based on a feature and a threshold or is a leaf node that provides a prediction.\n- **Traversal Logic**: Starting from the root, for each node, if it is not a leaf, the feature value of the sample is compared with the node's threshold to decide the next node to visit.\n- **Leaf Node Handling**: When a leaf node is reached, the prediction value stored in the node's threshold is recorded.\n- **Efficiency**: The approach efficiently processes each sample in linear time relative to the depth of the tree, ensuring that each prediction is made in a timely manner.\n\nThis solution ensures that each sample is processed correctly and efficiently, leveraging the structure of the decision tree to make accurate predictions."}
{"task_id": 38, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the arm-selection phase of the LinUCB algorithm for a disjoint contextual linear bandit. The goal is to select the arm that maximizes the LinUCB upper confidence bound, which balances the trade-off between exploitation and exploration.\n\n### Approach\nThe LinUCB algorithm selects the arm with the highest upper confidence bound, which is calculated using the formula:\n\\[ p_a = \\hat{\\theta}_a^\\top \\mathbf{c}_a + \\alpha \\sqrt{\\mathbf{c}_a^\\top A_a^{-1} \\mathbf{c}_a} \\]\nwhere:\n- \\(\\hat{\\theta}_a = A_a^{-1} \\mathbf{b}_a\\) is the estimated parameter vector for arm \\(a\\).\n- \\(\\mathbf{c}_a\\) is the context vector of arm \\(a\\).\n- \\(A_a\\) is the Gram matrix and \\(\\mathbf{b}_a\\) is the covariance vector for arm \\(a\\).\n- \\(\\alpha\\) is the exploration coefficient.\n\nThe steps to implement this are as follows:\n1. For each arm, extract its context vector from the given context matrix.\n2. Compute the inverse of the Gram matrix \\(A_a\\) for each arm.\n3. Calculate the estimated parameter vector \\(\\hat{\\theta}_a\\) using the inverse of the Gram matrix and the covariance vector.\n4. Compute the two terms of the LinUCB score: the inner product of \\(\\hat{\\theta}_a\\) and the context vector, and the exploration term involving the inverse of the Gram matrix.\n5. Sum these two terms to get the LinUCB score for each arm.\n6. Select the arm with the highest score, breaking ties by choosing the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\n\n    Parameters\n    ----------\n    context : np.ndarray\n        Matrix of shape (D, n_arms) containing the D-dimensional feature\n        vectors of every arm for the current round.\n    A : list\n        List where ``A[a]`` is the (D\u00d7D) Gram matrix of arm *a*.\n    b : list\n        List where ``b[a]`` is the length-D accumulated reward-context vector\n        of arm *a*.\n    alpha : float\n        Exploration coefficient (> 0).\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest LinUCB score.\n    \"\"\"\n    n_arms = context.shape[1]\n    p_values = []\n    for a in range(n_arms):\n        c_a = context[:, a]\n        inv_A_a = np.linalg.inv(A[a])\n        theta_hat_a = inv_A_a.dot(b[a])\n        term1 = np.dot(theta_hat_a, c_a)\n        temp = inv_A_a.dot(c_a)\n        term2 = alpha * np.sqrt(np.dot(c_a, temp))\n        p_a = term1 + term2\n        p_values.append(p_a)\n    max_index = np.argmax(p_values)\n    return max_index\n```\n\n### Explanation\n1. **Extract Context Vector**: For each arm, extract its context vector from the context matrix.\n2. **Matrix Inversion**: Compute the inverse of the Gram matrix \\(A_a\\) for each arm. This inverse is used to estimate the parameter vector and compute the exploration term.\n3. **Parameter Estimation**: Calculate the estimated parameter vector \\(\\hat{\\theta}_a\\) using the inverse of the Gram matrix and the covariance vector.\n4. **Score Calculation**: Compute the two terms of the LinUCB score. The first term is the inner product of the estimated parameter vector and the context vector. The second term involves the exploration coefficient and the square root of the context vector's quadratic form with the inverse of the Gram matrix.\n5. **Select Arm**: Sum the two terms to get the LinUCB score for each arm, then select the arm with the highest score. In case of a tie, the smallest index is chosen.\n\nThis approach ensures that we balance between exploiting the arm with the highest current estimate and exploring arms with higher uncertainty, as quantified by the LinUCB upper confidence bound."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that breaks a one-dimensional signal into overlapping frames using NumPy. The function should return a view on the original signal, not a copy, and handle various edge cases and pre-conditions.\n\n### Approach\n1. **Pre-Conditions Check**: Ensure the input signal is one-dimensional, the stride is positive, the frame width is positive, and the signal length is at least as long as the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Create View Using Stride Tricks**: Use `numpy.lib.stride_tricks.as_strided` to create a view of the original array with the desired shape and strides. The strides are calculated based on the frame width and stride.\n4. **Set Writeable Flag**: Adjust the writeable flag of the result based on the input parameter to ensure the result is either read-only or inherits the writeability of the original array.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    assert x.ndim == 1, \"x must be 1D\"\n    assert stride >= 1, \"stride must be positive\"\n    assert frame_width >= 1, \"frame_width must be positive\"\n    n = x.size\n    assert n >= frame_width, \"x must be at least frame_width long\"\n\n    # Calculate number of frames\n    n_frames = (n - frame_width) // stride + 1\n\n    # Calculate shape and strides\n    shape = (n_frames, frame_width)\n    strides = (stride * x.itemsize, x.itemsize)\n\n    # Create the view using as_strided\n    result = as_strided(x, shape=shape, strides=strides)\n\n    # Set writeable flag\n    if writeable:\n        result.flags.writeable = x.flags.writeable\n    else:\n        result.flags.writeable = False\n\n    return result\n```\n\n### Explanation\n1. **Pre-Conditions Check**: The function starts by verifying that the input signal is one-dimensional, the stride and frame width are positive, and the signal length is sufficient to form at least one frame.\n2. **Number of Frames Calculation**: The number of frames is computed using integer division to ensure we only include complete frames.\n3. **View Creation**: Using `as_strided`, the function creates a view of the original array with the specified shape and strides. This ensures the result is a view rather than a copy, optimizing memory usage.\n4. **Writeable Flag Adjustment**: The writeable flag of the result is set based on the input parameter, ensuring the result is either read-only or writeable as required.\n\nThis approach efficiently handles the problem requirements and constraints, providing a solution that is both correct and optimized for performance."}
{"task_id": 41, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbours (k-NN) classifier. The goal is to predict the class label for each query sample based on the majority vote among its k closest neighbours in the training set, using the Euclidean distance as the measure of closeness.\n\n### Approach\n1. **Convert Data to NumPy Arrays**: Convert the training data into a NumPy array for efficient computation.\n2. **Compute Distances**: For each query sample, compute the Euclidean distance to each training sample.\n3. **Sort by Distance**: Sort the training samples based on the computed distances.\n4. **Select Top k Neighbours**: Select the top k closest training samples. If k is larger than the number of training samples, use all samples.\n5. **Majority Vote**: Count the frequency of each class label among the selected top k samples. The class with the highest frequency is chosen as the prediction. In case of a tie, the lexicographically smallest label is selected.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_data: list[list[float]], y_data: list, X: list[list[float]], k: int = 5) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    # Convert X_data to a numpy array for efficient operations\n    X_data_np = np.array(X_data)\n    predictions = []\n    \n    for x in X:\n        # Convert query sample to a numpy array\n        x_np = np.array(x)\n        # Compute the Euclidean distances to all training samples\n        distances = np.sqrt(np.sum((X_data_np - x_np)**2, axis=1))\n        # Get the indices of the training samples sorted by distance\n        sorted_indices = np.argsort(distances)\n        # Take the first k indices\n        top_k_indices = sorted_indices[:k]\n        # Collect the corresponding labels\n        y_top_k = [y_data[i] for i in top_k_indices]\n        # Count the occurrences of each label\n        counter = Counter(y_top_k)\n        # Find the maximum count\n        max_count = max(counter.values(), default=0)\n        # Collect all labels with the maximum count\n        candidates = [label for label, count in counter.items() if count == max_count]\n        # Determine the lexicographically smallest label\n        predicted_label = min(candidates)\n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation\n1. **Data Conversion**: The training data is converted into a NumPy array to leverage efficient vectorized operations.\n2. **Distance Calculation**: For each query sample, the Euclidean distance to each training sample is computed using NumPy operations, which are efficient and concise.\n3. **Sorting and Selection**: The distances are sorted, and the indices of the closest k samples are selected. This ensures we only consider the most relevant samples for our prediction.\n4. **Majority Vote with Tie-breaking**: The labels of the selected samples are counted, and the most frequent label is chosen. In case of a tie, the lexicographically smallest label is selected to ensure deterministic results.\n\nThis approach efficiently handles the problem constraints and ensures accurate predictions using the k-NN algorithm."}
{"task_id": 42, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) for two given lists of non-negative numbers. RMSLE is a popular evaluation metric for regression models, especially when the target values span several orders of magnitude and cannot be negative.\n\n### Approach\n1. **Input Validation**: \n   - Ensure both input lists have the same length. If not, return -1.\n   - Check that the length of the lists is non-zero. If zero, return -1.\n   - Verify that all elements in both lists are non-negative. If any element is negative, return -1.\n\n2. **Compute RMSLE**:\n   - Convert the input lists into numpy arrays for efficient computation.\n   - Compute the natural logarithm of each element in both arrays after adding 1 to handle zero values.\n   - Calculate the difference between the logarithms of the predicted and actual values.\n   - Square these differences and compute their mean.\n   - Take the square root of the mean to get the RMSLE.\n   - Round the result to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    # Validate input\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    for a, p in zip(actual, predicted):\n        if a < 0 or p < 0:\n            return -1\n    \n    # Convert to numpy arrays for efficient computation\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute logarithms\n    log_actual = np.log(actual_np + 1)\n    log_predicted = np.log(predicted_np + 1)\n    \n    # Calculate differences and squared differences\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    \n    # Compute mean of squared differences\n    mean_squared = np.mean(squared_diff)\n    \n    # Compute RMSLE\n    rmsle_val = np.sqrt(mean_squared)\n    \n    # Round to four decimal places\n    return round(rmsle_val, 4)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input lists are valid. This includes checking their lengths and ensuring all elements are non-negative.\n- **Logarithmic Transformation**: By adding 1 to each element before taking the logarithm, we handle cases where elements are zero, ensuring the logarithm is defined.\n- **Efficient Computation**: Using numpy arrays allows for vectorized operations, which are efficient and concise.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function handles edge cases and computes the RMSLE efficiently and accurately."}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the tanh activation function and its derivative. The tanh function is a popular nonlinearity used in deep learning, and its derivative is essential for training neural networks using gradient-based methods.\n\n### Approach\nThe task requires us to compute the tanh function and its derivative element-wise for a given input. The tanh function is defined as:\n$$\\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$$\n\nThe derivative of the tanh function is given by:\n$$\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)$$\n\nThe approach involves the following steps:\n1. Convert the input to a NumPy array to facilitate vectorized operations.\n2. Compute the exponent term \\(e^{-2x}\\) using NumPy's `exp` function.\n3. Calculate the denominator \\(1 + e^{-2x}\\).\n4. Compute the tanh values using the formula.\n5. If the derivative flag is set, compute the derivative using \\(1 - \\tanh^2(x)\\).\n6. Round the results to 4 decimal places and convert the NumPy array to a Python list before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_activation(x, derivative: bool = False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh. Otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to the nearest 4 decimals.\n    \"\"\"\n    x = np.array(x)\n    exponent = np.exp(-2 * x)\n    denominator = 1 + exponent\n    tanh_x = (2 / denominator) - 1\n    if derivative:\n        result = 1 - tanh_x ** 2\n    else:\n        result = tanh_x\n    result = np.round(result, 4)\n    return result.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to leverage vectorized operations, which are efficient and concise.\n2. **Exponent Calculation**: The term \\(e^{-2x}\\) is computed using `np.exp(-2 * x)`, which efficiently handles element-wise exponentiation.\n3. **Denominator Calculation**: The denominator \\(1 + e^{-2x}\\) is computed to use in the tanh formula.\n4. **Tanh Calculation**: Using the formula, we compute the tanh values for each element in `x`.\n5. **Derivative Calculation**: If the derivative flag is set, we compute the derivative using the formula \\(1 - \\tanh^2(x)\\).\n6. **Rounding and Conversion**: The results are rounded to 4 decimal places and converted to a Python list to meet the output requirements.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's capabilities for vectorized operations and handling both the function and its derivative seamlessly."}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the one-dimensional autocorrelation of a given finite real-valued signal. The autocorrelation for non-negative lags is defined as the sum of the products of the signal elements at different lags. The solution must be implemented in O(N\u00b2) time complexity using a direct approach without leveraging optimized functions like FFT.\n\n### Approach\n1. **Problem Understanding**: The autocorrelation coefficient at lag `k` is computed as the sum of the products of the signal elements `x[n]` and `x[n+k]` for all valid `n`. This needs to be done for each lag from 0 to N-1, where N is the length of the signal.\n2. **Direct Computation**: For each lag `k`, iterate through all valid indices `n` and compute the product of `x[n]` and `x[n+k]`. Sum these products to get the autocorrelation coefficient for that lag.\n3. **Edge Case Handling**: If the input signal is empty (length 0), return an empty list immediately.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        a_k = 0\n        for n in range(N - k):\n            a_k += x[n] * x[n + k]\n        result.append(a_k)\n    return result\n```\n\n### Explanation\n1. **Initialization**: Determine the length `N` of the input signal `x`. If `N` is 0, return an empty list.\n2. **Loop Over Lags**: For each lag `k` from 0 to N-1, initialize the autocorrelation coefficient `a_k` to 0.\n3. **Inner Loop for Products**: For each valid index `n` from 0 to N-k-1, compute the product of `x[n]` and `x[n+k]` and accumulate the sum to get `a_k`.\n4. **Store Results**: Append each computed `a_k` to the result list.\n5. **Return Result**: After processing all lags, return the result list containing all autocorrelation coefficients.\n\nThis approach ensures that we directly compute the autocorrelation using nested loops, achieving the required O(N\u00b2) time complexity. The solution handles both lists and NumPy arrays, ensuring flexibility in input types while maintaining clarity and efficiency."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given matrix is row-stochastic. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\n1. **Input Validation**: Check if the input is a list of lists or a numpy array. If it's a list, ensure all elements are lists and all rows have the same length. If it's a numpy array, ensure it is two-dimensional.\n2. **Element Check**: Verify that every element in the matrix is between 0 and 1 inclusive.\n3. **Row Sum Check**: Ensure that each row sums to 1 within a numerical tolerance of \\(1 \\times 10^{-8}\\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Check if X is a list of lists or a numpy array\n    if isinstance(X, list):\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Check if X is empty\n        if not X:\n            return False\n        # Check if all rows have the same length\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        # Convert to numpy array\n        X_np = np.array(X)\n    else:\n        # Check if it's a numpy array and 2D\n        if X.ndim != 2:\n            return False\n        X_np = X\n\n    # Check if all elements are between 0 and 1\n    if not ((X_np >= 0).all() and (X_np <= 1).all()):\n        return False\n\n    # Check if each row sums to 1 within a tolerance\n    row_sums = np.sum(X_np, axis=1)\n    if not np.allclose(row_sums, 1, atol=1e-8):\n        return False\n\n    # All conditions met\n    return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a list of lists or a numpy array. For a list, it ensures all elements are lists and each row has the same length. For a numpy array, it checks if it is two-dimensional.\n2. **Element Check**: The function converts the input to a numpy array and checks if all elements are between 0 and 1.\n3. **Row Sum Check**: The function calculates the sum of each row and checks if each sum is approximately 1 within a tolerance of \\(1 \\times 10^{-8}\\) using `numpy.allclose`.\n\nThis approach ensures that the matrix meets all the criteria for being row-stochastic efficiently and correctly."}
{"task_id": 49, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the k smallest elements in a given list without sorting the entire list or using Python's built-in `heapq` module. The approach involves using a max-heap to efficiently track and maintain the k smallest elements as we iterate through the list.\n\n### Approach\n1. **Edge Case Handling**: First, check if k is non-positive or greater than the length of the input list. If so, return -1.\n2. **Max-Heap Initialization**: Use a list to simulate a max-heap. This heap will store at most k elements.\n3. **Iterate Through List**: For each number in the list:\n   - If the heap has fewer than k elements, add the number to the heap and maintain the heap property using the `_move_up` function.\n   - If the heap is full (size k), compare the current number with the root of the heap (the largest element in the heap). If the current number is smaller, replace the root with this number and maintain the heap property using the `_move_down` function.\n4. **Sort and Return**: After processing all elements, the heap contains the k smallest elements in arbitrary order. Sort this heap and return the sorted list.\n\n### Solution Code\n```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    The function should scan the input list once, maintaining a custom\n    max-heap (size at most k) implemented with a Python list.  At the end,\n    return the k smallest elements in ascending order.  If *k* is invalid\n    (\u22640 or larger than len(nums)) return -1.\n    \n    The use of the built-in ``heapq`` module is **not** allowed.\n    \"\"\"\n    if k <= 0 or k > len(nums):\n        return -1\n\n    def _move_up(heap, index):\n        while index > 0:\n            parent = (index - 1) // 2\n            if heap[index] > heap[parent]:\n                heap[index], heap[parent] = heap[parent], heap[index]\n                index = parent\n            else:\n                break\n\n    def _move_down(heap, index):\n        while True:\n            left = 2 * index + 1\n            right = 2 * index + 2\n            max_child = None\n            if right < len(heap):\n                if heap[left] > heap[right]:\n                    max_child = left\n                else:\n                    max_child = right\n            elif left < len(heap):\n                max_child = left\n            else:\n                break  # No children\n            if heap[index] < heap[max_child]:\n                heap[index], heap[max_child] = heap[max_child], heap[index]\n                index = max_child\n            else:\n                break\n\n    heap = []\n    for num in nums:\n        if len(heap) < k:\n            heap.append(num)\n            _move_up(heap, len(heap) - 1)\n        else:\n            if num < heap[0]:\n                heap[0] = num\n                _move_down(heap, 0)\n    return sorted(heap)\n```\n\n### Explanation\n- **Edge Case Handling**: The function immediately returns -1 if k is invalid.\n- **Max-Heap Operations**: The `_move_up` function ensures that after adding an element, the heap property is maintained by moving the element up to its correct position. The `_move_down` function ensures that after replacing the root, the heap property is maintained by moving the new root down to its correct position.\n- **Efficiency**: Each insertion and replacement operation maintains the heap property in O(log k) time, leading to an overall time complexity of O(n log k), where n is the length of the input list. This approach efficiently finds the k smallest elements without sorting the entire list."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Newton's method to find the weight vector that minimizes the mean squared error (MSE) for a two-variable linear least squares problem. The method involves iteratively updating the weights using the gradient and Hessian of the MSE function until a stopping condition is met.\n\n### Approach\n1. **Initialization**: Start with an initial weight vector `w = [1, 1]^T`.\n2. **Gradient Calculation**: Compute the gradient of the MSE function at the current weight vector. The gradient is given by `\u2207 = (2/n) * X\u1d40(Xw - y)`.\n3. **Hessian Calculation**: Compute the Hessian matrix of the MSE function, which is `H = (2/n) * X\u1d40X`.\n4. **Pseudo-Inverse of Hessian**: If the Hessian is singular, use its Moore-Penrose pseudo-inverse to ensure the algorithm works for rank-deficient data.\n5. **Weight Update**: Update the weight vector using the formula `w \u2190 w \u2212 H\u207b\u00b9\u2207`.\n6. **Stopping Condition**: Iterate until the Euclidean norm of the gradient drops below a specified threshold `epsilon` or the maximum number of iterations is reached.\n7. **Return Result**: Return the final weight vector rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n    \"\"\"Implements Newton's method for 2-D linear regression to minimize the mean squared error.\"\"\"\n    w = np.array([[1.0], [1.0]])\n    n = X.shape[0]\n    \n    for _ in range(max_iter):\n        # Compute the gradient\n        Xw = X.dot(w)\n        residual = Xw - y\n        grad = (2.0 / n) * X.T.dot(residual)\n        \n        # Check if the gradient norm is below epsilon\n        if np.linalg.norm(grad) < epsilon:\n            break\n        \n        # Compute the Hessian\n        H = (2.0 / n) * X.T.dot(X)\n        \n        # Compute the pseudo-inverse of the Hessian\n        H_inv = np.linalg.pinv(H)\n        \n        # Update the weights\n        w = w - H_inv.dot(grad)\n    \n    # Round the result to 4 decimal places and convert to the required format\n    rounded_w = np.round(w, 4)\n    return rounded_w.tolist()\n```\n\n### Explanation\n- **Initialization**: The weight vector `w` is initialized to `[1, 1]^T`.\n- **Gradient Calculation**: The gradient is computed using the formula `(2/n) * X\u1d40(Xw - y)`, which points in the direction of steepest ascent of the MSE function.\n- **Hessian Calculation**: The Hessian matrix is computed as `(2/n) * X\u1d40X`, which is positive semi-definite and helps determine the curvature of the MSE function.\n- **Pseudo-Inverse**: The Moore-Penrose pseudo-inverse is used to handle cases where the Hessian is singular, ensuring the algorithm can still make progress.\n- **Weight Update**: The weight vector is updated using the Newton step, which moves it in the direction that reduces the MSE most effectively.\n- **Stopping Condition**: The algorithm stops early if the gradient becomes sufficiently small, indicating a minimum has been reached, or if the maximum number of iterations is exceeded.\n\nThis approach efficiently finds the optimal weight vector using Newton's method, ensuring convergence and correctness even for rank-deficient data matrices."}
{"task_id": 60, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the impurity of a node in a decision tree based on different criteria. The impurity metric helps in determining the quality of a split at each node. The three common metrics used are Entropy, Gini Index, and Mean Squared Error (MSE). Each of these metrics is used for different purposes: Entropy for classification, Gini Index for classification, and MSE for regression.\n\n### Approach\n1. **Entropy**: This metric is used for classification tasks. It measures the impurity based on the probability of each class in the node. The formula for entropy is given by:\n   \\[\n   \\text{Entropy} = -\\sum (p_i \\log_2 p_i)\n   \\]\n   where \\( p_i \\) is the probability of class \\( i \\).\n\n2. **Gini Index**: This is another metric used for classification. It measures the impurity by calculating the probability of a randomly chosen element being misclassified. The formula for Gini Index is:\n   \\[\n   \\text{Gini} = \\sum (p_i (1 - p_i))\n   \\]\n   where \\( p_i \\) is the probability of class \\( i \\).\n\n3. **Mean Squared Error (MSE)**: This metric is used for regression tasks. It measures the average of the squared differences between the predicted values (mean of the node) and the actual values. The formula for MSE is:\n   \\[\n   \\text{MSE} = \\frac{1}{n} \\sum (y_i - \\bar{y})^2\n   \\]\n   where \\( \\bar{y} \\) is the mean of the values in the node.\n\nThe function first checks if the input array is empty and returns 0.0 in that case. It then checks the validity of the criterion and computes the respective impurity metric based on the given criterion.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    if criterion not in [\"entropy\", \"gini\", \"mse\"]:\n        raise ValueError(\"Invalid criterion\")\n    \n    if criterion == \"entropy\":\n        values, counts = np.unique(y, return_counts=True)\n        probs = counts / len(y)\n        entropy = 0.0\n        for p in probs:\n            entropy += -p * np.log2(p)\n        return round(entropy, 4)\n    elif criterion == \"gini\":\n        values, counts = np.unique(y, return_counts=True)\n        probs = counts / len(y)\n        gini = sum(p * (1 - p) for p in probs)\n        return round(gini, 4)\n    elif criterion == \"mse\":\n        mean = np.mean(y)\n        squared_errors = (y - mean) ** 2\n        mse = np.mean(squared_errors)\n        return round(mse, 4)\n```\n\n### Explanation\n- **Entropy Calculation**: The function calculates the probability of each class using `np.unique` to get the counts of each unique value. It then computes the entropy using the formula for each class and sums these values.\n- **Gini Index Calculation**: Similar to entropy, the function calculates the probability of each class and then computes the Gini Index by summing the product of each probability and its complement.\n- **MSE Calculation**: The function computes the mean of the target values and then calculates the average of the squared differences between each value and the mean.\n\nThe function ensures that the result is rounded to four decimal places for precision and handles edge cases such as empty arrays and invalid criteria appropriately."}
{"task_id": 61, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the gradient of the mean squared error (MSE) cost function for linear regression using vectorized operations. The function must handle various input types, validate dimensions, and return the result in a specific format.\n\n### Approach\n1. **Input Conversion**: Convert the input lists to NumPy arrays to facilitate vectorized operations.\n2. **Reshape Inputs**: Ensure that the target vector `y` and weight vector `W` are column vectors if they are 1D arrays.\n3. **Dimension Validation**: Check that the design matrix `X` is 2D, the target vector `y` has the same number of rows as `X`, and the weight vector `W` has the same number of columns as the number of features in `X`.\n4. **Gradient Calculation**: Compute the gradient using the formula \\((X^\\top (XW - y)) / N\\), where \\(N\\) is the number of samples.\n5. **Result Formatting**: Round the gradient values to 4 decimal places and return them as a nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    X = np.asarray(X)\n    y = np.asarray(y)\n    W = np.asarray(W)\n    \n    # Reshape y to a column vector if it's 1D\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    elif y.ndim == 2:\n        if y.shape[1] != 1:\n            return -1\n    else:\n        return -1\n    \n    # Reshape W to a column vector if it's 1D\n    if W.ndim == 1:\n        W = W.reshape(-1, 1)\n    elif W.ndim == 2:\n        if W.shape[1] != 1:\n            return -1\n    else:\n        return -1\n    \n    # Check if X is 2D\n    if X.ndim != 2:\n        return -1\n    \n    # Check if y has the same number of rows as X\n    if y.shape[0] != X.shape[0]:\n        return -1\n    \n    # Check if W has the same number of rows as X[0]\n    if W.shape[0] != X.shape[1]:\n        return -1\n    \n    # Compute the gradient\n    n = X.shape[0]\n    XW = np.dot(X, W)\n    error = XW - y\n    gradient = (X.T.dot(error)) / n\n    \n    # Round to 4 decimal places\n    gradient_rounded = np.round(gradient, 4)\n    \n    # Convert to nested list of lists\n    result = gradient_rounded.tolist()\n    \n    return result\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input lists to NumPy arrays to leverage efficient vectorized operations.\n2. **Reshaping**: The target vector `y` and weight vector `W` are reshaped into column vectors if they are 1D. This ensures that all subsequent operations are performed correctly.\n3. **Dimension Checks**: The function validates that `X` is a 2D array, `y` has the same number of rows as `X`, and `W` has the same number of columns as the number of features in `X`. If any of these checks fail, the function returns -1.\n4. **Gradient Calculation**: Using vectorized operations, the function computes the gradient of the MSE cost function. This involves matrix multiplication and subtraction to compute the error, followed by transposition and dot product to get the final gradient.\n5. **Result Formatting**: The gradient values are rounded to 4 decimal places and converted to a nested list format for the final output.\n\nThis approach ensures that the function handles various input types and dimensions correctly, computes the gradient efficiently using vectorized operations, and returns the result in the required format."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The backward probability \\(\\beta_t(i)\\) represents the probability of seeing the remaining observations from time \\(t+1\\) onward given that the system is in state \\(i\\) at time \\(t\\).\n\n### Approach\n1. **Initialization**: Start by initializing the backward probability vector for the last time step \\(T-1\\) to 1 for all states, as there are no future observations beyond the last step.\n2. **Backward Calculation**: Compute the backward probabilities iteratively from the second last time step down to the first time step. For each time step \\(t\\), compute the probability for each state \\(i\\) by summing over all possible next states \\(j\\), using the transition probabilities, emission probabilities, and the previously computed backward probabilities.\n3. **Result Extraction**: After computing the backward probabilities for all relevant time steps, extract the backward probability vector for the given time index \\(t\\) and round the values to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n    \n    Args:\n        A: Transition probability matrix of shape (N, N)\n        B: Emission probability matrix of shape (N, M)\n        obs: List of observation indices (length T)\n        t: Integer time index (0 \u2264 t < T)\n        \n    Returns:\n        A list of N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)\n    T = len(obs)\n    \n    # Initialize beta vectors\n    beta = [[0.0 for _ in range(N)] for _ in range(T)]\n    \n    # Base case: t = T-1\n    for i in range(N):\n        beta[T-1][i] = 1.0\n    \n    # Compute beta for t from T-2 down to 0\n    for current_t in range(T-2, -1, -1):\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                a_ij = A[i][j]\n                b_j = B[j][obs[current_t + 1]]\n                beta_j = beta[current_t + 1][j]\n                total += a_ij * b_j * beta_j\n            beta[current_t][i] = total\n    \n    # Round the result to 4 decimal places\n    result = [round(beta[t][i], 4) for i in range(N)]\n    return result\n```\n\n### Explanation\n1. **Initialization**: The backward probability vector for the last time step \\(T-1\\) is initialized to 1 for all states because there are no future observations to consider.\n2. **Backward Calculation**: For each time step from \\(T-2\\) down to 0, the backward probability for each state \\(i\\) is computed by summing the product of the transition probability from state \\(i\\) to state \\(j\\), the emission probability of the next observation in state \\(j\\), and the previously computed backward probability for state \\(j\\).\n3. **Result Extraction**: The computed backward probabilities for the specified time index \\(t\\) are extracted and rounded to four decimal places for the final result.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that we only compute the necessary values and reuse them as needed."}
{"task_id": 64, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a naive 1-D Discrete Fourier Transform (DFT) for a given signal frame. The DFT is a fundamental algorithm in signal processing used to convert a time-domain signal into its frequency-domain representation. The task requires us to compute the DFT coefficients explicitly without using any optimized Fast Fourier Transform (FFT) functions, ensuring an O(N^2) complexity.\n\n### Approach\n1. **Problem Understanding**: The DFT of a signal frame \\( x \\) of length \\( N \\) is computed using the formula:\n   \\[\n   c_k = \\sum_{n=0}^{N-1} x_n e^{-2\\pi i k n / N}, \\quad k = 0, \\dots, N-1\n   \\]\n   Each coefficient \\( c_k \\) is computed by summing the product of each signal sample \\( x_n \\) and a complex exponential term.\n\n2. **Naive Implementation**: We will use nested loops to compute each DFT coefficient explicitly. This involves iterating over each frequency index \\( k \\) and each time index \\( n \\), computing the exponential term, and accumulating the sum.\n\n3. **Positive Only Option**: If the `positive_only` flag is set to `True`, we return only the non-negative frequency terms, which are the first \\( N//2 + 1 \\) coefficients. Otherwise, we return all \\( N \\) coefficients.\n\n4. **Rounding**: Each coefficient's real and imaginary parts are rounded to four decimal places before converting the result into a Python list of complex numbers.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft(frame: np.ndarray, positive_only: bool = True) -> list:\n    \"\"\"Compute the Discrete Fourier Transform (DFT) of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array containing the signal samples.\n        positive_only: Whether to return only the non-negative frequency\n            terms (first *N//2+1* coefficients).  Defaults to ``True``.\n\n    Returns:\n        list[complex]: DFT coefficients rounded to 4 decimals.  Length is\n        *N//2+1* when *positive_only* is ``True`` and *N* otherwise.\n    \"\"\"\n    N = len(frame)\n    if N == 0:\n        return []\n    \n    # Initialize the DFT coefficients array\n    c = np.zeros(N, dtype=np.complex128)\n    \n    for k in range(N):\n        for n in range(N):\n            exponent = (-2j * np.pi * k * n) / N\n            term = frame[n] * np.exp(exponent)\n            c[k] += term\n    \n    # Select the appropriate portion based on positive_only\n    if positive_only:\n        c = c[: N//2 + 1]\n    \n    # Round each coefficient's real and imaginary parts to 4 decimals\n    result = []\n    for coeff in c:\n        real_part = round(coeff.real, 4)\n        imag_part = round(coeff.imag, 4)\n        result.append(complex(real_part, imag_part))\n    \n    return result\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the length \\( N \\) of the input frame. It initializes an array `c` to store the DFT coefficients, all set to zero.\n\n2. **Nested Loops for DFT Calculation**: For each frequency index \\( k \\) and each time index \\( n \\), the exponential term is computed using the formula \\( e^{-2\\pi i k n / N} \\). This term is multiplied by the corresponding signal sample \\( x_n \\) and accumulated into the coefficient \\( c_k \\).\n\n3. **Selecting Coefficients**: Depending on the `positive_only` flag, the function selects either the first \\( N//2 + 1 \\) coefficients (non-negative frequencies) or all \\( N \\) coefficients.\n\n4. **Rounding and Conversion**: Each selected coefficient is rounded to four decimal places for both its real and imaginary parts. These rounded values are then converted into a Python list of complex numbers, which is returned as the result.\n\nThis approach ensures that we compute the DFT in a straightforward manner, adhering to the O(N^2) complexity requirement, and correctly handle both real and complex input signals."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by leveraging the concept of backward variables, which represent the probability of observing a sequence from a certain point onward given the current state.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the probability of an observation sequence given an HMM using the backward algorithm. The HMM is defined by transition matrix `A`, emission matrix `B`, and initial distribution `pi`. The backward variables are used to compute this probability efficiently.\n\n2. **Backward Variables**: The backward variable `beta_t(i)` represents the probability of observing the sequence from time `t+1` to the end given that the state at time `t` is `i`. These variables are computed recursively starting from the last observation and moving backward.\n\n3. **Initialization**: The backward variables for the last time step are initialized to 1, as there are no future observations to consider.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to `0`, compute the backward variables using the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution, emission probabilities, and the backward variables.\n\n6. **Edge Cases**: If any of the input lists are empty, the function returns 0.0 as no valid probability can be computed.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    N = len(A)\n    T = len(obs)\n\n    if T == 0:\n        return 0.0\n\n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                new_beta[i] += A[i][j] * B[j][obs[t+1]] * beta[j]\n        beta = new_beta\n\n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n\n    return round(prob, 6)\n```\n\n### Explanation\n1. **Initialization Check**: The function first checks if any of the input lists are empty. If so, it returns 0.0 immediately.\n\n2. **Initialization of Backward Variables**: The backward variables for the last time step are initialized to 1.0.\n\n3. **Recursive Calculation**: For each time step from `T-2` down to `0`, the function computes the backward variables using the transition matrix `A`, emission matrix `B`, and the previously computed backward variables.\n\n4. **Final Probability Calculation**: The final probability is computed by summing the contributions from each state at the initial time step, considering the initial distribution `pi`, emission probabilities, and the backward variables.\n\n5. **Rounding**: The result is rounded to six decimal places before returning.\n\nThis approach efficiently computes the probability using dynamic programming principles, ensuring that the solution is both optimal and easy to understand."}
{"task_id": 68, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Stochastic Gradient Descent (SGD) with Momentum update rule for a single optimization step. This method helps in accelerating the convergence of the optimization process by incorporating a momentum term that takes into account the previous update.\n\n### Approach\n1. **Problem Analysis**: The task is to update the parameters of a model using the SGD with Momentum algorithm. The update rule involves computing a new velocity (momentum update) based on the previous velocity and the current gradient. The parameters are then updated using this new velocity.\n2. **Intuition**: The momentum update helps in accelerating the optimization process in the direction of the gradient while dampening oscillations. This is particularly useful in scenarios where the loss landscape is complex and noisy.\n3. **Algorithm**:\n   - Compute the new velocity using the formula: \\( u_t = \\text{momentum} \\times u_{t-1} + (1 - \\text{momentum}) \\times \\nabla_w \\)\n   - Update the parameters using the new velocity: \\( w_{t+1} = w_t - \\text{learning\\_rate} \\times u_t \\)\n4. **Edge Cases**: Handle the case where the previous update is `None` by initializing it to a zero tensor of the same shape as the parameters.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float = 0.01, momentum: float = 0.0, prev_update: np.ndarray | None = None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n    \"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    \n    # Compute the new update (velocity)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    \n    # Update the parameters\n    new_w = w - learning_rate * new_update\n    \n    # Round to 4 decimal places and convert to lists\n    new_w_rounded = np.round(new_w, 4)\n    new_update_rounded = np.round(new_update, 4)\n    \n    return (new_w_rounded.tolist(), new_update_rounded.tolist())\n```\n\n### Explanation\n1. **Initialization**: If the previous update is `None`, initialize it to a zero array of the same shape as the parameters `w`.\n2. **Velocity Update**: Compute the new velocity using the given momentum factor and the current gradient.\n3. **Parameter Update**: Update the parameters by subtracting the product of the learning rate and the new velocity from the current parameters.\n4. **Rounding and Conversion**: Convert the updated parameters and the new velocity into Python lists, rounding each element to four decimal places for precision.\n\nThis approach ensures that the optimization step is both efficient and precise, leveraging the momentum term to accelerate convergence while maintaining numerical stability."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the likelihood of an observation sequence given a Hidden Markov Model (HMM) using the forward algorithm. The HMM is defined by the initial state probabilities, state-transition probabilities, and emission probabilities. The function must validate the input dimensions and return the probability rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: \n   - Check if the observations list is empty.\n   - Ensure the dimensions of the initial state probabilities (S), state-transition matrix (A), and emission matrix (B) are correct.\n   - Verify that each observation index is within the valid range.\n\n2. **Forward Algorithm**:\n   - Initialize a vector `alpha` where `alpha[i]` represents the probability of being in state `i` at the first observation.\n   - For each subsequent observation, update `alpha` using the state-transition and emission probabilities.\n   - The result is the sum of the probabilities in the `alpha` vector after processing all observations.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check if observations is empty\n    if not observations:\n        return -1\n    \n    n = len(S)\n    # Check dimensions of S\n    if len(S) != n:\n        return -1\n    \n    # Check dimensions of A\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check dimensions of B\n    if len(B) != n:\n        return -1\n    if not B:\n        return -1  # B is empty\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check each observation is within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Compute forward algorithm\n    T = len(observations)\n    alpha = [0.0] * n\n    k0 = observations[0]\n    for i in range(n):\n        alpha[i] = S[i] * B[i][k0]\n    \n    for t in range(1, T):\n        kt = observations[t]\n        new_alpha = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += alpha[j] * A[j][i]\n            new_alpha[i] = sum_val * B[i][kt]\n        alpha = new_alpha\n    \n    result = sum(alpha)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the observations list is empty and returns -1 if it is. It then verifies the dimensions of the matrices S, A, and B to ensure they are consistent with the model's requirements. Each observation is checked to ensure it falls within the valid range.\n\n2. **Initialization**: The `alpha` vector is initialized based on the initial state probabilities and the emission probabilities for the first observation.\n\n3. **Forward Algorithm**: For each subsequent observation, the `alpha` vector is updated by considering all possible transitions from the previous states. This involves summing the probabilities of transitioning to each state and multiplying by the emission probability for the current observation.\n\n4. **Result Calculation**: The final result is the sum of the probabilities in the `alpha` vector after processing all observations, which is then rounded to four decimal places and returned."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to count how many times each class label appears in a tabular dataset. The dataset is provided as a list of samples, where each sample is a list and its last element is the class label. The function should return a dictionary mapping each unique label to its respective count.\n\n### Approach\n1. **Check for Empty Data**: If the input list is empty, return an empty dictionary immediately.\n2. **Initialize a Dictionary**: Create an empty dictionary to keep track of the counts of each label.\n3. **Iterate Through Samples**: Loop through each sample in the dataset. For each sample, extract the class label, which is the last element of the sample list.\n4. **Update Counts**: For each extracted label, update the dictionary. If the label already exists in the dictionary, increment its count by 1. If it does not exist, add the label to the dictionary with an initial count of 1.\n5. **Return the Result**: After processing all samples, return the dictionary containing the counts of each label.\n\n### Solution Code\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function returns a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists). The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts. If *data* is empty, an empty dictionary is\n        returned.\n    \"\"\"\n    if not data:\n        return {}\n    counts = {}\n    for sample in data:\n        label = sample[-1]\n        if label in counts:\n            counts[label] += 1\n        else:\n            counts[label] = 1\n    return counts\n```\n\n### Explanation\n- **Empty Data Check**: The function first checks if the input list is empty. If it is, an empty dictionary is returned immediately.\n- **Dictionary Initialization**: An empty dictionary `counts` is initialized to store the frequency of each label.\n- **Loop Through Samples**: Each sample in the dataset is processed in a loop. The class label is extracted as the last element of each sample.\n- **Update Counts**: For each label, the function checks if it already exists in the dictionary. If it does, the count is incremented. If it does not, the label is added to the dictionary with a count of 1.\n- **Return Result**: Finally, the dictionary containing the counts of each label is returned.\n\nThis approach ensures that the function efficiently counts the occurrences of each label in a single pass through the dataset, making it both time and space efficient."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the variance reduction (VR) for a regression tree split. The variance reduction is a measure of how much the variance of the target values is reduced by splitting the data into two child nodes. \n\n### Approach\n1. **Compute the Variance of the Parent Node**: The variance of the target values in the parent node is calculated for each feature (column) and then summed up. This gives the total variance of the parent node.\n2. **Compute the Variance of the Child Nodes**: Similarly, compute the variance for each feature in the left and right child nodes, then sum these variances to get the total variance for each child node.\n3. **Calculate the Weighted Sum of Variances**: The variances of the child nodes are weighted by the proportion of samples in each child relative to the parent node. This weighted sum represents the combined variance of the child nodes.\n4. **Compute the Variance Reduction**: Subtract the weighted sum of the child variances from the parent variance to get the variance reduction. This value is then rounded to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"Calculate the variance reduction for a regression tree split.\n    \n    Args:\n        y: Target values of the parent node.\n        y_left: Target values of the left child node.\n        y_right: Target values of the right child node.\n        \n    Returns:\n        The variance reduction, rounded to four decimal places.\n    \"\"\"\n    # Calculate the number of samples in the parent node\n    n = y.shape[0]\n    \n    # Calculate the variance of the parent node\n    var_parent = np.sum(np.var(y, axis=0, ddof=0))\n    \n    # Calculate the number of samples in the child nodes\n    n_left = y_left.shape[0]\n    n_right = y_right.shape[0]\n    \n    # Calculate the variances of the child nodes\n    var_left = np.sum(np.var(y_left, axis=0, ddof=0))\n    var_right = np.sum(np.var(y_right, axis=0, ddof=0))\n    \n    # Compute the weighted sum of the child variances\n    weighted_var = (n_left / n) * var_left + (n_right / n) * var_right\n    \n    # Calculate the variance reduction\n    vr = var_parent - weighted_var\n    \n    # Round to four decimal places\n    return round(vr, 4)\n```\n\n### Explanation\n- **Variance Calculation**: The variance for each feature is computed using the population variance formula, which is the mean of the squared deviations from the mean. This is done using `np.var` with `ddof=0` to ensure it calculates the population variance.\n- **Weighted Sum**: The variances of the child nodes are weighted by the proportion of samples they contribute relative to the parent node. This step ensures that each child's contribution to the overall variance is appropriately scaled.\n- **Variance Reduction**: The final variance reduction is obtained by subtracting the weighted sum of the child variances from the parent variance. This value is then rounded to four decimal places to meet the output requirements.\n\nThis approach efficiently computes the variance reduction, ensuring that both the parent and child node variances are correctly accounted for, and handles both single and multi-dimensional target vectors."}
{"task_id": 74, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the magnitude spectrum for a batch of real-valued frames. The magnitude spectrum is the absolute value of the Discrete Fourier Transform (DFT) for each frame, focusing only on the non-redundant positive-frequency part. The result should be returned as a nested list of lists, with each element rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The input can be either a 1D array (single frame) or a 2D array (multiple frames). We need to handle both cases appropriately.\n2. **Real FFT Computation**: For each frame, compute the Real Fast Fourier Transform (RFFT) using NumPy's `rfft` function. This efficiently computes the positive-frequency components for real-valued signals.\n3. **Magnitude Calculation**: Convert the complex results of the RFFT to their absolute values to get the magnitude spectrum.\n4. **Rounding and Conversion**: Round each magnitude value to four decimal places and convert the result from a NumPy array to a Python list.\n5. **Result Compilation**: Collect the results for each frame into a nested list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef magnitude_spectrum(frames: \"np.ndarray\") -> \"list[list[float]]\":\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    result = []\n    if frames.ndim == 1:\n        # Single frame\n        frame = frames\n        fft_result = np.fft.rfft(frame)\n        magnitude = np.abs(fft_result)\n        rounded = np.round(magnitude, 4)\n        result.append(rounded.tolist())\n    else:\n        # Multiple frames\n        for frame in frames:\n            fft_result = np.fft.rfft(frame)\n            magnitude = np.abs(fft_result)\n            rounded = np.round(magnitude, 4)\n            result.append(rounded.tolist())\n    return result\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a 1D array (single frame) or a 2D array (multiple frames). This is done using the `ndim` attribute of the NumPy array.\n2. **RFFT Computation**: For each frame, the RFFT is computed using `np.fft.rfft`, which efficiently handles real-valued signals and returns only the non-redundant positive-frequency components.\n3. **Magnitude Calculation**: The absolute values of the RFFT results are computed to get the magnitude spectrum.\n4. **Rounding and Conversion**: Each magnitude value is rounded to four decimal places using `np.round`, and the result is converted to a Python list using `tolist()`.\n5. **Result Compilation**: The results for each frame are collected into a list, which is then returned as the final output.\n\nThis approach ensures that we efficiently compute the magnitude spectrum for each frame, handle both single and multiple frames, and return the results in the required format."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The classifier will predict the class label for each test sample based on the majority vote among its k closest training samples using the Euclidean distance metric. If there is a tie in the majority vote, the smallest class label among the tied ones will be chosen. If k is not a valid positive integer or larger than the number of training samples, the function should return -1.\n\n### Approach\n1. **Validation of k**: Check if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, return a list of -1s with the same length as the test set.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. This avoids the computational cost of taking square roots while maintaining the order of distances.\n3. **Sorting and Selection**: Sort the training samples based on the computed distances and select the top k nearest samples.\n4. **Majority Vote**: Among the k nearest samples, determine the class label with the highest frequency. In case of a tie, select the smallest class label.\n\n### Solution Code\n```python\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    \n    # Check if k is valid\n    if k < 1 or k > n_train:\n        return [-1] * n_test\n    \n    predictions = []\n    for x in X_test:\n        # Compute distances to all training samples\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            dist_sq = sum((x[i] - xi[i])**2 for i in range(len(x)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort()\n        \n        # Take the first k samples\n        selected = distances[:k]\n        \n        # Collect the class labels\n        ys = [y for (d, y) in selected]\n        \n        # Find the mode, breaking ties by choosing the smallest class label\n        freq = {}\n        for y in ys:\n            freq[y] = freq.get(y, 0) + 1\n        \n        max_freq = max(freq.values())\n        candidates = [y for y in freq if freq[y] == max_freq]\n        predicted = min(candidates)\n        \n        predictions.append(predicted)\n    \n    return predictions\n```\n\n### Explanation\n1. **Validation of k**: The function first checks if k is a valid positive integer and does not exceed the number of training samples. If k is invalid, it returns a list of -1s.\n2. **Distance Calculation**: For each test sample, the squared Euclidean distance to each training sample is computed. This avoids the computational cost of taking square roots.\n3. **Sorting and Selection**: The computed distances are sorted, and the top k nearest samples are selected.\n4. **Majority Vote**: The class labels of the k nearest samples are counted. The class with the highest frequency is chosen. In case of a tie, the smallest class label is selected.\n\nThis approach ensures that the function efficiently computes the k-NN classification for each test sample while handling edge cases such as invalid k and ties in the majority vote."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the unnormalized categorical cross-entropy loss for a batch of one-hot encoded target labels. The cross-entropy loss is a common metric used in machine learning to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function accepts inputs as either lists or NumPy arrays. We first convert these inputs to NumPy arrays to facilitate element-wise operations.\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to all elements of the predicted probabilities.\n3. **Compute Logarithm of Predictions**: We compute the natural logarithm of the adjusted predicted probabilities.\n4. **Element-wise Multiplication**: Multiply the true labels (one-hot encoded) with the logarithm of the predicted probabilities. This step effectively extracts the log probability of the true class for each sample.\n5. **Sum and Negate**: Sum all the elements of the resulting matrix and negate the sum to get the cross-entropy loss.\n6. **Round the Result**: Finally, round the computed loss to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = 1e-10\n    y_pred += epsilon\n    log_y_pred = np.log(y_pred)\n    product = y * log_y_pred\n    sum_product = np.sum(product)\n    loss = -sum_product\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This ensures that we can perform element-wise operations efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in an undefined value.\n- **Logarithm and Multiplication**: By taking the logarithm of the predicted probabilities and multiplying by the one-hot encoded true labels, we isolate the log probability of the correct class for each sample.\n- **Summing and Negating**: The sum of these log probabilities is negated to compute the cross-entropy loss, which measures the dissimilarity between the predicted and true distributions.\n- **Rounding**: The result is rounded to four decimal places for precision and returned as a float.\n\nThis approach efficiently computes the cross-entropy loss using NumPy operations, ensuring numerical stability and correctness."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward propagation for an L-layer neural network. The network uses ReLU activation for all hidden layers and sigmoid activation for the output layer. The goal is to compute the activation of the last layer and store the necessary information for backpropagation in a cache.\n\n### Approach\n1. **Initialization**: Start with the input matrix X as the initial activation (A_prev).\n2. **Iterate through each layer**: For each layer from 1 to L:\n   - Compute the linear step: \\( Z = W \\cdot A_{prev} + b \\)\n   - Apply the activation function: ReLU for hidden layers and sigmoid for the output layer.\n   - Store the cache for each layer, which includes the previous activation (A_prev), the linear output (Z), and the activation output (A).\n3. **Final Activation**: After processing all layers, the activation of the last layer (AL) is rounded to 4 decimal places and converted to a Python list.\n4. **Return**: The function returns a tuple containing the final activation (AL) and the list of caches.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]\n        A tuple containing the final activation AL and the list of caches.\n    \"\"\"\n    caches = []\n    A_prev = X\n    # Determine the number of layers\n    W_layers = [int(key[1:]) for key in parameters.keys() if key.startswith('W')]\n    L = max(W_layers) if W_layers else 0\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))\n        else:\n            A = np.maximum(0, Z)\n        caches.append((A_prev, Z, A))\n        A_prev = A\n    AL = A\n    AL_rounded = np.round(AL, 4)\n    AL_list = AL_rounded.tolist()\n    return (AL_list, caches)\n```\n\n### Explanation\n- **Initialization**: The input matrix X is used as the initial activation (A_prev).\n- **Linear Step**: For each layer, the linear combination of the previous activation and the current weights plus bias is computed.\n- **Activation Step**: ReLU is applied to all hidden layers, and sigmoid is applied to the output layer.\n- **Cache Storage**: Each layer's cache stores the previous activation, the linear output, and the activation output for use in backpropagation.\n- **Final Activation**: The output of the last layer is rounded and converted to a list before being returned along with the caches.\n\nThis approach ensures that we correctly compute the forward pass through the network and store the necessary information for efficient backpropagation."}
{"task_id": 78, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs one step of the Adamax optimizer. Adamax is a variant of the Adam optimizer that uses an exponentially weighted infinity-norm for the gradient updates. The function will take the current parameters, gradients, and other optimizer states, and return the updated parameters and states after one optimization step.\n\n### Approach\n1. **Convert Input Lists to NumPy Arrays**: This allows for efficient element-wise operations.\n2. **Compute First Moment Estimates (m)**: Update the first moment estimates using the given gradient and the decay rate \u03b21.\n3. **Compute Exponentially Weighted Infinity-Norms (u)**: Update the infinity-norms using the given gradient and the decay rate \u03b22.\n4. **Compute the Step Size**: Calculate the step size using the learning rate, the first moment estimates, and the infinity-norms.\n5. **Update Parameters**: Adjust the parameters by subtracting the computed step size from each parameter.\n6. **Increment Time Step**: Update the time step counter.\n7. **Round Results**: Round the updated parameters, first moments, and infinity-norms to 6 decimal places and convert them back to lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\n\n    Args:\n        params:  Current parameter vector.\n        grads:   Current gradient vector (same length as `params`).\n        m:       First-moment estimates (same length as `params`).\n        u:       Exponentially weighted infinity-norms (same length as `params`).\n        t:       Current time-step counter (starts at 1).\n        learning_rate: Learning rate \u03b7 (default 0.002).\n        beta1:   Exponential decay rate for the first moment \u03b2\u2081.\n        beta2:   Exponential decay rate for the infinity-norm \u03b2\u2082.\n        epsilon: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple `(new_params, new_m, new_u, new_t)` where each list is rounded\n        to 6 decimal places and `new_t = t + 1`.\n    \"\"\"\n    # Convert input lists to numpy arrays for element-wise operations\n    params_np = np.array(params)\n    grads_np = np.array(grads)\n    m_np = np.array(m)\n    u_np = np.array(u)\n    \n    # Compute new first moment estimates\n    new_m = beta1 * m_np + (1 - beta1) * grads_np\n    \n    # Compute new exponentially weighted infinity-norms\n    new_u = np.maximum(beta2 * u_np, np.abs(grads_np))\n    \n    # Compute the step size\n    denominator = 1 - (beta1 ** t)\n    step = (learning_rate / denominator) * (new_m / (new_u + epsilon))\n    \n    # Update parameters\n    new_params = params_np - step\n    \n    # Increment the time step\n    new_t = t + 1\n    \n    # Round each element to 6 decimal places and convert back to lists\n    new_params_rounded = np.round(new_params, 6).tolist()\n    new_m_rounded = np.round(new_m, 6).tolist()\n    new_u_rounded = np.round(new_u, 6).tolist()\n    \n    return (new_params_rounded, new_m_rounded, new_u_rounded, new_t)\n```\n\n### Explanation\n1. **Convert to NumPy Arrays**: The input lists are converted to NumPy arrays to facilitate efficient element-wise operations.\n2. **First Moment Update**: The first moment estimates are updated using the formula \\( m_t = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot \\text{grads} \\).\n3. **Infinity-Norm Update**: The infinity-norms are updated using the element-wise maximum of \\( \\beta_2 \\cdot u \\) and the absolute value of the gradients.\n4. **Step Size Calculation**: The step size is computed using the learning rate, the first moment estimates, and the infinity-norms, adjusted by the time step decay factor.\n5. **Parameter Update**: The parameters are updated by subtracting the computed step size from each parameter.\n6. **Rounding and Conversion**: The updated parameters, first moments, and infinity-norms are rounded to 6 decimal places and converted back to lists for the final output.\n\nThis approach ensures that the function efficiently computes the Adamax update step and returns the results in the required format."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the first update step of the Adadelta optimization algorithm. Adadelta is an adaptive learning rate method that maintains two running averages per parameter: the running average of the squared gradients (E_grad) and the running average of the squared parameter updates (E_delta). The goal is to update the weights using these running averages and return the new weights after one step.\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the input weight vector and gradient vector to NumPy arrays to facilitate element-wise operations.\n2. **Compute Squared Gradients**: Calculate the squared gradient for each element.\n3. **Update E_grad**: Compute the running average of the squared gradients using the formula \\( E_{\\text{grad}} = \\rho \\cdot E_{\\text{grad}} + (1 - \\rho) \\cdot g^2 \\). Since it's the first step, the initial value of E_grad is zero.\n4. **Compute RMS Grad**: Calculate the root mean square (RMS) of the gradients using the updated E_grad and a small constant \u03b5 to avoid division by zero.\n5. **Compute RMS Delta**: Since E_delta is zero at the first step, RMS_delta is the square root of \u03b5.\n6. **Adaptive Learning Rate**: Compute the adaptive learning rate as the ratio of RMS_delta to RMS_grad.\n7. **Compute Delta W**: Calculate the parameter update \u0394w using the adaptive learning rate and the gradient.\n8. **Update E_delta**: Compute the running average of the squared updates using the formula \\( E_{\\text{delta}} = \\rho \\cdot E_{\\text{delta}} + (1 - \\rho) \\cdot \\Delta w^2 \\).\n9. **Update Weights**: Adjust the weights by subtracting \u0394w from each element.\n10. **Round and Convert**: Round the updated weights to four decimal places and convert the result to a regular Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform **one** Adadelta optimisation step.\n\n    The function assumes that it is the very first update so the two running\n    averages (squared gradients and squared updates) are still zeros.  It must\n    therefore implement the mathematical rules of Adadelta and finally return\n    the updated weight vector rounded to 4 decimal places as a regular Python\n    list.\n\n    Args:\n        w:      Current weights (list or NumPy array of any shape).\n        grad_w: Corresponding gradient \u2013 same shape as *w*.\n        rho:    Decay coefficient used by Adadelta (default 0.95).\n        eps:    Small constant to avoid division by zero (default 1e-6).\n\n    Returns:\n        List[float]: weights after one Adadelta update (rounded to 4 decimals).\n    \"\"\"\n    w = np.asarray(w)\n    grad_w = np.asarray(grad_w)\n    \n    g_squared = grad_w ** 2\n    E_grad = (1 - rho) * g_squared\n    RMS_grad = np.sqrt(E_grad + eps)\n    RMS_delta = np.sqrt(eps)\n    adaptive_lr = RMS_delta / RMS_grad\n    delta_w = adaptive_lr * grad_w\n    new_w = w - delta_w\n    rounded_new_w = np.round(new_w, 4)\n    return rounded_new_w.tolist()\n```\n\n### Explanation\nThe provided code implements the Adadelta optimization algorithm for a single update step. It handles both list and NumPy array inputs, processes each element to compute the necessary running averages, and updates the weights accordingly. The result is rounded to four decimal places and returned as a Python list, ensuring the output meets the specified constraints. This approach efficiently handles any n-D input and correctly processes each element to apply the Adadelta update rules."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost (log-loss) between the predicted probabilities and the ground-truth labels for a binary classifier. The binary cross-entropy cost is a measure of how well the predicted probabilities match the actual labels.\n\n### Approach\n1. **Clip Predicted Probabilities**: To avoid taking the logarithm of zero or one, we clip the predicted probabilities to a small interval [\u03b5, 1-\u03b5], where \u03b5 is 1e-15. This ensures that the values are within a valid range for logarithmic functions.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements (1 - predicted probabilities).\n3. **Compute Terms**: For each element, compute the terms involving the logarithms of the predicted probabilities and their complements, weighted by the ground-truth labels.\n4. **Sum and Average**: Sum all the computed terms and then average them by dividing by the number of samples.\n5. **Round Result**: Finally, round the result to six decimal places to get the final cost.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    # Clip A2 to avoid log(0) and log(1)\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    # Compute the logarithms\n    log_a = np.log(A2_clipped)\n    log_1a = np.log(1 - A2_clipped)\n    # Calculate each term\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1a\n    # Sum the terms\n    sum_terms = term1 + term2\n    sum_total = np.sum(sum_terms)\n    m = Y.size\n    # Compute the cost\n    J = (-1 / m) * sum_total\n    # Round to six decimal places\n    return round(J, 6)\n```\n\n### Explanation\n1. **Clipping**: The predicted probabilities are clipped to ensure they lie within a small range around 0 and 1, preventing invalid logarithm operations.\n2. **Logarithms**: The natural logarithm of the clipped probabilities and their complements are computed to handle the log terms in the cost function.\n3. **Terms Calculation**: Each term in the cost function is computed by multiplying the logarithm values with the corresponding ground-truth labels and their complements.\n4. **Summation and Averaging**: The terms are summed up and then averaged over the number of samples to get the final cost.\n5. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be 0 or 1."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the optimal step size (gamma) for adding a new base learner in gradient boosting when optimizing the mean-squared-error (MSE) loss. The step size is crucial as it determines how much the new learner's predictions contribute to the current model's predictions.\n\n### Approach\nThe approach to find the optimal step size gamma involves the following steps:\n\n1. **Compute the Numerator**: The numerator is the sum of the product of the residuals (difference between true targets and current predictions) and the new learner's predictions. This captures how well the new learner fits the residuals.\n2. **Compute the Denominator**: The denominator is the sum of the squares of the new learner's predictions. This measures the magnitude of the new learner's predictions.\n3. **Check for Zero Denominator**: If the denominator is zero, it means the new learner's predictions are all zero, and thus the residuals are already perfectly fitted. In this case, the step size gamma is set to 1.0 by convention.\n4. **Calculate Gamma**: If the denominator is not zero, gamma is computed as the ratio of the numerator to the denominator.\n5. **Round Gamma**: The computed gamma is rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean squared error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Array of true target values.\n    y_pred : np.ndarray\n        Array of current predictions produced by the model.\n    h_pred : np.ndarray\n        Array of predictions from the new base learner.\n\n    Returns\n    -------\n    float\n        The optimal step size \u03b3 rounded to four decimal places. If `h_pred`\n        perfectly fits the residuals the function should return 1.0.\n    \"\"\"\n    numerator = np.sum((y - y_pred) * h_pred)\n    denominator = np.sum(h_pred ** 2)\n    \n    if denominator == 0:\n        gamma = 1.0\n    else:\n        gamma = numerator / denominator\n    \n    gamma_rounded = round(gamma, 4)\n    return gamma_rounded\n```\n\n### Explanation\n- **Numerator Calculation**: The numerator is calculated as the sum of the product of the residuals and the new learner's predictions. This step assesses how well the new learner aligns with the current residuals.\n- **Denominator Calculation**: The denominator is the sum of the squares of the new learner's predictions, which normalizes the contribution of the new learner.\n- **Zero Denominator Check**: If the denominator is zero, it indicates that the new learner's predictions are all zero, meaning the residuals are already perfectly fitted, so gamma is set to 1.0.\n- **Gamma Calculation**: The ratio of the numerator to the denominator gives the optimal step size, which is then rounded to four decimal places for precision.\n\nThis approach ensures that the step size is optimally determined, balancing the contribution of the new learner to the current model's predictions efficiently."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the predictions from a Random Forest classifier using majority voting. Each decision tree in the forest makes its own prediction for every input sample, and the final prediction is determined by taking the majority vote across all trees. In case of a tie, the smallest label (lexicographically for strings, numerically for numbers) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions such that each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions together.\n2. **Count Votes**: For each sample, count the occurrences of each class label using a counter.\n3. **Determine the Winner**: Identify the class label with the highest count. If there is a tie, select the smallest label based on lexicographic order for strings or numerical order for numbers.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Transpose the predictions to process each sample's predictions together\n    transposed = zip(*predictions)\n    result = []\n    for sample_predictions in transposed:\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        # Collect all keys with the maximum count\n        candidates = [k for k, v in counts.items() if v == max_count]\n        # Select the smallest candidate\n        selected = min(candidates)\n        result.append(selected)\n    return result\n```\n\n### Explanation\n1. **Transposing Predictions**: The `zip(*predictions)` function is used to transpose the 2D list, converting rows of tree predictions into columns of sample predictions. This allows us to handle each sample's predictions together.\n2. **Counting Votes**: For each sample, we use `Counter` from the `collections` module to count how many times each class label appears across all trees.\n3. **Selecting the Winner**: After counting, we determine the class label with the highest count. If multiple labels have the same highest count, we use the `min` function to select the smallest label, ensuring the correct tie-breaking rule is applied.\n\nThis approach efficiently aggregates the predictions and handles ties correctly, ensuring the solution is both optimal and easy to understand."}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the sizes of the input, hidden, and output layers in a feed-forward neural network based on the given input and output data matrices.\n\n### Approach\nThe task is to write a function `layer_sizes` that takes two NumPy arrays, `X` and `Y`, as input. The function should return a tuple containing the number of input features, the fixed size of the hidden layer, and the number of output units. \n\n1. **Input Features (n_x)**: The number of input features is determined by the number of rows in the input matrix `X`. This is because each row in `X` represents a different feature, and each column represents an example.\n2. **Hidden Layer Size (n_h)**: The hidden layer size is fixed at 10, as specified in the problem statement.\n3. **Output Units (n_y)**: The number of output units is determined by the number of rows in the output matrix `Y`. Each row in `Y` represents a different output unit, and each column represents an example.\n\nThe function will compute these values by extracting the number of rows from `X` and `Y`, and then return them as a tuple along with the fixed hidden layer size.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\n```\n\n### Explanation\n- **Input Matrix (X)**: The input matrix `X` is a 2-D NumPy array where each column represents an example and each row represents a feature. The number of rows in `X` gives the number of input features, `n_x`.\n- **Output Matrix (Y)**: The output matrix `Y` is a 2-D NumPy array where each column represents an example and each row represents an output unit. The number of rows in `Y` gives the number of output units, `n_y`.\n- **Hidden Layer Size (n_h)**: The hidden layer size is fixed at 10, as specified in the problem.\n\nThe function computes these values using the `shape` attribute of the NumPy arrays, which returns the dimensions of the array. The first element of the shape tuple gives the number of rows, which is used to determine `n_x` and `n_y`. The fixed hidden layer size `n_h` is always 10. The function then returns these values as a tuple in the order `(n_x, n_h, n_y)`."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function, which is a smooth approximation of the ReLU activation function. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation of this function can lead to numerical instability for very large or very small values of \\( z \\). To handle this, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept a scalar, a Python list, or a NumPy array. The output should be a float for scalar inputs and a Python list for array or list inputs.\n2. **Numerically Stable Computation**: Use `np.logaddexp(0.0, z)` to compute the softplus values. This function avoids overflow and underflow issues.\n3. **Rounding**: Each computed softplus value is rounded to 4 decimal places.\n4. **Output Formatting**: For scalar inputs, return a single float. For array or list inputs, return a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    sp = np.logaddexp(0.0, z)\n    sp_rounded = np.round(sp, 4)\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return float(sp_rounded)\n    else:\n        return sp_rounded.tolist()\n```\n\n### Explanation\n1. **Numerically Stable Calculation**: The function uses `np.logaddexp(0.0, z)` to compute the softplus values. This method is chosen because it avoids numerical issues that can arise from directly computing \\( e^z \\) for very large or very small values of \\( z \\).\n2. **Rounding**: The computed values are rounded to 4 decimal places using `np.round`.\n3. **Output Handling**: The function checks if the input is a scalar (int, float, or 0D array) and returns a float. For other inputs (lists or arrays), it returns a Python list using `tolist()`.\n\nThis approach ensures that the function handles all types of inputs correctly and efficiently, providing accurate results without numerical instability."}
{"task_id": 89, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs forward propagation for a single layer in a neural network. This involves computing the linear transformation of the input data, adding a bias vector, applying an activation function, and returning the result along with necessary cache information for backpropagation.\n\n### Approach\n1. **Linear Transformation**: Compute the linear part of the layer using the formula \\( Z = W \\cdot A_{\\text{prev}} + b \\). Here, \\( W \\) is the weight matrix, \\( A_{\\text{prev}} \\) is the activations from the previous layer, and \\( b \\) is the bias vector. The bias is added to each column of the resulting matrix due to broadcasting.\n2. **Activation Function**: Apply the specified activation function element-wise to the computed \\( Z \\). The activation function can be either ReLU or Sigmoid.\n3. **Rounding and Conversion**: Round the resulting activations to 4 decimal places and convert them to a plain Python list.\n4. **Cache Preparation**: Store the necessary information for backpropagation in a cache. This includes the linear cache (containing \\( A_{\\text{prev}} \\), \\( W \\), and \\( b \\)) and the activation cache (containing the computed \\( Z \\)).\n5. **Shape Verification**: Ensure the output activations have the correct shape using an assertion.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        The activation to be used: either \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        First element is the layer output (rounded to four decimals and\n        converted to a Python list).  Second element is a cache that will\n        be needed for back-propagation.\n    \"\"\"\n    # Compute the linear part\n    Z = np.dot(W, A_prev) + b\n    \n    # Apply the activation function\n    if activation == 'relu':\n        A = np.maximum(0, Z)\n    elif activation == 'sigmoid':\n        A = 1 / (1 + np.exp(-Z))\n    else:\n        raise ValueError(\"Invalid activation function\")\n    \n    # Round the activations to 4 decimal places and convert to list\n    A_rounded = np.round(A, 4)\n    A_list = A_rounded.tolist()\n    \n    # Prepare the cache\n    linear_cache = (A_prev, W, b)\n    activation_cache = Z\n    \n    # Assert the shape of the output\n    assert A.shape == (W.shape[0], A_prev.shape[1]), f\"Shape mismatch: {A.shape} vs {(W.shape[0], A_prev.shape[1])}\"\n    \n    return (A_list, (linear_cache, activation_cache))\n```\n\n### Explanation\n1. **Linear Transformation**: The function starts by computing the linear transformation using matrix multiplication of the weight matrix \\( W \\) and the previous layer's activations \\( A_{\\text{prev}} \\). The bias vector \\( b \\) is added to this result.\n2. **Activation Function**: Depending on the specified activation function, either ReLU or Sigmoid is applied to the result of the linear transformation. ReLU sets all negative values to zero, while Sigmoid maps values to a range between 0 and 1.\n3. **Rounding and Conversion**: The activations are rounded to 4 decimal places to ensure precision and converted to a list for easier handling in subsequent layers.\n4. **Cache Preparation**: The cache stores the inputs and intermediate results needed for backpropagation, including the previous layer's activations, weights, bias, and the linear transformation result.\n5. **Shape Verification**: An assertion ensures the output shape matches the expected dimensions, preventing any shape mismatches in the neural network.\n\nThis approach ensures that the function correctly performs forward propagation for a single layer, handling both ReLU and Sigmoid activations, and prepares the necessary information for efficient backpropagation."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a given input value is numeric. A numeric value is defined as any instance of `numbers.Number` (such as int, float, complex, Fraction, Decimal, etc.) excluding boolean values (True and False).\n\n### Approach\nThe approach to solve this problem involves two main steps:\n1. **Check if the input is an instance of `numbers.Number`**: This ensures that the input is a numeric type as defined by the `numbers` module.\n2. **Exclude boolean values**: Since boolean values (True and False) are technically instances of `int`, we need to explicitly check and exclude them.\n\nBy combining these two checks, we can accurately determine if the input is a numeric value as per the problem's requirements.\n\n### Solution Code\n```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```\n\n### Explanation\n- **Importing the `numbers` module**: This module provides a hierarchy of numeric types, which we use to check if the input is a numeric value.\n- **Function Definition**: The function `is_number` takes a single argument `a` and returns a boolean.\n- **Check for Numeric Type**: The function first checks if `a` is an instance of `numbers.Number` using `isinstance(a, numbers.Number)`.\n- **Exclude Booleans**: The function then checks if `a` is a boolean using `isinstance(a, bool)`. If `a` is a boolean, the function returns False.\n- **Return Result**: The function returns True if `a` is a numeric type and not a boolean, otherwise it returns False.\n\nThis approach ensures that all numeric types are correctly identified while explicitly excluding boolean values, meeting the problem's requirements efficiently."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function in Python. The ReLU function is widely used in neural networks and is defined as \\( A = \\max(0, Z) \\), where \\( Z \\) is the input. The function should handle various input types, including scalars, lists, and NumPy arrays, and return a tuple of Python lists for both the activation result and the original input.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input \\( Z \\) can be a scalar, list, or NumPy array. We first convert \\( Z \\) to a NumPy array to ensure consistent handling of all input types.\n2. **Compute ReLU Activation**: Using NumPy's `maximum` function, we compute the ReLU activation element-wise. This function is vectorized and efficient, avoiding explicit loops.\n3. **Convert to Python Lists**: After computing the activation, both the result and the original input are converted to Python lists using NumPy's `tolist()` method to ensure the output is JSON-serializable.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_array = np.array(Z)\n    A = np.maximum(0, Z_array)\n    return (A.tolist(), Z_array.tolist())\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input \\( Z \\) is converted to a NumPy array to handle all input types uniformly. This allows us to use vectorized operations for efficient computation.\n- **ReLU Activation Calculation**: Using `np.maximum(0, Z_array)`, we compute the ReLU activation. This function efficiently handles all elements of the array, setting negative values to zero and leaving positive values unchanged.\n- **Conversion to Python Lists**: The results are converted to Python lists using `tolist()`, ensuring the output is in a format that can be easily serialized, such as for JSON.\n\nThis approach ensures that the function works efficiently for any input type and dimension, providing both the activation result and the original input in a suitable format for further processing."}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the majority class label among the k-Nearest Neighbours (k-NN) using a majority vote mechanism. The function should return the label that appears most frequently. In case of a tie, the smallest label according to Python\u2019s default ordering should be returned.\n\n### Approach\n1. **Count Occurrences**: Use the `Counter` class from the `collections` module to count how many times each label appears in the list of neighbor targets.\n2. **Identify Maximum Count**: Determine the highest frequency of any label from the counted occurrences.\n3. **Collect Labels with Maximum Count**: Extract all labels that have this highest frequency.\n4. **Resolve Ties**: If there are multiple labels with the highest frequency, return the smallest label based on Python\u2019s default ordering (numerical for integers, lexicographic for strings).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for label, cnt in counts.items() if cnt == max_count]\n    return min(max_labels)\n```\n\n### Explanation\n1. **Counting Occurrences**: The `Counter` class efficiently counts the occurrences of each label in the input list.\n2. **Finding Maximum Count**: The maximum value from the counted occurrences is determined to identify the highest frequency.\n3. **Collecting Labels**: All labels that have the highest frequency are collected into a list.\n4. **Resolving Ties**: The smallest label in the list of labels with the highest frequency is found using Python\u2019s `min` function, ensuring the correct label is returned even in case of ties.\n\nThis approach ensures that the function efficiently and correctly determines the majority class label, handling ties appropriately by leveraging Python\u2019s default ordering."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the element-wise derivative of the ReLU activation function for various types of inputs, including scalars, lists, and NumPy arrays. The derivative of ReLU is 1 for positive inputs and 0 for non-positive inputs. The result must preserve the original shape of the input.\n\n### Approach\n1. **Identify Input Type**: Check if the input is a scalar (int or float), a NumPy array, or another type (like a list).\n2. **Handle Scalar Input**: If the input is a scalar, directly compute the derivative based on whether the value is positive or non-positive.\n3. **Handle Array Input**: If the input is a NumPy array, compute the derivative for each element. Use NumPy's `where` function to efficiently compute the derivative element-wise.\n4. **Convert to List**: For non-scalar inputs, convert the resulting NumPy array back to a Python list to preserve the original structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        if Z.ndim == 0:\n            return 1.0 if Z > 0 else 0.0\n        else:\n            derivative = np.where(Z > 0, 1.0, 0.0)\n            return derivative.tolist()\n    else:\n        arr = np.array(Z)\n        derivative = np.where(arr > 0, 1.0, 0.0)\n        return derivative.tolist()\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar, the function directly returns 1.0 if the value is positive and 0.0 otherwise.\n- **Array Handling**: For NumPy arrays, the function uses `np.where` to compute the derivative efficiently. This function checks each element and returns an array where elements are 1.0 if the corresponding element in the input is positive, and 0.0 otherwise.\n- **List Handling**: For lists, the function converts the list to a NumPy array, computes the derivative, and then converts the result back to a list using `tolist()`, preserving the original structure.\n\nThis approach ensures that the function handles all valid input types correctly and returns the derivative in the appropriate format, maintaining the original shape of the input."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification error rate, which is the proportion of incorrectly predicted samples compared to the true class labels. The function should handle inputs of different types, check for equal lengths, and return the error rate rounded to four decimal places.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the actual and predicted labels are equal. If they are not, we return -1.\n2. **Convert to NumPy Arrays**: Convert the input lists, tuples, or arrays into NumPy arrays to facilitate vectorized operations.\n3. **Count Mismatches**: Use element-wise comparison to count the number of mismatches between the actual and predicted labels.\n4. **Compute Error Rate**: Calculate the error rate as the ratio of mismatches to the total number of samples.\n5. **Handle Edge Cases**: If both inputs are empty, return 0.0 as there are no incorrect predictions.\n6. **Round Result**: Round the computed error rate to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Predicted labels from a classifier.\n\n    Returns\n    -------\n    float | int\n        The classification error rate rounded to four decimals, or \u22121 when\n        the input lengths are not identical.\n    \"\"\"\n    # Check if the lengths of actual and predicted are the same\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Convert inputs to NumPy arrays for vectorized operations\n    actual_arr = np.array(actual)\n    predicted_arr = np.array(predicted)\n    \n    # Calculate the number of mismatches\n    mismatches = np.sum(actual_arr != predicted_arr)\n    \n    # Total number of samples\n    total = len(actual_arr)\n    \n    # Handle the case where there are no samples\n    if total == 0:\n        return 0.0\n    \n    # Compute the error rate\n    error_rate = mismatches / total\n    \n    # Round to four decimal places\n    return round(error_rate, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the actual and predicted labels are equal. If not, it returns -1.\n- **Convert to Arrays**: The inputs are converted to NumPy arrays to leverage efficient element-wise operations.\n- **Count Mismatches**: Using NumPy's vectorized operations, the function counts how many predicted labels do not match the actual labels.\n- **Compute Error Rate**: The error rate is calculated as the ratio of mismatches to the total number of samples.\n- **Edge Cases**: If both inputs are empty, the function returns 0.0 since there are no incorrect predictions.\n- **Round Result**: The error rate is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function efficiently handles different input types, checks for valid input lengths, and accurately computes the classification error rate."}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies the softmax activation function to a given NumPy ndarray along a specified axis. The function must handle arrays with at least two dimensions and raise an error if the input is a 1D array. The result should be rounded to four decimal places and converted to a nested list of Python lists.\n\n### Approach\n1. **Check for 1D Input**: If the input array is 1D, raise a ValueError as softmax cannot be applied to 1D tensors.\n2. **Compute Maximum Values**: Calculate the maximum values along the specified axis, keeping the dimensions to facilitate broadcasting.\n3. **Stable Softmax Calculation**: Subtract the computed maximum values from the input array to avoid numerical overflow, then compute the exponential of each element.\n4. **Sum and Normalize**: Sum the exponential values along the specified axis and normalize by dividing each element by the sum. This step ensures the output values are probabilities.\n5. **Round and Convert**: Round the result to four decimal places and convert the NumPy array to a nested list of Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n    \n    # Compute the maximum value along the specified axis, keeping dimensions\n    max_x = np.max(x, axis=axis, keepdims=True)\n    \n    # Subtract the max to ensure numerical stability\n    x_shifted = x - max_x\n    \n    # Compute the exponential of each element\n    exp_x = np.exp(x_shifted)\n    \n    # Sum along the specified axis, keeping dimensions\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    \n    # Compute the softmax by dividing each element by the sum\n    softmax_result = exp_x / sum_exp\n    \n    # Round to 4 decimal places\n    rounded_result = np.round(softmax_result, 4)\n    \n    # Convert to a nested list of Python lists\n    return rounded_result.tolist()\n```\n\n### Explanation\n- **Numerical Stability**: By subtracting the maximum value along the specified axis, we ensure that the exponential values do not cause overflow, which is crucial for accurate computation.\n- **Broadcasting**: Using `keepdims=True` in the max and sum operations allows the resulting arrays to be broadcasted correctly during subtraction and division.\n- **Conversion to List**: The final result is converted to a Python list using `tolist()`, which handles higher-dimensional arrays by returning a nested list structure.\n\nThis approach efficiently computes the softmax function while ensuring numerical stability and correct handling of array dimensions."}
{"task_id": 105, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gaussian (Normal) probability density function (PDF) in Python. The function will compute the PDF for each element in a given input and return the results rounded to five decimal places. The function must handle various input types and edge cases, such as invalid standard deviation values.\n\n### Approach\n1. **Validate Standard Deviation**: Check if the provided standard deviation (\u03c3) is strictly positive. If \u03c3 is zero or negative, return -1.\n2. **Convert Input to Array**: Convert the input values (x) into a NumPy array to handle various input types uniformly, including lists, tuples, and scalars.\n3. **Compute Exponent**: Calculate the exponent part of the Gaussian PDF formula, which is derived from the squared difference between each value and the mean, divided by twice the square of the standard deviation.\n4. **Compute Exponential Part**: Use the exponential function to compute the exponent part.\n5. **Normalization Factor**: Compute the normalization factor, which is the reciprocal of the product of the square root of (2\u03c0) and the standard deviation.\n6. **Compute PDF**: Multiply the exponential part by the normalization factor to get the PDF values.\n7. **Round Results**: Round each computed PDF value to five decimal places and convert the result into a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    x = np.asarray(x)\n    exponent = -(x - mean) ** 2 / (2 * sigma ** 2)\n    exp_part = np.exp(exponent)\n    normalization = 1.0 / (np.sqrt(2 * np.pi) * sigma)\n    pdf = exp_part * normalization\n    pdf_rounded = np.round(pdf, 5)\n    return pdf_rounded.tolist()\n```\n\n### Explanation\n- **Validation**: The function first checks if the standard deviation (\u03c3) is positive. If not, it returns -1 immediately.\n- **Input Handling**: The input values (x) are converted into a NumPy array to handle different input types uniformly, ensuring the function works for lists, tuples, and scalars.\n- **Exponent Calculation**: The exponent part of the Gaussian formula is computed using the squared difference between each value and the mean, adjusted by the standard deviation.\n- **Exponential and Normalization**: The exponential of the computed exponent is taken, and the normalization factor is calculated to ensure the PDF integrates to 1 over the entire domain.\n- **Result Processing**: The computed PDF values are rounded to five decimal places and converted into a list for the final output.\n\nThis approach ensures that the function handles various input types and edge cases efficiently while providing accurate results."}
{"task_id": 107, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single step of the Adam optimization algorithm. The Adam optimizer combines the benefits of two different adaptive moment estimation techniques, making it suitable for training deep learning models. The optimizer maintains and updates two moment estimates for each parameter: the first moment (mean of gradients) and the second moment (mean of squared gradients). These estimates are then used to adjust the learning rate for each parameter.\n\n### Approach\n1. **Compute the First Moment (m_t):** This is an exponential moving average of the gradient. It is calculated using the formula:\n   \\[\n   m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g\n   \\]\n   where \\( \\beta_1 \\) is the decay rate for the first moment and \\( g \\) is the gradient.\n\n2. **Compute the Second Moment (v_t):** This is an exponential moving average of the squared gradient. It is calculated using the formula:\n   \\[\n   v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g^2\n   \\]\n   where \\( \\beta_2 \\) is the decay rate for the second moment.\n\n3. **Bias Correction:** The first and second moments are bias-corrected to get rid of the initial phase bias. The corrected moments are:\n   \\[\n   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n   \\]\n   \\[\n   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n   \\]\n\n4. **Parameter Update:** The parameters are updated using the bias-corrected moments. The update rule is:\n   \\[\n   \\theta_{\\text{new}} = \\theta - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n   \\]\n   where \\( \\alpha \\) is the learning rate and \\( \\epsilon \\) is a small constant for numerical stability.\n\n5. **Return Values:** The function returns the updated parameters, the new first and second moment estimates, and the incremented time step, all rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\n\n    Parameters\n    ----------\n    theta : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to *theta*.\n    m_prev : np.ndarray\n        First moment vector from the previous time step.\n    v_prev : np.ndarray\n        Second moment vector from the previous time step.\n    t : int\n        Current time-step counter (starting at 1).\n    alpha : float, optional\n        Learning rate. Default is 0.01.\n    beta1 : float, optional\n        Exponential decay rate for the first moment. Default is 0.9.\n    beta2 : float, optional\n        Exponential decay rate for the second moment. Default is 0.99.\n    epsilon : float, optional\n        Small constant for numerical stability. Default is 1e-9.\n\n    Returns\n    -------\n    tuple[list, list, list, int]\n        A tuple containing the updated parameters, the new first and\n        second moment estimates, and the incremented time step. All\n        numerical values are rounded to 6 decimal places and converted\n        to regular Python lists.\n    \"\"\"\n    # Compute the new first moment estimates\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    \n    # Compute the new second moment estimates\n    v_t = beta2 * v_prev + (1 - beta2) * (grad ** 2)\n    \n    # Bias correction for the first moment\n    m_correction = 1.0 / (1.0 - (beta1 ** t))\n    m_hat = m_t * m_correction\n    \n    # Bias correction for the second moment\n    v_correction = 1.0 / (1.0 - (beta2 ** t))\n    v_hat = v_t * v_correction\n    \n    # Compute the denominator for the parameter update\n    denominator = np.sqrt(v_hat) + epsilon\n    \n    # Update the parameters\n    theta_new = theta - alpha * (m_hat / denominator)\n    \n    # Round the results to 6 decimal places and convert to lists\n    theta_new_rounded = np.round(theta_new, 6).tolist()\n    m_new_rounded = np.round(m_t, 6).tolist()\n    v_new_rounded = np.round(v_t, 6).tolist()\n    \n    # Increment the time step\n    new_t = t + 1\n    \n    return (theta_new_rounded, m_new_rounded, v_new_rounded, new_t)\n```\n\n### Explanation\nThe provided code implements one step of the Adam optimization algorithm. It computes the new moment estimates, applies bias correction, and updates the parameters accordingly. The function handles all necessary computations and returns the updated values in the required format. The approach ensures numerical stability and efficiency, making it suitable for training deep learning models."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to factorize a given real-valued matrix using the regularized Alternating Least Squares (ALS) algorithm. The goal is to find two low-rank factor matrices that minimize a regularized Frobenius reconstruction loss. The algorithm alternates between optimizing these matrices until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with random initial values for the factor matrices W and H using a fixed random seed for reproducibility.\n2. **Iterative Optimization**: In each iteration, update one matrix while keeping the other fixed. Specifically, solve a regularized least squares problem for each matrix using the current estimate of the other matrix.\n3. **Loss Calculation**: Compute the regularized Frobenius loss after each update to check for convergence.\n4. **Stopping Condition**: Stop the iterations when the loss drops below a specified tolerance or the maximum number of iterations is reached.\n5. **Reconstruction**: After convergence, compute the reconstructed matrix using the final estimates of W and H, round it to four decimal places, and return it as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K) * 0.1\n    H = np.random.randn(K, M) * 0.1\n\n    def compute_loss(X, W, H, alpha):\n        recon = W @ H\n        loss = np.linalg.norm(X - recon)**2 + alpha * (np.linalg.norm(W)**2 + np.linalg.norm(H)**2)\n        return loss\n\n    current_loss = compute_loss(X, W, H, alpha)\n    if current_loss < tol:\n        pass  # Already converged\n    else:\n        for _ in range(max_iter):\n            # Update W\n            Ht = H.T\n            H_Ht = H @ Ht\n            A = H_Ht + alpha * np.eye(K)\n            b = X @ Ht\n            new_W = np.linalg.solve(A, b)\n\n            # Update H\n            Wt = new_W.T\n            Wt_W = Wt @ new_W\n            A = Wt_W + alpha * np.eye(K)\n            b = Wt @ X\n            new_H = np.linalg.solve(A, b)\n\n            # Update W and H\n            W, H = new_W, new_H\n\n            # Compute loss\n            current_loss = compute_loss(X, W, H, alpha)\n            if current_loss < tol:\n                break\n\n    # Compute the reconstructed matrix\n    X_hat = W @ H\n    X_hat_rounded = np.round(X_hat, 4)\n    return X_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrices W and H are initialized with small random values using a fixed seed to ensure reproducibility.\n2. **Loss Function**: The loss function combines the Frobenius norm of the difference between the original matrix and the product of the factor matrices, plus a regularization term.\n3. **Matrix Updates**: In each iteration, the matrices W and H are updated by solving regularized least squares problems. This involves computing the necessary matrix inverses to find the optimal updates.\n4. **Convergence Check**: After each update, the loss is computed and checked against the tolerance. If the loss is sufficiently small, the algorithm stops early.\n5. **Reconstruction**: The final reconstructed matrix is computed, rounded to four decimal places, and returned as a list of lists.\n\nThis approach ensures that the factorization is both efficient and numerically stable, leveraging matrix operations to solve the least squares problems at each step."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions to handle character-level sequence encoding and decoding. These functions will be used in a neural model to convert raw text into a fixed-length sequence of integer tokens, process the sequence through the model, and then decode the model's predictions back into human-readable text.\n\n### Approach\n1. **string_to_int**: This function converts each character of the input text into an integer using a provided vocabulary. The resulting list of integers is adjusted to a fixed length of 20, padding with zeros if the text is shorter and truncating if it is longer. Characters not found in the vocabulary are encoded as zeros.\n\n2. **int_to_string**: This function converts a list of integer tokens back into text using an inverse vocabulary. Padding tokens (zeros) are ignored during this conversion.\n\n3. **run_example**: This function orchestrates the entire process by encoding the input text, feeding it to the model, processing the model's predictions, and decoding the results back into a string.\n\n### Solution Code\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a text string into a fixed-length list of integer token ids.\"\"\"\n    # Convert each character to its corresponding integer, defaulting to 0 for unknown characters\n    encoded = [vocabulary.get(c, 0) for c in text]\n    # Adjust the length to match time_steps, padding with zeros or truncating as needed\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    # Filter out padding tokens and convert to characters\n    filtered = []\n    for idx in indices:\n        if idx != 0:\n            char = inverse_vocab.get(idx, '')\n            filtered.append(char)\n    return ''.join(filtered)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on encoded text and decodes the prediction.\"\"\"\n    # Encode the input text into a fixed-length sequence\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to a numpy array with shape (1, TIME_STEPS)\n    encoded_np = np.array(encoded, dtype=np.int32).reshape(1, -1)\n    # Get model predictions\n    predictions = model.predict(encoded_np)\n    # Use argmax to get the predicted token ids\n    predicted_ids = np.argmax(predictions, axis=-1).flatten()\n    # Decode the predicted ids into a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n1. **string_to_int**:\n   - Converts each character in the input text to an integer using the provided vocabulary.\n   - Ensures the resulting list is exactly 20 elements long by padding with zeros if the text is shorter or truncating if it is longer.\n\n2. **int_to_string**:\n   - Converts a list of integer tokens back into text using the inverse vocabulary.\n   - Ignores any padding tokens (zeros) during the conversion process.\n\n3. **run_example**:\n   - Encodes the input text into a fixed-length sequence.\n   - Feeds this sequence to the model to get predictions.\n   - Processes the model's predictions to extract the most probable token ids.\n   - Decodes these token ids back into a human-readable string.\n\nThis approach ensures that the text is properly encoded and decoded, handling both padding and truncation as needed, and efficiently processes the model's predictions to produce the final output."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a raw text string into a list of Token objects, each representing a distinct word with its frequency. The words are sequences of English alphabetic characters, and the comparison is case-insensitive. The resulting list must be sorted by descending frequency and, in case of ties, lexicographically.\n\n### Approach\n1. **Extract Words**: Use regular expressions to find all sequences of alphabetic characters in the input text.\n2. **Convert to Lowercase**: Convert each extracted word to lowercase to ensure case-insensitive comparison.\n3. **Count Frequencies**: Use a counter to tally the occurrences of each word.\n4. **Create Tokens**: For each unique word, create a Token object with the word and its count.\n5. **Sort Tokens**: Sort the tokens first by descending frequency and then lexicographically for words with the same frequency.\n\n### Solution Code\n```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    # Extract all words (sequences of alphabetic characters)\n    words = re.findall(r'[a-zA-Z]+', text)\n    if not words:\n        return []\n    \n    # Convert each word to lowercase for case-insensitive comparison\n    lower_words = [word.lower() for word in words]\n    \n    # Count the frequency of each word\n    counts = Counter(lower_words)\n    \n    # Create Token objects for each unique word\n    tokens = []\n    for word, count in counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    \n    # Sort the tokens by descending count and then lexicographically\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    \n    return tokens\n```\n\n### Explanation\n1. **Extract Words**: The regular expression `[a-zA-Z]+` is used to find all sequences of alphabetic characters in the input text. This ensures that we only consider valid words and ignore any non-alphabetic characters.\n2. **Convert to Lowercase**: Each word is converted to lowercase to handle case insensitivity. This means \"Cat\" and \"cat\" are treated as the same word.\n3. **Count Frequencies**: Using the `Counter` from the `collections` module, we count how many times each word appears in the text.\n4. **Create Tokens**: For each unique word, a `Token` object is created. The word is stored in lowercase, and its count is set based on the frequency from the counter.\n5. **Sort Tokens**: The tokens are sorted using a custom key that sorts primarily by descending count and secondarily by lexicographical order of the word. This ensures that the most frequent words come first, and in case of ties, words are ordered alphabetically.\n\nThis approach efficiently processes the text, handles case insensitivity, and ensures the correct ordering of the tokens as required."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the input examples and the output formatting, making it easier to quickly evaluate the model's performance on various test cases.\n\n### Approach\n1. **Function Parameters Handling**: The function `run_examples` takes four parameters: a model (a callable function), an input vocabulary dictionary, an inverse output vocabulary dictionary, and an optional examples iterable. If the examples iterable is not provided, the function falls back to a global constant `EXAMPLES`.\n2. **Example Processing**: For each input string in the examples, the function calls another helper function `run_example` to get the model's predictions. This helper function returns a list of characters, which are then concatenated into a single string.\n3. **Output Formatting and Collection**: Each input string and its corresponding prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    results = []\n    for example in examples:\n        output = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output)\n        print(f\"input: {example}\")\n        print(f\"output: {output_str}\")\n        results.append(output_str)\n    return results\n```\n\n### Explanation\n- **Function Parameters**: The function is designed to accept the model, input vocabulary, inverse output vocabulary, and an optional examples iterable. If examples are not provided, it uses a global constant `EXAMPLES`.\n- **Processing Each Example**: For each input string, the function calls `run_example` to get the model's predictions. The predictions are returned as a list of characters, which are then joined into a single string.\n- **Output Handling**: Each input string and its prediction are printed in a specified format. The predictions are collected into a list and returned in the same order as the input examples.\n\nThis approach ensures that the function is both efficient and easy to use, allowing for quick evaluation of the model on multiple test cases."}
{"task_id": 114, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function and its derivative. The SELU function is designed to maintain the mean and variance of the activations close to zero and one, respectively, which helps in self-normalizing the network.\n\n### Approach\nThe SELU activation function is defined as:\n\\[ \\operatorname{selu}(x) = \\lambda \\begin{cases} x, & x \\ge 0 \\\\ \\alpha (e^x - 1), & x < 0 \\end{cases} \\]\nThe derivative of SELU is:\n\\[ \\operatorname{selu}'(x) = \\lambda \\begin{cases} 1, & x \\ge 0 \\\\ \\alpha e^x, & x < 0 \\end{cases} \\]\n\nThe constants are predefined as:\n- \\(\\alpha = 1.6732632423543772848170429916717\\)\n- \\(\\lambda = 1.0507009873554804934193349852946\\)\n\nThe function should handle both the activation and its derivative based on the input flag. The output must be a list of values rounded to six decimal places, preserving the original shape of the input array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    lambda_ = 1.0507009873554804934193349852946\n    mask = x >= 0\n    \n    if not derivative:\n        positive = x * lambda_\n        negative = alpha * (np.exp(x) - 1) * lambda_\n        output = np.where(mask, positive, negative)\n    else:\n        positive_deriv = lambda_\n        negative_deriv = alpha * np.exp(x) * lambda_\n        output = np.where(mask, positive_deriv, negative_deriv)\n    \n    rounded = np.round(output, 6)\n    if rounded.ndim == 0:\n        return [rounded.item()]\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Constants Definition**: The constants \\(\\alpha\\) and \\(\\lambda\\) are defined as given.\n2. **Mask Creation**: A boolean mask is created to identify where the input values are non-negative.\n3. **Activation Calculation**: If the derivative flag is `False`, the function computes the SELU activation using the defined formula. For non-negative values, it uses \\(x \\times \\lambda\\), and for negative values, it uses \\(\\alpha (e^x - 1) \\times \\lambda\\).\n4. **Derivative Calculation**: If the derivative flag is `True`, the function computes the gradient of SELU. For non-negative values, the derivative is \\(\\lambda\\), and for negative values, it is \\(\\alpha e^x \\times \\lambda\\).\n5. **Rounding and Conversion**: The result is rounded to six decimal places. If the result is a scalar (0D array), it is converted to a list with one element. Otherwise, it is converted to a nested list preserving the original shape.\n\nThis approach ensures that the function efficiently computes the SELU activation and its derivative while handling various input shapes and dimensions correctly."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for a logistic regression model. The function should handle numerical stability issues by ensuring predicted probabilities are within a safe range for logarithmic operations.\n\n### Approach\n1. **Compute the Linear Combination (z):** For each sample, compute the linear combination of features and weights, which is given by \\( z = X \\cdot w \\).\n2. **Compute the Sigmoid Function (p):** Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities:** To avoid numerical issues when taking logarithms, clip the probabilities to a small range around 0 and 1.\n4. **Compute the Loss:** Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p)] \\).\n5. **Compute the Gradient:** Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Return Results:** Return the loss rounded to 4 decimal places and the gradient as a list of lists, each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    y = y.ravel()\n    \n    loss = (-1 / m) * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    gradient = (1 / m) * X.T.dot(p - y)\n    gradient = gradient.reshape(-1, 1)\n    \n    loss_rounded = round(loss, 4)\n    gradient_rounded = np.round(gradient, 4).tolist()\n    \n    return (loss_rounded, gradient_rounded)\n```\n\n### Explanation\n1. **Linear Combination (z):** The linear combination of features and weights is computed using matrix multiplication, resulting in a vector of scores for each sample.\n2. **Sigmoid Function (p):** The scores are converted into probabilities using the sigmoid function, ensuring values are between 0 and 1.\n3. **Clipping Probabilities:** To avoid taking the logarithm of 0 or 1, probabilities are clipped to a small range around these values.\n4. **Loss Calculation:** The binary cross-entropy loss is computed by averaging the negative log-likelihood over all samples.\n5. **Gradient Calculation:** The gradient is computed using the formula involving the transpose of the feature matrix and the difference between predicted and actual probabilities.\n6. **Result Formatting:** The loss is rounded to 4 decimal places, and the gradient is converted to a list of lists with each element rounded to 4 decimal places for the final output."}
{"task_id": 116, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the reduction in mean-squared error (MSE) achieved by splitting a target vector into subsets. This reduction, known as \u0394MSE, is a key criterion in decision tree regression for evaluating the effectiveness of a split.\n\n### Approach\n1. **Compute Parent MSE**: The parent MSE is calculated as the mean of the squared differences between each target value and the mean of the parent target vector.\n2. **Compute Child MSE**: For each subset created by the split, compute the mean of the squared differences between each target value in the subset and the mean of that subset. This is the child MSE for each subset.\n3. **Weighted Child MSE**: Each child MSE is weighted by the proportion of samples in the subset relative to the parent node. Sum these weighted child MSEs to get the total child MSE.\n4. **Calculate \u0394MSE**: Subtract the total child MSE from the parent MSE to get the reduction in MSE. Round this value to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    n = len(y)\n    parent_mean = np.mean(y)\n    mse_parent = np.mean((y - parent_mean) ** 2)\n    \n    total_child_mse = 0.0\n    for s in splits:\n        m = len(s)\n        child_mean = np.mean(s)\n        child_mse = np.mean((s - child_mean) ** 2)\n        weight = m / n\n        total_child_mse += child_mse * weight\n    \n    delta_mse = mse_parent - total_child_mse\n    return round(delta_mse, 4)\n```\n\n### Explanation\n1. **Parent MSE Calculation**: The parent MSE is computed by first finding the mean of the target vector `y`. Then, each value in `y` is squared after subtracting the mean, and the average of these squared differences is taken.\n2. **Child MSE Calculation**: For each subset in `splits`, the mean of the subset is calculated. Each value in the subset is squared after subtracting this mean, and the average of these squared differences is taken to get the child MSE for that subset.\n3. **Weighted Sum of Child MSEs**: Each child MSE is multiplied by the proportion of samples in its subset relative to the parent node. These weighted MSEs are summed to get the total child MSE.\n4. **\u0394MSE Calculation**: The reduction in MSE is found by subtracting the total child MSE from the parent MSE. This value is then rounded to four decimal places to provide the final result.\n\nThis approach ensures that we accurately measure the effectiveness of a split in reducing the MSE, which is crucial for building optimal decision trees in regression tasks."}
{"task_id": 119, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Stochastic Gradient Descent (SGD) update step for a list of model parameters. The function will take the current parameters, their corresponding gradients, and a learning rate, and return the updated parameters after applying the SGD rule.\n\n### Approach\n1. **Check for Valid Input**: First, we check if either the list of parameters or gradients is empty, or if their lengths differ. If any of these conditions are met, we return an empty list.\n2. **Update Parameters**: For each parameter and its corresponding gradient, we compute the new parameter value using the SGD update rule: \\(\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L\\). Here, \\(\\theta\\) is the parameter, \\(\\alpha\\) is the learning rate, and \\(\\nabla_\\theta L\\) is the gradient.\n3. **Round and Convert**: Each updated parameter is rounded to four decimal places and converted from a NumPy array to a Python list to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    The function receives a list of parameters and a list of corresponding\n    gradients and must return the updated parameters after applying the SGD\n    rule using the supplied learning-rate `alpha`.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Gradients for each parameter.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        Updated parameters converted to Python lists, each rounded to four\n        decimal places.\n    \"\"\"\n    if not params or not grads or len(params) != len(grads):\n        return []\n    new_params = []\n    for p, g in zip(params, grads):\n        new_p = p - alpha * g\n        new_p_rounded = np.round(new_p, 4)\n        new_params.append(new_p_rounded.tolist())\n    return new_params\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input parameters are valid. If either the parameters or gradients list is empty, or if their lengths do not match, it returns an empty list.\n- **Parameter Update**: For each parameter and its corresponding gradient, the function computes the new parameter value by subtracting the product of the learning rate and the gradient from the current parameter value.\n- **Rounding and Conversion**: Each updated parameter value is rounded to four decimal places using NumPy's `round` function and then converted to a Python list to ensure the output format meets the specified requirements.\n\nThis approach ensures that the function correctly updates each parameter according to the SGD rule, handles edge cases, and returns the results in the required format."}
{"task_id": 120, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple Linear Regression learner using batch gradient descent to minimize the mean-squared error (MSE). The goal is to learn the weight vector that best fits the given training data.\n\n### Approach\n1. **Check Compatibility**: First, we check if the number of samples in the feature matrix `X` is the same as the number of samples in the target vector `y`. If they are not compatible, we return -1.\n2. **Add Bias Term**: We add a constant '1' column to the feature matrix `X` to account for the bias term in the linear regression model.\n3. **Initialize Weights**: We initialize the weight vector randomly from a uniform distribution within the interval `[-1/sqrt(N), 1/sqrt(N)]`, where `N` is the number of features including the bias term.\n4. **Batch Gradient Descent**: For the specified number of iterations, we perform batch gradient descent to update the weights. The update rule is derived from the gradient of the MSE cost function.\n5. **Return Weights**: After completing the specified number of iterations, we round the final weights to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef linear_regression(X: list[list[int | float]],\n                      y: list[int | float],\n                      n_iterations: int,\n                      learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\n\n    Args:\n        X: Training samples \u2013 list of lists with numerical feature values.\n        y: Target values \u2013 list of numbers, same length as X.\n        n_iterations: Number of gradient-descent steps to perform.\n        learning_rate: Positive learning rate controlling the step size.\n\n    Returns:\n        The learned weight vector as a list (bias first) rounded to 4 decimals.\n        If the input dimensions are incompatible the function must return -1.\n    \"\"\"\n    # Check if the number of samples in X and y are the same\n    if len(X) != len(y):\n        return -1\n    \n    # Convert X and y to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add the bias column to X\n    X_np = np.insert(X_np, 0, 1, axis=1)\n    \n    # Get the number of features (including bias)\n    m, n = X_np.shape\n    \n    # Initialize weights\n    w = np.random.uniform(low=-1/math.sqrt(n), high=1/math.sqrt(n), size=(n, 1))\n    \n    # Perform gradient descent\n    for _ in range(n_iterations):\n        y_pred = X_np.dot(w)\n        error = y_pred - y_np\n        gradient = (X_np.T.dot(error)) / m\n        w = w - learning_rate * gradient\n    \n    # Round the weights and convert to list\n    result = w.flatten().round(4).tolist()\n    return result\n```\n\n### Explanation\n1. **Compatibility Check**: The function first checks if the lengths of `X` and `y` are the same. If not, it returns -1.\n2. **Matrix Conversion and Bias Addition**: The input lists `X` and `y` are converted to numpy arrays. A bias column is added to `X` to include the intercept term in the model.\n3. **Weight Initialization**: The weights are initialized using a uniform distribution scaled by the square root of the number of features to ensure proper initialization.\n4. **Gradient Descent**: For each iteration, the model makes a prediction, computes the error, and updates the weights using the gradient of the error. This process is repeated for the specified number of iterations.\n5. **Result Conversion**: After training, the weights are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that we efficiently learn the optimal weights using batch gradient descent, providing a robust solution to the linear regression problem."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to compute the weight vector for an ordinary least-squares linear regression model. The goal is to find the weight vector that minimizes the squared error between the predicted and actual target values.\n\n### Approach\n1. **Convert Input to NumPy Arrays**: Convert the given input lists into NumPy arrays to leverage efficient numerical computations.\n2. **Augment the Design Matrix**: Add a column of ones to the design matrix `X` to account for the intercept term in the linear regression model.\n3. **Compute the Moore-Penrose Pseudo-Inverse**: Use the Moore-Penrose pseudo-inverse to solve the least-squares problem. This method is robust and works even when the matrix is not invertible.\n4. **Calculate Weights**: Multiply the pseudo-inverse of the augmented design matrix by the target vector `y` to obtain the optimal weight vector.\n5. **Round the Weights**: Round each element of the weight vector to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    # Convert the input lists to NumPy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Augment X with a column of ones for the intercept term\n    ones_col = np.ones((X_np.shape[0], 1))\n    X_augmented = np.hstack((ones_col, X_np))\n    \n    # Compute the Moore-Penrose pseudo-inverse of the augmented X\n    X_pseudo = np.linalg.pinv(X_augmented)\n    \n    # Calculate the optimal weights\n    w = X_pseudo @ y_np\n    \n    # Round each weight to 4 decimal places and convert to a Python list\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists `X` and `y` are converted to NumPy arrays to facilitate efficient matrix operations.\n2. **Augmentation**: A column of ones is added to `X` to include the intercept term in the regression model, resulting in an augmented matrix.\n3. **Pseudo-Inverse Calculation**: The Moore-Penrose pseudo-inverse of the augmented matrix is computed using `np.linalg.pinv`, which provides a solution even when the matrix is not invertible.\n4. **Weight Calculation**: The optimal weights are obtained by multiplying the pseudo-inverse of the augmented matrix by the target vector `y`.\n5. **Rounding**: Each weight is rounded to four decimal places to meet the output requirements, and the result is returned as a Python list.\n\nThis approach ensures that we efficiently and accurately compute the weight vector for the linear regression model, handling various edge cases and ensuring numerical stability."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array of categorical values into its one-hot encoded representation. The one-hot encoding process involves creating a binary matrix where each row corresponds to an element of the input array, and each column corresponds to a unique category. The matrix cells contain 1 if the row\u2019s element belongs to the column\u2019s category and 0 otherwise.\n\n### Approach\n1. **Extract Unique Values**: Identify all unique values from the input array. These values will be used to form the columns of the one-hot encoded matrix.\n2. **Sort Unique Values**: The unique values are sorted in ascending order if they are integers or in alphabetical order if they are strings.\n3. **Create One-Hot Matrix**: For each element in the input array, create a row where each position corresponds to a unique category. The position is set to 1 if the element matches the corresponding category, otherwise, it is set to 0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\n\n    You should detect the unique categories in *y*, sort them in ascending/\n    alphabetical order, and then build a binary matrix where each row\n    corresponds to an element of *y* and each column corresponds to one of\n    the unique categories.  A cell must contain **1** if the row\u2019s element\n    belongs to the column\u2019s category and **0** otherwise.\n\n    Args:\n        y: A one-dimensional NumPy array (or array-like object) containing\n           categorical values.\n\n    Returns:\n        A Python list of lists with 0/1 integers representing the one-hot\n        encoded matrix.\n    \"\"\"\n    y = np.asarray(y)\n    unique = np.unique(y)\n    n_unique = len(unique)\n    category_to_index = {category: idx for idx, category in enumerate(unique)}\n    result = []\n    for element in y:\n        row = [0] * n_unique\n        row[category_to_index[element]] = 1\n        result.append(row)\n    return result\n```\n\n### Explanation\n1. **Extract Unique Values**: The function uses `np.unique(y)` to get the unique values from the input array `y`. This function also sorts these values in ascending order for integers and lexicographical order for strings.\n2. **Create Index Mapping**: A dictionary `category_to_index` is created to map each unique category to its corresponding index in the sorted list of unique values.\n3. **Generate One-Hot Rows**: For each element in the input array, a row of zeros is created. The position corresponding to the element's category is set to 1 using the index from the dictionary. This row is then added to the result list.\n\nThis approach ensures that the function efficiently converts the input array into a one-hot encoded matrix, handling both integers and strings appropriately. The resulting matrix is returned as a list of lists, each containing only 0s and 1s."}
{"task_id": 126, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the polynomial kernel Gram matrix for given data matrices X and Y. The polynomial kernel is a common kernel function used in machine learning, particularly in Support Vector Machines (SVMs). The task involves efficiently computing the kernel values for all pairs of samples from X and Y, and then constructing the Gram matrix.\n\n### Approach\n1. **Handle Input Parameters**: If Y is not provided (i.e., Y is None), we set Y to be the same as X. This ensures that we compute the Gram matrix for the same data when Y is not specified.\n2. **Convert to NumPy Arrays**: Convert the input lists X and Y into NumPy arrays for efficient numerical operations.\n3. **Determine Gamma**: If gamma is not provided (i.e., gamma is None), compute it as 1 divided by the number of features (C) in X.\n4. **Compute Dot Product**: Calculate the dot product between each row of X and each row of Y. This results in a matrix where each element (i, j) is the dot product of the i-th row of X and the j-th row of Y.\n5. **Scale and Transform**: Scale the dot product by gamma, add the bias term c0, and then raise each element to the power of d to get the kernel values.\n6. **Round and Convert**: Round the resulting kernel values to 4 decimal places and convert the NumPy array to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    # Convert X to a NumPy array\n    X_np = np.array(X)\n    \n    # Handle the case where Y is None\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    \n    # Determine the number of features\n    C = X_np.shape[1]\n    \n    # Compute gamma if it's None\n    if gamma is None:\n        gamma = 1.0 / C\n    \n    # Compute the dot product matrix\n    dot_product = np.dot(X_np, Y_np.T)\n    \n    # Apply gamma scaling and add c0\n    scaled_dot = gamma * dot_product\n    scaled_dot_plus_c0 = scaled_dot + c0\n    \n    # Raise to the power of d\n    kernel_matrix = scaled_dot_plus_c0 ** d\n    \n    # Round to 4 decimal places\n    rounded_kernel = np.round(kernel_matrix, 4)\n    \n    # Convert to a nested Python list\n    result = rounded_kernel.tolist()\n    \n    return result\n```\n\n### Explanation\n- **Input Handling**: The function first checks if Y is None and sets it to X if necessary. This ensures that the Gram matrix is computed correctly when Y is not provided.\n- **NumPy Conversion**: Converting the input lists to NumPy arrays allows for efficient matrix operations, which are crucial for performance, especially with large datasets.\n- **Gamma Calculation**: If gamma is not provided, it is computed as the inverse of the number of features, which is a common practice in kernel methods.\n- **Dot Product and Kernel Calculation**: The dot product matrix is computed efficiently using NumPy's matrix multiplication. This is then scaled by gamma, adjusted by the bias term c0, and raised to the power of d to form the kernel matrix.\n- **Rounding and Conversion**: The kernel values are rounded to 4 decimal places to meet the problem's requirements, and the resulting NumPy array is converted to a nested Python list for the final output.\n\nThis approach ensures that the polynomial kernel Gram matrix is computed efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 127, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a confusion matrix, which is a fundamental tool in evaluating classification models. The confusion matrix provides a clear view of the model's performance by showing the number of correct and incorrect predictions made by the model.\n\n### Approach\n1. **Check Input Lengths**: First, we verify if the lengths of the true labels (`y_true`) and predicted labels (`y_pred`) are the same. If they are not, we return -1 as the evaluation cannot be performed.\n2. **Identify Unique Labels**: We combine the true and predicted labels into a single list and extract the unique labels. These unique labels are then sorted to determine the order of rows and columns in the confusion matrix.\n3. **Create Index Mapping**: We create a dictionary to map each unique label to its corresponding index in the sorted list of unique labels. This helps in quickly locating where each label should be placed in the matrix.\n4. **Initialize Confusion Matrix**: We initialize a 2D list (matrix) with dimensions equal to the number of unique labels. Each element is initially set to zero.\n5. **Populate the Matrix**: We iterate through each pair of true and predicted labels, using the index mapping to find the correct position in the matrix and increment the count at that position.\n\n### Solution Code\n```python\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to\n        the *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n\n    all_labels = y_true + y_pred\n    unique_labels = sorted(list(set(all_labels)))\n    class_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n    C = len(unique_labels)\n    matrix = [[0 for _ in range(C)] for _ in range(C)]\n\n    for true, pred in zip(y_true, y_pred):\n        row = class_to_index[true]\n        col = class_to_index[pred]\n        matrix[row][col] += 1\n\n    return matrix\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal. If not, it returns -1.\n- **Unique Labels Extraction**: By combining `y_true` and `y_pred`, we ensure all possible labels are considered. These labels are sorted to maintain a consistent order.\n- **Index Mapping**: This step allows quick lookup of where each label should be placed in the matrix, ensuring efficient matrix population.\n- **Matrix Initialization and Population**: The matrix is initialized with zeros and then filled by iterating through each pair of true and predicted labels, updating the count at the appropriate position.\n\nThis approach ensures that the confusion matrix is built efficiently and correctly, providing a clear view of the model's performance."}
{"task_id": 129, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that mimics the behavior of an infinite data stream from a finite sequence, returning the first `samples` elements. This function should handle edge cases where the sequence is empty or the number of samples is non-positive by returning an empty list.\n\n### Approach\n1. **Check for Edge Cases**: If the input sequence is empty or the number of samples is non-positive, return an empty list immediately.\n2. **Infinite Cycle Creation**: Use the `itertools.cycle` function to create an infinite iterator from the given sequence.\n3. **Extract First Samples**: Use `itertools.islice` to extract the first `samples` elements from the infinite iterator.\n4. **Return Result**: Convert the sliced iterator to a list and return it.\n\nThis approach ensures that we efficiently handle the infinite cycling and extraction of elements using standard library functions, which are optimized for performance.\n\n### Solution Code\n```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    it = cycle(sequence)\n    return list(islice(it, samples))\n```\n\n### Explanation\n- **Edge Cases Handling**: The function first checks if the sequence is empty or if the number of samples is non-positive. If either condition is true, it returns an empty list.\n- **Infinite Cycle**: The `itertools.cycle` function is used to create an infinite iterator from the input sequence. This allows us to repeatedly loop over the sequence.\n- **Extracting Elements**: The `itertools.islice` function is used to efficiently extract the first `samples` elements from the infinite iterator. This function is particularly useful as it handles the iteration in a memory-efficient manner.\n- **Conversion to List**: The result from `islice` is converted to a list before returning, ensuring the output is in the required format.\n\nThis solution efficiently handles the problem requirements and edge cases, providing a clear and concise implementation."}
{"task_id": 130, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the empirical probability distribution over the next states for a given (state, action) pair in a tabular Markov-Decision-Process. The solution involves filtering relevant transitions, counting occurrences of each next state, calculating probabilities, and formatting the results as specified.\n\n### Approach\n1. **Filter Transitions**: Extract all transitions where the state and action match the given query.\n2. **Check for Empty Results**: If no transitions match the query, return an empty list.\n3. **Collect Next States**: Extract all next states from the filtered transitions.\n4. **Count Occurrences**: Use a counter to tally the occurrences of each next state.\n5. **Calculate Probabilities**: For each next state, compute the probability by dividing its count by the total number of transitions.\n6. **Format and Sort Results**: Round probabilities to four decimal places, create a list of [next_state, probability] pairs, and sort this list by next_state in ascending order.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    # Filter the transitions to find those matching the state and action\n    filtered = [t for t in transitions if t[0] == state and t[1] == action]\n    \n    # If no transitions found, return an empty list\n    if not filtered:\n        return []\n    \n    # Extract all next_states from the filtered transitions\n    next_states = [t[3] for t in filtered]\n    \n    # Count the occurrences of each next_state\n    counts = Counter(next_states)\n    \n    # Calculate probabilities and round to 4 decimal places\n    total = len(next_states)\n    probabilities = []\n    for ns, cnt in counts.items():\n        prob = cnt / total\n        rounded_prob = round(prob, 4)\n        probabilities.append([ns, rounded_prob])\n    \n    # Sort the probabilities by next_state\n    probabilities.sort(key=lambda x: x[0])\n    \n    return probabilities\n```\n\n### Explanation\n1. **Filtering Transitions**: The code first filters the list of transitions to include only those that match the given state and action.\n2. **Handling No Matches**: If no transitions match, an empty list is returned immediately.\n3. **Extracting Next States**: From the filtered transitions, the next states are collected into a list.\n4. **Counting Occurrences**: Using the `Counter` class, the occurrences of each next state are counted efficiently.\n5. **Calculating Probabilities**: For each unique next state, the probability is calculated by dividing its count by the total number of transitions. This probability is then rounded to four decimal places.\n6. **Formatting and Sorting**: The results are formatted into a list of [next_state, probability] pairs and sorted by next_state to ensure the correct order.\n\nThis approach ensures that we efficiently compute the empirical probabilities and return them in the required format, handling all edge cases such as no matching transitions."}
{"task_id": 131, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that retrieves an element from a specific sequence identified by a unique identifier (uid). The function should handle cases where the uid or the index (i) is invalid by returning `None` instead of raising an exception.\n\n### Approach\n1. **Check Validity of uid**: The function first checks if the provided uid is a valid index for the `_SHARED_SEQUENCES` list. If uid is negative or exceeds the length of `_SHARED_SEQUENCES`, the function returns `None`.\n2. **Retrieve the Sequence**: If the uid is valid, the corresponding sequence is retrieved from `_SHARED_SEQUENCES`.\n3. **Check Sequence Validity**: If the retrieved sequence is empty, the function returns `None`.\n4. **Normalize Index**: The function normalizes the index `i` to handle both positive and negative values. Negative indices are converted to their positive counterparts by adding the length of the sequence to them.\n5. **Check Index Validity**: After normalizing, the function checks if the index is within the valid range of the sequence. If it is not, the function returns `None`.\n6. **Return Element**: If all checks pass, the function returns the element at the normalized index.\n\n### Solution Code\n```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # Check if uid is a valid index for _SHARED_SEQUENCES\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    # Get the sequence identified by uid\n    sequence = _SHARED_SEQUENCES[uid]\n    # Check if the sequence is empty\n    if not sequence:\n        return None\n    # Calculate the length of the sequence\n    seq_len = len(sequence)\n    # Normalize the index i\n    if i >= 0:\n        normalized_i = i\n    else:\n        normalized_i = seq_len + i\n    # Check if the normalized index is within the valid range\n    if normalized_i < 0 or normalized_i >= seq_len:\n        return None\n    # Return the element at the normalized index\n    return sequence[normalized_i]\n```\n\n### Explanation\n- **Validity Check for uid**: The function ensures that uid is within the bounds of `_SHARED_SEQUENCES` to avoid index errors.\n- **Sequence Retrieval**: The sequence corresponding to the valid uid is fetched.\n- **Empty Sequence Handling**: If the sequence is empty, the function immediately returns `None`.\n- **Index Normalization**: The function converts negative indices to their positive counterparts to handle indexing from the end of the sequence.\n- **Index Validity Check**: After normalization, the function checks if the index is within the valid range to avoid out-of-bounds errors.\n- **Element Retrieval**: If all checks are passed, the function returns the element at the specified index.\n\nThis approach ensures that the function handles all edge cases gracefully and returns `None` for invalid inputs, adhering to the problem requirements."}
{"task_id": 132, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies the linear (identity) activation function to an input. The function should handle different types of inputs and return the appropriate output as specified.\n\n### Approach\nThe function `linear(z)` is designed to handle four types of inputs:\n1. **Scalar (int or float)**: If the input is a scalar, the function returns the same scalar value.\n2. **List or Tuple**: If the input is a list or tuple, the function converts it into a NumPy ndarray and returns the array.\n3. **NumPy ndarray**: If the input is already a NumPy ndarray, the function returns the same array without any modifications.\n4. **Other Types**: If the input is neither a scalar, list, tuple, nor a NumPy ndarray, the function raises a TypeError.\n\nThe function ensures that non-scalar inputs are converted to NumPy arrays, while scalar inputs are returned as they are. This approach guarantees that the output is consistent and meets the specified requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    if isinstance(z, (int, float)) and not isinstance(z, bool):\n        return z\n    elif isinstance(z, (list, tuple)):\n        return np.array(z)\n    elif isinstance(z, np.ndarray):\n        return z\n    else:\n        raise TypeError(\"Input must be a scalar, list, tuple, or ndarray.\")\n```\n\n### Explanation\n1. **Scalar Handling**: The function first checks if the input is a scalar (int or float) and not a boolean. If true, it returns the scalar value directly.\n2. **List or Tuple Handling**: If the input is a list or tuple, the function converts it into a NumPy array using `np.array(z)` and returns the array.\n3. **NumPy Array Handling**: If the input is already a NumPy array, the function returns the array as is without any modifications.\n4. **Error Handling**: If the input does not match any of the specified types, the function raises a TypeError indicating that the input must be a scalar, list, tuple, or NumPy ndarray.\n\nThis approach ensures that the function behaves correctly for all specified input types and adheres to the requirements of returning the same value for scalars and a NumPy array for non-scalar inputs."}
{"task_id": 133, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a batch of integer-encoded sequences into a 3-D one-hot encoded representation. This is a common requirement in sequence-to-sequence and language modeling tasks where the targets are required in a specific format for training deep learning models.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Validation Check**: First, we need to ensure that all token indices in the input sequences are valid. A token index is considered valid if it is within the range [0, vocab_length-1]. If any token index is negative or greater than or equal to the vocabulary length, the function should return -1.\n\n2. **One-Hot Encoding**: If all token indices are valid, we proceed to convert each token into its one-hot encoded vector. For each token, we create a vector of length `vocab_length` where all elements are 0 except for a 1 at the position corresponding to the token.\n\n3. **Constructing the 3-D Structure**: We construct a 3-D list where each element is a sequence of one-hot encoded vectors. Each sequence in the input is converted into a list of one-hot vectors, resulting in a nested list structure of shape (m, time_steps, vocab_length).\n\n### Solution Code\n```python\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    # Check for invalid indices\n    for sequence in X_train:\n        for token in sequence:\n            if token < 0 or token >= vocab_length:\n                return -1\n    \n    # Create one-hot encoded targets\n    Y_train = []\n    for sequence in X_train:\n        one_hot_sequence = []\n        for token in sequence:\n            hot_vector = [0] * vocab_length\n            hot_vector[token] = 1\n            one_hot_sequence.append(hot_vector)\n        Y_train.append(one_hot_sequence)\n    \n    return Y_train\n```\n\n### Explanation\n1. **Validation Check**: The function first iterates through each token in each sequence to check if any token is invalid. If an invalid token is found, the function immediately returns -1.\n\n2. **One-Hot Encoding**: For each valid token, a one-hot vector is created. This vector is initialized with zeros, and the position corresponding to the token is set to 1.\n\n3. **Constructing the 3-D Structure**: Each sequence of tokens is converted into a list of one-hot vectors, and these lists are collected into a 3-D structure. This structure is then returned as the result.\n\nThis approach ensures that the function efficiently checks for invalid indices and constructs the required 3-D one-hot encoded structure, which can be directly used in deep learning models after converting it to a NumPy array."}
{"task_id": 134, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to identify the optimal arm in a Bernoulli bandit, where each arm returns a binary reward (1 or 0) with a given probability. The goal is to find the arm with the highest expected reward and return both this maximum expected reward and the corresponding arm index.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return (-1.0, -1) as there are no arms to evaluate.\n2. **Validate Probabilities**: Ensure all probabilities in the list are within the valid range [0, 1]. If any probability is outside this range, return (-1.0, -1).\n3. **Find Maximum Probability**: Determine the highest probability from the valid list of probabilities.\n4. **Identify Optimal Arm**: Locate the smallest index where this maximum probability occurs. This ensures that if multiple arms have the same highest probability, the smallest index is chosen.\n\n### Solution Code\n```python\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        A list where each element is the success probability of the\n        corresponding arm. Probabilities must lie in the interval [0, 1].\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid probability (<0 or >1), the function must return\n        (-1.0, -1).\n    \"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    \n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    \n    max_p = max(payoff_probs)\n    for index, p in enumerate(payoff_probs):\n        if p == max_p:\n            best_index = index\n            break\n    \n    return (max_p, best_index)\n```\n\n### Explanation\n1. **Empty List Check**: The function first checks if the input list is empty. If it is, the function immediately returns (-1.0, -1).\n2. **Probability Validation**: The function iterates through each probability in the list to ensure all values are within the valid range [0, 1]. If any value is found to be outside this range, the function returns (-1.0, -1).\n3. **Finding Maximum Probability**: Using the `max()` function, the highest probability in the list is identified.\n4. **Finding the Optimal Arm**: The function then iterates through the list to find the first occurrence of this maximum probability, ensuring the smallest index is selected if there are multiple arms with the same highest probability.\n\nThis approach efficiently handles all edge cases and ensures the optimal arm is identified correctly."}
{"task_id": 136, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a numerically stable version of the softmax function. The softmax function converts raw scores into probabilities, but directly exponentiating large or very small numbers can lead to overflow or underflow issues. The stable approach involves subtracting the maximum value along the specified axis before exponentiation, which helps in maintaining numerical stability.\n\n### Approach\n1. **Subtract the Maximum Value**: For each element in the input array, subtract the maximum value along the specified axis. This step ensures that the largest value is zero, reducing the risk of overflow when exponentiating.\n2. **Exponentiate**: Compute the exponential of each element in the shifted array.\n3. **Sum the Exponentials**: Sum the exponentials along the specified axis. This sum is used to normalize the probabilities.\n4. **Normalize**: Divide each element by the sum of exponentials along the same axis to get the probability distribution.\n5. **Round the Results**: Round each probability to four decimal places.\n6. **Convert to List**: Convert the resulting NumPy array to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Normalise by their sum along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: -1).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    # Subtract the maximum value along the specified axis to ensure numerical stability\n    max_val = z.max(axis=axis, keepdims=True)\n    shifted = z - max_val\n    \n    # Exponentiate the shifted values\n    exp_shifted = np.exp(shifted)\n    \n    # Sum the exponentials along the specified axis\n    sum_exp = exp_shifted.sum(axis=axis, keepdims=True)\n    \n    # Compute the softmax by normalizing with the sum\n    result = exp_shifted / sum_exp\n    \n    # Round each element to four decimal places\n    rounded = np.round(result, 4)\n    \n    # Convert the NumPy array to a Python list and return\n    return rounded.tolist()\n```\n\n### Explanation\n- **Subtracting the Maximum Value**: This step ensures that the largest value in the array is zero, which helps in preventing overflow when exponentiating.\n- **Exponentiation**: Using the shifted values, we compute the exponentials. This step is numerically stable because the values are not too large or too small.\n- **Summing Exponentials**: The sum of exponentials along the specified axis is computed to normalize the probabilities.\n- **Normalization**: Each element is divided by the sum of exponentials, resulting in a probability distribution.\n- **Rounding and Conversion**: The results are rounded to four decimal places and converted to a Python list to match the required output format.\n\nThis approach ensures that the softmax function is computed accurately and efficiently, avoiding numerical issues that can arise with direct computation."}
{"task_id": 138, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification accuracy between two lists of labels, `y_true` (ground-truth labels) and `y_pred` (predicted labels). The accuracy is defined as the proportion of elements that are identical in the same positions of both lists. \n\n### Approach\n1. **Input Validation**: First, we check if the lengths of `y_true` and `y_pred` are equal and non-zero. If they are not, we return -1 as the result is invalid.\n2. **Compute Correct Predictions**: We iterate through each pair of elements from `y_true` and `y_pred`, counting how many times they are equal.\n3. **Calculate Accuracy**: The accuracy is computed as the ratio of correct predictions to the total number of predictions. This value is then rounded to four decimal places and returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    n = len(y_true)\n    if n == 0:\n        return -1\n    \n    correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n    accuracy = correct / n\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal and non-zero. If either condition is not met, it returns -1.\n2. **Counting Correct Predictions**: Using a generator expression with `zip`, we compare each corresponding element of `y_true` and `y_pred`. The sum of these comparisons gives the count of correct predictions.\n3. **Compute and Round Accuracy**: The accuracy is calculated by dividing the count of correct predictions by the total number of predictions. This value is then rounded to four decimal places using Python's `round` function and returned.\n\nThis approach ensures that the function handles various data types correctly and efficiently computes the classification accuracy."}
{"task_id": 140, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to navigate an agent through a maze while avoiding spreading fire. The maze is represented as an \\(n \\times n\\) grid where each cell can be a free cell, a wall, or on fire. The agent starts at the top-left corner and aims to reach the bottom-right corner. The fire spreads to adjacent cells each minute before the agent moves. The goal is to determine the minimum time required for the agent to reach the destination safely or determine if it's impossible.\n\n### Approach\n1. **Initial Checks**: \n   - If the starting or ending cell is on fire, return -1 immediately.\n   - If the grid is 1x1 and the cell is free, return 0 as the agent is already at the goal.\n\n2. **Precompute Fire Spread**:\n   - Use a multi-source BFS to compute the earliest time each cell catches fire. This helps in determining if a cell is safe for the agent to move into at a given time.\n\n3. **BFS for Agent Movement**:\n   - Use BFS to explore the agent's movement level by level, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival.\n\n### Solution Code\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    # Check if start or end is on fire at t=0\n    if grid[0][0] == 'F' or grid[-1][-1] == 'F':\n        return -1\n    # Check 1x1 case\n    if n == 1:\n        return 0\n    \n    # Precompute fire_time for each cell\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        i, j = q.popleft()\n        for dx, dy in dirs:\n            ni, nj = i + dx, j + dy\n            if 0 <= ni < n and 0 <= nj < n:\n                if fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    \n    # BFS for the agent's movement\n    visited = [[math.inf for _ in range(n)] for _ in range(n)]\n    visited[0][0] = 0\n    q = deque()\n    q.append((0, 0))\n    agent_dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    while q:\n        x, y = q.popleft()\n        current_time = visited[x][y]\n        # Check if reached the end\n        if x == n - 1 and y == n - 1:\n            return current_time\n        for dx, dy in agent_dirs:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] == '#':\n                    continue\n                if fire_time[nx][ny] <= current_time + 1:\n                    continue\n                if visited[nx][ny] > current_time + 1:\n                    visited[nx][ny] = current_time + 1\n                    q.append((nx, ny))\n    return -1\n```\n\n### Explanation\n1. **Initial Checks**: The function first checks if the starting or ending cell is on fire, returning -1 if true. It also handles the 1x1 grid case directly.\n\n2. **Fire Spread Calculation**: Using a multi-source BFS, the function calculates the earliest time each cell catches fire. This helps in determining the safety of each cell at any given time.\n\n3. **Agent Movement**: The agent's movement is explored using BFS, where each level corresponds to a time step. The agent can only move to a cell if it is not on fire at the time of arrival. The BFS ensures that the earliest time to reach the goal is found efficiently.\n\nThis approach ensures that the agent navigates the maze safely while avoiding the spreading fire, providing the minimum time required or determining if it's impossible to reach the goal."}
{"task_id": 141, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier. The goal is to predict the class of each test sample based on the majority vote of its k nearest neighbors in the training set. The distance between samples can be computed using one of three metrics: Euclidean, Manhattan, or Cosine. If an unknown metric is provided, we default to Euclidean.\n\n### Approach\n1. **Compute Distances**: For each test sample, compute the distance to every training sample using the specified metric. The distance metrics are:\n   - **Euclidean**: L2 distance, computed as the square root of the sum of squared differences.\n   - **Manhattan**: L1 distance, computed as the sum of absolute differences.\n   - **Cosine**: 1 minus the cosine similarity, computed as 1 minus the dot product of the vectors divided by the product of their magnitudes.\n\n2. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances. This is done efficiently using NumPy's `argpartition` to find the indices of the k smallest distances without fully sorting the distances.\n\n3. **Majority Vote with Tie-breaking**: For each test sample, collect the labels of the k nearest neighbors. Determine the most frequent label, and in case of a tie, select the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement the k-Nearest Neighbors classifier.\"\"\"\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    m = X_test.shape[0]\n    n = X_train.shape[0]\n    \n    # Compute the distance matrix based on the metric\n    if metric == 'euclidean':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        squared_diff = differences ** 2  # m x n x d\n        sum_squared = squared_diff.sum(axis=2)  # m x n\n        distances = np.sqrt(sum_squared)  # m x n\n    elif metric == 'manhattan':\n        X_test_reshaped = X_test[:, np.newaxis]  # m x 1 x d\n        X_train_reshaped = X_train[np.newaxis, :]  # 1 x n x d\n        differences = X_test_reshaped - X_train_reshaped  # m x n x d\n        abs_diff = np.abs(differences)  # m x n x d\n        sum_abs = abs_diff.sum(axis=2)  # m x n\n        distances = sum_abs  # m x n\n    elif metric == 'cosine':\n        dot_products = X_test @ X_train.T  # m x n\n        norms_test = np.linalg.norm(X_test, axis=1)  # m x 1\n        norms_train = np.linalg.norm(X_train, axis=1)  # n x 1\n        cosine_similarities = dot_products / (norms_test[:, np.newaxis] * norms_train[np.newaxis, :])\n        distances = 1 - cosine_similarities  # m x n\n    \n    # Find the indices of the k nearest neighbors for each test sample\n    knn_indices = np.zeros((m, k), dtype=int)\n    for i in range(m):\n        dist_row = distances[i]\n        # Use argpartition to find the k smallest indices\n        k_smallest = np.argpartition(dist_row, k-1)[:k]\n        knn_indices[i] = k_smallest\n    \n    # Determine the predicted labels using majority vote with tie-breaking\n    y_knn = np.zeros(m, dtype=y_train.dtype)\n    for i in range(m):\n        indices = knn_indices[i]\n        y_group = y_train[indices]\n        unique_labels, label_counts = np.unique(y_group, return_counts=True)\n        max_count = np.max(label_counts)\n        candidates = unique_labels[label_counts == max_count]\n        predicted_label = np.min(candidates)\n        y_knn[i] = predicted_label\n    \n    return y_knn\n```\n\n### Explanation\n1. **Distance Calculation**: The function first computes the distance matrix between each test sample and training sample using the specified metric. This is done efficiently using NumPy's vectorized operations.\n\n2. **Finding Nearest Neighbors**: For each test sample, the function identifies the k nearest neighbors by finding the indices of the smallest distances using `argpartition`, which is more efficient than sorting for large datasets.\n\n3. **Majority Vote**: For each test sample, the function collects the labels of the k nearest neighbors and determines the most frequent label. In case of a tie, the smallest label is chosen.\n\nThis approach ensures that the function efficiently computes the predictions while handling different distance metrics and tie-breaking scenarios correctly."}
{"task_id": 143, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is widely used in deep learning models. The function should handle both the forward pass and the derivative computation efficiently using vectorized NumPy operations.\n\n### Approach\nThe Leaky ReLU function is defined as follows:\n- For an input value \\( x \\geq 0 \\), the function returns \\( x \\).\n- For an input value \\( x < 0 \\), the function returns \\( \\alpha \\cdot x \\), where \\( \\alpha \\) is a small positive slope (default value is 0.2).\n\nThe derivative of the Leaky ReLU function is:\n- For \\( x \\geq 0 \\), the derivative is 1.\n- For \\( x < 0 \\), the derivative is \\( \\alpha \\).\n\nThe function should handle both the activation and its derivative. If the input is not a NumPy array, it will be converted to one. The solution uses vectorized operations to ensure efficiency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x)\n    if derivative:\n        return np.where(x >= 0, 1, alpha)\n    else:\n        return np.where(x >= 0, x, alpha * x)\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a NumPy array if it is not already one. This ensures that all subsequent operations are performed on a NumPy array, which supports vectorized operations.\n\n2. **Activation Computation**: When `derivative` is `False`, the function uses `np.where` to compute the Leaky ReLU activation. For each element in `x`, if the element is non-negative, it returns the element itself; otherwise, it returns the element multiplied by `alpha`.\n\n3. **Derivative Computation**: When `derivative` is `True`, the function again uses `np.where` to compute the derivative. For each element in `x`, if the element is non-negative, the derivative is 1; otherwise, it is `alpha`.\n\nThis approach ensures that the function efficiently handles both the activation and its derivative using vectorized operations, making it suitable for deep learning applications."}
{"task_id": 144, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Mean Absolute Error (MAE) between two sequences. The MAE is a commonly used metric in regression analysis to measure the average absolute difference between the actual and predicted values. The function should handle various input types and return the MAE rounded to four decimal places, or -1 if the inputs do not have the same shape.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert both the actual and predicted inputs to NumPy arrays to facilitate shape checking and numerical operations.\n2. **Check Shape Compatibility**: Ensure both arrays have the same shape. If they do not, return -1.\n3. **Compute Absolute Differences**: Calculate the absolute differences between corresponding elements of the actual and predicted arrays.\n4. **Calculate Mean Absolute Error**: Compute the mean of these absolute differences to get the MAE.\n5. **Handle Special Cases**: If the computed MAE is NaN (which can happen with empty arrays), return -1. Otherwise, round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise -1.\n    \"\"\"\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    if actual.shape != predicted.shape:\n        return -1\n    \n    diff = np.abs(actual - predicted)\n    mae = np.mean(diff)\n    \n    if np.isnan(mae):\n        return -1\n    \n    return round(mae, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can use NumPy functions for efficient numerical operations and shape checking.\n- **Shape Check**: By comparing the shapes of the two arrays, we ensure that the inputs are compatible for element-wise operations. If they are not, the function returns -1 immediately.\n- **Absolute Differences**: The absolute differences between corresponding elements are computed to avoid negative values, which is essential for the MAE calculation.\n- **Mean Calculation**: The mean of these absolute differences gives the MAE, which is then rounded to four decimal places for the final result.\n- **Special Case Handling**: If the result is NaN (which occurs with empty arrays), the function returns -1 to indicate an undefined MAE.\n\nThis approach ensures that the function handles various input types and edge cases efficiently while providing the correct MAE value or indicating when it cannot be computed."}
{"task_id": 147, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression model from scratch using batch gradient descent. The goal is to learn a set of weights and an intercept that minimize the negative log-likelihood (or equivalently, maximize the likelihood) on a training set. Using these learned parameters, we then predict the class labels for an unseen test set.\n\n### Approach\n1. **Check for Single Class**: If all labels in the training set are the same, predict that class for all test examples.\n2. **Initialize Parameters**: Start with zero-initialized weights and an intercept.\n3. **Batch Gradient Descent**: For each epoch, compute the hypothesis using the sigmoid function, calculate the gradients for the weights and intercept, and update the parameters using the learning rate.\n4. **Prediction**: After training, use the learned parameters to predict the class labels for the test set by thresholding the sigmoid output at 0.5.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    # Check if all training labels are the same\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * len(X_test)\n    \n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(epochs):\n        # Compute the linear combination and hypothesis\n        linear = np.dot(X_train, w) + b\n        h = sigmoid(linear)\n        \n        # Compute gradients\n        gradient_w = (X_train.T @ (h - y_train)) / n_samples\n        gradient_b = (h - y_train).mean()\n        \n        # Update parameters\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n    \n    # Make predictions on the test set\n    linear_test = np.dot(X_test, w) + b\n    h_test = sigmoid(linear_test)\n    y_pred = (h_test >= 0.5).astype(int).tolist()\n    \n    return y_pred\n```\n\n### Explanation\n1. **Sigmoid Function**: The sigmoid function is used to squash the linear combination of features into a probability between 0 and 1.\n2. **Gradient Descent**: The weights and intercept are updated iteratively using the gradients of the loss function. The gradients are computed using the entire training set (batch gradient descent).\n3. **Prediction**: After training, the model predicts the class labels for the test set by applying the sigmoid function to the linear combination of the test features and weights, then thresholding the result at 0.5.\n\nThis approach ensures that we efficiently learn the parameters to separate the classes in the training data and use these parameters to make accurate predictions on new data."}
{"task_id": 148, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Moving Average (EMA) for a given sequence of numerical observations using a specified smoothing factor. The EMA is a common technique used in time series analysis to smooth out short-term fluctuations and highlight longer-term trends.\n\n### Approach\n1. **Special Cases Handling**:\n   - If the smoothing factor `alpha` is outside the interval [0, 1] or the input list is empty, return -1.\n   - If `alpha` is 0, every element in the output will be the first observation of the input list.\n   - If `alpha` is 1, the output will be the same as the input list.\n\n2. **General Case**:\n   - For values of `alpha` between 0 and 1, compute the EMA using the recursive formula:\n     \\[\n     s_k = (1 - \\alpha) \\cdot s_{k-1} + \\alpha \\cdot x_k\n     \\]\n     where \\( s_0 = x_0 \\).\n\n3. **Rounding**:\n   - Each computed value is rounded to four decimal places before being added to the result list.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    if not (0 <= alpha <= 1) or not data:\n        return -1\n    \n    if alpha == 0:\n        first = round(data[0], 4)\n        return [first] * len(data)\n    elif alpha == 1:\n        return [round(x, 4) for x in data]\n    else:\n        n = len(data)\n        smoothed = [0.0] * n\n        smoothed[0] = data[0]\n        for k in range(1, n):\n            smoothed[k] = (1 - alpha) * smoothed[k-1] + alpha * data[k]\n        return [round(x, 4) for x in smoothed]\n```\n\n### Explanation\n- **Special Cases Handling**: The function first checks if the input is invalid (either `alpha` is out of bounds or the data list is empty) and returns -1 if so. It then handles the special cases where `alpha` is 0 or 1 directly.\n- **General Case Calculation**: For valid `alpha` values between 0 and 1, the function initializes the first smoothed value to the first element of the data. It then iterates through the data, computing each subsequent smoothed value using the EMA formula.\n- **Rounding**: Each computed smoothed value is rounded to four decimal places to ensure the result meets the specified precision.\n\nThis approach ensures that the function efficiently computes the EMA while handling edge cases and providing the required precision in the results."}
{"task_id": 152, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the accuracy score of a classifier's predictions. The accuracy score is the proportion of correctly classified samples out of the total samples. This function should handle both Python lists and NumPy arrays and return the result rounded to four decimal places.\n\n### Approach\n1. **Check Input Validity**: First, we check if the lengths of the true labels (`y_true`) and predicted labels (`y_pred`) are the same. If not, we return -1. Additionally, if either of the inputs is empty, we return -1.\n2. **Convert to NumPy Arrays**: Convert both `y_true` and `y_pred` to NumPy arrays to handle both list and array inputs uniformly.\n3. **Calculate Correct Predictions**: Use element-wise comparison to determine where the true and predicted labels match. Sum these matches to get the count of correct predictions.\n4. **Compute Accuracy**: Divide the count of correct predictions by the total number of samples to get the accuracy.\n5. **Round the Result**: Use Python's built-in `round` function to round the accuracy to four decimal places and return the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return -1\n    \n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    \n    accuracy = correct / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal and if they are non-empty. If any condition fails, it returns -1.\n- **Conversion to Arrays**: Both inputs are converted to NumPy arrays to facilitate element-wise comparison and efficient computation.\n- **Correct Predictions Calculation**: Using NumPy's element-wise comparison and summation, we count how many predictions match the true labels.\n- **Accuracy Calculation**: The accuracy is computed as the ratio of correct predictions to the total number of samples.\n- **Rounding**: The result is rounded to four decimal places to meet the specified output format.\n\nThis approach ensures that the function works efficiently with both lists and arrays, handles edge cases, and returns the accuracy score in the required format."}
{"task_id": 154, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Radial Basis Function (RBF) kernel matrix for two sets of vectors. The RBF kernel matrix is a popular method in machine learning for measuring similarity between data points, which is useful in various applications such as kernelized machine learning algorithms.\n\n### Approach\nThe RBF kernel matrix is defined as follows: Given two sets of vectors \\( X \\in \\mathbb{R}^{n \\times d} \\) and \\( Y \\in \\mathbb{R}^{m \\times d} \\), the element \\( K_{ij} \\) of the RBF kernel matrix \\( K \\) is given by:\n\n\\[ K_{ij} = \\exp\\left(-\\gamma \\|x_i - y_j\\|_2^2\\right) \\]\n\nwhere \\( \\gamma > 0 \\) is a user-defined parameter that controls the rate at which similarity decays with distance.\n\nThe approach to compute the RBF kernel matrix efficiently involves the following steps:\n\n1. **Reshape Input Arrays**: Ensure that both input arrays \\( X \\) and \\( Y \\) are 2-dimensional using `np.atleast_2d`.\n2. **Compute Sum of Squares**: Calculate the sum of squares for each row in \\( X \\) and \\( Y \\) to facilitate efficient computation of pairwise distances.\n3. **Compute Dot Product Matrix**: Use matrix multiplication to compute the dot product of each pair of vectors from \\( X \\) and \\( Y \\).\n4. **Compute Pairwise Distances**: Using the sums of squares and the dot product matrix, compute the pairwise squared Euclidean distances efficiently.\n5. **Compute Kernel Matrix**: Apply the exponential function to the pairwise distances scaled by \\( \\gamma \\) to obtain the kernel matrix.\n6. **Round and Convert to List**: Round each element of the kernel matrix to six decimal places and convert the resulting matrix into a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # Ensure X and Y are at least 2D\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    \n    # Compute sum of squares for each row in X and Y\n    sum_X = np.sum(X ** 2, axis=1)\n    sum_Y = np.sum(Y ** 2, axis=1)\n    \n    # Compute dot product matrix\n    dot_product = X @ Y.T\n    \n    # Compute the distance matrix\n    distance_matrix = sum_X[:, np.newaxis] + sum_Y[np.newaxis, :] - 2 * dot_product\n    \n    # Compute the kernel matrix\n    kernel_matrix = np.exp(-gamma * distance_matrix)\n    \n    # Round to six decimal places\n    kernel_matrix_rounded = np.round(kernel_matrix, decimals=6)\n    \n    # Convert to nested Python list\n    return kernel_matrix_rounded.tolist()\n```\n\n### Explanation\n1. **Reshape Input Arrays**: The function starts by ensuring that both input arrays \\( X \\) and \\( Y \\) are 2-dimensional using `np.atleast_2d`. This handles cases where the inputs might be 1-dimensional.\n2. **Sum of Squares**: The sums of squares for each row in \\( X \\) and \\( Y \\) are computed to facilitate the efficient calculation of pairwise distances.\n3. **Dot Product Matrix**: The dot product matrix is computed using matrix multiplication, which allows for efficient calculation of all pairwise dot products.\n4. **Pairwise Distances**: Using the sums of squares and the dot product matrix, the pairwise squared distances are computed efficiently without using explicit loops.\n5. **Kernel Matrix**: The exponential function is applied to the pairwise distances scaled by \\( \\gamma \\) to obtain the kernel matrix.\n6. **Rounding and Conversion**: The kernel matrix is rounded to six decimal places and converted into a nested Python list for the final output.\n\nThis approach ensures that the computation is efficient and leverages matrix operations to avoid the computational overhead of explicit loops, making it suitable for large datasets."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a two-hidden-layer actor network in a reinforcement learning context. The network takes a state vector as input and outputs an action vector after passing through three layers with specific activation functions.\n\n### Approach\nThe network architecture consists of three layers:\n1. The first layer is a linear transformation followed by a ReLU activation.\n2. The second layer is another linear transformation followed by a ReLU activation.\n3. The third layer is a linear transformation followed by a tanh activation, which ensures the output values are within the range [-1, 1].\n\nThe steps to compute the forward pass are as follows:\n1. Convert the input state vector into a NumPy array to handle both list and array inputs uniformly.\n2. Compute the output of the first hidden layer by applying the linear transformation using the first weight matrix and bias vector, followed by the ReLU activation.\n3. Compute the output of the second hidden layer similarly using the second weight matrix and bias vector, followed by another ReLU activation.\n4. Compute the final action vector using the third weight matrix and bias vector, followed by the tanh activation.\n5. Round the resulting action vector to four decimal places and convert it to a list for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.asarray(state)\n    \n    # First hidden layer\n    h1 = np.dot(state, weights['W1']) + weights['b1']\n    h1_relu = np.maximum(0, h1)\n    \n    # Second hidden layer\n    h2 = np.dot(h1_relu, weights['W2']) + weights['b2']\n    h2_relu = np.maximum(0, h2)\n    \n    # Output layer\n    action = np.dot(h2_relu, weights['W3']) + weights['b3']\n    action_tanh = np.tanh(action)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_action = np.round(action_tanh, 4).tolist()\n    \n    return rounded_action\n```\n\n### Explanation\n1. **Input Handling**: The input state is converted to a NumPy array to ensure consistent handling of both list and array inputs.\n2. **First Hidden Layer**: The state vector is transformed using the first weight matrix and bias vector. The ReLU activation function is applied to introduce non-linearity.\n3. **Second Hidden Layer**: The output from the first hidden layer is transformed using the second weight matrix and bias vector, followed by another ReLU activation.\n4. **Output Layer**: The output from the second hidden layer is transformed using the third weight matrix and bias vector. The tanh activation function is applied to ensure the output values are within the range [-1, 1].\n5. **Rounding and Conversion**: The resulting action vector is rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the forward pass is computed efficiently using NumPy operations, adhering to the specified network architecture and activation functions."}
{"task_id": 157, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the classification accuracy score between two sets of class labels: the true labels and the predicted labels. The accuracy score is a measure of how often the model's predictions match the true labels.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check Lengths**: First, we check if the lengths of the true and predicted labels are different. If they are, we return -1 as the accuracy cannot be computed.\n2. **Handle Empty Containers**: If both containers are empty, we return 0.0 since there are no predictions to evaluate.\n3. **Count Matches**: If the lengths are the same and the containers are not empty, we count the number of correct predictions by comparing each corresponding element in the true and predicted labels.\n4. **Compute Accuracy**: The accuracy is computed as the ratio of correct predictions to the total number of predictions. This value is then rounded to four decimal places and returned.\n\n### Solution Code\n```python\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return -1 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or -1 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0 and len(y_pred) == 0:\n        return 0.0\n    n = len(y_true)\n    matches = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            matches += 1\n    accuracy = matches / n\n    return round(accuracy, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of `y_true` and `y_pred` are different. If they are, it returns -1 immediately.\n- **Handle Empty Containers**: If both `y_true` and `y_pred` are empty, the function returns 0.0.\n- **Count Matches**: Using a loop, the function iterates through each pair of true and predicted labels, counting how many times they match.\n- **Compute Accuracy**: The accuracy is calculated by dividing the number of correct predictions by the total number of predictions. This value is then rounded to four decimal places and returned.\n\nThis approach ensures that all edge cases are handled efficiently and the accuracy is computed accurately and efficiently."}
{"task_id": 158, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the closed-form solution for Ridge Regression, which is a regularized form of linear regression. The goal is to find the weight vector that minimizes the given cost function, which includes a regularization term to prevent overfitting.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the weight vector \u03b2 that minimizes the cost function ||y \u2212 X\u03b2||\u00b2 + \u03b1||\u03b2||\u00b2. This can be solved using the closed-form solution \u03b2\u0302 = (X\u1d40X + A)^{-1} X\u1d40 y, where A is a regularization matrix. If an intercept term is needed, the feature matrix X is augmented with a column of ones, and the first diagonal element of A is set to 0 to exclude the intercept from regularization.\n\n2. **Intuition**: The closed-form solution for Ridge Regression involves matrix inversion. By augmenting the feature matrix with an intercept term and constructing the regularization matrix appropriately, we can compute the weight vector efficiently.\n\n3. **Algorithm**:\n   - Convert the input feature matrix X and target vector y into numpy arrays.\n   - If an intercept term is required, augment X with a column of ones.\n   - Compute the transpose of X and the product X\u1d40X.\n   - Construct the regularization matrix A, ensuring the intercept term is not regularized.\n   - Add the regularization matrix A to X\u1d40X and compute the inverse of the resulting matrix.\n   - Multiply the inverse matrix by X\u1d40 and y to get the weight vector \u03b2.\n   - Round the resulting weights to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add intercept term if required\n    if fit_intercept:\n        intercept = np.ones((X.shape[0], 1))\n        X = np.hstack([intercept, X])\n    \n    # Compute transpose of X\n    Xt = X.T\n    \n    # Compute X^T X\n    XtX = Xt @ X\n    \n    # Create regularization matrix A\n    n = X.shape[1]\n    A = alpha * np.identity(n)\n    if fit_intercept:\n        A[0, 0] = 0  # Do not regularize the intercept\n    \n    # Compute the matrix to invert: X^T X + A\n    matrix = XtX + A\n    \n    # Compute the inverse of the matrix\n    inv_matrix = np.linalg.inv(matrix)\n    \n    # Compute the coefficients\n    beta = inv_matrix @ Xt @ y\n    \n    # Round to four decimal places\n    rounded_beta = np.round(beta, 4)\n    \n    # Convert to list of floats\n    return rounded_beta.tolist()\n```\n\n### Explanation\n- **Matrix Conversion and Augmentation**: The input feature matrix X and target vector y are converted to numpy arrays. If an intercept term is required, a column of ones is added to X.\n- **Transpose and Product**: The transpose of X is computed, and the product X\u1d40X is formed, which is a key component in the closed-form solution.\n- **Regularization Matrix**: The matrix A is constructed with diagonal elements set to the regularization strength \u03b1. The first diagonal element is set to 0 if an intercept term is included.\n- **Matrix Inversion and Multiplication**: The matrix X\u1d40X + A is inverted, and the resulting matrix is multiplied by X\u1d40 and y to obtain the weight vector \u03b2.\n- **Rounding and Conversion**: The resulting weights are rounded to four decimal places and converted to a list for the final output.\n\nThis approach efficiently computes the Ridge Regression coefficients using matrix operations, ensuring the solution is both optimal and easy to implement."}
{"task_id": 159, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the log probability density of a multivariate Gaussian distribution given a sample vector, mean vector, and covariance matrix. The formula for the log probability density is provided, and we need to ensure numerical stability by using specific numpy functions.\n\n### Approach\n1. **Problem Analysis**: The task involves computing the log probability density using the given formula. The formula includes terms involving the determinant of the covariance matrix and a quadratic form involving the inverse of the covariance matrix. Direct computation of the determinant and inverse can be numerically unstable, so we use numpy's `slogdet` for the log determinant and `solve` to compute the product of the inverse covariance matrix with a vector efficiently.\n\n2. **Intuitions and Insights**:\n   - The dimensionality `d` is derived from the sample vector.\n   - The term `d * ln(2\u03c0)` is straightforward to compute.\n   - The log determinant of the covariance matrix is obtained using `slogdet` to avoid numerical issues.\n   - The quadratic term is computed efficiently by solving a linear system using `solve` instead of directly inverting the matrix.\n\n3. **Algorithm Selection**: Using numpy's `slogdet` and `solve` ensures numerical stability and efficiency. These functions are well-suited for handling the matrix operations required without explicitly forming the inverse, which can be computationally expensive and unstable.\n\n4. **Complexity Analysis**: The operations involve matrix inversion and solving linear systems, which have a time complexity of O(d^3) for the determinant and O(d^3) for solving the system, where d is the dimensionality. This is efficient for typical machine learning applications where d is not excessively large.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = x_i.shape[0]\n    term1 = d * np.log(2 * np.pi)\n    log_det = np.linalg.slogdet(sigma)[1]\n    diff = x_i - mu\n    a = np.linalg.solve(sigma, diff)\n    quad_term = np.dot(diff, a)\n    sum_terms = term1 + log_det + quad_term\n    log_pdf = -0.5 * sum_terms\n    return round(log_pdf, 4)\n```\n\n### Explanation\n1. **Dimensionality**: The dimensionality `d` is determined from the shape of the sample vector `x_i`.\n2. **First Term**: Compute `d * ln(2\u03c0)` which is a constant term in the log probability formula.\n3. **Log Determinant**: Use `np.linalg.slogdet` to compute the log determinant of the covariance matrix, ensuring numerical stability.\n4. **Difference Vector**: Compute the vector difference between the sample and mean vectors.\n5. **Quadratic Term**: Solve the linear system using `np.linalg.solve` to compute the product of the inverse covariance matrix with the difference vector efficiently, then compute the quadratic form using the dot product.\n6. **Sum Terms**: Combine all terms and multiply by -0.5 to get the log probability.\n7. **Rounding**: The result is rounded to four decimal places for the final output.\n\nThis approach ensures that the computation is both efficient and numerically stable, leveraging numpy's optimized functions for matrix operations."}
{"task_id": 161, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that predicts binary class labels using a Factorization Machine. The Factorization Machine model efficiently captures both linear and pairwise feature interactions, making it suitable for a wide range of prediction tasks.\n\n### Approach\nThe Factorization Machine prediction is given by the formula:\n\\[ s(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j \\]\nwhere \\( w_0 \\) is a global bias, \\( \\mathbf{w} \\) are linear weights, and \\( \\mathbf{V} \\) contains latent vectors for each feature.\n\nTo efficiently compute the pairwise interactions, we use a computational trick that reduces the complexity from \\( O(n^2 k) \\) to \\( O(nk) \\). This trick involves:\n1. Computing the dot product of the feature vector with the latent factors matrix.\n2. Using element-wise operations to compute the interaction terms efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary class labels using a Factorization Machine.\"\"\"\n    m = X.shape[0]\n    labels = []\n    V_squared = V ** 2  # Precompute V squared to avoid recomputing for each sample\n    for x in X:\n        # Compute the linear part of the prediction\n        linear_part = w0 + np.dot(w, x)\n        \n        # Compute the interaction part using the computational trick\n        xV = np.dot(x, V)\n        term1 = np.sum(xV ** 2)\n        \n        x_squared = x ** 2\n        term2 = np.dot(x_squared, V_squared)\n        term2_total = np.sum(term2)\n        \n        interaction_part = 0.5 * (term1 - term2_total)\n        \n        # Calculate the total score\n        score = linear_part + interaction_part\n        \n        # Determine the binary label\n        labels.append(1 if score >= 0 else -1)\n    \n    return labels\n```\n\n### Explanation\n1. **Linear Part Calculation**: For each sample, we compute the linear part of the prediction using the global bias \\( w_0 \\) and the linear weights \\( \\mathbf{w} \\).\n\n2. **Interaction Part Calculation**: \n   - Compute the dot product of the feature vector with the latent factors matrix to get \\( \\mathbf{xV} \\).\n   - Calculate the sum of the squares of the elements of \\( \\mathbf{xV} \\) to get the first term of the interaction part.\n   - Compute the element-wise square of the feature vector and the latent factors matrix, then compute their dot product to get the second term of the interaction part.\n   - Subtract the second term from the first term and multiply by 0.5 to get the interaction part.\n\n3. **Score Calculation**: Sum the linear and interaction parts to get the total score for each sample.\n\n4. **Binary Label Assignment**: Assign the label as 1 if the score is non-negative, otherwise assign -1.\n\nThis approach efficiently computes the prediction using the Factorization Machine model, ensuring both accuracy and computational efficiency."}
{"task_id": 162, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the cross-entropy loss and its gradient for a multi-class classification problem. The cross-entropy loss is a common loss function used in training neural networks, and its gradient is essential for backpropagation.\n\n### Approach\n1. **Check Shape Compatibility**: Ensure that the shapes of the predicted probabilities (Y_hat) and the true labels (Y) match. If they do not, raise an assertion error.\n2. **Compute Loss**: The cross-entropy loss is calculated using the formula:\n   \\[\n   \\text{loss} = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{c=1}^{C} y_{ic}\\,\\log(\\hat y_{ic} + \\varepsilon)\n   \\]\n   where \\(m\\) is the batch size, \\(C\\) is the number of classes, and \\(\\varepsilon\\) is a small constant added to avoid numerical instability when taking the logarithm.\n3. **Compute Gradient**: The gradient of the loss with respect to the soft-max output is given by:\n   \\[\n   \\nabla_{\\hat Y} = \\frac{1}{m}(\\hat Y - Y)\n   \\]\n4. **Rounding**: Both the loss and the gradient are rounded to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, \"Shapes of Y_hat and Y must match.\"\n    \n    m = Y_hat.shape[0]\n    \n    # Compute the loss\n    log_term = np.log(Y_hat + epsilon)\n    loss = - (1.0 / m) * np.sum(Y * log_term)\n    \n    # Compute the gradient\n    gradient = (Y_hat - Y) / m\n    \n    # Round the loss and gradient to four decimal places\n    rounded_loss = round(loss, 4)\n    rounded_gradient = np.round(gradient, 4).tolist()\n    \n    return (rounded_loss, rounded_gradient)\n```\n\n### Explanation\n1. **Shape Check**: The function starts by verifying that the shapes of Y_hat and Y are the same. This is crucial to ensure that the matrix operations proceed without errors.\n2. **Loss Calculation**: The loss is computed by taking the element-wise logarithm of Y_hat after adding a small constant \\(\\varepsilon\\) to avoid numerical issues. The result is then multiplied by the corresponding elements in Y and summed up, followed by averaging over the batch size.\n3. **Gradient Calculation**: The gradient is derived from the difference between the predicted probabilities (Y_hat) and the true labels (Y), scaled by the batch size.\n4. **Rounding**: Both the loss and the gradient are rounded to four decimal places to meet the output requirements, ensuring numerical precision and consistency.\n\nThis approach efficiently computes the cross-entropy loss and its gradient, ensuring numerical stability and correctness for training neural networks."}
{"task_id": 163, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic Net regression from scratch. Elastic Net regression combines the penalties of both Lasso (L1) and Ridge (L2) regression, making it useful for feature selection and handling multicollinearity. The task involves expanding the input feature matrix into polynomial features, fitting the model using either analytical methods or gradient descent, and making predictions.\n\n### Approach\n1. **Expand Features**: Convert the input feature list into a polynomial feature matrix. Each element in the input list is expanded into a row vector containing powers of the element from 0 up to the specified degree, including a bias term (x^0 = 1).\n\n2. **Fit Model**:\n   - **Analytical Solution (L1 Ratio = 0)**: When the L1 ratio is 0, the problem reduces to Ridge regression. We solve this using the normal equations with a Tikhonov regularization term that only penalizes non-bias terms.\n   - **Gradient Descent (L1 Ratio > 0)**: For non-zero L1 ratios, we use batch gradient descent to minimize the Elastic Net cost function. The gradient computation includes both the squared error term and the penalty terms (L1 and L2).\n\n3. **Make Predictions**: Using the learned coefficients, compute the predictions on the original input features and return them rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Fit an Elastic Net regression model and return the predictions for X.\n\n    Parameters\n    ----------\n    X : list[float]\n        One-dimensional input feature values.\n    y : list[float]\n        Target values corresponding to *X*.\n    degree : int\n        Highest polynomial degree to be used (degree \u2265 0).\n    reg_factor : float\n        Overall regularisation strength \u03bb (\u03bb \u2265 0).\n    l1_ratio : float\n        Mixing parameter \u03b1 between L1 and L2 penalty (0 \u2264 \u03b1 \u2264 1).\n    learning_rate : float\n        Step size for gradient descent (ignored when \u03b1 == 0).\n    n_iterations : int\n        Number of gradient-descent iterations (ignored when \u03b1 == 0).\n\n    Returns\n    -------\n    list[float]\n        Model predictions on the supplied *X*, rounded to 4 decimals.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    n = len(X_np)\n    d = degree\n    feature_matrix = np.zeros((n, d + 1))\n    for i in range(n):\n        x = X_np[i]\n        feature_matrix[i] = [x**k for k in range(d + 1)]\n    \n    if l1_ratio == 0:\n        m = feature_matrix.shape[1]\n        XTX = feature_matrix.T @ feature_matrix\n        penalty = np.zeros((m, m))\n        penalty[1:, 1:] = reg_factor * (1 - l1_ratio) * np.eye(m - 1)\n        XTX += penalty\n        XTy = feature_matrix.T @ y_np\n        \u03b2 = np.linalg.inv(XTX) @ XTy\n    else:\n        m = feature_matrix.shape[1]\n        \u03b2 = np.zeros(m)\n        for _ in range(n_iterations):\n            se = feature_matrix @ \u03b2 - y_np\n            gradient = np.zeros(m)\n            for j in range(m):\n                if j == 0:\n                    gradient_j = (2 / n) * se.sum()\n                else:\n                    se_grad = (2 / n) * np.dot(se, feature_matrix[:, j])\n                    if \u03b2[j] > 0:\n                        sign = 1\n                    elif \u03b2[j] < 0:\n                        sign = -1\n                    else:\n                        sign = 0\n                    pen_grad = reg_factor * ((1 - l1_ratio) * \u03b2[j] + l1_ratio * sign)\n                    gradient_j = se_grad + pen_grad\n                gradient[j] = gradient_j\n            \u03b2 -= learning_rate * gradient\n    \n    y_pred = feature_matrix @ \u03b2\n    y_pred_list = [round(y, 4) for y in y_pred]\n    return y_pred_list\n```\n\n### Explanation\n1. **Feature Expansion**: The input feature list is converted into a polynomial feature matrix where each row contains powers of the input value from 0 up to the specified degree. This includes a bias term (x^0 = 1).\n\n2. **Analytical Solution**: When the L1 ratio is 0, the problem is solved using the normal equations with a Tikhonov regularization term that only penalizes non-bias terms. This involves constructing a penalty matrix and solving the linear system using matrix inversion.\n\n3. **Gradient Descent**: For non-zero L1 ratios, the model parameters are updated iteratively using gradient descent. The gradient computation includes both the squared error term and the penalty terms (L1 and L2). The L1 penalty uses sub-gradients, which are zero when the coefficient is exactly zero.\n\n4. **Predictions**: The learned coefficients are used to compute predictions on the original input features, which are then rounded to four decimal places and returned as a list.\n\nThis approach ensures that we handle both L1 and L2 regularization efficiently, providing a robust solution to regression problems with feature selection capabilities."}
{"task_id": 164, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary classification log-loss (also known as cross-entropy loss) for given ground-truth labels and predicted probabilities. The log-loss measures the performance of a classification model by comparing the predicted probabilities to the actual labels.\n\n### Approach\n1. **Clipping Probabilities**: Since the logarithm function is undefined for 0 and 1, we first clip the predicted probabilities to ensure they lie within a small range around 0 and 1. Specifically, we clip each probability to the interval \\([10^{-15}, 1 - 10^{-15}]\\).\n\n2. **Logarithm Calculation**: For each predicted probability, compute the natural logarithm of the clipped probability and the logarithm of one minus the clipped probability.\n\n3. **Term Calculation**: For each sample, compute two terms based on the actual label and the predicted probabilities. If the actual label is 1, use the logarithm of the predicted probability. If the actual label is 0, use the logarithm of one minus the predicted probability.\n\n4. **Sum and Average**: Sum these terms for all samples and then compute the average by dividing by the number of samples.\n\n5. **Log-Loss Calculation**: The log-loss is the negative of this average. Finally, round the result to six decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_log_loss(actual, predicted):\n    \"\"\"Compute the binary classification log-loss (cross-entropy loss).\n    \n    Parameters\n    ----------\n    actual : list or array-like of shape (n_samples,)\n        Ground-truth labels, each element is 0 or 1.\n    predicted : list or array-like of shape (n_samples,)\n        Predicted probabilities, each element is a real number in [0, 1].\n    \n    Returns\n    -------\n    float\n        The average log-loss rounded to six decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    predicted_clipped = np.clip(predicted, \u03b5, 1 - \u03b5)\n    log_p = np.log(predicted_clipped)\n    log_1p = np.log(1 - predicted_clipped)\n    term1 = actual * log_p\n    term2 = (1 - actual) * log_1p\n    sum_terms = term1 + term2\n    total = np.sum(sum_terms)\n    N = len(actual)\n    average = total / N\n    log_loss = -average\n    return round(log_loss, 6)\n```\n\n### Explanation\n- **Clipping Probabilities**: This step ensures that the logarithm function is defined for all predicted probabilities, avoiding undefined values and numerical instability.\n- **Logarithm Calculation**: Using numpy's `log` function, we compute the natural logarithm of the clipped probabilities and their complements.\n- **Term Calculation**: For each sample, we compute the contribution to the loss based on whether the actual label is 0 or 1, ensuring that only relevant terms are considered.\n- **Sum and Average**: Summing these contributions and averaging gives the mean log-loss, which is then negated to get the final log-loss value.\n- **Rounding**: The result is rounded to six decimal places to meet the output requirements.\n\nThis approach efficiently computes the log-loss using vectorized operations, ensuring both accuracy and performance."}
{"task_id": 165, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a measure of how many positions two vectors differ, normalized by the length of the vectors. This distance is widely used in various fields such as information theory and data science.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input vectors are the same. If they are not, we return -1 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty, the Hamming distance is 0.0 because there are no elements to compare.\n3. **Convert to NumPy Arrays**: If the inputs are lists, we convert them to NumPy arrays for efficient element-wise operations.\n4. **Compute Differences**: Using NumPy, we compute a boolean array where each element is True if the corresponding elements in the input vectors differ.\n5. **Sum Differences**: We sum the True values in the boolean array to get the count of differing elements.\n6. **Normalize and Round**: We normalize the count by dividing by the length of the vectors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    N = len(x)\n    \n    if N == 0:\n        return 0.0\n    \n    # Convert to NumPy arrays if they are lists\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Compute element-wise differences\n    diff = x != y\n    \n    # Sum the Trues (count of differing elements)\n    sum_diff = np.sum(diff)\n    \n    # Calculate the normalized Hamming distance\n    distance = sum_diff / N\n    \n    # Round to 4 decimal places\n    return round(distance, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the input vectors have the same length. If not, it returns -1.\n- **Empty Vectors**: If both vectors are empty, the function returns 0.0 since there are no differing elements.\n- **Conversion to Arrays**: The function converts lists to NumPy arrays to leverage efficient element-wise operations.\n- **Element-wise Comparison**: Using NumPy, the function creates a boolean array indicating where elements differ.\n- **Summing Differences**: The sum of the boolean array gives the count of differing elements.\n- **Normalization and Rounding**: The count is divided by the vector length to get the normalized Hamming distance, which is then rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently computes the normalized Hamming distance while handling various input types and edge cases appropriately."}
{"task_id": 171, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression classifier using mini-batch gradient descent. The goal is to train the model on a given training set and use it to predict labels for a test set. The model should handle any two distinct numeric labels and return predictions in the same format as the training labels.\n\n### Approach\n1. **Data Preprocessing**: Convert the input data into a format suitable for training. This includes adding an intercept term to the feature matrix to account for the bias in the model.\n2. **Label Mapping**: Map the given labels to binary values (0 and 1) for internal processing. This allows the model to work with any two distinct numeric labels.\n3. **Special Case Handling**: If all training labels are the same, the model can trivially predict that label for all test samples without training.\n4. **Model Initialization**: Initialize the weight vector to zeros. This vector will be updated during the training process.\n5. **Training with Mini-Batch Gradient Descent**: Divide the training data into mini-batches and update the weights iteratively using the gradient of the loss function. This approach helps in efficiently finding the minimum of the loss function.\n6. **Prediction**: After training, use the model to predict labels for the test set by applying the logistic function and using a decision threshold of 0.5. Convert the predictions back to the original label format before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Handle the case where all labels are the same\n    unique_labels = sorted(list(set(y_train)))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Create a mapping from original labels to 0 and 1\n    label_to_idx = {unique_labels[0]: 0, unique_labels[1]: 1}\n    y_train_mapped = [label_to_idx[label] for label in y_train]\n    \n    # Convert to numpy arrays and add intercept\n    X_train_np = np.array(X_train)\n    X_train_np = np.hstack([X_train_np, np.ones((X_train_np.shape[0], 1))])\n    \n    X_test_np = np.array(X_test)\n    X_test_np = np.hstack([X_test_np, np.ones((X_test_np.shape[0], 1))])\n    \n    # Initialize weights\n    n_features = X_train_np.shape[1]\n    w = np.zeros((n_features, 1))\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Shuffle the data\n        indices = np.random.permutation(len(X_train_np))\n        X_shuffled = X_train_np[indices]\n        y_shuffled = np.array(y_train_mapped)[indices]\n        \n        # Process in batches\n        for i in range(0, len(X_shuffled), batch_size):\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n            \n            # Compute the activation\n            z = np.dot(X_batch, w)\n            a = 1 / (1 + np.exp(-z))\n            \n            # Compute the gradient\n            gradient = np.dot(X_batch.T, (a - y_batch)) / len(X_batch)\n            \n            # Update the weights\n            w = w - learning_rate * gradient\n    \n    # Make predictions on the test set\n    z = np.dot(X_test_np, w)\n    a = 1 / (1 + np.exp(-z))\n    predicted_classes = (a >= 0.5).astype(int)\n    \n    # Map back to original labels\n    original_predictions = [unique_labels[pred] for pred in predicted_classes]\n    \n    return original_predictions\n```\n\n### Explanation\n1. **Data Preprocessing**: The feature matrices `X_train` and `X_test` are converted to numpy arrays and an intercept term is added to each row. This allows the model to learn the bias term.\n2. **Label Mapping**: The labels are mapped to binary values (0 and 1) to simplify the training process. After training, the predictions are converted back to the original label format.\n3. **Special Case Handling**: If all training labels are the same, the model returns that label for all test samples without training.\n4. **Model Initialization**: The weight vector is initialized to zeros, providing a starting point for the optimization process.\n5. **Training with Mini-Batch Gradient Descent**: The training data is shuffled and processed in mini-batches. For each batch, the gradient of the loss function is computed and used to update the weights. This process is repeated for the specified number of epochs.\n6. **Prediction**: After training, the model predicts labels for the test set by applying the logistic function and using a decision threshold of 0.5. The predictions are then converted back to the original label format and returned."}
{"task_id": 172, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the hyperbolic tangent (tanh) function. The derivative of tanh(z) is given by the formula \\(1 - \\text{tanh}^2(z)\\). The function should handle both scalar and array inputs, returning the result as a float for scalar inputs and a list of floats for array inputs, each rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (int, float, or NumPy number). If it is, the function computes the derivative for that single value.\n2. **Array Handling**: If the input is not a scalar, it is treated as an array. The function converts the input to a NumPy array to leverage vectorized operations for efficiency.\n3. **Derivative Calculation**: Using NumPy's `tanh` function, the derivative is computed element-wise as \\(1 - \\text{tanh}^2(z)\\).\n4. **Rounding**: Each element of the result is rounded to four decimal places.\n5. **Output Formatting**: If the input was a scalar, the result is returned as a float. If the input was an array, the result is converted to a list of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        # Handle scalar case\n        derivative = 1 - (np.tanh(z)) ** 2\n        return round(derivative, 4)\n    else:\n        # Handle array or list case\n        z_array = np.array(z)\n        derivative = 1 - (np.tanh(z_array)) ** 2\n        rounded_derivative = np.round(derivative, 4)\n        return rounded_derivative.tolist()\n```\n\n### Explanation\n- **Scalar Input**: When the input is a scalar, the function directly computes the derivative using NumPy's `tanh` function and returns the result as a float rounded to four decimal places.\n- **Array Input**: For array inputs, the function converts the input to a NumPy array, computes the derivative for each element, rounds each element to four decimal places, and returns the result as a list of floats.\n- **Efficiency**: Using NumPy's vectorized operations ensures that the function efficiently handles both scalar and array inputs, providing the correct derivative values in an optimal manner."}
{"task_id": 173, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the alphabetical distance between two strings. The alphabetical distance is defined as the sum of the absolute differences of the ASCII values of each corresponding pair of characters in the two strings. If the strings are of different lengths, we return -1.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two strings are equal. If they are not, we return -1 immediately.\n2. **Initialize Total Distance**: If the lengths are equal, we initialize a variable to keep track of the total distance.\n3. **Iterate Through Characters**: Using the `zip` function, we iterate through each pair of corresponding characters from the two strings.\n4. **Calculate Distance**: For each pair of characters, we compute the absolute difference of their ASCII values and add this to the total distance.\n5. **Return Result**: Finally, we return the total distance.\n\nThis approach ensures that we efficiently compute the distance by leveraging the `zip` function to pair characters and sum their differences.\n\n### Solution Code\n```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for c, t in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the candidate and target strings are the same. If not, it returns -1.\n- **Iterate Through Characters**: Using `zip`, the function pairs each character from the candidate string with the corresponding character from the target string.\n- **Calculate Distance**: For each pair of characters, the function calculates the absolute difference of their ASCII values and accumulates this into a total distance.\n- **Return Result**: The total accumulated distance is returned as the result.\n\nThis solution efficiently handles all edge cases, including empty strings and strings of different lengths, ensuring correct and optimal computation of the alphabetical distance."}
{"task_id": 174, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute two metrics for a GAN discriminator: the average categorical cross-entropy loss and the classification accuracy for a mini-batch of data. The discriminator's output is given as probabilities, and the true labels are provided in one-hot encoded format.\n\n### Approach\n1. **Categorical Cross-Entropy Loss**: This loss is calculated using the formula:\n   \\[\n   \\text{loss} = -\\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\sum_{j=0}^{n_{\\text{classes}}-1} y_{\\text{true}}^{(i)} \\log(y_{\\text{pred}}^{(i,j)})\n   \\]\n   Here, \\( y_{\\text{true}}^{(i)} \\) is 1 for the true class and 0 otherwise, and \\( y_{\\text{pred}}^{(i,j)} \\) is the predicted probability for class \\( j \\) of sample \\( i \\).\n\n2. **Classification Accuracy**: This is the proportion of samples where the predicted class matches the true class. We determine the predicted class using the argmax function on the predictions and compare it with the true classes derived from the one-hot encoded labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Soft-max probabilities with shape (n_samples, n_classes).\n    labels : np.ndarray\n        One-hot encoded true labels with the same shape as *predictions*.\n\n    Returns\n    -------\n    tuple[float, float]\n        (loss, accuracy) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    # Calculate the categorical cross-entropy loss\n    loss = -np.mean(labels * np.log(predictions))\n    \n    # Calculate the accuracy\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(labels, axis=1)\n    correct = np.sum(predicted_classes == true_classes)\n    accuracy = correct / predictions.shape[0]\n    \n    # Round to 4 decimal places\n    loss_rounded = round(loss, 4)\n    accuracy_rounded = round(accuracy, 4)\n    \n    return (loss_rounded, accuracy_rounded)\n```\n\n### Explanation\n1. **Loss Calculation**: The loss is computed by taking the element-wise product of the true labels and the log of the predicted probabilities, then averaging the result and negating it. This gives the average cross-entropy loss for the mini-batch.\n\n2. **Accuracy Calculation**: The predicted classes are determined by finding the maximum value in each row of the predictions. The true classes are derived from the one-hot encoded labels by finding the index of the maximum value in each row. The accuracy is the ratio of correct predictions to the total number of samples.\n\n3. **Rounding**: Both the loss and accuracy are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the required metrics using numpy operations, ensuring both correctness and performance."}
{"task_id": 175, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the L2 regularization penalty for a given set of weights. The L2 penalty is a common technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This penalty term is based on the squared magnitude of each weight, scaled by a regularization strength coefficient.\n\n### Approach\n1. **Convert Weights to NumPy Array**: The weights can be provided as either a Python list or a 1-D NumPy array. To handle both cases uniformly, we convert the weights into a NumPy array.\n2. **Compute Sum of Squares**: Calculate the sum of the squares of each weight. This is done efficiently using NumPy's vectorized operations.\n3. **Apply Regularization Strength**: Multiply the computed sum of squares by the regularization strength coefficient \\( C \\) to get the penalty.\n4. **Round the Result**: The final penalty value is rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    weights = np.array(weights)\n    sum_squares = np.sum(weights ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: This step ensures that both lists and NumPy arrays are handled uniformly, allowing us to use efficient NumPy operations.\n- **Sum of Squares Calculation**: Using NumPy's `sum` and element-wise squaring operations, we efficiently compute the sum of the squares of the weights.\n- **Regularization Application**: The sum of squares is scaled by the regularization strength \\( C \\) to compute the penalty.\n- **Rounding**: The result is rounded to four decimal places to ensure the output meets the specified precision.\n\nThis approach efficiently handles both small and large weight vectors, including edge cases where the weights might be empty or all zeros, ensuring the penalty is computed correctly and efficiently."}
{"task_id": 178, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as \\( f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2 \\) where each \\( x_i \\) is within the range \\([-1, 1]\\). The goal is to find the minimum value of this function using PSO.\n\n### Approach\n1. **Problem Analysis**: The Sphere function is a simple, continuous, and differentiable function used to test optimization algorithms. The PSO algorithm is a population-based optimization technique that simulates the social behavior of birds or fish. Each particle in the swarm has a position and velocity, and it updates its position based on its own best position and the best position of the entire swarm.\n\n2. **Algorithm Setup**: \n   - **Initialization**: Each particle starts with a random position within the specified bounds and a velocity of zero.\n   - **Velocity Update**: The velocity of each particle is updated using the inertia weight, cognitive weight, and social weight, along with random numbers.\n   - **Position Update**: The position of each particle is updated by adding the new velocity, ensuring it stays within the bounds.\n   - **Personal Best and Global Best**: Each particle keeps track of its best position (pbest), and the swarm keeps track of the best position found so far (gbest).\n\n3. **Iteration Process**: For each iteration, all particles update their velocities and positions. After updating, each particle checks if its new position is better than its personal best. The global best is updated if any particle's personal best is better than the current global best.\n\n4. **Termination**: The algorithm runs for a specified number of iterations, and the best value found is returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for invalid inputs\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize particles' positions, velocities, and personal bests\n    positions = rng.uniform(-1, 1, (num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    pbest = positions.copy()\n\n    # Calculate initial global best\n    initial_spheres = np.array([np.sum(p**2) for p in pbest])\n    initial_best_idx = np.argmin(initial_spheres)\n    gbest = pbest[initial_best_idx].copy()\n\n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Generate random numbers\n            r1 = rng.random()\n            r2 = rng.random()\n            \n            # Update velocity\n            velocities[i] = w * velocities[i] + c1 * r1 * (pbest[i] - positions[i]) + c2 * r2 * (gbest - positions[i])\n            \n            # Update position\n            new_position = positions[i] + velocities[i]\n            new_position_clipped = np.clip(new_position, -1, 1)\n            positions[i] = new_position_clipped\n            \n            # Update personal best if new position is better\n            new_sphere = np.sum(new_position_clipped**2)\n            current_pbest_sphere = np.sum(pbest[i]**2)\n            if new_sphere < current_pbest_sphere:\n                pbest[i] = new_position_clipped\n\n        # Update global best\n        pbest_spheres = np.array([np.sum(p**2) for p in pbest])\n        best_pbest_idx = np.argmin(pbest_spheres)\n        new_gbest = pbest[best_pbest_idx]\n        new_gbest_sphere = np.sum(new_gbest**2)\n        current_gbest_sphere = np.sum(gbest**2)\n        if new_gbest_sphere < current_gbest_sphere:\n            gbest = new_gbest\n\n    # Return the best value rounded to four decimal places\n    best_value = np.sum(gbest**2)\n    return round(best_value, 4)\n```\n\n### Explanation\n- **Initialization**: The particles' initial positions are randomly set within the bounds \\([-1, 1]\\). Velocities are initialized to zero.\n- **Velocity Update**: Each particle's velocity is updated using the inertia weight, cognitive weight, and social weight, along with random numbers to ensure exploration.\n- **Position Update**: The new position is calculated by adding the updated velocity to the current position, ensuring it stays within the bounds.\n- **Personal Best Update**: If a particle's new position yields a lower Sphere function value, it updates its personal best.\n- **Global Best Update**: After each iteration, the global best is updated if any particle's personal best is better than the current global best.\n- **Termination**: The algorithm runs for the specified number of iterations, and the best value found is returned, rounded to four decimal places."}
{"task_id": 179, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that tokenizes a given text string into individual characters after applying optional preprocessing steps. The preprocessing steps include converting the text to lowercase, removing punctuation, and handling spaces.\n\n### Approach\n1. **Handle None Input**: If the input string is `None`, return an empty list immediately.\n2. **Convert to Lowercase**: If the `lowercase` parameter is `True`, convert the entire string to lowercase.\n3. **Remove Punctuation**: If the `filter_punctuation` parameter is `True`, remove all standard ASCII punctuation marks from the string using a regular expression.\n4. **Collapse Consecutive Spaces**: Replace any runs of two or more consecutive spaces with a single space.\n5. **Trim Leading/Trailing Spaces**: Remove any leading or trailing spaces from the string.\n6. **Return Characters**: If the resulting string is empty after preprocessing, return an empty list. Otherwise, split the string into individual characters and return them as a list.\n\n### Solution Code\n```python\nimport re\nimport string\n\ndef tokenize_chars(line: str, lowercase: bool = True, filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    \n    # Convert to lowercase if required\n    if lowercase:\n        line = line.lower()\n    \n    # Remove punctuation if required\n    if filter_punctuation:\n        punct_pattern = re.escape(string.punctuation)\n        line = re.sub(r'[' + punct_pattern + r']', '', line)\n    \n    # Replace multiple spaces with a single space\n    line = re.sub(r' {2,}', ' ', line)\n    \n    # Strip leading and trailing whitespace\n    line = line.strip()\n    \n    # Return empty list if the result is empty\n    if not line:\n        return []\n    \n    # Split into individual characters\n    return list(line)\n```\n\n### Explanation\n1. **None Handling**: The function first checks if the input is `None` and returns an empty list if true.\n2. **Lowercase Conversion**: If the `lowercase` parameter is `True`, the input string is converted to lowercase to ensure uniformity.\n3. **Punctuation Removal**: Using a regular expression, all punctuation marks specified in `string.punctuation` are removed from the string if `filter_punctuation` is `True`.\n4. **Space Handling**: Consecutive spaces are collapsed into a single space using a regular expression, and any leading or trailing spaces are removed using the `strip()` method.\n5. **Result Construction**: The processed string is split into individual characters and returned as a list. If the processed string is empty, an empty list is returned.\n\nThis approach ensures that the input string is preprocessed correctly and efficiently, handling various edge cases such as empty strings and strings with only punctuation or spaces."}
{"task_id": 180, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbours (k-NN) algorithm that can work both as a classifier and as a regressor. The function should take training feature vectors, their corresponding target values, test feature vectors, the number of neighbours to consider (k), and the task type (classification or regression). The goal is to predict the target values for the test feature vectors based on the k nearest neighbours in the training set.\n\n### Approach\n1. **Compute Distances**: For each test feature vector, compute the Euclidean distance to each training feature vector. Since we only need the relative distances, we can use the squared Euclidean distance to save computation time.\n2. **Sort and Select Neighbours**: For each test feature vector, sort the training vectors based on their computed distances and select the top k nearest vectors. If k is 0 or larger than the number of training vectors, use all training vectors.\n3. **Predict Based on Task**:\n   - **Classification**: Determine the most frequent label among the k nearest neighbours. In case of a tie, return the smallest label.\n   - **Regression**: Compute the arithmetic mean of the target values of the k nearest neighbours and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    predictions = []\n    n_train = len(X_train)\n    \n    # Determine the effective k\n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    \n    for test_point in X_test:\n        # Compute distances to all training points\n        distances = []\n        for xi, yi in zip(X_train, y_train):\n            # Calculate squared Euclidean distance\n            dist_sq = sum((test_point[i] - xi[i])**2 for i in range(len(test_point)))\n            distances.append((dist_sq, yi))\n        \n        # Sort by distance\n        distances.sort(key=lambda x: x[0])\n        \n        # Select the first 'effective_k' neighbours\n        selected = distances[:effective_k]\n        y_neighbors = [y for (d, y) in selected]\n        \n        if task == \"classification\":\n            # Count the frequency of each label\n            counter = Counter(y_neighbors)\n            # Find the most common label(s)\n            max_count = max(counter.values())\n            # Get all labels with the maximum count\n            candidates = [label for label, count in counter.items() if count == max_count]\n            # Choose the smallest label in case of a tie\n            predicted_label = min(candidates)\n            predictions.append(predicted_label)\n        elif task == \"regression\":\n            # Compute the average and round to 4 decimal places\n            average = sum(y_neighbors) / len(y_neighbors)\n            predictions.append(round(average, 4))\n        else:\n            raise ValueError(\"Task must be 'classification' or 'regression'.\")\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: For each test point, the squared Euclidean distance to each training point is computed. This avoids the computational cost of taking square roots while maintaining the order of distances.\n2. **Neighbour Selection**: The distances are sorted, and the top k (or all if k is 0 or larger than the number of training points) nearest neighbours are selected.\n3. **Classification**: The most frequent label among the selected neighbours is determined. In case of a tie, the smallest label is chosen.\n4. **Regression**: The average of the target values of the selected neighbours is computed and rounded to 4 decimal places.\n\nThis approach ensures that the function efficiently handles both classification and regression tasks while preserving the order of the test samples in the output."}
{"task_id": 182, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the polynomial kernel matrix for given input matrices X and Y, and a specified degree. The polynomial kernel matrix is used in various machine learning algorithms, particularly in Support Vector Machines, to handle non-linearly separable data by transforming it into a higher-dimensional space.\n\n### Approach\n1. **Input Validation**: \n   - Convert the input matrices X and Y into NumPy arrays. If this conversion fails, return -1.\n   - Ensure that both X and Y are 2-dimensional arrays. If not, return -1.\n   - Check that the number of features (columns) in X and Y are the same. If not, return -1.\n   - Ensure that the degree is a positive integer. If not, return -1.\n\n2. **Kernel Matrix Computation**:\n   - Compute the dot product of each row of X with each row of Y transposed. This results in a matrix where each element is the dot product of the corresponding rows.\n   - Raise each element of the resulting matrix to the power of the specified degree.\n\n3. **Rounding and Conversion**:\n   - Round each element of the resulting matrix to four decimal places.\n   - Convert the NumPy array to a nested Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"Compute the polynomial kernel matrix for given X, Y, and degree.\n    \n    Parameters:\n        X (list or ndarray): A 2D array of shape (n, p).\n        Y (list or ndarray): A 2D array of shape (m, p).\n        degree (int, optional): The polynomial degree. Default is 2.\n        \n    Returns:\n        list: A nested list representing the kernel matrix of shape (n, m), with each element rounded to four decimal places.\n        \n    Raises:\n        -1: If any input is invalid.\n    \"\"\"\n    try:\n        X = np.array(X)\n        Y = np.array(Y)\n    except:\n        return -1\n\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n\n    if X.shape[1] != Y.shape[1]:\n        return -1\n\n    if not isinstance(degree, int) or degree < 1:\n        return -1\n\n    K = (X @ Y.T) ** degree\n    K_rounded = np.round(K, 4)\n    return K_rounded.tolist()\n```\n\n### Explanation\n- **Input Validation**: The function first converts the inputs to NumPy arrays and checks their dimensions and feature counts. This ensures that the inputs are valid for matrix operations.\n- **Kernel Matrix Computation**: The polynomial kernel matrix is computed using matrix multiplication and exponentiation. This step transforms the input data into a higher-dimensional space as defined by the polynomial degree.\n- **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted to a nested Python list for the final output.\n\nThis approach efficiently handles the computation and ensures that the inputs are validated before proceeding with the kernel matrix calculation, making it robust and reliable for various applications in machine learning."}
{"task_id": 183, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a weight tensor for a neural network using the He uniform initialization method. This method is particularly useful for layers followed by a ReLU activation function. The initialization ensures that the weights are scaled appropriately to maintain stable gradients during training.\n\n### Approach\n1. **Determine the Fan-In**: The fan-in is the number of incoming connections to a layer. For a dense layer, it is the number of input features, which is the first dimension of the weight shape. For a convolutional layer, it is the product of the filter size (spatial dimensions) and the number of input channels.\n2. **Calculate the Limit**: Using the fan-in, compute the limit for the uniform distribution. The limit is given by the formula \\( \\sqrt{\\frac{6}{\\text{fan-in}}} \\).\n3. **Generate Weights**: Create a weight tensor of the specified shape where each element is uniformly sampled from the interval \\([- \\text{limit}, \\text{limit}]\\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    # Determine the fan-in based on the weight shape\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError(\"Invalid weight shape. Must be 2D or 4D.\")\n    \n    # Calculate the limit for the uniform distribution\n    limit = np.sqrt(6.0 / fan_in)\n    \n    # Generate the weight tensor with values from the uniform distribution\n    weights = np.random.uniform(-limit, limit, weight_shape)\n    \n    return weights\n```\n\n### Explanation\n1. **Fan-In Calculation**: The function first checks the length of the weight shape to determine if it is a dense layer (2D) or a convolutional layer (4D). For dense layers, the fan-in is the first dimension, while for convolutional layers, it is the product of the first three dimensions.\n2. **Limit Calculation**: Using the fan-in, the limit is computed to ensure the weights are scaled appropriately. This limit defines the range from which each weight is uniformly sampled.\n3. **Weight Generation**: The function then generates a tensor of the specified shape, with each element drawn from a uniform distribution between -limit and limit. This ensures that the initial weights are initialized in a way that helps maintain stable gradients during training."}
{"task_id": 184, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary decision tree classifier using the CART algorithm from scratch. The decision tree will use Gini impurity and recursive binary splitting to make predictions. The goal is to build a tree on the training data and then use it to predict labels for unseen test data.\n\n### Approach\n1. **Gini Impurity Calculation**: The Gini impurity measures the probability of misclassifying an element. For a node, it is calculated as 1 minus the sum of the squares of the probabilities of each class.\n2. **Tree Construction**: The tree is built recursively. Each node will either be a leaf or have left and right children. The best feature and threshold for splitting are determined by the maximum Gini gain.\n3. **Splitting Rule**: For each feature, sort the values and evaluate every midpoint between consecutive different values. The feature and threshold with the highest Gini gain are chosen for the split.\n4. **Stopping Criteria**: The tree stops splitting if all labels are identical, the maximum depth is reached, or a split produces an empty child.\n5. **Prediction**: Once the tree is built, each test sample is traversed through the tree to find the predicted class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    def gini(y):\n        count_0 = np.sum(y == 0)\n        count_1 = np.sum(y == 1)\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p0 = count_0 / n\n        p1 = count_1 / n\n        return 1 - (p0 ** 2 + p1 ** 2)\n\n    def build_tree(X, y, current_depth, max_depth):\n        if len(X) == 0:\n            return {'is_leaf': True, 'prediction': 0}\n        if len(np.unique(y)) == 1:\n            return {'is_leaf': True, 'prediction': y[0]}\n        if max_depth is not None and current_depth >= max_depth:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n        \n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_X = None\n        best_left_y = None\n        best_right_X = None\n        best_right_y = None\n        \n        n_features = X.shape[1]\n        for feature in range(n_features):\n            column = X[:, feature]\n            sorted_col = np.sort(column)\n            unique = np.unique(sorted_col)\n            if len(unique) == 1:\n                continue\n            for i in range(len(unique) - 1):\n                threshold = (unique[i] + unique[i + 1]) / 2\n                left_mask = column <= threshold\n                left_X = X[left_mask]\n                left_y = y[left_mask]\n                right_X = X[~left_mask]\n                right_y = y[~left_mask]\n                \n                G_parent = gini(y)\n                G_left = gini(left_y) if len(left_y) > 0 else 0\n                G_right = gini(right_y) if len(right_y) > 0 else 0\n                \n                n_total = len(y)\n                n_left = len(left_y)\n                n_right = len(right_y)\n                if n_total == 0:\n                    continue\n                weighted_G = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n                gain = G_parent - weighted_G\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_left_X = left_X\n                    best_left_y = left_y\n                    best_right_X = right_X\n                    best_right_y = right_y\n        \n        if best_gain > 0:\n            left_child = None\n            right_child = None\n            if len(best_left_X) == 0:\n                majority = np.bincount(y).argmax()\n                left_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                left_child = build_tree(best_left_X, best_left_y, current_depth + 1, max_depth)\n            \n            if len(best_right_X) == 0:\n                majority = np.bincount(y).argmax()\n                right_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                right_child = build_tree(best_right_X, best_right_y, current_depth + 1, max_depth)\n            \n            return {\n                'is_leaf': False,\n                'feature': best_feature,\n                'threshold': best_threshold,\n                'left': left_child,\n                'right': right_child\n            }\n        else:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n\n    def predict_sample(sample, tree):\n        if tree['is_leaf']:\n            return tree['prediction']\n        else:\n            feature = tree['feature']\n            threshold = tree['threshold']\n            if sample[feature] <= threshold:\n                return predict_sample(sample, tree['left'])\n            else:\n                return predict_sample(sample, tree['right'])\n\n    tree = build_tree(X_train, y_train, 0, max_depth)\n    predictions = []\n    for sample in X_test:\n        predictions.append(predict_sample(sample, tree))\n    return predictions\n```\n\n### Explanation\n1. **Gini Impurity Calculation**: The `gini` function calculates the Gini impurity for a given set of labels.\n2. **Tree Construction**: The `build_tree` function recursively constructs the tree. It checks for stopping criteria and finds the best feature and threshold to split on based on Gini gain.\n3. **Prediction**: The `predict_sample` function traverses the tree for a given sample to determine its predicted class label.\n4. **Main Function**: The `decision_tree_predict` function builds the tree using the training data and then uses it to predict labels for the test data.\n\nThis approach ensures that the decision tree is built deterministically based on the given rules and efficiently makes predictions for unseen data."}
{"task_id": 186, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a baseline mean regressor that predicts the mean of the training target values for each test sample. This provides a simple reference score that any reasonable model should beat.\n\n### Approach\n1. **Compute the Mean of Training Targets**: The first step is to compute the arithmetic mean of the training target values, `y_train`. This mean will be used as the prediction for all test samples.\n2. **Handle Empty Test Samples**: If the test feature matrix `X_test` is empty, we return an empty list immediately.\n3. **Generate Predictions**: For each row in `X_test`, generate a prediction equal to the computed mean of `y_train`.\n4. **Round Predictions**: Each prediction is rounded to four decimal places to meet the output requirements.\n5. **Return as List**: Convert the predictions from a numpy array to a list of floats and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Computes the arithmetic mean of the training targets and returns this mean\n    for each test sample, rounded to four decimal places.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places.\n    \"\"\"\n    y_mean = np.mean(y_train)\n    m_samples = len(X_test)\n    if m_samples == 0:\n        return []\n    predictions = np.full(m_samples, y_mean)\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()\n```\n\n### Explanation\n- **Compute Mean**: The mean of `y_train` is calculated using `np.mean(y_train)`, which efficiently computes the average of the target values.\n- **Check for Empty Test Samples**: If `X_test` is empty, the function returns an empty list immediately to handle this edge case.\n- **Generate Predictions**: Using `np.full`, we create an array where each element is the computed mean, repeated for each test sample.\n- **Round Predictions**: The `np.round` function is used to round each prediction to four decimal places.\n- **Convert to List**: The numpy array of predictions is converted to a list of floats to meet the output requirements.\n\nThis approach ensures that the function is efficient and meets all specified constraints, providing a simple yet effective baseline for regression problems."}
{"task_id": 188, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the exponentially decaying exploration rate (\u03b5) for an \u03b5-greedy policy in reinforcement learning. The function must validate its inputs, compute the decayed \u03b5 value using a given formula, and return the result rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: The function first checks if the inputs are valid. The constraints are:\n   - `max_epsilon` must be greater than `min_epsilon`.\n   - `min_epsilon` must be non-negative.\n   - `decay_rate` must be positive.\n   - `step` must be a non-negative integer.\n   If any of these conditions are not met, the function returns -1.\n\n2. **Compute \u03b5(t)**: Using the formula \u03b5(t) = \u03b5_min + (\u03b5_max - \u03b5_min) * e^(-\u03bb * t), where:\n   - `\u03b5_max` is the initial exploration rate.\n   - `\u03b5_min` is the lower bound for \u03b5.\n   - `\u03bb` is the decay rate.\n   - `t` is the current time step.\n\n3. **Clipping the Result**: The computed \u03b5 value is clipped to ensure it does not fall below `\u03b5_min`.\n\n4. **Rounding**: The result is rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport math\n\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    # Validate input parameters\n    if max_epsilon <= min_epsilon:\n        return -1\n    if min_epsilon < 0:\n        return -1\n    if decay_rate <= 0:\n        return -1\n    if step < 0:\n        return -1\n\n    # Compute the exponential decay\n    exponent = -decay_rate * step\n    exponential = math.exp(exponent)\n    term = (max_epsilon - min_epsilon) * exponential\n    epsilon = min_epsilon + term\n\n    # Ensure the result does not fall below min_epsilon\n    epsilon = max(epsilon, min_epsilon)\n\n    # Round to 4 decimal places\n    epsilon_rounded = round(epsilon, 4)\n    return epsilon_rounded\n```\n\n### Explanation\n- **Input Validation**: The function checks if the inputs meet the required constraints. If any constraint is violated, it returns -1.\n- **Exponential Decay Calculation**: The function computes the decayed \u03b5 value using the given formula, ensuring numerical stability by clipping the result to the minimum bound.\n- **Rounding**: The result is rounded to four decimal places to ensure precision and meet the problem's requirements.\n\nThis approach ensures that the exploration rate starts high and gradually decreases, providing a balance between exploration and exploitation in the learning process."}
{"task_id": 193, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth alternative to the hyperbolic tangent function. The Softsign function is defined as:\n\nSoftsign(z) = z / (1 + |z|)\n\nThe function should handle both scalar and array-like inputs, returning the results rounded to 4 decimal places.\n\n### Approach\n1. **Input Handling**: The function should accept either a scalar (int or float) or a one-dimensional list/NumPy array of real numbers.\n2. **Scalar Input**: For a scalar input, compute the Softsign value directly and return it as a float rounded to 4 decimal places.\n3. **Array-like Input**: For an array-like input, compute the Softsign value for each element, round each result to 4 decimal places, and return the results as a Python list preserving the original shape.\n4. **Efficiency**: Use NumPy for efficient computation on array-like inputs, leveraging vectorized operations to compute the Softsign function element-wise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    The function accepts either a scalar or a list/NumPy array. For a scalar it\n    returns a float; for an array-like input it returns a list of floats, with\n    every element rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, (list, np.ndarray)):\n        arr = np.array(z)\n        result = arr / (1 + np.abs(arr))\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n    else:\n        result = z / (1 + abs(z))\n        rounded = round(result, 4)\n        return rounded\n```\n\n### Explanation\n- **Scalar Input Handling**: If the input is a scalar, the function computes the Softsign value using the formula `z / (1 + abs(z))` and returns the result rounded to 4 decimal places.\n- **Array-like Input Handling**: If the input is a list or a NumPy array, the function converts the input to a NumPy array for efficient computation. It then applies the Softsign formula element-wise, rounds each result to 4 decimal places, and converts the result back to a Python list.\n- **Efficiency and Correctness**: By using NumPy for array processing, the function ensures efficient computation, especially for large inputs. The use of vectorized operations ensures that each element is processed correctly and efficiently.\n\nThis approach ensures that the function handles both scalar and array-like inputs correctly, providing the expected results rounded to 4 decimal places."}
{"task_id": 194, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adagrad optimization algorithm, which adjusts the learning rate for each model parameter based on the historical magnitude of its gradients. This approach helps in efficiently navigating the parameter space during training by adapting the learning rate to each parameter individually.\n\n### Approach\nThe Adagrad algorithm consists of two main steps:\n1. **Accumulate the squared gradients**: For each parameter, we maintain a running sum of the squared gradients. This sum is updated at each iteration by adding the square of the current gradient.\n2. **Update the parameters**: Using the accumulated squared gradients, we adjust the learning rate for each parameter. The new parameter value is computed by subtracting the product of the learning rate, the current gradient, and the reciprocal of the square root of the accumulated squared gradients (plus a small constant for numerical stability).\n\nThe steps are as follows:\n1. If the running sum of squared gradients (G) is None, initialize it as a list of zeros with the same length as the weights.\n2. Update each element of G by adding the square of the corresponding gradient.\n3. For each weight, compute the denominator as the square root of the corresponding element in the updated G plus a small constant (epsilon) to avoid division by zero.\n4. Update each weight using the formula: new_weight = weight - learning_rate * gradient / denominator.\n5. Round the updated weights and the new G to 6 decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adagrad_update(weights: list[float],\n                   gradients: list[float],\n                   G: list[float] | None = None,\n                   learning_rate: float = 0.01,\n                   eps: float = 1e-8) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss with respect to *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple  (new_weights, new_G) where\n            new_weights \u2013 list of updated parameters rounded to 6 decimals,\n            new_G       \u2013 list of updated accumulated squared gradients\n                           rounded to 6 decimals.\n    \"\"\"\n    # Initialize new_G if G is None\n    if G is None:\n        new_G = [0.0 for _ in weights]\n    else:\n        new_G = list(G)  # create a copy to avoid modifying the original\n    \n    # Accumulate the squared gradients\n    for i in range(len(weights)):\n        new_G[i] += gradients[i] ** 2\n    \n    # Compute the new weights\n    new_weights = []\n    for i in range(len(weights)):\n        denom = np.sqrt(new_G[i] + eps)\n        update = learning_rate * gradients[i] / denom\n        new_weight = weights[i] - update\n        new_weights.append(round(new_weight, 6))\n    \n    # Round new_G to 6 decimals\n    new_G_rounded = [round(g, 6) for g in new_G]\n    \n    return (new_weights, new_G_rounded)\n```\n\n### Explanation\n- **Initialization**: If G is None, we initialize it as a list of zeros. This is the first iteration, so there are no previous gradients to accumulate.\n- **Accumulating Gradients**: For each parameter, we add the square of the current gradient to the corresponding element in G. This step ensures that we maintain a running total of the squared gradients.\n- **Updating Weights**: Using the accumulated squared gradients, we compute the denominator for each parameter. This denominator helps in adjusting the learning rate. The new weight is then computed by subtracting the product of the learning rate, gradient, and the reciprocal of the denominator from the current weight.\n- **Rounding**: Both the updated weights and the new G are rounded to 6 decimal places to ensure precision and consistency in the results.\n\nThis approach efficiently adapts the learning rate for each parameter, making the optimization process more effective, especially when dealing with sparse data or varying gradient magnitudes."}
{"task_id": 198, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to update the word\u2013topic matrix \u03b2 in Latent Dirichlet Allocation (LDA) based on the current variational parameters and the corpus. The goal is to compute the maximum-likelihood estimate of \u03b2 using the given variational parameters and then normalize each column to sum to one.\n\n### Approach\n1. **Initialization**: Create a matrix \u03b2 initialized to zero with dimensions (V, T), where V is the vocabulary size and T is the number of topics.\n2. **Accumulate Sums**: For each document and each token within the document, accumulate the contributions to \u03b2 using the current variational parameters. Specifically, for each token, if it corresponds to word v, add the variational parameter value for that token and topic to \u03b2[v][t].\n3. **Normalization**: After accumulating the sums, normalize each column of \u03b2 so that the sum of each column equals one. This ensures that each column represents a valid probability distribution over words for each topic.\n4. **Rounding**: Round each entry of the normalized matrix \u03b2 to four decimal places.\n5. **Conversion**: Convert the resulting matrix from a NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    if not phi:\n        return [[0.0 for _ in range(phi[0].shape[1])] for _ in range(V)]\n    \n    T = phi[0].shape[1]\n    beta = np.zeros((V, T), dtype=np.float64)\n    \n    for d in range(len(phi)):\n        current_phi = phi[d]\n        current_corpus = corpus[d]\n        for n in range(current_phi.shape[0]):\n            v = current_corpus[n]\n            beta[v] += current_phi[n, :]\n    \n    for t in range(T):\n        sum_t = beta[:, t].sum()\n        if sum_t == 0:\n            beta[:, t] = 0.0\n        else:\n            beta[:, t] /= sum_t\n    \n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrix \u03b2 is initialized to zero to store the accumulated sums for each word-topic pair.\n2. **Accumulation**: For each document and each token, the corresponding word index is used to update the \u03b2 matrix with the variational parameter values. This step aggregates the contributions of each token to the word-topic probabilities.\n3. **Normalization**: Each column of \u03b2 is normalized to ensure that the sum of probabilities for each topic equals one, which is necessary for a valid probability distribution.\n4. **Rounding**: The values in \u03b2 are rounded to four decimal places to meet the problem's requirements.\n5. **Conversion**: The resulting matrix is converted from a NumPy array to a list of lists for the final output, ensuring the correct format is returned.\n\nThis approach efficiently computes the updated word-topic distribution matrix \u03b2, ensuring it adheres to the constraints and requirements of the LDA model."}
{"task_id": 199, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a label vector into two parts based on a threshold applied to a corresponding feature vector. This is a common task in machine learning, particularly in algorithms like decision-tree learning, where data partitioning is essential.\n\n### Approach\nThe approach involves the following steps:\n1. **Create a Boolean Mask**: We first create a boolean mask where each element is `True` if the corresponding feature value in the feature vector is strictly less than the given threshold, and `False` otherwise.\n2. **Index Labels Using the Mask**: Using this boolean mask, we index the label vector to extract two parts:\n   - `y_left`: Contains labels where the corresponding feature value is strictly less than the threshold.\n   - `y_right`: Contains labels where the corresponding feature value is greater than or equal to the threshold.\n3. **Convert to Python Lists**: Since the output requires regular Python lists, we convert the resulting NumPy arrays to lists before returning them.\n\nThis approach ensures that the order of labels is preserved as per their original positions in the input label vector.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\n```\n\n### Explanation\n- **Boolean Mask Creation**: The boolean mask is created using the condition `X < value`. This mask helps in identifying which elements in the feature vector meet the threshold condition.\n- **Indexing Labels**: Using the boolean mask, we index the label vector `y` to get two separate lists. The elements where the mask is `True` form `y_left`, and the elements where the mask is `False` form `y_right`.\n- **Conversion to Lists**: The resulting NumPy arrays from indexing are converted to Python lists to meet the output requirements.\n\nThis method efficiently partitions the labels based on the given threshold, ensuring that the order of elements is preserved as per their original positions in the input vectors."}
{"task_id": 200, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the L1 regularization penalty for a given set of model weights and a regularization strength. The L1 penalty is calculated as the sum of the absolute values of all the weights multiplied by the regularization strength. This penalty helps in reducing over-fitting by discouraging large weights in the model.\n\n### Approach\n1. **Convert Weights to NumPy Array**: The weights can be provided as a nested list or a NumPy array. We first convert the weights into a NumPy array to handle both cases uniformly.\n2. **Flatten the Array**: Convert the 2D, 3D, or higher-dimensional array into a 1D array to easily iterate over all elements.\n3. **Compute Sum of Absolute Values**: Calculate the sum of the absolute values of all elements in the flattened array.\n4. **Apply Regularization Strength**: Multiply the computed sum by the regularization strength `C` to get the L1 penalty.\n5. **Round the Result**: Round the resulting penalty to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    weights_array = np.asarray(weights)\n    flattened = weights_array.flatten()\n    sum_abs = np.sum(np.abs(flattened))\n    l1 = C * sum_abs\n    return round(l1, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The function starts by converting the input `weights` into a NumPy array. This handles both nested lists and NumPy arrays seamlessly.\n- **Flattening the Array**: Using `flatten()`, the weights array is converted into a 1D array, ensuring all elements are processed regardless of their original dimensionality.\n- **Sum of Absolute Values**: The sum of the absolute values of all elements in the flattened array is computed using `np.sum(np.abs(flattened))`.\n- **Applying Regularization Strength**: The sum is then multiplied by the regularization strength `C` to compute the L1 penalty.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently handles various input structures and ensures the L1 penalty is computed accurately and efficiently."}
{"task_id": 204, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to count the number of missing values in a one-dimensional data container. An element is considered missing if it is either `None` or the special floating-point value `NaN` (Not-a-Number). Strings such as `'nan'` or `'NaN'` are not treated as missing.\n\n### Approach\n1. **Problem Analysis**: The task is to iterate through each element in the given data container and count how many elements are either `None` or `NaN`. The data can be of any type that can be converted into a list, such as a list, tuple, or NumPy array.\n2. **Handling Different Data Types**: We need to handle different data types correctly. Specifically, we need to check if an element is `None` or if it is a float (including NumPy float types) and is `NaN`.\n3. **Special Cases**: \n   - `None` is straightforward to check.\n   - For `NaN`, we use `math.isnan()` function, which works with both Python floats and NumPy float types.\n4. **Efficiency**: The solution efficiently iterates through each element exactly once, making the time complexity O(n), where n is the number of elements in the data container.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or ``NaN``.\n\n    Args:\n        data: Any 1-D iterable (list, tuple, NumPy array, etc.).\n\n    Returns:\n        int: The number of missing elements.\n    \"\"\"\n    count = 0\n    for item in data:\n        if item is None:\n            count += 1\n        else:\n            if isinstance(item, (float, np.floating)) and math.isnan(item):\n                count += 1\n    return count\n```\n\n### Explanation\n- **Initialization**: We start by initializing a counter `count` to zero.\n- **Iteration**: We loop through each element in the input data.\n- **Check for `None`**: If an element is `None`, we increment the counter.\n- **Check for `NaN`**: If an element is a float (including NumPy float types) and is `NaN`, we increment the counter.\n- **Return Result**: After processing all elements, we return the count of missing values.\n\nThis approach ensures that we correctly identify and count all missing values in the data container, handling various data types efficiently."}
{"task_id": 205, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to extract unique environment IDs from a given list of items, where each item may or may not be a dictionary containing an 'id' key. The goal is to return a list of these unique IDs in the order they first appear.\n\n### Approach\n1. **Initialization**: We start by initializing an empty list `unique_ids` to store the unique environment IDs in the order they are first encountered. We also use a set `seen` to keep track of the IDs we have already encountered to avoid duplicates.\n2. **Iteration**: We iterate over each item in the input list `registry`. For each item, we check if it is a dictionary and if it contains the key 'id'.\n3. **Check and Collect IDs**: If an item is a dictionary and contains the 'id' key, we retrieve the value of 'id'. We then check if this ID has already been seen. If not, we add it to both the `unique_ids` list and the `seen` set.\n4. **Return Result**: After processing all items, we return the `unique_ids` list, which contains all unique environment IDs in the order of their first appearance.\n\n### Solution Code\n```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []\n    seen = set()\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                unique_ids.append(env_id)\n                seen.add(env_id)\n    return unique_ids\n```\n\n### Explanation\n- **Initialization**: `unique_ids` is initialized as an empty list to store the result, and `seen` is a set to track encountered IDs.\n- **Iteration**: Each item in the registry is checked to see if it is a dictionary and contains the 'id' key.\n- **Check and Collect IDs**: For each valid dictionary with an 'id', the ID is checked against the `seen` set. If it is new, it is added to both the result list and the set.\n- **Return Result**: The function returns the list of unique IDs, preserving their order of first appearance.\n\nThis approach ensures that we efficiently collect unique IDs while maintaining their order, using a set for quick lookups to avoid duplicates."}
{"task_id": 207, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the spatial dimensions of the output produced by a 2-D convolution layer in a convolutional neural network (CNN). The output dimensions are determined by the input image size, filter size, stride, and padding. If the configuration is invalid, we should return -1.\n\n### Approach\n1. **Extract Parameters**: Extract the height and width of the input image, filter, stride, and padding from the given parameters.\n2. **Validate Parameters**: Ensure all parameters are valid. Specifically, check that the input dimensions, filter dimensions, stride, and padding are positive where required.\n3. **Compute Numerators**: Calculate the numerators for the output height and width using the given formula.\n4. **Check Non-Negative Numerators**: Ensure the numerators are non-negative.\n5. **Check Divisibility**: Ensure the numerators are divisible by the respective stride values.\n6. **Compute Output Dimensions**: If all checks pass, compute the output height and width. If any check fails, return -1.\n\n### Solution Code\n```python\ndef convolution_shape(img_height: int,\n                      img_width: int,\n                      filter_shape: tuple[int, int],\n                      stride: tuple[int, int],\n                      padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height of the input image.\n        img_width (int):  Width of the input image.\n        filter_shape (tuple[int, int]): (kernel_height, kernel_width).\n        stride (tuple[int, int]):       (stride_height, stride_width).\n        padding (tuple[int, int]):      (pad_height, pad_width).\n\n    Returns:\n        tuple[int, int] | int: (output_height, output_width) if the parameters\n            yield a valid integer output size; otherwise -1.\n    \"\"\"\n    # Extract the parameters\n    KH, KW = filter_shape\n    SH, SW = stride\n    PH, PW = padding\n\n    # Check all parameters are valid\n    if (img_height <= 0 or img_width <= 0 or\n        KH <= 0 or KW <= 0 or\n        SH <= 0 or SW <= 0 or\n        PH < 0 or PW < 0):\n        return -1\n\n    # Compute numerators\n    num_h = img_height + 2 * PH - KH\n    num_w = img_width + 2 * PW - KW\n\n    # Check numerators are non-negative\n    if num_h < 0 or num_w < 0:\n        return -1\n\n    # Check divisibility\n    if num_h % SH != 0 or num_w % SW != 0:\n        return -1\n\n    # Compute output dimensions\n    OH = (num_h // SH) + 1\n    OW = (num_w // SW) + 1\n\n    # Check output dimensions are positive\n    if OH <= 0 or OW <= 0:\n        return -1\n\n    return (OH, OW)\n```\n\n### Explanation\n1. **Extract Parameters**: The function starts by extracting the height and width of the input image, filter, stride, and padding from the given tuples.\n2. **Validate Parameters**: It checks if all parameters are valid. For example, input dimensions, filter dimensions, stride, and padding must be positive where required.\n3. **Compute Numerators**: The numerators for the output height and width are computed using the formula provided.\n4. **Check Non-Negative Numerators**: The function ensures that the numerators are non-negative to avoid invalid dimensions.\n5. **Check Divisibility**: The function checks if the numerators are divisible by the respective stride values to ensure integer output dimensions.\n6. **Compute Output Dimensions**: If all checks pass, the output dimensions are computed and returned. If any check fails, the function returns -1 indicating an invalid configuration."}
{"task_id": 208, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a list of categorical labels into their one-hot encoded representation. One-hot encoding is a common strategy in machine learning where each categorical label is converted into a vector where only one element is 1 and the rest are 0, corresponding to the category's position in a predefined list of categories.\n\n### Approach\n1. **Determine Categories**: If the `categories` argument is provided, we use it directly. If not, we infer the categories from the input labels, preserving the order of their first appearance.\n2. **Validation**: If `categories` is provided, we check that all labels in the input are present in the provided categories. If any label is not found, we return -1.\n3. **One-Hot Encoding**: For each label, create a vector where the position corresponding to the category is 1, and all other positions are 0. This is efficiently done using NumPy arrays for better performance, especially with large datasets.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encode(labels: list[str], categories: list[str] | None = None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: A list of categorical labels of length *N*.\n        categories: Optional complete list of unique categories of length *C*.\n            If *None*, the set of categories is inferred from *labels* in the\n            order of their first appearance.\n\n    Returns:\n        A list of lists that represents the one-hot encoded matrix of shape\n        (N, C) where *C* is the number of categories. If an unknown label is\n        found when *categories* is supplied, the function returns -1.\n    \"\"\"\n    if categories is None:\n        # Build categories based on first occurrence of each label\n        categories = []\n        for label in labels:\n            if label not in categories:\n                categories.append(label)\n    else:\n        # Check if all labels are present in the provided categories\n        label_to_index = {label: idx for idx, label in enumerate(categories)}\n        for label in labels:\n            if label not in label_to_index:\n                return -1\n\n    # Create a dictionary mapping each label to its index for quick lookups\n    label_to_index = {label: idx for idx, label in enumerate(categories)}\n\n    # Initialize the one-hot encoding matrix using NumPy for efficiency\n    n = len(labels)\n    c = len(categories)\n    matrix = np.zeros((n, c), dtype=int)\n\n    # Populate the matrix\n    for i in range(n):\n        label = labels[i]\n        idx = label_to_index[label]\n        matrix[i, idx] = 1\n\n    # Convert the NumPy array to a list of lists\n    return matrix.tolist()\n```\n\n### Explanation\n1. **Determine Categories**: The function first checks if `categories` is provided. If not, it builds the categories list by iterating through the input labels and adding each label the first time it appears.\n2. **Validation**: If `categories` is provided, the function checks that all labels in the input are present in the provided categories. If any label is missing, the function returns -1.\n3. **One-Hot Encoding**: Using NumPy, the function creates an initial matrix of zeros. It then iterates through each label, finds its index in the categories list, and sets the corresponding position in the matrix to 1. Finally, the matrix is converted to a list of lists and returned.\n\nThis approach ensures efficient handling of both small and large datasets, leveraging NumPy for performance and clarity in the implementation."}
{"task_id": 211, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification accuracy between two sequences of labels. The classification accuracy is the proportion of correctly predicted labels, which is closely related to the classification error. The classification error is the proportion of misclassified samples, and the accuracy is simply 1 minus the classification error.\n\n### Approach\n1. **Input Validation**: First, we need to check if the input sequences are valid. The sequences must be of the same length and non-empty. If either sequence is empty or their lengths differ, the function should return -1.\n2. **Compute Mismatches**: If the inputs are valid, we then compute the number of mismatches between the actual and predicted labels.\n3. **Calculate Accuracy**: Using the number of mismatches, we compute the classification error and then derive the accuracy by subtracting the classification error from 1.\n4. **Round the Result**: Finally, we round the computed accuracy to four decimal places and return it.\n\n### Solution Code\n```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n    mismatches = sum(1 for a, p in zip(actual, predicted) if a != p)\n    total = len(actual)\n    acc = 1.0 - (mismatches / total)\n    return round(acc, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the actual and predicted sequences are different or if either sequence is empty. If any of these conditions are met, the function returns -1.\n2. **Mismatch Calculation**: Using a generator expression, the function counts the number of mismatches between corresponding elements of the actual and predicted sequences.\n3. **Accuracy Calculation**: The classification error is computed as the number of mismatches divided by the total number of samples. The accuracy is then calculated as 1 minus this error.\n4. **Rounding**: The computed accuracy is rounded to four decimal places using Python's `round` function and returned as the result.\n\nThis approach ensures that the function handles invalid inputs correctly and efficiently computes the classification accuracy for valid inputs."}
{"task_id": 212, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to extract the minimal subgraph from a given neural network model represented as an adjacency list. The goal is to find the shortest path from the layer named \"input\" to the layer named \"output_realtime\". If there are multiple shortest paths, we must return the lexicographically smallest one. If \"output_realtime\" is not reachable from \"input\", we return an empty list.\n\n### Approach\n1. **Graph Representation**: The model is represented as a dictionary where each key is a layer name, and the value is a list of layers it connects to.\n2. **Breadth-First Search (BFS)**: We use BFS to find the shortest path in an unweighted graph. BFS is suitable because it explores all nodes at the present depth level before moving on to nodes at the next depth level, ensuring the shortest path is found.\n3. **Tracking Paths**: For each node, we track the shortest distance from \"input\" and the lexicographically smallest path to reach it. This is done using a dictionary where each key is a node, and the value is a tuple containing the distance and the path.\n4. **Queue Management**: We use a queue to manage the nodes to be processed. Each entry in the queue is a tuple containing the current node and the path taken to reach it.\n5. **Lexicographical Order**: When multiple paths of the same length are found, we compare them lexicographically and keep the smallest one.\n\n### Solution Code\n```python\nfrom collections import deque\n\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Find a shortest path from 'input' to 'output_realtime'.\n\n    The *model* is given as an adjacency list `graph` where every key is the\n    name of a layer and the associated list contains the names of its outgoing\n    neighbours.\n\n    The function must return the names of the layers that lie on ONE shortest\n    path starting at the layer named ``\"input\"`` and ending at the layer named\n    ``\"output_realtime\"``. If several shortest paths are possible the\n    lexicographically smallest one (as complete lists) is returned.  When no\n    path exists an empty list is returned.\n\n    Parameters\n    ----------\n    graph : dict[str, list[str]]\n        Adjacency list of the directed graph.\n\n    Returns\n    -------\n    list[str]\n        Ordered list of layer names on the desired path or an empty list.\n    \"\"\"\n    if 'input' not in graph or 'output_realtime' not in graph:\n        return []\n    \n    best = {}\n    best['input'] = (0, ['input'])\n    queue = deque()\n    queue.append(('input', ['input']))\n    \n    while queue:\n        current_node, current_path = queue.popleft()\n        current_distance = len(current_path) - 1  # since path starts with 'input' as step 0\n        \n        # Check if this is the best path for current_node\n        if best.get(current_node, (float('inf'), []))[1] != current_path:\n            continue\n        \n        for neighbor in graph.get(current_node, []):\n            new_distance = current_distance + 1\n            new_path = current_path + [neighbor]\n            \n            if neighbor not in best:\n                best[neighbor] = (new_distance, new_path)\n                queue.append((neighbor, new_path))\n            else:\n                existing_distance, existing_path = best[neighbor]\n                if new_distance < existing_distance:\n                    best[neighbor] = (new_distance, new_path)\n                    queue.append((neighbor, new_path))\n                elif new_distance == existing_distance:\n                    if new_path < existing_path:\n                        best[neighbor] = (new_distance, new_path)\n                        queue.append((neighbor, new_path))\n    \n    if 'output_realtime' in best:\n        return best['output_realtime'][1]\n    else:\n        return []\n```\n\n### Explanation\n1. **Initialization**: We start by checking if \"input\" and \"output_realtime\" are present in the graph. If not, we return an empty list.\n2. **BFS Setup**: We initialize the BFS with the starting node \"input\" and an empty path. The queue is used to manage the nodes to be processed.\n3. **Processing Nodes**: For each node dequeued, we check if the current path is the best known path. If not, we skip processing this node to avoid redundant work.\n4. **Updating Best Paths**: For each neighbor of the current node, we calculate the new distance and path. If this new path is better (shorter or lexicographically smaller), we update our records and enqueue the neighbor.\n5. **Result Extraction**: After processing all nodes, we check if \"output_realtime\" is reachable. If it is, we return the best path found; otherwise, we return an empty list.\n\nThis approach ensures that we efficiently find the shortest path while considering lexicographical order when multiple shortest paths exist."}
{"task_id": 213, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that initializes weights using the He Normal initialization strategy. This strategy is commonly used in deep learning to speed up the training process by providing a good initial distribution of weights.\n\n### Approach\nThe He Normal initialization draws weights from a normal distribution with a mean of 0 and a standard deviation calculated based on the number of incoming connections (fan_in). The steps to achieve this are as follows:\n\n1. **Compute fan_in and fan_out**: These values are derived from the shape of the weight tensor. The rules for computation are:\n   - For a 1D shape, both fan_in and fan_out are equal to the single element of the shape.\n   - For a 2D shape, fan_in is the first dimension and fan_out is the second dimension.\n   - For shapes with 3 or more dimensions, the first two dimensions are considered as out_channels and in_channels, respectively. The remaining dimensions form the receptive field, and fan_in and fan_out are computed using the product of the in_channels and the receptive field size, and the product of the out_channels and the receptive field size, respectively.\n\n2. **Handle edge cases**: If fan_in is zero, return an empty list as no valid initialization is possible.\n\n3. **Generate weights**: Using NumPy's random number generator with a fixed seed, generate weights from a normal distribution with the computed standard deviation.\n\n4. **Return the result**: Convert the generated NumPy array to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with He Normal strategy.\n\n    Args:\n        shape: A tuple that represents the desired tensor shape.\n\n    Returns:\n        Nested Python list containing the initialised weights.\n    \"\"\"\n    ndim = len(shape)\n    fan_in = 0\n    fan_out = 0\n\n    if ndim == 1:\n        fan_in = shape[0]\n        fan_out = shape[0]\n    elif ndim == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    else:\n        out_channels = shape[0]\n        in_channels = shape[1]\n        receptive_field_size = 1\n        for dim in shape[2:]:\n            receptive_field_size *= dim\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n\n    if fan_in == 0:\n        return []\n\n    std = np.sqrt(2.0 / fan_in)\n    np.random.seed(42)\n    W = np.random.normal(0, std, shape)\n    return W.tolist()\n```\n\n### Explanation\n- **Compute fan_in and fan_out**: The function first determines the dimensions of the tensor. Depending on the number of dimensions, it calculates fan_in and fan_out using the specified rules.\n- **Edge case handling**: If fan_in is zero, the function returns an empty list immediately.\n- **Generate weights**: Using NumPy's `normal` function, weights are generated from a normal distribution with the mean set to 0 and the standard deviation calculated as the square root of (2 divided by fan_in). The seed is fixed to ensure reproducibility.\n- **Return result**: The generated array is converted to a nested list and returned.\n\nThis approach ensures that the weights are initialized in a way that helps in training deep learning models efficiently by maintaining the stability of the gradients."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input containers `actual` and `predicted` into NumPy arrays to facilitate vectorized computations.\n2. **Compute Probability**: Using the sigmoid function, we compute the probability for each predicted value. The sigmoid function transforms any real number into a value between 0 and 1.\n3. **Compute Gradient**: The gradient of the logistic loss is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the element-wise product of `actual` and the sigmoid of the negative product of `actual` and `predicted`.\n4. **Compute Hessian**: The Hessian is the second derivative of the logistic loss and is computed as `sigmoid(predicted_i) * (1 - sigmoid(predicted_i))`.\n5. **Round Results**: Each of the computed gradient, Hessian, and probability values is rounded to six decimal places to meet the problem's requirements.\n6. **Convert to Lists**: Finally, the NumPy arrays are converted back to Python lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute the probability using the sigmoid function\n    prob = 1 / (1 + np.exp(-predicted))\n    \n    # Compute the gradient of the logistic loss\n    z = -actual * predicted\n    sigmoid_z = 1 / (1 + np.exp(z))\n    gradient = actual * sigmoid_z\n    \n    # Compute the Hessian of the logistic loss\n    hessian = prob * (1 - prob)\n    \n    # Round each component to six decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert numpy arrays to lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Probability Calculation**: The sigmoid function is used to transform the predicted values into probabilities. This is done using the formula `1 / (1 + exp(-predicted))`.\n- **Gradient Calculation**: The gradient is derived from the logistic loss function and is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the sigmoid function applied to the negative product of `actual` and `predicted`.\n- **Hessian Calculation**: The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Rounding and Conversion**: The results are rounded to six decimal places and converted from NumPy arrays to Python lists for the final output.\n\nThis approach ensures that the computations are efficient and accurate, leveraging vectorized operations provided by NumPy for optimal performance."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is used in neural networks to avoid the \"dying\" neurons issue. The function should handle inputs that are scalars, nested lists, or numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For a given input value \\( x \\), if \\( x \\) is non-negative, the output is \\( x \\).\n- If \\( x \\) is negative, the output is \\( a \\times x \\), where \\( a \\) is a small positive float (default value is 0.01).\n\nThe approach to solve this problem involves:\n1. **Handling Numpy Arrays**: If the input is a numpy array, apply the Leaky ReLU activation element-wise using vectorized operations for efficiency.\n2. **Handling Lists**: If the input is a list (which can be nested), recursively process each element. This ensures that the output maintains the same nested structure as the input.\n3. **Handling Scalars**: If the input is a scalar, directly apply the Leaky ReLU activation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(e, a) for e in z]\n    else:\n        if z >= 0:\n            return z\n        else:\n            return a * z\n```\n\n### Explanation\n1. **Numpy Array Handling**: The function first checks if the input `z` is a numpy array. If it is, the Leaky ReLU activation is applied element-wise using `np.where`, which efficiently processes the entire array. The result is then converted to a list using `tolist()` to maintain the required output format.\n2. **List Handling**: If `z` is a list, the function processes each element recursively. This ensures that each element, whether it is a scalar, another list, or a numpy array, is processed correctly, maintaining the nested structure of the input.\n3. **Scalar Handling**: If `z` is a scalar, the function directly applies the Leaky ReLU activation. If the scalar is non-negative, it is returned as is. If it is negative, it is multiplied by `a` before being returned.\n\nThis approach ensures that the function works efficiently for all types of inputs and maintains the structure of the input, providing the correct Leaky ReLU activation for each element."}
{"task_id": 225, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given frequency or array of frequencies from Hertz (Hz) to the mel scale using either the HTK or Slaney formula. The mel scale is a perceptually motivated scale used in speech and audio processing, and it provides a more accurate representation of how humans perceive sound frequencies.\n\n### Approach\n1. **Input Handling**: The function accepts an array-like input which can be one or multi-dimensional. This input is converted into a numpy array for efficient computation.\n2. **Formula Selection**: The function checks the provided formula parameter to determine whether to use the HTK or Slaney formula. If an unsupported formula is provided, an AssertionError is raised.\n3. **Conversion**: Depending on the selected formula, the function converts the Hz values to mel values using the respective formula:\n   - HTK formula: `mel = 2595 * log10(1 + hz / 700)`\n   - Slaney formula: `mel = 1127 * ln(1 + hz / 700)`\n4. **Rounding**: The resulting mel values are rounded to four decimal places.\n5. **Output Conversion**: The result is converted back to a Python list, preserving the original shape of the input. If the input was a scalar, the result is returned as a single-element list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str = \"htk\") -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    if formula not in [\"htk\", \"slaney\"]:\n        raise AssertionError(\"formula must be 'htk' or 'slaney'\")\n    \n    original_is_scalar = np.isscalar(hz)\n    hz_array = np.asarray(hz)\n    \n    if formula == \"htk\":\n        mel = 2595 * np.log10(1 + hz_array / 700)\n    else:\n        mel = 1127 * np.log(1 + hz_array / 700)\n    \n    rounded_mel = np.round(mel, 4)\n    \n    if original_is_scalar:\n        return [rounded_mel.item()]\n    else:\n        return rounded_mel.tolist()\n```\n\n### Explanation\n- **Input Handling**: The input is converted to a numpy array to handle both scalar and array-like inputs efficiently.\n- **Formula Selection**: The function checks if the provided formula is valid and raises an error if it is not.\n- **Conversion**: Depending on the formula, the appropriate mathematical transformation is applied to convert Hz to mel values.\n- **Rounding**: The results are rounded to four decimal places to meet the specified precision.\n- **Output Conversion**: The result is converted back to a Python list, ensuring the output maintains the same shape as the input, including handling scalar inputs appropriately.\n\nThis approach ensures that the function is both efficient and versatile, handling various input types and shapes while providing accurate mel scale conversions."}
{"task_id": 227, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a forward pass of a two-hidden-layer value network using NumPy. The network architecture consists of a state input, two hidden layers with hyperbolic tangent (tanh) activation functions, and a linear output layer. The task involves validating the input dimensions, performing the forward pass, and returning the result rounded to four decimal places.\n\n### Approach\n1. **Dimension Validation**: \n   - Ensure the state vector has the correct length.\n   - Verify the dimensions of the weight matrices W1, W2, and W3 to ensure they match the expected shapes based on the network architecture.\n   \n2. **Forward Pass**:\n   - Compute the output of the first hidden layer using the state vector and the first weight matrix, followed by applying the tanh activation function.\n   - Compute the output of the second hidden layer using the output from the first hidden layer and the second weight matrix, followed by applying the tanh activation function.\n   - Compute the final value by multiplying the output of the second hidden layer with the third weight matrix.\n\n3. **Result Processing**:\n   - Round the resulting scalar value to four decimal places and return it as a float. If any dimension check fails, return -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef value_network_forward(state: list[float],\n                          W1: list[list[float]],\n                          W2: list[list[float]],\n                          W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\n\n    Parameters\n    ----------\n    state : list[float]\n        1-D state vector of length N.\n    W1    : list[list[float]]\n        Weight matrix with shape (N, H1).\n    W2    : list[list[float]]\n        Weight matrix with shape (H1, H2).\n    W3    : list[list[float]] | list[float]\n        Weight matrix with shape (H2, 1) or (H2,) for the output layer.\n\n    Returns\n    -------\n    float\n        The scalar state-value rounded to 4 decimals, or \u22121 on shape error.\n    \"\"\"\n    # Convert state to numpy array\n    state_np = np.array(state)\n    N = len(state_np)\n    \n    # Check W1 dimensions\n    if len(W1) != N:\n        return -1\n    H1 = len(W1[0])\n    for row in W1:\n        if len(row) != H1:\n            return -1\n    \n    # Check W2 dimensions\n    if len(W2) != H1:\n        return -1\n    H2 = len(W2[0])\n    for row in W2:\n        if len(row) != H2:\n            return -1\n    \n    # Check W3 dimensions\n    if isinstance(W3[0], list):\n        # W3 is 2D\n        if len(W3) != H2:\n            return -1\n        for row in W3:\n            if len(row) != 1:\n                return -1\n    else:\n        # W3 is 1D\n        if len(W3) != H2:\n            return -1\n    \n    # Perform forward pass\n    h1 = np.tanh(np.dot(state_np, W1))\n    h2 = np.tanh(np.dot(h1, W2))\n    W3_np = np.array(W3)\n    v = h2.dot(W3_np)\n    \n    # Round to 4 decimals\n    rounded_v = round(v, 4)\n    \n    return rounded_v\n```\n\n### Explanation\n1. **Dimension Validation**:\n   - The function first checks if the dimensions of the input matrices match the expected shapes. If any dimension mismatch is found, the function returns -1.\n\n2. **Forward Pass**:\n   - The state vector is converted to a NumPy array for efficient matrix operations.\n   - The first hidden layer output is computed by multiplying the state vector with the first weight matrix and applying the tanh activation function.\n   - The second hidden layer output is computed similarly using the output from the first hidden layer and the second weight matrix.\n   - The final value is obtained by multiplying the output of the second hidden layer with the third weight matrix.\n\n3. **Result Processing**:\n   - The resulting value is rounded to four decimal places and returned. If any dimension check fails, the function returns -1 to indicate an error."}
{"task_id": 231, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given hand in the game of Blackjack is a natural blackjack. A natural blackjack is defined as a hand consisting of exactly two cards: one Ace (represented as 1) and one ten-valued card (represented as 10).\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Length of the Hand**: The hand must contain exactly two cards. If the length of the hand is not 2, the function should return False.\n2. **Count the Specific Cards**: For a hand to be a natural blackjack, it must contain exactly one Ace (1) and exactly one ten-valued card (10). We can achieve this by counting the occurrences of 1 and 10 in the hand. If both counts are exactly 1, the function returns True; otherwise, it returns False.\n\n### Solution Code\n```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determines if the given hand is a natural blackjack.\n    \n    A natural blackjack is a two-card hand consisting of an Ace (1) and a ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1\n```\n\n### Explanation\n- **Check Length**: The function first checks if the length of the hand is exactly 2. If not, it immediately returns False.\n- **Count Cards**: The function then counts how many times 1 (Ace) and 10 (ten-valued card) appear in the hand. If both counts are exactly 1, the function returns True, indicating a natural blackjack. Otherwise, it returns False.\n\nThis approach efficiently checks the necessary conditions using simple counting operations, ensuring that the solution is both correct and optimal."}
{"task_id": 232, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given NumPy array is a valid one-hot matrix. A valid one-hot matrix must satisfy three conditions: it must be two-dimensional, every element must be either 0 or 1, and each row must contain exactly one 1.\n\n### Approach\n1. **Check Dimensionality**: The array must be two-dimensional. This is verified using `x.ndim == 2`.\n2. **Check Binary Values**: Every element in the array must be either 0 or 1. This is checked using `np.all((x == 0) | (x == 1))`.\n3. **Check Row Sums**: Each row must sum to exactly 1. This is verified by checking if all elements of the sum of each row (along axis=1) are equal to 1, using `np.all(x.sum(axis=1) == 1)`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if x.ndim != 2:\n        return False\n    if not np.all((x == 0) | (x == 1)):\n        return False\n    if not np.all(x.sum(axis=1) == 1):\n        return False\n    return True\n```\n\n### Explanation\n1. **Dimension Check**: The function first checks if the array is two-dimensional. If not, it returns `False`.\n2. **Binary Check**: It then ensures all elements are either 0 or 1. If any element is outside this range, it returns `False`.\n3. **Row Sum Check**: Finally, it checks that each row sums to exactly 1. If any row does not meet this condition, it returns `False`.\n\nIf all three conditions are satisfied, the function returns `True`, indicating the array is a valid one-hot matrix. This approach efficiently verifies each condition using NumPy operations, ensuring the solution is both concise and performant."}
{"task_id": 234, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement single-step Thompson sampling for a Bernoulli multi-armed bandit. The goal is to select the arm with the highest sampled payoff probability, using a Beta prior distribution for each arm's success probability.\n\n### Approach\n1. **Input Validation**: Check if the input lists `successes` and `failures` are valid. If either list is empty or their lengths differ, return `-1`.\n2. **Set Random Seed**: If a seed is provided, set the NumPy random seed to ensure reproducibility.\n3. **Posterior Distribution**: For each arm, compute the parameters of the Beta posterior distribution using the observed successes and failures, combined with the prior parameters.\n4. **Sample from Posterior**: Draw a sample from each arm's posterior distribution.\n5. **Select Best Arm**: Determine the arm with the highest sampled value. In case of a tie, select the arm with the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta:  float = 1.0,\n                                    seed: int | None = None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\n\n    Parameters\n    ----------\n    successes : list[int]\n        Number of observed successes for every arm.\n    failures  : list[int]\n        Number of observed failures for every arm (same length as *successes*).\n    prior_alpha : float, optional (default=1.0)\n        Shared Alpha parameter of the Beta prior.\n    prior_beta  : float, optional (default=1.0)\n        Shared Beta  parameter of the Beta prior.\n    seed : int | None, optional (default=None)\n        If given, NumPy\u2019s random seed is set to this value to obtain\n        deterministic samples.\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest sampled payoff probability, or\n        -1 if the input is malformed.\n    \"\"\"\n    # Check for invalid input\n    if not successes or not failures:\n        return -1\n    if len(successes) != len(failures):\n        return -1\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate samples for each arm\n    samples = []\n    for i in range(len(successes)):\n        a = prior_alpha + successes[i]\n        b = prior_beta + failures[i]\n        sample = np.random.beta(a, b)\n        samples.append(sample)\n    \n    # Find the arm with the maximum sample, breaking ties by choosing the smallest index\n    best_arm = np.argmax(samples)\n    \n    return best_arm\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input lists are valid. If either list is empty or their lengths differ, it returns `-1`.\n2. **Setting Seed**: If a seed is provided, it sets the random seed to ensure that the sampling process is reproducible.\n3. **Posterior Parameters**: For each arm, the parameters of the Beta posterior distribution are calculated by adding the prior parameters to the observed successes and failures.\n4. **Sampling**: A sample is drawn from each arm's posterior distribution using NumPy's `beta` function.\n5. **Best Arm Selection**: The arm with the highest sampled value is selected. In case of a tie, the smallest index is chosen, which is handled by NumPy's `argmax` function.\n\nThis approach ensures that we efficiently and correctly implement the Thompson sampling algorithm for a Bernoulli multi-armed bandit, providing the optimal arm selection based on the sampled probabilities."}
{"task_id": 237, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the total score of a Blackjack hand, considering the special value of Aces. The goal is to determine the highest possible score without exceeding 21 points. If the score exceeds 21, the hand is considered bust, and the function should return 0.\n\n### Approach\n1. **Card Values**: Each card has a specific value:\n   - Numeric cards ('2' to '10') are worth their face value.\n   - Face cards ('J', 'Q', 'K') are each worth 10 points.\n   - Aces ('A') can be worth either 11 or 1 point. The optimal value for each Ace is determined to maximize the total score without exceeding 21.\n\n2. **Initial Calculation**: Sum the values of all cards, treating each Ace as 11 initially. Keep track of the number of Aces encountered.\n\n3. **Adjust for Aces**: If the total score exceeds 21, convert Aces from 11 to 1 one at a time until the total is 21 or less, or until there are no more Aces to convert.\n\n4. **Return Result**: If the adjusted total is 21 or less, return the total. Otherwise, return 0 indicating a bust.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    total = 0\n    num_aces = 0\n    for card in hand:\n        if card in ['J', 'Q', 'K']:\n            total += 10\n        elif card == 'A':\n            total += 11\n            num_aces += 1\n        else:\n            total += int(card)\n    \n    # Adjust for aces if total exceeds 21\n    while total > 21 and num_aces > 0:\n        total -= 10\n        num_aces -= 1\n    \n    return total if total <= 21 else 0\n```\n\n### Explanation\n1. **Initialization**: The function initializes `total` to 0 and `num_aces` to 0 to keep track of the score and the number of Aces.\n\n2. **Card Processing**: Each card is processed to add its value to `total`. Aces are initially counted as 11, and their count is tracked separately.\n\n3. **Adjusting Aces**: If the total score exceeds 21, the function converts Aces from 11 to 1 one at a time until the total is 21 or less or there are no more Aces to convert.\n\n4. **Result**: The function returns the total score if it is 21 or less. If the total exceeds 21 even after adjusting all possible Aces, it returns 0, indicating a bust.\n\nThis approach ensures that the highest possible score is calculated efficiently while adhering to the rules of Blackjack."}
{"task_id": 239, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the amount of zero-padding required for a 2-D convolution operation in a convolutional neural network (CNN). The padding is determined based on the filter size and the desired output shape, which can be either \"same\" or \"valid\".\n\n### Approach\n1. **Understanding Output Shapes**:\n   - For \"valid\" output shape, no padding is required. Thus, we return `((0, 0), (0, 0))`.\n   - For \"same\" output shape, the goal is to ensure that the output dimensions match the input dimensions. This requires calculating the necessary padding such that the convolution operation does not reduce the spatial resolution of the input.\n\n2. **Padding Calculation**:\n   - When the output shape is \"same\", the padding is calculated to ensure that the output height and width are the same as the input height and width. This is done by adding zeros around the input image.\n   - The padding is computed for both height and width dimensions. For each dimension, the total padding required is `(filter_size - 1)`. This total padding is split such that the smaller part is added to the top/left and the larger part to the bottom/right. This ensures that if the padding is odd, the smaller part is placed on the top/left and the larger part on the bottom/right.\n\n### Solution Code\n```python\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str = \"same\") -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\n\n    Parameters\n    ----------\n    filter_shape : tuple[int, int]\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : str, optional (default=\"same\")\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    tuple[tuple[int, int], tuple[int, int]]\n        ((pad_top, pad_bottom), (pad_left, pad_right)) where every value is an\n        integer \u2265 0.\n    \"\"\"\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n    else:\n        h, w = filter_shape\n        # Calculate padding for height dimension\n        pad_h_total = h - 1\n        pad_top = pad_h_total // 2\n        pad_bottom = pad_h_total - pad_top\n        # Calculate padding for width dimension\n        pad_w_total = w - 1\n        pad_left = pad_w_total // 2\n        pad_right = pad_w_total - pad_left\n        return ((pad_top, pad_bottom), (pad_left, pad_right))\n```\n\n### Explanation\n- **Valid Output Shape**: When the output shape is \"valid\", no padding is needed, so we return `((0, 0), (0, 0))`.\n- **Same Output Shape**: For \"same\" output shape, we calculate the padding required for both height and width dimensions. The total padding for each dimension is `(filter_size - 1)`, which is split into two parts. The smaller part is added to the top/left, and the larger part is added to the bottom/right. This ensures that the output dimensions match the input dimensions after the convolution operation.\n\nThis approach efficiently computes the necessary padding to maintain the spatial resolution of the input image when using a \"same\" convolution, while ensuring no padding is added for a \"valid\" convolution."}
{"task_id": 240, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to build four dictionaries that map observation and action identifiers to unique integers and vice versa. These mappings are essential for efficiently indexing tables, tensors, or neural-network outputs in reinforcement learning.\n\n### Approach\n1. **Problem Analysis**: The task requires creating two sets of mappings (forward and inverse) for observations and actions. If explicit encoders are provided, they are used directly. Otherwise, the mappings are created by enumerating the identifiers in the order they appear in the provided lists.\n\n2. **Intuition**: The forward mappings (`obs2num` and `act2num`) convert identifiers to integers, while the inverse mappings (`num2obs` and `num2act`) convert integers back to identifiers. If an explicit encoder is provided, it is used; otherwise, the identifiers are mapped to integers based on their order in the list.\n\n3. **Algorithm**:\n   - Check if explicit encoders are provided for observations and actions.\n   - If provided, use them directly to create the forward mappings.\n   - If not provided, create the forward mappings by enumerating the identifiers.\n   - Construct the inverse mappings by swapping keys and values of the forward mappings.\n\n4. **Special Cases**:\n   - If a list is empty and no encoder is provided, the corresponding dictionaries are empty.\n   - Ensure the inverse dictionaries are exact inverses of their forward counterparts.\n\n### Solution Code\n```python\ndef create_encoding_dicts(obs_ids: list, action_ids: list,\n                          obs_encoder: dict | None = None,\n                          act_encoder: dict | None = None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    # Initialize the dictionaries\n    obs2num = {}\n    act2num = {}\n    \n    # Handle observation encodings\n    if obs_encoder is not None:\n        obs2num = obs_encoder.copy()\n    else:\n        if obs_ids:\n            obs2num = {id: idx for idx, id in enumerate(obs_ids)}\n    \n    # Create inverse observation mapping\n    num2obs = {v: k for k, v in obs2num.items()}\n    \n    # Handle action encodings\n    if act_encoder is not None:\n        act2num = act_encoder.copy()\n    else:\n        if action_ids:\n            act2num = {id: idx for idx, id in enumerate(action_ids)}\n    \n    # Create inverse action mapping\n    num2act = {v: k for k, v in act2num.items()}\n    \n    # Return all four mappings\n    return {\n        'obs2num': obs2num,\n        'num2obs': num2obs,\n        'act2num': act2num,\n        'num2act': num2act\n    }\n```\n\n### Explanation\n- **Forward Mappings**: `obs2num` and `act2num` are created either from provided encoders or by enumerating the identifiers in the order they appear in the lists.\n- **Inverse Mappings**: `num2obs` and `num2act` are created by swapping the keys and values of the forward mappings, ensuring they are exact inverses.\n- **Special Handling**: If a list is empty and no encoder is provided, the corresponding dictionaries are left empty. This ensures consistency and efficiency in mapping operations."}
{"task_id": 242, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the UCB1 (Upper-Confidence-Bound) algorithm for the stochastic multi-armed bandit problem. The goal is to select the arm that maximizes the expected reward while balancing exploration and exploitation.\n\n### Approach\nThe UCB1 algorithm works by maintaining empirical mean rewards and the number of times each arm has been pulled. It uses these values to compute a score for each arm, which is a combination of the empirical mean reward and an exploration term. The arm with the highest score is selected, ensuring that every arm is tried at least once.\n\nThe algorithm follows these steps:\n1. **Check for Unpulled Arms**: If any arm has never been pulled, select the first such arm to ensure exploration.\n2. **Compute Scores**: For each arm, compute the UCB score using the formula:\n   \\[\n   \\text{score}(a) = \\text{ev\\_estimates}[a] + C \\times \\sqrt{\\frac{2 \\ln(\\text{current\\_step} + 1)}{\\text{pull\\_counts}[a]}}\n   \\]\n   where \\(C\\) is a parameter that controls the exploration-exploitation trade-off.\n3. **Select the Best Arm**: Choose the arm with the highest score. In case of ties, select the arm with the smallest index.\n\n### Solution Code\n```python\nimport math\n\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float = 1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    # Check if any arm has not been pulled yet\n    for a in range(len(pull_counts)):\n        if pull_counts[a] == 0:\n            return a\n    \n    current_step_plus_1 = current_step + 1\n    log_term = math.log(current_step_plus_1)\n    max_score = -math.inf\n    best_arm = 0\n    \n    for a in range(len(ev_estimates)):\n        denominator = pull_counts[a]\n        sqrt_term = math.sqrt((2 * log_term) / denominator)\n        score = ev_estimates[a] + C * sqrt_term\n        \n        if score > max_score:\n            max_score = score\n            best_arm = a\n    \n    return best_arm\n```\n\n### Explanation\n1. **Check for Unpulled Arms**: The function first checks if any arm has a pull count of zero. If found, it immediately returns the index of the first such arm to ensure that every arm is tried at least once.\n2. **Compute Scores**: If all arms have been pulled, the function calculates the UCB score for each arm. The score is computed using the empirical mean reward and an exploration term that decreases as the number of pulls for the arm increases.\n3. **Select the Best Arm**: The function iterates through each arm, computes its score, and keeps track of the highest score. If multiple arms have the same score, the function selects the arm with the smallest index, ensuring consistency with Python's max function behavior.\n\nThis approach efficiently balances exploration and exploitation, ensuring that the algorithm converges to the optimal arm while exploring all available options."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using basic NumPy operations. The network architecture consists of three fully-connected layers followed by two separate output heads for the actor (policy) and the critic (state-value function). The weights and biases are fixed and identical to one, which simplifies the computation.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers (each with ReLU activation) followed by two output heads. The actor head uses a softmax activation to produce action probabilities, while the critic head uses a linear activation to produce a scalar state value.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0. This simplifies the computation as each layer's output can be derived directly from the input without complex weight matrices.\n3. **Computation Steps**:\n   - Compute the sum of the input state.\n   - Use this sum to compute the outputs of the hidden layers using ReLU activation.\n   - Compute the action probabilities using the actor head, which results in equal probabilities for each action.\n   - Compute the state value using the critic head, which is zero if the sum of the state is non-positive, otherwise, it is 64 times the sum of the state.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_s = sum(state)\n    action_probs = [round(1/3, 4), round(1/3, 4), round(1/3, 4)]\n    state_value = 64.0 * sum_s if sum_s > 0 else 0.0\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)\n```\n\n### Explanation\n1. **Sum of State**: The input state is a list of three floats. The sum of these floats is computed to determine the subsequent values.\n2. **Action Probabilities**: The actor head computes logits which are all equal, leading to equal probabilities for each action when applying the softmax function. These probabilities are rounded to four decimal places.\n3. **State Value**: The critic head computes the state value based on the sum of the state. If the sum is positive, the state value is 64 times the sum; otherwise, it is zero. The state value is rounded to four decimal places.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, given the fixed weights and biases."}
{"task_id": 244, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a random binary tensor of a specified shape where each element is independently set to `1.0` with a given probability (sparsity) and `0.0` otherwise. The function must validate the inputs and return `-1` if any input is invalid.\n\n### Approach\n1. **Input Validation**: \n   - Ensure the shape is a non-empty tuple of positive integers.\n   - Ensure the sparsity is a real number within the range [0, 1].\n   \n2. **Generate Random Tensor**:\n   - Use `numpy.random.rand` to generate a tensor of the specified shape filled with random values between 0 and 1.\n   \n3. **Convert to Binary Tensor**:\n   - Convert each element to `1.0` if it is greater than or equal to `(1 - sparsity)`, otherwise set it to `0.0`.\n   \n4. **Return Result**:\n   - Convert the resulting binary tensor to a nested Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    # Validate the shape\n    if not isinstance(shape, tuple):\n        return -1\n    if len(shape) == 0:\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    \n    # Validate the sparsity\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if isinstance(sparsity, bool):\n        return -1\n    if not (0 <= sparsity <= 1):\n        return -1\n    \n    # Generate the random tensor\n    tensor = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (tensor >= threshold).astype(float)\n    \n    return binary_tensor.tolist()\n```\n\n### Explanation\n1. **Input Validation**:\n   - The function first checks if the provided shape is a tuple and not empty. Each dimension in the shape must be a positive integer.\n   - The function then checks if the sparsity is a valid number within the range [0, 1]. Booleans are explicitly excluded as they are not considered valid for this context.\n\n2. **Random Tensor Generation**:\n   - Using `numpy.random.rand`, a tensor of the specified shape is generated with values uniformly distributed between 0 and 1.\n\n3. **Binary Conversion**:\n   - Each element in the generated tensor is compared against a threshold `(1 - sparsity)`. Elements greater than or equal to this threshold are set to `1.0`, others to `0.0`.\n\n4. **Result Conversion**:\n   - The resulting binary tensor is converted to a nested Python list using `tolist()`, ensuring the output format is correct.\n\nThis approach ensures that the function handles all edge cases and returns the correct binary tensor or `-1` for invalid inputs."}
{"task_id": 248, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple baseline classifier that always predicts the same probability for every example. This probability is based on the empirical proportion of the positive class observed in the training labels. The function should return a list of probabilities, each rounded to four decimal places, with the same length as the input feature matrix for the test data.\n\n### Approach\n1. **Compute the Positive-Class Probability**: Calculate the probability of the positive class (labeled as 1) in the training data. This is done by dividing the number of positive labels by the total number of labels in the training set.\n2. **Create Predictions Array**: Generate an array of the same length as the test feature matrix, filled with the computed positive-class probability.\n3. **Round the Probabilities**: Round each value in the predictions array to four decimal places.\n4. **Return as List**: Convert the NumPy array to a regular Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix. Its values are ignored by this baseline\n        predictor; only its length is potentially useful for sanity checks.\n    y_train : np.ndarray\n        One-dimensional array of binary labels (0 for negative, 1 for positive).\n    X_test : np.ndarray\n        Feature matrix for which the predictions are required. The number of\n        returned probabilities must match the number of rows in this matrix.\n\n    Returns\n    -------\n    list[float]\n        A list containing the same probability repeated for every row of\n        `X_test`. The probability is rounded to four decimal places.\n    \"\"\"\n    # Compute the positive-class probability\n    p = np.sum(y_train) / len(y_train)\n    \n    # Determine the number of test samples\n    n_samples = X_test.shape[0]\n    \n    # Return empty list if there are no test samples\n    if n_samples == 0:\n        return []\n    \n    # Create an array filled with the probability p\n    predictions = np.full(n_samples, p)\n    \n    # Round each prediction to four decimal places\n    rounded_predictions = np.round(predictions, 4)\n    \n    # Convert the numpy array to a Python list\n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Compute Positive-Class Probability**: The probability `p` is calculated as the ratio of the number of positive labels (1s) in the training set to the total number of labels. This gives the empirical proportion of the positive class.\n2. **Generate Predictions Array**: Using NumPy's `np.full`, we create an array where each element is the computed probability `p`. The length of this array matches the number of rows in the test feature matrix.\n3. **Round Probabilities**: Each value in the predictions array is rounded to four decimal places using NumPy's `np.round` function.\n4. **Return as List**: The NumPy array is converted to a Python list using the `tolist()` method and returned.\n\nThis approach ensures that the function efficiently computes and returns the required probabilities while handling edge cases such as an empty test feature matrix."}
