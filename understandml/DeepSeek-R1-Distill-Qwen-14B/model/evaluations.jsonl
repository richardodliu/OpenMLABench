{"task_id": 2, "completion_id": 0, "passed": false, "result": "failed: 'link_prime'", "solution": "import numpy as np\nimport numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {'identity': {'link': lambda mu: mu, 'inv_link': lambda eta: eta, 'derivative': lambda mu: 1}, 'log': {'link': np.log, 'inv_link': np.exp, 'derivative': lambda mu: 1.0 / mu}, 'logit': {'link': lambda mu: np.log(mu / (1 - mu)), 'inv_link': lambda eta: 1.0 / (1 + np.exp(-eta)), 'derivative': lambda mu: 1.0 / (mu * (1 - mu))}}\n    return links\nassert np.allclose(glm_links()[\"identity\"][\"link\"](np.array([3.0,-1.0])), np.array([3.0,-1.0])), \"test case failed: identity link\"\nassert np.allclose(glm_links()[\"identity\"][\"inv_link\"](np.array([-0.5,2.6])), np.array([-0.5,2.6])), \"test case failed: identity inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link\"](np.array([1.0,2.0])), np.log(np.array([1.0,2.0]))), \"test case failed: log link\"\nassert np.allclose(glm_links()[\"log\"][\"inv_link\"](np.array([0.0,1.0])), np.exp(np.array([0.0,1.0]))), \"test case failed: log inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link_prime\"](np.array([2.0,4.0])), np.array([0.5,0.25])), \"test case failed: log link derivative\"\nassert np.allclose(glm_links()[\"logit\"][\"link\"](np.array([0.2,0.8])), np.array([-1.38629436,1.38629436])), \"test case failed: logit link\"\nassert np.allclose(glm_links()[\"logit\"][\"inv_link\"](np.array([-1.38629436,1.38629436])), np.array([0.2,0.8])), \"test case failed: logit inverse link\"\nassert np.allclose(glm_links()[\"logit\"][\"link_prime\"](np.array([0.2,0.8])), 1.0/(np.array([0.2,0.8])*(1.0-np.array([0.2,0.8])))), \"test case failed: logit link derivative\"\nimport numpy as _np; _rng=_np.random.default_rng(1); _mu=_rng.uniform(0.05,0.95,50); _eta=glm_links()[\"logit\"][\"link\"](_mu); assert _np.allclose(glm_links()[\"logit\"][\"inv_link\"](_eta), _mu), \"test case failed: random roundtrip\"\nmu_test = np.array([0.3,0.6]).reshape(1,2); eta_test = glm_links()[\"logit\"][\"link\"](mu_test); assert eta_test.shape == (1,2) and glm_links()[\"logit\"][\"inv_link\"](eta_test).shape == (1,2), \"test case failed: shape preservation\""}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    norms = np.sum(X ** 2, axis=1)\n    dot_product = X.dot(X.T)\n    D = norms[:, None] + norms[None, :] - 2 * dot_product\n    D = np.round(D, 4)\n    return D.tolist()\nassert l2_distance(np.array([[0, 0], [1, 0]], dtype=float)) == [[0.0, 1.0], [1.0, 0.0]], \"failed on two points, axis-aligned\"\nassert l2_distance(np.array([[1, 2]], dtype=float)) == [[0.0]], \"failed on single sample\"\nassert l2_distance(np.array([[1, 2], [3, 4], [5, 6]], dtype=float)) == [[0.0, 8.0, 32.0], [8.0, 0.0, 8.0], [32.0, 8.0, 0.0]], \"failed on three points 2-D\"\nassert l2_distance(np.array([[-1, -1], [1, 1]], dtype=float)) == [[0.0, 8.0], [8.0, 0.0]], \"failed on negative coordinates\"\nassert l2_distance(np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]], dtype=float)) == [[0.0, 3.0, 12.0], [3.0, 0.0, 3.0], [12.0, 3.0, 0.0]], \"failed on 3-D points\"\nassert l2_distance(np.array([[0.5, 0.5], [1.5, 1.5]], dtype=float)) == [[0.0, 2.0], [2.0, 0.0]], \"failed on float coordinates\"\nassert l2_distance(np.zeros((0, 5), dtype=float)) == [], \"failed on empty input\"\nrnd = np.random.RandomState(0); X_rand = rnd.randn(4, 3); D = l2_distance(X_rand); assert all(abs(D[i][i]) < 1e-8 for i in range(4)), \"diagonal not zero on random data\"\nX_same = np.array([[2, 3], [2, 3], [2, 3]], dtype=float); assert l2_distance(X_same) == [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], \"failed on identical points\""}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        A list where each element is a 2-D array-like object of shape\n        (n_samples, n_classes) containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning rate used by the booster.  This value scales every tree\n        output before it is added to the ensemble score.\n\n    Returns\n    -------\n    list[int]\n        A list of length *n_samples* with the predicted class label for every\n        sample.\n    \"\"\"\n    sum_predictions = np.sum(predictions, axis=0)\n    raw_score = sum_predictions * -learning_rate\n    exp = np.exp(raw_score)\n    sum_exp = exp.sum(axis=1, keepdims=True)\n    probabilities = exp / sum_exp\n    predicted_classes = np.argmax(probabilities, axis=1)\n    return predicted_classes.tolist()\nassert xgboost_predict([np.array([[0.2, -0.1], [1.0, -1.0]]), np.array([[-0.4, 0.3], [0.5, -0.2]])], 0.5) == [0, 1], \"test case failed: example case\"\nassert xgboost_predict([np.array([[2, 1], [1, 2], [0, 0]])], 0.1) == [1, 0, 0], \"test case failed: single tree\"\nassert xgboost_predict([np.array([[1, 5, 3]]), np.array([[1, 5, 3]])], 0.5) == [0], \"test case failed: two identical trees\"\nassert xgboost_predict([np.array([[-2, 2]])], 1.0) == [0], \"test case failed: negative vs positive\"\nassert xgboost_predict([np.array([[0, 1, 0]]), np.array([[0, 1, 0]])], 1.0) == [0], \"test case failed: tie situation\"\nassert xgboost_predict([np.array([[ 1, -1], [-1,  1], [0,  0]]), np.array([[ 1, -1], [-1,  1], [1, -1]]), np.array([[ 1, -1], [-1,  1], [2, -2]])], 0.2) == [1, 0, 1], \"test case failed: three trees, three samples\"\nassert xgboost_predict([np.zeros((2, 3)) for _ in range(5)], 0.5) == [0, 0], \"test case failed: all zeros\"\nassert xgboost_predict([np.array([[0.5, 1.5], [2.0, 0.0]])], 1.0) == [0, 1], \"test case failed: 1 tree, 2 samples\"\nassert xgboost_predict([np.array([[1, 2, 3, 4]]), np.array([[4, 3, 2, 1]])], 0.1) == [0], \"test case failed: four classes\"\nassert xgboost_predict([np.array([[1, 0, 0], [0, 1, 0]]), np.array([[0, 1, 0], [0, 0, 1]])], 0.3) == [2, 0], \"test case failed: 3 classes, 2 trees\""}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef logistic_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, n_iterations: int=4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        List with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n    m = X_train.shape[0]\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(n_iterations):\n        z = np.dot(X_train, w) + b\n        A = 1 / (1 + np.exp(-z))\n        gradient_w = X_train.T.dot(A - y_train) / m\n        gradient_b = (A - y_train).mean()\n        w = w - learning_rate * gradient_w\n        b = b - learning_rate * gradient_b\n    z_test = np.dot(X_test, w) + b\n    A_test = 1 / (1 + np.exp(-z_test))\n    preds = np.round(A_test).astype(int).tolist()\n    return preds\nassert logistic_regression_predict(np.array([[0],[10]]), np.array([0,1]), np.array([[0],[10]])) == [0,1], \"failed: simple two-point case\"\nassert logistic_regression_predict(np.array([[-10],[10]]), np.array([0,1]), np.array([[-5],[5]])) == [0,1], \"failed: symmetric case\"\nassert logistic_regression_predict(np.array([[-3],[-2],[-1],[1],[2],[3]]), np.array([0,0,0,1,1,1]), np.array([[-4],[4]])) == [0,1], \"failed: clustered 1-D case\"\ntrain_X4 = np.array([[1,1],[2,2],[3,3],[-1,-1],[-2,-2],[-3,-3]]); train_y4 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X4, train_y4, np.array([[0.5,0.5],[-2.5,-2.5]])) == [1,0], \"failed: 2-D diagonal case\"\ntrain_X5 = np.array([[1,4],[2,5],[3,6],[-1,-4],[-2,-5],[-3,-6]]); train_y5 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X5, train_y5, np.array([[4,7],[-4,-7]])) == [1,0], \"failed: 2-D magnitudes case\"\ntrain_X6 = np.array([[0,0],[0,1],[0,2],[4,0],[4,1],[4,2]]); train_y6 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X6, train_y6, np.array([[0,1.5],[4,1.5]])) == [0,1], \"failed: vertical split\"\ntrain_X7 = np.array([[-50],[-40],[-30],[30],[40],[50]]); train_y7 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X7, train_y7, np.array([[-25],[25]])) == [0,1], \"failed: wide 1-D separation\"\ntrain_X8 = np.array([[-5],[-4],[-3],[-2],[-1],[1],[2],[3],[4],[5]]); train_y8 = np.array([0,0,0,0,0,1,1,1,1,1]); assert logistic_regression_predict(train_X8, train_y8, np.array([[-6],[6]])) == [0,1], \"failed: dense clusters\"\ntrain_X9 = np.array([[-1,-3],[-2,-5],[-3,-4],[1,3],[2,5],[3,4]]); train_y9 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X9, train_y9, np.array([[-2,-4],[2,4]])) == [0,1], \"failed: mixed 2-D clusters\"\ntrain_X10 = np.array([[2,1],[3,1],[4,1],[-2,-1],[-3,-1],[-4,-1]]); train_y10 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X10, train_y10, np.array([[3,1],[-3,-1]])) == [1,0], \"failed: rotated 2-D split\""}
{"task_id": 10, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (\\\\(\tilde{X}_{t-1}\\\\)).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (\\\\(X_t\\\\)).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \\\\(\\x0barepsilon\\\\), 0 <= weight <= 1.  Scalars or one-\n        dimensional vectors are supported; vectors are used element-wise.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n    prev_arr = np.asarray(prev)\n    cur_arr = np.asarray(cur)\n    weight_arr = np.asarray(weight)\n    smoothed = weight_arr * prev_arr + (1 - weight_arr) * cur_arr\n    rounded = np.round(smoothed, 4)\n    if rounded.ndim == 0:\n        return rounded.item()\n    else:\n        return rounded.tolist()\nassert smooth(10.0, 12.0, 0.8) == 10.4, \"failed on (10,12,0.8)\"\nassert smooth(5, 7, 0.6) == 5.8, \"failed on (5,7,0.6)\"\nassert smooth([1,2,3], [4,5,6], 0.5) == [2.5, 3.5, 4.5], \"failed on vector weight scalar 0.5\"\nassert smooth([1,2,3], [4,5,6], 0.2) == [3.4, 4.4, 5.4], \"failed on scalar weight 0.2\"\nassert smooth([2,4,6], [8,10,12], [0.75,0.25,0.5]) == [3.5, 8.5, 9.0], \"failed on vector weight\"\nassert smooth(0, 1, 1/3) == 0.6667, \"failed on rounding check\"\nassert smooth(3, 9, 0.0) == 9.0, \"failed when weight=0\"\nassert smooth(3, 9, 1.0) == 3.0, \"failed when weight=1\"\nassert smooth([0,0,0], [10,20,30], 0) == [10.0, 20.0, 30.0], \"vector weight=0\"\nassert smooth([100,200], [50,  0], [0.5,0.2]) == [75.0, 40.0], \"mixed vector values\""}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimals.\n    \"\"\"\n    eps = 1e-15\n    p = 1 / (1 + np.exp(-y_pred))\n    p = np.clip(p, eps, 1 - eps)\n    loss = -(y * np.log(p) + (1 - y) * np.log(1 - p))\n    mean_loss = np.mean(loss)\n    gradient = p - y\n    hessian = p * (1 - p)\n    mean_loss_rounded = round(mean_loss, 4)\n    gradient_rounded = [round(g, 4) for g in gradient]\n    hessian_rounded = [round(h, 4) for h in hessian]\n    return (mean_loss_rounded, gradient_rounded, hessian_rounded)\nassert logistic_loss_metrics(np.array([1, 0, 1]), np.array([0.0, 0.0, 0.0])) == (0.6931, [-0.5, 0.5, -0.5], [0.25, 0.25, 0.25]), \"failed on zeros logits\"\nassert logistic_loss_metrics(np.array([0, 1]), np.array([2.0, -2.0])) == (2.1269, [0.8808, -0.8808], [0.105, 0.105]), \"failed on mixed logits 2 & -2\"\nassert logistic_loss_metrics(np.array([1]), np.array([1.0])) == (0.3133, [-0.2689], [0.1966]), \"failed on single positive logit\"\nassert logistic_loss_metrics(np.array([0]), np.array([-1.0])) == (0.3133, [0.2689], [0.1966]), \"failed on single negative logit\"\nassert logistic_loss_metrics(np.array([1, 0]), np.array([3.0, 3.0])) == (1.5486, [-0.0474, 0.9526], [0.0452, 0.0452]), \"failed on identical large positive logits\"\nassert logistic_loss_metrics(np.zeros(4), np.zeros(4)) == (0.6931, [0.5, 0.5, 0.5, 0.5], [0.25, 0.25, 0.25, 0.25]), \"failed on all-zero labels\"\nassert logistic_loss_metrics(np.ones(4), np.zeros(4)) == (0.6931, [-0.5, -0.5, -0.5, -0.5], [0.25, 0.25, 0.25, 0.25]), \"failed on all-one labels\"\nassert logistic_loss_metrics(np.array([0, 1, 0, 1]), np.array([-2.0, -2.0, 2.0, 2.0])) == (1.1269, [0.1192, -0.8808, 0.8808, -0.1192], [0.105, 0.105, 0.105, 0.105]), \"failed on alternating logits\"\nassert logistic_loss_metrics(np.array([0]), np.array([5.0])) == (5.0067, [0.9933], [0.0066]), \"failed on large positive logit with label 0\"\nassert logistic_loss_metrics(np.array([1]), np.array([-5.0])) == (5.0067, [-0.9933], [0.0066]), \"failed on large negative logit with label 1\""}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import Any, Iterable, List, Tuple\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    length = len(sequence)\n    if N > length:\n        return []\n    result = []\n    for i in range(length - N + 1):\n        gram = tuple(sequence[i:i + N])\n        result.append(gram)\n    return result\nassert ngrams([1, 2, 3, 4, 5], 2) == [(1, 2), (2, 3), (3, 4), (4, 5)], \"Test 1 failed: ngrams([1, 2, 3, 4, 5], 2)\"\nassert ngrams([1], 1) == [(1,)], \"Test 2 failed: ngrams([1], 1)\"\nassert ngrams([1], 2) == [], \"Test 3 failed: ngrams([1], 2)\"\nassert ngrams(\"hello\", 2) == [('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')], \"Test 4 failed: ngrams('hello', 2)\"\nassert ngrams((1, 2, 3, 4), 3) == [(1, 2, 3), (2, 3, 4)], \"Test 5 failed: ngrams((1,2,3,4), 3)\"\nassert ngrams(range(6), 4) == [(0, 1, 2, 3), (1, 2, 3, 4), (2, 3, 4, 5)], \"Test 6 failed: ngrams(range(6), 4)\"\nassert ngrams(['I', 'love', 'AI'], 2) == [('I', 'love'), ('love', 'AI')], \"Test 7 failed: ngrams(['I','love','AI'],2)\"\nassert ngrams([True, False, True, False], 3) == [(True, False, True), (False, True, False)], \"Test 8 failed: ngrams([True,False,True,False],3)\"\nassert ngrams([1, 1, 1, 1], 2) == [(1, 1), (1, 1), (1, 1)], \"Test 9 failed: ngrams([1,1,1,1],2)\"\nassert ngrams([7, 8, 9], 3) == [(7, 8, 9)], \"Test 10 failed: ngrams([7,8,9],3)\""}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    if not items:\n        return {}\n    root_index = None\n    for i in range(len(parents)):\n        if parents[i] == -1:\n            root_index = i\n            break\n    children_dict = {}\n    for i in range(len(parents)):\n        p = parents[i]\n        if p == -1:\n            continue\n        if p not in children_dict:\n            children_dict[p] = []\n        children_dict[p].append(i)\n\n    def build_node(index):\n        node = {'item': items[index], 'count': counts[index], 'children': []}\n        if index in children_dict:\n            for child_index in children_dict[index]:\n                node['children'].append(build_node(child_index))\n        return node\n    return build_node(root_index)\nassert build_tree([], [], []) == {}, \"test case failed: empty input lists\"\nassert build_tree(['A'], [5], [-1]) == {'item':'A','count':5,'children':[]}, \"test case failed: single node\"\nassert build_tree(['A','B','C','D'], [5,3,2,1], [-1,0,0,0]) == {'item':'A','count':5,'children':[{'item':'B','count':3,'children':[]},{'item':'C','count':2,'children':[]},{'item':'D','count':1,'children':[]}]}, \"test case failed: star tree\"\nassert build_tree(['A','B','C','D'], [7,6,5,4], [-1,0,1,2]) == {'item':'A','count':7,'children':[{'item':'B','count':6,'children':[{'item':'C','count':5,'children':[{'item':'D','count':4,'children':[]}]}]}]}, \"test case failed: linear chain\"\nassert build_tree(['R','A','B','C','D','E','F'], [9,4,4,2,2,1,1], [-1,0,0,1,1,2,2]) == {'item':'R','count':9,'children':[{'item':'A','count':4,'children':[{'item':'C','count':2,'children':[]},{'item':'D','count':2,'children':[]}]},{'item':'B','count':4,'children':[{'item':'E','count':1,'children':[]},{'item':'F','count':1,'children':[]}]}]}, \"test case failed: balanced tree\"\nassert build_tree(['root','n1','n2','n3','n4','n5','n6','n7','n8'], [9,8,7,6,5,4,3,2,1], [-1,0,0,1,1,2,2,3,6]) == {'item':'root','count':9,'children':[{'item':'n1','count':8,'children':[{'item':'n3','count':6,'children':[{'item':'n7','count':2,'children':[]}]},{'item':'n4','count':5,'children':[]}]},{'item':'n2','count':7,'children':[{'item':'n5','count':4,'children':[]},{'item':'n6','count':3,'children':[{'item':'n8','count':1,'children':[]}]}]}]}, \"test case failed: complex tree\"\nassert build_tree(['P','Q','R','S'], [1,1,1,1], [-1,0,1,2])['children'][0]['children'][0]['children'][0]['item'] == 'S', \"test case failed: verify deep child label\"\nassert len(build_tree(['A','B','C'], [1,1,1], [-1,0,0])['children']) == 2, \"test case failed: number of children\"\nassert build_tree(['A','B','C','D','E'], [6,4,3,2,1], [-1,0,1,1,2]) == {'item':'A','count':6,'children':[{'item':'B','count':4,'children':[{'item':'C','count':3,'children':[{'item':'E','count':1,'children':[]} ]},{'item':'D','count':2,'children':[]} ]} ]}, \"test case failed: provided example\""}
{"task_id": 18, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', None)\n    if dtype is None:\n        dtype = float\n    else:\n        dtype = np.dtype(dtype)\n    return np.zeros(shape, dtype=dtype)\nassert zero(5).shape == (5,), \"test case failed: zero(5) shape incorrect\"\nassert np.array_equal(zero(5), np.zeros(5)), \"test case failed: zero(5) values incorrect\"\nassert zero((2, 3)).shape == (2, 3), \"test case failed: zero((2, 3)) shape incorrect\"\nassert np.array_equal(zero((2, 3)), np.zeros((2, 3))), \"test case failed: zero((2, 3)) values incorrect\"\nassert zero((2, 3), dtype=int).dtype == np.int_, \"test case failed: dtype=int not respected\"\nassert zero((1, 1, 1)).shape == (1, 1, 1), \"test case failed: triple-dim shape\"\nassert zero((4,)).ndim == 1, \"test case failed: ndim for 1-tuple shape\"\nassert zero(()).shape == (), \"test case failed: zero-dimensional array shape\"\nassert zero(0).size == 0, \"test case failed: zero length vector size\"\nassert zero((2, 0, 3)).size == 0, \"test case failed: array with a zero axis size\""}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        if isinstance(x, (int, float)):\n            x_scalar = x\n        else:\n            x_scalar = x.item()\n        sigmoid = 1 / (1 + np.exp(-x_scalar))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        if isinstance(x, list):\n            x_array = np.array(x)\n        else:\n            x_array = x\n        sigmoid = 1 / (1 + np.exp(-x_array))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = np.round(sigmoid, 4)\n        gradient_rounded = np.round(gradient, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\nassert sigmoid_activation(0) == (0.5, 0.25), \"failed on scalar 0\"\nassert sigmoid_activation([-1, 0, 1]) == ([0.2689, 0.5, 0.7311], [0.1966, 0.25, 0.1966]), \"failed on list [-1,0,1]\"\nassert sigmoid_activation(np.array([-3, 3])) == ([0.0474, 0.9526], [0.0452, 0.0452]), \"failed on np.array([-3,3])\"\nassert sigmoid_activation([10]) == ([1.0], [0.0]), \"failed on list [10]\"\nassert sigmoid_activation(10) == (1.0, 0.0), \"failed on scalar 10\"\nassert sigmoid_activation(-1000) == (0.0, 0.0), \"failed on large negative scalar\"\nassert sigmoid_activation([0]) == ([0.5], [0.25]), \"failed on list [0]\""}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef random_tensor(shape: tuple[int, ...], standardize: bool=False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    offset = np.random.randint(-300, 300, size=shape)\n    u = np.random.uniform(0.0, 1.0, size=shape)\n    tensor = offset + u\n    if standardize:\n        means = tensor.mean(axis=0)\n        stds = tensor.std(axis=0)\n        epsilon = np.finfo(float).eps\n        tensor = (tensor - means) / (stds + epsilon)\n    return tensor\nassert random_tensor((5, 3), True).shape == (5, 3), \"test case failed: random_tensor((5, 3), True) shape check\"\nassert (random_tensor((7, 1), False) >= -300).all(), \"test case failed: value range check\"\nx = random_tensor((4, 2), True); import numpy as _np; assert _np.allclose(x.mean(axis=0), 0, atol=1e-10), \"test case failed: column means not zero\"\nx = random_tensor((4, 2), True); import numpy as _np; assert _np.allclose(x.std(axis=0), 1, atol=1e-10), \"test case failed: column stds not one\"\nx = random_tensor((3, 4), False); import numpy as _np; assert not _np.allclose(x.std(axis=0), 1, atol=1e-2), \"test case failed: standardization wrongly applied\"\nassert random_tensor((1, 5), False).shape == (1, 5), \"test case failed: random_tensor((1,5), False) shape check\"\nassert random_tensor((6, 6), True).shape == (6, 6), \"test case failed: random_tensor((6,6), True) shape check\"\nx = random_tensor((10, 2), True); import numpy as _np; assert _np.isfinite(x).all(), \"test case failed: finite values\"\nassert random_tensor((2, 3), False).dtype == float, \"test case failed: dtype check\"\nx = random_tensor((8, 4), True); import numpy as _np; assert _np.allclose(x.mean(axis=0), 0, atol=1e-10) and _np.allclose(x.std(axis=0), 1, atol=1e-10), \"test case failed: standardization (8,4)\""}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    w = np.array(w)\n    if not velocity:\n        velocity = np.zeros_like(w)\n    else:\n        velocity = np.array(velocity)\n    w_tilde = w - momentum * velocity\n    g = grad_func(w_tilde)\n    g_clipped = np.clip(g, -1, 1)\n    new_velocity = momentum * velocity + learning_rate * g_clipped\n    new_w = w - new_velocity\n    new_w_rounded = np.round(new_w, 4).tolist()\n    new_velocity_rounded = np.round(new_velocity, 4).tolist()\n    return (new_w_rounded, new_velocity_rounded)\nassert nesterov_update([1.5, -0.8], [0, 0], lambda x: 2 * x, 0.1, 0.9) == ([1.4, -0.7], [0.1, -0.1]), \"failed on basic 2-D update\"\nassert nesterov_update([1.4, -0.7], [0.1, -0.1], lambda x: 2 * x, 0.1, 0.9) == ([1.21, -0.51], [0.19, -0.19]), \"failed on consecutive 2-D step\"\nassert nesterov_update([2.0], [], lambda x: 4 * x, 0.05, 0.8) == ([1.95], [0.05]), \"failed on 1-D first step\"\nassert nesterov_update([-0.2, 0.2], [0, 0], lambda x: np.array([10, -10]), 0.2, 0.0) == ([-0.4, 0.4], [0.2, -0.2]), \"failed on gradient clipping\"\nassert nesterov_update([0.0, 0.0], [0, 0], lambda x: np.array([-0.5, 0.3]), 0.1, 0.5) == ([0.05, -0.03], [-0.05, 0.03]), \"failed on mixed-sign gradient\"\nassert nesterov_update([0.5, -0.5], [0.2, -0.1], lambda x: x, 0.1, 0.9) == ([0.288, -0.369], [0.212, -0.131]), \"failed on non-zero initial velocity\"\nassert nesterov_update([1.0, 1.0], [0, 0], lambda x: np.zeros_like(x), 0.1, 0.85) == ([1.0, 1.0], [0.0, 0.0]), \"failed on zero gradient\"\nassert nesterov_update([0.1], [0], lambda x: np.array([100]), 0.01, 0.0) == ([0.09], [0.01]), \"failed on extreme gradient clipping\"\nassert nesterov_update([2.0, -1.0, 0.5], [0, 0, 0], lambda x: 0.5 * x, 0.2, 0.4) == ([1.8, -0.9, 0.45], [0.2, -0.1, 0.05]), \"failed on 3-D first step\"\nassert nesterov_update([1.8, -0.9, 0.45], [0.2, -0.1, 0.05], lambda x: 0.5 * x, 0.2, 0.4) == ([1.548, -0.774, 0.387], [0.252, -0.126, 0.063]), \"failed on 3-D consecutive step\""}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: shapes (2,) and (1,2) not aligned: 2 (dim 0) != 1 (dim 0)", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    (n, d) = X_train.shape\n    m = X_test.shape[0]\n    X_train_reshaped = X_train[:, np.newaxis, :]\n    X_test_reshaped = X_test[np.newaxis, :, :]\n    diff = X_test_reshaped - X_train_reshaped\n    squared_norms = np.sum(diff ** 2, axis=2)\n    K = np.exp(-gamma * squared_norms)\n    sum_values = (alpha * y_train).dot(K.T)\n    g_z = sum_values + b\n    predictions = np.where(g_z >= 0, 1, -1)\n    return predictions.tolist()\nassert gaussian_svm_predict(np.array([[1,2],[2,3]]),np.array([1,-1]),np.array([0.6,0.4]),0.1,0.5,np.array([[1.5,2.5]]))==[1],\"Failed on single test sample.\"\nassert gaussian_svm_predict(np.array([[1,0],[0,1]]),np.array([1,-1]),np.array([0.9,0.9]),0.0,2.0,np.array([[0.9,0.1],[0.1,0.9]]))==[1,-1],\"Failed orthogonal samples.\"\nassert gaussian_svm_predict(np.array([[1,2],[3,4]]),np.array([1,1]),np.array([0.5,0.5]),-0.6,0.3,np.array([[2,3]]))==[-1],\"Bias impact failed.\"\nassert gaussian_svm_predict(np.array([[0,0],[0,1],[1,0],[1,1]]),np.array([1,-1,-1,1]),np.array([0.3,0.3,0.3,0.3]),0.0,1.0,np.array([[0.5,0.5],[1.5,1.5]]))==[1,1],\"Failed on XOR-like points.\"\nassert gaussian_svm_predict(np.array([[2]]),np.array([-1]),np.array([0.9]),0.0,1.0,np.array([[1],[3]]))==[-1,-1],\"Failed 1-D input.\"\nassert gaussian_svm_predict(np.array([[1,2,3],[4,5,6]]),np.array([1,-1]),np.array([0.4,0.6]),0.0,0.2,np.array([[1,2,3],[4,5,6]]))==[1,-1],\"Failed predictions identical to training points.\"\nassert gaussian_svm_predict(np.array([[1,2]]),np.array([1]),np.array([1.0]),-1.5,0.1,np.array([[10,20]]))==[-1],\"Far away point should follow bias.\""}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(0, x_array)\n    gradient = np.where(x_array >= 0, 1, 0)\n    return (activation.tolist(), gradient.tolist())\nassert relu([-2,-1,0,1,3]) == ([0,0,0,1,3],[0,0,1,1,1]), \"failed on 1-D mixed values\"\nassert relu([0,0,0]) == ([0,0,0],[1,1,1]), \"failed on all zeros\"\nassert relu([[1,-2],[3,-4]]) == ([[1,0],[3,0]],[[1,0],[1,0]]), \"failed on simple 2-D array\"\nassert relu([[-1.5,2.7,-3.2]]) == ([[0,2.7,0]],[[0,1,0]]), \"failed on float values\"\nassert relu([5]) == ([5],[1]), \"failed on single positive scalar\"\nassert relu([-5]) == ([0],[0]), \"failed on single negative scalar\"\nassert relu([[0.0,-0.0]]) == ([[0.0,0.0]],[[1,1]]), \"failed on signed zeros\"\nassert relu([[1000,-1000],[0,0.0001]]) == ([[1000,0],[0,0.0001]],[[1,0],[1,1]]), \"failed on large magnitude values\"\nassert relu([[-2,-1],[-0.5,0.5],[1,2]]) == ([[0,0],[0,0.5],[1,2]],[[0,0],[0,1],[1,1]]), \"failed on rectangular 2-D list\""}
{"task_id": 32, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    The function must accept integers, floats, or numeric strings.  Any string\n    should be converted with `float()`; existing numeric values stay as they\n    are.  The pair `(a, b)` must then be returned.\n    \"\"\"\n    if isinstance(a, str):\n        a = float(a)\n    if isinstance(b, str):\n        b = float(b)\n    return (a, b)\ndef validate_input(function):\n\n    def wrapper(a, b):\n        (a, b) = check_data(a, b)\n        return function(a, b)\n    return wrapper\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\"\"\"\n    total = a + b\n    if isinstance(total, float) and total.is_integer():\n        return int(total)\n    else:\n        return total\nassert sum_numbers(3, 4) == 7, \"failed on (3, 4)\"\nassert sum_numbers(\"3\", 4) == 7.0, \"failed on ('3', 4)\"\nassert sum_numbers(\"3.5\", \"4.5\") == 8.0, \"failed on ('3.5', '4.5')\"\nassert sum_numbers(-2, \"5\") == 3.0, \"failed on (-2, '5')\"\nassert sum_numbers(\"0\", \"0\") == 0, \"failed on ('0', '0')\"\nassert sum_numbers(\"10.75\", 1.25) == 12.0, \"failed on ('10.75', 1.25)\"\nassert sum_numbers(0.1, 0.2) == 0.30000000000000004, \"failed on (0.1, 0.2)\"\nassert sum_numbers(\"-2.5\", \"-2.5\") == -5.0, \"failed on ('-2.5', '-2.5')\"\nassert sum_numbers(\" 6 \", \"7\") == 13.0, \"failed on (' 6 ', '7')\"\nassert sum_numbers(100, \"200\") == 300, \"failed on (100, '200')\""}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "from typing import Any\nfrom typing import Any\nclass node:\n    \"\"\"A minimal tree node for decision-tree-like structures.\n\n    Attributes:\n        fea: int \u2013 index of the feature to split on. A value of -1 marks a leaf.\n        res: Any \u2013 label stored in the leaf. None for internal nodes.\n        child: dict \u2013 mapping from feature value to the corresponding sub-tree.\n    \"\"\"\n\n    def __init__(self, fea: int=-1, res: Any | None=None, child: dict | None=None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\n\n    Args:\n        root: The root node of the decision tree.\n        sample: A list of feature values representing one data point.\n\n    Returns:\n        The label stored in the reached leaf, or -1 if the sample cannot be\n        classified.\n    \"\"\"\n    current = root\n    while current.fea != -1:\n        fea = current.fea\n        if fea < 0 or fea >= len(sample):\n            return -1\n        val = sample[fea]\n        if val not in current.child:\n            return -1\n        current = current.child[val]\n    return current.res\nassert classify(node(res=\"Yes\"), []) == \"Yes\", \"test case failed: classify(node(res='Yes'), [])\"\nroot1 = node(fea=0, child={\"sunny\": node(res=\"No\"), \"rainy\": node(res=\"Yes\")})\nassert classify(root1, [\"sunny\"]) == \"No\", \"test case failed: classify(root1, ['sunny'])\"\nassert classify(root1, [\"rainy\"]) == \"Yes\", \"test case failed: classify(root1, ['rainy'])\"\nassert classify(root1, [\"cloudy\"]) == -1, \"test case failed: classify(root1, ['cloudy'])\"\nassert classify(root1, []) == -1, \"test case failed: classify(root1, [])\"\nsub = node(fea=1, child={\"Y\": node(res=\"Yes\"), \"Z\": node(res=\"No\")})\nroot2 = node(fea=0, child={\"A\": node(res=\"X\"), \"B\": sub})\nassert classify(root2, [\"B\", \"Y\"]) == \"Yes\", \"test case failed: classify(root2, ['B','Y'])\"\nassert classify(root2, [\"B\", \"Z\"]) == \"No\", \"test case failed: classify(root2, ['B','Z'])\"\nassert classify(root2, [\"B\", \"Unknown\"]) == -1, \"test case failed: classify(root2, ['B','Unknown'])\"\nassert classify(root2, [\"A\"]) == \"X\", \"test case failed: classify(root2, ['A'])\"\nlevel3 = node(fea=2, child={\"D\": node(res=\"OK\")})\nroot3 = node(fea=0, child={\"R\": node(fea=1, child={\"C\": level3})})\nassert classify(root3, [\"R\", \"C\", \"D\"]) == \"OK\", \"test case failed: classify(root3, ['R','C','D'])\""}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "from typing import List, Optional\nfrom typing import List, Optional\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node:   rule = (feature_index, threshold)\n    Leaf node:       rule = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"Predicts outcomes for a batch of samples using a binary decision tree.\"\"\"\n    if root is None:\n        return []\n    predictions = []\n    for sample in X:\n        current = root\n        while current.left is not None:\n            feature = current.feature\n            threshold = current.threshold\n            if sample[feature] <= threshold:\n                current = current.left\n            else:\n                current = current.right\n        predictions.append(current.threshold)\n    return predictions\nassert predict_tree(None, [[1, 2], [3, 4]]) == [], \"failed on empty tree\"\nsingle_leaf = Node(None, None, (None, 42))\nassert predict_tree(single_leaf, [[0], [100]]) == [42, 42], \"failed on single-leaf tree\"\nleaf0 = Node(None, None, (None, 0))\nleaf1 = Node(None, None, (None, 1))\nroot = Node(leaf0, leaf1, (0, 2.5))\nassert predict_tree(root, [[1.4, 0], [4.0, 0]]) == [0, 1], \"failed on depth-1 tree\"\nleaf_left = Node(None, None, (None, 0))\nleaf_mid  = Node(None, None, (None, 1))\nleaf_right= Node(None, None, (None, 2))\nright_child = Node(leaf_mid, leaf_right, (1, 1.7))\nroot2 = Node(leaf_left, right_child, (0, 5))\nassert predict_tree(root2, [[2, 0], [7, 1.0], [7, 4]]) == [0, 1, 2], \"failed on depth-2 tree, assorted samples\"\nassert predict_tree(root2, [[1, 100], [3, -5]]) == [0, 0], \"failed when all samples end in the same leaf\"\nl1 = Node(None, None, (None, -1))\nl2 = Node(None, None, (None, -2))\nl3 = Node(None, None, (None, -3))\nint_node = Node(l2, l3, (2, 0.0))\nroot3 = Node(l1, int_node, (0, 10.0))\nassert predict_tree(root3, [[5, 0, -1], [20, 0, -2], [20, 0, 1]]) == [-1, -2, -3], \"failed on multi-feature tree\"\nleaf_neg = Node(None, None, (None, 9))\nleaf_pos = Node(None, None, (None, 8))\nroot4 = Node(leaf_neg, leaf_pos, (0, -3.0))\nassert predict_tree(root4, [[-10], [0]]) == [9, 8], \"failed on negative threshold splitting\"\nleaf_a = Node(None, None, (None, 0.1))\nleaf_b = Node(None, None, (None, 0.9))\nroot5 = Node(leaf_a, leaf_b, (0, 0.123))\nassert predict_tree(root5, [[0.0], [0.5]]) == [0.1, 0.9], \"failed on fractional values\"\nbig_batch = [[i] for i in range(100)]\nexpected  = [0 if i <= 50 else 1 for i in range(100)]\nbig_root  = Node(Node(None, None, (None, 0)), Node(None, None, (None, 1)), (0, 50))\nassert predict_tree(big_root, big_batch) == expected, \"failed on large batch\"\nleaf_c = Node(None, None, (None, 'left'))\nleaf_d = Node(None, None, (None, 'right'))\nroot6  = Node(leaf_c, leaf_d, (2, 3))\nassert predict_tree(root6, [[0, 0, 2], [0, 0, 5]]) == ['left', 'right'], \"failed when splitting on non-first feature\""}
{"task_id": 38, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\n\n    Parameters\n    ----------\n    context : np.ndarray\n        Matrix of shape (D, n_arms) containing the D-dimensional feature\n        vectors of every arm for the current round.\n    A : list\n        List where ``A[a]`` is the (D\u00d7D) Gram matrix of arm *a*.\n    b : list\n        List where ``b[a]`` is the length-D accumulated reward-context vector\n        of arm *a*.\n    alpha : float\n        Exploration coefficient (> 0).\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest LinUCB score.\n    \"\"\"\n    n_arms = context.shape[1]\n    p_values = []\n    for a in range(n_arms):\n        c_a = context[:, a]\n        inv_A_a = np.linalg.inv(A[a])\n        theta_hat_a = inv_A_a.dot(b[a])\n        term1 = np.dot(theta_hat_a, c_a)\n        temp = inv_A_a.dot(c_a)\n        term2 = alpha * np.sqrt(np.dot(c_a, temp))\n        p_a = term1 + term2\n        p_values.append(p_a)\n    max_index = np.argmax(p_values)\n    return max_index\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [np.eye(2), 2*np.eye(2)], [np.zeros(2), np.zeros(2)], 1.0) == 0, \"failed on orthogonal identity contexts\"\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [np.eye(2), 5*np.eye(2)], [np.zeros(2), np.zeros(2)], 3.0) == 0, \"failed high-alpha exploration preference\"\nassert linucb_select_arm(np.array([[1,0,0],[0,1,0],[0,0,1]]), [np.eye(3), np.eye(3), 2*np.eye(3)], [np.zeros(3), np.array([1,0,0]), np.zeros(3)], 1.0) == 0, \"failed tie-breaking smallest index\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.5]], 0.2) == 1, \"failed single-feature small-alpha exploitation\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.5]], 5.0) == 1, \"failed single-feature large-alpha\"\nassert linucb_select_arm(np.array([[1,0],[1,0]]), [np.eye(2), np.eye(2)], [np.array([2,2]), np.array([1,1])], 0.1) == 0, \"failed higher theta\"\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [10*np.eye(2), np.eye(2)], [np.zeros(2), np.zeros(2)], 5.0) == 1, \"failed uncertainty driven choice\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.0]], 0.5) == 0, \"failed identical arms pick smallest index\""}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: cannot set WRITEABLE flag to True of this array", "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    assert x.ndim == 1, 'x must be 1D'\n    assert stride >= 1, 'stride must be positive'\n    assert frame_width >= 1, 'frame_width must be positive'\n    n = x.size\n    assert n >= frame_width, 'x must be at least frame_width long'\n    n_frames = (n - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    strides = (stride * x.itemsize, x.itemsize)\n    result = as_strided(x, shape=shape, strides=strides)\n    if writeable:\n        result.flags.writeable = x.flags.writeable\n    else:\n        result.flags.writeable = False\n    return result\n# 1\nx = np.arange(6)\nassert np.array_equal(to_frames(x, 5, 1), np.array([[0,1,2,3,4],[1,2,3,4,5]])), 'test case failed: to_frames(np.arange(6),5,1)'\n# 2\nassert np.array_equal(to_frames(np.arange(10),3,2), np.array([[0,1,2],[2,3,4],[4,5,6],[6,7,8]])), 'test case failed: to_frames(np.arange(10),3,2)'\n# 3\nassert np.array_equal(to_frames(np.arange(10),4,4), np.array([[0,1,2,3],[4,5,6,7]])), 'test case failed: to_frames(np.arange(10),4,4)'\n# 4\nassert to_frames(np.arange(8),4,2).flags.writeable is False, 'test case failed: writeable flag when default False'\n# 5\nx = np.arange(8)\nframes = to_frames(x,4,2,writeable=True)\nframes[0,0]=123\nassert x[0]==123, 'test case failed: writeable True reflection'\n# 6\nrand = np.random.randn(17)\nframes = to_frames(rand,5,3)\nassert frames.shape == ((17-5)//3+1,5), 'test case failed: random shape'\n# 7\nassert np.array_equal(to_frames(np.arange(10),10,1), np.arange(10).reshape(1,-1)), 'test case failed: single frame'\n# 8\nassert np.array_equal(to_frames(np.arange(9),4,3), np.array([[0,1,2,3],[3,4,5,6]])), 'test case failed: tail trimming'\n# 9\nx = np.arange(4)\nassert np.array_equal(to_frames(x,1,1), x.reshape(-1,1)), 'test case failed: frame width 1'\n# 10\nx = np.arange(25)\nframes = to_frames(x,5,6)\nassert frames.shape == ((25-5)//6+1,5), 'test case failed: stride > width'"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef knn_predict(X_data: list[list[float]], y_data: list, X: list[list[float]], k: int=5) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    X_data_np = np.array(X_data)\n    predictions = []\n    for x in X:\n        x_np = np.array(x)\n        distances = np.sqrt(np.sum((X_data_np - x_np) ** 2, axis=1))\n        sorted_indices = np.argsort(distances)\n        top_k_indices = sorted_indices[:k]\n        y_top_k = [y_data[i] for i in top_k_indices]\n        counter = Counter(y_top_k)\n        max_count = max(counter.values(), default=0)\n        candidates = [label for (label, count) in counter.items() if count == max_count]\n        predicted_label = min(candidates)\n        predictions.append(predicted_label)\n    return predictions\nassert knn_predict([[1,2],[2,3],[3,3],[6,5],[7,7]], ['A','A','A','B','B'], [[1.5,2.5],[6,6]], 3) == ['A','B'], \"failed: basic 2-D example\"\nassert knn_predict([[0],[1],[2],[3]], [0,0,1,1], [[1.1],[2.9]], 1) == [0,1], \"failed: 1-D k=1\"\nassert knn_predict([[0],[1],[2],[3]], [0,0,1,1], [[1.1],[2.9]], 3) == [0,1], \"failed: 1-D k=3\"\nassert knn_predict([[0],[1],[5],[6]], ['A','A','B','B'], [[3]], 2) == ['A'], \"failed: tie case, lexicographic rule\"\nassert knn_predict([[2,2],[2,-2],[-2,2],[-2,-2]], ['X','Y','Y','X'], [[0,0]], 4) == ['X'], \"failed: even tie chose X\"\nassert knn_predict([[1,0],[2,0],[3,0]], [1,2,3], [[2,0]], 2) == [1], \"failed: numeric labels tie resolved to 1\"\nassert knn_predict([[1,1],[1,2],[1,3],[10,10]], ['a','a','a','b'], [[9,9]], 3) == ['a'], \"failed: distant outlier\"\ntrain_X = [[i] for i in range(20)]\ntrain_y = ['even' if i%2==0 else 'odd' for i in range(20)]\nassert knn_predict(train_X, train_y, [[0.1],[19.9]], 5) == ['even','odd'], \"failed: large set\""}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    for (a, p) in zip(actual, predicted):\n        if a < 0 or p < 0:\n            return -1\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    log_actual = np.log(actual_np + 1)\n    log_predicted = np.log(predicted_np + 1)\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    mean_squared = np.mean(squared_diff)\n    rmsle_val = np.sqrt(mean_squared)\n    return round(rmsle_val, 4)\nassert rmsle([3, 5, 2.5, 7], [2.5, 5, 4, 8]) == 0.1993, \"failed: example case\"\nassert rmsle([1, 2, 3], [1, 2, 3]) == 0.0, \"failed: perfect prediction\"\nassert rmsle([100], [150]) == 0.4022, \"failed: single element\"\nassert rmsle([1, 2, 3], [1, 2]) == -1, \"failed: length mismatch\"\nassert rmsle([-1, 2], [0, 2]) == -1, \"failed: negative value in actual\"\nassert rmsle([1000, 2000], [1000, 2000]) == 0.0, \"failed: large identical numbers\"\nassert rmsle([0, 0], [0, 0]) == 0.0, \"failed: all zeros prediction\"\nassert rmsle([0], [1]) == 0.6931, \"failed: zero actual, positive prediction\"\nassert rmsle([1], [0]) == 0.6931, \"failed: positive actual, zero prediction\"\nassert rmsle([5, 10, 15], [6, 12, 18]) == 0.1645, \"failed: moderate deviation\""}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef tanh_activation(x, derivative: bool=False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh. Otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to the nearest 4 decimals.\n    \"\"\"\n    x = np.array(x)\n    exponent = np.exp(-2 * x)\n    denominator = 1 + exponent\n    tanh_x = 2 / denominator - 1\n    if derivative:\n        result = 1 - tanh_x ** 2\n    else:\n        result = tanh_x\n    result = np.round(result, 4)\n    return result.tolist()\nassert tanh_activation([0, 1, -1], False) == [0.0, 0.7616, -0.7616], \"failed on basic forward pass\"\nassert tanh_activation([2], False) == [0.9640], \"failed on single positive value\"\nassert tanh_activation([2], True) == [0.0707], \"failed on gradient single\"\nassert tanh_activation([-2], False) == [-0.9640], \"failed on single negative value\"\nassert tanh_activation([-2], True) == [0.0707], \"failed on gradient negative\"\nassert tanh_activation([3, -3], False) == [0.9951, -0.9951], \"failed on larger magnitudes\"\nassert tanh_activation([3, -3], True) == [0.0099, 0.0099], \"failed on larger gradient\"\nassert tanh_activation([0.5, -0.5], False) == [0.4621, -0.4621], \"failed on half values\"\nassert tanh_activation([0.5, -0.5], True) == [0.7864, 0.7864], \"failed on half gradient\""}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        a_k = 0\n        for n in range(N - k):\n            a_k += x[n] * x[n + k]\n        result.append(a_k)\n    return result\nassert autocorrelate_1d([1, 2, 3]) == [14, 8, 3], \"failed: autocorrelate_1d([1, 2, 3])\"\nassert autocorrelate_1d([1]) == [1], \"failed: autocorrelate_1d([1])\"\nassert autocorrelate_1d([1, 0, 0]) == [1, 0, 0], \"failed: autocorrelate_1d([1, 0, 0])\"\nassert autocorrelate_1d([1, -1, 1, -1]) == [4, -3, 2, -1], \"failed: autocorrelate_1d([1, -1, 1, -1])\"\nassert autocorrelate_1d([0.5, 0.5]) == [0.5, 0.25], \"failed: autocorrelate_1d([0.5, 0.5])\"\nassert autocorrelate_1d([1, 2, 3, 4]) == [30, 20, 11, 4], \"failed: autocorrelate_1d([1, 2, 3, 4])\"\nassert autocorrelate_1d([-1, 2, -3]) == [14, -8, 3], \"failed: autocorrelate_1d([-1, 2, -3])\"\nassert autocorrelate_1d([]) == [], \"failed: autocorrelate_1d([])\"\nassert autocorrelate_1d([2, 2]) == [8, 4], \"failed: autocorrelate_1d([2, 2])\"\nassert autocorrelate_1d([1, 3, 5, 7, 9]) == [165, 116, 71, 34, 9], \"failed: autocorrelate_1d([1, 3, 5, 7, 9])\""}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, list):\n        if not all((isinstance(row, list) for row in X)):\n            return False\n        if not X:\n            return False\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        X_np = np.array(X)\n    else:\n        if X.ndim != 2:\n            return False\n        X_np = X\n    if not ((X_np >= 0).all() and (X_np <= 1).all()):\n        return False\n    row_sums = np.sum(X_np, axis=1)\n    if not np.allclose(row_sums, 1, atol=1e-08):\n        return False\n    return True\nassert is_stochastic([[1.0]]) is True, \"Failed on 1\u00d71 identity\"\nassert is_stochastic([[0.5, 0.5], [0.1, 0.9]]) is True, \"Failed on simple 2\u00d72 matrix\"\nassert is_stochastic([[0.2, 0.5], [0.3, 0.3]]) is False, \"Rows do not sum to 1\"\nassert is_stochastic([[0.2, -0.2], [0.4, 0.6]]) is False, \"Negative entry allowed\"\nassert is_stochastic([[1.2, -0.2], [0.4, 0.6]]) is False, \"Entry greater than 1 allowed\"\nassert is_stochastic([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) is True, \"Failed on 3\u00d73 identity\"\nassert is_stochastic([[0.3333, 0.3333, 0.3334]]) is True, \"Tolerance handling failed\"\nassert is_stochastic([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]]) is True, \"Failed on 2\u00d73 valid matrix\"\nassert is_stochastic([[0.2, 0.3, 0.6], [0.1, 0.1, 0.8]]) is False, \"First row sums to 1.1\"\nassert is_stochastic(np.array([[0.7, 0.2, 0.1]])) is True, \"Failed on NumPy input\""}
{"task_id": 49, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List, Union\nNumber = Union[int, float]\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    The function should scan the input list once, maintaining a custom\n    max-heap (size at most k) implemented with a Python list.  At the end,\n    return the k smallest elements in ascending order.  If *k* is invalid\n    (\u22640 or larger than len(nums)) return -1.\n    \n    The use of the built-in ``heapq`` module is **not** allowed.\n    \"\"\"\n    if k <= 0 or k > len(nums):\n        return -1\n\n    def _move_up(heap, index):\n        while index > 0:\n            parent = (index - 1) // 2\n            if heap[index] > heap[parent]:\n                (heap[index], heap[parent]) = (heap[parent], heap[index])\n                index = parent\n            else:\n                break\n\n    def _move_down(heap, index):\n        while True:\n            left = 2 * index + 1\n            right = 2 * index + 2\n            max_child = None\n            if right < len(heap):\n                if heap[left] > heap[right]:\n                    max_child = left\n                else:\n                    max_child = right\n            elif left < len(heap):\n                max_child = left\n            else:\n                break\n            if heap[index] < heap[max_child]:\n                (heap[index], heap[max_child]) = (heap[max_child], heap[index])\n                index = max_child\n            else:\n                break\n    heap = []\n    for num in nums:\n        if len(heap) < k:\n            heap.append(num)\n            _move_up(heap, len(heap) - 1)\n        elif num < heap[0]:\n            heap[0] = num\n            _move_down(heap, 0)\n    return sorted(heap)\nassert k_smallest([7,10,4,3,20,15],3) == [3,4,7], \"failed for [7,10,4,3,20,15], k=3\"\nassert k_smallest([1,2,3,4,5],5) == [1,2,3,4,5], \"failed for already sorted list\"\nassert k_smallest([5,4,3,2,1],2) == [1,2], \"failed for reverse order list, k=2\"\nassert k_smallest([3,1,4,1,5,9,2],4) == [1,1,2,3], \"failed with duplicates, k=4\"\nassert k_smallest([8.5,3.2,7.7,4.4,2.2],1) == [2.2], \"failed for k=1 with floats\"\nassert k_smallest([0],1) == [0], \"failed for single element list\"\nassert k_smallest([9,8,7,6,5],3) == [5,6,7], \"failed for [9,8,7,6,5], k=3\"\nassert k_smallest([2,2,2,2],3) == [2,2,2], \"failed for all identical values, k=3\"\nassert k_smallest([10,9,8,7],4) == [7,8,9,10], \"failed when k equals length\"\nassert k_smallest([1,2,3],0) == -1, \"failed for invalid k (0)\""}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float=1e-06, max_iter: int=1000) -> list[list[float]]:\n    \"\"\"Implements Newton's method for 2-D linear regression to minimize the mean squared error.\"\"\"\n    w = np.array([[1.0], [1.0]])\n    n = X.shape[0]\n    for _ in range(max_iter):\n        Xw = X.dot(w)\n        residual = Xw - y\n        grad = 2.0 / n * X.T.dot(residual)\n        if np.linalg.norm(grad) < epsilon:\n            break\n        H = 2.0 / n * X.T.dot(X)\n        H_inv = np.linalg.pinv(H)\n        w = w - H_inv.dot(grad)\n    rounded_w = np.round(w, 4)\n    return rounded_w.tolist()\nassert newton(np.array([[1, 2], [3, 4], [5, 6]]), np.array([[3], [7], [11]])) == [[1.0], [1.0]], \"test case failed: newton([[1,2],[3,4],[5,6]],[3,7,11])\"\nassert newton(np.array([[1, 0], [0, 1], [1, 1]]), np.array([[2], [3], [5]])) == [[2.0], [3.0]], \"test case failed: newton([[1,0],[0,1],[1,1]],[2,3,5])\"\nassert newton(np.array([[2, 4], [1, 3], [0, 1], [1, 2]]), np.array([[-3], [-2.5], [-1], [-1.5]])) == [[0.5], [-1.0]], \"test case failed: newton([[2,4],[1,3],[0,1],[1,2]],y)\"\nassert newton(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([[3], [5], [7], [9]])) == [[1.0], [1.0]], \"test case failed: newton(sequential data)\"\nassert newton(np.array([[1, 0], [0, 1], [2, 2]]), np.array([[2], [1], [6]])) == [[2.0], [1.0]], \"test case failed: newton([[1,0],[0,1],[2,2]],y)\"\nassert newton(np.array([[1, 3], [2, 1], [3, 2]]), np.array([[1], [7], [10]])) == [[4.0], [-1.0]], \"test case failed: newton([[1,3],[2,1],[3,2]],y)\"\nassert newton(np.array([[1, 2], [2, 1], [3, 3], [4, 1]]), np.array([[-1], [1], [0], [3]])) == [[1.0], [-1.0]], \"test case failed: newton(mixed signs)\"\nassert newton(np.array([[1, 4], [2, 2], [3, 1]]), np.array([[-0.5], [2], [4]])) == [[1.5], [-0.5]], \"test case failed: newton([[1,4],[2,2],[3,1]],y)\"\nassert newton(np.array([[1, 0], [0, 2], [2, 1], [3, 4], [4, 2]]), np.array([[0.75], [2.5], [2.75], [7.25], [5.5]])) == [[0.75], [1.25]], \"test case failed: newton(varied data)\"\n\n"}
{"task_id": 60, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    if criterion not in ['entropy', 'gini', 'mse']:\n        raise ValueError('Invalid criterion')\n    if criterion == 'entropy':\n        (values, counts) = np.unique(y, return_counts=True)\n        probs = counts / len(y)\n        entropy = 0.0\n        for p in probs:\n            entropy += -p * np.log2(p)\n        return round(entropy, 4)\n    elif criterion == 'gini':\n        (values, counts) = np.unique(y, return_counts=True)\n        probs = counts / len(y)\n        gini = sum((p * (1 - p) for p in probs))\n        return round(gini, 4)\n    elif criterion == 'mse':\n        mean = np.mean(y)\n        squared_errors = (y - mean) ** 2\n        mse = np.mean(squared_errors)\n        return round(mse, 4)\nassert compute_impurity(np.array([0, 1]), \"gini\") == 0.5, \"failed on gini([0,1])\"\nassert compute_impurity(np.array([0, 0, 1, 1]), \"entropy\") == 1.0, \"failed on entropy([0,0,1,1])\"\nassert compute_impurity(np.array([0, 0, 0, 0]), \"gini\") == 0.0, \"failed on gini(all same)\"\nassert compute_impurity(np.array([0, 0, 0, 0]), \"entropy\") == 0.0, \"failed on entropy(all same)\"\nassert compute_impurity(np.array([1.0, 2.0, 3.0, 4.0]), \"mse\") == 1.25, \"failed on mse([1,2,3,4])\"\nassert compute_impurity(np.array([3.0, 3.0, 4.0, 5.0]), \"mse\") == 0.6875, \"failed on mse([3,3,4,5])\"\nassert compute_impurity(np.array([0, 1, 2]), \"gini\") == 0.6667, \"failed on gini([0,1,2])\"\nassert compute_impurity(np.array([0, 1, 2]), \"entropy\") == 1.585, \"failed on entropy([0,1,2])\"\nassert compute_impurity(np.array([10, 10]), \"mse\") == 0.0, \"failed on mse([10,10])\"\nassert compute_impurity(np.array([0, 1, 1, 1, 1]), \"gini\") == 0.32, \"failed on gini(1 dominant)\""}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    W = np.asarray(W)\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    elif y.ndim == 2:\n        if y.shape[1] != 1:\n            return -1\n    else:\n        return -1\n    if W.ndim == 1:\n        W = W.reshape(-1, 1)\n    elif W.ndim == 2:\n        if W.shape[1] != 1:\n            return -1\n    else:\n        return -1\n    if X.ndim != 2:\n        return -1\n    if y.shape[0] != X.shape[0]:\n        return -1\n    if W.shape[0] != X.shape[1]:\n        return -1\n    n = X.shape[0]\n    XW = np.dot(X, W)\n    error = XW - y\n    gradient = X.T.dot(error) / n\n    gradient_rounded = np.round(gradient, 4)\n    result = gradient_rounded.tolist()\n    return result\nassert grad([[1, 2], [3, 4]], [[5], [6]], [[0], [0]]) == [[-11.5], [-17.0]], \"test case 1 failed\"\nassert grad([[1, 0], [0, 1], [1, 1]], [[1], [2], [3]], [[0], [0]]) == [[-1.3333], [-1.6667]], \"test case 2 failed\"\nassert grad([[1, 2], [3, 4]], [[1], [2], [3]], [[1], [1]]) == -1, \"test case 3 failed\"\nassert grad([[1, 2], [3, 4]], [[1], [2]], [[1], [1], [1]]) == -1, \"test case 4 failed\"\nassert grad([[2, 1], [1, 3]], [[1], [2]], [[0.5], [-1]]) == [[-3.25], [-7.25]], \"test case 5 failed\"\nassert grad([[1, 2, 3], [4, 5, 6]], [[1], [0]], [[1], [0], [-1]]) == [[-5.5], [-8.0], [-10.5]], \"test case 6 failed\"\nassert grad([[2, 4, 6]], [[5]], [[1], [1], [1]]) == [[14.0], [28.0], [42.0]], \"test case 7 failed\"\nassert grad([[1, 2], [3, 4], [5, 6]], [1, 2, 3], [0, 0]) == [[-7.3333], [-9.3333]], \"test case 8 failed\"\nassert grad([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], [[0],[0],[0],[0]], [[4],[8],[12],[16]]) == [[1.0], [2.0], [3.0], [4.0]], \"test case 9 failed\"\nassert grad([[1], [1], [1]], [[0], [1], [2]], [[1]]) == [[0.0]], \"test case 10 failed\""}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: name 'A1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n    \n    Args:\n        A: Transition probability matrix of shape (N, N)\n        B: Emission probability matrix of shape (N, M)\n        obs: List of observation indices (length T)\n        t: Integer time index (0 \u2264 t < T)\n        \n    Returns:\n        A list of N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)\n    T = len(obs)\n    beta = [[0.0 for _ in range(N)] for _ in range(T)]\n    for i in range(N):\n        beta[T - 1][i] = 1.0\n    for current_t in range(T - 2, -1, -1):\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                a_ij = A[i][j]\n                b_j = B[j][obs[current_t + 1]]\n                beta_j = beta[current_t + 1][j]\n                total += a_ij * b_j * beta_j\n            beta[current_t][i] = total\n    result = [round(beta[t][i], 4) for i in range(N)]\n    return result\nassert backward_beta(A1, B1, obs1, 0) == [0.2032, 0.2164], \"failed: example t=0\"\nassert backward_beta(A1, B1, obs1, 1) == [0.38, 0.26], \"failed: example t=1\"\nassert backward_beta(A1, B1, obs1, 2) == [1.0, 1.0], \"failed: example t=2 (last index)\"\nassert backward_beta(A2, B2, obs2, 3) == [1.0, 1.0, 1.0], \"failed: 3-state t=3\"\nassert backward_beta(A2, B2, obs2, 2) == [0.49, 0.5, 0.47], \"failed: 3-state t=2\"\nassert backward_beta(A2, B2, obs2, 1) == [0.2488, 0.2434, 0.2552], \"failed: 3-state t=1\"\nassert backward_beta(A2, B2, obs2, 0) == [0.1267, 0.1245, 0.133], \"failed: 3-state t=0\"\nassert backward_beta(A3, B3, obs3, 0) == [1.0, 1.0], \"failed: single observation\"\nassert backward_beta(A4, B4, obs4, 0) == [0.45, 0.54], \"failed: custom 2-state t=0\"\nassert backward_beta(A4, B4, obs4, 1) == [1.0, 1.0], \"failed: custom 2-state t=1\""}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dft(frame: np.ndarray, positive_only: bool=True) -> list:\n    \"\"\"Compute the Discrete Fourier Transform (DFT) of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array containing the signal samples.\n        positive_only: Whether to return only the non-negative frequency\n            terms (first *N//2+1* coefficients).  Defaults to ``True``.\n\n    Returns:\n        list[complex]: DFT coefficients rounded to 4 decimals.  Length is\n        *N//2+1* when *positive_only* is ``True`` and *N* otherwise.\n    \"\"\"\n    N = len(frame)\n    if N == 0:\n        return []\n    c = np.zeros(N, dtype=np.complex128)\n    for k in range(N):\n        for n in range(N):\n            exponent = -2j * np.pi * k * n / N\n            term = frame[n] * np.exp(exponent)\n            c[k] += term\n    if positive_only:\n        c = c[:N // 2 + 1]\n    result = []\n    for coeff in c:\n        real_part = round(coeff.real, 4)\n        imag_part = round(coeff.imag, 4)\n        result.append(complex(real_part, imag_part))\n    return result\nassert dft(np.array([1, 0, -1, 0]), True) == [0j, (2+0j), 0j], \"failed on basic 4-point signal (positive_only)\"\nassert dft(np.array([1, 0, -1, 0]), False) == [0j, (2+0j), 0j, (2+0j)], \"failed on basic 4-point signal (full)\"\nassert dft(np.array([0, 1, 0, -1])) == (np.round(np.fft.fft(np.array([0,1,0,-1]))[:3].real,4)+1j*np.round(np.fft.fft(np.array([0,1,0,-1]))[:3].imag,4)).tolist(), \"failed on shifted square wave (positive_only)\"\nassert dft(np.arange(8), False) == (np.round(np.fft.fft(np.arange(8)).real,4)+1j*np.round(np.fft.fft(np.arange(8)).imag,4)).tolist(), \"failed on ramp signal length 8 (full)\"\nassert dft(np.random.RandomState(0).rand(5)) == (np.round(np.fft.fft(np.random.RandomState(0).rand(5))[:3].real,4)+1j*np.round(np.fft.fft(np.random.RandomState(0).rand(5))[:3].imag,4)).tolist(), \"failed on random len-5 frame (positive_only)\"\nassert dft(np.array([7])) == [(7+0j)], \"failed on length-1 signal\"\nassert dft(np.array([1,2]), False) == (np.round(np.fft.fft(np.array([1,2])).real,4)+1j*np.round(np.fft.fft(np.array([1,2])).imag,4)).tolist(), \"failed on length-2 signal\"\nassert dft(np.array([1+1j,2-2j,-3+3j])) == (np.round(np.fft.fft(np.array([1+1j,2-2j,-3+3j]))[:2].real,4)+1j*np.round(np.fft.fft(np.array([1+1j,2-2j,-3+3j]))[:2].imag,4)).tolist(), \"failed on complex input (positive_only)\"\nassert dft(np.zeros(6)) == [0j]*4, \"failed on all-zeros input\"\nassert dft(np.ones(6), False) == (np.round(np.fft.fft(np.ones(6)).real,4)+1j*np.round(np.fft.fft(np.ones(6)).imag,4)).tolist(), \"failed on all-ones input\""}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    N = len(A)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n    beta = [1.0 for _ in range(N)]\n    for t in range(T - 2, -1, -1):\n        new_beta = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                new_beta[i] += A[i][j] * B[j][obs[t + 1]] * beta[j]\n        beta = new_beta\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    return round(prob, 6)\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [0,1,2]) == 0.033612, \"test case failed: example sequence\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [2,1]) == 0.1246, \"test case failed: sequence [2,1]\"\nassert backward_prob([[1.0]], [[0.2,0.8]], [1.0], [0,1,1,0]) == 0.0256, \"test case failed: single-state model 1\"\nassert backward_prob([[1.0]], [[0.5,0.5]], [1.0], [1,1,1]) == 0.125, \"test case failed: single-state model 2\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [1]) == 0.36, \"test case failed: length-1 sequence\"\nassert backward_prob([[0.5,0.5,0.0],[0.2,0.3,0.5],[0.0,0.0,1.0]], [[0.1,0.9],[0.7,0.3],[0.4,0.6]], [1.0,0.0,0.0], [0,1]) == 0.06, \"test case failed: 3-state model\"\nassert backward_prob([[1,0],[0,1]], [[0.6,0.4],[0.2,0.8]], [0.5,0.5], [0,1]) == 0.2, \"test case failed: deterministic transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [0,0]) == 0.25, \"test case failed: symmetric transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [1]) == 0.5, \"test case failed: length-1 identity emissions\"\nassert backward_prob([[1.0]], [[0.3,0.7]], [1.0], [1,1,0]) == 0.147, \"test case failed: single-state model 3\""}
{"task_id": 68, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float=0.01, momentum: float=0.0, prev_update: np.ndarray | None=None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n    \"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    new_w = w - learning_rate * new_update\n    new_w_rounded = np.round(new_w, 4)\n    new_update_rounded = np.round(new_update, 4)\n    return (new_w_rounded.tolist(), new_update_rounded.tolist())\nassert sgd_momentum_update(np.array([1.0, 2.0]), np.array([0.1, -0.2]), 0.1, 0.9, None) == ([0.999, 2.002], [0.01, -0.02]), \"failed on basic initial step\"\nassert sgd_momentum_update(np.array([0.5]), np.array([0.2]), 0.05, 0.8, np.array([0.03])) == ([0.4968], [0.064]), \"failed when previous update is given\"\nassert sgd_momentum_update(np.array([2.0, -3.0]), np.array([-0.5, 0.5]), 0.01, 0.0, None) == ([2.005, -3.005], [-0.5, 0.5]), \"failed with zero momentum\"\nassert sgd_momentum_update(np.array([1.0]), np.array([0.1]), 0.1, 1.0, np.array([0.2])) == ([0.98], [0.2]), \"failed with momentum equal to 1\"\nassert sgd_momentum_update(np.array([10.0]), np.array([1.0]), 0.001, 0.5, None) == ([9.9995], [0.5]), \"failed scalar update with 0.5 momentum\"\nassert sgd_momentum_update(np.array([1.0, 2.0, 3.0]), np.array([0.0, 0.0, 1.0]), 0.1, 0.5, np.zeros(3)) == ([1.0, 2.0, 2.95], [0.0, 0.0, 0.5]), \"failed on higher-dim vector\"\nassert sgd_momentum_update(np.zeros(2), np.ones(2), 0.5, 0.8, None) == ([-0.1, -0.1], [0.2, 0.2]), \"failed on zero parameters\"\nassert sgd_momentum_update(np.array([5.0]), np.array([-2.0]), 0.01, 0.7, np.array([-0.5])) == ([5.0095], [-0.95]), \"failed on negative gradient and prev_update\"\nassert sgd_momentum_update(np.array([[0.0,1.0],[1.0,0.0]]), np.array([[0.5,0.0],[0.0,0.5]]), 0.2, 0.6, np.full((2,2),0.1)) == ([[-0.052,0.988],[0.988,-0.052]], [[0.26,0.06],[0.06,0.26]]), \"failed on mixed matrix with previous velocity\""}
{"task_id": 69, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    if not observations:\n        return -1\n    n = len(S)\n    if len(S) != n:\n        return -1\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    if not B:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    T = len(observations)\n    alpha = [0.0] * n\n    k0 = observations[0]\n    for i in range(n):\n        alpha[i] = S[i] * B[i][k0]\n    for t in range(1, T):\n        kt = observations[t]\n        new_alpha = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += alpha[j] * A[j][i]\n            new_alpha[i] = sum_val * B[i][kt]\n        alpha = new_alpha\n    result = sum(alpha)\n    return round(result, 4)\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [0,1,2]) == 0.0363, \"failed: basic 2-state example\"\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [1]) == 0.36, \"failed: single observation\"\nassert forward_algorithm([1.0,0.0], [[0.5,0.5],[0.2,0.8]], [[0.6,0.4],[0.3,0.7]], [0,1,1]) == 0.2004, \"failed: asymmetric model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [2,0]) == 0.097, \"failed: different start distribution\"\nassert forward_algorithm([1.0], [[1.0]], [[0.2,0.3]], [1,1,0]) == 0.018, \"failed: single-state model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.5]], [0]) == -1, \"failed: B wrong dimensions\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [-1,0]) == -1, \"failed: negative observation index\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [0,2]) == -1, \"failed: observation index out of range\"\nassert forward_algorithm([], [], [], [0]) == -1, \"failed: empty S\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3]], [[0.5,0.5],[0.5,0.5]], [0]) == -1, \"failed: A not square\""}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function returns a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists). The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts. If *data* is empty, an empty dictionary is\n        returned.\n    \"\"\"\n    if not data:\n        return {}\n    counts = {}\n    for sample in data:\n        label = sample[-1]\n        if label in counts:\n            counts[label] += 1\n        else:\n            counts[label] = 1\n    return counts\nassert label_uniq_cnt([[1, 2, 'A'], [3, 5, 'B'], [7, 1, 'A']]) == {'A': 2, 'B': 1}, \"test case failed: multiple labels\"\nassert label_uniq_cnt([[0, 'cat'], [4, 'dog'], [9, 'dog'], [3, 'cat'], [1, 'cat']]) == {'cat': 3, 'dog': 2}, \"test case failed: string labels\"\nassert label_uniq_cnt([[0.2, 5.1, 1], [1.3, 3.3, 1], [9.0, 2.2, 2]]) == {1: 2, 2: 1}, \"test case failed: numeric labels\"\nassert label_uniq_cnt([[42, 'single']]) == {'single': 1}, \"test case failed: single sample\"\nassert label_uniq_cnt([[5, 7, 'same'], [1, 2, 'same'], [3, 4, 'same']]) == {'same': 3}, \"test case failed: all same label\"\nassert label_uniq_cnt([]) == {}, \"test case failed: empty data set\"\nassert label_uniq_cnt([[1,2,3,'x'],[2,3,4,'y'],[3,4,5,'x'],[4,5,6,'z']]) == {'x':2,'y':1,'z':1}, \"test case failed: longer samples\"\nassert label_uniq_cnt([[True, False, True, 'yes'], [False, True, False, 'no'], [True, True, True, 'yes']]) == {'yes':2,'no':1}, \"test case failed: boolean features\"\nassert label_uniq_cnt([[('tuple',1), 'T'], [('tuple',2), 'T'], [('tuple',3), 'S']]) == {'T':2,'S':1}, \"test case failed: tuple feature\"\nassert label_uniq_cnt([[1,'A'],[2,'B'],[3,'C'],[4,'D']]) == {'A':1,'B':1,'C':1,'D':1}, \"test case failed: all unique labels\""}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"Calculate the variance reduction for a regression tree split.\n    \n    Args:\n        y: Target values of the parent node.\n        y_left: Target values of the left child node.\n        y_right: Target values of the right child node.\n        \n    Returns:\n        The variance reduction, rounded to four decimal places.\n    \"\"\"\n    n = y.shape[0]\n    var_parent = np.sum(np.var(y, axis=0, ddof=0))\n    n_left = y_left.shape[0]\n    n_right = y_right.shape[0]\n    var_left = np.sum(np.var(y_left, axis=0, ddof=0))\n    var_right = np.sum(np.var(y_right, axis=0, ddof=0))\n    weighted_var = n_left / n * var_left + n_right / n * var_right\n    vr = var_parent - weighted_var\n    return round(vr, 4)\nassert variance_reduction(np.array([3,7,4,6]), np.array([3,4]), np.array([7,6])) == 2.25, \"failed on simple 1-D split\"\nassert variance_reduction(np.array([1,1,1,1]), np.array([1,1]), np.array([1,1])) == 0.0, \"failed when all variances are zero\"\nassert variance_reduction(np.array([[1,2],[3,4],[5,6]]), np.array([[1,2]]), np.array([[3,4],[5,6]])) == 4.0, \"failed on 2-D target\"\nassert variance_reduction(np.array([-3,-1,-4,-2]), np.array([-3,-4]), np.array([-1,-2])) == 1.0, \"failed on negative values\"\nassert variance_reduction(np.column_stack((np.arange(5), np.arange(5,10))), np.column_stack((np.arange(3), np.arange(5,8))), np.column_stack((np.arange(3,5), np.arange(8,10)))) == 3.0, \"failed on larger 2-D input\"\nassert variance_reduction(np.array([[2,2],[2,2],[2,2]]), np.array([[2,2]]), np.array([[2,2],[2,2]])) == 0.0, \"failed on identical multi-output\"\nassert variance_reduction(np.array([0,1]), np.array([0]), np.array([1])) == 0.25, \"failed on minimal split\""}
{"task_id": 74, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef magnitude_spectrum(frames: 'np.ndarray') -> 'list[list[float]]':\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    result = []\n    if frames.ndim == 1:\n        frame = frames\n        fft_result = np.fft.rfft(frame)\n        magnitude = np.abs(fft_result)\n        rounded = np.round(magnitude, 4)\n        result.append(rounded.tolist())\n    else:\n        for frame in frames:\n            fft_result = np.fft.rfft(frame)\n            magnitude = np.abs(fft_result)\n            rounded = np.round(magnitude, 4)\n            result.append(rounded.tolist())\n    return result\nassert magnitude_spectrum(np.array([[1.0, 0.0, -1.0, 0.0]])) == [[0.0, 2.0, 0.0]], \"test case failed: single frame [1,0,-1,0]\"\nassert magnitude_spectrum(np.array([[1,1,1,1]])) == [[4.0, 0.0, 0.0]], \"test case failed: constant frame ones length 4\"\nassert magnitude_spectrum(np.array([[1,0,0,0,0,0,0,0]])) == [[1.0,1.0,1.0,1.0,1.0]], \"test case failed: impulse length 8\"\nassert magnitude_spectrum(np.array([[0,0,0,0]])) == [[0.0,0.0,0.0]], \"test case failed: all zeros\"\nassert magnitude_spectrum(np.array([1.0, 0.0, -1.0, 0.0])) == [[0.0, 2.0, 0.0]], \"test case failed: 1-D input\"\nassert magnitude_spectrum(np.array([[1,1,1,1],[1,0,-1,0]])) == [[4.0,0.0,0.0],[0.0,2.0,0.0]], \"test case failed: two frames batch\"\nassert magnitude_spectrum(np.zeros((2,6))) == [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]], \"test case failed: zeros batch 6 samples\"\nassert magnitude_spectrum(np.array([[1,2,3,4]])) == [[10.0,2.8284,2.0]], \"test case failed: frame [1,2,3,4]\"\nassert magnitude_spectrum(np.array([[2,2,2,2],[0,0,0,0]])) == [[8.0,0.0,0.0],[0.0,0.0,0.0]], \"test case failed: constant twos and zeros\""}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: failed on majority >1", "solution": "import numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    if k < 1 or k > n_train:\n        return [-1] * n_test\n    predictions = []\n    for x in X_test:\n        distances = []\n        for (xi, yi) in zip(X_train, y_train):\n            dist_sq = sum(((x[i] - xi[i]) ** 2 for i in range(len(x))))\n            distances.append((dist_sq, yi))\n        distances.sort()\n        selected = distances[:k]\n        ys = [y for (d, y) in selected]\n        freq = {}\n        for y in ys:\n            freq[y] = freq.get(y, 0) + 1\n        max_freq = max(freq.values())\n        candidates = [y for y in freq if freq[y] == max_freq]\n        predicted = min(candidates)\n        predictions.append(predicted)\n    return predictions\nassert knn_predict([[1,2],[2,3],[3,4]],[0,1,1],[[2,2]],2)==[0],\"failed on single sample, tie case\"\nassert knn_predict([[0,0],[1,1],[2,2],[3,3]],[1,1,0,0],[[1.1,1.1],[2.9,3.1]],3)==[1,0],\"failed on multi predict\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,0,1],[[0,0]],1)==[0],\"failed on k=1\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,0,1],[[0,0]],3)==[0],\"failed on k==n_train\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,2,2],[[0.9,0.9]],2)==[2],\"failed on majority >1\"\nassert knn_predict([[1,1]], [3], [[2,2],[0,0]], 1)==[3,3],\"failed on single-point training set\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],2)==[0],\"failed on tie chooses smaller label\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],0)==-1,\"failed on invalid k=0\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],3)==-1,\"failed on k>n_train\"\nassert knn_predict([[1,2],[2,1],[0,0],[2,2]],[1,1,0,0],[[1,1]],2)==[1],\"failed on mixed labels\""}
{"task_id": 76, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'type' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = 1e-10\n    y_pred += epsilon\n    log_y_pred = np.log(y_pred)\n    product = y * log_y_pred\n    sum_product = np.sum(product)\n    loss = -sum_product\n    return round(loss, 4)\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0]], [[0.8, 0.1, 0.1], [0.2, 0.5, 0.3]]) == 0.9163, \"failed: basic 3-class example\"\nassert cross_entropy_loss([[1, 0], [0, 1]], [[0.6, 0.4], [0.3, 0.7]]) == 0.8675, \"failed: 2-class example\"\nassert cross_entropy_loss([[1, 0, 0, 0]], [[0.25, 0.25, 0.25, 0.25]]) == 1.3863, \"failed: uniform prediction\"\nassert cross_entropy_loss([[1, 0], [0, 1], [0, 1]], [[0.9, 0.1], [0.4, 0.6], [0.2, 0.8]]) == 0.8393, \"failed: three samples\"\nassert cross_entropy_loss([[1, 0]], [[1.0, 0.0]]) == 0.0, \"failed: perfect prediction\"\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0.7, 0.2, 0.1], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]) == 1.5606, \"failed: three-class batch\"\nassert cross_entropy_loss([[0, 1, 0]], [[0.1, 0.7, 0.2]]) == 0.3567, \"failed: single sample\"\nassert cross_entropy_loss([[0, 0, 1], [1, 0, 0]], [[0.05, 0.15, 0.8], [0.9, 0.05, 0.05]]) == 0.3285, \"failed: swapped classes\"\nassert cross_entropy_loss([[0, 1], [1, 0]], [[0.5, 0.5], [0.2, 0.8]]) == 2.3026, \"failed: high loss case\"\nassert cross_entropy_loss([[1], [1]], [[0.99], [0.98]]) == 0.0303, \"failed: single-class edge case\""}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: name 'X1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    tuple[np.ndarray, list[tuple[np.ndarray, np.ndarray, np.ndarray]]]\n        A tuple containing the final activation AL and the list of caches.\n    \"\"\"\n    caches = []\n    A_prev = X\n    W_layers = [int(key[1:]) for key in parameters.keys() if key.startswith('W')]\n    L = max(W_layers) if W_layers else 0\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))\n        else:\n            A = np.maximum(0, Z)\n        caches.append((A_prev, Z, A))\n        A_prev = A\n    AL = A\n    AL_rounded = np.round(AL, 4)\n    AL_list = AL_rounded.tolist()\n    return (AL_list, caches)\nassert L_model_forward(X1, params1) == [[0.5374, 0.5671]], \"test case failed: example input\"\nassert L_model_forward(X2, params2) == [[0.5]], \"test case failed: zero weights and bias\"\nassert L_model_forward(X2, params3) == [[0.7311]], \"test case failed: bias = 1\"\nassert L_model_forward(X4, params4) == [[0.5]], \"test case failed: two-layer, Z=0\"\nassert L_model_forward(X5, params5) == [[0.5]], \"test case failed: ReLU zeros\"\nassert L_model_forward(X6, params6) == [[0.8808, 0.5]], \"test case failed: batch of 2 examples\"\nassert L_model_forward(X7, params7) == [[0.3775]], \"test case failed: mixed weights\"\nassert L_model_forward(X8, params8) == [[0.2689]], \"test case failed: negative bias\"\nassert L_model_forward(X9, params9) == [[0.6457]], \"test case failed: realistic 2-layer\"\nassert L_model_forward(X10, params10) == [[0.4502]], \"test case failed: small negative Z\""}
{"task_id": 78, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adamax_step(params: list[float], grads: list[float], m: list[float], u: list[float], t: int, learning_rate: float=0.002, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\n\n    Args:\n        params:  Current parameter vector.\n        grads:   Current gradient vector (same length as `params`).\n        m:       First-moment estimates (same length as `params`).\n        u:       Exponentially weighted infinity-norms (same length as `params`).\n        t:       Current time-step counter (starts at 1).\n        learning_rate: Learning rate \u03b7 (default 0.002).\n        beta1:   Exponential decay rate for the first moment \u03b2\u2081.\n        beta2:   Exponential decay rate for the infinity-norm \u03b2\u2082.\n        epsilon: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple `(new_params, new_m, new_u, new_t)` where each list is rounded\n        to 6 decimal places and `new_t = t + 1`.\n    \"\"\"\n    params_np = np.array(params)\n    grads_np = np.array(grads)\n    m_np = np.array(m)\n    u_np = np.array(u)\n    new_m = beta1 * m_np + (1 - beta1) * grads_np\n    new_u = np.maximum(beta2 * u_np, np.abs(grads_np))\n    denominator = 1 - beta1 ** t\n    step = learning_rate / denominator * (new_m / (new_u + epsilon))\n    new_params = params_np - step\n    new_t = t + 1\n    new_params_rounded = np.round(new_params, 6).tolist()\n    new_m_rounded = np.round(new_m, 6).tolist()\n    new_u_rounded = np.round(new_u, 6).tolist()\n    return (new_params_rounded, new_m_rounded, new_u_rounded, new_t)\nassert adamax_step([1.0, 2.0], [0.1, -0.2], [0.0, 0.0], [0.0, 0.0], 1) == ([0.998, 2.002], [0.01, -0.02], [0.1, 0.2], 2), \"test case failed: basic two-dim update\"\nassert adamax_step([1.5], [0.5], [0.0], [0.0], 1) == ([1.498], [0.05], [0.5], 2), \"test case failed: positive grad single dim\"\nassert adamax_step([-1.0], [-1.0], [0.0], [0.0], 1) == ([-0.998], [-0.1], [1.0], 2), \"test case failed: negative grad single dim\"\nassert adamax_step([0.0, 0.0, 0.0], [1.0, 2.0, 3.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], 1) == ([-0.002, -0.002, -0.002], [0.1, 0.2, 0.3], [1.0, 2.0, 3.0], 2), \"test case failed: three-dim positive grads\"\nassert adamax_step([5.0, -3.0], [-2.0, 4.0], [0.0, 0.0], [0.0, 0.0], 1) == ([5.002, -3.002], [-0.2, 0.4], [2.0, 4.0], 2), \"test case failed: mixed sign grads\"\nassert adamax_step([1.498], [0.5], [0.05], [0.5], 2) == ([1.496], [0.095], [0.5], 3), \"test case failed: consecutive update positive grad\"\nassert adamax_step([-0.998], [-1.0], [-0.1], [1.0], 2) == ([-0.996], [-0.19], [1.0], 3), \"test case failed: consecutive update negative grad\"\nassert adamax_step([-0.002, -0.002, -0.002], [1.0, 2.0, 3.0], [0.1, 0.2, 0.3], [1.0, 2.0, 3.0], 2) == ([-0.004, -0.004, -0.004], [0.19, 0.38, 0.57], [1.0, 2.0, 3.0], 3), \"test case failed: consecutive update three-dim\"\nassert adamax_step([0.0], [0.3], [0.0], [0.0], 1) == ([-0.002], [0.03], [0.3], 2), \"test case failed: small positive grad\"\nassert adamax_step([0.0], [-0.3], [0.0], [0.0], 1) == ([0.002], [-0.03], [0.3], 2), \"test case failed: small negative grad\""}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adadelta_update(w: list[float] | np.ndarray, grad_w: list[float] | np.ndarray, rho: float=0.95, eps: float=1e-06) -> list[float]:\n    \"\"\"Perform **one** Adadelta optimisation step.\n\n    The function assumes that it is the very first update so the two running\n    averages (squared gradients and squared updates) are still zeros.  It must\n    therefore implement the mathematical rules of Adadelta and finally return\n    the updated weight vector rounded to 4 decimal places as a regular Python\n    list.\n\n    Args:\n        w:      Current weights (list or NumPy array of any shape).\n        grad_w: Corresponding gradient \u2013 same shape as *w*.\n        rho:    Decay coefficient used by Adadelta (default 0.95).\n        eps:    Small constant to avoid division by zero (default 1e-6).\n\n    Returns:\n        List[float]: weights after one Adadelta update (rounded to 4 decimals).\n    \"\"\"\n    w = np.asarray(w)\n    grad_w = np.asarray(grad_w)\n    g_squared = grad_w ** 2\n    E_grad = (1 - rho) * g_squared\n    RMS_grad = np.sqrt(E_grad + eps)\n    RMS_delta = np.sqrt(eps)\n    adaptive_lr = RMS_delta / RMS_grad\n    delta_w = adaptive_lr * grad_w\n    new_w = w - delta_w\n    rounded_new_w = np.round(new_w, 4)\n    return rounded_new_w.tolist()\nassert adadelta_update([1, 2], [0, 0]) == [1.0, 2.0], \"test failed: zero gradient\"\nassert adadelta_update([1, 2], [1, 1]) == [0.9955, 1.9955], \"test failed: grad=[1,1]\"\nassert adadelta_update([1, 2], [1, -1]) == [0.9955, 2.0045], \"test failed: grad=[1,-1]\"\nassert adadelta_update([1, 2], [10, -10]) == [0.9955, 2.0045], \"test failed: large gradients\"\nassert adadelta_update([0, 0, 0], [1, -2, 0.5]) == [-0.0045, 0.0045, -0.0045], \"test failed: mixed gradients\"\nassert adadelta_update([-1, -2], [5, -5]) == [-1.0045, -1.9955], \"test failed: negative weights\"\nassert adadelta_update([3.1415], [0]) == [3.1415], \"test failed: single weight zero grad\"\nassert adadelta_update([100, 200], [50, 50]) == [99.9955, 199.9955], \"test failed: large weights\"\nassert adadelta_update([0.5, -0.5], [-0.1, 0.1]) == [0.5045, -0.5045], \"test failed: sign check\"\nassert adadelta_update([1, 1, 1, 1], [0, 1, 2, 3]) == [1.0, 0.9955, 0.9955, 0.9955], \"test failed: partial zero grad\""}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    log_a = np.log(A2_clipped)\n    log_1a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1a\n    sum_terms = term1 + term2\n    sum_total = np.sum(sum_terms)\n    m = Y.size\n    J = -1 / m * sum_total\n    return round(J, 6)\nassert compute_cost(np.array([[0.8, 0.9, 0.4, 0.2]]), np.array([[1, 1, 0, 0]])) == 0.265618, \"Failed on example 1\"\nassert compute_cost(np.array([[0.999, 0.001]]), np.array([[1, 0]])) == 0.001001, \"Failed on example 2\"\nassert compute_cost(np.array([[0.5, 0.5, 0.5]]), np.array([[1, 0, 1]])) == 0.693147, \"Failed on example 3\"\nassert compute_cost(np.array([[0.2, 0.8, 0.6, 0.4]]), np.array([[0, 1, 1, 0]])) == 0.366985, \"Failed on example 4\"\nassert compute_cost(np.array([[0.999, 0.001, 0.999, 0.001]]), np.array([[1, 0, 1, 0]])) == 0.001001, \"Failed on example 5\"\nassert compute_cost(np.array([[0.1, 0.2, 0.3, 0.9, 0.8]]), np.array([[1, 1, 1, 0, 0]])) == 1.805604, \"Failed on example 6\"\nassert compute_cost(np.array([[0.7]]), np.array([[1]])) == 0.356675, \"Failed on example 7\"\nassert compute_cost(np.array([[0.05]]), np.array([[0]])) == 0.051293, \"Failed on example 8\"\nassert compute_cost(np.array([[0.5, 0.5]]), np.array([[0, 1]])) == 0.693147, \"Failed on example 9\"\nassert compute_cost(np.array([[0.0, 1.0]]), np.array([[0, 1]])) == 0.0, \"Failed on example 10\""}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean squared error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Array of true target values.\n    y_pred : np.ndarray\n        Array of current predictions produced by the model.\n    h_pred : np.ndarray\n        Array of predictions from the new base learner.\n\n    Returns\n    -------\n    float\n        The optimal step size \u03b3 rounded to four decimal places. If `h_pred`\n        perfectly fits the residuals the function should return 1.0.\n    \"\"\"\n    numerator = np.sum((y - y_pred) * h_pred)\n    denominator = np.sum(h_pred ** 2)\n    if denominator == 0:\n        gamma = 1.0\n    else:\n        gamma = numerator / denominator\n    gamma_rounded = round(gamma, 4)\n    return gamma_rounded\nassert mse_line_search(np.array([1.0]), np.array([0.0]), np.array([1.0])) == 1.0, \"failed on single element perfect fit\"\nassert mse_line_search(np.array([3, -0.5, 2]), np.array([2.5, 0.0, 2]), np.array([-0.5, 0.5, 0])) == -1.0, \"failed on example with negative gamma\"\nassert mse_line_search(np.array([0, 0]), np.array([0, 0]), np.array([1, 1])) == 0.0, \"failed on zero numerator\"\nassert mse_line_search(np.array([1, 2, 3]), np.array([1, 2, 3]), np.array([1, 2, 3])) == 0.0, \"failed when residuals are zero\"\nassert mse_line_search(np.array([1, 2]), np.array([0, 0]), np.array([0, 0])) == 1.0, \"failed when h_pred is all zeros\"\nassert mse_line_search(np.array([2, 4, 6]), np.array([1, 3, 5]), np.array([1, 1, 1])) == 1.0, \"failed on uniform positive residuals\"\nassert mse_line_search(np.array([1, 2, 3, 4]), np.array([4, 3, 2, 1]), np.array([-3, -1, 1, 3])) == 1.0, \"failed on symmetric residuals\"\nassert mse_line_search(np.array([1, 3]), np.array([2, 2]), np.array([0.5, -0.5])) == -2.0, \"failed on fractional h_pred\"\nassert mse_line_search(np.array([1]), np.array([1]), np.array([0])) == 1.0, \"failed on single zero h_pred\"\nassert mse_line_search(np.array([10, 20, 30, 40, 50]), np.array([8, 18, 26, 39, 49]), np.array([2, 2, 4, 1, 1])) == 1.0, \"failed on larger array\""}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    transposed = zip(*predictions)\n    result = []\n    for sample_predictions in transposed:\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        candidates = [k for (k, v) in counts.items() if v == max_count]\n        selected = min(candidates)\n        result.append(selected)\n    return result\nassert aggregate_random_forest_votes([[1,0,1,1],[0,0,1,1],[1,1,1,0]]) == [1,0,1,1], \"test case failed: basic majority voting\"\nassert aggregate_random_forest_votes([[1,2],[2,1]]) == [1,1], \"test case failed: numeric tie breaking\"\nassert aggregate_random_forest_votes([[\"cat\",\"dog\",\"cat\"],[\"dog\",\"dog\",\"cat\"]]) == [\"cat\",\"dog\",\"cat\"], \"test case failed: string votes\"\nassert aggregate_random_forest_votes([[42,42,42]]) == [42,42,42], \"test case failed: single tree\"\nassert aggregate_random_forest_votes([[1],[0],[1],[0]]) == [0], \"test case failed: single sample tie\"\nassert aggregate_random_forest_votes([[3,3,2,2],[2,2,3,3],[3,2,3,2]]) == [3,2,3,2], \"test case failed: alternating ties\"\nassert aggregate_random_forest_votes([]) == [], \"test case failed: empty input\"\nassert aggregate_random_forest_votes([[1,1,1],[1,1,1]]) == [1,1,1], \"test case failed: identical votes\"\nassert aggregate_random_forest_votes([[5,4,3,2,1],[1,2,3,4,5],[5,4,3,2,1]]) == [5,4,3,2,1], \"test case failed: diverse votes\"\nassert aggregate_random_forest_votes([[\"a\",\"b\",\"a\",\"c\"],[\"b\",\"b\",\"a\",\"c\"],[\"a\",\"c\",\"a\",\"c\"]]) == [\"a\",\"b\",\"a\",\"c\"], \"test case failed: mixed string votes\""}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\nassert layer_sizes(np.zeros((3, 5)), np.zeros((1, 5))) == (3, 10, 1), \"failed on (3x5,1x5)\"\nassert layer_sizes(np.zeros((1, 10)), np.zeros((1, 10))) == (1, 10, 1), \"failed on (1x10,1x10)\"\nassert layer_sizes(np.zeros((8, 2)), np.zeros((4, 2))) == (8, 10, 4), \"failed on (8x2,4x2)\"\nassert layer_sizes(np.zeros((6, 9)), np.zeros((3, 9))) == (6, 10, 3), \"failed on (6x9,3x9)\"\nassert layer_sizes(np.zeros((10, 1)), np.zeros((5, 1))) == (10, 10, 5), \"failed on (10x1,5x1)\"\nassert layer_sizes(np.zeros((2, 7)), np.zeros((2, 7))) == (2, 10, 2), \"failed on (2x7,2x7)\"\nassert layer_sizes(np.zeros((4, 4)), np.zeros((6, 4))) == (4, 10, 6), \"failed on (4x4,6x4)\"\nassert layer_sizes(np.zeros((9, 3)), np.zeros((3, 3))) == (9, 10, 3), \"failed on (9x3,3x3)\"\nassert layer_sizes(np.zeros((12, 6)), np.zeros((2, 6))) == (12, 10, 2), \"failed on (12x6,2x6)\"\nassert layer_sizes(np.zeros((5, 20)), np.zeros((8, 20))) == (5, 10, 8), \"failed on (5x20,8x20)\""}
{"task_id": 88, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    sp = np.logaddexp(0.0, z)\n    sp_rounded = np.round(sp, 4)\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return float(sp_rounded)\n    else:\n        return sp_rounded.tolist()\nassert softplus(-1000) == 0.0, \"failed: softplus(-1000)\"\nassert softplus(0) == 0.6931, \"failed: softplus(0)\"\nassert softplus(1) == 1.3133, \"failed: softplus(1)\"\nassert softplus(20) == 20.0, \"failed: softplus(20)\"\nassert softplus([-1, 0, 1]) == [0.3133, 0.6931, 1.3133], \"failed: softplus([-1,0,1])\"\nimport numpy as np\nassert softplus(np.array([-2, 5])) == [0.1269, 5.0067], \"failed: softplus(np.array([-2,5]))\"\nassert softplus(np.array([[0]])) == [[0.6931]], \"failed: softplus(np.array([[0]]))\"\nassert softplus(-0.5) == 0.4741, \"failed: softplus(-0.5)\"\nassert softplus([1000, -1000]) == [1000.0, 0.0], \"failed: softplus([1000,-1000])\""}
{"task_id": 89, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        The activation to be used: either \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        First element is the layer output (rounded to four decimals and\n        converted to a Python list).  Second element is a cache that will\n        be needed for back-propagation.\n    \"\"\"\n    Z = np.dot(W, A_prev) + b\n    if activation == 'relu':\n        A = np.maximum(0, Z)\n    elif activation == 'sigmoid':\n        A = 1 / (1 + np.exp(-Z))\n    else:\n        raise ValueError('Invalid activation function')\n    A_rounded = np.round(A, 4)\n    A_list = A_rounded.tolist()\n    linear_cache = (A_prev, W, b)\n    activation_cache = Z\n    assert A.shape == (W.shape[0], A_prev.shape[1]), f'Shape mismatch: {A.shape} vs {(W.shape[0], A_prev.shape[1])}'\n    return (A_list, (linear_cache, activation_cache))\nassert linear_activation_forward(np.array([[1, 2], [3, 4]]), np.array([[0.2, 0.8]]), np.array([[0.1]]), \"sigmoid\")[0] == [[0.937, 0.9759]], \"test case 1 failed\"\nassert linear_activation_forward(np.array([[1, 2], [3, 4]]), np.array([[0.2, 0.8]]), np.array([[0.1]]), \"relu\")[0] == [[2.7, 3.7]], \"test case 2 failed\"\nassert linear_activation_forward(np.array([[1, 2], [3, 4], [5, 6]]), np.array([[1, -1, 0.5], [0.5, 0.5, 0.5]]), np.array([[0.0], [1.0]]), \"relu\")[0] == [[0.5, 1.0], [5.5, 7.0]], \"test case 3 failed\"\nassert linear_activation_forward(np.array([[1, 2], [3, 4], [5, 6]]), np.array([[1, -1, 0.5], [0.5, 0.5, 0.5]]), np.array([[0.0], [1.0]]), \"sigmoid\")[0] == [[0.6225, 0.7311], [0.9959, 0.9991]], \"test case 4 failed\"\nassert linear_activation_forward(np.array([[0, -1, 2]]), np.array([[-3]]), np.array([[0]]), \"relu\")[0] == [[0.0, 3.0, 0.0]], \"test case 5 failed\"\nassert linear_activation_forward(np.array([[0, -1, 2]]), np.array([[-3]]), np.array([[0]]), \"sigmoid\")[0] == [[0.5, 0.9526, 0.0025]], \"test case 6 failed\"\nassert linear_activation_forward(np.array([[1], [2]]), np.array([[1, 1], [-1, 2]]), np.array([[0.5], [-0.5]]), \"sigmoid\")[0] == [[0.9707], [0.9241]], \"test case 7 failed\"\nassert linear_activation_forward(np.array([[1], [2]]), np.array([[1, 1], [-1, 2]]), np.array([[0.5], [-0.5]]), \"relu\")[0] == [[3.5], [2.5]], \"test case 8 failed\"\nassert linear_activation_forward(np.array([[1], [-1]]), np.array([[2, 3]]), np.array([[-0.5]]), \"relu\")[0] == [[0.0]], \"test case 9 failed\"\nassert linear_activation_forward(np.array([[0], [0]]), np.array([[2, 3]]), np.array([[-0.5]]), \"sigmoid\")[0] == [[0.3775]], \"test case 10 failed\""}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numbers\nimport numbers\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and (not isinstance(a, bool))\nassert is_number(10) is True, \"test case failed: is_number(10)\"\nassert is_number(3.14) is True, \"test case failed: is_number(3.14)\"\nassert is_number(1+2j) is True, \"test case failed: is_number(1+2j)\"\nassert is_number(\"5\") is False, \"test case failed: is_number('5')\"\nassert is_number([1,2]) is False, \"test case failed: is_number([1,2])\"\nassert is_number(True) is False, \"test case failed: is_number(True)\"\nassert is_number(None) is False, \"test case failed: is_number(None)\"\nassert is_number(-7) is True, \"test case failed: is_number(-7)\"\nassert is_number(float('nan')) is True, \"test case failed: is_number(float('nan'))\"\nassert is_number({'num': 5}) is False, \"test case failed: is_number({'num': 5})\""}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_array = np.array(Z)\n    A = np.maximum(0, Z_array)\n    return (A.tolist(), Z_array.tolist())\nassert relu(np.array([[-1, 2], [3, -4]])) == ([[0, 2], [3, 0]], [[-1.0, 2.0], [3.0, -4.0]]), \"test case failed: relu([[-1,2],[3,-4]])\"\nassert relu(np.array([[0, 0], [0, 0]])) == ([[0, 0], [0, 0]], [[0.0, 0.0], [0.0, 0.0]]), \"test case failed: relu([[0,0],[0,0]])\"\nassert relu(np.array([[1, -1, 0]])) == ([[1, 0, 0]], [[1.0, -1.0, 0.0]]), \"test case failed: relu([[1,-1,0]])\"\nassert relu(np.array([-5, 4, -3, 2, 0])) == ([0, 4, 0, 2, 0], [-5.0, 4.0, -3.0, 2.0, 0.0]), \"test case failed: relu([-5,4,-3,2,0])\"\nassert relu(np.array(5)) == (5.0, 5.0), \"test case failed: relu(5)\"\nassert relu(np.array(-7)) == (0.0, -7.0), \"test case failed: relu(-7)\"\nassert relu(np.zeros((3, 3))) == ([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), \"test case failed: relu(zeros)\"\nassert relu([[1, 2], [3, 4]]) == ([[1, 2], [3, 4]], [[1.0, 2.0], [3.0, 4.0]]), \"test case failed: relu([[1,2],[3,4]])\"\nassert relu([[-0.1, -0.2, -0.3]]) == ([[0, 0, 0]], [[-0.1, -0.2, -0.3]]), \"test case failed: relu([[-0.1,-0.2,-0.3]])\"\nassert relu(np.arange(-4, 5)) == ([0, 0, 0, 0, 0, 1, 2, 3, 4], [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]), \"test case failed: relu(arange(-4,5))\""}
{"task_id": 94, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for (label, cnt) in counts.items() if cnt == max_count]\n    return min(max_labels)\nassert knn_majority_vote(['cat', 'dog', 'cat', 'bird']) == 'cat', \"failed on ['cat', 'dog', 'cat', 'bird']\"\nassert knn_majority_vote([1, 2, 2, 3, 1, 2]) == 2, \"failed on [1, 2, 2, 3, 1, 2]\"\nassert knn_majority_vote(['apple', 'banana', 'apple', 'banana']) == 'apple', \"failed on ['apple', 'banana', 'apple', 'banana']\"\nassert knn_majority_vote([42]) == 42, \"failed on [42]\"\nassert knn_majority_vote(['a', 'b', 'c']) == 'a', \"failed on ['a', 'b', 'c']\"\nassert knn_majority_vote([3, 3, 2, 2]) == 2, \"failed on [3, 3, 2, 2]\"\nassert knn_majority_vote([5, 4, 5, 4, 4, 5]) == 4, \"failed on [5, 4, 5, 4, 4, 5]\"\nassert knn_majority_vote(['x', 'y', 'y', 'x', 'z', 'z']) == 'x', \"failed on ['x', 'y', 'y', 'x', 'z', 'z']\"\nassert knn_majority_vote(list('aabbccd')) == 'a', \"failed on list('aabbccd')\"\nassert knn_majority_vote([0, 0, 1]) == 0, \"failed on [0, 0, 1]\""}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        if Z.ndim == 0:\n            return 1.0 if Z > 0 else 0.0\n        else:\n            derivative = np.where(Z > 0, 1.0, 0.0)\n            return derivative.tolist()\n    else:\n        arr = np.array(Z)\n        derivative = np.where(arr > 0, 1.0, 0.0)\n        return derivative.tolist()\nassert drelu([-2, -1, 0, 1, 5]) == [0.0, 0.0, 0.0, 1.0, 1.0], \"failed: drelu([-2, -1, 0, 1, 5])\"\nassert drelu([[1, -1], [-0.5, 2]]) == [[1.0, 0.0], [0.0, 1.0]], \"failed: drelu([[1, -1], [-0.5, 2]])\"\nassert drelu(3) == 1.0, \"failed: drelu(3)\"\nassert drelu(0) == 0.0, \"failed: drelu(0)\"\nassert drelu([[[-1, 2], [3, 0]], [[0, -2], [4, 5]]]) == [[[0.0, 1.0], [1.0, 0.0]], [[0.0, 0.0], [1.0, 1.0]]], \"failed: multi-dimensional input\"\nassert drelu(np.array([-3, -0.1, 0, 0.1, 5])) == [0.0, 0.0, 0.0, 1.0, 1.0], \"failed: ndarray input\"\nassert drelu(np.zeros((2, 3))) == [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], \"failed: zeros array\"\nassert drelu([-9999, 0.0001]) == [0.0, 1.0], \"failed: extreme values\"\nassert drelu([0, 0, 0]) == [0.0, 0.0, 0.0], \"failed: list of zeros\"\nassert drelu([[5]]) == [[1.0]], \"failed: single element 2-D list\""}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Predicted labels from a classifier.\n\n    Returns\n    -------\n    float | int\n        The classification error rate rounded to four decimals, or \u22121 when\n        the input lengths are not identical.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    actual_arr = np.array(actual)\n    predicted_arr = np.array(predicted)\n    mismatches = np.sum(actual_arr != predicted_arr)\n    total = len(actual_arr)\n    if total == 0:\n        return 0.0\n    error_rate = mismatches / total\n    return round(error_rate, 4)\nassert classification_error([1, 0, 1, 1], [1, 0, 1, 1]) == 0.0, \"Failed: identical arrays\"\nassert classification_error([0, 1, 2, 2, 1], [0, 0, 2, 1, 1]) == 0.4, \"Failed: basic example\"\nassert classification_error([0, 0, 0], [1, 1, 1]) == 1.0, \"Failed: all mismatched\"\nassert classification_error([1, 2, 3], [1, 2]) == -1, \"Failed: length mismatch\"\nassert classification_error(np.array([3, 3, 2, 1]), np.array([3, 2, 2, 1])) == 0.25, \"Failed: numpy arrays\"\nassert classification_error((5, 6, 7, 8), (5, 6, 0, 0)) == 0.5, \"Failed: tuples\"\nassert classification_error([\"cat\", \"dog\"], [\"dog\", \"dog\"]) == 0.5, \"Failed: string labels\"\nassert classification_error([True, False, True], [True, True, False]) == 0.6667, \"Failed: boolean labels\"\nassert classification_error(range(8), [0,1,2,3,4,5,6,7]) == 0.0, \"Failed: range vs list\"\nassert classification_error(np.array([1]), np.array([2])) == 1.0, \"Failed: single element mismatch\""}
{"task_id": 101, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softmax(x: np.ndarray, axis: int=1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax_result = exp_x / sum_exp\n    rounded_result = np.round(softmax_result, 4)\n    return rounded_result.tolist()\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]]), 1) == [[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]], \"test case failed: softmax([[1,2,3],[4,5,6]], axis=1)\"\nassert softmax(np.array([[1, 1], [2, 2]]), 0) == [[0.2689, 0.2689], [0.7311, 0.7311]], \"test case failed: softmax([[1,1],[2,2]], axis=0)\"\nassert softmax(np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]]), 2) == [[[0.2689, 0.7311], [0.2689, 0.7311]], [[0.2689, 0.7311], [0.2689, 0.7311]]], \"test case failed: softmax(3-D array, axis=2)\"\nassert softmax(np.array([[[1, 2]], [[3, 4]]]), 1) == [[[1.0, 1.0]], [[1.0, 1.0]]], \"test case failed: softmax(shape=(2,1,2), axis=1)\"\nassert softmax(np.array([[1000, 1000], [1000, 1000]]), 1) == [[0.5, 0.5], [0.5, 0.5]], \"test case failed: softmax(large values)\"\nassert softmax(np.array([[0, 1, 2], [2, 3, 4]]), 0) == [[0.1192, 0.1192, 0.1192], [0.8808, 0.8808, 0.8808]], \"test case failed: softmax([[0,1,2],[2,3,4]], axis=0)\"\nassert softmax(np.zeros((2, 2, 3)), -1) == [[[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]], [[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]]], \"test case failed: softmax(zeros 3-D, axis=-1)\"\nassert softmax(np.array([[[1, 2, 3], [4, 5, 6]]]), 2) == [[[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]]], \"test case failed: softmax(shape=(1,2,3), axis=2)\"\nassert softmax(np.array([[-1, -1, -1], [0, 0, 0]]), 1) == [[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]], \"test case failed: softmax(equal values)\""}
{"task_id": 105, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    x = np.asarray(x)\n    exponent = -(x - mean) ** 2 / (2 * sigma ** 2)\n    exp_part = np.exp(exponent)\n    normalization = 1.0 / (np.sqrt(2 * np.pi) * sigma)\n    pdf = exp_part * normalization\n    pdf_rounded = np.round(pdf, 5)\n    return pdf_rounded.tolist()\nassert gaussian_pdf([1, 2, 3], 2, 1) == [0.24197, 0.39894, 0.24197], \"test case failed: gaussian_pdf([1, 2, 3], 2, 1)\"\nassert gaussian_pdf([0], 0, 1) == [0.39894], \"test case failed: gaussian_pdf([0], 0, 1)\"\nassert gaussian_pdf([0], 0, 0.5) == [0.79788], \"test case failed: gaussian_pdf([0], 0, 0.5)\"\nassert gaussian_pdf([-1, 0, 1], 0, 1) == [0.24197, 0.39894, 0.24197], \"test case failed: gaussian_pdf([-1, 0, 1], 0, 1)\"\nassert gaussian_pdf([2], 1, 1) == [0.24197], \"test case failed: gaussian_pdf([2], 1, 1)\"\nassert gaussian_pdf([1], 0, -1) == -1, \"test case failed: gaussian_pdf([1], 0, -1)\"\nassert gaussian_pdf([1], 0, 0) == -1, \"test case failed: gaussian_pdf([1], 0, 0)\"\nassert gaussian_pdf([5], 2, 3) == [0.08066], \"test case failed: gaussian_pdf([5], 2, 3)\"\nassert gaussian_pdf([2.5, 3.5], 3, 0.5) == [0.48394, 0.48394], \"test case failed: gaussian_pdf([2.5, 3.5], 3, 0.5)\"\nassert gaussian_pdf([], 0, 1) == [], \"test case failed: gaussian_pdf([], 0, 1)\""}
{"task_id": 107, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_step(theta: np.ndarray, grad: np.ndarray, m_prev: np.ndarray, v_prev: np.ndarray, t: int, alpha: float=0.01, beta1: float=0.9, beta2: float=0.99, epsilon: float=1e-09) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\n\n    Parameters\n    ----------\n    theta : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to *theta*.\n    m_prev : np.ndarray\n        First moment vector from the previous time step.\n    v_prev : np.ndarray\n        Second moment vector from the previous time step.\n    t : int\n        Current time-step counter (starting at 1).\n    alpha : float, optional\n        Learning rate. Default is 0.01.\n    beta1 : float, optional\n        Exponential decay rate for the first moment. Default is 0.9.\n    beta2 : float, optional\n        Exponential decay rate for the second moment. Default is 0.99.\n    epsilon : float, optional\n        Small constant for numerical stability. Default is 1e-9.\n\n    Returns\n    -------\n    tuple[list, list, list, int]\n        A tuple containing the updated parameters, the new first and\n        second moment estimates, and the incremented time step. All\n        numerical values are rounded to 6 decimal places and converted\n        to regular Python lists.\n    \"\"\"\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * grad ** 2\n    m_correction = 1.0 / (1.0 - beta1 ** t)\n    m_hat = m_t * m_correction\n    v_correction = 1.0 / (1.0 - beta2 ** t)\n    v_hat = v_t * v_correction\n    denominator = np.sqrt(v_hat) + epsilon\n    theta_new = theta - alpha * (m_hat / denominator)\n    theta_new_rounded = np.round(theta_new, 6).tolist()\n    m_new_rounded = np.round(m_t, 6).tolist()\n    v_new_rounded = np.round(v_t, 6).tolist()\n    new_t = t + 1\n    return (theta_new_rounded, m_new_rounded, v_new_rounded, new_t)\nassert adam_step(np.array([1.0, 2.0]), np.array([0.1, -0.2]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([0.99, 2.01], [0.01, -0.02], [0.0001, 0.0004], 2), \"test case failed: adam_step([1,2],[0.1,-0.2])\"\nassert adam_step(np.array([1.0]), np.array([0.0]), np.array([0.0]), np.array([0.0]), 1) == ([1.0], [0.0], [0.0], 2), \"test case failed: zero gradient\"\nassert adam_step(np.array([-1.0, 2.0, -3.0]), np.array([-0.3, -0.5, 0.7]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 1) == ([-0.99, 2.01, -3.01], [-0.03, -0.05, 0.07], [0.0009, 0.0025, 0.0049], 2), \"test case failed: mixed signs\"\nassert adam_step(np.array([0.0]), np.array([1.0]), np.array([0.0]), np.array([0.0]), 1) == ([-0.01], [0.1], [0.01], 2), \"test case failed: positive gradient scalar\"\nassert adam_step(np.array([0.0, 0.0]), np.array([1.0, 1.0]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([-0.01, -0.01], [0.1, 0.1], [0.01, 0.01], 2), \"test case failed: identical gradients\"\nassert adam_step(np.array([2.0]), np.array([-2.0]), np.array([0.0]), np.array([0.0]), 1) == ([2.01], [-0.2], [0.04], 2), \"test case failed: large negative gradient\"\nassert adam_step(np.array([1.0]), np.array([0.3]), np.array([0.0]), np.array([0.0]), 2) == ([0.992575], [0.03], [0.0009], 3), \"test case failed: second step\"\nassert adam_step(np.array([1.0]), np.array([-1.0]), np.array([0.0]), np.array([0.0]), 1) == ([1.01], [-0.1], [0.01], 2), \"test case failed: negative gradient scalar\"\nassert adam_step(np.array([1.0, 2.0, 3.0]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 1) == ([1.0, 2.0, 3.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], 2), \"test case failed: all zero gradients\"\nassert adam_step(np.array([1.2, 3.4]), np.array([0.2, 0.2]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([1.19, 3.39], [0.02, 0.02], [0.0004, 0.0004], 2), \"test case failed: small identical gradients\""}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: solve: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (m,m),(m,n)->(m,n) (size 3 is different from 2)", "solution": "import numpy as np\nimport numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    (N, M) = X.shape\n    W = np.random.randn(N, K) * 0.1\n    H = np.random.randn(K, M) * 0.1\n\n    def compute_loss(X, W, H, alpha):\n        recon = W @ H\n        loss = np.linalg.norm(X - recon) ** 2 + alpha * (np.linalg.norm(W) ** 2 + np.linalg.norm(H) ** 2)\n        return loss\n    current_loss = compute_loss(X, W, H, alpha)\n    if current_loss < tol:\n        pass\n    else:\n        for _ in range(max_iter):\n            Ht = H.T\n            H_Ht = H @ Ht\n            A = H_Ht + alpha * np.eye(K)\n            b = X @ Ht\n            new_W = np.linalg.solve(A, b)\n            Wt = new_W.T\n            Wt_W = Wt @ new_W\n            A = Wt_W + alpha * np.eye(K)\n            b = Wt @ X\n            new_H = np.linalg.solve(A, b)\n            (W, H) = (new_W, new_H)\n            current_loss = compute_loss(X, W, H, alpha)\n            if current_loss < tol:\n                break\n    X_hat = W @ H\n    X_hat_rounded = np.round(X_hat, 4)\n    return X_hat_rounded.tolist()\nassert np.allclose(als_factorization(np.array([[1.,2.],[3.,4.]]),2,alpha=0.0,max_iter=1000,tol=1e-8),[[1.0,2.0],[3.0,4.0]],atol=1e-3),\"failed on 2\u00d72 full-rank matrix\"\nassert np.allclose(als_factorization(np.array([[1.,2.,3.],[4.,5.,6.]]),2,alpha=0.0),[[1.0,2.0,3.0],[4.0,5.0,6.0]],atol=1e-3),\"failed on 2\u00d73 full-rank matrix\"\nassert np.allclose(als_factorization(np.array([[7.,8.],[2.,1.],[3.,4.]]),2,alpha=0.0),[[7.0,8.0],[2.0,1.0],[3.0,4.0]],atol=1e-3),\"failed on 3\u00d72 matrix\"\nassert np.allclose(als_factorization(np.array([[1.5,2.5],[3.5,4.5]]),2,alpha=0.0),[[1.5,2.5],[3.5,4.5]],atol=1e-3),\"failed on decimal matrix\"\nassert np.allclose(als_factorization(np.array([[10.]]),1,alpha=0.0),[[10.0]],atol=1e-3),\"failed on 1\u00d71 matrix\"\nassert np.allclose(als_factorization(np.array([[2.,2.],[2.,2.]]),1,alpha=0.0),[[2.0,2.0],[2.0,2.0]],atol=1e-3),\"failed on constant matrix\"\nassert np.allclose(als_factorization(np.array([[1.,2.],[2.,4.]]),1,alpha=0.0),[[1.0,2.0],[2.0,4.0]],atol=1e-3),\"failed on rank-1 2\u00d72 matrix\""}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: name '_identity_predict' is not defined", "solution": "import numpy as np\nimport numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a text string into a fixed-length list of integer token ids.\"\"\"\n    encoded = [vocabulary.get(c, 0) for c in text]\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    filtered = []\n    for idx in indices:\n        if idx != 0:\n            char = inverse_vocab.get(idx, '')\n            filtered.append(char)\n    return ''.join(filtered)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on encoded text and decodes the prediction.\"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    encoded_np = np.array(encoded, dtype=np.int32).reshape(1, -1)\n    predictions = model.predict(encoded_np)\n    predicted_ids = np.argmax(predictions, axis=-1).flatten()\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hello') == 'hello', 'failed on \"hello\"'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'HELLO') == 'hello', 'failed on upper-case input'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '') == '', 'failed on empty string'\na20 = 'a' * 25\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, a20) == 'a' * 20, 'failed on long input truncation'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hi!') == 'hi', 'failed on unknown character removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'data science') == 'data science', 'failed on string with space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '       ') == '       ', 'failed on only spaces'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc xyz') == 'abc xyz', 'failed on mixed letters and space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc_def') == 'abcdef', 'failed on underscore removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'padding test') == 'padding test', 'failed on general case'"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "import re\nfrom collections import Counter\nimport re\nfrom collections import Counter\nclass Token:\n\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    words = re.findall('[a-zA-Z]+', text)\n    if not words:\n        return []\n    lower_words = [word.lower() for word in words]\n    counts = Counter(lower_words)\n    tokens = []\n    for (word, count) in counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    return tokens\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"Hello, hello world!\")] == [(\"hello\", 2), (\"world\", 1)], \"test case failed: 'Hello, hello world!'\"\nassert tokenize_and_count(\"\") == [], \"test case failed: empty string\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"A a A a b B\")] == [(\"a\", 4), (\"b\", 2)], \"test case failed: 'A a A a b B'\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"b a\")] == [(\"a\", 1), (\"b\", 1)], \"test case failed: 'b a'\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"This is a test. This test is only a test.\")] == [(\"test\", 3), (\"a\", 2), (\"is\", 2), (\"this\", 2), (\"only\", 1)], \"test case failed: sentence with punctuation\"\nassert tokenize_and_count(\"123 456 !!!\") == [], \"test case failed: numeric input\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"co-op co op\")] == [(\"co\", 2), (\"op\", 2)], \"test case failed: hyphenated words\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"Python\")] == [(\"python\", 1)], \"test case failed: single word\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"one1 two2 two2 one1 one1\")] == [(\"one\", 3), (\"two\", 2)], \"test case failed: alphanumeric words\"\nlarge_text = \"word \" * 1000 + \"test \" * 500\nassert [(tok.word, tok.count) for tok in tokenize_and_count(large_text)][:2] == [(\"word\", 1000), (\"test\", 500)], \"test case failed: large repetition\""}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: name 'run_example' is not defined", "solution": "\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    results = []\n    for example in examples:\n        output = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output)\n        print(f'input: {example}')\n        print(f'output: {output_str}')\n        results.append(output_str)\n    return results\nassert run_examples(lambda s: s[::-1], None, None, [\"abc\"]) == [\"cba\"], \"test case failed: single example reversal\"\nassert run_examples(lambda s: s.upper(), None, None, [\"hello\", \"world\"]) == [\"HELLO\", \"WORLD\"], \"test case failed: uppercase mapping\"\nassert run_examples(lambda s: s, None, None, []) == [], \"test case failed: empty example list\"\nassert run_examples(lambda s: s[::-1], None, None, [\"\", \"a\"]) == [\"\", \"a\"], \"test case failed: empty and single char strings\"\nassert run_examples(lambda s: s*2, None, None, [\"cat\"]) == [\"catcat\"], \"test case failed: duplication model\"\nassert run_examples(lambda s: ''.join(sorted(s)), None, None, [\"cba\", \"fed\"] ) == [\"abc\", \"def\"], \"test case failed: sort characters\"\nassert run_examples(lambda s: ''.join(chr(ord(c)+1) for c in s), None, None, [\"abc\"]) == [\"bcd\"], \"test case failed: shift characters\"\nassert run_examples(lambda s: s[::-1].upper(), None, None, [\"Python\", \"AI\"]) == [\"NOHTYP\", \"IA\"], \"test case failed: reverse and uppercase\"\nassert run_examples(lambda s: ''.join('*' for _ in s), None, None, [\"mask\"]) == [\"****\"], \"test case failed: masking model\"\nassert run_examples(lambda s: s.strip(), None, None, [\"  spaced  \"]) == [\"spaced\"], \"test case failed: strip whitespaces\""}
{"task_id": 114, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef selu(x: np.ndarray, derivative: bool=False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    lambda_ = 1.0507009873554805\n    mask = x >= 0\n    if not derivative:\n        positive = x * lambda_\n        negative = alpha * (np.exp(x) - 1) * lambda_\n        output = np.where(mask, positive, negative)\n    else:\n        positive_deriv = lambda_\n        negative_deriv = alpha * np.exp(x) * lambda_\n        output = np.where(mask, positive_deriv, negative_deriv)\n    rounded = np.round(output, 6)\n    if rounded.ndim == 0:\n        return [rounded.item()]\n    else:\n        return rounded.tolist()\nimport numpy as np\nassert selu(np.array([-1, 0, 1])) == [-1.111331, 0.0, 1.050701], \"test case failed: selu([-1,0,1])\"\nassert selu(np.array([0])) == [0.0], \"test case failed: selu([0])\"\nassert selu(np.array([0]), derivative=True) == [1.050701], \"test case failed: selu([0], True)\"\nassert selu(np.array([])) == [], \"test case failed: empty array activation\"\nassert selu(np.array([]), derivative=True) == [], \"test case failed: empty array derivative\""}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: name '_expected' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    y = y.ravel()\n    loss = -1 / m * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n    gradient = 1 / m * X.T.dot(p - y)\n    gradient = gradient.reshape(-1, 1)\n    loss_rounded = round(loss, 4)\n    gradient_rounded = np.round(gradient, 4).tolist()\n    return (loss_rounded, gradient_rounded)\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])) == _expected(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])), \"test case failed: #1\"\nassert logistic_loss_and_gradient(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])) == _expected(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])), \"test case failed: #2\"\nassert logistic_loss_and_gradient(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])) == _expected(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])), \"test case failed: #3\"\nassert logistic_loss_and_gradient(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])) == _expected(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])), \"test case failed: #4\"\nassert logistic_loss_and_gradient(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])) == _expected(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])), \"test case failed: #5\"\nassert logistic_loss_and_gradient(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])) == _expected(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])), \"test case failed: #6\"\nassert logistic_loss_and_gradient(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])) == _expected(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])), \"test case failed: #7\"\nassert logistic_loss_and_gradient(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])) == _expected(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])), \"test case failed: #8\"\nassert logistic_loss_and_gradient(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])) == _expected(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])), \"test case failed: #9\"\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])) == _expected(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])), \"test case failed: #10\""}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    n = len(y)\n    parent_mean = np.mean(y)\n    mse_parent = np.mean((y - parent_mean) ** 2)\n    total_child_mse = 0.0\n    for s in splits:\n        m = len(s)\n        child_mean = np.mean(s)\n        child_mse = np.mean((s - child_mean) ** 2)\n        weight = m / n\n        total_child_mse += child_mse * weight\n    delta_mse = mse_parent - total_child_mse\n    return round(delta_mse, 4)\nimport numpy as np\nassert mse_criterion(np.array([1, 2, 3, 4]), [np.array([1, 2]), np.array([3, 4])]) == 1.0, \"test case failed: basic two-way split\"\nassert mse_criterion(np.array([1, 2, 3]), [np.array([1, 2, 3])]) == 0.0, \"test case failed: no real split\"\nassert mse_criterion(np.array([1, 1, 1]), [np.array([1, 1]), np.array([1])]) == 0.0, \"test case failed: zero variance data\"\nassert mse_criterion(np.array([0, 10, 20, 30]), [np.array([0, 10]), np.array([20, 30])]) == 100.0, \"test case failed: large variance reduction\"\nassert mse_criterion(np.array([1, 4, 7, 10, 13]), [np.array([1, 4, 7]), np.array([10, 13])]) == 13.5, \"test case failed: uneven split sizes\"\nassert mse_criterion(np.array([2, 2, 2, 2]), [np.array([2, 2]), np.array([2, 2])]) == 0.0, \"test case failed: identical targets\"\nassert mse_criterion(np.array([1, 2]), [np.array([1]), np.array([2])]) == 0.25, \"test case failed: singletons\"\nassert mse_criterion(np.array([1, 2, 3, 4, 5, 6]), [np.array([1, 2, 3]), np.array([4, 5, 6])]) == 2.25, \"test case failed: six elements split in half\"\nassert mse_criterion(np.array([1, 4, 7, 10]), [np.array([1]), np.array([4]), np.array([7, 10])]) == 10.125, \"test case failed: three-way split\"\nassert mse_criterion(np.array([0, 0, 10, 10, 20, 20]), [np.array([0, 0, 10]), np.array([10, 20, 20])]) == 44.4444, \"test case failed: mixed targets split\""}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sgd_update(params: list, grads: list, alpha: float=0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    The function receives a list of parameters and a list of corresponding\n    gradients and must return the updated parameters after applying the SGD\n    rule using the supplied learning-rate `alpha`.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Gradients for each parameter.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        Updated parameters converted to Python lists, each rounded to four\n        decimal places.\n    \"\"\"\n    if not params or not grads or len(params) != len(grads):\n        return []\n    new_params = []\n    for (p, g) in zip(params, grads):\n        new_p = p - alpha * g\n        new_p_rounded = np.round(new_p, 4)\n        new_params.append(new_p_rounded.tolist())\n    return new_params\nassert sgd_update([np.array([1.0, 2.0, 3.0])], [np.array([0.1, -0.2, 0.3])], 0.1) == [[0.99, 2.02, 2.97]], \"failed on basic vector update\"\nassert sgd_update([np.array([[1.5, -2.0], [3.0, 4.0]])], [np.array([[0.5, 0.5], [1.0, -1.0]])], 0.2) == [[ [1.4, -2.1], [2.8, 4.2] ]], \"failed on matrix update\"\nassert sgd_update([], [], 0.1) == [], \"failed on empty input\"\nassert sgd_update([np.array(5.0)], [np.array(2.0)], 0.5) == [4.0], \"failed on scalar update\"\nassert sgd_update([np.array([0.0, 0.0])], [np.array([0.0, 0.0])], 10.0) == [[0.0, 0.0]], \"failed on zero gradients\"\nassert sgd_update([np.array([1.0, 1.0])], [np.array([1.0, 1.0])], 1.0) == [[0.0, 0.0]], \"failed on alpha=1\"\nassert sgd_update([np.array([-1.0, -2.0])], [np.array([-0.5, 0.5])], 0.4) == [[-0.8, -2.2]], \"failed on negative params/gradients\"\nassert sgd_update([np.array([[2.0, 3.0, 4.0]])], [np.array([[1.0, 1.0, 1.0]])], 0.25) == [[[1.75, 2.75, 3.75]]], \"failed on 1xN matrix\""}
{"task_id": 120, "completion_id": 0, "passed": false, "result": "failed: failed: slope 2 intercept 0", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef linear_regression(X: list[list[int | float]], y: list[int | float], n_iterations: int, learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\n\n    Args:\n        X: Training samples \u2013 list of lists with numerical feature values.\n        y: Target values \u2013 list of numbers, same length as X.\n        n_iterations: Number of gradient-descent steps to perform.\n        learning_rate: Positive learning rate controlling the step size.\n\n    Returns:\n        The learned weight vector as a list (bias first) rounded to 4 decimals.\n        If the input dimensions are incompatible the function must return -1.\n    \"\"\"\n    if len(X) != len(y):\n        return -1\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.insert(X_np, 0, 1, axis=1)\n    (m, n) = X_np.shape\n    w = np.random.uniform(low=-1 / math.sqrt(n), high=1 / math.sqrt(n), size=(n, 1))\n    for _ in range(n_iterations):\n        y_pred = X_np.dot(w)\n        error = y_pred - y_np\n        gradient = X_np.T.dot(error) / m\n        w = w - learning_rate * gradient\n    result = w.flatten().round(4).tolist()\n    return result\nassert linear_regression([[1], [2], [3]], [2, 4, 6], 10000, 0.01) == [0.0, 2.0], \"failed: slope 2 intercept 0\"\nassert linear_regression([[1], [2], [3], [4]], [3, 5, 7, 9], 10000, 0.01) == [1.0, 2.0], \"failed: slope 2 intercept 1\"\nassert linear_regression([[0], [1], [2], [3]], [5, 4, 3, 2], 15000, 0.01) == [5.0, -1.0], \"failed: slope -1 intercept 5\"\nassert linear_regression([[1, 2, 3], [2, 1, 0], [0, 0, 1], [3, 5, 1]], [6, 3, 1, 9], 25000, 0.01) == [0.0, 1.0, 1.0, 1.0], \"failed: three-feature weights [0,1,1,1]\"\nassert linear_regression([[1], [2], [3]], [7, 7, 7], 15000, 0.01) == [7.0, 0.0], \"failed: constant function\"\nassert linear_regression([[-1], [0], [1]], [-1, 1, 3], 10000, 0.01) == [1.0, 2.0], \"failed: negative feature values\"\nassert linear_regression([[2], [4], [6], [8]], [4, 8, 12, 16], 10000, 0.01) == [0.0, 2.0], \"failed: larger inputs slope 2\"\nassert linear_regression([[1, 1], [2, 3], [3, 2], [4, 0]], [0, -1, 1, 4], 20000, 0.01) == [0.0, 1.0, -1.0], \"failed: weights [0,1,-1]\"\nassert linear_regression([[1, 2], [2, 0], [0, 1], [3, 4]], [-0.5, -2.0, -2.0, 2.5], 20000, 0.01) == [-3.0, 0.5, 1.0], \"failed: weights [-3,0.5,1]\""}
{"task_id": 122, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    ones_col = np.ones((X_np.shape[0], 1))\n    X_augmented = np.hstack((ones_col, X_np))\n    X_pseudo = np.linalg.pinv(X_augmented)\n    w = X_pseudo @ y_np\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\nassert linear_regression_weights([[0], [1], [2]], [1, 2, 3]) == [1.0, 1.0], \"failed on y = x + 1\"\nassert linear_regression_weights([[0], [1], [2], [3]], [5, 5, 5, 5]) == [5.0, 0.0], \"failed on constant target\"\nassert linear_regression_weights([[1], [2], [3]], [2, 4, 6]) == [0.0, 2.0], \"failed on y = 2x\"\nassert linear_regression_weights([[1, 2], [2, 3], [3, 4], [4, 5]], [3, 5, 7, 9]) == [0.0, 1.0, 1.0], \"failed on y = x1 + x2\"\nassert linear_regression_weights([[1, 0], [0, 1], [1, 1], [2, 3]], [3, 3, 4, 7]) == [2.0, 1.0, 1.0], \"failed on y = 2 + x1 + x2\"\nassert linear_regression_weights([[4], [5], [6]], [9, 11.5, 14]) == [-1.0, 2.5], \"failed on y = -1 + 2.5x\"\nassert linear_regression_weights([[2, 1], [0, 1], [1, 3]], [0, 2, 5]) == [0.0, -1.0, 2.0], \"failed on y = -x1 + 2x2\"\nassert linear_regression_weights([[1], [2], [3]], [1.5, 1.5, 1.5]) == [1.5, 0.0], \"failed on y = 1.5\"\nassert linear_regression_weights([[0, 1], [1, 0], [2, 2]], [0, 1.5, 1.5]) == [0.5, 1.0, -0.5], \"failed on mixed weights\"\nassert linear_regression_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]], [1, 2, 3, 6]) == [0.0, 1.0, 2.0, 3.0], \"failed on 3-feature dataset\""}
{"task_id": 123, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\n\n    You should detect the unique categories in *y*, sort them in ascending/\n    alphabetical order, and then build a binary matrix where each row\n    corresponds to an element of *y* and each column corresponds to one of\n    the unique categories.  A cell must contain **1** if the row\u2019s element\n    belongs to the column\u2019s category and **0** otherwise.\n\n    Args:\n        y: A one-dimensional NumPy array (or array-like object) containing\n           categorical values.\n\n    Returns:\n        A Python list of lists with 0/1 integers representing the one-hot\n        encoded matrix.\n    \"\"\"\n    y = np.asarray(y)\n    unique = np.unique(y)\n    n_unique = len(unique)\n    category_to_index = {category: idx for (idx, category) in enumerate(unique)}\n    result = []\n    for element in y:\n        row = [0] * n_unique\n        row[category_to_index[element]] = 1\n        result.append(row)\n    return result\nassert one_hot_encoding(np.array([0, 1, 2])) == [[1, 0, 0], [0, 1, 0], [0, 0, 1]], \"Test case failed: integers [0,1,2]\"\nassert one_hot_encoding(np.array([2, 2, 1, 0])) == [[0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0]], \"Test case failed: integers with repetition [2,2,1,0]\"\nassert one_hot_encoding(np.array([-1, 0, -1])) == [[1, 0], [0, 1], [1, 0]], \"Test case failed: negative integers [-1,0,-1]\"\nassert one_hot_encoding(np.array([\"cat\", \"dog\", \"cat\"])) == [[1, 0], [0, 1], [1, 0]], \"Test case failed: strings ['cat','dog','cat']\"\nassert one_hot_encoding([3, 1, 3, 3]) == [[0, 1], [1, 0], [0, 1], [0, 1]], \"Test case failed: list [3,1,3,3]\"\nassert one_hot_encoding(np.array([42])) == [[1]], \"Test case failed: single element [42]\"\nassert one_hot_encoding(np.array([1, 1, 2, 2])) == [[1, 0], [1, 0], [0, 1], [0, 1]], \"Test case failed: sorted integers [1,1,2,2]\"\nlarge_input = np.arange(10)\nexpected_large = np.eye(10, dtype=int).tolist()\nassert one_hot_encoding(large_input) == expected_large, \"Test case failed: large range 0-9\"\nassert one_hot_encoding([\"a\", \"b\", \"c\", \"a\"]) == [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], \"Test case failed: list of strings ['a','b','c','a']\"\nassert one_hot_encoding(np.array([\"x\", \"z\", \"x\", \"y\"])) == [[1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]], \"Test case failed: strings ['x','z','x','y']\""}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef polynomial_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, d: int=3, gamma: float | None=None, c0: float=1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    C = X_np.shape[1]\n    if gamma is None:\n        gamma = 1.0 / C\n    dot_product = np.dot(X_np, Y_np.T)\n    scaled_dot = gamma * dot_product\n    scaled_dot_plus_c0 = scaled_dot + c0\n    kernel_matrix = scaled_dot_plus_c0 ** d\n    rounded_kernel = np.round(kernel_matrix, 4)\n    result = rounded_kernel.tolist()\n    return result\nassert polynomial_kernel([[1, 2], [3, 4]]) == [[42.875, 274.625], [274.625, 2460.375]], \"test failed: default parameters with 2\u00d72 input\"\nassert polynomial_kernel([[1, 0], [0, 1]], d=2, gamma=1, c0=0) == [[1.0, 0.0], [0.0, 1.0]], \"test failed: homogeneous degree-2 kernel on identity\"\nassert polynomial_kernel([[1, 2, 1]], [[0, 1, 0]], d=2, gamma=0.5, c0=1) == [[4.0]], \"test failed: explicit X, Y with gamma=.5\"\nassert polynomial_kernel([[2, 3]], [[2, 3]], d=1, gamma=1, c0=0) == [[13.0]], \"test failed: degree-1 (linear) kernel, c0=0\"\nassert polynomial_kernel([[2, 3]], [[2, 3]], d=1, gamma=1, c0=5) == [[18.0]], \"test failed: degree-1 with bias term\"\nassert polynomial_kernel([[1, 2], [3, 4]], [[5, 6], [7, 8]], d=2, gamma=1, c0=1) == [[324.0, 576.0], [1600.0, 2916.0]], \"test failed: 2\u00d72 vs 2\u00d72 different matrices\"\nassert polynomial_kernel([[1, 2, 3]], d=3, gamma=None, c0=0) == [[101.6296]], \"test failed: automatic gamma = 1/C\"\nassert polynomial_kernel([[0, 0]], [[0, 0]], d=3, gamma=2, c0=1) == [[1.0]], \"test failed: zero vectors\"\nassert polynomial_kernel([[1, 1]], [[-1, -1]], d=2, gamma=1, c0=0) == [[4.0]], \"test failed: negative correlation\"\nassert polynomial_kernel([[1, 2]], [[3, 4]], d=2, gamma=0.5, c0=0) == [[30.25]], \"test failed: gamma=0.5, degree=2\""}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to\n        the *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    all_labels = y_true + y_pred\n    unique_labels = sorted(list(set(all_labels)))\n    class_to_index = {label: idx for (idx, label) in enumerate(unique_labels)}\n    C = len(unique_labels)\n    matrix = [[0 for _ in range(C)] for _ in range(C)]\n    for (true, pred) in zip(y_true, y_pred):\n        row = class_to_index[true]\n        col = class_to_index[pred]\n        matrix[row][col] += 1\n    return matrix\nassert confusion_matrix(['cat','dog','cat'],['cat','cat','dog']) == [[1,1],[1,0]], \"failed on example with strings\"\nassert confusion_matrix([1,2,3,4],[4,3,2,1]) == [[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]], \"failed on reversed labels\"\nassert confusion_matrix([0,0,0,0],[0,0,0,0]) == [[4]], \"failed on single class perfect match\"\nassert confusion_matrix([0,1,1,1],[1,1,1,1]) == [[0,1],[0,3]], \"failed on partial mismatch\"\nassert confusion_matrix([],[]) == [], \"failed on empty lists\"\nassert confusion_matrix([1,2],[1]) == -1, \"failed on unequal length detection\"\nassert confusion_matrix([2,2,1,0],[2,1,1,0]) == [[1,0,0],[0,1,0],[0,1,1]], \"failed on unsorted class order\"\nassert confusion_matrix(['a','b','c'],['c','b','a']) == [[0,0,1],[0,1,0],[1,0,0]], \"failed on character labels\"\nassert confusion_matrix([True,False,True],[True,True,False]) == [[0,1],[1,1]], \"failed on boolean labels\""}
{"task_id": 129, "completion_id": 0, "passed": true, "result": "passed", "solution": "from itertools import islice, cycle\nfrom itertools import islice, cycle\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    it = cycle(sequence)\n    return list(islice(it, samples))\nassert cycle_sequence([1,2,3], 8) == [1,2,3,1,2,3,1,2], \"failed on cycle_sequence([1,2,3], 8)\"\nassert cycle_sequence(['x'], 4) == ['x','x','x','x'], \"failed on single-element sequence\"\nassert cycle_sequence([], 5) == [], \"failed on empty sequence\"\nassert cycle_sequence([0,1], 0) == [], \"failed on zero samples\"\nassert cycle_sequence([0,1], -3) == [], \"failed on negative samples\"\nassert cycle_sequence(['a','b','c'], 1) == ['a'], \"failed on one sample\"\nassert cycle_sequence(['a','b','c'], 2) == ['a','b'], \"failed on two samples\"\nassert cycle_sequence(['a','b','c'], 3) == ['a','b','c'], \"failed on exactly full length\"\nassert cycle_sequence(['a','b','c'], 4) == ['a','b','c','a'], \"failed on length+1 samples\"\nassert cycle_sequence([True, False], 7) == [True, False, True, False, True, False, True], \"failed on boolean sequence\""}
{"task_id": 130, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import Any, List, Tuple\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]], state: Any, action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    filtered = [t for t in transitions if t[0] == state and t[1] == action]\n    if not filtered:\n        return []\n    next_states = [t[3] for t in filtered]\n    counts = Counter(next_states)\n    total = len(next_states)\n    probabilities = []\n    for (ns, cnt) in counts.items():\n        prob = cnt / total\n        rounded_prob = round(prob, 4)\n        probabilities.append([ns, rounded_prob])\n    probabilities.sort(key=lambda x: x[0])\n    return probabilities\nassert outcome_probs([(0,'left',-1,1),(0,'left',-1,1),(0,'left',-1,2),(0,'right',0,3)],0,'left') == [[1,0.6667],[2,0.3333]], \"failed: basic two-outcome distribution\"\nassert outcome_probs([(1,'up',0,2),(1,'up',0,2),(1,'up',0,2)],1,'up') == [[2,1.0]], \"failed: single deterministic outcome\"\nassert outcome_probs([(0,'left',-1,1),(0,'right',0,3)],99,'none') == [], \"failed: unseen pair\"\nassert outcome_probs([(2,0,1,3),(2,0,0,3),(2,0,2,4)],2,0) == [[3,0.6667],[4,0.3333]], \"failed: numeric action\"\nassert outcome_probs([('s0','jump',0,'s1'),('s0','jump',0,'s2'),('s0','jump',0,'s1')],'s0','jump') == [['s1',0.6667],['s2',0.3333]], \"failed: string labels\"\nassert outcome_probs([(10,'a',0,i) for i in range(10)] + [(10,'a',0,0)],10,'a') == [[0,0.1818]] + [[i,0.0909] for i in range(1,10)], \"failed: many outcomes\"\nassert outcome_probs([(1.2,'go',0,2),(1.2,'go',0,2),(1.2,'go',0,3)],1.2,'go') == [[2,0.6667],[3,0.3333]], \"failed: float state\"\nassert outcome_probs([(1.2,'go',0,2)],1.2,'stop') == [], \"failed: unseen action\"\nassert outcome_probs([(0,'a',0,1)]*333 + [(0,'a',0,2)]*667,0,'a') == [[1,0.333],[2,0.667]], \"failed: rounding\"\nassert outcome_probs([(5,'x',0,7),(5,'x',0,6),(5,'x',0,7)],5,'x') == [[6,0.3333],[7,0.6667]], \"failed: sorting order\""}
{"task_id": 131, "completion_id": 0, "passed": false, "result": "failed: name '_SHARED_SEQUENCES' is not defined", "solution": "\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    sequence = _SHARED_SEQUENCES[uid]\n    if not sequence:\n        return None\n    seq_len = len(sequence)\n    if i >= 0:\n        normalized_i = i\n    else:\n        normalized_i = seq_len + i\n    if normalized_i < 0 or normalized_i >= seq_len:\n        return None\n    return sequence[normalized_i]\nassert get_index(1, 0) == 5, \"test case failed: get_index(1, 0)\"\nassert get_index(2, 0) is None, \"test case failed: get_index(2, 0) \u2013 empty sequence should return None\"\nassert get_index(1, -1) == 2, \"test case failed: get_index(1, -1)\"\nassert get_index(0, 3) is None, \"test case failed: get_index(0, 3) \u2013 out of range\"\n_SHARED_SEQUENCES.append([100, 200])\nassert get_index(3, 1) == 200, \"test case failed: get_index(3, 1) after append\"\n_SHARED_SEQUENCES[0] = ['a', 'b']\nassert get_index(0, 1) == 'b', \"test case failed: get_index(0, 1) after replacement\"\n_SHARED_SEQUENCES.append([])\nassert get_index(4, 0) is None, \"test case failed: get_index(4, 0) \u2013 newly appended empty sequence\""}
{"task_id": 132, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    if isinstance(z, (int, float)) and (not isinstance(z, bool)):\n        return z\n    elif isinstance(z, (list, tuple)):\n        return np.array(z)\n    elif isinstance(z, np.ndarray):\n        return z\n    else:\n        raise TypeError('Input must be a scalar, list, tuple, or ndarray.')\nassert linear(5) == 5, \"Failed on integer scalar 5\"\nassert linear(3.14159) == 3.14159, \"Failed on float scalar 3.14159\"\na = np.array([1, 2, 3]); assert linear(a) is a and np.array_equal(linear(a), np.array([1, 2, 3])), \"Failed on ndarray identity property\"\nassert np.array_equal(linear([4, 5, 6]), np.array([4.0, 5.0, 6.0])), \"Failed on list to ndarray conversion\"\nassert np.array_equal(linear([[1, 2], [3, 4]]), np.array([[1.0, 2.0], [3.0, 4.0]])), \"Failed on nested list conversion\"\nb = np.array([[7.5, -8.2]]); assert linear(b) is b, \"Failed on ndarray object identity (no copy)\"\nassert np.array_equal(linear((9, 10, 11)), np.array([9.0, 10.0, 11.0])), \"Failed on tuple to ndarray conversion\"\nc = np.arange(12).reshape(3, 4); assert linear(c) is c and np.array_equal(linear(c), c), \"Failed on multi-dimensional ndarray identity\"\nassert linear(-42) == -42, \"Failed on negative scalar\"\nd = np.zeros((2, 2, 2)); assert linear(d) is d and np.array_equal(linear(d), d), \"Failed on 3-D zeros ndarray\""}
{"task_id": 133, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    for sequence in X_train:\n        for token in sequence:\n            if token < 0 or token >= vocab_length:\n                return -1\n    Y_train = []\n    for sequence in X_train:\n        one_hot_sequence = []\n        for token in sequence:\n            hot_vector = [0] * vocab_length\n            hot_vector[token] = 1\n            one_hot_sequence.append(hot_vector)\n        Y_train.append(one_hot_sequence)\n    return Y_train\nassert one_hot_targets([[0, 2, 1], [1, 3, 2]], 4) == [[[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]], [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]], \"failed: basic example\"\nassert one_hot_targets([[3, 3, 3]], 4) == [[[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]]], \"failed: repeated tokens\"\nassert one_hot_targets([[0]], 1) == [[[1]]], \"failed: single token single vocab\"\nassert one_hot_targets([[1, 0], [0, 1]], 2) == [[[0, 1], [1, 0]], [[1, 0], [0, 1]]], \"failed: two-class alternating\"\nassert one_hot_targets([], 5) == [], \"failed: empty input list\"\nassert one_hot_targets([[0, 4]], 4) == -1, \"failed: index equal to vocab_length should be invalid\"\nassert one_hot_targets([[0, -1]], 3) == -1, \"failed: negative index should be invalid\"\nassert one_hot_targets([[2, 1, 0]], 3) == [[[0, 0, 1], [0, 1, 0], [1, 0, 0]]], \"failed: descending indices\"\nassert one_hot_targets([[1, 2, 1, 0]], 3) == [[[0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]], \"failed: longer single sequence\"\nassert one_hot_targets([[2, 2], [2, 2], [2, 2]], 3) == [[[0, 0, 1], [0, 0, 1]], [[0, 0, 1], [0, 0, 1]], [[0, 0, 1], [0, 0, 1]]], \"failed: all same tokens multiple samples\""}
{"task_id": 134, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        A list where each element is the success probability of the\n        corresponding arm. Probabilities must lie in the interval [0, 1].\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid probability (<0 or >1), the function must return\n        (-1.0, -1).\n    \"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    for (index, p) in enumerate(payoff_probs):\n        if p == max_p:\n            best_index = index\n            break\n    return (max_p, best_index)\nassert best_arm([0.15, 0.6, 0.35]) == (0.6, 1), \"failed on [0.15, 0.6, 0.35]\"\nassert best_arm([0.8, 0.8, 0.5]) == (0.8, 0), \"failed on tie with first max\"\nassert best_arm([0.0, 0.0, 0.0]) == (0.0, 0), \"failed on all zeros\"\nassert best_arm([1.0]) == (1.0, 0), \"failed on single arm\"\nassert best_arm([]) == (-1.0, -1), \"failed on empty list\"\nassert best_arm([0.2, -0.1, 0.3]) == (-1.0, -1), \"failed on probability < 0\"\nassert best_arm([0.2, 1.1, 0.3]) == (-1.0, -1), \"failed on probability > 1\"\nassert best_arm([0.333, 0.333, 0.3329]) == (0.333, 0), \"failed on close tie\"\nassert best_arm([0.25, 0.5, 0.75, 0.74]) == (0.75, 2), \"failed on normal case\"\nassert best_arm([0.9, 0.1]) == (0.9, 0), \"failed on two arms\""}
{"task_id": 136, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softmax(z: np.ndarray, axis: int=-1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Normalise by their sum along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: -1).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    max_val = z.max(axis=axis, keepdims=True)\n    shifted = z - max_val\n    exp_shifted = np.exp(shifted)\n    sum_exp = exp_shifted.sum(axis=axis, keepdims=True)\n    result = exp_shifted / sum_exp\n    rounded = np.round(result, 4)\n    return rounded.tolist()\nassert softmax(np.array([1, 2, 3])) == [0.09, 0.2447, 0.6652], \"test 1 failed: softmax([1,2,3])\"\nassert softmax(np.array([0, 0])) == [0.5, 0.5], \"test 2 failed: softmax([0,0])\"\nassert softmax(np.array([-1, -2, -3])) == [0.6652, 0.2447, 0.09], \"test 3 failed: softmax([-1,-2,-3])\"\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]])) == [[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]], \"test 4 failed: row-wise softmax\"\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]]), axis=0) == [[0.0474, 0.0474, 0.0474], [0.9526, 0.9526, 0.9526]], \"test 5 failed: column-wise softmax\"\nassert softmax(np.array([[1000, 1000], [1000, 1000]]), axis=1) == [[0.5, 0.5], [0.5, 0.5]], \"test 6 failed: large identical values\"\nassert softmax(np.array([1000, 1000])) == [0.5, 0.5], \"test 7 failed: vector of large identical values\"\nassert softmax(np.array([[1], [2]]), axis=0) == [[0.2689], [0.7311]], \"test 8 failed: column vector, axis=0\"\nassert softmax(np.array([[1], [2]]), axis=1) == [[1.0], [1.0]], \"test 9 failed: column vector, axis=1\"\nassert softmax(np.array([-1, 1])) == [0.1192, 0.8808], \"test 10 failed: softmax([-1,1])\""}
{"task_id": 138, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    n = len(y_true)\n    if n == 0:\n        return -1\n    correct = sum((yt == yp for (yt, yp) in zip(y_true, y_pred)))\n    accuracy = correct / n\n    return round(accuracy, 4)\nassert accuracy_score([1, 0, 1, 1], [1, 0, 0, 1]) == 0.75, \"test case failed: basic numerical example\"\nassert accuracy_score([1, 1, 1], [1, 1, 1]) == 1.0, \"test case failed: all correct predictions\"\nassert accuracy_score([0, 0, 0], [1, 1, 1]) == 0.0, \"test case failed: no correct predictions\"\nassert accuracy_score([\"cat\", \"dog\", \"cat\"], [\"cat\", \"cat\", \"cat\"]) == 0.6667, \"test case failed: string labels\"\nassert accuracy_score([True, False, True, False], [True, False, False, False]) == 0.75, \"test case failed: boolean labels\"\nassert accuracy_score([1.2, 3.4, 5.6], [1.2, 3.5, 5.6]) == 0.6667, \"test case failed: float labels\"\nassert accuracy_score([42], [42]) == 1.0, \"test case failed: single element correct\"\nassert accuracy_score([], []) == -1, \"test case failed: empty input sequences\"\nassert accuracy_score([1, 2, 3], [1, 2]) == -1, \"test case failed: different length sequences\"\nassert accuracy_score(list(\"abcdef\"), list(\"abcxyz\")) == 0.5, \"test case failed: half correct predictions\""}
{"task_id": 140, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import deque\nimport math\nfrom collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    if grid[0][0] == 'F' or grid[-1][-1] == 'F':\n        return -1\n    if n == 1:\n        return 0\n    fire_time = [[math.inf for _ in range(n)] for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        (i, j) = q.popleft()\n        for (dx, dy) in dirs:\n            (ni, nj) = (i + dx, j + dy)\n            if 0 <= ni < n and 0 <= nj < n:\n                if fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    visited = [[math.inf for _ in range(n)] for _ in range(n)]\n    visited[0][0] = 0\n    q = deque()\n    q.append((0, 0))\n    agent_dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        (x, y) = q.popleft()\n        current_time = visited[x][y]\n        if x == n - 1 and y == n - 1:\n            return current_time\n        for (dx, dy) in agent_dirs:\n            (nx, ny) = (x + dx, y + dy)\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] == '#':\n                    continue\n                if fire_time[nx][ny] <= current_time + 1:\n                    continue\n                if visited[nx][ny] > current_time + 1:\n                    visited[nx][ny] = current_time + 1\n                    q.append((nx, ny))\n    return -1\nassert escape_fire_maze([\".\"]) == 0, \"test case failed: grid=['.']\"\nassert escape_fire_maze([\"F\"]) == -1, \"test case failed: grid=['F']\"\nassert escape_fire_maze([\"..\", \"..\"]) == 2, \"test case failed: grid=['..','..']\"\nassert escape_fire_maze([\"F.\", \"..\"]) == -1, \"test case failed: grid=['F.','..']\"\nassert escape_fire_maze([\"..F\", \"...\", \"...\"]) == -1, \"test case failed: grid=['..F','...','...']\"\nassert escape_fire_maze([\"...\", \".F.\", \"...\"]) == -1, \"test case failed: grid=['...','.F.','...']\"\nassert escape_fire_maze([\"..\", \".F\"]) == -1, \"test case failed: grid=['..','.F']\"\nassert escape_fire_maze([\"...\", \"...\", \"...\"]) == 4, \"test case failed: grid=['...','...','...']\"\nassert escape_fire_maze([\"....\", \"....\", \"....\", \"....\"]) == 6, \"test case failed: grid=['....','....','....','....']\"\nassert escape_fire_maze([\"F..\", \".F.\", \"..F\"]) == -1, \"test case failed: grid=['F..','.F.','..F']\""}
{"task_id": 141, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, metric: str='euclidean') -> np.ndarray:\n    \"\"\"Implement the k-Nearest Neighbors classifier.\"\"\"\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    m = X_test.shape[0]\n    n = X_train.shape[0]\n    if metric == 'euclidean':\n        X_test_reshaped = X_test[:, np.newaxis]\n        X_train_reshaped = X_train[np.newaxis, :]\n        differences = X_test_reshaped - X_train_reshaped\n        squared_diff = differences ** 2\n        sum_squared = squared_diff.sum(axis=2)\n        distances = np.sqrt(sum_squared)\n    elif metric == 'manhattan':\n        X_test_reshaped = X_test[:, np.newaxis]\n        X_train_reshaped = X_train[np.newaxis, :]\n        differences = X_test_reshaped - X_train_reshaped\n        abs_diff = np.abs(differences)\n        sum_abs = abs_diff.sum(axis=2)\n        distances = sum_abs\n    elif metric == 'cosine':\n        dot_products = X_test @ X_train.T\n        norms_test = np.linalg.norm(X_test, axis=1)\n        norms_train = np.linalg.norm(X_train, axis=1)\n        cosine_similarities = dot_products / (norms_test[:, np.newaxis] * norms_train[np.newaxis, :])\n        distances = 1 - cosine_similarities\n    knn_indices = np.zeros((m, k), dtype=int)\n    for i in range(m):\n        dist_row = distances[i]\n        k_smallest = np.argpartition(dist_row, k - 1)[:k]\n        knn_indices[i] = k_smallest\n    y_knn = np.zeros(m, dtype=y_train.dtype)\n    for i in range(m):\n        indices = knn_indices[i]\n        y_group = y_train[indices]\n        (unique_labels, label_counts) = np.unique(y_group, return_counts=True)\n        max_count = np.max(label_counts)\n        candidates = unique_labels[label_counts == max_count]\n        predicted_label = np.min(candidates)\n        y_knn[i] = predicted_label\n    return y_knn\nassert knn_predict(np.array([[0, 0], [1, 1], [0, 1], [1, 0]]), np.array([0, 0, 1, 1]), np.array([[0.9, 0.9]]), 3, 'euclidean').tolist() == [1], 'Test 1 failed.'\nassert knn_predict(np.array([[0, 0], [1, 2], [3, 4]]), np.array([0, 1, 1]), np.array([[0, 1]]), 1, 'manhattan').tolist() == [0], 'Test 2 failed.'\nassert knn_predict(np.array([[1, 0], [0, 1]]), np.array([0, 1]), np.array([[1, 0.1]]), 1, 'cosine').tolist() == [0], 'Test 3 failed.'\nassert knn_predict(np.array([[0, 0], [0, 2]]), np.array([0, 1]), np.array([[0, 1]]), 2, 'euclidean').tolist() == [0], 'Test 4 failed.'\nassert knn_predict(np.array([[1, 1], [2, 2], [3, 3], [8, 8], [9, 9]]), np.array([0, 0, 0, 1, 1]), np.array([[1.5, 1.5], [9, 8.9]]), 3, 'euclidean').tolist() == [0, 1], 'Test 5 failed.'\nassert knn_predict(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 0, 0]), np.array([[2.1, 2]]), 1).tolist() == [0], 'Test 6 failed.'\nassert knn_predict(np.array([[0, 0], [0, 1], [1, 1]]), np.array([0, 0, 1]), np.array([[0.1, 0.1]]), 2, 'minkowski').tolist() == [0], 'Test 7 failed.'\nassert knn_predict(np.array([[0], [1], [2]]), np.array([0, 1, 1]), np.array([[1.1]]), 3, 'manhattan').tolist() == [1], 'Test 8 failed.'\nassert knn_predict(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 2]), np.array([[0.9, 0.05, 0.05]]), 1, 'cosine').tolist() == [0], 'Test 9 failed.'\nassert knn_predict(np.array([[-1, -1], [-2, -2], [1, 1], [2, 2]]), np.array([0, 0, 1, 1]), np.array([[-1.5, -1.5]]), 2, 'euclidean').tolist() == [0], 'Test 10 failed.'"}
{"task_id": 143, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef leaky_relu(x, alpha: float=0.2, derivative: bool=False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x)\n    if derivative:\n        return np.where(x >= 0, 1, alpha)\n    else:\n        return np.where(x >= 0, x, alpha * x)\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.1), np.array([-0.3, -0.1, 0., 1., 3.])), \"failed on mixed signs activation\"\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.05), np.array([-0.15, -0.05, 0., 1., 3.])), \"failed on custom alpha activation\"\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.1, derivative=True), np.array([0.1, 0.1, 1., 1., 1.])), \"failed on gradient computation\"\nassert leaky_relu(5) == 5, \"scalar positive input failed\"\nassert leaky_relu(-5, alpha=0.3) == -1.5, \"scalar negative input failed\"\nassert leaky_relu(-5, alpha=0.3, derivative=True) == 0.3, \"scalar gradient negative failed\"\nx = np.random.randn(1000)\nassert leaky_relu(x).shape == x.shape, \"shape mismatch on random vector\"\nmat = np.random.randn(5, 4, 3)\nassert leaky_relu(mat, derivative=True).shape == mat.shape, \"shape mismatch on tensor derivative\"\nzero_arr = np.zeros((10,))\nassert np.array_equal(leaky_relu(zero_arr), zero_arr), \"zeros activation failed\""}
{"task_id": 144, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise -1.\n    \"\"\"\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    if actual.shape != predicted.shape:\n        return -1\n    diff = np.abs(actual - predicted)\n    mae = np.mean(diff)\n    if np.isnan(mae):\n        return -1\n    return round(mae, 4)\nassert mean_absolute_error([3, -0.5, 2, 7], [2.5, 0.0, 2, 8]) == 0.5, \"failed: basic 1-D list input\"\nassert mean_absolute_error((1, 2, 3), (1, 2, 3)) == 0.0, \"failed: identical tuples must give 0\"\nassert mean_absolute_error(np.array([[1, 2], [3, 4]]), np.array([[2, 3], [4, 5]])) == 1.0, \"failed: 2-D numpy arrays\"\nassert mean_absolute_error([1, 2, 3], [1, 2]) == -1, \"failed: length mismatch expected \u20111\"\nassert mean_absolute_error([[1, 2, 3]], [[1, 2, 3]]) == 0.0, \"failed: single-row matrix\"\nassert mean_absolute_error([0.12345], [0.12344]) == 0.0, \"failed: rounding to 4 decimals\"\nassert mean_absolute_error([10, 20, 30], [0, 0, 0]) == 20.0, \"failed: large errors\"\nassert mean_absolute_error(np.linspace(0, 1, 5), np.full(5, 0.5)) == 0.3, \"failed: linspace vs constant\""}
{"task_id": 147, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, epochs: int=10000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * len(X_test)\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(epochs):\n        linear = np.dot(X_train, w) + b\n        h = sigmoid(linear)\n        gradient_w = X_train.T @ (h - y_train) / n_samples\n        gradient_b = (h - y_train).mean()\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n    linear_test = np.dot(X_test, w) + b\n    h_test = sigmoid(linear_test)\n    y_pred = (h_test >= 0.5).astype(int).tolist()\n    return y_pred\nassert logistic_regression(np.array([[-2,-1],[-3,-2],[-1,-2],[-2,-3],[2,1],[3,2],[1,2],[2,3]]), np.array([0,0,0,0,1,1,1,1]), np.array([[-2,-2],[2,2]])) == [0,1], \"test case failed: basic 2D separation\"\nassert logistic_regression(np.array([[-2],[-1.5],[-1],[5],[6],[7]]), np.array([0,0,0,1,1,1]), np.array([[-1.25],[6]])) == [0,1], \"test case failed: 1D simple\"\nassert logistic_regression(np.array([[-3,1],[-2,2],[-1,1],[4,1],[5,2],[6,2]]), np.array([0,0,0,1,1,1]), np.array([[-2,1],[5,1]])) == [0,1], \"test case failed: 2D x-axis split\"\nassert logistic_regression(np.array([[-4,0,1],[-3,1,0],[-2,1,1],[3,0,0],[4,1,1],[5,0,1]]), np.array([0,0,0,1,1,1]), np.array([[-3,0,1],[4,0,0]])) == [0,1], \"test case failed: 3D features\"\nassert logistic_regression(np.array([[-1,-1],[-2,-1],[-1,-2],[1,1],[2,1],[1,2]]), np.array([0,0,0,1,1,1]), np.array([[-1.5,-1.5],[1.5,1.5]])) == [0,1], \"test case failed: negative vs positive quadrant\"\nassert logistic_regression(np.array([[-5],[-4],[-3],[3],[4],[5]]), np.array([0,0,0,1,1,1]), np.array([[-4.5],[4.5]])) == [0,1], \"test case failed: symmetric 1D\"\nassert logistic_regression(np.array([[-3,0],[-2,-2],[-1,0],[1,1],[2,2],[3,1]]), np.array([0,0,0,1,1,1]), np.array([[-2,-1],[2,1]])) == [0,1], \"test case failed: diagonal separation\"\nassert logistic_regression(np.array([[-1,10],[-2,9],[-3,11],[4,10],[5,9],[6,11]]), np.array([0,0,0,1,1,1]), np.array([[-2,10],[5,10]])) == [0,1], \"test case failed: irrelevant second feature\"\nassert logistic_regression(np.array([[-20],[-15],[-10],[10],[15],[20]]), np.array([0,0,0,1,1,1]), np.array([[-12],[12]])) == [0,1], \"test case failed: large magnitude 1D\"\nassert logistic_regression(np.array([[-3,-2,0],[-2,-1,-1],[-4,-1,-2],[3,2,1],[2,1,1],[4,2,2]]), np.array([0,0,0,1,1,1]), np.array([[-3,-1,-1],[3,1,1]])) == [0,1], \"test case failed: sum of features\""}
{"task_id": 148, "completion_id": 0, "passed": true, "result": "passed", "solution": "from typing import List\nfrom typing import List\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    if not 0 <= alpha <= 1 or not data:\n        return -1\n    if alpha == 0:\n        first = round(data[0], 4)\n        return [first] * len(data)\n    elif alpha == 1:\n        return [round(x, 4) for x in data]\n    else:\n        n = len(data)\n        smoothed = [0.0] * n\n        smoothed[0] = data[0]\n        for k in range(1, n):\n            smoothed[k] = (1 - alpha) * smoothed[k - 1] + alpha * data[k]\n        return [round(x, 4) for x in smoothed]\nassert exp_smoothing([10, 20, 30, 25, 15], 0.1) == [10.0, 11.0, 12.9, 14.11, 14.199], \"failed on basic example\"\nassert exp_smoothing([5, 4, 3, 2, 1], 0.3) == [5.0, 4.7, 4.19, 3.533, 2.7731], \"failed on decreasing list\"\nassert exp_smoothing([1, 1, 1, 1], 0.25) == [1.0, 1.0, 1.0, 1.0], \"failed on constant list\"\nassert exp_smoothing([3, 6, 9], 0.5) == [3.0, 4.5, 6.75], \"failed on alpha 0.5\"\nassert exp_smoothing([7, 8, 9], 0) == [7.0, 7.0, 7.0], \"failed on alpha 0\"\nassert exp_smoothing([7, 8, 9], 1) == [7.0, 8.0, 9.0], \"failed on alpha 1\"\nassert exp_smoothing([], 0.3) == -1, \"failed on empty data list\"\nassert exp_smoothing([1, 2, 3], -0.1) == -1, \"failed on negative alpha\"\nassert exp_smoothing([1, 2, 3], 1.1) == -1, \"failed on alpha greater than 1\"\nassert exp_smoothing([100], 0.6) == [100.0], \"failed on single element list\""}
{"task_id": 152, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return -1\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\nassert accuracy_score([1, 0, 2, 2, 1], [1, 0, 1, 2, 0]) == 0.6, \"Test case failed: basic mixed correctness\"\nassert accuracy_score([0, 1, 2], [0, 1, 2]) == 1.0, \"Test case failed: perfect accuracy\"\nassert accuracy_score([1, 2, 3], [1, 2]) == -1, \"Test case failed: unequal length should return -1\"\nassert accuracy_score([0, 1, 1, 1], [1, 1, 1, 1]) == 0.75, \"Test case failed: one misclassification\"\nassert accuracy_score([1, 2, 3, 4], [4, 3, 2, 1]) == 0.0, \"Test case failed: zero accuracy\"\nassert accuracy_score([1], [1]) == 1.0, \"Test case failed: single element correct\"\nassert accuracy_score([0, 0, 0, 0, 0], [0, 1, 0, 1, 0]) == 0.6, \"Test case failed: majority correct\"\nimport numpy as np\nassert accuracy_score(np.array([1, 2, 1, 2]), np.array([1, 2, 2, 2])) == 0.75, \"Test case failed: numpy array input\"\nassert accuracy_score([], []) == -1, \"Test case failed: empty input\"\nassert accuracy_score([1, 1, 1, 0, 0], [1, 0, 1, 0, 0]) == 0.8, \"Test case failed: 80 percent accuracy\""}
{"task_id": 154, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float=0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    sum_X = np.sum(X ** 2, axis=1)\n    sum_Y = np.sum(Y ** 2, axis=1)\n    dot_product = X @ Y.T\n    distance_matrix = sum_X[:, np.newaxis] + sum_Y[np.newaxis, :] - 2 * dot_product\n    kernel_matrix = np.exp(-gamma * distance_matrix)\n    kernel_matrix_rounded = np.round(kernel_matrix, decimals=6)\n    return kernel_matrix_rounded.tolist()\nassert rbf_kernel(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [5, 5]]), 0.1) == [[1.0, 0.082085], [0.449329, 0.606531]], \"test case failed: example\"\nassert rbf_kernel(np.array([[1, 2, 3]]), np.array([[1, 2, 4]]), 0.5) == [[0.606531]], \"test case failed: single samples\"\nassert rbf_kernel(np.array([[1]]), np.array([[3]]), 0.0) == [[1.0]], \"test case failed: gamma 0\"\nassert rbf_kernel(np.array([[0, 0], [1, 1], [2, 2]]), np.array([[0, 0], [2, 2]]), 1.0) == [[1.0, 0.000335], [0.135335, 0.135335], [0.000335, 1.0]], \"test case failed: multiple samples\"\nassert rbf_kernel(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), 0.5) == [[1.0, 0.367879], [0.367879, 1.0]], \"test case failed: symmetric matrix\"\nassert rbf_kernel(np.array([[1, 2]]), np.array([[1, 2]]), 10.0) == [[1.0]], \"test case failed: large gamma\"\nassert rbf_kernel(np.array([[0, 0]]), np.array([[100, 100]]), 0.01) == [[0.0]], \"test case failed: distant points\"\nassert rbf_kernel(np.zeros((2, 2)), np.ones((2, 2)), 0.2) == [[0.67032, 0.67032], [0.67032, 0.67032]], \"test case failed: zeros vs ones\"\nassert rbf_kernel(np.array([[0], [1], [2]]), np.array([[1], [2]]), 0.5) == [[0.606531, 0.135335], [1.0, 0.606531], [0.606531, 1.0]], \"test case failed: 1-D features\"\nassert rbf_kernel(np.array([[1, 2, 3]]), np.array([[1, 2, 3]])) == [[1.0]], \"test case failed: default gamma\""}
{"task_id": 155, "completion_id": 0, "passed": false, "result": "failed: name 'weights_simple' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.asarray(state)\n    h1 = np.dot(state, weights['W1']) + weights['b1']\n    h1_relu = np.maximum(0, h1)\n    h2 = np.dot(h1_relu, weights['W2']) + weights['b2']\n    h2_relu = np.maximum(0, h2)\n    action = np.dot(h2_relu, weights['W3']) + weights['b3']\n    action_tanh = np.tanh(action)\n    rounded_action = np.round(action_tanh, 4).tolist()\n    return rounded_action\nassert actor_forward([0.5, -0.5], weights_simple) == [0.4621], \"test case failed: actor_forward([0.5,-0.5],weights_simple)\"\nassert actor_forward([2.0, 3.0], weights_simple) == [0.9999], \"test case failed: actor_forward([2,3],weights_simple)\"\nassert actor_forward([-2.0, 3.0], weights_simple) == [0.9951], \"test case failed: actor_forward([-2,3],weights_simple)\"\nassert actor_forward([1.0, 1.0], weights_simple) == [0.964], \"test case failed: actor_forward([1,1],weights_simple)\"\nassert actor_forward([0.0, 0.0], weights_simple) == [0.0], \"test case failed: actor_forward([0,0],weights_simple)\"\nassert actor_forward([-1.0, -1.0], weights_simple) == [0.0], \"test case failed: actor_forward([-1,-1],weights_simple)\"\nassert actor_forward([100.0, -100.0], weights_simple) == [1.0], \"test case failed: actor_forward([100,-100],weights_simple)\"\nassert actor_forward([-0.1, 0.1], weights_simple) == [0.0997], \"test case failed: actor_forward([-0.1,0.1],weights_simple)\"\nassert actor_forward([0.3, 0.3], weights_simple) == [0.537], \"test case failed: actor_forward([0.3,0.3],weights_simple)\"\nassert actor_forward([-0.3, 0.7], weights_simple) == [0.6044], \"test case failed: actor_forward([-0.3,0.7],weights_simple)\""}
{"task_id": 157, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return -1 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or -1 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0 and len(y_pred) == 0:\n        return 0.0\n    n = len(y_true)\n    matches = 0\n    for (yt, yp) in zip(y_true, y_pred):\n        if yt == yp:\n            matches += 1\n    accuracy = matches / n\n    return round(accuracy, 4)\nassert accuracy_score([1, 2, 3], [1, 2, 3]) == 1.0, \"test case failed: identical labels\"\nassert accuracy_score([1, 2, 3], [3, 2, 1]) == 0.3333, \"test case failed: one correct out of three\"\nassert accuracy_score([], []) == 0, \"test case failed: empty inputs\"\nassert accuracy_score([1], []) == -1, \"test case failed: length mismatch\"\nassert accuracy_score([0, 1, 2, 2, 1], [0, 0, 2, 2, 1]) == 0.8, \"test case failed: mixed correctness\"\nassert accuracy_score([\"cat\", \"dog\", \"cat\"], [\"cat\", \"cat\", \"cat\"]) == 0.6667, \"test case failed: string labels\"\nassert accuracy_score([True, False, True, False], [True, True, True, False]) == 0.75, \"test case failed: boolean labels\"\nassert accuracy_score(list(range(100)), list(range(100))) == 1.0, \"test case failed: large perfect match\"\nassert accuracy_score(list(range(100)), list(range(99)) + [101]) == 0.99, \"test case failed: one error in 100\"\nassert accuracy_score((1, 1, 1, 1), (1, 0, 1, 0)) == 0.5, \"test case failed: tuple input\""}
{"task_id": 158, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_regression(X: list[list[int | float]], y: list[int | float], alpha: float=1.0, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    if fit_intercept:\n        intercept = np.ones((X.shape[0], 1))\n        X = np.hstack([intercept, X])\n    Xt = X.T\n    XtX = Xt @ X\n    n = X.shape[1]\n    A = alpha * np.identity(n)\n    if fit_intercept:\n        A[0, 0] = 0\n    matrix = XtX + A\n    inv_matrix = np.linalg.inv(matrix)\n    beta = inv_matrix @ Xt @ y\n    rounded_beta = np.round(beta, 4)\n    return rounded_beta.tolist()\nassert ridge_regression([[1, 2], [3, 4], [5, 6]], [1, 2, 3], 1.0, True) == [0.3529, 0.2353, 0.2353], \"failed: example case\"\nassert ridge_regression([[1], [2], [3]], [1, 2, 3], 0.0, False) == [1.0], \"failed: simple OLS without intercept\"\nassert ridge_regression([[1], [2], [3]], [1, 2, 3], 1.0, False) == [0.9333], \"failed: ridge without intercept, \u03b1=1\"\nassert ridge_regression([[1, 0], [0, 1]], [1, 1], 1.0, True) == [1.0, 0.0, 0.0], \"failed: tiny matrix with intercept\"\nassert ridge_regression([[1, 2], [3, 4]], [5, 6], 0.0, False) == [-4.0, 4.5], \"failed: 2\u00d72 OLS\"\nassert ridge_regression([[1, 2], [3, 4]], [5, 6], 1.0, False) == [0.2, 1.4857], \"failed: 2\u00d72 ridge \u03b1=1\"\nassert ridge_regression([[1], [2]], [1, 1], 1.0, True) == [1.0, 0.0], \"failed: intercept only fit\"\nassert ridge_regression([[0], [0], [0]], [1, 1, 1], 1.0, True) == [1.0, 0.0], \"failed: zero feature with intercept\"\nassert ridge_regression([[1], [1]], [2, 4], 1.0, False) == [2.0], \"failed: single feature duplicates\"\nassert ridge_regression([[2, 0], [0, 2]], [1, 2], 0.0, False) == [0.5, 1.0], \"failed: diagonal X matrix\""}
{"task_id": 159, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = x_i.shape[0]\n    term1 = d * np.log(2 * np.pi)\n    log_det = np.linalg.slogdet(sigma)[1]\n    diff = x_i - mu\n    a = np.linalg.solve(sigma, diff)\n    quad_term = np.dot(diff, a)\n    sum_terms = term1 + log_det + quad_term\n    log_pdf = -0.5 * sum_terms\n    return round(log_pdf, 4)\nassert log_gaussian_pdf(np.array([0]), np.array([0]), np.array([[1]])) == -0.9189, \"failed on 1D standard normal at mean\"\nassert log_gaussian_pdf(np.array([1]), np.array([0]), np.array([[1]])) == -1.4189, \"failed on 1D standard normal at x=1\"\nassert log_gaussian_pdf(np.array([0,0]), np.array([0,0]), np.eye(2)) == -1.8379, \"failed on 2D standard normal at mean\"\nassert log_gaussian_pdf(np.array([1,1]), np.array([0,0]), np.eye(2)) == -2.8379, \"failed on 2D standard normal at [1,1]\"\nassert log_gaussian_pdf(np.array([2,0]), np.array([1,0]), np.eye(2)) == -2.3379, \"failed on 2D standard normal shifted point\"\nassert log_gaussian_pdf(np.array([0,0]), np.array([0,0]), np.diag([2,2])) == -2.5310, \"failed on 2D diagonal covariance det>1\"\nassert log_gaussian_pdf(np.array([1,0]), np.array([0,0]), np.diag([2,2])) == -2.7810, \"failed on 2D diagonal covariance at [1,0]\"\nassert log_gaussian_pdf(np.array([0,0,0]), np.array([0,0,0]), np.eye(3)) == -2.7568, \"failed on 3D standard normal at mean\"\nassert log_gaussian_pdf(np.array([0]), np.array([0]), np.array([[0.5]])) == -0.5724, \"failed on 1D variance 0.5 at mean\"\nassert log_gaussian_pdf(np.array([1]), np.array([0]), np.array([[0.5]])) == -1.5724, \"failed on 1D variance 0.5 at x=1\""}
{"task_id": 161, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary class labels using a Factorization Machine.\"\"\"\n    m = X.shape[0]\n    labels = []\n    V_squared = V ** 2\n    for x in X:\n        linear_part = w0 + np.dot(w, x)\n        xV = np.dot(x, V)\n        term1 = np.sum(xV ** 2)\n        x_squared = x ** 2\n        term2 = np.dot(x_squared, V_squared)\n        term2_total = np.sum(term2)\n        interaction_part = 0.5 * (term1 - term2_total)\n        score = linear_part + interaction_part\n        labels.append(1 if score >= 0 else -1)\n    return labels\nassert factorization_machine_predict(np.array([[1, 0], [0, 1]]), 0.0, np.array([-0.5, 0.5]), np.zeros((2, 1))) == [-1, 1], \"test case 1 failed\"\nassert factorization_machine_predict(np.array([[1, 1]]), 0.0, np.array([0.2, 0.2]), np.zeros((2, 1))) == [1], \"test case 2 failed\"\nassert factorization_machine_predict(np.array([[1, 0], [1, 1]]), -1.0, np.array([1.0, 0.0]), np.zeros((2, 1))) == [1, 1], \"test case 3 failed\"\nassert factorization_machine_predict(np.array([[1, 0, 1], [0, 1, 1]]), 0.5, np.array([0.2, -0.3, 0.8]), np.array([[0.1, 0.2], [0.0, -0.1], [0.5, 0.6]])) == [1, 1], \"test case 4 failed\"\nassert factorization_machine_predict(np.array([[1, 0, 1], [0, 1, 1]]), 0.0, np.zeros(3), np.array([[0.1, 0.2], [0.0, -0.1], [0.5, 0.6]])) == [1, -1], \"test case 5 failed\"\nassert factorization_machine_predict(np.array([[1, 1]]), 0.0, np.zeros(2), np.array([[1.0], [1.0]])) == [1], \"test case 6 failed\"\nassert factorization_machine_predict(np.array([[3, 4]]), 0.0, np.zeros(2), np.zeros((2, 1))) == [1], \"test case 7 failed\"\nassert factorization_machine_predict(np.array([[1, 2]]), -0.2, np.array([0.2, 0.1]), np.array([[0.1, 0.2], [0.3, 0.4]])) == [1], \"test case 8 failed\"\nassert factorization_machine_predict(np.array([[2, 1]]), 0.0, np.array([-0.4, -0.3]), np.zeros((2, 1))) == [-1], \"test case 9 failed\"\nassert factorization_machine_predict(np.array([[0, 0], [1, 0], [0, 1]]), 0.2, np.array([0.1, 0.1]), np.zeros((2, 1))) == [1, 1, 1], \"test case 10 failed\""}
{"task_id": 162, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float=1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, 'Shapes of Y_hat and Y must match.'\n    m = Y_hat.shape[0]\n    log_term = np.log(Y_hat + epsilon)\n    loss = -(1.0 / m) * np.sum(Y * log_term)\n    gradient = (Y_hat - Y) / m\n    rounded_loss = round(loss, 4)\n    rounded_gradient = np.round(gradient, 4).tolist()\n    return (rounded_loss, rounded_gradient)\nassert cross_entropy(np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1]]), np.array([[1, 0, 0], [0, 1, 0]])) == (0.2899, [[-0.15, 0.1, 0.05], [0.05, -0.1, 0.05]]), \"test case failed: example 1\"\nassert cross_entropy(np.array([[0.9, 0.1], [0.2, 0.8]]), np.array([[1, 0], [0, 1]])) == (0.1643, [[-0.05, 0.05], [0.1, -0.1]]), \"test case failed: example 2\"\nassert cross_entropy(np.array([[0.5, 0.5]]), np.array([[1, 0]])) == (0.6931, [[-0.5, 0.5]]), \"test case failed: example 3\"\nassert cross_entropy(np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]), np.array([[1, 0], [0, 1], [1, 0]])) == (0.0, [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]), \"test case failed: example 4\"\nassert cross_entropy(np.array([[0.2, 0.3, 0.5]]), np.array([[0, 0, 1]])) == (0.6931, [[0.2, 0.3, -0.5]]), \"test case failed: example 5\"\nassert cross_entropy(np.array([[0.0, 1.0]]), np.array([[1, 0]])) == (46.0517, [[-1.0, 1.0]]), \"test case failed: example 6\"\nassert cross_entropy(np.array([[0.25, 0.25, 0.25, 0.25], [0.1, 0.2, 0.3, 0.4]]), np.array([[0, 0, 0, 1], [0, 0, 1, 0]])) == (1.2951, [[0.125, 0.125, 0.125, -0.375], [0.05, 0.1, -0.35, 0.2]]), \"test case failed: example 7\"\nassert cross_entropy(np.array([[0.6, 0.4], [0.3, 0.7], [0.5, 0.5], [0.9, 0.1]]), np.array([[1, 0], [0, 1], [1, 0], [0, 1]])) == (0.9658, [[-0.1, 0.1], [0.075, -0.075], [-0.125, 0.125], [0.225, -0.225]]), \"test case failed: example 8\"\nassert cross_entropy(np.array([[1.0, 0.0]]), np.array([[1, 0]])) == (0.0, [[0.0, 0.0]]), \"test case failed: example 9\"\nassert cross_entropy(np.array([[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.2, 0.2, 0.6]]), np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])) == (0.646, [[0.0333, -0.0667, 0.0333], [-0.2333, 0.1333, 0.1], [0.0667, 0.0667, -0.1333]]), \"test case failed: example 10\""}
{"task_id": 163, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Fit an Elastic Net regression model and return the predictions for X.\n\n    Parameters\n    ----------\n    X : list[float]\n        One-dimensional input feature values.\n    y : list[float]\n        Target values corresponding to *X*.\n    degree : int\n        Highest polynomial degree to be used (degree \u2265 0).\n    reg_factor : float\n        Overall regularisation strength \u03bb (\u03bb \u2265 0).\n    l1_ratio : float\n        Mixing parameter \u03b1 between L1 and L2 penalty (0 \u2264 \u03b1 \u2264 1).\n    learning_rate : float\n        Step size for gradient descent (ignored when \u03b1 == 0).\n    n_iterations : int\n        Number of gradient-descent iterations (ignored when \u03b1 == 0).\n\n    Returns\n    -------\n    list[float]\n        Model predictions on the supplied *X*, rounded to 4 decimals.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    n = len(X_np)\n    d = degree\n    feature_matrix = np.zeros((n, d + 1))\n    for i in range(n):\n        x = X_np[i]\n        feature_matrix[i] = [x ** k for k in range(d + 1)]\n    if l1_ratio == 0:\n        m = feature_matrix.shape[1]\n        XTX = feature_matrix.T @ feature_matrix\n        penalty = np.zeros((m, m))\n        penalty[1:, 1:] = reg_factor * (1 - l1_ratio) * np.eye(m - 1)\n        XTX += penalty\n        XTy = feature_matrix.T @ y_np\n        \u03b2 = np.linalg.inv(XTX) @ XTy\n    else:\n        m = feature_matrix.shape[1]\n        \u03b2 = np.zeros(m)\n        for _ in range(n_iterations):\n            se = feature_matrix @ \u03b2 - y_np\n            gradient = np.zeros(m)\n            for j in range(m):\n                if j == 0:\n                    gradient_j = 2 / n * se.sum()\n                else:\n                    se_grad = 2 / n * np.dot(se, feature_matrix[:, j])\n                    if \u03b2[j] > 0:\n                        sign = 1\n                    elif \u03b2[j] < 0:\n                        sign = -1\n                    else:\n                        sign = 0\n                    pen_grad = reg_factor * ((1 - l1_ratio) * \u03b2[j] + l1_ratio * sign)\n                    gradient_j = se_grad + pen_grad\n                gradient[j] = gradient_j\n            \u03b2 -= learning_rate * gradient\n    y_pred = feature_matrix @ \u03b2\n    y_pred_list = [round(y, 4) for y in y_pred]\n    return y_pred_list\nassert elastic_net_regression([0, 1, 2, 3], [3, 5, 7, 9], 1, 0.0, 0.0, 0.01, 1000) == [3.0, 5.0, 7.0, 9.0], \"Test case 1 failed\"\nassert elastic_net_regression([0, 1, 2], [1, 3, 7], 2, 0.0, 0.0, 0.01, 1000) == [1.0, 3.0, 7.0], \"Test case 2 failed\"\nassert elastic_net_regression([-1, 0, 1], [1, 0, 1], 2, 0.0, 0.0, 0.01, 1000) == [1.0, 0.0, 1.0], \"Test case 3 failed\"\nassert elastic_net_regression([1, 2, 3], [6, 11, 18], 2, 0.0, 0.0, 0.01, 1000) == [6.0, 11.0, 18.0], \"Test case 4 failed\"\nassert elastic_net_regression([0.5, 1.5, 2.5], [1, 3, 5], 1, 0.0, 0.0, 0.01, 1000) == [1.0, 3.0, 5.0], \"Test case 5 failed\"\nassert elastic_net_regression([-1, 0, 1], [-1, 0, 1], 1, 0.0, 0.0, 0.01, 1000) == [-1.0, 0.0, 1.0], \"Test case 6 failed\"\nassert elastic_net_regression([0, 1, 2, 3], [5, 6, 7, 8], 1, 1.0, 0.0, 0.01, 1000) == [5.25, 6.0833, 6.9167, 7.75], \"Test case 7 failed\"\nassert elastic_net_regression([0, 1, 2], [1, 2, 3], 1, 0.5, 0.0, 0.01, 1000) == [1.2, 2.0, 2.8], \"Test case 8 failed\"\nassert elastic_net_regression([1, 3, 5], [2, 4, 6], 1, 2.0, 0.0, 0.01, 1000) == [2.4, 4.0, 5.6], \"Test case 9 failed\"\nassert elastic_net_regression([0, 1, 2], [5, 5, 5], 1, 0.0, 0.0, 0.01, 1000) == [5.0, 5.0, 5.0], \"Test case 10 failed\""}
{"task_id": 164, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_log_loss(actual, predicted):\n    \"\"\"Compute the binary classification log-loss (cross-entropy loss).\n    \n    Parameters\n    ----------\n    actual : list or array-like of shape (n_samples,)\n        Ground-truth labels, each element is 0 or 1.\n    predicted : list or array-like of shape (n_samples,)\n        Predicted probabilities, each element is a real number in [0, 1].\n    \n    Returns\n    -------\n    float\n        The average log-loss rounded to six decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    predicted_clipped = np.clip(predicted, \u03b5, 1 - \u03b5)\n    log_p = np.log(predicted_clipped)\n    log_1p = np.log(1 - predicted_clipped)\n    term1 = actual * log_p\n    term2 = (1 - actual) * log_1p\n    sum_terms = term1 + term2\n    total = np.sum(sum_terms)\n    N = len(actual)\n    average = total / N\n    log_loss = -average\n    return round(log_loss, 6)\nassert compute_log_loss([1, 0, 1, 1], [0.9, 0.2, 0.8, 0.7]) == 0.227081, \"test case failed: basic example\"\nassert compute_log_loss([1, 0, 1, 0], [1, 0, 1, 0]) == 0.0, \"test case failed: perfect predictions\"\nassert compute_log_loss([1, 1, 0, 0], [0.5, 0.5, 0.5, 0.5]) == 0.693147, \"test case failed: fifty-fifty predictions\"\nassert compute_log_loss([1], [0.01]) == 4.60517, \"test case failed: single poor prediction\"\nassert compute_log_loss([0, 0, 0], [0.1, 0.2, 0.3]) == 0.228393, \"test case failed: all zeros\"\nassert compute_log_loss([1, 1, 1], [0.9, 0.8, 0.7]) == 0.228393, \"test case failed: all ones\"\nassert compute_log_loss([1, 0, 1, 0, 1], [0.9, 0.1, 0.8, 0.2, 0.99]) == 0.133412, \"test case failed: mixed high confidence\"\nassert compute_log_loss([0], [0.999]) == 6.907755, \"test case failed: near-certain wrong prediction\"\nassert compute_log_loss([1], [1e-15]) == 34.538776, \"test case failed: clipped to EPS\"\nassert compute_log_loss([1, 0, 1, 0, 0, 1], [0.6, 0.4, 0.8, 0.3, 0.2, 0.9]) == 0.321662, \"test case failed: general mixture\""}
{"task_id": 165, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef hamming_distance(x: list[int] | 'np.ndarray', y: list[int] | 'np.ndarray') -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    N = len(x)\n    if N == 0:\n        return 0.0\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    diff = x != y\n    sum_diff = np.sum(diff)\n    distance = sum_diff / N\n    return round(distance, 4)\nassert hamming_distance([0, 1, 0, 1], [0, 1, 0, 1]) == 0.0, \"Failed identical vectors case\"\nassert hamming_distance([0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]) == 1.0, \"Failed all differ case\"\nassert hamming_distance([1, 0, 1, 1], [1, 1, 0, 1]) == 0.5, \"Failed example case\"\nassert hamming_distance([1, 2, 3], [1, 2]) == -1, \"Failed unequal length case\"\nassert hamming_distance([3, 5, 7, 9, 11], [3, 4, 7, 8, 11]) == 0.4, \"Failed mixed ints case\"\nimport numpy as np\narr1 = np.array([2, 2, 2, 2])\narr2 = np.array([2, 3, 2, 3])\nassert hamming_distance(arr1, arr2) == 0.5, \"Failed NumPy array input case\"\nassert hamming_distance([7], [8]) == 1.0, \"Failed single element different case\"\nassert hamming_distance([-1, -2, -3], [-1, -2, -4]) == 0.3333, \"Failed negative ints case\"\nassert hamming_distance([1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 0, 1, 0, 0, 1]) == 0.5, \"Failed additional binary case\""}
{"task_id": 171, "completion_id": 0, "passed": false, "result": "failed: only integer scalar arrays can be converted to a scalar index", "solution": "import numpy as np\nimport numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    unique_labels = sorted(list(set(y_train)))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    label_to_idx = {unique_labels[0]: 0, unique_labels[1]: 1}\n    y_train_mapped = [label_to_idx[label] for label in y_train]\n    X_train_np = np.array(X_train)\n    X_train_np = np.hstack([X_train_np, np.ones((X_train_np.shape[0], 1))])\n    X_test_np = np.array(X_test)\n    X_test_np = np.hstack([X_test_np, np.ones((X_test_np.shape[0], 1))])\n    n_features = X_train_np.shape[1]\n    w = np.zeros((n_features, 1))\n    for epoch in range(epochs):\n        indices = np.random.permutation(len(X_train_np))\n        X_shuffled = X_train_np[indices]\n        y_shuffled = np.array(y_train_mapped)[indices]\n        for i in range(0, len(X_shuffled), batch_size):\n            X_batch = X_shuffled[i:i + batch_size]\n            y_batch = y_shuffled[i:i + batch_size]\n            z = np.dot(X_batch, w)\n            a = 1 / (1 + np.exp(-z))\n            gradient = np.dot(X_batch.T, a - y_batch) / len(X_batch)\n            w = w - learning_rate * gradient\n    z = np.dot(X_test_np, w)\n    a = 1 / (1 + np.exp(-z))\n    predicted_classes = (a >= 0.5).astype(int)\n    original_predictions = [unique_labels[pred] for pred in predicted_classes]\n    return original_predictions\nassert logistic_regression_train_predict([[-3],[-2],[2],[3]],[0,0,1,1],[[-2.5],[2.5]]) == [0,1], \"failed on shifted 1D separation\"\nassert logistic_regression_train_predict([[1],[2],[3]],[0,0,0],[[10],[-10]]) == [0,0], \"failed on constant-zero label case\"\nassert logistic_regression_train_predict([[1],[2],[3]],[1,1,1],[[0],[4]]) == [1,1], \"failed on constant-one label case\"\nassert logistic_regression_train_predict([[1,1],[1,2],[2,3],[3,3]],[0,0,1,1],[[1,1.5],[3,4]]) == [0,1], \"failed on simple 2D separation\"\nassert logistic_regression_train_predict([[0,0,1],[1,1,1],[2,2,3],[3,3,3]],[0,0,1,1],[[0,0,0.5],[3,3,4]]) == [0,1], \"failed on 3D separation\"\nassert logistic_regression_train_predict([[1],[2],[3],[4]],[1,1,0,0],[[1.5],[3.5]]) == [1,0], \"failed on negative slope separation\"\nassert logistic_regression_train_predict([[0],[1],[10],[11]],[0,0,1,1],[[0.5],[10.5]]) == [0,1], \"failed on large gap separation\"\nassert logistic_regression_train_predict([[1,0],[0,1],[1,1],[2,2]],[0,0,1,1],[[0.2,0.2],[1.5,1.5]]) == [0,1], \"failed on mixed 2D separation\""}
{"task_id": 172, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        derivative = 1 - np.tanh(z) ** 2\n        return round(derivative, 4)\n    else:\n        z_array = np.array(z)\n        derivative = 1 - np.tanh(z_array) ** 2\n        rounded_derivative = np.round(derivative, 4)\n        return rounded_derivative.tolist()\nassert tanh_grad(0) == 1.0, \"test case failed: tanh_grad(0)\"\nassert tanh_grad(1) == 0.42, \"test case failed: tanh_grad(1)\"\nassert tanh_grad(-1) == 0.42, \"test case failed: tanh_grad(-1)\"\nassert tanh_grad(2) == 0.0707, \"test case failed: tanh_grad(2)\"\nassert tanh_grad(-2) == 0.0707, \"test case failed: tanh_grad(-2)\"\nassert tanh_grad(0.5) == 0.7864, \"test case failed: tanh_grad(0.5)\"\nassert tanh_grad([0, 1, -1, 2]) == [1.0, 0.42, 0.42, 0.0707], \"test case failed: tanh_grad([0, 1, -1, 2])\"\nassert tanh_grad(np.array([4])) == [0.0013], \"test case failed: tanh_grad(np.array([4]))\"\nassert tanh_grad(np.array([-4])) == [0.0013], \"test case failed: tanh_grad(np.array([-4]))\"\nassert tanh_grad(np.array([0.123])) == [0.985], \"test case failed: tanh_grad(np.array([0.123]))\""}
{"task_id": 173, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for (c, t) in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\nassert alphabetical_distance(\"HELLO\", \"WORLD\") == 42, \"Test 1 failed: alphabetical_distance('HELLO', 'WORLD')\"\nassert alphabetical_distance(\"ABC\", \"ABC\") == 0, \"Test 2 failed: alphabetical_distance('ABC', 'ABC')\"\nassert alphabetical_distance(\"aaaa\", \"bbbb\") == 4, \"Test 3 failed: alphabetical_distance('aaaa', 'bbbb')\"\nassert alphabetical_distance(\"abcd\", \"dcba\") == 8, \"Test 4 failed: alphabetical_distance('abcd', 'dcba')\"\nassert alphabetical_distance(\"HI\", \"HI\") == 0, \"Test 5 failed: alphabetical_distance('HI', 'HI')\"\nassert alphabetical_distance(\"AZ\", \"ZA\") == 50, \"Test 6 failed: alphabetical_distance('AZ', 'ZA')\"\nassert alphabetical_distance(\"\", \"\") == 0, \"Test 7 failed: alphabetical_distance('', '')\"\nassert alphabetical_distance(\"Genetic\", \"Algorithm\") == -1, \"Test 8 failed: alphabetical_distance('Genetic', 'Algorithm')\"\nassert alphabetical_distance(\"2023\", \"2024\") == 1, \"Test 9 failed: alphabetical_distance('2023', '2024')\"\nassert alphabetical_distance(\"Python3\", \"Python3\") == 0, \"Test 10 failed: alphabetical_distance('Python3', 'Python3')\""}
{"task_id": 174, "completion_id": 0, "passed": false, "result": "failed: test case failed: perfect separation", "solution": "import numpy as np\nimport numpy as np\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Soft-max probabilities with shape (n_samples, n_classes).\n    labels : np.ndarray\n        One-hot encoded true labels with the same shape as *predictions*.\n\n    Returns\n    -------\n    tuple[float, float]\n        (loss, accuracy) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    loss = -np.mean(labels * np.log(predictions))\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(labels, axis=1)\n    correct = np.sum(predicted_classes == true_classes)\n    accuracy = correct / predictions.shape[0]\n    loss_rounded = round(loss, 4)\n    accuracy_rounded = round(accuracy, 4)\n    return (loss_rounded, accuracy_rounded)\nassert gan_discriminator_metrics(np.array([[0.9,0.1],[0.1,0.9]]), np.array([[1,0],[0,1]])) == (0.1054,1.0), \"test case failed: perfect separation\"\nassert gan_discriminator_metrics(np.array([[0.7,0.3],[0.6,0.4],[0.2,0.8]]), np.array([[1,0],[1,0],[0,1]])) == (0.3635,1.0), \"test case failed: mixed high confidence\"\nassert gan_discriminator_metrics(np.array([[0.4,0.6],[0.4,0.6]]), np.array([[1,0],[0,1]])) == (0.7136,0.5), \"test case failed: one correct, one wrong\"\nassert gan_discriminator_metrics(np.array([[0.5,0.5]]), np.array([[1,0]])) == (0.6931,1.0), \"test case failed: tie handled by argmax\"\nassert gan_discriminator_metrics(np.array([[0.2,0.8],[0.7,0.3]]), np.array([[1,0],[0,1]])) == (1.4067,0.0), \"test case failed: all misclassified\"\nassert gan_discriminator_metrics(np.array([[0.8,0.2],[0.8,0.2],[0.8,0.2],[0.8,0.2]]), np.array([[1,0],[1,0],[1,0],[1,0]])) == (0.2231,1.0), \"test case failed: uniform batch\"\nassert gan_discriminator_metrics(np.array([[0.1,0.7,0.2],[0.25,0.25,0.5]]), np.array([[0,1,0],[0,0,1]])) == (0.5249,1.0), \"test case failed: 3-class perfect\"\nassert gan_discriminator_metrics(np.array([[0.33,0.33,0.34]]), np.array([[1,0,0]])) == (1.1087,0.0), \"test case failed: wrong prediction with 3 classes\"\nassert gan_discriminator_metrics(np.array([[0.0,1.0]]), np.array([[0,1]])) == (0.0,1.0), \"test case failed: clipping at boundaries\"\nassert gan_discriminator_metrics(np.array([[0.6,0.4],[0.3,0.7],[0.8,0.2],[0.1,0.9],[0.55,0.45]]), np.array([[1,0],[0,1],[1,0],[0,1],[1,0]])) == (0.3588,1.0), \"test case failed: mixed confident batch\""}
{"task_id": 175, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    weights = np.array(weights)\n    sum_squares = np.sum(weights ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)\nassert l2_penalty([1, 2, 3], 1) == 14.0, \"failed: l2_penalty([1, 2, 3], 1)\"\nassert l2_penalty(np.array([1, -2, 3]), 0.5) == 7.0, \"failed: l2_penalty(np.array([1,-2,3]),0.5)\"\nassert l2_penalty([], 2) == 0.0, \"failed: l2_penalty([],2)\"\nassert l2_penalty([0, 0, 0], 1.5) == 0.0, \"failed: l2_penalty([0,0,0],1.5)\"\nassert l2_penalty([1.1111, 2.2222], 0.3) == round(0.3 * ((1.1111**2) + (2.2222**2)), 4), \"failed: l2_penalty([1.1111,2.2222],0.3)\"\nassert l2_penalty(np.array([5]), 0.1) == 2.5, \"failed: l2_penalty([5],0.1)\"\nassert l2_penalty(np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]), 0.05) == round(0.05 * np.sum(np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]) ** 2), 4), \"failed: mixed range\"\nassert l2_penalty([10, 20, 30], 0.0) == 0.0, \"failed: zero C\"\nassert l2_penalty([1e-3, -1e-3], 1) == round(((1e-3)**2 + (1e-3)**2), 4), \"failed: small weights\"\nassert l2_penalty([3.1415, 2.7182], 0.25) == round(0.25 * ((3.1415**2) + (2.7182**2)), 4), \"failed: l2_penalty([pi,e],0.25)\""}
{"task_id": 178, "completion_id": 0, "passed": false, "result": "failed: test failed: particle_swarm_optimisation(10,60,300,7)", "solution": "import numpy as np\nimport numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    rng = np.random.default_rng(seed)\n    positions = rng.uniform(-1, 1, (num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    pbest = positions.copy()\n    initial_spheres = np.array([np.sum(p ** 2) for p in pbest])\n    initial_best_idx = np.argmin(initial_spheres)\n    gbest = pbest[initial_best_idx].copy()\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            r1 = rng.random()\n            r2 = rng.random()\n            velocities[i] = w * velocities[i] + c1 * r1 * (pbest[i] - positions[i]) + c2 * r2 * (gbest - positions[i])\n            new_position = positions[i] + velocities[i]\n            new_position_clipped = np.clip(new_position, -1, 1)\n            positions[i] = new_position_clipped\n            new_sphere = np.sum(new_position_clipped ** 2)\n            current_pbest_sphere = np.sum(pbest[i] ** 2)\n            if new_sphere < current_pbest_sphere:\n                pbest[i] = new_position_clipped\n        pbest_spheres = np.array([np.sum(p ** 2) for p in pbest])\n        best_pbest_idx = np.argmin(pbest_spheres)\n        new_gbest = pbest[best_pbest_idx]\n        new_gbest_sphere = np.sum(new_gbest ** 2)\n        current_gbest_sphere = np.sum(gbest ** 2)\n        if new_gbest_sphere < current_gbest_sphere:\n            gbest = new_gbest\n    best_value = np.sum(gbest ** 2)\n    return round(best_value, 4)\nassert particle_swarm_optimisation(2, 30, 100, seed=42) == 0.0, \"test failed: particle_swarm_optimisation(2,30,100,42)\"\nassert particle_swarm_optimisation(5, 50, 200, seed=3) == 0.0, \"test failed: particle_swarm_optimisation(5,50,200,3)\"\nassert particle_swarm_optimisation(10, 60, 300, seed=7) == 0.0, \"test failed: particle_swarm_optimisation(10,60,300,7)\"\nassert particle_swarm_optimisation(3, 10, 150, seed=11) == 0.0, \"test failed: particle_swarm_optimisation(3,10,150,11)\"\nassert particle_swarm_optimisation(4, 80, 250, seed=19) == 0.0, \"test failed: particle_swarm_optimisation(4,80,250,19)\"\nassert particle_swarm_optimisation(6, 40, 120, seed=23) == 0.0, \"test failed: particle_swarm_optimisation(6,40,120,23)\"\nassert particle_swarm_optimisation(1, 20, 90, seed=29) == 0.0, \"test failed: particle_swarm_optimisation(1,20,90,29)\"\nassert particle_swarm_optimisation(8, 70, 300, seed=31) == 0.0, \"test failed: particle_swarm_optimisation(8,70,300,31)\"\nassert particle_swarm_optimisation(2, 1, 1, seed=2) == 0.0, \"test failed: particle_swarm_optimisation(2,1,1,2)\"\nassert particle_swarm_optimisation(-1, 30, 100) == -1, \"test failed: invalid input not handled\""}
{"task_id": 179, "completion_id": 0, "passed": true, "result": "passed", "solution": "import re\nimport string\nimport re\nimport string\ndef tokenize_chars(line: str, lowercase: bool=True, filter_punctuation: bool=True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    if lowercase:\n        line = line.lower()\n    if filter_punctuation:\n        punct_pattern = re.escape(string.punctuation)\n        line = re.sub('[' + punct_pattern + ']', '', line)\n    line = re.sub(' {2,}', ' ', line)\n    line = line.strip()\n    if not line:\n        return []\n    return list(line)\nassert tokenize_chars(\"Hello, World!\") == ['h','e','l','l','o',' ','w','o','r','l','d'], \"failed: default params\"\nassert tokenize_chars(\"Hello, World!\", lowercase=False, filter_punctuation=True) == ['H','e','l','l','o',' ','W','o','r','l','d'], \"failed: keep case\"\nassert tokenize_chars(\"Hello, World!\", lowercase=False, filter_punctuation=False) == ['H','e','l','l','o',',',' ','W','o','r','l','d','!'], \"failed: keep case & punctuation\"\nassert tokenize_chars(\"  Hi   there  \") == ['h','i',' ','t','h','e','r','e'], \"failed: extra spaces\"\nassert tokenize_chars(\"!!!\") == [], \"failed: only punctuation removed\"\nassert tokenize_chars(\"!!!\", filter_punctuation=False) == ['!','!','!'], \"failed: keep punctuation only\"\nassert tokenize_chars(\"\") == [], \"failed: empty string\"\nassert tokenize_chars(None) == [], \"failed: None input\"\nassert tokenize_chars(\"A.B,C\", filter_punctuation=True) == ['a','b','c'], \"failed: punctuation removal with dots and commas\"\nassert tokenize_chars(\"MixED CaSe\") == ['m','i','x','e','d',' ','c','a','s','e'], \"failed: lowercasing mixed case\""}
{"task_id": 180, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    predictions = []\n    n_train = len(X_train)\n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    for test_point in X_test:\n        distances = []\n        for (xi, yi) in zip(X_train, y_train):\n            dist_sq = sum(((test_point[i] - xi[i]) ** 2 for i in range(len(test_point))))\n            distances.append((dist_sq, yi))\n        distances.sort(key=lambda x: x[0])\n        selected = distances[:effective_k]\n        y_neighbors = [y for (d, y) in selected]\n        if task == 'classification':\n            counter = Counter(y_neighbors)\n            max_count = max(counter.values())\n            candidates = [label for (label, count) in counter.items() if count == max_count]\n            predicted_label = min(candidates)\n            predictions.append(predicted_label)\n        elif task == 'regression':\n            average = sum(y_neighbors) / len(y_neighbors)\n            predictions.append(round(average, 4))\n        else:\n            raise ValueError(\"Task must be 'classification' or 'regression'.\")\n    return predictions\nassert knn_predict([[0,0],[1,1],[2,2]], [0,0,1], [[1,1]], 2, \"classification\") == [0], \"failed: basic classification\"\nassert knn_predict([[0],[1],[2]], [1.0,1.5,3.0], [[1]], 2, \"regression\") == [1.25], \"failed: basic regression\"\nassert knn_predict([[0,0],[3,3]], [\"A\",\"B\"], [[1,1]], 5, \"classification\") == [\"A\"], \"failed: k larger than samples\"\nassert knn_predict([[0,0],[2,0],[0,2],[2,2]], [1,2,2,3], [[1,1]], 0, \"regression\") == [2.0], \"failed: k == 0 (use all)\"\nassert knn_predict([[0],[1],[2],[3]], [1,2,2,3], [[1.5]], 3, \"classification\") == [2], \"failed: tie vote numeric\"\nassert knn_predict([[0],[10]], [5.5555, 5.5555], [[5]], 1, \"regression\") == [5.5555], \"failed: rounding unchanged\"\nassert knn_predict([[1,2],[2,3],[3,4]], [10,20,30], [[2,3]], 2, \"regression\") == [15.0], \"failed: regression average\"\nassert knn_predict([[0,0],[1,1],[1,-1]], [\"yes\",\"no\",\"no\"], [[1,0]], 2, \"classification\") == [\"no\"], \"failed: majority vote\"\nassert knn_predict([[0,0],[0,0],[1,1]], [1,1,2], [[0,0]], 2, \"classification\") == [1], \"failed: duplicate points\""}
{"task_id": 182, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"Compute the polynomial kernel matrix for given X, Y, and degree.\n    \n    Parameters:\n        X (list or ndarray): A 2D array of shape (n, p).\n        Y (list or ndarray): A 2D array of shape (m, p).\n        degree (int, optional): The polynomial degree. Default is 2.\n        \n    Returns:\n        list: A nested list representing the kernel matrix of shape (n, m), with each element rounded to four decimal places.\n        \n    Raises:\n        -1: If any input is invalid.\n    \"\"\"\n    try:\n        X = np.array(X)\n        Y = np.array(Y)\n    except:\n        return -1\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    if not isinstance(degree, int) or degree < 1:\n        return -1\n    K = (X @ Y.T) ** degree\n    K_rounded = np.round(K, 4)\n    return K_rounded.tolist()\nassert polynomial_kernel([[1, 2], [3, 4]], [[5, 6], [7, 8]], 2) == [[289.0, 529.0], [1521.0, 2809.0]], \"failed: basic 2\u00d72 degree-2 example\"\nassert polynomial_kernel([[1, 0], [0, 1]], [[1, 0], [0, 1]], 1) == [[1.0, 0.0], [0.0, 1.0]], \"failed: identity dot product degree 1\"\nassert polynomial_kernel([[1, 2, 3]], [[4, 5, 6]], 3) == [[32768.0]], \"failed: single row, degree 3\"\nassert polynomial_kernel([[0.5, 1.5]], [[2.0, 3.0], [1.0, 1.0]], 2) == [[30.25, 4.0]], \"failed: float inputs\"\nassert polynomial_kernel([[1, 2]], [[3, 4, 5]], 2) == -1, \"failed: mismatched feature dimensions should return -1\"\nassert polynomial_kernel([[1, 2]], [[3, 4]], 0) == -1, \"failed: degree 0 should return -1\"\nassert polynomial_kernel([[1, -1], [2, -2]], [[3, -3], [4, -4]], 2) == [[36.0, 64.0], [144.0, 256.0]], \"failed: negatives and degree 2\"\nassert polynomial_kernel([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], 1) == [[50.0], [122.0]], \"failed: rectangular result\"\nassert polynomial_kernel([[1, 2]], [[3, 4]], 1) == [[11.0]], \"failed: simple dot, degree 1\""}
{"task_id": 183, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError('Invalid weight shape. Must be 2D or 4D.')\n    limit = np.sqrt(6.0 / fan_in)\n    weights = np.random.uniform(-limit, limit, weight_shape)\n    return weights\nw = he_uniform((2, 3)); assert w.shape == (2, 3) and np.all(w >= -np.sqrt(6/2)) and np.all(w <= np.sqrt(6/2)), \"test case failed: he_uniform((2, 3))\"\nw = he_uniform((5, 10)); assert w.shape == (5, 10) and np.all(w >= -np.sqrt(6/5)) and np.all(w <= np.sqrt(6/5)), \"test case failed: he_uniform((5, 10))\"\nw = he_uniform((3, 3, 1, 32)); assert w.shape == (3, 3, 1, 32) and np.all(w >= -np.sqrt(6/9)) and np.all(w <= np.sqrt(6/9)), \"test case failed: he_uniform((3, 3, 1, 32))\"\nw = he_uniform((5, 5, 16, 32)); assert w.shape == (5, 5, 16, 32) and np.all(w >= -np.sqrt(6/400)) and np.all(w <= np.sqrt(6/400)), \"test case failed: he_uniform((5, 5, 16, 32))\"\nw = he_uniform((1, 1, 64, 128)); assert w.shape == (1, 1, 64, 128) and np.all(w >= -np.sqrt(6/64)) and np.all(w <= np.sqrt(6/64)), \"test case failed: he_uniform((1, 1, 64, 128))\"\nw = he_uniform((10, 1)); assert w.shape == (10, 1) and np.all(w >= -np.sqrt(6/10)) and np.all(w <= np.sqrt(6/10)), \"test case failed: he_uniform((10, 1))\"\nw = he_uniform((7, 7, 3, 64)); assert w.shape == (7, 7, 3, 64) and np.all(w >= -np.sqrt(6/147)) and np.all(w <= np.sqrt(6/147)), \"test case failed: he_uniform((7, 7, 3, 64))\"\nw = he_uniform((128, 256)); assert w.shape == (128, 256) and np.all(w >= -np.sqrt(6/128)) and np.all(w <= np.sqrt(6/128)), \"test case failed: he_uniform((128, 256))\"\nw = he_uniform((4, 4)); assert w.shape == (4, 4) and np.all(w >= -np.sqrt(6/4)) and np.all(w <= np.sqrt(6/4)), \"test case failed: he_uniform((4, 4))\"\nw = he_uniform((3, 3, 3, 3)); assert w.shape == (3, 3, 3, 3) and np.all(w >= -np.sqrt(6/27)) and np.all(w <= np.sqrt(6/27)), \"test case failed: he_uniform((3, 3, 3, 3))\""}
{"task_id": 184, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n\n    def gini(y):\n        count_0 = np.sum(y == 0)\n        count_1 = np.sum(y == 1)\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p0 = count_0 / n\n        p1 = count_1 / n\n        return 1 - (p0 ** 2 + p1 ** 2)\n\n    def build_tree(X, y, current_depth, max_depth):\n        if len(X) == 0:\n            return {'is_leaf': True, 'prediction': 0}\n        if len(np.unique(y)) == 1:\n            return {'is_leaf': True, 'prediction': y[0]}\n        if max_depth is not None and current_depth >= max_depth:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_X = None\n        best_left_y = None\n        best_right_X = None\n        best_right_y = None\n        n_features = X.shape[1]\n        for feature in range(n_features):\n            column = X[:, feature]\n            sorted_col = np.sort(column)\n            unique = np.unique(sorted_col)\n            if len(unique) == 1:\n                continue\n            for i in range(len(unique) - 1):\n                threshold = (unique[i] + unique[i + 1]) / 2\n                left_mask = column <= threshold\n                left_X = X[left_mask]\n                left_y = y[left_mask]\n                right_X = X[~left_mask]\n                right_y = y[~left_mask]\n                G_parent = gini(y)\n                G_left = gini(left_y) if len(left_y) > 0 else 0\n                G_right = gini(right_y) if len(right_y) > 0 else 0\n                n_total = len(y)\n                n_left = len(left_y)\n                n_right = len(right_y)\n                if n_total == 0:\n                    continue\n                weighted_G = n_left / n_total * G_left + n_right / n_total * G_right\n                gain = G_parent - weighted_G\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_left_X = left_X\n                    best_left_y = left_y\n                    best_right_X = right_X\n                    best_right_y = right_y\n        if best_gain > 0:\n            left_child = None\n            right_child = None\n            if len(best_left_X) == 0:\n                majority = np.bincount(y).argmax()\n                left_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                left_child = build_tree(best_left_X, best_left_y, current_depth + 1, max_depth)\n            if len(best_right_X) == 0:\n                majority = np.bincount(y).argmax()\n                right_child = {'is_leaf': True, 'prediction': majority}\n            else:\n                right_child = build_tree(best_right_X, best_right_y, current_depth + 1, max_depth)\n            return {'is_leaf': False, 'feature': best_feature, 'threshold': best_threshold, 'left': left_child, 'right': right_child}\n        else:\n            majority = np.bincount(y).argmax()\n            return {'is_leaf': True, 'prediction': majority}\n\n    def predict_sample(sample, tree):\n        if tree['is_leaf']:\n            return tree['prediction']\n        else:\n            feature = tree['feature']\n            threshold = tree['threshold']\n            if sample[feature] <= threshold:\n                return predict_sample(sample, tree['left'])\n            else:\n                return predict_sample(sample, tree['right'])\n    tree = build_tree(X_train, y_train, 0, max_depth)\n    predictions = []\n    for sample in X_test:\n        predictions.append(predict_sample(sample, tree))\n    return predictions\nassert decision_tree_predict(np.array([[1],[2],[3],[4],[5],[6]]), np.array([0,0,0,1,1,1]), np.array([[1.5],[3.5],[5.5]])) == [0,0,1], \"failed: simple 1-D split\"\nassert decision_tree_predict(np.array([[0,0],[1,1],[2,2],[9,9],[10,10],[11,11]]), np.array([0,0,0,1,1,1]), np.array([[1,1],[10,10]])) == [0,1], \"failed: two-dimensional clearly separated\"\nassert decision_tree_predict(np.array([[1],[2],[3],[10],[11],[12],[20],[21],[22]]), np.array([0,0,0,1,1,1,2,2,2]), np.array([[2.5],[11.5],[21]])) == [0,1,2], \"failed: three-class 1-D split\"\nassert decision_tree_predict(np.array([[1],[2],[3]]), np.array([1,1,1]), np.array([[0],[5]])) == [1,1], \"failed: all labels identical\"\nassert decision_tree_predict(np.array([[0.1],[0.2],[0.3],[0.4],[0.5],[0.6]]), np.array([0,0,0,1,1,1]), np.array([[0.25],[0.55]])) == [0,1], \"failed: threshold around 0.35\"\nassert decision_tree_predict(np.array([[5.1,3.5,1.4],[4.9,3.0,1.4],[6.2,3.4,5.4],[5.9,3.0,5.1]]), np.array([0,0,1,1]), np.array([[5.0,3.4,1.5],[6.0,3.1,5.0]])) == [0,1], \"failed: example in task description\"\nassert decision_tree_predict(np.array([[0,2],[1,2],[2,2],[0,10],[1,10],[2,10]]), np.array([0,0,0,1,1,1]), np.array([[0,3],[0,9]])) == [0,1], \"failed: split on second feature\"\nassert decision_tree_predict(np.array([[1],[2],[3],[4],[5]]), np.array([0,0,0,1,1]), np.array([[1.5],[4.5]])) == [0,1], \"failed: odd number of samples\"\nassert decision_tree_predict(np.array([[1],[1],[2],[2],[3],[3],[10],[10],[11],[11]]), np.array([0,0,0,0,0,0,1,1,1,1]), np.array([[1],[10],[3]])) == [0,1,0], \"failed: duplicates in features\"\nassert decision_tree_predict(np.array([[0],[5],[10],[15]]), np.array([0,1,2,3]), np.array([[12],[1]])) == [2,0], \"failed: multi-class, arbitrary values\""}
{"task_id": 186, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Computes the arithmetic mean of the training targets and returns this mean\n    for each test sample, rounded to four decimal places.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places.\n    \"\"\"\n    y_mean = np.mean(y_train)\n    m_samples = len(X_test)\n    if m_samples == 0:\n        return []\n    predictions = np.full(m_samples, y_mean)\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()\nassert baseline_mean_predict(np.array([[1],[2],[3]]), np.array([2, 4, 6]), np.array([[4],[5]])) == [4.0, 4.0], \"test case failed: basic integer example\"\nassert baseline_mean_predict([[1],[2],[3],[4]], [1, 1, 1, 1], [[3],[4]]) == [1.0, 1.0], \"test case failed: identical targets\"\nassert baseline_mean_predict([[1],[2],[3]], [1.5, 2.5, 4.0], [[0],[1],[2]]) == [2.6667, 2.6667, 2.6667], \"test case failed: mean needs rounding\"\nassert baseline_mean_predict([], [1.1111, 2.2222, 3.3333], [[1],[2]]) == [2.2222, 2.2222], \"test case failed: 4-decimal precision targets\"\nassert baseline_mean_predict([[1],[2],[3]], [-1, -2, -3], [[10]]) == [-2.0], \"test case failed: negative targets\"\nassert baseline_mean_predict([[1],[2],[3]], [5, 6, 7], np.empty((0, 1))) == [], \"test case failed: empty X_test\"\nassert baseline_mean_predict([[42]], [10], [[4],[5],[6]]) == [10.0, 10.0, 10.0], \"test case failed: single training sample\"\nassert baseline_mean_predict([[1,2,3],[4,5,6]], [0, 10], [[7,8,9],[9,8,7]]) == [5.0, 5.0], \"test case failed: multi-feature X_test\"\nassert baseline_mean_predict([[1],[2]], [3, 5], [[9],[9],[9]]) == [4.0, 4.0, 4.0], \"test case failed: pure python lists\"\nassert baseline_mean_predict(np.random.rand(1000, 5), list(range(1,1001)), np.zeros((5,5))) == [500.5]*5, \"test case failed: large dataset\""}
{"task_id": 188, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    if max_epsilon <= min_epsilon:\n        return -1\n    if min_epsilon < 0:\n        return -1\n    if decay_rate <= 0:\n        return -1\n    if step < 0:\n        return -1\n    exponent = -decay_rate * step\n    exponential = math.exp(exponent)\n    term = (max_epsilon - min_epsilon) * exponential\n    epsilon = min_epsilon + term\n    epsilon = max(epsilon, min_epsilon)\n    epsilon_rounded = round(epsilon, 4)\n    return epsilon_rounded\nassert epsilon_decay(0.9, 0.1, 0.005, 0) == 0.9, \"failed: step 0 should return max_epsilon\"\nassert epsilon_decay(0.9, 0.1, 0.005, 100) == 0.5852, \"failed: step 100, \u03b5 should be 0.5852\"\nassert epsilon_decay(0.9, 0.1, 0.005, 200) == 0.3943, \"failed: step 200, \u03b5 should be 0.3943\"\nassert epsilon_decay(0.9, 0.1, 0.005, 500) == 0.1657, \"failed: step 500, \u03b5 should be 0.1657\"\nassert epsilon_decay(0.9, 0.1, 0.005, 1000) == 0.1054, \"failed: step 1000, \u03b5 should be 0.1054\"\nassert epsilon_decay(0.9, 0.1, 0.005, 10000) == 0.1, \"failed: \u03b5 must never drop below \u03b5_min\"\nassert epsilon_decay(0.5, 0.5, 0.01, 10) == -1, \"failed: \u03b5_max equal to \u03b5_min should be invalid\"\nassert epsilon_decay(0.5, 0.6, 0.01, 10) == -1, \"failed: \u03b5_max smaller than \u03b5_min should be invalid\"\nassert epsilon_decay(0.9, 0.1, -0.01, 10) == -1, \"failed: negative decay_rate should be invalid\"\nassert epsilon_decay(0.9, 0.1, 0.005, -5) == -1, \"failed: negative step should be invalid\""}
{"task_id": 193, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    The function accepts either a scalar or a list/NumPy array. For a scalar it\n    returns a float; for an array-like input it returns a list of floats, with\n    every element rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, (list, np.ndarray)):\n        arr = np.array(z)\n        result = arr / (1 + np.abs(arr))\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n    else:\n        result = z / (1 + abs(z))\n        rounded = round(result, 4)\n        return rounded\nassert softsign(3) == 0.75, \"test case failed: softsign(3)\"\nassert softsign(-4) == -0.8, \"test case failed: softsign(-4)\"\nassert softsign(0) == 0.0, \"test case failed: softsign(0)\"\nassert softsign([1, -2, 0]) == [0.5, -0.6667, 0.0], \"test case failed: softsign([1, -2, 0])\"\nassert softsign(np.array([10, -10])) == [0.9091, -0.9091], \"test case failed: softsign(np.array([10, -10]))\"\nassert softsign([]) == [], \"test case failed: softsign([])\"\nassert softsign(np.array([])).__eq__([]), \"test case failed: softsign(np.array([]))\"\nassert softsign([0.5]) == [0.3333], \"test case failed: softsign([0.5])\"\nassert softsign(np.array([1000])) == [0.999], \"test case failed: softsign(np.array([1000]))\"\nassert softsign([-1, -2, -3]) == [-0.5, -0.6667, -0.75], \"test case failed: softsign([-1, -2, -3])\""}
{"task_id": 194, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adagrad_update(weights: list[float], gradients: list[float], G: list[float] | None=None, learning_rate: float=0.01, eps: float=1e-08) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss with respect to *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple  (new_weights, new_G) where\n            new_weights \u2013 list of updated parameters rounded to 6 decimals,\n            new_G       \u2013 list of updated accumulated squared gradients\n                           rounded to 6 decimals.\n    \"\"\"\n    if G is None:\n        new_G = [0.0 for _ in weights]\n    else:\n        new_G = list(G)\n    for i in range(len(weights)):\n        new_G[i] += gradients[i] ** 2\n    new_weights = []\n    for i in range(len(weights)):\n        denom = np.sqrt(new_G[i] + eps)\n        update = learning_rate * gradients[i] / denom\n        new_weight = weights[i] - update\n        new_weights.append(round(new_weight, 6))\n    new_G_rounded = [round(g, 6) for g in new_G]\n    return (new_weights, new_G_rounded)\nassert adagrad_update([1.0], [0.1], None, 0.1) == ([0.9], [0.01]), \"test 1 failed: first single-dim step\"\nassert adagrad_update([3.0, 4.0], [0.1, -0.2], None, 0.1) == ([2.9, 4.1], [0.01, 0.04]), \"test 2 failed: example from description\"\nassert adagrad_update([2.0], [-0.5], None, 0.05) == ([2.05], [0.25]), \"test 3 failed: negative gradient single-dim\"\nassert adagrad_update([1.5, -2.0], [0.2, 0.1], [0.04, 0.01], 0.1) == ([1.429289, -2.070711], [0.08, 0.02]), \"test 4 failed: update with existing G\"\nassert adagrad_update([2.0, -3.5], [0.0, 0.0], [0.01, 0.04], 0.1) == ([2.0, -3.5], [0.01, 0.04]), \"test 5 failed: zero gradient should keep weights\"\nassert adagrad_update([0.9], [0.05], [0.01], 0.1) == ([0.855279], [0.0125]), \"test 6 failed: second single-dim step\"\nassert adagrad_update([1.0, 2.0, 3.0], [1.0, -1.0, 0.5], None, 0.05) == ([0.95, 2.05, 2.95], [1.0, 1.0, 0.25]), \"test 7 failed: three-dim first step\"\nassert adagrad_update([10.0, -10.0], [1.0, 1.0], [100.0, 100.0], 1.0) == ([9.900496, -10.099504], [101.0, 101.0]), \"test 8 failed: large prior G\"\nassert adagrad_update([0.0, 0.0], [-2.0, -2.0], None, 0.1) == ([0.1, 0.1], [4.0, 4.0]), \"test 9 failed: negative gradient both dims\"\nassert adagrad_update([0.95, 2.05, 2.95], [0.1, 0.1, 0.1], [1.0, 1.0, 0.25], 0.05) == ([0.945025, 2.045025, 2.940194], [1.01, 1.01, 0.26]), \"test 10 failed: three-dim second step\""}
{"task_id": 198, "completion_id": 0, "passed": false, "result": "failed: name 'phi1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    if not phi:\n        return [[0.0 for _ in range(phi[0].shape[1])] for _ in range(V)]\n    T = phi[0].shape[1]\n    beta = np.zeros((V, T), dtype=np.float64)\n    for d in range(len(phi)):\n        current_phi = phi[d]\n        current_corpus = corpus[d]\n        for n in range(current_phi.shape[0]):\n            v = current_corpus[n]\n            beta[v] += current_phi[n, :]\n    for t in range(T):\n        sum_t = beta[:, t].sum()\n        if sum_t == 0:\n            beta[:, t] = 0.0\n        else:\n            beta[:, t] /= sum_t\n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\nassert update_beta(phi1, corpus1, 3) == [[0.3684, 0.0968], [0.4737, 0.6774], [0.1579, 0.2258]], \"failed: test case 1\"\nassert update_beta(phi2, corpus2, 2) == [[0.6667], [0.3333]], \"failed: test case 2\"\nassert update_beta(phi3, corpus3, 3) == [[0.1429, 0.5], [0.3571, 0.3125], [0.5, 0.1875]], \"failed: test case 3\"\nassert update_beta(phi4, corpus4, 2) == [[1.0, 0.0], [0.0, 1.0]], \"failed: test case 4\"\nassert update_beta(phi5, corpus5, 2) == [[0.3333, 0.3333], [0.6667, 0.6667]], \"failed: test case 5\"\nassert update_beta(phi6, corpus6, 3) == [[0.9474, 0.0952, 0.0952], [0.0263, 0.8571, 0.0476], [0.0263, 0.0476, 0.8571]], \"failed: test case 6\"\nassert update_beta(phi7, corpus7, 1) == [[1.0, 1.0]], \"failed: test case 7\"\nassert update_beta(phi8, corpus8, 4) == [[0.0, 0.0], [0.3333, 0.6364], [0.6667, 0.3636], [0.0, 0.0]], \"failed: test case 8\"\nassert update_beta(phi9, corpus9, 2) == [[0.5, 0.5], [0.5, 0.5]], \"failed: test case 9\"\nassert update_beta(phi10, corpus10, 2) == [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], \"failed: test case 10\""}
{"task_id": 199, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\nassert split(np.array([2,3,1,5,4]), np.array([0,1,0,1,0]), 3) == ([0,0],[1,1,0]), \"failed: basic split\"\nassert split(np.array([1,2,3,4]), np.array([10,20,30,40]), 4) == ([10,20,30],[40]), \"failed: threshold at max element\"\nassert split(np.array([1,2,3,4]), np.array([10,20,30,40]), 0) == ([],[10,20,30,40]), \"failed: threshold below min\"\nassert split(np.array([1,1,1,1]), np.array([7,8,9,10]), 1) == ([],[7,8,9,10]), \"failed: all equal to threshold\"\nassert split(np.array([1,1,1,1]), np.array([7,8,9,10]), 2) == ([7,8,9,10],[]), \"failed: all below threshold\"\nassert split(np.array([-5,-2,0,3]), np.array([5,4,3,2]), 0) == ([5,4],[3,2]), \"failed: negative values\"\nassert split(np.array([0.1,0.2,0.3]), np.array([1,2,3]), 0.25) == ([1,2],[3]), \"failed: float threshold\"\nassert split(np.array([5]), np.array([42]), 5) == ([],[42]), \"failed: single element equal threshold\"\nassert split(np.array([5]), np.array([42]), 10) == ([42],[]), \"failed: single element below threshold\"\nassert split(np.array([5]), np.array([42]), 0) == ([],[42]), \"failed: single element above threshold\""}
{"task_id": 200, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    weights_array = np.asarray(weights)\n    flattened = weights_array.flatten()\n    sum_abs = np.sum(np.abs(flattened))\n    l1 = C * sum_abs\n    return round(l1, 4)\nassert compute_l1_penalty([1.5, -2.0, 0.0, 3.2], 0.1) == 0.67, \"failed on simple 1-D list\"\nassert compute_l1_penalty([[1, -2], [3, -4]], 0.25) == 2.5, \"failed on 2-D list\"\nassert compute_l1_penalty([], 1.0) == 0.0, \"failed on empty list\"\nassert compute_l1_penalty([0, 0, 0], 10.0) == 0.0, \"failed on all zeros\"\nassert compute_l1_penalty([-3.3, 4.4], 1.0) == 7.7, \"failed on negative values\"\nassert compute_l1_penalty(np.array([0.3333, -0.6666]), 3) == 2.9997, \"failed on numpy 1-D array\"\nassert compute_l1_penalty(np.array([[1.1, -1.1], [2.2, -2.2]]), 0.5) == 3.3, \"failed on numpy 2-D array\"\nassert compute_l1_penalty([1]*1000, 0.001) == 1.0, \"failed on large list\"\nassert compute_l1_penalty([[[]]], 5) == 0.0, \"failed on deeply nested empty list\"\nassert compute_l1_penalty([[0.12345]], 2) == 0.2469, \"failed on rounding rule\""}
{"task_id": 204, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or ``NaN``.\n\n    Args:\n        data: Any 1-D iterable (list, tuple, NumPy array, etc.).\n\n    Returns:\n        int: The number of missing elements.\n    \"\"\"\n    count = 0\n    for item in data:\n        if item is None:\n            count += 1\n        elif isinstance(item, (float, np.floating)) and math.isnan(item):\n            count += 1\n    return count\nassert num_missing([1, 2, None, 4]) == 1, \"failed on [1, 2, None, 4]\"\nassert num_missing([np.nan, np.nan]) == 2, \"failed on two nans\"\nassert num_missing([]) == 0, \"failed on empty list\"\nassert num_missing([None, np.nan, 5]) == 2, \"failed on mixed missing\"\nassert num_missing(list(range(10))) == 0, \"failed on no missing\"\nassert num_missing([None, None, None]) == 3, \"failed on all missing\"\nassert num_missing(np.array([1.0, np.nan, 2.0])) == 1, \"failed on numpy array\"\nassert num_missing([float('nan'), 'nan', None]) == 2, \"failed on string 'nan' handling\"\nassert num_missing([True, False, None]) == 1, \"failed on boolean values\"\nassert num_missing([3.0, 4.5, 6.7]) == 0, \"failed on all valid numbers\""}
{"task_id": 205, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []\n    seen = set()\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                unique_ids.append(env_id)\n                seen.add(env_id)\n    return unique_ids\nassert get_gym_environs([]) == [], \"test case failed: empty list\"\nassert get_gym_environs([{\"id\": \"CartPole-v1\"}]) == [\"CartPole-v1\"], \"test case failed: single element\"\nassert get_gym_environs([{\"id\": \"A\"}, {\"id\": \"B\"}, {\"id\": \"A\"}]) == [\"A\", \"B\"], \"test case failed: duplicates must be removed\"\nassert get_gym_environs([{\"name\": \"NoId\"}, {\"id\": \"X\"}]) == [\"X\"], \"test case failed: items without 'id' key are ignored\"\nassert get_gym_environs([{\"id\": \"X\"}, 42, \"string\", (1, 2), {\"id\": \"Y\"}]) == [\"X\", \"Y\"], \"test case failed: non-dict elements should be ignored\"\nassert get_gym_environs([{\"id\": \"D\"}, {\"id\": \"C\"}, {\"id\": \"B\"}, {\"id\": \"A\"}, {\"id\": \"C\"}, {\"id\": \"D\"}]) == [\"D\", \"C\", \"B\", \"A\"], \"test case failed: preserve order of first occurrence\"\nassert get_gym_environs([{\"id\": 1}, {\"id\": 2}, {\"id\": 1}]) == [1, 2], \"test case failed: ids can be non-string\"\nassert get_gym_environs([{\"id\": \"same\"}, {\"id\": \"same\"}, {\"id\": \"same\"}]) == [\"same\"], \"test case failed: all ids identical\"\nassert get_gym_environs([{\"id\": \"X\"}, {\"name\": \"n/a\"}, {\"id\": \"Y\"}, {\"foo\": \"bar\"}, {\"id\": \"X\"}]) == [\"X\", \"Y\"], \"test case failed: mixed valid and invalid elements\"\nassert get_gym_environs([{\"id\": \"first\"}, {\"ignore\": \"me\"}, {\"id\": \"second\"}, {\"id\": \"third\"}, {\"id\": \"second\"}]) == [\"first\", \"second\", \"third\"], \"test case failed: general behaviour\""}
{"task_id": 207, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef convolution_shape(img_height: int, img_width: int, filter_shape: tuple[int, int], stride: tuple[int, int], padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height of the input image.\n        img_width (int):  Width of the input image.\n        filter_shape (tuple[int, int]): (kernel_height, kernel_width).\n        stride (tuple[int, int]):       (stride_height, stride_width).\n        padding (tuple[int, int]):      (pad_height, pad_width).\n\n    Returns:\n        tuple[int, int] | int: (output_height, output_width) if the parameters\n            yield a valid integer output size; otherwise -1.\n    \"\"\"\n    (KH, KW) = filter_shape\n    (SH, SW) = stride\n    (PH, PW) = padding\n    if img_height <= 0 or img_width <= 0 or KH <= 0 or (KW <= 0) or (SH <= 0) or (SW <= 0) or (PH < 0) or (PW < 0):\n        return -1\n    num_h = img_height + 2 * PH - KH\n    num_w = img_width + 2 * PW - KW\n    if num_h < 0 or num_w < 0:\n        return -1\n    if num_h % SH != 0 or num_w % SW != 0:\n        return -1\n    OH = num_h // SH + 1\n    OW = num_w // SW + 1\n    if OH <= 0 or OW <= 0:\n        return -1\n    return (OH, OW)\nassert convolution_shape(32, 32, (3, 3), (1, 1), (1, 1)) == (32, 32), \"failed: case (32,32),(3,3),(1,1),(1,1)\"\nassert convolution_shape(32, 32, (5, 5), (1, 1), (0, 0)) == (28, 28), \"failed: case (32,32),(5,5),(1,1),(0,0)\"\nassert convolution_shape(64, 64, (7, 7), (1, 1), (3, 3)) == (64, 64), \"failed: case (64,64),(7,7),(1,1),(3,3)\"\nassert convolution_shape(32, 32, (4, 4), (2, 2), (1, 1)) == (16, 16), \"failed: case (32,32),(4,4),(2,2),(1,1)\"\nassert convolution_shape(28, 28, (3, 3), (2, 2), (0, 0)) == -1, \"failed: invalid divisibility (28,28),(3,3),(2,2),(0,0)\"\nassert convolution_shape(28, 28, (3, 3), (2, 2), (1, 1)) == -1, \"failed: invalid divisibility (28,28),(3,3),(2,2),(1,1)\"\nassert convolution_shape(10, 10, (11, 11), (1, 1), (0, 0)) == -1, \"failed: kernel larger than input (10,10),(11,11)\"\nassert convolution_shape(227, 227, (11, 11), (4, 4), (0, 0)) == (55, 55), \"failed: case (227,227),(11,11),(4,4),(0,0)\"\nassert convolution_shape(32, 32, (3, 3), (0, 1), (1, 1)) == -1, \"failed: stride height zero invalid (32,32),(3,3),(0,1),(1,1)\""}
{"task_id": 208, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef one_hot_encode(labels: list[str], categories: list[str] | None=None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: A list of categorical labels of length *N*.\n        categories: Optional complete list of unique categories of length *C*.\n            If *None*, the set of categories is inferred from *labels* in the\n            order of their first appearance.\n\n    Returns:\n        A list of lists that represents the one-hot encoded matrix of shape\n        (N, C) where *C* is the number of categories. If an unknown label is\n        found when *categories* is supplied, the function returns -1.\n    \"\"\"\n    if categories is None:\n        categories = []\n        for label in labels:\n            if label not in categories:\n                categories.append(label)\n    else:\n        label_to_index = {label: idx for (idx, label) in enumerate(categories)}\n        for label in labels:\n            if label not in label_to_index:\n                return -1\n    label_to_index = {label: idx for (idx, label) in enumerate(categories)}\n    n = len(labels)\n    c = len(categories)\n    matrix = np.zeros((n, c), dtype=int)\n    for i in range(n):\n        label = labels[i]\n        idx = label_to_index[label]\n        matrix[i, idx] = 1\n    return matrix.tolist()\nassert one_hot_encode(['red','green','blue','green'], ['red','green','blue']) == [[1,0,0],[0,1,0],[0,0,1],[0,1,0]], \"Test case failed: predefined categories\"\nassert one_hot_encode(['cat','dog','cat'], None) == [[1,0],[0,1],[1,0]], \"Test case failed: inferred categories\"\nassert one_hot_encode(['sun','moon','star'], ['sun','moon','star','cloud']) == [[1,0,0,0],[0,1,0,0],[0,0,1,0]], \"Test case failed: extra unused category\"\nassert one_hot_encode(['a','b','c','a'], ['a','b','c']) == [[1,0,0],[0,1,0],[0,0,1],[1,0,0]], \"Test case failed: repeated labels\"\nassert one_hot_encode(['apple','banana'], ['banana','apple']) == [[0,1],[1,0]], \"Test case failed: category order\"\nassert one_hot_encode(['x','y'], ['x','y','z']) == [[1,0,0],[0,1,0]], \"Test case failed: missing label category present\"\nassert one_hot_encode(['alpha','beta'], ['alpha','beta','gamma','delta']) == [[1,0,0,0],[0,1,0,0]], \"Test case failed: larger category list\"\nassert one_hot_encode(['dog'], None) == [[1]], \"Test case failed: single label\"\nassert one_hot_encode(['unknown'], ['a','b','c']) == -1, \"Test case failed: unknown label should return -1\""}
{"task_id": 211, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n    mismatches = sum((1 for (a, p) in zip(actual, predicted) if a != p))\n    total = len(actual)\n    acc = 1.0 - mismatches / total\n    return round(acc, 4)\nassert accuracy([1, 0, 1, 1], [1, 1, 1, 0]) == 0.5, \"failed: accuracy([1, 0, 1, 1], [1, 1, 1, 0])\"\nassert accuracy(['cat', 'dog', 'bird'], ['cat', 'dog', 'bird']) == 1.0, \"failed: perfect prediction\"\nassert accuracy([1, 1, 1], [0, 0, 0]) == 0.0, \"failed: completely wrong prediction\"\nassert accuracy([1, 2], [1]) == -1, \"failed: different lengths should give -1\"\nassert accuracy([], []) == -1, \"failed: empty lists should give -1\"\nassert accuracy([True, False, True, False], [True, False, False, False]) == 0.75, \"failed: boolean labels\"\nassert accuracy(['a', 'b', 'c', 'd'], ['a', 'x', 'c', 'y']) == 0.5, \"failed: string labels\"\nassert accuracy([0, 1, 2, 3, 4], [0, 2, 2, 3, 5]) == 0.6, \"failed: mixed correct/incorrect\"\nassert accuracy([1, 2, 3, 4, 5], [5, 4, 3, 2, 1]) == 0.2, \"failed: reversed list\"\nassert accuracy([0], [1]) == 0.0, \"failed: single element wrong\""}
{"task_id": 212, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import deque\nfrom collections import deque\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Find a shortest path from 'input' to 'output_realtime'.\n\n    The *model* is given as an adjacency list `graph` where every key is the\n    name of a layer and the associated list contains the names of its outgoing\n    neighbours.\n\n    The function must return the names of the layers that lie on ONE shortest\n    path starting at the layer named ``\"input\"`` and ending at the layer named\n    ``\"output_realtime\"``. If several shortest paths are possible the\n    lexicographically smallest one (as complete lists) is returned.  When no\n    path exists an empty list is returned.\n\n    Parameters\n    ----------\n    graph : dict[str, list[str]]\n        Adjacency list of the directed graph.\n\n    Returns\n    -------\n    list[str]\n        Ordered list of layer names on the desired path or an empty list.\n    \"\"\"\n    if 'input' not in graph or 'output_realtime' not in graph:\n        return []\n    best = {}\n    best['input'] = (0, ['input'])\n    queue = deque()\n    queue.append(('input', ['input']))\n    while queue:\n        (current_node, current_path) = queue.popleft()\n        current_distance = len(current_path) - 1\n        if best.get(current_node, (float('inf'), []))[1] != current_path:\n            continue\n        for neighbor in graph.get(current_node, []):\n            new_distance = current_distance + 1\n            new_path = current_path + [neighbor]\n            if neighbor not in best:\n                best[neighbor] = (new_distance, new_path)\n                queue.append((neighbor, new_path))\n            else:\n                (existing_distance, existing_path) = best[neighbor]\n                if new_distance < existing_distance:\n                    best[neighbor] = (new_distance, new_path)\n                    queue.append((neighbor, new_path))\n                elif new_distance == existing_distance:\n                    if new_path < existing_path:\n                        best[neighbor] = (new_distance, new_path)\n                        queue.append((neighbor, new_path))\n    if 'output_realtime' in best:\n        return best['output_realtime'][1]\n    else:\n        return []\nassert extract_realtime_model({\"input\": [\"a\"], \"a\": [\"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"a\", \"output_realtime\"], \"Test-1 failed\"\nassert extract_realtime_model({\"input\": [\"a\", \"b\"], \"a\": [\"c\"], \"b\": [\"c\"], \"c\": [\"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"a\", \"c\", \"output_realtime\"], \"Test-2 failed\"\nassert extract_realtime_model({\"input\": [\"a\", \"b\"], \"a\": [\"output_realtime\"], \"b\": [\"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"a\", \"output_realtime\"], \"Test-3 failed\"\nassert extract_realtime_model({\"input\": [\"a\"], \"a\": [\"b\"], \"b\": [\"a\", \"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"a\", \"b\", \"output_realtime\"], \"Test-4 failed\"\nassert extract_realtime_model({\"input\": [\"a\"], \"a\": [], \"output_realtime\": []}) == [], \"Test-5 failed\"\nassert extract_realtime_model({\"a\": [\"output_realtime\"], \"output_realtime\": []}) == [], \"Test-6 failed\"\nassert extract_realtime_model({\"input\": [\"a\"], \"a\": []}) == [], \"Test-7 failed\"\nassert extract_realtime_model({\"input\": [\"b\", \"a\"], \"a\": [\"d\"], \"b\": [\"c\"], \"c\": [\"d\"], \"d\": [\"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"a\", \"d\", \"output_realtime\"], \"Test-8 failed\"\nassert extract_realtime_model({\"input\": [\"output_realtime\"], \"output_realtime\": []}) == [\"input\", \"output_realtime\"], \"Test-9 failed\"\nassert extract_realtime_model({\"input\": [\"a\"], \"a\": [\"output_realtime\"], \"output_realtime\": [], \"junk1\": [\"junk2\"], \"junk2\": []}) == [\"input\", \"a\", \"output_realtime\"], \"Test-10 failed\""}
{"task_id": 213, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\nimport numpy as np\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with He Normal strategy.\n\n    Args:\n        shape: A tuple that represents the desired tensor shape.\n\n    Returns:\n        Nested Python list containing the initialised weights.\n    \"\"\"\n    ndim = len(shape)\n    fan_in = 0\n    fan_out = 0\n    if ndim == 1:\n        fan_in = shape[0]\n        fan_out = shape[0]\n    elif ndim == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    else:\n        out_channels = shape[0]\n        in_channels = shape[1]\n        receptive_field_size = 1\n        for dim in shape[2:]:\n            receptive_field_size *= dim\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    if fan_in == 0:\n        return []\n    std = np.sqrt(2.0 / fan_in)\n    np.random.seed(42)\n    W = np.random.normal(0, std, shape)\n    return W.tolist()\nassert tuple(len(row) for row in he_normal((2, 3))) == (3, 3), \"test failed: wrong shape for (2, 3)\"\nw1 = np.array(he_normal((100, 50))); fan_in_1 = 100; expected_std_1 = (2.0 / fan_in_1) ** 0.5; assert abs(w1.mean()) < 0.05 and abs(w1.std(ddof=0) - expected_std_1) < 0.05, \"test failed: stats check for (100, 50)\"\nw2 = np.array(he_normal((64, 3, 3, 3))); fan_in_2 = 3 * 3 * 3; expected_std_2 = (2.0 / fan_in_2) ** 0.5; assert abs(w2.mean()) < 0.05 and abs(w2.std(ddof=0) - expected_std_2) < 0.05, \"test failed: stats check for conv kernel\"\nassert he_normal(()) == [], \"test failed: empty shape should return empty list\"\nassert len(he_normal((1,))) == 1, \"test failed: shape (1,)\"\nw3 = np.array(he_normal((3, 2))); assert w3.shape == (3, 2), \"test failed: shape (3,2)\"\nw5 = np.array(he_normal((10, 10))); fan_in_5 = 10; assert abs(w5.mean()) < 0.1, \"test failed: mean too far from 0\"\nw6 = np.array(he_normal((10,))); fan_in_6 = 10; expected_std_6 = (2.0 / fan_in_6) ** 0.5; assert abs(w6.std(ddof=0) - expected_std_6) < 0.3, \"test failed: 1D tensor std\"\nassert isinstance(he_normal((2, 3)), list), \"test failed: output type must be list\""}
{"task_id": 217, "completion_id": 0, "passed": false, "result": "failed: failed on ([1, -1], [0.5, -0.5])", "solution": "import numpy as np\nimport numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    prob = 1 / (1 + np.exp(-predicted))\n    z = -actual * predicted\n    sigmoid_z = 1 / (1 + np.exp(z))\n    gradient = actual * sigmoid_z\n    hessian = prob * (1 - prob)\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    return (gradient_list, hessian_list, probability_list)\nassert logistic_components([1, -1], [0.5, -0.5]) == ([0.377541, -0.377541], [0.235004, 0.235004], [0.622459, 0.377541]), \"failed on ([1, -1], [0.5, -0.5])\"\nassert logistic_components([1, -1], [10, -10]) == ([0.000045, -0.000045], [0.000045, 0.000045], [0.999955, 0.000045]), \"failed on large magnitude scores\"\nassert logistic_components([1, 1, 1], [0, 0, 0]) == ([0.5, 0.5, 0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with positive labels\"\nassert logistic_components([-1, -1, -1], [0, 0, 0]) == ([-0.5, -0.5, -0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with negative labels\"\nassert logistic_components([1], [-2]) == ([0.880797], [0.104994], [0.119203]), \"failed on single sample (1, -2)\"\nassert logistic_components([-1], [2]) == ([-0.880797], [0.104994], [0.880797]), \"failed on single sample (-1, 2)\"\nassert logistic_components([1, -1, 1, -1], [1, 1, -1, -1]) == ([0.268941, -0.731059, 0.731059, -0.268941], [0.196612, 0.196612, 0.196612, 0.196612], [0.731059, 0.731059, 0.268941, 0.268941]), \"failed on mixed signs\"\nassert logistic_components([1], [0]) == ([0.5], [0.25], [0.5]), \"failed on ([1], [0])\"\nassert logistic_components([1, -1], [5, -5]) == ([0.006693, -0.006693], [0.006648, 0.006648], [0.993307, 0.006693]), \"failed on moderate magnitude scores\"\nassert logistic_components([-1], [3]) == ([-0.952574], [0.045177], [0.952574]), \"failed on (-1, 3)\""}
{"task_id": 224, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(e, a) for e in z]\n    elif z >= 0:\n        return z\n    else:\n        return a * z\nassert leaky_relu([-2, -1, 0, 1, 2]) == [-0.02, -0.01, 0.0, 1.0, 2.0], \"failed on default slope, 1-D list\"\nassert leaky_relu(np.array([-5.0, 5.0]), 0.2) == [-1.0, 5.0], \"failed on numpy input, custom slope\"\nassert leaky_relu(np.array([[[-1]]]), 0.5) == [[[-0.5]]], \"failed on 3-D array\"\nassert leaky_relu(0) == 0.0, \"failed on scalar zero\"\nassert leaky_relu(-4, 0.25) == -1.0, \"failed on scalar negative\"\nassert leaky_relu(3.3) == 3.3, \"failed on scalar positive\"\nassert leaky_relu([[0]]) == [[0.0]], \"failed on zero inside nested list\""}
{"task_id": 225, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef hz2mel(hz, formula: str='htk') -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    if formula not in ['htk', 'slaney']:\n        raise AssertionError(\"formula must be 'htk' or 'slaney'\")\n    original_is_scalar = np.isscalar(hz)\n    hz_array = np.asarray(hz)\n    if formula == 'htk':\n        mel = 2595 * np.log10(1 + hz_array / 700)\n    else:\n        mel = 1127 * np.log(1 + hz_array / 700)\n    rounded_mel = np.round(mel, 4)\n    if original_is_scalar:\n        return [rounded_mel.item()]\n    else:\n        return rounded_mel.tolist()\nassert hz2mel(np.array([0.0]), \"htk\") == [0.0], \"failed: hz2mel([0], 'htk')\"\nassert hz2mel([6300], \"htk\") == [2595.0], \"failed: hz2mel([6300], 'htk')\"\nassert hz2mel([69300], \"htk\") == [5190.0], \"failed: hz2mel([69300], 'htk')\"\nassert hz2mel([0.0, 6300, 69300], \"htk\") == [0.0, 2595.0, 5190.0], \"failed: hz2mel([...], 'htk')\"\nassert hz2mel(np.array([0.0]), \"slaney\") == [0.0], \"failed: hz2mel([0], 'slaney')\"\nassert hz2mel([1202.7972799213314], \"slaney\") == [1127.0], \"failed: hz2mel([1202.7972799], 'slaney')\"\nassert hz2mel([4472.339268], \"slaney\") == [2254.0], \"failed: hz2mel([4472.339268], 'slaney')\"\nassert hz2mel([0.0, 1202.7972799213314, 4472.339268], \"slaney\") == [0.0, 1127.0, 2254.0], \"failed: hz2mel([...], 'slaney')\"\nassert hz2mel([[0, 6300],[6300, 0]], \"htk\") == [[0.0, 2595.0],[2595.0, 0.0]], \"failed: hz2mel(2D, 'htk')\"\nassert hz2mel([[0, 1202.7972799213314],[4472.339268, 0]], \"slaney\") == [[0.0, 1127.0],[2254.0, 0.0]], \"failed: hz2mel(2D, 'slaney')\""}
{"task_id": 227, "completion_id": 0, "passed": false, "result": "failed: type numpy.ndarray doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef value_network_forward(state: list[float], W1: list[list[float]], W2: list[list[float]], W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\n\n    Parameters\n    ----------\n    state : list[float]\n        1-D state vector of length N.\n    W1    : list[list[float]]\n        Weight matrix with shape (N, H1).\n    W2    : list[list[float]]\n        Weight matrix with shape (H1, H2).\n    W3    : list[list[float]] | list[float]\n        Weight matrix with shape (H2, 1) or (H2,) for the output layer.\n\n    Returns\n    -------\n    float\n        The scalar state-value rounded to 4 decimals, or \u22121 on shape error.\n    \"\"\"\n    state_np = np.array(state)\n    N = len(state_np)\n    if len(W1) != N:\n        return -1\n    H1 = len(W1[0])\n    for row in W1:\n        if len(row) != H1:\n            return -1\n    if len(W2) != H1:\n        return -1\n    H2 = len(W2[0])\n    for row in W2:\n        if len(row) != H2:\n            return -1\n    if isinstance(W3[0], list):\n        if len(W3) != H2:\n            return -1\n        for row in W3:\n            if len(row) != 1:\n                return -1\n    elif len(W3) != H2:\n        return -1\n    h1 = np.tanh(np.dot(state_np, W1))\n    h2 = np.tanh(np.dot(h1, W2))\n    W3_np = np.array(W3)\n    v = h2.dot(W3_np)\n    rounded_v = round(v, 4)\n    return rounded_v\nassert value_network_forward([0.0, 0.0, 0.0], [[3,1], [2,2], [1,3]], [[5],[4]], [[7.0]]) == 0.0, \"test case 2 failed\"\nassert value_network_forward([3,4], [[0,0,0],[0,0,0]], [[1,2,3],[1,2,3],[1,2,3]], [[1],[1],[1]]) == 0.0, \"test case 3 failed\"\nassert value_network_forward([2], [[2]], [[0]], [[5]]) == 0.0, \"test case 4 failed\"\nassert value_network_forward([2], [[2]], [[2]], [[0]]) == 0.0, \"test case 5 failed\"\nassert value_network_forward([1,0], [[-1],[ -1]], [[-1]], [[-1]]) == -0.642, \"test case 6 failed\"\nassert value_network_forward([1,2], [[1]], [[1]], [[1]]) == -1, \"test case 7 failed (shape mismatch)\"\nassert value_network_forward([1], [[1]], [[1]], [[1,1]]) == -1, \"test case 8 failed (shape mismatch)\"\nassert value_network_forward([0.5], [[2]], [[1]], [[1]]) == 0.642, \"test case 10 failed\""}
{"task_id": 231, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determines if the given hand is a natural blackjack.\n    \n    A natural blackjack is a two-card hand consisting of an Ace (1) and a ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1\nassert is_natural([1, 10]) is True, \"failed on: [1, 10]\"\nassert is_natural([10, 1]) is True, \"failed on: [10, 1]\"\nassert is_natural([1, 5]) is False, \"failed on: [1, 5]\"\nassert is_natural([10, 10]) is False, \"failed on: [10, 10]\"\nassert is_natural([1, 1]) is False, \"failed on: [1, 1]\"\nassert is_natural([5, 10]) is False, \"failed on: [5, 10]\"\nassert is_natural([1, 9]) is False, \"failed on: [1, 9]\"\nassert is_natural([7, 1]) is False, \"failed on: [7, 1]\"\nassert is_natural([10, 11]) is False, \"failed on: [10, 11] (11 is not allowed, no Ace)\"\nassert is_natural([1, 10, 5]) is False, \"failed on: [1, 10, 5] (more than two cards)\""}
{"task_id": 232, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if x.ndim != 2:\n        return False\n    if not np.all((x == 0) | (x == 1)):\n        return False\n    if not np.all(x.sum(axis=1) == 1):\n        return False\n    return True\nassert is_one_hot(np.array([[1, 0, 0], [0, 1, 0]])) == True, \"failed: basic valid matrix\"\nassert is_one_hot(np.array([[0, 1], [1, 0], [0, 1]])) == True, \"failed: rectangular valid matrix\"\nassert is_one_hot(np.array([[0, 1], [1, 1]])) == False, \"failed: row with two ones\"\nassert is_one_hot(np.array([[0, 2], [1, 0]])) == False, \"failed: value other than 0/1 present\"\nassert is_one_hot(np.array([[0, 0, 0], [0, 1, 0]])) == False, \"failed: row with no ones\"\nassert is_one_hot(np.array([1, 0, 0])) == False, \"failed: not a 2-D array\"\nassert is_one_hot(np.array([[True, False], [False, True]])) == True, \"failed: boolean dtype matrix\"\nassert is_one_hot(np.array([[False, False], [False, True]])) == False, \"failed: boolean matrix with invalid row\"\nassert is_one_hot(np.eye(5)) == True, \"failed: identity matrix\"\nassert is_one_hot(np.array([[0, 0], [0, 0]])) == False, \"failed: all zeros matrix\""}
{"task_id": 234, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef thompson_sampling_beta_binomial(successes: list[int], failures: list[int], prior_alpha: float=1.0, prior_beta: float=1.0, seed: int | None=None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\n\n    Parameters\n    ----------\n    successes : list[int]\n        Number of observed successes for every arm.\n    failures  : list[int]\n        Number of observed failures for every arm (same length as *successes*).\n    prior_alpha : float, optional (default=1.0)\n        Shared Alpha parameter of the Beta prior.\n    prior_beta  : float, optional (default=1.0)\n        Shared Beta  parameter of the Beta prior.\n    seed : int | None, optional (default=None)\n        If given, NumPy\u2019s random seed is set to this value to obtain\n        deterministic samples.\n\n    Returns\n    -------\n    int\n        Index of the arm with the highest sampled payoff probability, or\n        -1 if the input is malformed.\n    \"\"\"\n    if not successes or not failures:\n        return -1\n    if len(successes) != len(failures):\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    samples = []\n    for i in range(len(successes)):\n        a = prior_alpha + successes[i]\n        b = prior_beta + failures[i]\n        sample = np.random.beta(a, b)\n        samples.append(sample)\n    best_arm = np.argmax(samples)\n    return best_arm\nassert thompson_sampling_beta_binomial([10,0],[0,10],seed=42)==0, \"test case failed: ([10,0],[0,10],seed=42)\"\nassert thompson_sampling_beta_binomial([0,10],[10,0],seed=42)==1, \"test case failed: ([0,10],[10,0],seed=42)\"\nassert thompson_sampling_beta_binomial([100,0,50],[0,100,30])==0, \"test case failed: ([100,0,50],[0,100,30])\"\nassert thompson_sampling_beta_binomial([5],[3])==0, \"test case failed: single arm ([5],[3])\"\nassert thompson_sampling_beta_binomial([],[])==-1, \"test case failed: empty input\"\nassert thompson_sampling_beta_binomial([1,2,3],[3,2,1],seed=7)==2, \"test case failed: ([1,2,3],[3,2,1],seed=7)\"\nassert thompson_sampling_beta_binomial([20,5],[5,20])==0, \"test case failed: ([20,5],[5,20])\"\nassert thompson_sampling_beta_binomial([0,1,0,1],[1,0,1,0],seed=123)==3, \"test case failed: ([0,1,0,1],[1,0,1,0],seed=123)\"\n"}
{"task_id": 237, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    total = 0\n    num_aces = 0\n    for card in hand:\n        if card in ['J', 'Q', 'K']:\n            total += 10\n        elif card == 'A':\n            total += 11\n            num_aces += 1\n        else:\n            total += int(card)\n    while total > 21 and num_aces > 0:\n        total -= 10\n        num_aces -= 1\n    return total if total <= 21 else 0\nassert score(['A','K']) == 21, \"Test case 1 failed: ['A','K']\"\nassert score(['10','8','5']) == 0, \"Test case 2 failed: ['10','8','5']\"\nassert score(['A','9','A']) == 21, \"Test case 3 failed: ['A','9','A']\"\nassert score(['5','3','7','6']) == 21, \"Test case 4 failed: ['5','3','7','6']\"\nassert score(['J','Q']) == 20, \"Test case 5 failed: ['J','Q']\"\nassert score(['A','A','9']) == 21, \"Test case 6 failed: ['A','A','9']\"\nassert score(['A','A','A','7']) == 20, \"Test case 7 failed: ['A','A','A','7']\"\nassert score(['2','3','4','5','6']) == 20, \"Test case 8 failed: ['2','3','4','5','6']\"\nassert score(['K','Q','2']) == 0, \"Test case 9 failed: ['K','Q','2']\"\nassert score(['A']) == 11, \"Test case 10 failed: ['A']\""}
{"task_id": 239, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str='same') -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\n\n    Parameters\n    ----------\n    filter_shape : tuple[int, int]\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : str, optional (default=\"same\")\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    tuple[tuple[int, int], tuple[int, int]]\n        ((pad_top, pad_bottom), (pad_left, pad_right)) where every value is an\n        integer \u2265 0.\n    \"\"\"\n    if output_shape == 'valid':\n        return ((0, 0), (0, 0))\n    else:\n        (h, w) = filter_shape\n        pad_h_total = h - 1\n        pad_top = pad_h_total // 2\n        pad_bottom = pad_h_total - pad_top\n        pad_w_total = w - 1\n        pad_left = pad_w_total // 2\n        pad_right = pad_w_total - pad_left\n        return ((pad_top, pad_bottom), (pad_left, pad_right))\nassert determine_padding((3, 3), \"same\") == ((1, 1), (1, 1)), \"failed: (3,3) same\"\nassert determine_padding((1, 1), \"same\") == ((0, 0), (0, 0)), \"failed: (1,1) same\"\nassert determine_padding((5, 5), \"same\") == ((2, 2), (2, 2)), \"failed: (5,5) same\"\nassert determine_padding((4, 4), \"same\") == ((1, 2), (1, 2)), \"failed: (4,4) same\"\nassert determine_padding((4, 2), \"same\") == ((1, 2), (0, 1)), \"failed: (4,2) same\"\nassert determine_padding((2, 4), \"same\") == ((0, 1), (1, 2)), \"failed: (2,4) same\"\nassert determine_padding((3, 3), \"valid\") == ((0, 0), (0, 0)), \"failed: (3,3) valid\"\nassert determine_padding((2, 2), \"valid\") == ((0, 0), (0, 0)), \"failed: (2,2) valid\"\nassert determine_padding((2, 2), \"same\") == ((0, 1), (0, 1)), \"failed: (2,2) same\"\nassert determine_padding((7, 3), \"same\") == ((3, 3), (1, 1)), \"failed: (7,3) same\""}
{"task_id": 240, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef create_encoding_dicts(obs_ids: list, action_ids: list, obs_encoder: dict | None=None, act_encoder: dict | None=None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    obs2num = {}\n    act2num = {}\n    if obs_encoder is not None:\n        obs2num = obs_encoder.copy()\n    elif obs_ids:\n        obs2num = {id: idx for (idx, id) in enumerate(obs_ids)}\n    num2obs = {v: k for (k, v) in obs2num.items()}\n    if act_encoder is not None:\n        act2num = act_encoder.copy()\n    elif action_ids:\n        act2num = {id: idx for (idx, id) in enumerate(action_ids)}\n    num2act = {v: k for (k, v) in act2num.items()}\n    return {'obs2num': obs2num, 'num2obs': num2obs, 'act2num': act2num, 'num2act': num2act}\nassert create_encoding_dicts(['a', 'b'], ['x', 'y']) == {'obs2num': {'a': 0, 'b': 1}, 'num2obs': {0: 'a', 1: 'b'}, 'act2num': {'x': 0, 'y': 1}, 'num2act': {0: 'x', 1: 'y'}}, 'test case failed: basic enumeration'\nres = create_encoding_dicts(['left', 'right', 'front', 'back'], ['move', 'turn']); assert res['obs2num']['front'] == 2 and res['num2obs'][2] == 'front', 'test case failed: larger lists (obs)'\nobs_enc = {'cold': 5, 'hot': 7}; act_enc = {'go': 9, 'stop': 3}; res = create_encoding_dicts(['cold', 'hot'], ['go', 'stop'], obs_enc, act_enc); assert res['obs2num'] == obs_enc and res['num2obs'] == {5: 'cold', 7: 'hot'}, 'test case failed: explicit obs encoder'\nassert create_encoding_dicts([], []) == {'obs2num': {}, 'num2obs': {}, 'act2num': {}, 'num2act': {}}, 'test case failed: empty lists'\nres = create_encoding_dicts(['sun', 'rain'], ['walk', 'run'], {'sun': 2, 'rain': 4}); assert res['act2num']['walk'] == 0 and res['num2act'][1] == 'run', 'test case failed: mixed encoders (act)'\nres = create_encoding_dicts([], ['jump'], None, {'jump': 42}); assert res['act2num']['jump'] == 42 and res['num2act'][42] == 'jump', 'test case failed: encoder with empty obs list'\nres = create_encoding_dicts(['x1', 'x2', 'x3'], ['y']); assert all(res['num2obs'][res['obs2num'][o]] == o for o in ['x1', 'x2', 'x3']), 'test case failed: consistency (obs)'\nres = create_encoding_dicts([10, 20], [30, 40]); assert res['obs2num'][10] == 0 and res['num2act'][0] == 30, 'test case failed: numerical identifiers'\nres = create_encoding_dicts(['only_obs'], ['only_act']); assert res['obs2num']['only_obs'] == 0 and res['act2num']['only_act'] == 0, 'test case failed: single element lists'\nres = create_encoding_dicts(['A','B','C'], ['L']); assert res['num2act'][res['act2num']['L']] == 'L', 'test case failed: consistency (act)'"}
{"task_id": 242, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float=1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    for a in range(len(pull_counts)):\n        if pull_counts[a] == 0:\n            return a\n    current_step_plus_1 = current_step + 1\n    log_term = math.log(current_step_plus_1)\n    max_score = -math.inf\n    best_arm = 0\n    for a in range(len(ev_estimates)):\n        denominator = pull_counts[a]\n        sqrt_term = math.sqrt(2 * log_term / denominator)\n        score = ev_estimates[a] + C * sqrt_term\n        if score > max_score:\n            max_score = score\n            best_arm = a\n    return best_arm\nassert ucb1_select([0.5, 0.7], [1, 1], 2) == 1, \"failed on basic two-arm example\"\nassert ucb1_select([0.2, 0.9, 0.1], [0, 5, 3], 8) == 0, \"failed when an arm was never pulled\"\nassert ucb1_select([0.6, 0.6], [10, 1], 11) == 1, \"failed when counts differ greatly\"\nassert ucb1_select([0.3, 0.4, 0.5, 0.6], [5, 5, 5, 5], 20) == 3, \"failed on equal counts different EVs\"\nassert ucb1_select([0.5, 0.5], [1, 1], 2) == 0, \"failed tie-breaking rule\"\nassert ucb1_select([0.2], [10], 10) == 0, \"failed single-arm bandit\"\nassert ucb1_select([0.4, 0.4], [3, 3], 6, C=0.5) == 0, \"failed tie with custom C\"\nassert ucb1_select([0.1, 0.2, 0.3], [2, 4, 6], 12) == 0, \"failed three-arm varying counts\"\nassert ucb1_select([0.9, 0.1], [100, 1], 101) == 1, \"failed exploration against high EV arm\"\nassert ucb1_select([0.0, 0.0, 0.0], [1, 0, 1], 2) == 1, \"failed when exactly one arm untried\""}
{"task_id": 243, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_s = sum(state)\n    action_probs = [round(1 / 3, 4), round(1 / 3, 4), round(1 / 3, 4)]\n    state_value = 64.0 * sum_s if sum_s > 0 else 0.0\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)\nassert actor_critic_forward([1.0, -1.0, 0.5]) == ([0.3333, 0.3333, 0.3333], 32.0), \"failed on state [1.0, -1.0, 0.5]\"\nassert actor_critic_forward([2.0, 3.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 384.0), \"failed on state [2.0, 3.0, 1.0]\"\nassert actor_critic_forward([-2.0, 0.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-2.0, 0.0, 1.0]\"\nassert actor_critic_forward([0.0, 0.0, 0.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [0.0, 0.0, 0.0]\"\nassert actor_critic_forward([1.0, 1.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [1.0, 1.0, 1.0]\"\nassert actor_critic_forward([-1.0, -1.0, -1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-1.0, -1.0, -1.0]\"\nassert actor_critic_forward([4.0, -2.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [4.0, -2.0, 1.0]\"\nassert actor_critic_forward([10.0, -10.0, 5.0]) == ([0.3333, 0.3333, 0.3333], 320.0), \"failed on state [10.0, -10.0, 5.0]\"\nassert actor_critic_forward([0.25, 0.25, 0.25]) == ([0.3333, 0.3333, 0.3333], 48.0), \"failed on state [0.25, 0.25, 0.25]\"\nassert actor_critic_forward([0.1, -0.2, 0.3]) == ([0.3333, 0.3333, 0.3333], 12.8), \"failed on state [0.1, -0.2, 0.3]\""}
{"task_id": 244, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef random_binary_tensor(shape: tuple, sparsity: float=0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    if not isinstance(shape, tuple):\n        return -1\n    if len(shape) == 0:\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if isinstance(sparsity, bool):\n        return -1\n    if not 0 <= sparsity <= 1:\n        return -1\n    tensor = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (tensor >= threshold).astype(float)\n    return binary_tensor.tolist()\nnp.random.seed(0)\nassert random_binary_tensor((2, 3), 0.5) == [[1.0, 1.0, 1.0], [1.0, 0.0, 1.0]], \"test case failed: seed=0, shape=(2,3), sparsity=0.5\"\nnp.random.seed(1)\nassert random_binary_tensor((5,), 0.3) == [0.0, 1.0, 0.0, 0.0, 0.0], \"test case failed: seed=1, shape=(5,), sparsity=0.3\"\nnp.random.seed(2)\nassert random_binary_tensor((1, 4), 0.0) == [[0.0, 0.0, 0.0, 0.0]], \"test case failed: seed=2, shape=(1,4), sparsity=0.0\"\nnp.random.seed(3)\nassert random_binary_tensor((2, 2), 1.0) == [[1.0, 1.0], [1.0, 1.0]], \"test case failed: seed=3, shape=(2,2), sparsity=1.0\"\nnp.random.seed(4)\nassert random_binary_tensor((3, 2), 0.7) == [[1.0, 1.0], [1.0, 1.0], [1.0, 0.0]], \"test case failed: seed=4, shape=(3,2), sparsity=0.7\"\nassert random_binary_tensor((2, 2), 1.2) == -1, \"test case failed: sparsity > 1\"\nassert random_binary_tensor((2, 2), -0.1) == -1, \"test case failed: sparsity < 0\"\nassert random_binary_tensor([2, 3], 0.5) == -1, \"test case failed: shape is not tuple\"\nassert random_binary_tensor((0, 3), 0.5) == -1, \"test case failed: zero dimension in shape\"\nassert random_binary_tensor((2.5, 3), 0.5) == -1, \"test case failed: non-integer dimension\""}
{"task_id": 248, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix. Its values are ignored by this baseline\n        predictor; only its length is potentially useful for sanity checks.\n    y_train : np.ndarray\n        One-dimensional array of binary labels (0 for negative, 1 for positive).\n    X_test : np.ndarray\n        Feature matrix for which the predictions are required. The number of\n        returned probabilities must match the number of rows in this matrix.\n\n    Returns\n    -------\n    list[float]\n        A list containing the same probability repeated for every row of\n        `X_test`. The probability is rounded to four decimal places.\n    \"\"\"\n    p = np.sum(y_train) / len(y_train)\n    n_samples = X_test.shape[0]\n    if n_samples == 0:\n        return []\n    predictions = np.full(n_samples, p)\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()\nassert baseline_prob_predict(np.array([[1, 2], [3, 4], [5, 6]]), np.array([0, 1, 1]), np.array([[7, 8], [9, 10]])) == [0.6667, 0.6667], \"test case failed: p = 2/3\"\nassert baseline_prob_predict(np.array([[0], [1]]), np.array([0, 0]), np.array([[5], [6], [7]])) == [0.0, 0.0, 0.0], \"test case failed: all negatives\"\nassert baseline_prob_predict(np.array([[0], [1], [2]]), np.array([1, 1, 1]), np.array([[5], [6]])) == [1.0, 1.0], \"test case failed: all positives\"\nassert baseline_prob_predict(np.array([[0], [1], [2], [3]]), np.array([1, 0, 0, 0]), np.array([[9], [9], [9], [9]])) == [0.25, 0.25, 0.25, 0.25], \"test case failed: p = 0.25\"\nassert baseline_prob_predict(np.array([[0], [1], [2], [3], [4]]), np.array([1, 0, 1, 0, 0]), np.array([[0], [0], [0], [0], [0]])) == [0.4, 0.4, 0.4, 0.4, 0.4], \"test case failed: p = 0.4\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([1]), np.empty((0, 2))) == [], \"test case failed: empty X_test\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([0]), np.array([[3, 4]])) == [0.0], \"test case failed: single negative\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([1]), np.array([[3, 4], [5, 6]])) == [1.0, 1.0], \"test case failed: single positive\"\nassert baseline_prob_predict(np.array([[0], [1], [2]]), np.array([1, 0, 0]), np.array([[8], [8], [8]])) == [0.3333, 0.3333, 0.3333], \"test case failed: rounding 1/3\"\nassert baseline_prob_predict(np.arange(100).reshape(100, 1), np.array([1] * 55 + [0] * 45), np.array([[0], [0], [0], [0]])) == [0.55, 0.55, 0.55, 0.55], \"test case failed: p = 0.55\""}
