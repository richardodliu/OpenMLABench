{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a:\n        if not b:\n            return []\n        else:\n            return -1\n    else:\n        cols_a = len(a[0])\n        if cols_a != len(b):\n            return -1\n        result = []\n        for row in a:\n            dp = 0\n            for i in range(cols_a):\n                dp += row[i] * b[i]\n            result.append(dp)\n        return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    total_elements = sum((len(row) for row in a))\n    new_total = new_shape[0] * new_shape[1]\n    if total_elements != new_total:\n        return []\n    arr = np.array(a)\n    reshaped_arr = arr.reshape(new_shape)\n    return reshaped_arr.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    else:\n        return [sum(col) / len(col) for col in zip(*matrix)]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigen1 = (trace + sqrt_discriminant) / 2\n    eigen2 = (trace - sqrt_discriminant) / 2\n    eigenvalues = [eigen1, eigen2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    epsilon = 1e-10\n    if abs(det_T) < epsilon or abs(det_S) < epsilon:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    try:\n        temp = T_inv @ A_np\n        result = temp @ S_np\n    except np.linalg.LinAlgError:\n        return -1\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    inverse = [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]\n    return inverse"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not b:\n        return -1\n    m = len(a)\n    n = len(a[0]) if m > 0 else 0\n    p = len(b)\n    q = len(b[0]) if p > 0 else 0\n    for row in a:\n        if len(row) != n:\n            return -1\n    for row in b:\n        if len(row) != q:\n            return -1\n    if n != p:\n        return -1\n    result = [[0 for _ in range(q)] for _ in range(m)]\n    for i in range(m):\n        for j in range(q):\n            total = 0\n            for k in range(n):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors:\n        return []\n    k = len(vectors)\n    n = len(vectors[0])\n    means = [sum(feature) / n for feature in vectors]\n    cov_matrix = [[0.0 for _ in range(k)] for _ in range(k)]\n    for i in range(k):\n        for j in range(k):\n            x = vectors[i]\n            y = vectors[j]\n            mean_x = means[i]\n            mean_y = means[j]\n            sum_product = 0.0\n            for (a, b) in zip(x, y):\n                sum_product += (a - mean_x) * (b - mean_y)\n            cov = sum_product / (n - 1)\n            cov_matrix[i][j] = cov\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    m = A.shape[0]\n    x_old = np.zeros(m)\n    for _ in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            sum_val = 0.0\n            for j in range(m):\n                if j != i:\n                    sum_val += A[i, j] * x_old[j]\n            x_new[i] = (b[i] - sum_val) / A[i, i]\n        x_old = x_new.copy()\n        x_old = np.round(x_old, 4)\n    return x_old.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    A_T_A = A.T @ A\n    p = A_T_A[0, 0]\n    q = A_T_A[0, 1]\n    r = A_T_A[1, 1]\n    trace = p + r\n    discriminant = (p - r) ** 2 + 4 * q ** 2\n    sqrt_discriminant = np.sqrt(discriminant)\n    lambda1 = (trace + sqrt_discriminant) / 2\n    lambda2 = (trace - sqrt_discriminant) / 2\n    singular_values = [np.sqrt(lambda1), np.sqrt(lambda2)]\n    singular_values.sort(reverse=True)\n    singular_values = [round(sv, 4) for sv in singular_values]\n    return tuple(singular_values)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(m):\n        (a, b, c) = m[0]\n        (d, e, f) = m[1]\n        (g, h, i) = m[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    det = 0.0\n    for j in range(4):\n        element = matrix[0][j]\n        minor = [row[:j] + row[j + 1:] for row in matrix[1:]]\n        minor_det = determinant_3x3(minor)\n        sign = (-1) ** j\n        det += element * sign * minor_det\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    Xt = X_np.T\n    Xt_X = Xt @ X_np\n    inv_Xt_X = np.linalg.inv(Xt_X)\n    Xt_y = Xt @ y_np\n    theta = inv_Xt_X @ Xt_y\n    theta_rounded = [round(x, 4) for x in theta]\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    theta = np.zeros(X.shape[1])\n    m = X.shape[0]\n    for _ in range(iterations):\n        h = X.dot(theta)\n        error = h - y\n        gradient = X.T.dot(error) / m\n        theta = theta - alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n\n    def standardize(column):\n        mean = np.mean(column)\n        std = np.std(column)\n        if std == 0:\n            return np.zeros_like(column)\n        return (column - mean) / std\n\n    def minmax(column):\n        min_val = np.min(column)\n        max_val = np.max(column)\n        if max_val == min_val:\n            return np.zeros_like(column)\n        return (column - min_val) / (max_val - min_val)\n    columns_standard = []\n    for i in range(data.shape[1]):\n        col = data[:, i]\n        scaled_col = standardize(col)\n        scaled_col_rounded = np.round(scaled_col, 4)\n        columns_standard.append(scaled_col_rounded.tolist())\n    scaled_standard = [list(row) for row in zip(*columns_standard)]\n    columns_minmax = []\n    for i in range(data.shape[1]):\n        col = data[:, i]\n        scaled_col = minmax(col)\n        scaled_col_rounded = np.round(scaled_col, 4)\n        columns_minmax.append(scaled_col_rounded.tolist())\n    scaled_minmax = [list(row) for row in zip(*columns_minmax)]\n    return (scaled_standard, scaled_minmax)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        clusters = [[] for _ in range(k)]\n        for point in points_np:\n            distances = np.sqrt(np.sum((point - centroids) ** 2, axis=1))\n            closest_index = np.argmin(distances)\n            clusters[closest_index].append(point)\n        new_centroids = []\n        for cluster in clusters:\n            if not cluster:\n                pass\n            else:\n                new_centroid = np.mean(cluster, axis=0)\n                new_centroids.append(new_centroid)\n        new_centroids_np = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids_np):\n            break\n        centroids = new_centroids_np\n    rounded_centroids = [tuple(np.round(c, 4)) for c in centroids]\n    return rounded_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n = len(X)\n    indices = np.arange(n)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    group_size = n // k\n    remainder = n % k\n    folds = []\n    for i in range(k):\n        if i < remainder:\n            fold_size = group_size + 1\n            start = i * (group_size + 1)\n        else:\n            fold_size = group_size\n            start = i * group_size + remainder\n        end = start + fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    standardized_data = (data - means) / stds\n    n_samples = data.shape[0]\n    covariance_matrix = standardized_data.T @ standardized_data / (n_samples - 1)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    eigen_pairs = list(enumerate(eigenvalues))\n    eigen_pairs.sort(key=lambda x: -x[1])\n    sorted_indices = [pair[0] for pair in eigen_pairs]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    k = min(k, len(sorted_eigenvectors))\n    principal_components_matrix = sorted_eigenvectors[:, :k]\n    principal_components = principal_components_matrix.T\n    rounded_components = [[round(x, 4) for x in row] for row in principal_components]\n    return rounded_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    target_values = [ex[target_attr] for ex in examples]\n    if len(set(target_values)) == 1:\n        return {'leaf': target_values[0]}\n    if not attributes:\n        majority = Counter(target_values).most_common(1)[0][0]\n        return {'leaf': majority}\n    current_counts = Counter(target_values)\n    total = len(examples)\n    entropy = 0\n    for count in current_counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    max_gain = -1\n    best_attr = None\n    for attr in attributes:\n        groups = {}\n        for ex in examples:\n            val = ex[attr]\n            if val not in groups:\n                groups[val] = []\n            groups[val].append(ex)\n        sum_entropy = 0\n        for (val, group) in groups.items():\n            group_counts = Counter([ex[target_attr] for ex in group])\n            group_total = len(group)\n            if group_total == 0:\n                continue\n            group_entropy = 0\n            for count in group_counts.values():\n                p = count / group_total\n                group_entropy -= p * math.log2(p)\n            sum_entropy += group_total / total * group_entropy\n        gain = entropy - sum_entropy\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    groups = {}\n    for ex in examples:\n        val = ex[best_attr]\n        if val not in groups:\n            groups[val] = []\n        groups[val].append(ex)\n    subtree = {}\n    new_attrs = [a for a in attributes if a != best_attr]\n    for val in groups:\n        subset = groups[val]\n        subtree_val = learn_decision_tree(subset, new_attrs, target_attr)\n        subtree[val] = subtree_val\n    return {best_attr: subtree}"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    max_score = max(scores)\n    adjusted_scores = [s - max_score for s in scores]\n    exponentials = [math.exp(s) for s in adjusted_scores]\n    denominator = sum(exponentials)\n    result = [round(e / denominator, 4) for e in exponentials]\n    return result"}
{"task_id": 24, "completion_id": 0, "solution": "import math\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predictions = []\n    for feature_vector in features:\n        linear_output = sum((f * w for (f, w) in zip(feature_vector, weights))) + bias\n        sigmoid_output = 1 / (1 + math.exp(-linear_output))\n        predictions.append(sigmoid_output)\n    mse = 0.0\n    for (p, y) in zip(predictions, labels):\n        mse += (p - y) ** 2\n    mse = mse / len(predictions)\n    rounded_predictions = [round(p, 4) for p in predictions]\n    rounded_mse = round(mse, 4)\n    return (rounded_predictions, rounded_mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_pred = 1 / (1 + np.exp(-z))\n        loss = np.mean((y_pred - labels) ** 2)\n        mse_list.append(round(loss, 4))\n        delta = (y_pred - labels) * y_pred * (1 - y_pred)\n        gradient_weights = np.dot(features.T, delta) / features.shape[0]\n        gradient_bias = np.mean(delta)\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    return (weights, bias, mse_list)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def add(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            (a, b) = out._prev\n            a.grad += out.grad\n            b.grad += out.grad\n            a._backward()\n            b._backward()\n        out._backward = _backward\n        return out\n\n    def mul(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            (a, b) = out._prev\n            a.grad += out.grad * b.data\n            b.grad += out.grad * a.data\n            a._backward()\n            b._backward()\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), _children=(self,), _op='relu')\n\n        def _backward():\n            a = next(iter(out._prev))\n            if out.data > 0:\n                a.grad += out.grad\n            a._backward()\n        out._backward = _backward\n        return out"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=np.float64)\n    C_matrix = np.array(C, dtype=np.float64)\n    B_inv = np.linalg.inv(B_matrix)\n    P = B_inv @ C_matrix\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_t_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_t_A)\n    sorted_indices = np.argsort(-eigenvalues)\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(sorted_eigenvalues)\n    if np.all(singular_values == 0):\n        U = np.identity(2)\n        S = np.zeros((2, 2))\n        V = np.identity(2)\n    else:\n        V = sorted_eigenvectors\n        U = np.zeros_like(A)\n        for i in range(2):\n            if singular_values[i] != 0:\n                v_i = V[:, i]\n                u_i = np.dot(A, v_i) / singular_values[i]\n                U[:, i] = u_i\n        zero_indices = np.where(singular_values == 0)[0]\n        for i in zero_indices:\n            non_zero_index = np.argmax(singular_values)\n            u_non_zero = U[:, non_zero_index]\n            u_i = np.array([-u_non_zero[1], u_non_zero[0]])\n            u_i = u_i / np.linalg.norm(u_i)\n            U[:, i] = u_i\n        S = np.diag(singular_values)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    perm = np.random.permutation(len(X))\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i + batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i + batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    mask = X[:, feature_i] >= threshold\n    subset1 = X[mask]\n    subset2 = X[~mask]\n    return (subset1.tolist(), subset2.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    if degree < 0:\n        return []\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    result = []\n    for d in range(0, degree + 1):\n        if d == 0:\n            new_feature = np.ones((n_samples, 1))\n            result.append(new_feature)\n        else:\n            for combo in combinations_with_replacement(range(n_features), d):\n                exponents = [0] * n_features\n                for idx in combo:\n                    exponents[idx] += 1\n                new_feature = np.ones(n_samples)\n                for i in range(n_samples):\n                    row = X[i]\n                    product = 1\n                    for j in range(n_features):\n                        product *= row[j] ** exponents[j]\n                    new_feature[i] = product\n                new_feature = new_feature.reshape(-1, 1)\n                result.append(new_feature)\n    if not result:\n        return []\n    final_features = np.concatenate(result, axis=1)\n    return final_features.tolist()"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is not None:\n        if not (np.all(x >= 0) and np.all(x < n_col)):\n            raise ValueError('x contains values out of the valid range [0, n_col-1)')\n        indices = x\n    else:\n        unique_x = np.sort(np.unique(x))\n        n_col = len(unique_x)\n        indices = np.searchsorted(unique_x, x)\n    encoded = np.eye(n_col)[indices]\n    return encoded.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diag(x).tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    corr_matrix = np.corrcoef(X, Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    y = np.where(y == 0, -1, 1)\n    n_samples = X.shape[0]\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature = -1\n        best_threshold = 0\n        best_direction = 0\n        for j in range(X.shape[1]):\n            feature_values = X[:, j]\n            unique_values = np.sort(np.unique(feature_values))\n            min_e_j = float('inf')\n            for i in range(len(unique_values) - 1):\n                t = (unique_values[i] + unique_values[i + 1]) / 2\n                pred0 = np.where(feature_values <= t, -1, 1)\n                error0 = np.sum(weights * (pred0 != y))\n                pred1 = np.where(feature_values <= t, 1, -1)\n                error1 = np.sum(weights * (pred1 != y))\n                current_error = min(error0, error1)\n                if current_error < min_e_j:\n                    min_e_j = current_error\n                    if error0 < error1:\n                        best_t_j = t\n                        best_dir_j = 0\n                    else:\n                        best_t_j = t\n                        best_dir_j = 1\n            if min_e_j < min_error:\n                min_error = min_e_j\n                best_feature = j\n                best_threshold = best_t_j\n                best_direction = best_dir_j\n        e = min_error\n        if e == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * math.log((1 - e) / e)\n        feature_values = X[:, best_feature]\n        if best_direction == 0:\n            pred = np.where(feature_values <= best_threshold, -1, 1)\n        else:\n            pred = np.where(feature_values <= best_threshold, 1, -1)\n        exponent = np.where(pred == y, -alpha, alpha)\n        new_weights = weights * np.exp(exponent)\n        Z = np.sum(new_weights)\n        if Z == 0:\n            Z = 1e-08\n        weights = new_weights / Z\n        rounded_t = round(best_threshold, 4)\n        rounded_alpha = round(alpha, 4) if not np.isinf(alpha) else 0.0\n        classifiers.append((best_feature, rounded_t, rounded_alpha))\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    adjusted = scores - max_score\n    exp_adj = np.exp(adjusted)\n    sum_exp = np.sum(exp_adj)\n    log_sum = np.log(sum_exp)\n    log_softmax = adjusted - log_sum\n    log_softmax_rounded = np.round(log_softmax, 4)\n    return log_softmax_rounded.tolist()"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    (H_padded, W_padded) = padded_input.shape\n    (k_h, k_w) = kernel.shape\n    output_height = (H_padded - k_h) // stride + 1\n    output_width = (W_padded - k_w) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(0, H_padded - k_h + 1, stride):\n        for j in range(0, W_padded - k_w + 1, stride):\n            region = padded_input[i:i + k_h, j:j + k_w]\n            output[i // stride, j // stride] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    m = len(y_true)\n    y_pred = X.dot(w)\n    sum_squared_errors = np.sum((y_pred - y_true) ** 2)\n    mse_term = sum_squared_errors / (2 * m)\n    reg_term = alpha / 2 * np.sum(w ** 2)\n    total_loss = mse_term + reg_term\n    return round(total_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    true_positives = np.sum(y_true * y_pred)\n    false_positives = np.sum(y_pred * (1 - y_true))\n    denominator = true_positives + false_positives\n    if denominator == 0:\n        return 0.0\n    return true_positives / denominator"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    weights = np.array(weights).copy()\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_X = X\n            batch_y = y\n        elif method == 'sgd':\n            idx = np.random.randint(0, n_samples)\n            batch_X = X[idx:idx + 1]\n            batch_y = y[idx:idx + 1]\n        elif method == 'mini-batch':\n            idx = np.random.choice(n_samples, batch_size, replace=False)\n            batch_X = X[idx]\n            batch_y = y[idx]\n        else:\n            raise ValueError('Invalid method')\n        y_pred = np.dot(batch_X, weights)\n        error = batch_y - y_pred\n        gradient = batch_X.T @ error / batch_size\n        weights -= learning_rate * gradient\n    rounded_weights = np.round(weights, 4)\n    return rounded_weights.tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    mat = np.array(matrix, dtype=np.float64)\n    rows = mat.shape[0]\n    cols = mat.shape[1]\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while i < rows and mat[i, lead] == 0:\n            i += 1\n        if i == rows:\n            lead += 1\n            r -= 1\n            continue\n        mat[[r, i]] = mat[[i, r]]\n        pivot = mat[r, lead]\n        if pivot != 0:\n            mat[r] = mat[r] / pivot\n        for i in range(rows):\n            if i != r:\n                factor = mat[i, lead]\n                mat[i] = mat[i] - factor * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        prev_w = w.copy()\n        prev_b = b\n        y_pred = X.dot(prev_w) + prev_b\n        error = y - y_pred\n        grad_w = X.T.dot(error) / n_samples + alpha * np.sign(prev_w)\n        grad_b = np.mean(error)\n        w = prev_w - learning_rate * grad_w\n        b = prev_b - learning_rate * grad_b\n        change_w = np.linalg.norm(w - prev_w)\n        change_b = abs(b - prev_b)\n        if change_w < tol and change_b < tol:\n            break\n    w = np.round(w, 4)\n    b = np.round(b, 4)\n    return (w.tolist(), [b])"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                sub_cost = dp[i - 1][j - 1]\n            else:\n                sub_cost = dp[i - 1][j - 1] + 1\n            del_cost = dp[i - 1][j] + 1\n            ins_cost = dp[i][j - 1] + 1\n            trans_cost = float('inf')\n            if i >= 2 and j >= 2:\n                if source[i - 2] == target[j - 1] and source[i - 1] == target[j - 2]:\n                    trans_cost = dp[i - 2][j - 2] + 1\n            dp[i][j] = min(sub_cost, del_cost, ins_cost, trans_cost)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(tp / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = Q @ K.T\n    exp_scores = np.exp(scores)\n    attention = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    output = attention @ V\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        input_contribution = Wx.dot(x)\n        hidden_contribution = Wh.dot(h_prev)\n        total = input_contribution + hidden_contribution + b\n        h_prev = np.tanh(total)\n    h_prev = np.round(h_prev, 4)\n    return h_prev.tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_np = np.array(points)\n    homogeneous_points = np.insert(points_np, 2, 1, axis=1)\n    homogeneous_points_transposed = homogeneous_points.T\n    translated = np.dot(matrix, homogeneous_points_transposed)\n    translated_points = translated.T\n    translated_2d = translated_points[:, :2]\n    result = translated_2d.tolist()\n    return result"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    log_term = np.log(var_q / var_p)\n    mu_diff_sq = (mu_p - mu_q) ** 2\n    term = (var_p + mu_diff_sq) / var_q\n    kl = 0.5 * (log_term + term - 1)\n    return kl"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x = np.zeros(len(b), dtype=np.float64)\n    else:\n        x = np.array(x_ini, dtype=np.float64)\n    for _ in range(n):\n        x_old = x.copy()\n        for i in range(len(x)):\n            sum_less = 0.0\n            for j in range(i):\n                sum_less += A[i, j] * x[j]\n            sum_more = 0.0\n            for j in range(i + 1, len(x)):\n                sum_more += A[i, j] * x_old[j]\n            denominator = A[i, i]\n            if denominator == 0:\n                raise ValueError('Division by zero encountered in Gauss-Seidel method.')\n            x[i] = (b[i] - sum_less - sum_more) / denominator\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    aug = np.hstack((A, b[:, np.newaxis]))\n    n = A.shape[0]\n    for i in range(n):\n        max_row = i\n        for j in range(i, n):\n            if abs(aug[j, i]) > abs(aug[max_row, i]):\n                max_row = j\n        (aug[i], aug[max_row]) = (aug[max_row], aug[i])\n        pivot = aug[i, i]\n        for j in range(i + 1, n):\n            factor = aug[j, i] / pivot\n            aug[j] = aug[j] - factor * aug[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        sum_val = 0.0\n        for j in range(i + 1, n):\n            sum_val += aug[i, j] * x[j]\n        x[i] = (aug[i, n] - sum_val) / aug[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        current_h = initial_hidden_state\n        current_c = initial_cell_state\n        for t in range(len(x)):\n            x_t = x[t].reshape(-1, 1)\n            gate_input = np.concatenate((current_h, x_t), axis=0)\n            ft = self.sigmoid(np.dot(self.Wf, gate_input) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, gate_input) + self.bi)\n            ct_tilde = np.tanh(np.dot(self.Wc, gate_input) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, gate_input) + self.bo)\n            new_c = ft * current_c + it * ct_tilde\n            new_h = ot * np.tanh(new_c)\n            hidden_states.append(new_h)\n            current_h = new_h\n            current_c = new_c\n        rounded_hidden = [np.round(h, 4).tolist() for h in hidden_states]\n        rounded_final_h = np.round(current_h, 4).tolist()\n        rounded_final_c = np.round(current_c, 4).tolist()\n        return (rounded_hidden, rounded_final_h, rounded_final_c)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    unique_query_terms = set(query)\n    df = {term: 0 for term in unique_query_terms}\n    for term in unique_query_terms:\n        for doc in corpus:\n            if term in doc:\n                df[term] += 1\n    N = len(corpus)\n    idf = {}\n    for term in unique_query_terms:\n        numerator = N + 1\n        denominator = df[term] + 1\n        idf_val = np.log(numerator / denominator)\n        idf[term] = idf_val\n    idf_list = [idf[term] for term in query]\n    result = []\n    for doc in corpus:\n        if not doc:\n            scores = [0.0] * len(query)\n        else:\n            counter = Counter(doc)\n            tf_list = []\n            for term in query:\n                count = counter.get(term, 0)\n                tf = count / len(doc)\n                tf_list.append(tf)\n            tf_array = np.array(tf_list)\n            tfidf_array = tf_array * idf_list\n            rounded = tfidf_array.round(4)\n            scores = rounded.tolist()\n        result.append(scores)\n    return result"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(y_true * y_pred)\n    fp = np.sum(y_pred) - tp\n    fn = np.sum(y_true) - tp\n    if tp + fp == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n    if tp + fn == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall\n    if denominator == 0:\n        f = 0.0\n    else:\n        f = numerator / denominator\n    return round(f, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = x.shape[0]\n        hidden_size = self.hidden_size\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        h_prev = np.zeros((hidden_size, 1))\n        for t in range(T):\n            x_t = x[t, :].reshape(-1, 1)\n            h_input = np.dot(self.W_xh, x_t) + np.dot(self.W_hh, h_prev) + self.b_h\n            h_current = np.tanh(h_input)\n            y_t = np.dot(self.W_hy, h_current) + self.b_y\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_current)\n            h_prev = h_current\n        outputs = np.array(outputs)\n        last_inputs = np.array(last_inputs)\n        last_hiddens = np.array(last_hiddens)\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT to compute gradients.\n        \"\"\"\n        T = input_sequence.shape[0]\n        hidden_size = self.hidden_size\n        output_size = self.W_hy.shape[0]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        delta = np.zeros((hidden_size, 1))\n        for t in range(T - 1, -1, -1):\n            y_t = outputs[t].reshape(-1, 1)\n            h_t = last_hiddens[t]\n            x_t = last_inputs[t]\n            expected_t = expected_output[t].reshape(-1, 1)\n            delta3 = y_t - expected_t\n            dW_hy += h_t.T @ delta3\n            db_y += delta3\n            delta2 = self.W_hy.T @ delta3 * (1 - h_t ** 2)\n            if t > 0:\n                h_prev = last_hiddens[t - 1]\n            else:\n                h_prev = np.zeros((hidden_size, 1))\n            dW_hh += h_prev.T @ delta2\n            db_h += delta2\n            dW_xh += x_t.T @ delta2\n            delta = self.W_hh.T @ delta2\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros(b.shape)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    for _ in range(n):\n        r_dot_r = np.dot(r, r)\n        if r_dot_r < tol ** 2:\n            break\n        p_dot_Ap = np.dot(p, A @ p)\n        alpha = r_dot_r / p_dot_Ap\n        x += alpha * p\n        r_new = r - alpha * (A @ p)\n        beta = np.dot(r_new, r_new) / r_dot_r\n        p = r_new + beta * p\n        r = r_new\n    x = np.round(x, 8)\n    return x.tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    n = len(y)\n    if n == 0:\n        return 0.0\n    (unique, counts) = np.unique(y, return_counts=True)\n    gini = 0.0\n    for count in counts:\n        p = count / n\n        gini += p * (1 - p)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    current_count = 0\n    for row in dense_matrix:\n        count = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        current_count += count\n        row_pointers.append(current_count)\n    return (values, column_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_v_L = sum((a * b for (a, b) in zip(v, L)))\n    dot_L_L = sum((a * a for a in L))\n    if dot_L_L == 0:\n        return []\n    scalar = dot_v_L / dot_L_L\n    projection = [scalar * component for component in L]\n    return [round(x, 3) for x in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    if rows == 0:\n        return ([], [], [])\n    cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(cols):\n        start = len(values)\n        for row in range(rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A_copy = np.array(A, dtype=np.float64)\n    (num_rows, num_cols) = A_copy.shape\n    pivot_cols = []\n    current_row = 0\n    current_col = 0\n    while current_row < num_rows and current_col < num_cols:\n        pivot = None\n        for r in range(current_row, num_rows):\n            if abs(A_copy[r, current_col]) > 1e-08:\n                pivot = r\n                break\n        if pivot is None:\n            current_col += 1\n            continue\n        (A_copy[current_row], A_copy[pivot]) = (A_copy[pivot], A_copy[current_row])\n        pivot_val = A_copy[current_row, current_col]\n        A_copy[current_row] = A_copy[current_row] / pivot_val\n        for r in range(num_rows):\n            if r != current_row:\n                factor = A_copy[r, current_col]\n                A_copy[r] -= factor * A_copy[current_row]\n        pivot_cols.append(current_col)\n        current_row += 1\n        current_col += 1\n    basis = []\n    for col in pivot_cols:\n        basis.append(A[:, col].tolist())\n    basis = [np.round(vec, 8).tolist() for vec in basis]\n    return basis"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_bar = np.mean(y_true)\n    sst = np.sum((y_true - y_bar) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    epsilon = 1e-10\n    if abs(sst) < epsilon:\n        if abs(sse) < epsilon:\n            return round(1.0, 3)\n        else:\n            return round(0.0, 3)\n    else:\n        r_sq = 1 - sse / sst\n        return round(r_sq, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    num_cols = len(img[0])\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n    if num_cols == 0:\n        return -1\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    total = 0\n    for row in img:\n        total += sum(row)\n    total_pixels = len(img) * num_cols\n    average = total / total_pixels\n    return round(average, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.size\n    if n == 0:\n        return 0.0\n    squared_diff = (y_true - y_pred) ** 2\n    mean_squared = np.mean(squared_diff)\n    rmse_value = np.sqrt(mean_squared)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = y_true.astype(bool)\n    y_pred = y_pred.astype(bool)\n    intersection = np.sum(y_true & y_pred)\n    union = np.sum(y_true | y_pred)\n    if union == 0:\n        return 1.0\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    composite = np.zeros(dim, dtype=int)\n    for (feature_name, feature_value) in row.items():\n        seed = random_seeds[feature_name]\n        rs = np.random.RandomState(seed)\n        name_vector = rs.choice([0, 1], size=dim)\n        value_vector = rs.choice([0, 1], size=dim)\n        feature_vector = (name_vector ^ value_vector).astype(int)\n        composite = (composite ^ feature_vector).astype(int)\n    return composite.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    labels = set()\n    for (y_true, y_pred) in data:\n        labels.add(y_true)\n        labels.add(y_pred)\n    unique_labels = sorted(labels)\n    label_to_idx = {label: idx for (idx, label) in enumerate(unique_labels)}\n    counts = Counter()\n    for (y_true, y_pred) in data:\n        y_true_idx = label_to_idx[y_true]\n        y_pred_idx = label_to_idx[y_pred]\n        counts[y_true_idx, y_pred_idx] += 1\n    matrix = [[counts.get((0, 0), 0), counts.get((0, 1), 0)], [counts.get((1, 0), 0), counts.get((1, 1), 0)]]\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_theta = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_theta, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    pairs = list(zip(actual, predicted))\n    counts = Counter(pairs)\n    tn = counts.get((0, 0), 0)\n    fp = counts.get((0, 1), 0)\n    fn = counts.get((1, 0), 0)\n    tp = counts.get((1, 1), 0)\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total = len(actual)\n    if total == 0:\n        accuracy = 0.0\n    else:\n        accuracy = (tp + tn) / total\n    precision = tp / (tp + fp) if tp + fp != 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    denominator_specificity = tn + fp\n    if denominator_specificity == 0:\n        specificity = 0.0\n    else:\n        specificity = tn / denominator_specificity\n    denominator_npv = tn + fn\n    if denominator_npv == 0:\n        negative_predictive_value = 0.0\n    else:\n        negative_predictive_value = tn / denominator_npv\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.mode(data).mode[0]\n    variance = np.var(data, ddof=0)\n    std_dev = np.std(data, ddof=0)\n    q25 = np.percentile(data, 25)\n    q50 = np.percentile(data, 50)\n    q75 = np.percentile(data, 75)\n    iqr = q75 - q25\n    rounded_mean = round(mean, 4)\n    rounded_median = round(median, 4)\n    rounded_variance = round(variance, 4)\n    rounded_std_dev = round(std_dev, 4)\n    rounded_q25 = round(q25, 4)\n    rounded_q50 = round(q50, 4)\n    rounded_q75 = round(q75, 4)\n    rounded_iqr = round(iqr, 4)\n    result = {'mean': rounded_mean, 'median': rounded_median, 'mode': mode, 'variance': rounded_variance, 'standard_deviation': rounded_std_dev, '25th_percentile': rounded_q25, '50th_percentile': rounded_q50, '75th_percentile': rounded_q75, 'interquartile_range': rounded_iqr}\n    return result"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    comb = math.comb(n, k)\n    prob = comb * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    exp_part = math.exp(exponent)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf = exp_part / denominator\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    prob = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(prob, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nimport itertools\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for data_point in data:\n        m = len(data_point)\n        exponents = itertools.product(range(degree + 1), repeat=m)\n        features = []\n        for exponents_tuple in exponents:\n            product = 1.0\n            for i in range(m):\n                feature = data_point[i]\n                exponent = exponents_tuple[i]\n                product *= feature ** exponent\n            features.append(round(product, 8))\n        transformed_data.append(features)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = []\n    for j in range(d_model):\n        exponent = 2 * j / d_model\n        denominator = 10000 ** exponent\n        argument = position / denominator\n        if j % 2 == 0:\n            value = np.sin(argument)\n        else:\n            value = np.cos(argument)\n        encoding.append(value)\n    encoding = np.array(encoding, dtype=np.float16).tolist()\n    return encoding"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    diff = training_accuracy - test_accuracy\n    if diff > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * grad ** 2\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    m_corrected = new_m / (1 - beta1_power)\n    v_corrected = new_v / (1 - beta2_power)\n    step = learning_rate * m_corrected / np.sqrt(v_corrected + epsilon)\n    updated_parameter = parameter - step\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    new_m = np.round(new_m, 5).tolist()\n    new_v = np.round(new_v, 5).tolist()\n    return (updated_parameter, new_m, new_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    tokens = encoder.encode(prompt)\n    generated_tokens = list(tokens)\n    for _ in range(n_tokens_to_generate):\n        if len(generated_tokens) >= hparams['n_ctx']:\n            input_tokens = generated_tokens[-hparams['n_ctx']:]\n        else:\n            input_tokens = generated_tokens + [0] * (hparams['n_ctx'] - len(generated_tokens))\n        token_embeddings = np.array([params['wte'][t] for t in input_tokens])\n        positional_embeddings = params['wpe'][:len(input_tokens)]\n        embeddings = token_embeddings + positional_embeddings\n        mean = np.mean(embeddings, axis=-1, keepdims=True)\n        var = np.var(embeddings, axis=-1, keepdims=True)\n        x_normalized = (embeddings - mean) / np.sqrt(var + 1e-08) * params['ln_f']['g'] + params['ln_f']['b']\n        logits = np.dot(x_normalized, params['wte'].T)\n        last_logits = logits[-1]\n        next_token = np.argmax(last_logits)\n        generated_tokens.append(next_token)\n    return encoder.decode(generated_tokens)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum()\n    result = []\n    for i in range(n):\n        scores = []\n        for j in range(n):\n            dot_product = sum((a * b for (a, b) in zip(crystal_values[i], crystal_values[j])))\n            scores.append(dot_product)\n        weights = softmax(scores)\n        weighted_sum = [0.0] * dimension\n        for j in range(n):\n            weight = weights[j]\n            for k in range(dimension):\n                weighted_sum[k] += crystal_values[j][k] * weight\n        output = sum(weighted_sum)\n        result.append(round(output, 4))\n    return result"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    docs_terms = [Counter(doc.split()) for doc in corpus]\n    L_d_list = [len(doc.split()) for doc in corpus]\n    if L_d_list:\n        L_avg = sum(L_d_list) / len(L_d_list)\n    else:\n        L_avg = 0\n    term_to_doc_count = {}\n    for doc in docs_terms:\n        for term in doc:\n            term_to_doc_count[term] = term_to_doc_count.get(term, 0) + 1\n    query_terms = query.split()\n    n_t_dict = {term: term_to_doc_count.get(term, 0) for term in query_terms}\n    scores = []\n    N = len(corpus)\n    for (doc_terms, L_d) in zip(docs_terms, L_d_list):\n        score = 0.0\n        for term in query_terms:\n            if term in doc_terms:\n                tf = doc_terms[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1\n                term_score = numerator / denominator\n                n_t = n_t_dict[term]\n                if n_t == 0:\n                    idf = 0.0\n                else:\n                    idf = np.log((N - n_t + 1) / n_t)\n                score += term_score * idf\n        if L_avg == 0:\n            length_part = 0.0\n        else:\n            length_part = b * np.log(L_d / L_avg)\n        total_score = score + length_part\n        scores.append(round(total_score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "from collections import defaultdict\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = defaultdict(int)\n    fp = defaultdict(int)\n    fn = defaultdict(int)\n    for (t, p) in zip(y_true, y_pred):\n        if t == p:\n            tp[t] += 1\n        else:\n            fp[p] += 1\n            fn[t] += 1\n    total_tp = sum(tp.values())\n    total_fp = sum(fp.values())\n    total_fn = sum(fn.values())\n    precision = total_tp / (total_tp + total_fp) if total_tp + total_fp != 0 else 0\n    recall = total_tp / (total_tp + total_fn) if total_tp + total_fn != 0 else 0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended = []\n    for i in range(10):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(consumption_data[i] - fluctuation)\n    x = list(range(1, 11))\n    y = detrended\n    n = len(x)\n    mean_x = sum(x) / n\n    mean_y = sum(y) / n\n    numerator = 0\n    denominator = 0\n    for (xi, yi) in zip(x, y):\n        numerator += (xi - mean_x) * (yi - mean_y)\n        denominator += (xi - mean_x) ** 2\n    a = numerator / denominator if denominator != 0 else 0\n    b = mean_y - a * mean_x\n    x_pred = 15\n    y_pred = a * x_pred + b\n    fluctuation_day15 = 10 * math.sin(2 * PI * 15 / 10)\n    y_pred += fluctuation_day15\n    rounded = round(y_pred)\n    safety_added = rounded * 1.05\n    final_value = math.ceil(safety_added)\n    return final_value"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_diffs = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_diffs)\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    d_model = X.shape[2]\n    d_head = W_q.shape[1] // n_heads\n    qkv_list = []\n    for h in range(n_heads):\n        Wq = W_q[:, h * d_head:(h + 1) * d_head]\n        Wk = W_k[:, h * d_head:(h + 1) * d_head]\n        Wv = W_v[:, h * d_head:(h + 1) * d_head]\n        q = X @ Wq\n        k = X @ Wk\n        v = X @ Wv\n        qkv_list.append((q, k, v))\n    return qkv_list\ndef self_attention(q, k, v):\n    d_head = q.shape[2]\n    scores = q @ k.T / np.sqrt(d_head)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=2, keepdims=True)\n    output = (v.T @ attention_weights.T).T\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    qkv_list = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    head_outputs = []\n    for (q, k, v) in qkv_list:\n        output = self_attention(q, k, v)\n        head_outputs.append(output)\n    result = []\n    for out in head_outputs:\n        rounded = np.round(out, 4)\n        result.append(rounded.tolist())\n    return result"}
{"task_id": 95, "completion_id": 0, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n11 = 0\n    n10 = 0\n    n01 = 0\n    n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:\n            n00 += 1\n    row1 = n11 + n10\n    row2 = n01 + n00\n    col1 = n11 + n01\n    col2 = n10 + n00\n    denominator_squared = row1 * row2 * col1 * col2\n    if denominator_squared == 0:\n        return 0.0\n    else:\n        denominator = math.sqrt(denominator_squared)\n        numerator = n11 * n00 - n10 * n01\n        phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(x, alpha * x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 20:\n        value = x + math.exp(-x)\n    elif x < -20:\n        value = math.exp(x)\n    else:\n        value = math.log(1 + math.exp(x))\n    return round(value, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rho = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    first_term = np.mean(clipped_rho * A)\n    sum_kl = 0.0\n    for (rho, pi_old, pi_ref) in zip(rhos, pi_theta_old, pi_theta_ref):\n        if rho == 0 or pi_old == 0 or pi_ref == 0:\n            continue\n        term = rho * pi_old * (np.log(rho) + np.log(pi_old) - np.log(pi_ref))\n        sum_kl += term\n    kl_penalty = beta * sum_kl\n    total = first_term + kl_penalty\n    return round(total, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    beta = 1.0\n    sigmoid = 1 / (1 + math.exp(-beta * x))\n    result = x * sigmoid\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = X.dot(weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    M = X.shape[1]\n    K = len(np.unique(y))\n    N = X.shape[0]\n    B = np.random.randn(M, K) * 0.01\n    y_one_hot = np.zeros((N, K))\n    y_one_hot[np.arange(N), y] = 1\n    losses = []\n    for _ in range(iterations):\n        z = X.dot(B)\n        z -= np.max(z, axis=1, keepdims=True)\n        p = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n        loss = -np.mean(np.log(p[np.arange(N), y]))\n        losses.append(round(loss, 4))\n        dz = p - y_one_hot\n        dW = X.T.dot(dz) / N\n        B -= learning_rate * dW\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    w = np.zeros(n)\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, w)\n        a = 1 / (1 + np.exp(-z))\n        loss = (-y * np.log(a) - (1 - y) * np.log(1 - a)).mean()\n        losses.append(loss)\n        gradient = X.T.dot(a - y) / m\n        w = w - learning_rate * gradient\n    w_rounded = np.round(w, 4).tolist()\n    losses_rounded = [np.round(loss, 4) for loss in losses]\n    return (w_rounded, losses_rounded)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.einsum('bnd,bmd->bnm', Q, K)\n    d_k = K.shape[-1]\n    scores = scores / np.sqrt(d_k)\n    mask = mask[np.newaxis, :, :]\n    scores = scores + (1 - mask) * -1000000000.0\n    exp_scores = np.exp(scores)\n    sum_exp = np.sum(exp_scores, axis=2, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    output = np.einsum('bnm,bmd->bnd', attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    n = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        p = count / n\n        entropy += -p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=2, keepdims=True)\n    X_centered = X - mean\n    var = np.mean(X_centered ** 2, axis=2, keepdims=True)\n    var += epsilon\n    std = np.sqrt(var)\n    X_normalized = X_centered / std\n    result = gamma * X_normalized + beta\n    return np.round(result, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    if not candidate or not reference:\n        return 0.0\n    ref_words = reference.split()\n    candidate_words = candidate.split()\n    ref_counts = Counter(ref_words)\n    candidate_counts = Counter(candidate_words)\n    correct_matches = 0\n    for word in candidate_counts:\n        correct_matches += min(ref_counts.get(word, 0), candidate_counts[word])\n\n    def lcs(a, b):\n        m = len(a)\n        n = len(b)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m):\n            for j in range(n):\n                if a[i] == b[j]:\n                    dp[i + 1][j + 1] = dp[i][j] + 1\n                else:\n                    dp[i + 1][j + 1] = max(dp[i + 1][j], dp[i][j + 1])\n        return dp[m][n]\n    correctly_ordered = lcs(candidate_words, ref_words)\n    if correct_matches == 0:\n        penalty = 0.0\n    else:\n        penalty = (correct_matches - correctly_ordered) / correct_matches\n    len_candidate = len(candidate_words)\n    len_ref = len(ref_words)\n    p = correct_matches / len_candidate if len_candidate > 0 else 0.0\n    r = correct_matches / len_ref if len_ref > 0 else 0.0\n    if alpha + beta == 0:\n        f_mean = 0.0\n    else:\n        f_mean = (alpha * p + beta * r) / (alpha + beta)\n    meteor = f_mean - gamma * penalty\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x == 0 or p_y == 0:\n        return 0.0\n    denominator = p_x * p_y\n    ratio = p_xy / denominator\n    pmi = np.log2(ratio)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    if max_x == min_x:\n        return [0.0 for _ in x]\n    else:\n        normalized = [(xi - min_x) / (max_x - min_x) for xi in x]\n        return [round(num, 4) for num in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    main = x.dot(w1)\n    main = np.maximum(0, main)\n    main = main.dot(w2)\n    main = np.maximum(0, main)\n    output = main + x\n    output = np.maximum(0, output)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    reshaped = x.reshape(-1, x.shape[2])\n    return reshaped.mean(axis=0)"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    X_normalized = (X - mean[:, None, None, None]) / np.sqrt(var[:, None, None, None] + epsilon)\n    output = gamma[:, None, None, None] * X_normalized + beta[:, None, None, None]\n    output = np.round(output, decimals=4)\n    return output.tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    if n == 0:\n        return 0.0\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=np.float64)\n        u = v.copy()\n        for u_i in basis:\n            proj = np.dot(u, u_i) * u_i\n            u = u - proj\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            u = u / norm\n            basis.append(u)\n    result = []\n    for vec in basis:\n        rounded = np.round(vec, decimals=4)\n        result.append(rounded.tolist())\n    return result"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    rounded = np.round(cross, 4)\n    return rounded.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    n = len(A)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x[i] = det_Ai / det_A\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    sum_bhatta = 0.0\n    for (pi, qi) in zip(p, q):\n        sum_bhatta += np.sqrt(pi * qi)\n    distance = -sum_bhatta\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    total_steps = 0\n    for episode in episodes:\n        rewards = [r for (s, a, r) in episode]\n        n = len(rewards)\n        returns = [0.0] * n\n        returns[-1] = rewards[-1]\n        for t in range(n - 2, -1, -1):\n            returns[t] = rewards[t] + returns[t + 1]\n        for t in range(n):\n            (s, a, r) = episode[t]\n            state_theta = theta[s, :]\n            max_theta = np.max(state_theta)\n            exp_theta = np.exp(state_theta - max_theta)\n            sum_exp = np.sum(exp_theta)\n            p = exp_theta / sum_exp\n            gradient_log = -p\n            gradient_log[a] += 1\n            gradient_contribution = gradient_log * returns[t]\n            gradient[s, :] += gradient_contribution\n        total_steps += n\n    if total_steps == 0:\n        return gradient.tolist()\n    gradient /= total_steps\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active / n_experts * flops_dense\n    saving = (flops_dense - flops_moe) / flops_dense * 100\n    return round(saving, 1)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    s = X.dot(W_g)\n    s_noisy = s + N\n    batch_size = X.shape[0]\n    num_experts = W_g.shape[1]\n    probs = np.zeros((batch_size, num_experts))\n    for i in range(batch_size):\n        s_i = s_noisy[i]\n        sorted_indices = np.argsort(s_i)[::-1]\n        top_k_indices = sorted_indices[:k]\n        sum_top = s_i[top_k_indices].sum()\n        if sum_top == 0:\n            probs[i, top_k_indices] = 1.0 / k\n        else:\n            probs[i, top_k_indices] = s_i[top_k_indices] / sum_top\n    probs_rounded = np.round(probs, 4)\n    return probs_rounded.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    g = x.dot(Wg)\n    p = np.exp(g) / np.exp(g).sum(axis=1, keepdims=True)\n    output_dim = We[0].shape[1]\n    output = np.zeros((x.shape[0], output_dim))\n    for i in range(x.shape[0]):\n        expert_probs = p[i]\n        sorted_indices = np.argsort(expert_probs)[::-1]\n        top_k_indices = sorted_indices[:top_k]\n        selected_We = We[top_k_indices]\n        x_i = x[i]\n        expert_outputs = x_i.dot(selected_We)\n        selected_probs = expert_probs[top_k_indices]\n        output[i] = (expert_outputs * selected_probs[:, np.newaxis]).sum(axis=0)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    C_per_group = C // num_groups\n    X_reshaped = X.reshape((B, num_groups, C_per_group, H, W))\n    for i in range(num_groups):\n        group = X_reshaped[:, i, :, :, :]\n        mean = group.mean(axis=(1, 2, 3), keepdims=True)\n        var = group.var(axis=(1, 2, 3), keepdims=True)\n        normalized = (group - mean) / np.sqrt(var + epsilon)\n        gamma_slice = gamma[i * C_per_group:(i + 1) * C_per_group]\n        beta_slice = beta[i * C_per_group:(i + 1) * C_per_group]\n        gamma_reshaped = gamma_slice.reshape((1, 1, C_per_group, 1, 1))\n        beta_reshaped = beta_slice.reshape((1, 1, C_per_group, 1, 1))\n        normalized = normalized * gamma_reshaped + beta_reshaped\n        X_reshaped[:, i, :, :, :] = normalized\n    X_normalized = X_reshaped.reshape((B, C, H, W))\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        current_x = x\n        derivative = 4 * current_x ** 3 - 9 * current_x ** 2\n        x = current_x - learning_rate * derivative\n        if abs(x - current_x) < tolerance:\n            break\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    y = alpha * np.tanh(gamma * x) + beta\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()"}
