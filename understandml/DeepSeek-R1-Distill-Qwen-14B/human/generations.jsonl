{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the dot product of a matrix and a vector. The function should return the resulting vector if the dimensions are compatible, or -1 if they are not.\n\n### Approach\n1. **Check for Empty Matrix**: If the matrix is empty, check if the vector is also empty. If both are empty, return an empty list. Otherwise, return -1.\n2. **Dimension Check**: For a non-empty matrix, determine the number of columns in the matrix. If the number of columns does not match the length of the vector, return -1.\n3. **Compute Dot Product**: For each row in the matrix, compute the dot product with the vector. The dot product is calculated as the sum of the products of corresponding elements from the row and the vector.\n\n### Solution Code\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    if not a:\n        if not b:\n            return []\n        else:\n            return -1\n    else:\n        cols_a = len(a[0])\n        if cols_a != len(b):\n            return -1\n        result = []\n        for row in a:\n            dp = 0\n            for i in range(cols_a):\n                dp += row[i] * b[i]\n            result.append(dp)\n        return result\n```\n\n### Explanation\n1. **Empty Matrix Handling**: The function first checks if the matrix is empty. If it is, it then checks if the vector is also empty. If both are empty, it returns an empty list. If the matrix is empty but the vector is not, it returns -1.\n2. **Dimension Compatibility**: For a non-empty matrix, the function checks if the number of columns in the matrix matches the length of the vector. If they do not match, it returns -1.\n3. **Dot Product Calculation**: For each row in the matrix, the function calculates the dot product with the vector by iterating through each element, multiplying corresponding elements, and summing the results. The resulting values are collected into a list which is then returned.\n\nThis approach ensures that the function handles edge cases such as empty matrices and vectors, and correctly computes the dot product for valid dimensions."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transpose of a given matrix. The transpose of a matrix is obtained by converting its rows into columns and its columns into rows. This means that the element at position (i, j) in the original matrix will be at position (j, i) in the transposed matrix.\n\n### Approach\nThe approach to solve this problem efficiently involves using Python's built-in `zip` function. The `zip` function is particularly useful for transposing matrices because it can group elements by their column indices when applied to the rows of the matrix. Here's a step-by-step breakdown of the approach:\n\n1. **Understanding the Transpose**: The transpose of a matrix is formed by turning rows into columns and columns into rows. For example, the element at (i, j) in the original matrix will be at (j, i) in the transposed matrix.\n2. **Using `zip` Function**: The `zip` function in Python takes multiple iterables and returns an iterator of tuples, where each tuple contains the ith element from each iterable. When applied to the rows of the matrix, `zip` effectively groups elements by their column indices, which is exactly what we need for transposing.\n3. **Converting Tuples to Lists**: The `zip` function returns tuples, but we need the result to be a list of lists. Therefore, each tuple returned by `zip` is converted into a list.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    return [list(row) for row in zip(*a)]\n```\n\n### Explanation\n- **Input Handling**: The function takes a matrix `a` as input, which is a list of lists where each inner list represents a row of the matrix.\n- **Transposing Using `zip`**: The expression `zip(*a)` transposes the matrix by grouping elements from each row by their column indices. This results in tuples where each tuple represents a row in the transposed matrix.\n- **Conversion to List of Lists**: Each tuple returned by `zip` is converted into a list using a list comprehension, resulting in the final transposed matrix.\n\nThis approach is efficient and concise, leveraging Python's built-in functions to achieve the desired result with minimal code."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to reshape a given matrix into a specified shape. If the matrix cannot be reshaped into the given shape, we should return an empty list.\n\n### Approach\n1. **Check Reshaping Feasibility**: The total number of elements in the original matrix must match the total number of elements in the new shape. This is calculated by summing the lengths of all rows in the original matrix and comparing it with the product of the dimensions of the new shape.\n2. **Convert to Numpy Array**: If reshaping is feasible, convert the original matrix into a numpy array for efficient manipulation.\n3. **Reshape the Array**: Use numpy's reshape function to transform the array into the new shape.\n4. **Convert Back to List**: Convert the reshaped numpy array back to a Python list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Calculate the total number of elements in the original matrix\n    total_elements = sum(len(row) for row in a)\n    # Calculate the total number of elements required for the new shape\n    new_total = new_shape[0] * new_shape[1]\n    \n    # Check if reshaping is possible\n    if total_elements != new_total:\n        return []\n    \n    # Convert the list of lists to a numpy array\n    arr = np.array(a)\n    # Reshape the array to the new shape\n    reshaped_arr = arr.reshape(new_shape)\n    # Convert the numpy array back to a list of lists\n    return reshaped_arr.tolist()\n```\n\n### Explanation\n1. **Feasibility Check**: The function first calculates the total number of elements in the original matrix by summing the lengths of each row. It then checks if this total matches the product of the dimensions of the new shape. If not, it returns an empty list.\n2. **Numpy Conversion**: The original matrix is converted into a numpy array to leverage numpy's efficient array operations.\n3. **Reshaping**: The numpy array is reshaped into the specified new dimensions using numpy's reshape function.\n4. **Conversion Back to List**: The reshaped numpy array is converted back to a Python list of lists using the tolist() method and returned.\n\nThis approach ensures that the matrix is reshaped efficiently and correctly, handling all edge cases such as empty matrices and varying row lengths appropriately."}
{"task_id": 4, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates the mean of a matrix either by row or by column, based on a given mode. The function will take a matrix (a list of lists of floats) and a mode ('row' or 'column') as input and return a list of means according to the specified mode.\n\n### Approach\nThe approach to solve this problem involves two main steps based on the mode provided:\n\n1. **Row-wise Calculation**: If the mode is 'row', we iterate through each row of the matrix, compute the mean of the elements in that row, and collect these means into a result list.\n\n2. **Column-wise Calculation**: If the mode is 'column', we use the `zip` function to transpose the matrix, effectively converting columns into tuples. We then iterate through each of these transposed columns, compute the mean of the elements in each column, and collect these means into a result list.\n\nThis approach ensures that we efficiently compute the required means using list comprehensions and the `zip` function, which simplifies the process of handling columns.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    else:\n        return [sum(col) / len(col) for col in zip(*matrix)]\n```\n\n### Explanation\n- **Row-wise Calculation**: For each row in the matrix, the sum of the elements is divided by the number of elements in the row to compute the mean. This is done using a list comprehension that iterates over each row.\n  \n- **Column-wise Calculation**: The `zip(*matrix)` expression transposes the matrix, converting each column into a tuple. We then iterate over each of these tuples, compute the sum of the elements, divide by the number of elements to get the mean, and collect these means into a list.\n\nThis solution efficiently handles both row and column-wise mean calculations using concise and readable Python constructs, ensuring that the function works correctly for any valid input matrix."}
{"task_id": 5, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies a given matrix by a scalar value and returns the resulting matrix. Scalar multiplication of a matrix involves multiplying each element of the matrix by the scalar value.\n\n### Approach\n1. **Iterate through each row**: The matrix is a 2D list, so we will loop through each row in the matrix.\n2. **Iterate through each element in the row**: For each row, we will loop through each element.\n3. **Multiply each element by the scalar**: Each element in the row is multiplied by the scalar value.\n4. **Construct the resulting matrix**: Create a new row with the multiplied values and add it to the resulting matrix.\n5. **Return the resulting matrix**: After processing all rows, return the new matrix.\n\nThis approach ensures that each element of the matrix is processed exactly once, making the solution efficient with a time complexity of O(n*m), where n is the number of rows and m is the number of columns in the matrix.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result\n```\n\n### Explanation\n- **Initialization**: We start by initializing an empty list `result` to store the resulting matrix.\n- **Row Processing**: For each row in the input matrix, we create a new row where each element is the product of the original element and the scalar.\n- **Building the Result Matrix**: Each new row is appended to the `result` list.\n- **Return**: Finally, the constructed `result` matrix is returned.\n\nThis solution efficiently handles all edge cases, including empty matrices and matrices with varying row lengths, ensuring correct scalar multiplication for each element."}
{"task_id": 6, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest. Eigenvalues are crucial in various applications, including solving systems of linear equations, analyzing matrix properties, and more.\n\n### Approach\nThe approach to find the eigenvalues of a 2x2 matrix involves the following steps:\n\n1. **Extract Matrix Elements**: Identify the elements of the 2x2 matrix, which are typically denoted as `a`, `b`, `c`, and `d`.\n2. **Compute Trace and Determinant**: The trace of the matrix is the sum of the diagonal elements (a + d), and the determinant is calculated as (ad - bc).\n3. **Form the Characteristic Equation**: The eigenvalues are found by solving the quadratic equation derived from the characteristic equation, which is given by \u03bb\u00b2 - trace(A)\u03bb + determinant(A) = 0.\n4. **Solve the Quadratic Equation**: Using the quadratic formula, the roots (eigenvalues) are computed as (trace \u00b1 sqrt(trace\u00b2 - 4 * determinant)) / 2.\n5. **Sort Eigenvalues**: The computed eigenvalues are then sorted in descending order to meet the problem's requirements.\n\n### Solution Code\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    \n    sqrt_discriminant = math.sqrt(discriminant)\n    eigen1 = (trace + sqrt_discriminant) / 2\n    eigen2 = (trace - sqrt_discriminant) / 2\n    \n    eigenvalues = [eigen1, eigen2]\n    eigenvalues.sort(reverse=True)\n    \n    return eigenvalues\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The elements `a`, `b`, `c`, and `d` are extracted from the given 2x2 matrix.\n2. **Compute Trace and Determinant**: The trace is the sum of the diagonal elements, and the determinant is calculated using the formula for a 2x2 matrix.\n3. **Quadratic Formula**: The eigenvalues are found using the quadratic formula, which involves computing the discriminant (trace squared minus four times the determinant) and then finding its square root.\n4. **Sort Eigenvalues**: The eigenvalues are sorted in descending order to ensure the result meets the specified format.\n\nThis approach efficiently computes the eigenvalues and ensures they are returned in the required order, handling both real and complex cases as needed."}
{"task_id": 7, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to transform a given matrix \\( A \\) using the operation \\( T^{-1}AS \\), where \\( T \\) and \\( S \\) are invertible matrices. The function should first validate if \\( T \\) and \\( S \\) are invertible and then perform the transformation. If any matrix is not invertible, the function should return -1.\n\n### Approach\n1. **Check Invertibility**: A matrix is invertible if its determinant is non-zero. We first check if \\( T \\) and \\( S \\) are square matrices and compute their determinants. If either determinant is zero (within a small tolerance to handle floating-point precision issues), the matrix is not invertible, and we return -1.\n2. **Matrix Inversion**: Compute the inverse of matrix \\( T \\) using numpy's `linalg.inv` function.\n3. **Matrix Multiplication**: Perform the matrix multiplications \\( T^{-1}A \\) and then multiply the result by \\( S \\) to get the transformed matrix.\n4. **Rounding and Conversion**: Round the resulting matrix elements to four decimal places and convert the numpy array to a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Calculate determinants of T and S\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    \n    # Check if determinants are effectively zero (using a small epsilon)\n    epsilon = 1e-10\n    if abs(det_T) < epsilon or abs(det_S) < epsilon:\n        return -1\n    \n    # Compute the inverse of T\n    T_inv = np.linalg.inv(T_np)\n    \n    # Perform the matrix transformation T^{-1} A S\n    try:\n        temp = T_inv @ A_np\n        result = temp @ S_np\n    except np.linalg.LinAlgError:\n        # In case of any matrix multiplication error (though T and S are invertible)\n        return -1\n    \n    # Round the result to 4 decimal places\n    rounded_result = np.round(result, 4)\n    \n    # Convert the numpy array to a Python list\n    return rounded_result.tolist()\n```\n\n### Explanation\n1. **Invertibility Check**: The function first checks if \\( T \\) and \\( S \\) are square matrices and computes their determinants. If either determinant is close to zero, the function returns -1.\n2. **Matrix Inversion**: The inverse of \\( T \\) is computed to perform the transformation.\n3. **Matrix Multiplication**: The transformation \\( T^{-1}AS \\) is performed step-by-step, first multiplying \\( T^{-1} \\) with \\( A \\), and then the result with \\( S \\).\n4. **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach ensures that the transformation is only performed when both \\( T \\) and \\( S \\) are invertible, and the result is accurately rounded and converted for the required output format."}
{"task_id": 8, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the inverse of a 2x2 matrix. If the matrix is not invertible, we should return `None`.\n\n### Approach\n1. **Extract Elements**: Extract the elements of the 2x2 matrix into variables `a`, `b`, `c`, and `d`.\n2. **Calculate Determinant**: Compute the determinant of the matrix using the formula `det = a*d - b*c`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is not invertible, so return `None`.\n4. **Compute Inverse**: If the determinant is non-zero, compute the inverse matrix using the formula:\n   \\[\n   \\text{inverse} = \\frac{1}{\\text{det}} \\times \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n   \\]\n5. **Return Result**: Return the computed inverse matrix.\n\n### Solution Code\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    inverse = [\n        [d * inv_det, -b * inv_det],\n        [-c * inv_det, a * inv_det]\n    ]\n    return inverse\n```\n\n### Explanation\n- **Extract Elements**: The elements of the matrix are extracted into variables `a`, `b`, `c`, and `d` for easier manipulation.\n- **Calculate Determinant**: The determinant is calculated to check if the matrix is invertible. A determinant of zero means the matrix is singular and cannot be inverted.\n- **Check Invertibility**: If the determinant is zero, the function returns `None` as the matrix is not invertible.\n- **Compute Inverse**: If the determinant is non-zero, the inverse matrix is computed using the formula for the inverse of a 2x2 matrix. Each element of the resulting matrix is scaled by the reciprocal of the determinant.\n- **Return Result**: The computed inverse matrix is returned as a list of lists of floats.\n\nThis approach ensures that we correctly handle both invertible and non-invertible matrices, providing the correct result or `None` as required."}
{"task_id": 9, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies two matrices. If the matrices cannot be multiplied, the function should return -1. Matrix multiplication is only possible if the number of columns in the first matrix is equal to the number of rows in the second matrix. \n\n### Approach\n1. **Check for Empty Matrices**: If either of the input matrices is empty, return -1 immediately.\n2. **Validate Matrix Dimensions**: Ensure that each row in both matrices has the same length as the first row. If any row does not meet this condition, return -1.\n3. **Check Multiplication Compatibility**: Verify that the number of columns in the first matrix is equal to the number of rows in the second matrix. If not, return -1.\n4. **Initialize Result Matrix**: Create a result matrix with dimensions equal to the number of rows of the first matrix and the number of columns of the second matrix, initialized with zeros.\n5. **Compute Matrix Product**: Use nested loops to compute each element of the result matrix as the dot product of the corresponding row from the first matrix and column from the second matrix.\n\n### Solution Code\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if either matrix is empty\n    if not a or not b:\n        return -1\n    \n    m = len(a)\n    n = len(a[0]) if m > 0 else 0\n    p = len(b)\n    q = len(b[0]) if p > 0 else 0\n    \n    # Check if any row in a has length different from n\n    for row in a:\n        if len(row) != n:\n            return -1\n    \n    # Check if any row in b has length different from q\n    for row in b:\n        if len(row) != q:\n            return -1\n    \n    # Check if multiplication is possible\n    if n != p:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(q)] for _ in range(m)]\n    \n    # Compute each element of the result matrix\n    for i in range(m):\n        for j in range(q):\n            total = 0\n            for k in range(n):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    \n    return result\n```\n\n### Explanation\n1. **Empty Matrix Check**: The function first checks if either matrix is empty. If so, it returns -1.\n2. **Dimension Validation**: It then checks if all rows in each matrix have the same length as the first row. This ensures that the matrices are valid.\n3. **Compatibility Check**: The function checks if the number of columns in the first matrix matches the number of rows in the second matrix. If not, it returns -1.\n4. **Result Matrix Initialization**: A result matrix is initialized with zeros, with dimensions based on the input matrices.\n5. **Matrix Multiplication**: Using nested loops, each element of the result matrix is computed as the sum of products of corresponding elements from the rows of the first matrix and columns of the second matrix.\n\nThis approach ensures that the function handles all edge cases and correctly computes the matrix product when possible."}
{"task_id": 10, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the covariance matrix for a given set of vectors. The covariance matrix is a square matrix where each element (i, j) represents the covariance between the i-th and j-th features. \n\n### Approach\n1. **Input Structure**: The input is a list of lists, where each inner list represents a feature vector with its observations.\n2. **Covariance Calculation**: The covariance between two features is calculated using the formula:\n   \\[\n   \\text{cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\n   \\]\n   where \\( \\bar{X} \\) and \\( \\bar{Y} \\) are the means of the respective features, and \\( n \\) is the number of observations.\n3. **Efficiency**: To optimize the computation, we precompute the mean of each feature vector once and reuse it for all covariance calculations.\n4. **Matrix Construction**: Construct a k x k matrix where each element (i, j) is the covariance between the i-th and j-th features.\n\n### Solution Code\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors:\n        return []\n    k = len(vectors)\n    n = len(vectors[0])\n    means = [sum(feature) / n for feature in vectors]\n    cov_matrix = [[0.0 for _ in range(k)] for _ in range(k)]\n    for i in range(k):\n        for j in range(k):\n            x = vectors[i]\n            y = vectors[j]\n            mean_x = means[i]\n            mean_y = means[j]\n            sum_product = 0.0\n            for a, b in zip(x, y):\n                sum_product += (a - mean_x) * (b - mean_y)\n            cov = sum_product / (n - 1)\n            cov_matrix[i][j] = cov\n    return cov_matrix\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input list is empty and returns an empty list if true.\n2. **Mean Calculation**: The mean of each feature vector is precomputed and stored in a list to avoid redundant calculations.\n3. **Covariance Matrix Initialization**: A k x k matrix is initialized with zeros, where k is the number of features.\n4. **Covariance Calculation**: For each pair of features (i, j), the covariance is computed using the precomputed means and the formula for covariance. The result is stored in the corresponding position in the covariance matrix.\n5. **Return Result**: The constructed covariance matrix is returned.\n\nThis approach ensures that the covariance matrix is computed efficiently and correctly, handling all edge cases such as empty input and single feature vectors."}
{"task_id": 11, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Jacobi method to iteratively solve a system of linear equations given by Ax = b. The Jacobi method is an iterative technique that updates each component of the solution vector based on the previous iteration's values. The goal is to perform a specified number of iterations, rounding each intermediate solution to four decimal places, and return the approximate solution as a list.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the solution vector, typically an array of zeros.\n2. **Iteration**: For each iteration, compute a new solution vector using the previous iteration's values. Each component of the new solution vector is calculated by solving the corresponding equation using the previous values of all other components.\n3. **Update and Rounding**: After computing the new solution vector for each iteration, update the solution vector and round it to four decimal places.\n4. **Termination**: After completing the specified number of iterations, return the solution vector as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    m = A.shape[0]  # Number of equations\n    x_old = np.zeros(m)  # Initial guess\n    \n    for _ in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            sum_val = 0.0\n            for j in range(m):\n                if j != i:\n                    sum_val += A[i, j] * x_old[j]\n            x_new[i] = (b[i] - sum_val) / A[i, i]\n        # Update x_old and round to 4 decimal places\n        x_old = x_new.copy()\n        x_old = np.round(x_old, 4)\n    \n    return x_old.tolist()\n```\n\n### Explanation\n1. **Initialization**: The solution vector `x_old` is initialized to an array of zeros with the same length as the number of equations.\n2. **Iteration**: For each iteration, a new solution vector `x_new` is computed. Each component `x_new[i]` is calculated using the formula derived from the Jacobi method, which involves summing the products of the matrix elements and the previous solution values, then dividing by the diagonal element of the matrix.\n3. **Update and Rounding**: After computing the new solution vector, it is assigned to `x_old` and rounded to four decimal places to ensure precision and prevent numerical errors from accumulating.\n4. **Termination**: After completing the specified number of iterations, the solution vector is converted to a list and returned.\n\nThis approach ensures that each iteration uses the most recent values of all components, leading to a convergent solution towards the true values of the system of linear equations."}
{"task_id": 12, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to approximate the Singular Value Decomposition (SVD) of a 2x2 matrix without using numpy's built-in SVD function. The goal is to compute the singular values of the matrix, which are the square roots of the eigenvalues of the matrix multiplied by its transpose.\n\n### Approach\n1. **Compute the Transpose and Product**: First, compute the product of the matrix with its transpose, denoted as \\( A^T A \\). This product is a symmetric matrix.\n2. **Extract Elements**: Extract the elements of the resulting matrix \\( A^T A \\) to form a 2x2 matrix.\n3. **Calculate Trace and Determinant**: Compute the trace (sum of diagonal elements) and the determinant of the matrix \\( A^T A \\).\n4. **Find Eigenvalues**: Use the quadratic formula to find the eigenvalues of \\( A^T A \\). The eigenvalues are derived from the characteristic equation of the matrix.\n5. **Compute Singular Values**: The singular values are the square roots of the eigenvalues.\n6. **Sort and Round**: Sort the singular values in descending order and round them to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    A_T_A = A.T @ A\n    p = A_T_A[0, 0]\n    q = A_T_A[0, 1]\n    r = A_T_A[1, 1]\n    \n    trace = p + r\n    discriminant = (p - r) ** 2 + 4 * q ** 2\n    sqrt_discriminant = np.sqrt(discriminant)\n    \n    lambda1 = (trace + sqrt_discriminant) / 2\n    lambda2 = (trace - sqrt_discriminant) / 2\n    \n    singular_values = [np.sqrt(lambda1), np.sqrt(lambda2)]\n    singular_values.sort(reverse=True)\n    \n    # Round to four decimal places\n    singular_values = [round(sv, 4) for sv in singular_values]\n    \n    return tuple(singular_values)\n```\n\n### Explanation\n1. **Matrix Product**: The matrix \\( A^T A \\) is computed to ensure it is symmetric, which simplifies finding eigenvalues.\n2. **Eigenvalue Calculation**: Using the trace and determinant of \\( A^T A \\), we solve the characteristic equation to find the eigenvalues. The eigenvalues are non-negative due to the properties of \\( A^T A \\).\n3. **Singular Values**: The singular values are obtained by taking the square roots of the eigenvalues, ensuring they are non-negative.\n4. **Sorting and Rounding**: The singular values are sorted in descending order and rounded to four decimal places for the final result.\n\nThis approach efficiently computes the singular values using basic linear algebra operations, avoiding the use of more complex numerical methods or built-in SVD functions."}
{"task_id": 13, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. The determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the matrix. Laplace's Expansion (cofactor expansion) is a method that allows us to compute the determinant by expanding along a row or column, which simplifies the problem into smaller subproblems.\n\n### Approach\n1. **Laplace Expansion**: We will expand the determinant along the first row of the matrix. This involves computing the minors for each element in the first row and then recursively calculating the determinants of these minors.\n2. **Minor Matrix**: For each element in the first row, we create a minor matrix by removing the corresponding row and column. This minor matrix will be a 3x3 matrix.\n3. **3x3 Determinant Calculation**: The determinant of each 3x3 minor matrix is computed using a helper function. This function applies the standard formula for the determinant of a 3x3 matrix.\n4. **Sign Alternation**: The sign for each term in the expansion alternates based on the column index. The sign is given by (-1)^(i+j), where i is the row index and j is the column index. Since we are expanding along the first row (i=0), the sign alternates as 1, -1, 1, -1 for the four elements.\n\n### Solution Code\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    def determinant_3x3(m):\n        a, b, c = m[0]\n        d, e, f = m[1]\n        g, h, i = m[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    \n    det = 0.0\n    for j in range(4):\n        element = matrix[0][j]\n        minor = [row[:j] + row[j+1:] for row in matrix[1:]]\n        minor_det = determinant_3x3(minor)\n        sign = (-1) ** j\n        det += element * sign * minor_det\n    return det\n```\n\n### Explanation\n1. **Helper Function for 3x3 Determinant**: The helper function `determinant_3x3` computes the determinant of a 3x3 matrix using the standard formula, which involves a combination of products of elements and their cofactors.\n2. **Laplace Expansion Along the First Row**: The main function `determinant_4x4` iterates over each element in the first row of the 4x4 matrix. For each element, it constructs the corresponding minor matrix by excluding the first row and the current column.\n3. **Recursive Calculation**: The determinant of each minor matrix is computed using the helper function. Each term in the expansion is then multiplied by the element, the sign based on its position, and the determinant of the minor.\n4. **Summing Terms**: The contributions from each term are summed to give the final determinant of the 4x4 matrix.\n\nThis approach efficiently breaks down the problem into smaller subproblems, leveraging the structure of matrix minors and cofactors to compute the determinant recursively."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform linear regression using the normal equation. The normal equation is a method to find the best fitting line for a linear regression model by solving the system of linear equations. The formula used is \u03b8 = (X^T X)^-1 X^T y, where \u03b8 represents the coefficients of the linear regression model.\n\n### Approach\n1. **Convert Input to Numpy Arrays**: Convert the input matrix X and vector y into numpy arrays to facilitate matrix operations.\n2. **Compute Transpose of X**: Calculate the transpose of the matrix X, denoted as X^T.\n3. **Compute X^T X**: Multiply the transpose of X by X to get the matrix X^T X.\n4. **Compute Inverse of X^T X**: Find the inverse of the matrix X^T X. This step assumes that the matrix is invertible.\n5. **Compute X^T y**: Multiply the transpose of X by the vector y.\n6. **Solve for \u03b8**: Multiply the inverse of X^T X by X^T y to get the coefficients \u03b8.\n7. **Round Results**: Round each element of the resulting coefficients to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    Xt = X_np.T\n    Xt_X = Xt @ X_np\n    inv_Xt_X = np.linalg.inv(Xt_X)\n    Xt_y = Xt @ y_np\n    theta = inv_Xt_X @ Xt_y\n    theta_rounded = [round(x, 4) for x in theta]\n    return theta_rounded\n```\n\n### Explanation\n1. **Convert Input**: The input features matrix X and target vector y are converted into numpy arrays for efficient matrix operations.\n2. **Transpose of X**: The transpose of X is computed to facilitate the multiplication needed for the normal equation.\n3. **Matrix Multiplication**: The product of the transpose of X and X is computed to form the matrix X^T X.\n4. **Inverse Calculation**: The inverse of the matrix X^T X is calculated to solve the system of linear equations.\n5. **Vector Multiplication**: The product of the transpose of X and y is computed to form the vector X^T y.\n6. **Solve for Coefficients**: The coefficients \u03b8 are found by multiplying the inverse of X^T X with X^T y.\n7. **Rounding**: The resulting coefficients are rounded to four decimal places to meet the output requirements.\n\nThis approach efficiently computes the coefficients of the linear regression model using the normal equation, ensuring accuracy and meeting the specified precision constraints."}
{"task_id": 15, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using the gradient descent algorithm. The goal is to find the coefficients of a linear regression model that best fit the given data by minimizing the cost function through iterative updates.\n\n### Approach\n1. **Problem Analysis**: The task involves using gradient descent to minimize the cost function (mean squared error) for linear regression. The input includes feature matrix `X` with an intercept column, target vector `y`, learning rate `alpha`, and number of iterations. The output should be the coefficients of the linear regression model, rounded to four decimal places.\n\n2. **Initialization**: Start with initializing the coefficients (`theta`) to zero. The shape of `theta` will match the number of features in `X`, including the intercept column.\n\n3. **Gradient Descent Iterations**: For each iteration, compute the hypothesis (predicted values) using the current coefficients. Calculate the error between the hypothesis and the actual target values. Update the coefficients by subtracting the product of the learning rate and the gradient of the cost function. The gradient is computed as the average of the product of the feature matrix transpose and the error vector.\n\n4. **Rounding and Conversion**: After completing the specified number of iterations, round the coefficients to four decimal places and convert them to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Initialize theta with zeros\n    theta = np.zeros(X.shape[1])\n    m = X.shape[0]  # Number of samples\n    \n    for _ in range(iterations):\n        # Compute the hypothesis\n        h = X.dot(theta)\n        # Calculate the error\n        error = h - y\n        # Compute the gradient\n        gradient = (X.T.dot(error)) / m\n        # Update theta\n        theta = theta - alpha * gradient\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    # Convert to list and return\n    return theta_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The coefficients `theta` are initialized to zero to start the optimization process.\n- **Hypothesis Calculation**: For each iteration, the hypothesis is computed using matrix multiplication of `X` and `theta`.\n- **Error Calculation**: The error is the difference between the hypothesis and the actual target values.\n- **Gradient Calculation**: The gradient is computed using the transpose of `X` multiplied by the error, scaled by the number of samples to get the average gradient.\n- **Coefficient Update**: The coefficients are updated in the direction that minimizes the cost function, using the learning rate `alpha` to control the step size.\n- **Rounding and Conversion**: After completing the iterations, the coefficients are rounded to four decimal places and converted to a list for the final output.\n\nThis approach efficiently minimizes the cost function using gradient descent, providing the optimal coefficients for the linear regression model."}
{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs feature scaling on a dataset using both standardization and min-max normalization. The function will take a 2D NumPy array as input and return two 2D lists: one scaled using standardization and the other using min-max normalization.\n\n### Approach\n1. **Standardization**: This technique transforms each feature to have a mean of 0 and a standard deviation of 1. The formula used is \\( z = \\frac{x - \\mu}{\\sigma} \\), where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the feature.\n2. **Min-Max Normalization**: This technique scales each feature to a fixed range, typically [0, 1]. The formula used is \\( y = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\), where min and max are the minimum and maximum values of the feature, respectively.\n\nThe function processes each column of the input array separately for both scaling techniques. After computing the scaled values, it rounds each value to four decimal places and constructs the resulting 2D lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Function to perform standardization\n    def standardize(column):\n        mean = np.mean(column)\n        std = np.std(column)\n        if std == 0:\n            return np.zeros_like(column)\n        return (column - mean) / std\n    \n    # Function to perform min-max normalization\n    def minmax(column):\n        min_val = np.min(column)\n        max_val = np.max(column)\n        if max_val == min_val:\n            return np.zeros_like(column)\n        return (column - min_val) / (max_val - min_val)\n    \n    # Process each column for standardization\n    columns_standard = []\n    for i in range(data.shape[1]):\n        col = data[:, i]\n        scaled_col = standardize(col)\n        scaled_col_rounded = np.round(scaled_col, 4)\n        columns_standard.append(scaled_col_rounded.tolist())\n    \n    # Transpose columns to get rows for standardization\n    scaled_standard = [list(row) for row in zip(*columns_standard)]\n    \n    # Process each column for min-max normalization\n    columns_minmax = []\n    for i in range(data.shape[1]):\n        col = data[:, i]\n        scaled_col = minmax(col)\n        scaled_col_rounded = np.round(scaled_col, 4)\n        columns_minmax.append(scaled_col_rounded.tolist())\n    \n    # Transpose columns to get rows for min-max normalization\n    scaled_minmax = [list(row) for row in zip(*columns_minmax)]\n    \n    return (scaled_standard, scaled_minmax)\n```\n\n### Explanation\n1. **Standardization**:\n   - For each column, compute the mean and standard deviation.\n   - Subtract the mean from each element and divide by the standard deviation.\n   - Handle cases where the standard deviation is zero by setting all values to zero.\n   - Round the results to four decimal places.\n\n2. **Min-Max Normalization**:\n   - For each column, compute the minimum and maximum values.\n   - Subtract the minimum value from each element and divide by the range (max - min).\n   - Handle cases where the range is zero by setting all values to zero.\n   - Round the results to four decimal places.\n\n3. **Constructing the Result**:\n   - After processing each column, transpose the list of columns to form the rows of the scaled datasets.\n   - Return the two scaled datasets as a tuple of lists.\n\nThis approach ensures that each feature is scaled appropriately, and the results are formatted as required."}
{"task_id": 17, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Means clustering algorithm, which partitions a given set of points into k clusters. The goal is to group similar points together and represent each cluster by its centroid. The algorithm starts with initial centroids and iteratively refines them until convergence or a specified maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Convert the input points and initial centroids into NumPy arrays for efficient computation.\n2. **Iteration**: For each iteration up to the maximum allowed:\n   - **Assignment**: Assign each point to the nearest centroid by computing the Euclidean distance from each point to each centroid.\n   - **Update**: Compute new centroids as the mean of the points assigned to each cluster.\n   - **Convergence Check**: If the centroids do not change between iterations, break early to save computation time.\n3. **Termination**: After the specified number of iterations or early convergence, round the final centroids to four decimal places and return them as a list of tuples.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to NumPy arrays for efficient computation\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        clusters = [[] for _ in range(k)]\n        for point in points_np:\n            # Calculate distances to each centroid\n            distances = np.sqrt(np.sum((point - centroids) ** 2, axis=1))\n            closest_index = np.argmin(distances)\n            clusters[closest_index].append(point)\n        \n        # Compute new centroids\n        new_centroids = []\n        for cluster in clusters:\n            if not cluster:\n                # Handle empty cluster by skipping (assuming no empty clusters in this problem)\n                pass\n            else:\n                new_centroid = np.mean(cluster, axis=0)\n                new_centroids.append(new_centroid)\n        \n        # Convert new centroids to a NumPy array\n        new_centroids_np = np.array(new_centroids)\n        \n        # Check if centroids have converged\n        if np.allclose(centroids, new_centroids_np):\n            break\n        centroids = new_centroids_np\n    \n    # Round the centroids to four decimal places and convert to list of tuples\n    rounded_centroids = [tuple(np.round(c, 4)) for c in centroids]\n    return rounded_centroids\n```\n\n### Explanation\n- **Initialization**: The points and initial centroids are converted into NumPy arrays to leverage efficient vectorized operations.\n- **Assignment**: For each point, the distance to each centroid is computed using vectorized operations, and the point is assigned to the nearest centroid.\n- **Update**: New centroids are computed as the mean of the points in each cluster. If a cluster becomes empty, it is skipped (though the problem assumes this does not happen).\n- **Convergence Check**: The algorithm stops early if the centroids do not change between iterations, optimizing performance.\n- **Termination**: The final centroids are rounded to four decimal places and returned as a list of tuples.\n\nThis approach ensures that the k-Means algorithm is implemented efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function for K-Fold Cross-Validation, which is a resampling method used to evaluate machine learning models. The idea is to split the dataset into k subsets, and for each subset, use it as a test set while the remaining subsets form the training set. This process is repeated k times, each time with a different test set.\n\n### Approach\n1. **Combine Indices**: Create an array of indices for the dataset to handle the splitting process.\n2. **Shuffle Indices**: If required, shuffle the indices to ensure randomness in the splits. This can be made reproducible by setting a random seed.\n3. **Calculate Group Sizes**: Determine the size of each group by dividing the number of samples by k. If the division is not exact, the first few groups will have one extra sample.\n4. **Generate Folds**: For each fold, compute the start and end indices to extract the test set. The remaining indices form the training set.\n5. **Return Folds**: Collect all the train-test splits and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n = len(X)\n    indices = np.arange(n)\n    \n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    group_size = n // k\n    remainder = n % k\n    \n    folds = []\n    for i in range(k):\n        if i < remainder:\n            fold_size = group_size + 1\n            start = i * (group_size + 1)\n        else:\n            fold_size = group_size\n            start = i * group_size + remainder\n        \n        end = start + fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_indices, test_indices))\n    \n    return folds\n```\n\n### Explanation\n1. **Combine Indices**: The function starts by creating an array of indices from 0 to n-1, where n is the number of samples in the dataset.\n2. **Shuffle Indices**: If shuffling is enabled, the indices are shuffled. This ensures that each fold is a random sample of the dataset.\n3. **Calculate Group Sizes**: The dataset is divided into k groups. The size of each group is determined by integer division of the number of samples by k. If there is a remainder, the first few groups will have one extra sample to ensure all samples are included.\n4. **Generate Folds**: For each fold, the start and end indices are calculated. The test set for the current fold is extracted using these indices. The training set is formed by concatenating all other indices not included in the test set.\n5. **Return Folds**: The function returns a list of tuples, each containing the indices for the training and test sets for each fold.\n\nThis approach ensures that each sample is used exactly once in each fold, providing a robust method for evaluating machine learning models."}
{"task_id": 19, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Principal Component Analysis (PCA) from scratch. PCA is a dimensionality reduction technique that transforms a dataset into a set of principal components, which are orthogonal directions that explain the most variance in the data. The goal is to write a function that performs PCA on a given dataset and returns the specified number of principal components.\n\n### Approach\nThe approach to implement PCA involves the following steps:\n\n1. **Standardize the Data**: \n   - Subtract the mean of each feature from all samples of that feature.\n   - Divide each feature by its standard deviation to scale it to unit variance. This step ensures that each feature contributes equally to the analysis.\n\n2. **Compute the Covariance Matrix**:\n   - The covariance matrix is calculated from the standardized data. This matrix represents the variance and covariance between each pair of features.\n\n3. **Compute Eigenvalues and Eigenvectors**:\n   - Using the covariance matrix, compute its eigenvalues and eigenvectors. Eigenvalues represent the amount of variance each principal component explains, while eigenvectors represent the direction of these components.\n\n4. **Sort Eigenvalues and Eigenvectors**:\n   - Sort the eigenvalues in descending order to prioritize the principal components that explain the most variance. Reorder the eigenvectors accordingly to maintain the association between eigenvalues and eigenvectors.\n\n5. **Select Top k Eigenvectors**:\n   - Select the top k eigenvectors corresponding to the largest eigenvalues. These eigenvectors are the principal components.\n\n6. **Round the Results**:\n   - Round each element of the principal components to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the data\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    standardized_data = (data - means) / stds\n\n    # Compute covariance matrix\n    n_samples = data.shape[0]\n    covariance_matrix = (standardized_data.T @ standardized_data) / (n_samples - 1)\n\n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n\n    # Sort eigenvalues and eigenvectors in descending order\n    eigen_pairs = list(enumerate(eigenvalues))\n    eigen_pairs.sort(key=lambda x: -x[1])\n\n    sorted_indices = [pair[0] for pair in eigen_pairs]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select top k eigenvectors\n    k = min(k, len(sorted_eigenvectors))  # Ensure k doesn't exceed the number of components\n    principal_components_matrix = sorted_eigenvectors[:, :k]\n\n    # Transpose to get each principal component as a row\n    principal_components = principal_components_matrix.T\n\n    # Round each component to 4 decimal places\n    rounded_components = [ [round(x, 4) for x in row] for row in principal_components ]\n\n    return rounded_components\n```\n\n### Explanation\n1. **Standardization**: The data is standardized to ensure each feature has a mean of 0 and a standard deviation of 1. This step is crucial because features with larger scales can dominate the covariance matrix.\n\n2. **Covariance Matrix**: The covariance matrix is computed from the standardized data. This matrix helps in understanding the variance and covariance between features.\n\n3. **Eigenvalues and Eigenvectors**: Using NumPy's `linalg.eigh` function, we compute the eigenvalues and eigenvectors of the covariance matrix. Eigenvalues indicate the importance of each principal component, while eigenvectors indicate the direction.\n\n4. **Sorting**: Eigenvalues and eigenvectors are sorted in descending order based on eigenvalues to prioritize the most significant principal components.\n\n5. **Selecting Principal Components**: The top k eigenvectors are selected to form the principal components matrix. This matrix is transposed to convert column vectors into row vectors for the final output.\n\n6. **Rounding**: Each element of the principal components is rounded to four decimal places to meet the output requirements.\n\nThis approach ensures that we correctly implement PCA from scratch, providing the required principal components in a structured and efficient manner."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a decision tree learning algorithm using recursive binary splitting based on entropy and information gain. The goal is to build a decision tree from a list of examples, where each example is a dictionary of attribute-value pairs, and a list of attribute names. The decision tree will be represented as a nested dictionary.\n\n### Approach\n1. **Base Cases**:\n   - If all examples belong to the same class, return a leaf node with that class.\n   - If no attributes are left to split on, return the majority class of the current examples as a leaf node.\n\n2. **Entropy Calculation**:\n   - Compute the entropy of the current set of examples. Entropy measures the impurity of the current set with respect to the target attribute.\n\n3. **Information Gain Calculation**:\n   - For each attribute, calculate the information gain, which is the reduction in entropy achieved by splitting the examples based on that attribute. The attribute with the highest information gain is chosen as the best attribute to split on.\n\n4. **Recursive Splitting**:\n   - Split the examples into subsets based on the values of the best attribute.\n   - Recursively build the decision tree for each subset, excluding the best attribute from further splits.\n\n5. **Tree Construction**:\n   - Construct the decision tree node for the best attribute, with branches corresponding to each value of the attribute leading to the recursively built subtrees.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Check if all examples are the same class\n    target_values = [ex[target_attr] for ex in examples]\n    if len(set(target_values)) == 1:\n        return {'leaf': target_values[0]}\n    \n    # If no attributes left, return majority class\n    if not attributes:\n        majority = Counter(target_values).most_common(1)[0][0]\n        return {'leaf': majority}\n    \n    # Compute current entropy\n    current_counts = Counter(target_values)\n    total = len(examples)\n    entropy = 0\n    for count in current_counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    \n    # Find the best attribute\n    max_gain = -1\n    best_attr = None\n    for attr in attributes:\n        groups = {}\n        for ex in examples:\n            val = ex[attr]\n            if val not in groups:\n                groups[val] = []\n            groups[val].append(ex)\n        \n        sum_entropy = 0\n        for val, group in groups.items():\n            group_counts = Counter([ex[target_attr] for ex in group])\n            group_total = len(group)\n            if group_total == 0:\n                continue\n            group_entropy = 0\n            for count in group_counts.values():\n                p = count / group_total\n                group_entropy -= p * math.log2(p)\n            sum_entropy += (group_total / total) * group_entropy\n        \n        gain = entropy - sum_entropy\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    \n    # Split on best_attr\n    groups = {}\n    for ex in examples:\n        val = ex[best_attr]\n        if val not in groups:\n            groups[val] = []\n        groups[val].append(ex)\n    \n    # Build subtrees\n    subtree = {}\n    new_attrs = [a for a in attributes if a != best_attr]\n    for val in groups:\n        subset = groups[val]\n        subtree_val = learn_decision_tree(subset, new_attrs, target_attr)\n        subtree[val] = subtree_val\n    \n    return {best_attr: subtree}\n```\n\n### Explanation\n- **Base Cases**: The function first checks if all examples are of the same class, returning a leaf node if true. If no attributes are left, it returns the majority class of the current examples.\n- **Entropy Calculation**: The entropy of the current set of examples is computed to measure impurity.\n- **Information Gain**: For each attribute, the function calculates the information gain by splitting the examples and computing the weighted sum of the entropies of the resulting subsets.\n- **Recursive Splitting**: The function recursively builds the decision tree by splitting the examples based on the best attribute and excluding it from further splits.\n- **Tree Construction**: The decision tree is constructed with nodes representing attributes and branches representing attribute values leading to child nodes or leaf nodes.\n\nThis approach ensures that the decision tree is built optimally by selecting the attribute that provides the highest information gain at each step, leading to accurate and efficient classification."}
{"task_id": 21, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a deterministic version of the Pegasos algorithm to train a kernel SVM classifier. The algorithm processes all samples in each iteration, making it deterministic rather than stochastic. The function should handle both linear and RBF kernels and return the model's alpha coefficients and bias.\n\n### Approach\n1. **Initialization**: Start with zero-initialized alpha coefficients and bias.\n2. **Iterative Update**: For each iteration, compute the learning rate and update the model parameters based on all samples.\n3. **Kernel Computation**: Depending on the kernel type (linear or RBF), compute the kernel values for each pair of data points.\n4. **Decision Function**: For each data point, compute the decision function using the current model parameters.\n5. **Update Rule**: If the decision function does not satisfy the margin constraint, update the alpha coefficients and bias.\n6. **Rounding**: After completing all iterations, round the results to four decimal places and return them as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n = data.shape[0]\n    alpha = np.zeros(n)\n    b = 0.0\n\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        delta_alpha = np.zeros(n)\n        delta_b = 0.0\n\n        for i in range(n):\n            if kernel == 'linear':\n                K_i = np.dot(data[i], data.T)\n            elif kernel == 'rbf':\n                distances = np.linalg.norm(data[i] - data, axis=1)\n                K_i = np.exp(-distances**2 / (2 * sigma**2))\n            else:\n                raise ValueError(\"Invalid kernel type\")\n\n            f_i = (alpha * labels).dot(K_i) + b\n\n            if labels[i] * f_i <= 1:\n                delta = eta * labels[i] * (1 - labels[i] * f_i)\n                delta_alpha[i] = delta\n                delta_b += eta * (labels[i] - f_i)\n\n        alpha += delta_alpha\n        b += delta_b\n\n    alpha_rounded = np.round(alpha, 4).tolist()\n    b_rounded = np.round(b, 4)\n    return [alpha_rounded, b_rounded]\n```\n\n### Explanation\n1. **Initialization**: The alpha coefficients are initialized to zero, and the bias is set to zero.\n2. **Iterative Update**: For each iteration, the learning rate is computed as `1 / (lambda * t)`, where `t` is the current iteration number.\n3. **Kernel Computation**: For each data point, the kernel values are computed. For the linear kernel, it's the dot product, and for the RBF kernel, it's the Gaussian function applied to the pairwise distances.\n4. **Decision Function**: The decision function for each data point is computed using the current alpha coefficients and bias.\n5. **Update Rule**: If the decision function does not satisfy the margin constraint, the alpha coefficients and bias are updated using the computed learning rate.\n6. **Rounding**: After all iterations, the alpha coefficients and bias are rounded to four decimal places and returned as lists.\n\nThis approach ensures that the model is trained deterministically using all samples in each iteration, adhering to the Pegasos algorithm's principles while handling both linear and RBF kernels efficiently."}
{"task_id": 22, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the output of the sigmoid activation function given an input value \\( z \\). The sigmoid function is a common activation function used in neural networks, and it maps any real-valued number to a value between 0 and 1. The function should return the result rounded to four decimal places.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nHere's the step-by-step approach to compute the sigmoid function:\n1. Compute \\( e^{-z} \\) using the exponential function.\n2. Add 1 to the result from step 1.\n3. Take the reciprocal of the result from step 2 to get the sigmoid value.\n4. Round the result to four decimal places.\n\nThis approach ensures numerical stability and handles both very large positive and negative values of \\( z \\) appropriately.\n\n### Solution Code\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)\n```\n\n### Explanation\n1. **Exponential Calculation**: The function computes \\( e^{-z} \\) using `math.exp(-z)`. This step is crucial as it transforms the input \\( z \\) into a positive exponent, ensuring the function handles both large positive and negative values correctly.\n2. **Reciprocal Calculation**: By taking the reciprocal of \\( 1 + e^{-z} \\), we ensure the output is always between 0 and 1, which is the desired behavior of the sigmoid function.\n3. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the problem's requirements.\n\nThis implementation efficiently computes the sigmoid function and handles edge cases where \\( z \\) is very large or very small, ensuring numerical stability and correctness."}
{"task_id": 23, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softmax activation function, which is commonly used in machine learning to convert logits into probabilities. The softmax function takes a vector of real numbers and maps it into a probability distribution, where each element is the exponential of the element divided by the sum of the exponentials of all elements.\n\n### Approach\nThe approach to compute the softmax function involves the following steps:\n\n1. **Subtract the Maximum Score**: To avoid numerical instability, especially with very large scores, we subtract the maximum score from each element in the input list. This adjustment ensures that the largest exponentiation value is manageable and prevents overflow or underflow issues.\n\n2. **Compute Exponentials**: For each adjusted score, compute the exponential value. This step transforms the scores into positive values, which are then used to compute the probability distribution.\n\n3. **Sum of Exponentials**: Calculate the sum of all the computed exponential values. This sum serves as the denominator for each probability calculation.\n\n4. **Compute Softmax Values**: For each exponential value, divide it by the sum of all exponentials to get the probability. Round each result to four decimal places to ensure the output is formatted correctly.\n\n### Solution Code\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    max_score = max(scores)\n    adjusted_scores = [s - max_score for s in scores]\n    exponentials = [math.exp(s) for s in adjusted_scores]\n    denominator = sum(exponentials)\n    result = [round(e / denominator, 4) for e in exponentials]\n    return result\n```\n\n### Explanation\n1. **Subtract the Maximum Score**: By subtracting the maximum score from each element, we ensure that the largest value in the list is zero, which helps in stabilizing the numerical computation. This step is crucial for handling very large or very small scores effectively.\n\n2. **Compute Exponentials**: Using the adjusted scores, we compute the exponential of each value. This transformation converts the scores into positive values, which are then used to compute the probability distribution.\n\n3. **Sum of Exponentials**: The sum of these exponential values forms the denominator. This sum is used to normalize the exponential values so that they sum up to 1, forming a valid probability distribution.\n\n4. **Compute Softmax Values**: Each exponential value is divided by the sum of all exponentials to get the probability. Rounding each result to four decimal places ensures the output is precise and formatted correctly.\n\nThis approach efficiently computes the softmax values while handling potential numerical issues, ensuring accurate and stable results."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with a sigmoid activation function for binary classification. The function will take in feature vectors, true binary labels, weights, and a bias, and return the predicted probabilities and the mean squared error (MSE) between the predicted probabilities and the true labels.\n\n### Approach\n1. **Compute Predictions**: For each feature vector, compute the linear combination of the features and weights, then add the bias. Apply the sigmoid function to this value to get the predicted probability.\n2. **Calculate Mean Squared Error (MSE)**: Compute the MSE by taking the average of the squared differences between the predicted probabilities and the true labels.\n3. **Rounding**: Round both the predicted probabilities and the MSE to four decimal places.\n4. **Return Results**: Return the predicted probabilities as a list and the MSE as a float.\n\n### Solution Code\n```python\nimport math\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Compute the predicted probabilities\n    predictions = []\n    for feature_vector in features:\n        # Calculate the linear combination of weights and features plus bias\n        linear_output = sum(f * w for f, w in zip(feature_vector, weights)) + bias\n        # Apply the sigmoid activation function\n        sigmoid_output = 1 / (1 + math.exp(-linear_output))\n        predictions.append(sigmoid_output)\n    \n    # Calculate the mean squared error\n    mse = 0.0\n    for p, y in zip(predictions, labels):\n        mse += (p - y) ** 2\n    mse = mse / len(predictions)\n    \n    # Round the results to four decimal places\n    rounded_predictions = [round(p, 4) for p in predictions]\n    rounded_mse = round(mse, 4)\n    \n    return (rounded_predictions, rounded_mse)\n```\n\n### Explanation\n1. **Predictions Calculation**: For each feature vector, the linear combination is computed by taking the dot product of the feature vector and the weights, then adding the bias. The sigmoid function is then applied to this linear combination to get the predicted probability.\n2. **MSE Calculation**: The MSE is calculated by summing the squared differences between each predicted probability and its corresponding true label, then dividing by the number of examples.\n3. **Rounding**: Both the predicted probabilities and the MSE are rounded to four decimal places to meet the output requirements.\n4. **Return**: The function returns a tuple containing the list of rounded predicted probabilities and the rounded MSE.\n\nThis approach ensures that we accurately simulate the behavior of a single neuron and evaluate its performance using the MSE metric."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with sigmoid activation and implement backpropagation to update the neuron's weights and bias using gradient descent based on the Mean Squared Error (MSE) loss. The function should return the updated weights, bias, and a list of MSE values for each epoch.\n\n### Approach\n1. **Initialization**: Start with the given initial weights and bias. These will be copied to avoid modifying the original values.\n2. **Forward Propagation**: For each epoch, compute the pre-activation (z) by taking the dot product of the features and weights, then adding the bias. Apply the sigmoid function to get the predicted output (y_pred).\n3. **Loss Calculation**: Compute the MSE loss between the predicted output and the true labels.\n4. **Backpropagation**: Calculate the gradients of the loss with respect to the weights and bias using the chain rule. The gradient for the weights is derived from the feature matrix and the error term, while the gradient for the bias is the mean of the error term.\n5. **Weight and Bias Update**: Adjust the weights and bias using the computed gradients and the given learning rate.\n6. **Store Loss**: Append the computed MSE loss for each epoch to a list, which is then returned along with the updated weights and bias.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    \n    for _ in range(epochs):\n        # Forward pass\n        z = np.dot(features, weights) + bias\n        y_pred = 1 / (1 + np.exp(-z))\n        \n        # Compute loss\n        loss = np.mean((y_pred - labels) ** 2)\n        mse_list.append(round(loss, 4))\n        \n        # Backpropagation\n        delta = (y_pred - labels) * y_pred * (1 - y_pred)\n        gradient_weights = np.dot(features.T, delta) / features.shape[0]\n        gradient_bias = np.mean(delta)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    \n    return (weights, bias, mse_list)\n```\n\n### Explanation\n- **Initialization**: The weights and bias are initialized with the provided values to ensure the original inputs remain unchanged.\n- **Forward Propagation**: The input features are passed through the neuron to compute the predicted output using the sigmoid activation function.\n- **Loss Calculation**: The MSE loss is calculated to measure the difference between the predicted and true labels.\n- **Backpropagation**: Gradients of the loss with respect to the weights and bias are computed using the chain rule. This involves calculating the error term (delta) and using it to find the gradients.\n- **Weight and Bias Update**: The weights and bias are updated using the computed gradients and the learning rate, which determines the step size in the direction of the negative gradient.\n- **Store Loss**: The MSE loss for each epoch is stored in a list and returned along with the updated weights and bias.\n\nThis approach ensures that the neuron's parameters are optimized to minimize the MSE loss over the given number of epochs, providing a clear progression of the loss values for monitoring the training process."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Python class similar to the provided 'Value' class that handles basic autograd operations: addition, multiplication, and ReLU activation. The class should manage scalar values and compute gradients correctly through automatic differentiation.\n\n### Approach\nThe goal is to create a class that can represent nodes in a computation graph, where each node knows how to compute its value and its gradient. The class should support addition, multiplication, and ReLU operations, and each operation should correctly compute gradients using automatic differentiation.\n\n1. **Class Structure**: The class `Value` will have attributes to store the data value, its gradient, and references to its children nodes and the operation it represents. The constructor initializes these attributes.\n\n2. **Operations**: The class will include methods for addition, multiplication, and ReLU activation. Each method will create a new `Value` node that represents the result of the operation. The new node will have a custom backward function to compute gradients for its inputs.\n\n3. **Backward Propagation**: Each operation's backward function will compute the gradient contributions to the input nodes and recursively call their backward functions to propagate gradients through the computation graph.\n\n### Solution Code\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def add(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n        def _backward():\n            a, b = out._prev\n            a.grad += out.grad\n            b.grad += out.grad\n            a._backward()\n            b._backward()\n        out._backward = _backward\n        return out\n\n    def mul(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n        def _backward():\n            a, b = out._prev\n            a.grad += out.grad * b.data\n            b.grad += out.grad * a.data\n            a._backward()\n            b._backward()\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), _children=(self,), _op='relu')\n        def _backward():\n            a = next(iter(out._prev))\n            if out.data > 0:\n                a.grad += out.grad\n            a._backward()\n        out._backward = _backward\n        return out\n```\n\n### Explanation\n- **Initialization**: The `Value` class is initialized with a data value, a set of children nodes, and an operation. The gradient is initialized to zero, and the backward function is set to a default lambda that does nothing.\n\n- **Addition**: The `add` method creates a new `Value` node representing the sum of the current node and another node (or scalar). The backward function for this node adds the gradient to both operands and recursively propagates the gradient.\n\n- **Multiplication**: The `mul` method creates a new `Value` node representing the product of the current node and another node (or scalar). The backward function computes the gradients using the chain rule and propagates them.\n\n- **ReLU Activation**: The `relu` method creates a new `Value` node that applies the ReLU function to the current node's data. The backward function checks if the output is positive and propagates the gradient accordingly.\n\nThis implementation ensures that each operation correctly computes gradients and propagates them through the computation graph, enabling efficient automatic differentiation."}
{"task_id": 27, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transformation matrix that converts vectors from one basis to another in a 3-dimensional space. The transformation matrix, when multiplied by a vector expressed in the original basis, will give the vector expressed in the new basis.\n\n### Approach\n1. **Construct Matrices from Bases**: Convert the given basis vectors into numpy matrices. Each basis is represented as a 3x3 matrix where each row corresponds to a basis vector.\n2. **Compute Inverse of Original Basis**: The transformation matrix from the original basis to the new basis is found by multiplying the inverse of the original basis matrix with the new basis matrix. This is because each vector in the new basis can be expressed as a linear combination of the original basis vectors.\n3. **Matrix Multiplication**: Multiply the inverse of the original basis matrix with the new basis matrix to get the transformation matrix.\n4. **Rounding and Conversion**: Round the elements of the resulting transformation matrix to four decimal places and convert the numpy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis vectors into numpy matrices\n    B_matrix = np.array(B, dtype=np.float64)\n    C_matrix = np.array(C, dtype=np.float64)\n    \n    # Compute the inverse of matrix B\n    B_inv = np.linalg.inv(B_matrix)\n    \n    # Calculate the transformation matrix P\n    P = B_inv @ C_matrix\n    \n    # Round each element to 4 decimal places\n    P_rounded = np.round(P, 4)\n    \n    # Convert the numpy array to a list of lists\n    return P_rounded.tolist()\n```\n\n### Explanation\n1. **Matrix Construction**: The given bases B and C are converted into numpy matrices. This allows us to use numpy's linear algebra functions for matrix inversion and multiplication.\n2. **Inverse Calculation**: The inverse of matrix B is computed because we need to express vectors from the new basis C in terms of the original basis B.\n3. **Transformation Matrix**: By multiplying the inverse of B with matrix C, we obtain the transformation matrix P. This matrix transforms any vector from the original basis B to the new basis C.\n4. **Rounding and Conversion**: The resulting matrix is rounded to four decimal places to meet the output constraints and then converted to a list of lists for the final result.\n\nThis approach efficiently computes the transformation matrix using linear algebra operations, ensuring accuracy and meeting the specified precision requirements."}
{"task_id": 28, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on the eigendecomposition of \\(A^T A\\). The SVD of a matrix \\(A\\) is given by \\(A = U \\cdot S \\cdot V\\), where \\(U\\) and \\(V\\) are orthogonal matrices, and \\(S\\) is a diagonal matrix containing the singular values of \\(A\\).\n\n### Approach\n1. **Compute \\(A^T A\\)**: This matrix is symmetric and positive semi-definite, ensuring real eigenvalues and orthogonal eigenvectors.\n2. **Eigendecomposition**: Compute the eigenvalues and eigenvectors of \\(A^T A\\). The eigenvalues are the squares of the singular values.\n3. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and reorder the corresponding eigenvectors to form the matrix \\(V\\).\n4. **Construct Singular Values**: The singular values are the square roots of the sorted eigenvalues, forming the diagonal matrix \\(S\\).\n5. **Compute Matrix \\(U\\)**: For each non-zero singular value, compute the corresponding column of \\(U\\) by normalizing \\(A \\cdot v_i\\). For zero singular values, compute orthogonal vectors to ensure \\(U\\) is orthogonal.\n6. **Handle Special Cases**: If all singular values are zero, set \\(U\\) and \\(V\\) to identity matrices and \\(S\\) to a zero matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    A_t_A = np.dot(A.T, A)\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(A_t_A)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(-eigenvalues)\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Compute singular values\n    singular_values = np.sqrt(sorted_eigenvalues)\n    \n    # Check if all singular values are zero\n    if np.all(singular_values == 0):\n        U = np.identity(2)\n        S = np.zeros((2, 2))\n        V = np.identity(2)\n    else:\n        # Construct V\n        V = sorted_eigenvectors\n        \n        # Compute U\n        U = np.zeros_like(A)\n        \n        # Compute non-zero sigma_i's\n        for i in range(2):\n            if singular_values[i] != 0:\n                v_i = V[:, i]\n                u_i = np.dot(A, v_i) / singular_values[i]\n                U[:, i] = u_i\n        \n        # Handle zero sigma_i's\n        zero_indices = np.where(singular_values == 0)[0]\n        for i in zero_indices:\n            # Find the non-zero index\n            non_zero_index = np.argmax(singular_values)\n            u_non_zero = U[:, non_zero_index]\n            # Compute orthogonal vector\n            u_i = np.array([-u_non_zero[1], u_non_zero[0]])\n            # Normalize\n            u_i = u_i / np.linalg.norm(u_i)\n            U[:, i] = u_i\n        \n        # Construct S\n        S = np.diag(singular_values)\n    \n    # Round the results to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to lists\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return (U_list, S_list, V_list)\n```\n\n### Explanation\n1. **Compute \\(A^T A\\)**: This step ensures we work with a symmetric matrix, which simplifies finding eigenvalues and eigenvectors.\n2. **Eigendecomposition**: The eigenvalues of \\(A^T A\\) are used to determine the singular values, and the eigenvectors form the matrix \\(V\\).\n3. **Sorting**: Sorting eigenvalues and eigenvectors ensures the correct order for constructing \\(S\\) and \\(V\\).\n4. **Constructing \\(U\\)**: For each non-zero singular value, the corresponding column of \\(U\\) is computed by normalizing the product of \\(A\\) and the corresponding eigenvector. For zero singular values, orthogonal vectors are computed to maintain orthogonality in \\(U\\).\n5. **Special Case Handling**: If \\(A\\) is the zero matrix, \\(U\\) and \\(V\\) are set to identity matrices, and \\(S\\) is a zero matrix.\n\nThis approach efficiently computes the SVD for a 2x2 matrix, ensuring numerical stability and correctness."}
{"task_id": 29, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that shuffles two numpy arrays, X and y, while maintaining the corresponding order between them. The function should also allow for reproducibility by using an optional seed parameter. The shuffled arrays should be returned as Python lists.\n\n### Approach\n1. **Set Seed for Reproducibility**: If a seed is provided, we set the random seed using `np.random.seed(seed)` to ensure that the shuffle is reproducible.\n2. **Generate Permutation Indices**: We generate a random permutation of indices using `np.random.permutation(len(X))`. This permutation will be used to shuffle both arrays.\n3. **Apply Permutation**: Using the generated permutation indices, we shuffle both X and y arrays. This ensures that the corresponding elements in X and y remain aligned after shuffling.\n4. **Convert to Lists**: Finally, we convert the shuffled numpy arrays to Python lists using the `tolist()` method and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    perm = np.random.permutation(len(X))\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\n```\n\n### Explanation\n- **Setting the Seed**: The seed ensures that the random permutation is the same every time the function is called with the same seed, which is crucial for reproducibility in experiments and testing.\n- **Generating Permutation**: The permutation indices are generated using `np.random.permutation(len(X))`, which creates a shuffled array of indices from 0 to len(X)-1.\n- **Shuffling Arrays**: By applying the permutation indices to both X and y, we ensure that the corresponding elements remain aligned after shuffling.\n- **Conversion to Lists**: The numpy arrays are converted to Python lists using the `tolist()` method, which is then returned as a tuple containing the shuffled X and y lists.\n\nThis approach efficiently shuffles the data while maintaining the correspondence between X and y, ensuring that the data is ready for training or analysis in a shuffled order."}
{"task_id": 30, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a batch iterator function that processes data in chunks (batches) of a specified size. The function should handle both input data and corresponding labels, if provided, and yield each batch as a Python list.\n\n### Approach\nThe task is to create a function that iterates over a given dataset in batches. The function should handle both the feature data (X) and the corresponding labels (y), if provided. Each batch should be returned as a Python list after converting the numpy arrays to lists.\n\n1. **Determine the Number of Samples**: The number of samples is determined by the first dimension of the feature array X.\n2. **Iterate in Batches**: Loop through the data in steps of the specified batch size. For each step, extract a batch of data from X and, if provided, from y.\n3. **Convert to List**: Convert each numpy array slice to a Python list using the `tolist()` method before yielding the result.\n4. **Yield Batches**: If labels (y) are provided, yield a tuple of feature batch and label batch. Otherwise, yield just the feature batch.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i+batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i+batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n```\n\n### Explanation\n- **Determine the Number of Samples**: The number of samples is obtained from the shape of the input array X, specifically the first dimension.\n- **Iterate in Batches**: Using a loop, we iterate from 0 to the number of samples in steps of the batch size. This ensures that each iteration processes a chunk of data of the specified size.\n- **Extract and Convert Batches**: For each iteration, extract the current batch from X and, if applicable, from y. Convert these numpy arrays to lists using the `tolist()` method.\n- **Yield Results**: Depending on whether labels are provided, yield either a tuple of feature and label batches or just the feature batch.\n\nThis approach ensures that the function efficiently processes the data in manageable chunks, making it suitable for large datasets where processing all data at once is impractical. The function handles both cases where labels are provided and where they are not, providing flexibility in its usage."}
{"task_id": 31, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that divides a dataset into two subsets based on a specified feature threshold. The function will return two subsets: one where the feature value is greater than or equal to the threshold, and another where it is less than the threshold.\n\n### Approach\n1. **Extract the Feature Column**: We start by extracting the column corresponding to the specified feature from the dataset.\n2. **Create a Boolean Mask**: Using the extracted feature column, we create a boolean mask where each entry is `True` if the feature value is greater than or equal to the threshold, and `False` otherwise.\n3. **Split the Dataset**: Using the boolean mask, we split the dataset into two subsets. The first subset contains all rows where the mask is `True`, and the second subset contains all rows where the mask is `False`.\n4. **Convert to Lists**: Finally, we convert the resulting numpy arrays into lists using the `tolist()` method and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    # Create a boolean mask where the feature is >= threshold\n    mask = X[:, feature_i] >= threshold\n    # Split the dataset based on the mask\n    subset1 = X[mask]\n    subset2 = X[~mask]\n    # Convert numpy arrays to lists\n    return subset1.tolist(), subset2.tolist()\n```\n\n### Explanation\n- **Extracting the Feature Column**: The feature column is extracted using `X[:, feature_i]`, which selects all rows and the specified column index.\n- **Boolean Mask**: The mask is created using `X[:, feature_i] >= threshold`, resulting in an array of boolean values.\n- **Splitting the Dataset**: The dataset is split into two subsets using the mask. `X[mask]` selects rows where the feature value meets the threshold condition, and `X[~mask]` selects rows where it does not.\n- **Conversion to Lists**: The resulting subsets are converted to lists using `tolist()`, ensuring the output meets the specified format.\n\nThis approach efficiently divides the dataset based on the given feature threshold, ensuring clarity and correctness in the results."}
{"task_id": 32, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate polynomial features for a given dataset. The function should take a 2D numpy array `X` and an integer `degree`, and return a new 2D numpy array with polynomial features up to the specified degree. The output should be a list after using numpy's `tolist()` method.\n\n### Approach\nThe approach to generate polynomial features involves creating all possible monomials of the input features up to the specified degree. Each monomial is a product of the input features raised to various powers, where the sum of the exponents in each product is less than or equal to the given degree.\n\n1. **Initialization**: Start by initializing an empty list to hold the new features.\n2. **Generate Features for Each Degree**: Loop through each degree from 0 to the specified degree. For each degree:\n   - **Degree 0**: Add a column of 1s, which represents the constant term.\n   - **Degrees 1 to Degree**: For each degree, generate all possible combinations of feature indices using `itertools.combinations_with_replacement`. For each combination, compute the product of the features raised to the respective exponents.\n3. **Combine Features**: Concatenate all generated features into a single 2D array and convert it to a list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    if degree < 0:\n        return []\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    result = []\n    \n    for d in range(0, degree + 1):\n        if d == 0:\n            # Add the constant term (1)\n            new_feature = np.ones((n_samples, 1))\n            result.append(new_feature)\n        else:\n            # Generate all combinations of feature indices with replacement of length d\n            for combo in combinations_with_replacement(range(n_features), d):\n                exponents = [0] * n_features\n                for idx in combo:\n                    exponents[idx] += 1\n                # Compute the product for each row\n                new_feature = np.ones(n_samples)\n                for i in range(n_samples):\n                    row = X[i]\n                    product = 1\n                    for j in range(n_features):\n                        product *= (row[j] ** exponents[j])\n                    new_feature[i] = product\n                # Reshape to column vector\n                new_feature = new_feature.reshape(-1, 1)\n                result.append(new_feature)\n    \n    # Concatenate all features\n    if not result:\n        return []\n    final_features = np.concatenate(result, axis=1)\n    return final_features.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by checking if the degree is valid. It then initializes variables to store the number of samples and features in the input array `X`.\n- **Loop Through Degrees**: For each degree from 0 to the specified degree, the function generates the corresponding polynomial features.\n  - **Degree 0**: A column of 1s is added to handle the constant term.\n  - **Higher Degrees**: For each degree, combinations of feature indices are generated. Each combination is used to compute the exponents for each feature. The product of the features raised to these exponents is computed for each row in `X`.\n- **Combine Features**: All generated features are concatenated into a single 2D array, which is then converted to a list and returned.\n\nThis approach efficiently generates all polynomial features up to the specified degree by leveraging combinations and exponentiation, ensuring that all possible monomials are included."}
{"task_id": 33, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate random subsets of a given dataset. Each subset should be a tuple containing a random selection of the dataset's features and corresponding labels. The function should allow for both with and without replacement sampling, ensuring reproducibility by using a specified seed.\n\n### Approach\n1. **Problem Analysis**: The task is to generate `n_subsets` random subsets from the given dataset. Each subset should be a tuple of two arrays: one for the features (X) and one for the labels (y). The subsets can be generated with or without replacement, depending on the boolean parameter `replacements`.\n\n2. **Intuition**: \n   - **Without Replacement**: When generating subsets without replacement, each subset is a permutation of the original dataset. This ensures that each sample is included exactly once in each subset.\n   - **With Replacement**: When generating subsets with replacement, each subset is a random sample where each element is independently selected, allowing for duplicates.\n\n3. **Algorithm Selection**: \n   - For without replacement, we use `np.random.permutation` to generate a random permutation of indices.\n   - For with replacement, we use `np.random.randint` to generate indices, allowing for duplicates.\n\n4. **Implementation**: The function will loop `n_subsets` times, each time generating the appropriate indices based on the `replacements` flag, and then selecting the corresponding rows from X and y. Each subset is converted to a list and added to the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    N = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.randint(N, size=N)\n        else:\n            indices = np.random.permutation(N)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n```\n\n### Explanation\n- **Seed Setting**: The seed is set at the beginning to ensure that the random number generation is reproducible.\n- **Index Generation**: Depending on whether replacements are allowed, either a permutation of indices or random indices with replacement are generated.\n- **Subset Creation**: For each subset, the corresponding rows from X and y are selected using the generated indices. These arrays are then converted to lists and stored as tuples in the result list.\n\nThis approach ensures that we efficiently generate the required number of random subsets, either with or without replacement, while maintaining reproducibility through the seed parameter."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform one-hot encoding on a 1D numpy array of integer values. One-hot encoding is a technique used to convert categorical variables into a binary vector representation. Each integer in the input array is converted into a binary vector where only one position is 1, representing the category of that integer.\n\n### Approach\n1. **Determine the Number of Columns (n_col):**\n   - If `n_col` is not provided, compute it as the number of unique elements in the input array. This is done by finding the unique values, sorting them, and setting `n_col` to the length of this sorted unique list.\n   - If `n_col` is provided, ensure that all elements in the input array are within the valid range [0, n_col-1]. If any element is out of this range, raise an error.\n\n2. **Create One-Hot Encoded Array:**\n   - Use numpy's `np.eye` function to create an identity matrix of size `n_col x n_col`.\n   - For each element in the input array, determine its corresponding row in the identity matrix. This row is then used to form the one-hot encoded vector for that element.\n\n3. **Return the Result:**\n   - Convert the resulting numpy array to a list using the `tolist()` method and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if n_col is not None:\n        if not (np.all(x >= 0) and np.all(x < n_col)):\n            raise ValueError(\"x contains values out of the valid range [0, n_col-1)\")\n        indices = x\n    else:\n        unique_x = np.sort(np.unique(x))\n        n_col = len(unique_x)\n        indices = np.searchsorted(unique_x, x)\n    encoded = np.eye(n_col)[indices]\n    return encoded.tolist()\n```\n\n### Explanation\n- **Determine n_col:** If `n_col` is not provided, the function calculates it by finding the number of unique values in the input array. This ensures that each unique value gets a distinct column in the encoded output.\n- **Check Validity:** When `n_col` is provided, the function checks if all values in the input array are within the valid range [0, n_col-1]. This ensures that the indices used to access the identity matrix are valid.\n- **One-Hot Encoding:** The function uses numpy's `np.eye` to create an identity matrix and then indexes into this matrix using the computed indices to form the one-hot encoded vectors.\n- **Return Result:** The resulting numpy array is converted to a list and returned, providing the one-hot encoded output in the required format.\n\nThis approach efficiently handles both cases where `n_col` is provided and where it is not, ensuring correct and robust one-hot encoding of the input array."}
{"task_id": 35, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a 1D numpy array into a diagonal matrix. A diagonal matrix is a square matrix where all the elements outside the main diagonal are zero, and the main diagonal elements are the same as the elements in the input array.\n\n### Approach\n1. **Understanding the Problem**: A diagonal matrix is constructed such that the elements of the input array form the main diagonal of the matrix, and all other elements are zero. For example, if the input array is `[1, 2, 3]`, the resulting diagonal matrix will be:\n   ```\n   [[1, 0, 0],\n    [0, 2, 0],\n    [0, 0, 3]]\n   ```\n2. **Using numpy's diag Function**: The numpy library provides a convenient function `np.diag` that can create a diagonal matrix from a 1D array. This function takes the input array and places its elements along the main diagonal of a square matrix, filling the rest with zeros.\n3. **Converting to Python List**: After creating the diagonal matrix using `np.diag`, we convert the resulting numpy array into a Python list using the `tolist()` method. This method converts the numpy array into a nested list structure, which is the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    return np.diag(x).tolist()\n```\n\n### Explanation\n- **Importing numpy**: We start by importing numpy as np to utilize its functions.\n- **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n- **Creating Diagonal Matrix**: Inside the function, `np.diag(x)` is used to create a diagonal matrix from the input array `x`.\n- **Converting to List**: The resulting numpy array is converted to a Python list using the `tolist()` method, which returns a nested list representing the diagonal matrix.\n\nThis approach efficiently constructs the diagonal matrix and converts it into the required list format, handling all edge cases such as empty arrays and single-element arrays correctly."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is defined as the ratio of correct predictions to the total number of predictions. This score is a fundamental metric in machine learning to evaluate the performance of classification models.\n\n### Approach\n1. **Problem Analysis**: The task is to compare two arrays, `y_true` (true labels) and `y_pred` (predicted labels), and compute how many predictions are correct. The accuracy is then the number of correct predictions divided by the total number of predictions.\n2. **Intuition**: By comparing each element in `y_true` with the corresponding element in `y_pred`, we can count the number of correct predictions. The accuracy is this count divided by the total number of elements in `y_true`.\n3. **Algorithm Selection**: Using numpy for element-wise comparison and summation is efficient and concise. The `np.sum` function can be used to count the number of correct predictions by summing a boolean array resulting from the comparison `y_true == y_pred`.\n4. **Complexity Analysis**: The operations involved (element-wise comparison and summation) are linear in terms of time complexity, O(n), where n is the number of elements in the arrays. This is efficient for large datasets.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Element-wise Comparison**: The expression `y_true == y_pred` generates a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n2. **Counting Correct Predictions**: Using `np.sum` on the boolean array gives the count of correct predictions.\n3. **Calculating Accuracy**: The accuracy is computed by dividing the count of correct predictions by the total number of predictions (length of `y_true`).\n4. **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the accuracy score is computed efficiently and accurately, providing a clear measure of the model's performance."}
{"task_id": 37, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the correlation matrix for a given dataset. The correlation matrix is a square matrix where each element (i, j) represents the correlation coefficient between the i-th and j-th variables. The function should handle both cases where the correlation matrix is computed between two datasets and where it is computed within a single dataset.\n\n### Approach\n1. **Problem Analysis**: The task requires computing the correlation matrix, which can be between two datasets or within a single dataset. The correlation coefficient used here is Pearson's correlation coefficient, which measures the linear relationship between two datasets.\n2. **Intuition**: Pearson's correlation coefficient is calculated as the covariance of two variables divided by the product of their standard deviations. Using numpy's built-in `corrcoef` function simplifies this computation, as it efficiently handles the matrix operations required.\n3. **Handling Optional Parameter**: If the optional parameter Y is not provided, the function should compute the correlation matrix of X with itself. This is achieved by setting Y to X when Y is None.\n4. **Rounding and Conversion**: The results are rounded to the nearest 4th decimal place and converted to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    corr_matrix = np.corrcoef(X, Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\n```\n\n### Explanation\n1. **Function Definition**: The function `calculate_correlation_matrix` takes two parameters, X and Y. Y is optional and defaults to None.\n2. **Handling Y as None**: If Y is None, it is set to X, ensuring that the function computes the correlation matrix of X with itself.\n3. **Computing Correlation Matrix**: Using numpy's `corrcoef` function, the correlation matrix is computed. This function efficiently handles both cases where Y is provided or not.\n4. **Rounding**: The computed correlation matrix is rounded to 4 decimal places to meet the output constraints.\n5. **Conversion to List**: The numpy array is converted to a Python list using the `tolist()` method for the final output.\n\nThis approach ensures that the function is both efficient and easy to use, leveraging numpy's optimized functions for matrix operations."}
{"task_id": 38, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm's fit method. AdaBoost is an ensemble method that combines multiple weak classifiers into a strong classifier. Each weak classifier is trained sequentially, and each round adjusts the weights of the training samples based on the previous classifier's performance.\n\n### Approach\n1. **Convert Labels**: Convert the labels from 0/1 to -1/1 for easier calculation.\n2. **Initialize Weights**: Start with equal weights for all samples.\n3. **Iterate for Each Classifier**: For each classifier, find the best feature and threshold that minimizes the classification error.\n4. **Compute Error and Alpha**: Calculate the error for the best feature and compute the weight for the classifier using the error.\n5. **Update Weights**: Adjust the weights of the samples based on the performance of the current classifier.\n6. **Store Classifiers**: Keep track of each classifier's parameters (feature, threshold, and weight).\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    # Convert y to -1 and 1 for easier calculations\n    y = np.where(y == 0, -1, 1)\n    n_samples = X.shape[0]\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature = -1\n        best_threshold = 0\n        best_direction = 0  # 0 for <= t predict -1, 1 for <= t predict 1\n        \n        for j in range(X.shape[1]):\n            feature_values = X[:, j]\n            unique_values = np.sort(np.unique(feature_values))\n            min_e_j = float('inf')\n            \n            # Consider all possible thresholds between unique values\n            for i in range(len(unique_values) - 1):\n                t = (unique_values[i] + unique_values[i+1]) / 2\n                # Compute error for both directions\n                # Direction 0: <= t predict -1, else 1\n                pred0 = np.where(feature_values <= t, -1, 1)\n                error0 = np.sum(weights * (pred0 != y))\n                # Direction 1: <= t predict 1, else -1\n                pred1 = np.where(feature_values <= t, 1, -1)\n                error1 = np.sum(weights * (pred1 != y))\n                current_error = min(error0, error1)\n                \n                if current_error < min_e_j:\n                    min_e_j = current_error\n                    if error0 < error1:\n                        best_t_j = t\n                        best_dir_j = 0\n                    else:\n                        best_t_j = t\n                        best_dir_j = 1\n            \n            # Update best feature if this feature is better\n            if min_e_j < min_error:\n                min_error = min_e_j\n                best_feature = j\n                best_threshold = best_t_j\n                best_direction = best_dir_j\n        \n        # Compute alpha\n        e = min_error\n        if e == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * math.log((1 - e) / e)\n        \n        # Compute new weights\n        feature_values = X[:, best_feature]\n        if best_direction == 0:\n            pred = np.where(feature_values <= best_threshold, -1, 1)\n        else:\n            pred = np.where(feature_values <= best_threshold, 1, -1)\n        \n        exponent = np.where(pred == y, -alpha, alpha)\n        new_weights = weights * np.exp(exponent)\n        Z = np.sum(new_weights)\n        if Z == 0:\n            Z = 1e-8  # Avoid division by zero\n        weights = new_weights / Z\n        \n        # Round the threshold and alpha\n        rounded_t = round(best_threshold, 4)\n        rounded_alpha = round(alpha, 4) if not np.isinf(alpha) else 0.0\n        classifiers.append((best_feature, rounded_t, rounded_alpha))\n    \n    return classifiers\n```\n\n### Explanation\n1. **Convert Labels**: The labels are converted from 0/1 to -1/1 to simplify the calculation of the classifier's performance.\n2. **Initialize Weights**: Each sample starts with equal weight, ensuring that all samples are considered equally important at the beginning.\n3. **Feature and Threshold Selection**: For each feature, the best threshold is determined by evaluating all possible thresholds and directions (predicting -1 or 1 for values less than or equal to the threshold). The feature with the smallest error is selected.\n4. **Error and Alpha Calculation**: The error for the selected feature is used to compute the classifier's weight (alpha), which determines how much the weights of the samples are adjusted.\n5. **Weight Update**: The weights of the samples are updated based on the performance of the current classifier, focusing more on the samples that were misclassified.\n6. **Store Classifiers**: Each classifier's parameters are stored, including the feature used, the threshold, and the weight, ensuring that the results are rounded to four decimal places for precision."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the log-softmax function for a given 1D numpy array of scores. The log-softmax function is useful in machine learning for converting scores into probabilities in a numerically stable manner.\n\n### Approach\nThe log-softmax function is derived from the softmax function, which converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function. To compute this efficiently and avoid numerical instability, we use the following steps:\n\n1. **Subtract the Maximum Value**: Subtract the maximum value of the scores from each element. This helps in preventing very large exponentials which could cause numerical overflow.\n2. **Compute Exponentials**: Compute the exponential of each adjusted score.\n3. **Sum the Exponentials**: Sum all these exponentials to get the denominator.\n4. **Compute Logarithm of the Sum**: Take the logarithm of the sum of the exponentials.\n5. **Compute Log-Softmax**: Subtract the logarithm of the sum from each adjusted score to get the log-softmax values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    adjusted = scores - max_score\n    exp_adj = np.exp(adjusted)\n    sum_exp = np.sum(exp_adj)\n    log_sum = np.log(sum_exp)\n    log_softmax = adjusted - log_sum\n    log_softmax_rounded = np.round(log_softmax, 4)\n    return log_softmax_rounded.tolist()\n```\n\n### Explanation\n1. **Convert to Numpy Array**: The input list is converted to a numpy array for efficient vectorized operations.\n2. **Adjust Scores**: Each score is adjusted by subtracting the maximum score to prevent overflow in exponential calculations.\n3. **Exponentiate Adjusted Scores**: Compute the exponential of each adjusted score.\n4. **Sum Exponentials**: Sum all the exponentials to get the denominator for the softmax function.\n5. **Logarithm of Sum**: Compute the natural logarithm of the sum of exponentials.\n6. **Compute Log-Softmax**: Subtract the logarithm of the sum from each adjusted score to get the log-softmax values.\n7. **Round and Convert to List**: The results are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures numerical stability and efficiency, making it suitable for large score values commonly encountered in machine learning applications."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a custom `Dense` layer for a neural network. This layer should be able to perform forward and backward passes, initialize its weights and biases, and provide the output shape. The implementation should ensure that all results are rounded to the nearest 4th decimal and returned as a Python list after reshaping using numpy's `tolist()` method.\n\n### Approach\n1. **Initialization**: The `Dense` class is initialized with the number of neurons (`n_units`) and an optional input shape. The weights (`W`) and biases (`w0`) are initialized in the `initialize` method using a uniform distribution for weights and zeros for biases.\n2. **Weight Initialization**: The weights are initialized using a uniform distribution with a limit of `1 / sqrt(input_shape[0])` to maintain stable gradients. The biases are initialized to zero.\n3. **Forward Pass**: The forward pass computes the output by performing a dot product between the input and the weights, then adding the biases. The result is rounded to 4 decimal places and returned as a list.\n4. **Backward Pass**: The backward pass computes the gradients with respect to the input and updates the weights and biases using a simple gradient descent update rule. The gradients are rounded to 4 decimal places and returned as a list.\n5. **Output Shape**: The output shape of the layer is determined by the number of neurons, returning a tuple `(n_units,)`.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        super(Dense, self).__init__()\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"Input shape is not set.\")\n        input_dim = self.input_shape[0]\n        limit = 1.0 / math.sqrt(input_dim)\n        self.W = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = X.dot(self.W) + self.w0\n        output = output.reshape(-1)\n        rounded_output = np.round(output, 4)\n        return rounded_output.tolist()\n\n    def backward_pass(self, accum_grad):\n        if not self.trainable:\n            return accum_grad\n        dW = (self.layer_input.T).dot(accum_grad)\n        dw0 = np.sum(accum_grad, axis=0)\n        learning_rate = 0.01\n        self.W -= learning_rate * dW\n        self.w0 -= learning_rate * dw0\n        grad = accum_grad.dot(self.W.T)\n        grad = grad.reshape(-1)\n        rounded_grad = np.round(grad, 4)\n        return rounded_grad.tolist()\n\n    def output_shape(self):\n        return (self.n_units,)\n```\n\n### Explanation\n- **Initialization**: The `__init__` method sets up the basic structure of the layer, including the number of neurons and input shape. The `initialize` method then sets up the weights and biases.\n- **Forward Pass**: The `forward_pass` method computes the output by performing matrix multiplication of the input with the weights and adding the biases. The result is reshaped and converted to a list.\n- **Backward Pass**: The `backward_pass` method computes the gradients for the weights and biases, updates them using a simple gradient descent update rule, and returns the gradient with respect to the input.\n- **Output Shape**: The `output_shape` method returns the shape of the layer's output, which is determined by the number of neurons.\n\nThis implementation ensures that the `Dense` layer can be integrated into a neural network framework, handling both forward and backward passes efficiently while maintaining numerical stability through proper weight initialization and gradient updates."}
{"task_id": 41, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a 2D convolutional layer in Python using numpy. The function will process an input matrix using a specified convolutional kernel, padding, and stride. The output should be a list after rounding the results to the nearest 4th decimal.\n\n### Approach\n1. **Padding**: The input matrix is padded with zeros on all sides according to the specified padding value. This ensures that the kernel can be applied to the edges of the input matrix.\n2. **Stride**: The kernel is moved over the padded input matrix with a step size equal to the stride. This determines how many positions the kernel will slide over the input matrix.\n3. **Convolution Operation**: For each position of the kernel on the padded input matrix, compute the dot product between the kernel and the corresponding region of the input matrix. This involves element-wise multiplication and summation.\n4. **Output Dimensions**: The dimensions of the output matrix are calculated based on the padded input dimensions, kernel size, and stride.\n5. **Rounding and Conversion**: The resulting output matrix is rounded to the nearest 4th decimal and converted to a list for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Pad the input matrix with the specified padding\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    \n    # Get the dimensions of the padded input and kernel\n    H_padded, W_padded = padded_input.shape\n    k_h, k_w = kernel.shape\n    \n    # Calculate the output dimensions\n    output_height = (H_padded - k_h) // stride + 1\n    output_width = (W_padded - k_w) // stride + 1\n    \n    # Initialize the output matrix with zeros\n    output = np.zeros((output_height, output_width))\n    \n    # Iterate over each possible position the kernel can take on the padded input\n    for i in range(0, H_padded - k_h + 1, stride):\n        for j in range(0, W_padded - k_w + 1, stride):\n            # Extract the region from the padded input\n            region = padded_input[i:i+k_h, j:j+k_w]\n            # Compute the convolution by taking the dot product of the region and kernel\n            output[i//stride, j//stride] = np.sum(region * kernel)\n    \n    # Round the output to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Convert the output to a list and return\n    return output.tolist()\n```\n\n### Explanation\n1. **Padding**: The input matrix is padded using numpy's pad function to ensure the kernel can be applied to all positions, including the edges.\n2. **Stride**: The kernel is moved over the input matrix in steps determined by the stride value, which affects the size of the output matrix.\n3. **Convolution**: For each position of the kernel, the corresponding region of the input matrix is extracted, multiplied element-wise with the kernel, and summed to produce the output value for that position.\n4. **Output Dimensions**: The dimensions of the output matrix are calculated based on the size of the padded input, kernel, and stride.\n5. **Rounding and Conversion**: The output matrix is rounded to four decimal places and converted to a list to meet the problem's requirements.\n\nThis approach ensures that the convolutional operation is performed efficiently and correctly, adhering to the specified constraints and requirements."}
{"task_id": 42, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function in Python. The ReLU function is a popular choice in neural networks due to its simplicity and effectiveness in training deep learning models.\n\n### Approach\nThe ReLU function takes a single float as input and returns the value after applying the ReLU activation. The function works as follows:\n- If the input value is greater than zero, it returns the input value.\n- If the input value is zero or negative, it returns zero.\n\nThis can be efficiently implemented using a conditional expression in Python, which provides a concise and readable solution.\n\n### Solution Code\n```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```\n\n### Explanation\nThe function `relu` takes a single parameter `z` of type `float`. It uses a conditional expression to check if `z` is greater than zero. If `z` is greater than zero, it returns `z`. Otherwise, it returns 0. This approach ensures that the function is both efficient and easy to understand, adhering to the definition of the ReLU activation function."}
{"task_id": 43, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Ridge Regression loss function, which combines the Mean Squared Error (MSE) with a regularization term to prevent overfitting. The function will take a feature matrix, coefficients, true labels, and a regularization parameter, and return the computed loss value rounded to four decimal places.\n\n### Approach\n1. **Compute Predictions**: Using the feature matrix `X` and coefficient vector `w`, compute the predicted values `y_pred` by taking the dot product of `X` and `w`.\n2. **Calculate Squared Errors**: Compute the squared differences between the predicted values `y_pred` and the true values `y_true`.\n3. **Compute MSE Term**: Sum the squared errors and divide by twice the number of samples to get the MSE term.\n4. **Compute Regularization Term**: Calculate the sum of the squares of the coefficients `w` and multiply by half the regularization parameter `alpha` to get the regularization term.\n5. **Combine Terms**: Add the MSE term and the regularization term to get the total loss.\n6. **Round Result**: Round the total loss to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    m = len(y_true)\n    y_pred = X.dot(w)\n    sum_squared_errors = np.sum((y_pred - y_true) ** 2)\n    mse_term = sum_squared_errors / (2 * m)\n    reg_term = (alpha / 2) * np.sum(w ** 2)\n    total_loss = mse_term + reg_term\n    return round(total_loss, 4)\n```\n\n### Explanation\n- **Feature Matrix and Coefficients**: The feature matrix `X` and coefficient vector `w` are used to compute the predicted values `y_pred`.\n- **Squared Errors**: The difference between predicted and true values is squared to avoid negative values and summed up.\n- **MSE Term**: The sum of squared errors is divided by twice the number of samples to get the MSE term, which is a measure of the model's error.\n- **Regularization Term**: The sum of the squares of the coefficients is multiplied by half the regularization parameter to penalize large coefficients, thus preventing overfitting.\n- **Total Loss**: The sum of the MSE term and the regularization term gives the total loss, which is then rounded to four decimal places for the final result.\n\nThis approach ensures that the model not only fits the data well but also remains generalizable by including the regularization term."}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky Rectified Linear Unit (Leaky ReLU) activation function in Python. The Leaky ReLU function is a variation of the Rectified Linear Unit (ReLU) function, which is commonly used in neural networks. The key difference between ReLU and Leaky ReLU is that Leaky ReLU provides a small, non-zero slope for negative inputs, which helps prevent the dying ReLU problem where neurons can become stuck at zero.\n\n### Approach\nThe Leaky ReLU function is defined as follows:\n- For positive values of the input `z`, the function returns `z` as is.\n- For negative values of `z`, the function returns `alpha * z`, where `alpha` is a small positive number that controls the slope of the negative part of the function. The default value for `alpha` is 0.01.\n\nThe function takes two parameters:\n1. `z`: A float representing the input value.\n2. `alpha`: An optional float with a default value of 0.01, representing the slope for negative inputs.\n\nThe function returns the result of applying the Leaky ReLU activation function to `z`.\n\n### Solution Code\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\n```\n\n### Explanation\n1. **Positive Input Handling**: If the input `z` is non-negative (i.e., `z >= 0`), the function simply returns `z`. This is the same behavior as the standard ReLU function.\n2. **Negative Input Handling**: If the input `z` is negative (i.e., `z < 0`), the function returns `alpha * z`. This introduces a small slope for negative inputs, preventing the output from being zero and thus helping to mitigate the dying ReLU problem.\n3. **Return Type**: The function returns a float or an integer depending on the result of the computation. For positive inputs, it returns the integer value of `z` if `z` is an integer, otherwise a float. For negative inputs, it always returns a float due to the multiplication by `alpha`.\n\nThis approach ensures that the Leaky ReLU function is both efficient and effective in handling both positive and negative input values, providing a smooth transition for negative inputs while maintaining the simplicity of the ReLU function for positive inputs."}
{"task_id": 45, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel between two input vectors. The linear kernel is defined as the dot product (inner product) of the two vectors. The dot product is a fundamental operation in linear algebra that takes two vectors and returns a scalar value.\n\n### Approach\nThe approach to solve this problem is straightforward. We will use the `numpy.dot` function to compute the dot product of the two input vectors. The `numpy.dot` function efficiently handles the computation of the dot product, even if the inputs are passed as lists, by converting them into numpy arrays internally. This ensures that the function works correctly for both lists and numpy arrays.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\n1. **Importing Numpy**: We import numpy as `np` to utilize the `numpy.dot` function for computing the dot product.\n2. **Function Definition**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are expected to be vectors (either numpy arrays or lists).\n3. **Dot Product Calculation**: Inside the function, we use `np.dot(x1, x2)` to compute the dot product of `x1` and `x2`. This function efficiently handles the multiplication and summation of corresponding elements of the vectors, returning the resulting scalar value.\n\nThis approach ensures that the function is both concise and efficient, leveraging numpy's optimized operations for numerical computations."}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the precision metric given two numpy arrays: `y_true` (true binary labels) and `y_pred` (predicted binary labels). Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n\n### Approach\n1. **Understanding Precision**: Precision is the measure of the proportion of correctly predicted positive instances out of all predicted positive instances. It is calculated as the number of true positives divided by the sum of true positives and false positives.\n2. **True Positives (TP)**: These are instances where both the true label and the predicted label are 1.\n3. **False Positives (FP)**: These are instances where the true label is 0 but the predicted label is 1.\n4. **Efficient Calculation**: Using element-wise multiplication and summation, we can efficiently compute TP and FP. Specifically, TP is the sum of the product of `y_true` and `y_pred`, and FP is the sum of the product of `y_pred` and `(1 - y_true)`.\n5. **Handling Division by Zero**: If the sum of TP and FP is zero, it means there are no positive predictions, and precision is undefined. In such cases, we return 0 to avoid division by zero errors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    true_positives = np.sum(y_true * y_pred)\n    false_positives = np.sum(y_pred * (1 - y_true))\n    denominator = true_positives + false_positives\n    if denominator == 0:\n        return 0.0\n    return true_positives / denominator\n```\n\n### Explanation\n1. **True Positives Calculation**: `true_positives` is calculated by summing the element-wise product of `y_true` and `y_pred`. This gives the count of instances where both true and predicted labels are 1.\n2. **False Positives Calculation**: `false_positives` is calculated by summing the element-wise product of `y_pred` and `(1 - y_true)`. This gives the count of instances where the predicted label is 1 but the true label is 0.\n3. **Denominator Calculation**: The sum of `true_positives` and `false_positives` gives the total number of positive predictions.\n4. **Precision Calculation**: If the denominator is zero, precision is set to 0.0 to handle the undefined case. Otherwise, precision is the ratio of `true_positives` to the denominator.\n\nThis approach efficiently computes the precision metric using vectorized operations, ensuring optimal performance and correctness."}
{"task_id": 47, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that can perform three variants of gradient descent\u2014Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent\u2014using the Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n\n### Approach\nThe function `gradient_descent` is designed to handle three different gradient descent methods: batch, stochastic, and mini-batch. Each method computes the gradient of the loss function differently, which affects how the weights are updated during training.\n\n1. **Batch Gradient Descent**: This method uses the entire dataset to compute the gradient in each iteration. It is stable but can be slow for large datasets.\n2. **Stochastic Gradient Descent (SGD)**: This method uses a single example to compute the gradient in each iteration. It is computationally efficient but can be noisy.\n3. **Mini-Batch Gradient Descent**: This method uses a small subset of the dataset to compute the gradient. It balances the stability of batch gradient descent with the efficiency of SGD.\n\nThe function computes the predictions, calculates the error, and updates the weights using the computed gradient for each iteration. The weights are updated using the learning rate multiplied by the gradient. After completing the specified number of iterations, the weights are rounded to four decimal places and returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    weights = np.array(weights).copy()\n    n_samples, n_features = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_X = X\n            batch_y = y\n        elif method == 'sgd':\n            idx = np.random.randint(0, n_samples)\n            batch_X = X[idx:idx+1]\n            batch_y = y[idx:idx+1]\n        elif method == 'mini-batch':\n            idx = np.random.choice(n_samples, batch_size, replace=False)\n            batch_X = X[idx]\n            batch_y = y[idx]\n        else:\n            raise ValueError(\"Invalid method\")\n        \n        y_pred = np.dot(batch_X, weights)\n        error = batch_y - y_pred\n        gradient = (batch_X.T @ error) / batch_size\n        weights -= learning_rate * gradient\n    \n    rounded_weights = np.round(weights, 4)\n    return rounded_weights.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by copying the initial weights to ensure the original weights are not modified. The number of samples and features are extracted from the input data matrix `X`.\n\n2. **Iteration**: For each iteration, the function selects a batch of data based on the specified method:\n   - **Batch Method**: Uses the entire dataset.\n   - **SGD Method**: Selects a single random sample.\n   - **Mini-Batch Method**: Selects a random subset of samples of size `batch_size`.\n\n3. **Prediction and Error Calculation**: For the selected batch, the function computes the predictions using the current weights. The error is then calculated as the difference between the actual target values and the predictions.\n\n4. **Gradient Calculation**: The gradient of the loss function with respect to the weights is computed. This gradient is the average gradient over the selected batch.\n\n5. **Weight Update**: The weights are updated by subtracting the product of the learning rate and the computed gradient.\n\n6. **Result**: After completing the specified number of iterations, the weights are rounded to four decimal places and converted to a list before being returned.\n\nThis approach ensures that the function efficiently handles different gradient descent methods and provides the updated weights after training."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given matrix into its Reduced Row Echelon Form (RREF). The RREF of a matrix is a form where each leading entry (pivot) in a row is 1, and all other elements in the column containing the pivot are zeros, except for the pivot itself. The process involves finding pivot positions, normalizing rows, and eliminating entries above and below each pivot.\n\n### Approach\n1. **Initialization**: Convert the input matrix into a numpy array for efficient manipulation.\n2. **Iterate through Columns**: For each column, starting from the left, find the pivot row (the first non-zero element in the column).\n3. **Row Swapping**: If a pivot is found, swap the pivot row with the current row to bring the pivot to the current position.\n4. **Normalization**: Normalize the pivot row so that the pivot element becomes 1.\n5. **Elimination**: Eliminate all other entries in the pivot column by subtracting appropriate multiples of the pivot row from all other rows.\n6. **Move to Next Column**: After processing a column, move to the next column and repeat the process until all columns are processed or all rows are exhausted.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rref(matrix):\n    mat = np.array(matrix, dtype=np.float64)\n    rows = mat.shape[0]\n    cols = mat.shape[1]\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        # Find the pivot row\n        i = r\n        while i < rows and mat[i, lead] == 0:\n            i += 1\n        if i == rows:\n            # No pivot in this column, move to next\n            lead += 1\n            r -= 1  # because we didn't process this row yet\n            continue\n        # Swap rows i and r\n        mat[[r, i]] = mat[[i, r]]\n        # Normalize the pivot row\n        pivot = mat[r, lead]\n        if pivot != 0:\n            mat[r] = mat[r] / pivot\n        # Eliminate all other entries in column lead\n        for i in range(rows):\n            if i != r:\n                factor = mat[i, lead]\n                mat[i] = mat[i] - factor * mat[r]\n        lead += 1\n    return mat.tolist()\n```\n\n### Explanation\n- **Initialization**: The matrix is converted to a numpy array to facilitate in-place operations.\n- **Finding Pivot**: For each column, the algorithm searches for the first non-zero element (pivot) starting from the current row.\n- **Row Swapping**: If a pivot is found, the rows are swapped to bring the pivot to the current row.\n- **Normalization**: The pivot row is normalized to make the pivot element 1.\n- **Elimination**: All other rows are adjusted to eliminate the pivot column entries, ensuring that only the pivot row has a non-zero entry in that column.\n- **Efficiency**: The algorithm efficiently processes each column and row, ensuring that the matrix is transformed into its RREF.\n\nThis approach ensures that the matrix is transformed into its RREF by systematically processing each column and row, making it both efficient and effective for the given problem."}
{"task_id": 49, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam (Adaptive Moment Estimation) optimization algorithm in Python. Adam is an optimization technique that adapts the learning rate for each parameter, combining the benefits of both AdaGrad and RMSProp. It uses two moving averages: one for the gradients (first moment) and another for the squared gradients (second moment).\n\n### Approach\nThe Adam optimization algorithm can be broken down into the following steps:\n\n1. **Initialization**: Start with initial parameter values and initialize two vectors, `m` and `v`, to store the moving averages of the gradients and squared gradients, respectively.\n\n2. **Iterative Update**: For each iteration:\n   - Compute the gradient of the objective function at the current parameter values.\n   - Update the moving average of the gradients (`m`) using the first moment estimate.\n   - Update the moving average of the squared gradients (`v`) using the second moment estimate.\n   - Bias-correct the moving averages to get `m_hat` and `v_hat` to account for the initial estimates being biased.\n   - Update the parameters using the bias-corrected estimates and the learning rate.\n\n3. **Termination**: After completing the specified number of iterations, round the optimized parameters to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = np.array(x0)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x = x - learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    \n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The parameters `x` are initialized to the given initial values `x0`. Two vectors `m` and `v` are initialized to zero to store the moving averages of the gradients and squared gradients.\n- **Gradient Computation**: In each iteration, the gradient of the objective function `f` is computed using the `grad` function.\n- **Moving Averages Update**: The moving averages `m` and `v` are updated using the gradients and squared gradients, scaled by the exponential decay rates `beta1` and `beta2`, respectively.\n- **Bias Correction**: The moving averages are bias-corrected to get `m_hat` and `v_hat` by dividing by the terms `(1 - beta1^t)` and `(1 - beta2^t)`, respectively.\n- **Parameter Update**: The parameters are updated using the learning rate, the bias-corrected moving averages, and a small constant `epsilon` for numerical stability.\n- **Rounding and Conversion**: After completing the iterations, the optimized parameters are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the optimization process efficiently adapts the learning rate for each parameter, leading to faster convergence and better performance compared to traditional optimization methods."}
{"task_id": 50, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Lasso Regression algorithm using Gradient Descent. Lasso Regression incorporates an L1 regularization penalty to the loss function, which helps in feature selection by potentially shrinking some feature coefficients to zero.\n\n### Approach\n1. **Problem Analysis**: The objective function for Lasso Regression includes both the mean squared error and the L1 penalty. The goal is to minimize this function using gradient descent.\n2. **Gradient Calculation**: The gradients for the weights (w) and bias (b) are computed considering both the mean squared error and the L1 penalty. The gradient for the mean squared error is derived using matrix operations for efficiency, while the gradient for the L1 penalty is computed using the sign function.\n3. **Parameter Update**: The weights and bias are updated iteratively using the computed gradients, scaled by the learning rate.\n4. **Convergence Check**: The algorithm stops early if the changes in the weights and bias fall below a specified tolerance, indicating convergence.\n5. **Rounding**: The final weights and bias are rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n\n    for _ in range(max_iter):\n        prev_w = w.copy()\n        prev_b = b\n\n        y_pred = X.dot(prev_w) + prev_b\n        error = y - y_pred\n\n        grad_w = (X.T.dot(error)) / n_samples + alpha * np.sign(prev_w)\n        grad_b = np.mean(error)\n\n        w = prev_w - learning_rate * grad_w\n        b = prev_b - learning_rate * grad_b\n\n        change_w = np.linalg.norm(w - prev_w)\n        change_b = abs(b - prev_b)\n\n        if change_w < tol and change_b < tol:\n            break\n\n    w = np.round(w, 4)\n    b = np.round(b, 4)\n\n    return (w.tolist(), [b])\n```\n\n### Explanation\n- **Initialization**: The weights (w) are initialized to zero, and the bias (b) is initialized to zero.\n- **Gradient Descent Loop**: For each iteration, the predictions are computed, and the error is calculated. The gradients for the weights and bias are then computed, considering both the mean squared error and the L1 penalty.\n- **Parameter Update**: The weights and bias are updated using the computed gradients, scaled by the learning rate.\n- **Convergence Check**: The loop terminates early if the changes in the weights and bias are below the specified tolerance, ensuring efficient computation.\n- **Rounding**: The final weights and bias are rounded to four decimal places to meet the precision requirement.\n\nThis approach efficiently minimizes the Lasso Regression objective function using gradient descent, incorporating both the mean squared error and the L1 regularization penalty."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Optimal String Alignment (OSA) distance between two given strings. The OSA distance is the minimum number of edits required to transform one string into another, considering four types of operations: insert, delete, substitute, and transpose adjacent characters, each with a cost of 1.\n\n### Approach\nThe approach to solve this problem involves using dynamic programming (DP) to efficiently compute the minimum number of edits. The key idea is to build a 2D DP table where `dp[i][j]` represents the minimum number of edits needed to convert the first `i` characters of the source string into the first `j` characters of the target string.\n\n1. **Initialization**: Create a DP table with dimensions `(len(source) + 1) x (len(target) + 1)`. Initialize the first row and column to represent the base cases where one of the strings is empty, requiring only insertions or deletions respectively.\n\n2. **Filling the DP Table**: For each character in the source and target strings, compute the minimum cost of converting the substring up to those characters using the four possible operations:\n   - **Substitution**: If the characters are the same, no cost is added; otherwise, the cost is 1 plus the value from the previous diagonal cell.\n   - **Insertion**: The cost is 1 plus the value from the cell to the left.\n   - **Deletion**: The cost is 1 plus the value from the cell above.\n   - **Transpose**: If the previous two characters in the source match the next two in the target (in reverse order), the cost is 1 plus the value from the cell two steps back and two steps left.\n\n3. **Result Extraction**: The value at `dp[len(source)][len(target)]` gives the minimum number of edits required to transform the entire source string into the target string.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    \n    # Create a DP table with (m+1) rows and (n+1) columns\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    # Initialize the first row and column\n    for i in range(m + 1):\n        dp[i][0] = i  # Deleting i characters from source\n    for j in range(n + 1):\n        dp[0][j] = j  # Inserting j characters into source\n    \n    # Fill the DP table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Substitution cost\n            if source[i-1] == target[j-1]:\n                sub_cost = dp[i-1][j-1]\n            else:\n                sub_cost = dp[i-1][j-1] + 1\n            \n            # Delete cost\n            del_cost = dp[i-1][j] + 1\n            \n            # Insert cost\n            ins_cost = dp[i][j-1] + 1\n            \n            # Transpose cost\n            trans_cost = float('inf')\n            if i >= 2 and j >= 2:\n                if source[i-2] == target[j-1] and source[i-1] == target[j-2]:\n                    trans_cost = dp[i-2][j-2] + 1\n            \n            # Determine the minimal cost\n            dp[i][j] = min(sub_cost, del_cost, ins_cost, trans_cost)\n    \n    return dp[m][n]\n```\n\n### Explanation\nThe solution uses dynamic programming to efficiently compute the minimum number of edits required to transform one string into another. By considering all possible operations (insert, delete, substitute, and transpose) and their respective costs, the DP table is filled in a bottom-up manner. This ensures that each cell in the table represents the minimum cost to convert the corresponding substrings of the source and target strings. The final result is found in the bottom-right cell of the table, which gives the OSA distance for the entire strings."}
{"task_id": 52, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the recall metric for a binary classification problem. Recall is a measure of how effectively a model identifies positive instances from all the actual positive cases in a dataset. The formula for recall is the ratio of true positives (TP) to the sum of true positives and false negatives (FN), i.e., Recall = TP / (TP + FN).\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the input lists of true labels (`y_true`) and predicted labels (`y_pred`) into numpy arrays for efficient element-wise operations.\n2. **Compute True Positives (TP)**: Count the instances where both the true label and the predicted label are 1.\n3. **Compute False Negatives (FN)**: Count the instances where the true label is 1 but the predicted label is 0.\n4. **Calculate Denominator**: The sum of TP and FN gives the denominator for the recall formula.\n5. **Handle Division by Zero**: If the denominator is zero, return 0.0 to avoid division by zero.\n6. **Compute Recall**: If the denominator is non-zero, compute the recall as TP divided by the denominator and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(tp / denominator, 3)\n```\n\n### Explanation\n- **Conversion to Arrays**: The function starts by converting the input lists into numpy arrays to facilitate efficient element-wise operations.\n- **True Positives (TP)**: This is calculated by summing up the instances where both `y_true` and `y_pred` are 1.\n- **False Negatives (FN)**: This is calculated by summing up the instances where `y_true` is 1 but `y_pred` is 0.\n- **Denominator Calculation**: The sum of TP and FN is computed to form the denominator of the recall formula.\n- **Division by Zero Handling**: If the denominator is zero, the function returns 0.0 to avoid division by zero errors.\n- **Recall Calculation**: The recall is computed as TP divided by the denominator, and the result is rounded to three decimal places for precision.\n\nThis approach ensures that the recall metric is computed efficiently and correctly, handling edge cases such as all true labels being zero or all predictions being incorrect."}
{"task_id": 53, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the self-attention mechanism, which is a key component of transformer models. The self-attention mechanism allows a model to dynamically focus on different parts of the input sequence when generating a contextualized representation.\n\n### Approach\nThe self-attention mechanism involves several steps:\n1. **Compute Queries, Keys, and Values**: Using the input sequence and three weight matrices, we compute the query (Q), key (K), and value (V) vectors for each element in the sequence.\n2. **Compute Attention Scores**: The attention scores are calculated by taking the dot product of the query vectors with the transpose of the key vectors. These scores represent how much each element should attend to every other element.\n3. **Apply Softmax**: The attention scores are normalized using the softmax function to convert them into probability distributions.\n4. **Compute Output**: The final output is obtained by taking the weighted sum of the value vectors, where the weights are the attention probabilities.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Compute queries, keys, and values\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute attention scores\n    scores = Q @ K.T\n    \n    # Apply softmax to get attention probabilities\n    exp_scores = np.exp(scores)\n    attention = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    \n    # Compute the output by taking the weighted sum of values\n    output = attention @ V\n    \n    # Round to 4 decimal places and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n```\n\n### Explanation\n1. **Compute Queries, Keys, and Values**: Each element in the input sequence is projected into query, key, and value spaces using the respective weight matrices. This results in three matrices Q, K, and V.\n2. **Compute Attention Scores**: The attention scores matrix is computed by taking the dot product of Q with the transpose of K. This matrix represents how much each element should attend to every other element.\n3. **Apply Softmax**: The softmax function is applied to the attention scores to convert them into probability distributions. This ensures that the sum of probabilities for each row is 1.\n4. **Compute Output**: The final output is obtained by multiplying the attention probabilities matrix with the values matrix. This step effectively computes a weighted sum of the values, where the weights are determined by the attention probabilities.\n\nThis approach ensures that each element in the sequence can dynamically focus on relevant parts of the sequence, leading to a more context-aware representation."}
{"task_id": 54, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple Recurrent Neural Network (RNN) cell that processes a sequence of input vectors and produces the final hidden state. The function will use the tanh activation function for the hidden state updates and return the final hidden state rounded to four decimal places.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the initial hidden state, weight matrices, and bias vector into numpy arrays for easier manipulation.\n2. **Iterate Over Input Sequence**: For each input vector in the sequence, compute the new hidden state using the given weight matrices and the current hidden state.\n3. **Compute Contributions**: Calculate the contributions from the input vector and the previous hidden state separately.\n4. **Sum and Apply Activation**: Sum these contributions along with the bias vector and apply the tanh activation function to get the new hidden state.\n5. **Update Hidden State**: Update the current hidden state to the new hidden state and proceed to the next input vector.\n6. **Round and Return**: After processing all input vectors, round the final hidden state to four decimal places and return it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert initial hidden state to a numpy array\n    h_prev = np.array(initial_hidden_state)\n    \n    # Convert weight matrices and bias to numpy arrays\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    for x in input_sequence:\n        x = np.array(x)\n        # Compute the input contribution\n        input_contribution = Wx.dot(x)\n        # Compute the hidden contribution\n        hidden_contribution = Wh.dot(h_prev)\n        # Sum the contributions and add bias\n        total = input_contribution + hidden_contribution + b\n        # Apply tanh activation\n        h_prev = np.tanh(total)\n    \n    # Round the final hidden state to four decimal places\n    h_prev = np.round(h_prev, 4)\n    \n    # Convert to list and return\n    return h_prev.tolist()\n```\n\n### Explanation\n- **Initialization**: The initial hidden state, weight matrices, and bias vector are converted into numpy arrays to facilitate matrix operations.\n- **Processing Each Input**: For each input vector, the contributions from the input and the previous hidden state are computed using matrix multiplication. These contributions are summed with the bias vector.\n- **Activation Function**: The tanh activation function is applied to the summed contributions to compute the new hidden state.\n- **Final State**: After processing all input vectors, the final hidden state is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the RNN processes each input vector sequentially, updating the hidden state at each step, and efficiently computes the final hidden state using matrix operations and the tanh activation function."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies a 2D translation matrix to a set of points. The translation matrix is used to move points in 2D space by specified distances in the x and y directions.\n\n### Approach\nThe approach involves using homogeneous coordinates to handle the translation. Homogeneous coordinates allow us to represent points in 2D space as 3D vectors by adding a third coordinate (1) to each point. This makes it possible to use matrix multiplication to apply the translation.\n\n1. **Create the Translation Matrix**: The translation matrix in homogeneous coordinates is a 3x3 matrix that includes the translation distances in the x and y directions.\n2. **Convert Points to Homogeneous Coordinates**: Each point [x, y] is converted to [x, y, 1].\n3. **Matrix Multiplication**: Multiply the translation matrix by the homogeneous coordinates of each point. This is done efficiently using matrix multiplication.\n4. **Convert Back to 2D Coordinates**: After applying the translation, the resulting homogeneous coordinates are converted back to 2D coordinates by removing the third element (1).\n5. **Return Result**: The translated points are returned as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create the translation matrix\n    matrix = np.array([[1, 0, tx],\n                       [0, 1, ty],\n                       [0, 0, 1]])\n    \n    # Convert points to numpy array and add homogeneous coordinate\n    points_np = np.array(points)\n    homogeneous_points = np.insert(points_np, 2, 1, axis=1)  # shape (n,3)\n    \n    # Multiply the matrix with the homogeneous points\n    homogeneous_points_transposed = homogeneous_points.T  # (3, n)\n    translated = np.dot(matrix, homogeneous_points_transposed)  # (3, n)\n    \n    # Transpose back to (n,3)\n    translated_points = translated.T  # (n,3)\n    \n    # Extract the x and y coordinates\n    translated_2d = translated_points[:, :2]\n    \n    # Convert to list of lists\n    result = translated_2d.tolist()\n    \n    return result\n```\n\n### Explanation\n1. **Translation Matrix**: The matrix is constructed to include the translation distances `tx` and `ty` in the x and y directions, respectively.\n2. **Homogeneous Coordinates**: Each point is converted to a 3D vector by adding a third coordinate (1). This allows the use of matrix multiplication for the translation.\n3. **Matrix Multiplication**: The translation matrix is multiplied by the homogeneous coordinates of each point. This operation is performed efficiently using numpy's matrix multiplication.\n4. **Conversion Back to 2D**: After translating, the homogeneous coordinates are converted back to 2D by removing the third element.\n5. **Result**: The translated points are returned as a list of lists, each containing the new x and y coordinates.\n\nThis approach ensures that the translation is applied correctly and efficiently to each point in the input list."}
{"task_id": 56, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Kullback-Leibler (KL) divergence between two normal distributions. The KL divergence measures how one probability distribution differs from another. For two normal distributions \\( P \\sim N(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\), the KL divergence can be computed using a specific formula.\n\n### Approach\nThe KL divergence between two normal distributions \\( P \\) and \\( Q \\) is given by the formula:\n\\[ \\text{KL}(P || Q) = 0.5 \\times \\left[ \\ln\\left(\\frac{\\sigma_Q^2}{\\sigma_P^2}\\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{\\sigma_Q^2} - 1 \\right] \\]\n\nHere are the steps to compute this:\n1. Compute the variances of the two distributions, \\( \\text{var}_P = \\sigma_P^2 \\) and \\( \\text{var}_Q = \\sigma_Q^2 \\).\n2. Calculate the logarithmic term \\( \\ln\\left(\\frac{\\text{var}_Q}{\\text{var}_P}\\right) \\).\n3. Compute the squared difference of the means \\( (\\mu_P - \\mu_Q)^2 \\).\n4. Combine the variances and the squared difference of the means to form the term \\( \\frac{\\text{var}_P + (\\mu_P - \\mu_Q)^2}{\\text{var}_Q} \\).\n5. Sum the logarithmic term, the combined term, and subtract 1, then multiply by 0.5 to get the KL divergence.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    log_term = np.log(var_q / var_p)\n    mu_diff_sq = (mu_p - mu_q) ** 2\n    term = (var_p + mu_diff_sq) / var_q\n    kl = 0.5 * (log_term + term - 1)\n    return kl\n```\n\n### Explanation\nThe function `kl_divergence_normal` takes the means and standard deviations of two normal distributions as inputs. It computes the KL divergence using the formula outlined above. The steps are as follows:\n1. Compute the variances of both distributions.\n2. Calculate the logarithmic term which accounts for the ratio of the variances.\n3. Compute the squared difference of the means to measure the distance between the two means.\n4. Combine the variances and the squared mean difference to form a term that measures the relative scale of the distributions.\n5. Sum the logarithmic term, the combined term, and subtract 1, then multiply by 0.5 to get the final KL divergence value.\n\nThis approach efficiently computes the KL divergence using basic arithmetic operations and logarithms, ensuring accurate results for normal distributions."}
{"task_id": 57, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gauss-Seidel method, an iterative technique for solving a system of linear equations (Ax = b). The method is efficient because it uses the most recent values during each iteration, which can lead to faster convergence compared to other iterative methods like Jacobi.\n\n### Approach\nThe Gauss-Seidel method works by decomposing the matrix A into lower triangular (L), diagonal (D), and upper triangular (U) parts. The iteration formula is given by:\n\n\\[ x^{(k+1)} = D^{-1}(b - Lx^{(k)} - Ux^{(k+1)}) \\]\n\nHowever, in practice, the method is implemented by updating each component of the solution vector using the most recent values as soon as they are available. This means that for each equation, we compute the new value using the already updated values of previous components and the old values of subsequent components.\n\nThe steps are as follows:\n1. Initialize the solution vector with an initial guess (if provided) or zeros.\n2. For each iteration, create a copy of the current solution vector.\n3. For each component of the solution vector, compute the new value using the most recent values of previous components and the old values of subsequent components.\n4. Update the solution vector with the new values.\n5. Repeat the process for the specified number of iterations.\n6. Return the solution vector rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x = np.zeros(len(b), dtype=np.float64)\n    else:\n        x = np.array(x_ini, dtype=np.float64)\n    \n    for _ in range(n):\n        x_old = x.copy()\n        for i in range(len(x)):\n            sum_less = 0.0\n            for j in range(i):\n                sum_less += A[i, j] * x[j]\n            sum_more = 0.0\n            for j in range(i + 1, len(x)):\n                sum_more += A[i, j] * x_old[j]\n            denominator = A[i, i]\n            if denominator == 0:\n                raise ValueError(\"Division by zero encountered in Gauss-Seidel method.\")\n            x[i] = (b[i] - sum_less - sum_more) / denominator\n    \n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The solution vector `x` is initialized to zeros if no initial guess is provided. If an initial guess is given, it is converted to a numpy array for consistency.\n2. **Iteration**: For each iteration, a copy of the current solution vector is made to use the old values for subsequent components.\n3. **Component Update**: For each component `i`, the sum of contributions from previous components (using the most recent values) and subsequent components (using the old values) is computed. The new value for component `i` is then calculated using these sums.\n4. **Convergence Check**: The method does not explicitly check for convergence but relies on the specified number of iterations. If a division by zero occurs, an error is raised.\n5. **Result Formatting**: After completing the specified iterations, the solution vector is rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the most recent values are used as soon as they are available, which can lead to faster convergence compared to other iterative methods."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gaussian Elimination method with partial pivoting to solve a system of linear equations represented by the matrix equation \\(Ax = b\\). This method transforms the system into an upper triangular matrix, which can then be solved using backward substitution.\n\n### Approach\n1. **Augmented Matrix Creation**: Combine the coefficient matrix \\(A\\) and the constants vector \\(b\\) into an augmented matrix. This matrix will have the form \\([A | b]\\).\n2. **Partial Pivoting**: For each column, find the row with the maximum absolute value in that column (starting from the current row). This row is then swapped with the current row to improve numerical stability.\n3. **Forward Elimination**: Transform the augmented matrix into an upper triangular matrix by eliminating the elements below each pivot. This is done by subtracting a multiple of the pivot row from each subsequent row.\n4. **Backward Substitution**: Once the matrix is in upper triangular form, solve for the variables starting from the last row and moving upwards. Each variable is computed based on the values of the subsequent variables.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Create the augmented matrix\n    aug = np.hstack((A, b[:, np.newaxis]))\n    n = A.shape[0]\n    \n    for i in range(n):\n        # Find the pivot row\n        max_row = i\n        for j in range(i, n):\n            if abs(aug[j, i]) > abs(aug[max_row, i]):\n                max_row = j\n        \n        # Swap the current row with the max row\n        aug[i], aug[max_row] = aug[max_row], aug[i]\n        \n        # Eliminate the entries below the pivot\n        pivot = aug[i, i]\n        for j in range(i + 1, n):\n            factor = aug[j, i] / pivot\n            aug[j] = aug[j] - factor * aug[i]\n    \n    # Perform back substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        sum_val = 0.0\n        for j in range(i + 1, n):\n            sum_val += aug[i, j] * x[j]\n        x[i] = (aug[i, n] - sum_val) / aug[i, i]\n    \n    # Round the solution to 4 decimal places\n    x = np.round(x, 4)\n    \n    return x.tolist()\n```\n\n### Explanation\n1. **Augmented Matrix**: The matrix is created by concatenating \\(A\\) and \\(b\\) to form a single matrix, which simplifies row operations.\n2. **Partial Pivoting**: This step ensures that the largest element in the current column is used as the pivot, reducing the risk of numerical errors.\n3. **Forward Elimination**: This step transforms the matrix into an upper triangular form by creating zeros below each pivot. This is done using row operations.\n4. **Backward Substitution**: Starting from the last row, each variable is solved by substituting the known values of the subsequent variables. This step converts the upper triangular matrix into the solution vector.\n\nThis approach efficiently solves the system of linear equations using Gaussian Elimination with partial pivoting, ensuring numerical stability and accuracy."}
{"task_id": 59, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Long Short-Term Memory (LSTM) network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs. The LSTM network should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n\n### Approach\n1. **Initialization**: The LSTM class is initialized with input size and hidden size. Weights and biases for the gates (forget, input, candidate, and output) are initialized using random values, while biases are initialized to zero.\n\n2. **Forward Propagation**: The `forward` method processes each input in the sequence. For each time step:\n   - Compute the gate inputs by concatenating the current hidden state and the current input.\n   - Calculate the forget gate, input gate, candidate cell state, and output gate using the respective weights and biases.\n   - Update the cell state using the forget gate and input gate.\n   - Update the hidden state using the output gate and the updated cell state.\n   - Store the hidden state at each time step.\n\n3. **Rounding and Conversion**: After processing all inputs, the hidden states, final hidden state, and final cell state are rounded to four decimal places and converted to lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        current_h = initial_hidden_state\n        current_c = initial_cell_state\n\n        for t in range(len(x)):\n            x_t = x[t].reshape(-1, 1)\n            gate_input = np.concatenate((current_h, x_t), axis=0)\n            \n            # Compute gates\n            ft = self.sigmoid(np.dot(self.Wf, gate_input) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, gate_input) + self.bi)\n            ct_tilde = np.tanh(np.dot(self.Wc, gate_input) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, gate_input) + self.bo)\n            \n            # Update cell and hidden states\n            new_c = ft * current_c + it * ct_tilde\n            new_h = ot * np.tanh(new_c)\n            \n            # Append new_h to hidden_states\n            hidden_states.append(new_h)\n            current_h = new_h\n            current_c = new_c\n\n        # Round and convert to lists\n        rounded_hidden = [np.round(h, 4).tolist() for h in hidden_states]\n        rounded_final_h = np.round(current_h, 4).tolist()\n        rounded_final_c = np.round(current_c, 4).tolist()\n\n        return (rounded_hidden, rounded_final_h, rounded_final_c)\n```\n\n### Explanation\n- **Initialization**: The constructor initializes the weights and biases for the LSTM gates. These are crucial for transforming the input and hidden state into the gate outputs.\n- **Forward Method**: This method processes each input in the sequence. For each time step, it computes the gates, updates the cell and hidden states, and stores the hidden states.\n- **Gates Calculation**: The forget gate, input gate, candidate cell state, and output gate are computed using the respective weights and biases. These gates control the flow of information in the LSTM cell.\n- **State Updates**: The cell state is updated using the forget and input gates, while the hidden state is updated using the output gate and the cell state.\n- **Rounding and Conversion**: After processing all inputs, the results are rounded to four decimal places and converted to lists to meet the output requirements.\n\nThis approach ensures that the LSTM network correctly processes the input sequence and returns the required states in the specified format."}
{"task_id": 60, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the TF-IDF (Term Frequency-Inverse Document Frequency) scores for a given query against a corpus of documents. The TF-IDF score is a measure that reflects how important a term is to a document in a corpus. \n\n### Approach\n1. **Handle Empty Corpus**: If the corpus is empty, return an empty list immediately.\n2. **Compute Document Frequency (DF)**: For each term in the query, count how many documents contain that term.\n3. **Compute Inverse Document Frequency (IDF)**: Using the document frequency, compute the inverse document frequency with smoothing to avoid division by zero. The formula used is `log((N + 1) / (df + 1))`, where `N` is the total number of documents.\n4. **Compute Term Frequency (TF)**: For each document, compute the term frequency for each query term. The term frequency is the number of times the term appears in the document divided by the total number of words in the document.\n5. **Compute TF-IDF**: For each document, multiply the TF of each query term by its corresponding IDF to get the TF-IDF score.\n6. **Round Results**: Round the TF-IDF scores to four decimal places and return them as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    \n    unique_query_terms = set(query)\n    df = {term: 0 for term in unique_query_terms}\n    for term in unique_query_terms:\n        for doc in corpus:\n            if term in doc:\n                df[term] += 1\n    \n    N = len(corpus)\n    idf = {}\n    for term in unique_query_terms:\n        numerator = N + 1\n        denominator = df[term] + 1\n        idf_val = np.log(numerator / denominator)\n        idf[term] = idf_val\n    \n    idf_list = [idf[term] for term in query]\n    \n    result = []\n    for doc in corpus:\n        if not doc:\n            scores = [0.0] * len(query)\n        else:\n            counter = Counter(doc)\n            tf_list = []\n            for term in query:\n                count = counter.get(term, 0)\n                tf = count / len(doc)\n                tf_list.append(tf)\n            tf_array = np.array(tf_list)\n            tfidf_array = tf_array * idf_list\n            rounded = tfidf_array.round(4)\n            scores = rounded.tolist()\n        result.append(scores)\n    \n    return result\n```\n\n### Explanation\n1. **Handling Empty Corpus**: The function first checks if the corpus is empty and returns an empty list if true.\n2. **Document Frequency Calculation**: For each term in the query, the function counts how many documents contain the term.\n3. **Inverse Document Frequency Calculation**: Using the document frequency, the function computes the inverse document frequency with smoothing to avoid division by zero.\n4. **Term Frequency Calculation**: For each document, the function calculates the term frequency for each query term using a counter to efficiently count occurrences.\n5. **TF-IDF Calculation**: The function computes the TF-IDF score by multiplying the term frequency with the inverse document frequency for each term in the query.\n6. **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach ensures that the function handles edge cases such as empty documents, terms not present in the corpus, and empty corpus gracefully, providing robust and accurate TF-IDF scores."}
{"task_id": 61, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the F-Score for a binary classification task. The F-Score is a metric that combines both Precision and Recall, providing a balanced measure of a model's performance. The formula for the F-Score is designed to adjust the importance of Precision and Recall using a parameter \u03b2 (beta).\n\n### Approach\n1. **Understand the Formula**: The F-Score is calculated using the formula:\n   \\[\n   F = \\frac{(1 + \\beta^2) \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}\n   \\]\n   where Precision is the ratio of true positives to the total number of predicted positives, and Recall is the ratio of true positives to the total number of actual positives.\n\n2. **Compute True Positives (TP), False Positives (FP), and False Negatives (FN)**:\n   - TP is the count of correctly predicted positive instances.\n   - FP is the count of incorrectly predicted positive instances.\n   - FN is the count of incorrectly predicted negative instances.\n\n3. **Calculate Precision and Recall**:\n   - Precision is TP / (TP + FP), handling division by zero by setting Precision to 0 if there are no predicted positives.\n   - Recall is TP / (TP + FN), similarly handling division by zero.\n\n4. **Compute the F-Score**:\n   - Using the computed Precision and Recall, apply the F-Score formula.\n   - Handle division by zero in the denominator by setting the F-Score to 0 if the denominator is zero.\n\n5. **Return the Result**: Round the computed F-Score to three decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Compute true positives, false positives, and false negatives\n    tp = np.sum(y_true * y_pred)\n    fp = np.sum(y_pred) - tp\n    fn = np.sum(y_true) - tp\n\n    # Calculate Precision and Recall\n    if (tp + fp) == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n    \n    if (tp + fn) == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n    \n    # Compute numerator and denominator for F-score\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall\n\n    # Handle division by zero\n    if denominator == 0:\n        f = 0.0\n    else:\n        f = numerator / denominator\n\n    # Round to three decimal places\n    return round(f, 3)\n```\n\n### Explanation\nThe function `f_score` calculates the F-Score by first determining the number of true positives (TP), false positives (FP), and false negatives (FN). Using these values, it computes Precision and Recall. The F-Score is then derived using the given formula, ensuring to handle cases where division by zero might occur. The result is rounded to three decimal places for precision. This approach efficiently combines the metrics of Precision and Recall to provide a balanced evaluation of a binary classification model's performance."}
{"task_id": 62, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Simple Recurrent Neural Network (RNN) with Backpropagation Through Time (BPTT) to learn from sequential data. The RNN will process input sequences, update hidden states, and perform backpropagation to adjust weights based on the error gradient. The task involves implementing both the forward pass and the backward pass of the RNN.\n\n### Approach\n1. **Initialization**: The RNN is initialized with random weights and zero biases. The weights are scaled by 0.01 to ensure they start with small values, which helps in the initial training phase.\n\n2. **Forward Pass**: The forward pass processes each element in the input sequence one by one, updating the hidden state at each step. The output at each step is generated based on the hidden state. The hidden state is computed using a combination of the current input, the previous hidden state, and biases, followed by a tanh activation function. The output is then computed using a linear transformation of the hidden state.\n\n3. **Backward Pass**: The backward pass uses BPTT to compute the gradients of the weights based on the error. The loss function is 1/2 * Mean Squared Error (MSE), summed over all time steps. The gradients are computed by propagating the error backward through each time step, starting from the last time step and moving to the first. The gradients for each weight are accumulated and then used to update the weights.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = x.shape[0]\n        hidden_size = self.hidden_size\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        h_prev = np.zeros((hidden_size, 1))  # initial hidden state\n\n        for t in range(T):\n            x_t = x[t, :].reshape(-1, 1)  # make it a column vector\n            # Compute the hidden state\n            h_input = np.dot(self.W_xh, x_t) + np.dot(self.W_hh, h_prev) + self.b_h\n            h_current = np.tanh(h_input)\n            # Compute output\n            y_t = np.dot(self.W_hy, h_current) + self.b_y\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_current)\n            h_prev = h_current  # update for next step\n\n        # Convert outputs, last_inputs, last_hiddens to arrays\n        outputs = np.array(outputs)\n        last_inputs = np.array(last_inputs)\n        last_hiddens = np.array(last_hiddens)\n\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT to compute gradients.\n        \"\"\"\n        T = input_sequence.shape[0]\n        hidden_size = self.hidden_size\n        output_size = self.W_hy.shape[0]\n\n        # Initialize gradients\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        # delta for the previous time step\n        delta = np.zeros((hidden_size, 1))\n\n        # Iterate over time steps in reverse order\n        for t in range(T-1, -1, -1):\n            # Get the output and hidden state for this time step\n            y_t = outputs[t].reshape(-1, 1)\n            h_t = last_hiddens[t]\n            x_t = last_inputs[t]\n            expected_t = expected_output[t].reshape(-1, 1)\n\n            # Compute delta for output layer\n            delta3 = (y_t - expected_t)  # since output is linear, derivative is 1\n\n            # Compute gradients for W_hy and b_y\n            dW_hy += h_t.T @ delta3\n            db_y += delta3\n\n            # Compute delta for hidden layer\n            delta2 = (self.W_hy.T @ delta3) * (1 - h_t**2)\n\n            # Compute gradients for W_hh and b_h\n            if t > 0:\n                h_prev = last_hiddens[t-1]\n            else:\n                h_prev = np.zeros((hidden_size, 1))  # for t=0, h_prev is zero\n\n            dW_hh += h_prev.T @ delta2\n            db_h += delta2\n\n            # Compute gradients for W_xh\n            dW_xh += x_t.T @ delta2\n\n            # Update delta for the previous time step\n            delta = self.W_hh.T @ delta2\n\n        # Update the weights with the gradients\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n```\n\n### Explanation\n- **Initialization**: The constructor initializes the weights and biases. The weights are initialized with small random values, and biases are zero.\n- **Forward Pass**: The forward method processes each input in the sequence, computes the hidden state using tanh activation, and generates the output using a linear transformation.\n- **Backward Pass**: The backward method computes the gradients using BPTT. It starts from the last time step, computes the error gradients for each weight, and updates the weights using the computed gradients and a learning rate.\n\nThis approach ensures that the RNN can learn from sequential data by adjusting its weights based on the error gradients propagated through time."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. The goal is to find the solution vector `x` for the system of equations `Ax = b`, where `A` is a given matrix, `b` is a given vector, and `x` is the unknown vector we need to determine.\n\n### Approach\nThe Conjugate Gradient method is particularly effective for solving systems where the matrix `A` is symmetric and positive-definite. The algorithm starts with an initial guess for the solution vector `x0` and iteratively improves this guess until the residual (the difference between the right-hand side vector `b` and the product of matrix `A` and the current solution vector `x`) is sufficiently small.\n\nThe steps of the CG method are as follows:\n1. **Initialization**: Start with an initial guess for the solution vector `x0`. If no initial guess is provided, use a zero vector.\n2. **Residual Calculation**: Compute the initial residual vector `r` as `b - Ax0`.\n3. **Direction Vector**: Set the initial direction vector `p` to be the residual vector `r`.\n4. **Iteration**: For a maximum of `n` iterations:\n   - Compute the dot product of the residual vector with itself to check for convergence.\n   - If the residual is below the specified tolerance, stop the iteration.\n   - Compute the step size `alpha` using the dot product of the residual and the product of the direction vector with matrix `A`.\n   - Update the solution vector `x` using the computed `alpha` and the direction vector `p`.\n   - Compute the new residual vector and update the direction vector using the new residual and the previous direction vector.\n5. **Termination**: After completing the iterations or early termination due to convergence, return the solution vector `x` rounded to 8 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros(b.shape)\n    else:\n        x = x0.copy()\n    \n    r = b - A @ x\n    p = r.copy()\n    \n    for _ in range(n):\n        r_dot_r = np.dot(r, r)\n        if r_dot_r < tol**2:\n            break\n        \n        p_dot_Ap = np.dot(p, A @ p)\n        alpha = r_dot_r / p_dot_Ap\n        x += alpha * p\n        \n        r_new = r - alpha * (A @ p)\n        beta = np.dot(r_new, r_new) / r_dot_r\n        p = r_new + beta * p\n        r = r_new\n    \n    x = np.round(x, 8)\n    return x.tolist()\n```\n\n### Explanation\n- **Initialization**: The solution vector `x` is initialized to a zero vector if no initial guess is provided. Otherwise, it is set to the given initial guess.\n- **Residual Calculation**: The residual vector `r` is computed as the difference between the right-hand side vector `b` and the product of matrix `A` and the current solution vector `x`.\n- **Direction Vector**: The initial direction vector `p` is set to the residual vector `r`.\n- **Iteration**: For each iteration, the algorithm checks if the residual is small enough to terminate early. If not, it computes the step size `alpha`, updates the solution vector, computes the new residual, and updates the direction vector using the new residual.\n- **Termination**: The loop runs for a maximum of `n` iterations or until the residual is sufficiently small. The solution vector is then rounded to 8 decimal places and returned as a list.\n\nThis approach ensures that the solution is found efficiently, even for large sparse matrices, by leveraging the properties of the Conjugate Gradient method."}
{"task_id": 64, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Gini Impurity for a given list of class labels. Gini Impurity is a measure used in decision tree algorithms to determine the disorder within a node. It helps in deciding how to split the data into different classes to minimize the impurity.\n\n### Approach\nThe approach to calculate the Gini Impurity involves the following steps:\n\n1. **Count the Occurrences**: First, we count the occurrences of each class label in the given list. This helps us determine the frequency of each class.\n2. **Calculate Probabilities**: For each class, we calculate the probability of an element belonging to that class. This is done by dividing the count of each class by the total number of elements.\n3. **Compute Gini Impurity**: Using the probabilities, we compute the Gini Impurity using the formula: \\( G = \\sum (p_i \\times (1 - p_i)) \\) for each class, where \\( p_i \\) is the probability of class \\( i \\).\n4. **Round the Result**: Finally, we round the computed Gini Impurity to three decimal places for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    n = len(y)\n    if n == 0:\n        return 0.0\n    unique, counts = np.unique(y, return_counts=True)\n    gini = 0.0\n    for count in counts:\n        p = count / n\n        gini += p * (1 - p)\n    return round(gini, 3)\n```\n\n### Explanation\n1. **Counting Occurrences**: The function uses `numpy.unique` with `return_counts=True` to get the unique class labels and their respective counts.\n2. **Calculating Probabilities**: For each unique class, the probability \\( p_i \\) is calculated by dividing the count of that class by the total number of elements \\( n \\).\n3. **Computing Gini Impurity**: For each class, the term \\( p_i \\times (1 - p_i) \\) is computed and summed up to get the total Gini Impurity.\n4. **Rounding**: The result is rounded to three decimal places to provide a clean and precise output.\n\nThis approach efficiently handles the calculation using numpy functions, ensuring that the solution is both optimal and easy to understand."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given dense matrix into its Compressed Row Sparse (CSR) format. The CSR format efficiently stores sparse matrices by only keeping track of non-zero elements and their positions, which significantly reduces memory usage.\n\n### Approach\nThe approach to convert a dense matrix to the CSR format involves the following steps:\n\n1. **Initialize Lists**: Create three lists to store the non-zero values, their corresponding column indices, and the cumulative count of non-zero elements up to each row (row pointers).\n\n2. **Iterate Through Rows**: For each row in the dense matrix, iterate through each element to identify non-zero values.\n\n3. **Collect Non-Zero Elements**: For each non-zero element, add its value to the values list and its column index to the column indices list. Keep a count of non-zero elements in the current row.\n\n4. **Update Row Pointers**: After processing each row, update the cumulative count of non-zero elements and append this count to the row pointers list. This helps in determining the starting index of each row in the values list.\n\n5. **Return Result**: Finally, return the values, column indices, and row pointers as a tuple.\n\n### Solution Code\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # The first pointer is always 0\n    current_count = 0\n\n    for row in dense_matrix:\n        count = 0\n        for col_idx, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        current_count += count\n        row_pointers.append(current_count)\n\n    return (values, column_indices, row_pointers)\n```\n\n### Explanation\n- **Initialization**: We start by initializing three lists: `values` to store non-zero elements, `column_indices` to store their respective column indices, and `row_pointers` to keep track of the cumulative count of non-zero elements up to each row. The `row_pointers` list starts with a zero to indicate the starting index of the first row.\n\n- **Row Processing**: For each row in the dense matrix, we iterate through each element. If an element is non-zero, it is added to the `values` list, and its column index is added to the `column_indices` list. We also maintain a count of non-zero elements in the current row.\n\n- **Updating Pointers**: After processing each row, the count of non-zero elements is added to the cumulative count (`current_count`), which is then appended to the `row_pointers` list. This cumulative count helps in determining where each row starts in the `values` list.\n\n- **Result**: The function returns a tuple containing the `values`, `column_indices`, and `row_pointers` lists, which together represent the dense matrix in the CSR format.\n\nThis approach efficiently handles sparse matrices by only storing non-zero elements and their positions, thus reducing memory usage significantly."}
{"task_id": 66, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the orthogonal projection of a vector `v` onto another vector `L`. The orthogonal projection is the vector on `L` that is closest to `v`. This can be achieved using the formula from linear algebra, which involves the dot product of the vectors.\n\n### Approach\n1. **Compute the Dot Product of `v` and `L`**: This gives a scalar value that represents the projection of `v` onto `L` in terms of magnitude.\n2. **Compute the Dot Product of `L` with Itself**: This gives the squared magnitude of `L`, which is used to normalize the projection.\n3. **Calculate the Scalar Multiplier**: This is obtained by dividing the dot product of `v` and `L` by the squared magnitude of `L`.\n4. **Compute the Projection Vector**: Multiply each component of `L` by the scalar multiplier to get the projection vector.\n5. **Round the Result**: Each component of the resulting projection vector is rounded to three decimal places for precision.\n\n### Solution Code\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Compute the dot product of v and L\n    dot_v_L = sum(a * b for a, b in zip(v, L))\n    # Compute the dot product of L with itself\n    dot_L_L = sum(a * a for a in L)\n    # Avoid division by zero, though problem assumes L is non-zero\n    if dot_L_L == 0:\n        return []\n    # Compute the scalar multiplier\n    scalar = dot_v_L / dot_L_L\n    # Compute the projection vector\n    projection = [scalar * component for component in L]\n    # Round each component to three decimal places\n    return [round(x, 3) for x in projection]\n```\n\n### Explanation\n- **Dot Product Calculation**: The dot product of two vectors is computed by summing the products of their corresponding components. This helps in determining the projection's magnitude.\n- **Normalization**: Dividing by the squared magnitude of `L` normalizes the projection, ensuring it lies along the direction of `L`.\n- **Projection Vector**: Each component of `L` is scaled by the scalar multiplier to get the projection vector, which is the closest vector on `L` to `v`.\n- **Rounding**: The result is rounded to three decimal places to ensure precision and readability.\n\nThis approach efficiently computes the orthogonal projection using basic vector operations, ensuring accuracy and clarity in the result."}
{"task_id": 67, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format efficiently stores non-zero elements of a matrix, which is particularly useful for matrices with a high number of zero elements. The output is a tuple of three lists: `values`, `row indices`, and `column pointer`.\n\n### Approach\n1. **Initialization**: Start by initializing the necessary lists: `values` to store non-zero elements, `row_indices` to store the corresponding row indices, and `column_pointer` to store the starting index of each column in the `values` list.\n2. **Iterate through Columns**: For each column in the dense matrix, collect all non-zero elements and their corresponding row indices.\n3. **Update Pointers**: After processing each column, update the `column_pointer` list to reflect the starting index of the next column in the `values` list.\n4. **Handle Edge Cases**: Ensure the function handles empty matrices and matrices with all zero elements correctly.\n\n### Solution Code\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    if rows == 0:\n        return ([], [], [])\n    cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    column_pointer = [0]  # Start with the initial pointer at 0\n    \n    for col in range(cols):\n        start = len(values)\n        for row in range(rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of rows and columns in the dense matrix. If the matrix is empty, it returns empty lists for all three output lists.\n2. **Processing Each Column**: For each column, the function iterates through each row, collecting non-zero elements and their corresponding row indices. These are added to the `values` and `row_indices` lists.\n3. **Updating Pointers**: After processing each column, the function records the current length of the `values` list, which serves as the starting index for the next column. This is appended to the `column_pointer` list.\n4. **Return Result**: Finally, the function returns the three lists as a tuple, representing the CSC format of the input matrix.\n\nThis approach ensures that the function efficiently converts the dense matrix into its compressed form, handling all edge cases and ensuring correct formatting."}
{"task_id": 68, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the column space (image) of a given matrix \\( A \\). The column space is the set of all possible linear combinations of the columns of \\( A \\). To determine this, we can use the concept of Reduced Row Echelon Form (RREF) to identify the pivot columns, which form a basis for the column space.\n\n### Approach\n1. **Row Reduction to RREF**: Convert the given matrix \\( A \\) into its Reduced Row Echelon Form (RREF). This involves performing Gaussian elimination to transform the matrix such that each leading coefficient (pivot) is 1 and all entries above and below each pivot are zero.\n2. **Identify Pivot Columns**: Track the pivot columns during the row reduction process. These columns in the original matrix \\( A \\) are linearly independent and form the basis of the column space.\n3. **Extract Basis Vectors**: Extract the columns from the original matrix \\( A \\) corresponding to the pivot columns identified in the RREF.\n4. **Round and Convert**: Round the resulting vectors to 8 decimal places and convert them into a list format for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Make a copy of the matrix to avoid modifying the original\n    A_copy = np.array(A, dtype=np.float64)\n    num_rows, num_cols = A_copy.shape\n\n    pivot_cols = []\n    current_row = 0\n    current_col = 0\n\n    while current_row < num_rows and current_col < num_cols:\n        # Find the pivot in the current column\n        pivot = None\n        for r in range(current_row, num_rows):\n            if abs(A_copy[r, current_col]) > 1e-8:\n                pivot = r\n                break\n        if pivot is None:\n            current_col += 1\n            continue\n\n        # Swap the current row with the pivot row\n        A_copy[current_row], A_copy[pivot] = A_copy[pivot], A_copy[current_row]\n\n        # Normalize the pivot row\n        pivot_val = A_copy[current_row, current_col]\n        A_copy[current_row] = A_copy[current_row] / pivot_val\n\n        # Eliminate all other entries in the current column\n        for r in range(num_rows):\n            if r != current_row:\n                factor = A_copy[r, current_col]\n                A_copy[r] -= factor * A_copy[current_row]\n\n        # Record the pivot column\n        pivot_cols.append(current_col)\n\n        current_row += 1\n        current_col += 1\n\n    # Extract the corresponding columns from the original matrix A\n    basis = []\n    for col in pivot_cols:\n        basis.append(A[:, col].tolist())\n\n    # Round each vector to 8 decimal places\n    basis = [np.round(vec, 8).tolist() for vec in basis]\n\n    return basis\n```\n\n### Explanation\n1. **Row Reduction to RREF**: The matrix is transformed into its RREF by iteratively finding the pivot element, normalizing the pivot row, and eliminating all other entries in the pivot column. This process continues until all pivot positions are identified.\n2. **Identify Pivot Columns**: During the row reduction, each pivot column is recorded. These columns in the original matrix are the basis vectors for the column space.\n3. **Extract Basis Vectors**: The columns corresponding to the pivot columns from the original matrix \\( A \\) are extracted to form the basis.\n4. **Round and Convert**: The resulting vectors are rounded to 8 decimal places and converted into a list format to meet the output requirements.\n\nThis approach efficiently finds the column space by leveraging the properties of RREF and pivot columns, ensuring that the solution is both accurate and optimal."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the R-squared value, also known as the coefficient of determination, for a given set of true values and predicted values. The R-squared value indicates how well the independent variables explain the variability of the dependent variable in a regression model.\n\n### Approach\nThe R-squared value is calculated using the formula:\n\\[ R^2 = 1 - \\frac{SSE}{SST} \\]\nwhere:\n- \\( SSE \\) is the sum of squared errors, which measures the discrepancy between the predicted values and the true values.\n- \\( SST \\) is the sum of squared total, which measures the variability of the true values around their mean.\n\nThe steps to compute the R-squared value are as follows:\n1. Compute the mean of the true values.\n2. Calculate the sum of squared total (SST) using the mean.\n3. Calculate the sum of squared errors (SSE) using the predicted values.\n4. Handle the special case where SST is zero (i.e., all true values are the same) to avoid division by zero.\n5. Compute the R-squared value using the formula and return it rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    y_bar = np.mean(y_true)\n    sst = np.sum((y_true - y_bar) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    \n    epsilon = 1e-10\n    if abs(sst) < epsilon:\n        if abs(sse) < epsilon:\n            return round(1.0, 3)\n        else:\n            return round(0.0, 3)\n    else:\n        r_sq = 1 - (sse / sst)\n        return round(r_sq, 3)\n```\n\n### Explanation\n1. **Compute the Mean**: The mean of the true values (`y_bar`) is calculated to determine the central value around which the variability of the true values is measured.\n2. **Sum of Squared Total (SST)**: This is the sum of the squared differences between each true value and the mean of the true values. It measures the total variability in the true values.\n3. **Sum of Squared Errors (SSE)**: This is the sum of the squared differences between each true value and the corresponding predicted value. It measures the error in the predictions.\n4. **Special Case Handling**: If the sum of squared total (SST) is effectively zero (within a small epsilon threshold to handle floating point precision issues), it means all true values are the same. In this case, if the sum of squared errors (SSE) is also zero, the R-squared value is 1.0 (perfect prediction). Otherwise, it is 0.0 (no prediction).\n5. **Compute R-squared**: For non-zero SST, the R-squared value is computed using the formula and rounded to three decimal places to ensure the result is precise and formatted correctly.\n\nThis approach ensures that we handle edge cases and floating point precision issues appropriately, providing an accurate and reliable R-squared value."}
{"task_id": 70, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the average brightness of a grayscale image represented as a 2D matrix. The function should handle various edge cases and return the average brightness rounded to two decimal places, or -1 if any edge case is encountered.\n\n### Approach\n1. **Check for Empty Image**: If the image matrix is empty, return -1.\n2. **Check Row Consistency**: Ensure all rows in the matrix have the same length. If any row has a different length, return -1.\n3. **Check for Empty Rows**: If any row is empty (length zero), return -1.\n4. **Validate Pixel Values**: Ensure all pixel values are within the valid range (0-255). If any pixel is outside this range, return -1.\n5. **Calculate Total Brightness**: Sum all pixel values in the matrix.\n6. **Compute Average Brightness**: Divide the total brightness by the number of pixels to get the average brightness.\n7. **Round the Result**: Return the average brightness rounded to two decimal places.\n\n### Solution Code\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img:\n        return -1\n    \n    # Get the number of columns from the first row\n    num_cols = len(img[0])\n    \n    # Check if all rows have the same length\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n    \n    # Check if the number of columns is zero\n    if num_cols == 0:\n        return -1\n    \n    # Check each pixel value is within 0-255\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    \n    # Calculate the total brightness\n    total = 0\n    for row in img:\n        total += sum(row)\n    \n    # Calculate the average brightness\n    total_pixels = len(img) * num_cols\n    average = total / total_pixels\n    \n    # Round to two decimal places\n    return round(average, 2)\n```\n\n### Explanation\n1. **Empty Image Check**: The function first checks if the image matrix is empty. If it is, the function returns -1 immediately.\n2. **Row Consistency Check**: The function verifies that all rows have the same length as the first row. If any row's length differs, the function returns -1.\n3. **Empty Rows Check**: If the number of columns (from the first row) is zero, the function returns -1 to handle cases where rows are empty.\n4. **Pixel Value Validation**: Each pixel is checked to ensure it falls within the valid range of 0 to 255. If any pixel is out of this range, the function returns -1.\n5. **Total Brightness Calculation**: The function sums all pixel values across the entire matrix.\n6. **Average Brightness Calculation**: The total brightness is divided by the total number of pixels to compute the average brightness.\n7. **Result Rounding**: The average brightness is rounded to two decimal places and returned as the result.\n\nThis approach ensures that all edge cases are handled efficiently and the average brightness is calculated accurately."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the Root Mean Square Error (RMSE) between two arrays of actual and predicted values. RMSE is a commonly used metric for evaluating the accuracy of regression models. The function should handle various edge cases, such as mismatched array shapes, empty arrays, and invalid input types.\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the input arrays to numpy arrays to handle cases where inputs might be lists.\n2. **Check Shapes**: Ensure both arrays have the same shape. If not, raise a ValueError.\n3. **Handle Empty Arrays**: If the arrays are empty, return 0.0 as RMSE cannot be computed with no observations.\n4. **Compute RMSE**: Calculate the squared differences between corresponding elements of the arrays, compute the mean of these squared differences, and then take the square root of this mean to get the RMSE.\n5. **Round Result**: Return the RMSE value rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Check if the shapes of y_true and y_pred match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Handle empty arrays\n    n = y_true.size\n    if n == 0:\n        return 0.0\n    \n    # Calculate the squared differences\n    squared_diff = (y_true - y_pred) ** 2\n    \n    # Compute the mean of squared differences\n    mean_squared = np.mean(squared_diff)\n    \n    # Compute the RMSE\n    rmse_value = np.sqrt(mean_squared)\n    \n    # Round to three decimal places\n    return round(rmse_value, 3)\n```\n\n### Explanation\n- **Conversion to Arrays**: The function starts by converting the inputs to numpy arrays to handle cases where inputs might be lists or other iterable types.\n- **Shape Check**: It checks if the shapes of the two arrays match. If they don't, a ValueError is raised to indicate mismatched dimensions.\n- **Empty Arrays Handling**: If the arrays are empty, the function returns 0.0 immediately since RMSE cannot be computed with no data points.\n- **RMSE Calculation**: The function computes the squared differences between corresponding elements, takes the mean of these squared differences, and then takes the square root of this mean to get the RMSE.\n- **Rounding**: The result is rounded to three decimal places for precision and returned.\n\nThis approach ensures that the function handles various edge cases and provides an accurate measure of the model's performance."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Jaccard Index for binary classification tasks. The Jaccard Index measures the similarity between two sets and is defined as the ratio of the size of their intersection to the size of their union. This index ranges from 0 (no overlap) to 1 (perfect overlap).\n\n### Approach\n1. **Convert to Boolean Arrays**: Convert the input arrays to boolean arrays to handle any non-binary values correctly, treating any non-zero value as True (1) and zero as False (0).\n2. **Compute Intersection**: Calculate the number of elements where both arrays have True values. This is done using element-wise logical AND.\n3. **Compute Union**: Calculate the number of elements where either of the arrays has a True value. This is done using element-wise logical OR.\n4. **Handle Division by Zero**: If the union is zero (both arrays are all zeros), return 1.0 as the Jaccard Index, indicating perfect overlap.\n5. **Calculate Jaccard Index**: If the union is non-zero, compute the Jaccard Index as the ratio of the intersection to the union and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    y_true = y_true.astype(bool)\n    y_pred = y_pred.astype(bool)\n    intersection = np.sum(y_true & y_pred)\n    union = np.sum(y_true | y_pred)\n    if union == 0:\n        return 1.0\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)\n```\n\n### Explanation\n- **Boolean Conversion**: The input arrays are converted to boolean type to ensure that any non-zero values are treated as True, which is essential for correctly identifying the elements in the intersection and union.\n- **Intersection Calculation**: Using element-wise logical AND (`&`), we find where both arrays have True values, which gives us the intersection count.\n- **Union Calculation**: Using element-wise logical OR (`|`), we find where either array has a True value, which gives us the union count.\n- **Division by Zero Handling**: If both arrays are all zeros, the union count is zero. In this case, we return 1.0 to indicate perfect overlap, as there are no positive elements to compare.\n- **Jaccard Index Calculation**: The Jaccard Index is computed as the ratio of the intersection count to the union count. The result is rounded to three decimal places for precision.\n\nThis approach ensures that the function handles all edge cases, including when both arrays are all zeros, and provides an accurate measure of similarity between the true and predicted labels."}
{"task_id": 73, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the Dice Score, also known as the S\u00f8rensen-Dice coefficient, for binary classification. The Dice Score measures the similarity between two sets and is particularly useful in tasks like image segmentation and binary classification.\n\n### Approach\nThe Dice Score is defined as:\n\\[ \\text{Dice Score} = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}} \\]\n\nThe approach to solve this problem involves the following steps:\n1. **Convert Input Arrays**: Ensure the input arrays are converted to NumPy arrays for efficient computation.\n2. **Calculate Sums**: Compute the sum of elements in both the true and predicted arrays to get the sizes of the sets.\n3. **Compute Intersection**: Calculate the number of elements where both arrays have a value of 1, which represents the intersection of the two sets.\n4. **Handle Edge Cases**: If both arrays have no elements (sum of both arrays is zero), return a Dice Score of 1.0 as they are perfectly similar in this case.\n5. **Compute Dice Score**: Use the formula to compute the Dice Score and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    \n    if (sum_true + sum_pred) == 0:\n        return 1.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)\n```\n\n### Explanation\n1. **Convert Input Arrays**: The function starts by converting the input arrays `y_true` and `y_pred` into NumPy arrays to leverage efficient array operations.\n2. **Calculate Sums**: The sums of `y_true` and `y_pred` are computed to determine the sizes of the sets. These sums represent the number of true positives and predicted positives, respectively.\n3. **Compute Intersection**: The intersection of the two arrays is calculated by element-wise multiplication, which results in an array where each element is 1 only if both corresponding elements in `y_true` and `y_pred` are 1. Summing this array gives the count of true positives.\n4. **Handle Edge Cases**: If both arrays have no elements (sum of both arrays is zero), the function returns a Dice Score of 1.0, as they are perfectly similar.\n5. **Compute Dice Score**: The Dice Score is computed using the formula and the result is rounded to three decimal places for precision.\n\nThis approach ensures that the function handles all edge cases appropriately and efficiently computes the Dice Score for binary classification tasks."}
{"task_id": 74, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). Each feature in the row is represented by binding hypervectors for the feature name and its value. The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary to ensure reproducibility. All feature hypervectors are then combined to create a composite hypervector for the row.\n\n### Approach\n1. **Generate Hypervectors for Features**: For each feature in the row, generate two hypervectors: one for the feature name and one for its value. These hypervectors are generated using the same seed from the `random_seeds` dictionary to ensure reproducibility.\n2. **Bind Hypervectors**: Combine the hypervectors for the feature name and value using the XOR operation. This results in a single hypervector for each feature.\n3. **Combine Feature Hypervectors**: Combine all the feature hypervectors using the XOR operation to form the composite hypervector for the entire row.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    composite = np.zeros(dim, dtype=int)\n    for feature_name, feature_value in row.items():\n        seed = random_seeds[feature_name]\n        rs = np.random.RandomState(seed)\n        name_vector = rs.choice([0, 1], size=dim)\n        value_vector = rs.choice([0, 1], size=dim)\n        feature_vector = (name_vector ^ value_vector).astype(int)\n        composite = (composite ^ feature_vector).astype(int)\n    return composite.tolist()\n```\n\n### Explanation\n1. **Initialization**: The composite hypervector is initialized as a zero vector of the given dimensionality.\n2. **Feature Processing**: For each feature in the row, the corresponding seed is retrieved from `random_seeds`. Using this seed, two hypervectors are generated: one for the feature name and one for its value.\n3. **Binding**: The hypervectors for the feature name and value are combined using the XOR operation to form a feature hypervector.\n4. **Combining Features**: Each feature hypervector is combined with the composite hypervector using the XOR operation. This ensures that the composite hypervector represents the entire row in a high-dimensional space.\n\nThis approach ensures that each feature contributes to the composite hypervector in a reproducible manner, leveraging the properties of the XOR operation to combine information from both the feature names and their values."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a confusion matrix for a binary classification problem. The confusion matrix is a 2x2 table that summarizes the prediction results, showing how many data points were correctly or incorrectly labeled.\n\n### Approach\n1. **Extract Unique Labels**: First, we extract all unique labels from the given data. Since it's a binary classification problem, there should be exactly two unique labels.\n2. **Map Labels to Indices**: We map these unique labels to indices 0 and 1. This helps in converting the actual and predicted labels into a consistent format for counting.\n3. **Count Occurrences**: Using a counter, we count how many times each combination of actual and predicted labels (after mapping) occurs.\n4. **Build Confusion Matrix**: We then construct the confusion matrix using these counts. The matrix is structured such that:\n   - The first row represents actual label 0, with the first element being true negatives (TN) and the second being false positives (FP).\n   - The second row represents actual label 1, with the first element being false negatives (FN) and the second being true positives (TP).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Extract all unique labels from the data\n    labels = set()\n    for y_true, y_pred in data:\n        labels.add(y_true)\n        labels.add(y_pred)\n    # Convert to a sorted list to ensure consistent order\n    unique_labels = sorted(labels)\n    # Create a mapping from each label to 0 or 1\n    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n    # Count each (y_true, y_pred) pair after mapping to indices\n    counts = Counter()\n    for y_true, y_pred in data:\n        y_true_idx = label_to_idx[y_true]\n        y_pred_idx = label_to_idx[y_pred]\n        counts[(y_true_idx, y_pred_idx)] += 1\n    # Build the confusion matrix\n    matrix = [\n        [counts.get((0, 0), 0), counts.get((0, 1), 0)],\n        [counts.get((1, 0), 0), counts.get((1, 1), 0)]\n    ]\n    return matrix\n```\n\n### Explanation\n- **Extract Unique Labels**: We iterate through each data point to collect all unique labels. This helps in determining the two classes involved in the binary classification.\n- **Map Labels to Indices**: By sorting the unique labels and mapping them to 0 and 1, we ensure that the confusion matrix is consistently structured regardless of the original labels.\n- **Count Occurrences**: Using a counter, we tally each occurrence of (actual, predicted) pairs after converting them to indices.\n- **Build Confusion Matrix**: The matrix is constructed by placing the counts into the appropriate positions based on the indices, resulting in a clear and concise summary of classification results.\n\nThis approach ensures that the confusion matrix is correctly generated for any binary classification problem, providing insights into the model's performance."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the cosine similarity between two vectors. Cosine similarity measures the cosine of the angle between two vectors, which indicates their directional similarity. This is useful in various applications such as natural language processing and recommendation systems.\n\n### Approach\nThe cosine similarity between two vectors can be computed using the following steps:\n1. **Compute the Dot Product**: The dot product of the two vectors is calculated. This gives a scalar value that represents how similar the vectors are in direction.\n2. **Compute the Magnitudes**: The magnitudes (or lengths) of both vectors are computed. This is done using the Euclidean norm.\n3. **Calculate Cosine Similarity**: The cosine similarity is the dot product divided by the product of the magnitudes of the two vectors. This gives a value between -1 and 1, where 1 indicates identical direction, -1 indicates opposite direction, and 0 indicates perpendicular directions.\n4. **Round the Result**: The result is rounded to three decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_theta = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_theta, 3)\n```\n\n### Explanation\n- **Dot Product Calculation**: Using `np.dot(v1, v2)`, we compute the dot product, which is a measure of how aligned the two vectors are.\n- **Magnitude Calculation**: Using `np.linalg.norm(v1)` and `np.linalg.norm(v2)`, we compute the magnitudes of the vectors. This gives the length of each vector.\n- **Cosine Similarity Calculation**: The cosine similarity is obtained by dividing the dot product by the product of the magnitudes. This value is then rounded to three decimal places using Python's `round` function to ensure the result is precise and formatted correctly.\n\nThis approach efficiently computes the cosine similarity using vector operations provided by numpy, ensuring both accuracy and performance."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates various performance metrics for a binary classification model. The metrics include the confusion matrix, accuracy, F1 score, specificity, and negative predictive value. These metrics help evaluate the performance of the model in different aspects such as correct predictions, precision, recall, and the ability to correctly predict negative outcomes.\n\n### Approach\n1. **Confusion Matrix**: This is a 2x2 matrix that summarizes the model's performance. It includes four key components:\n   - True Positives (TP): Correctly predicted positive outcomes.\n   - True Negatives (TN): Correctly predicted negative outcomes.\n   - False Positives (FP): Incorrectly predicted positive outcomes.\n   - False Negatives (FN): Incorrectly predicted negative outcomes.\n\n2. **Accuracy**: This measures the proportion of correct predictions out of all predictions made. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n\n3. **F1 Score**: This is the harmonic mean of precision and recall. Precision is TP / (TP + FP), and recall is TP / (TP + FN). The F1 score is given by 2 * (precision * recall) / (precision + recall).\n\n4. **Specificity**: This measures the proportion of actual negative outcomes that were correctly predicted. It is calculated as TN / (TN + FP).\n\n5. **Negative Predictive Value (NPV)**: This measures the proportion of actual negative outcomes that were correctly predicted when the model predicted negative. It is calculated as TN / (TN + FN).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Create a list of tuples for each actual and predicted pair\n    pairs = list(zip(actual, predicted))\n    # Count occurrences of each pair\n    counts = Counter(pairs)\n    \n    # Extract the counts for each component of the confusion matrix\n    tn = counts.get((0, 0), 0)\n    fp = counts.get((0, 1), 0)\n    fn = counts.get((1, 0), 0)\n    tp = counts.get((1, 1), 0)\n    \n    # Construct the confusion matrix\n    confusion_matrix = [\n        [tn, fp],\n        [fn, tp]\n    ]\n    \n    # Calculate the total number of samples\n    total = len(actual)\n    if total == 0:\n        accuracy = 0.0\n    else:\n        accuracy = (tp + tn) / total\n    \n    # Calculate precision and recall for F1 score\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n    \n    # Calculate F1 score\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    \n    # Calculate specificity\n    denominator_specificity = tn + fp\n    if denominator_specificity == 0:\n        specificity = 0.0\n    else:\n        specificity = tn / denominator_specificity\n    \n    # Calculate negative predictive value\n    denominator_npv = tn + fn\n    if denominator_npv == 0:\n        negative_predictive_value = 0.0\n    else:\n        negative_predictive_value = tn / denominator_npv\n    \n    # Round each metric to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n```\n\n### Explanation\n1. **Confusion Matrix Calculation**: The function first creates pairs of actual and predicted values and counts the occurrences of each pair using the `Counter` from the `collections` module. This helps in determining the values for TN, FP, FN, and TP.\n\n2. **Accuracy Calculation**: The accuracy is computed by dividing the sum of true positives and true negatives by the total number of predictions.\n\n3. **F1 Score Calculation**: The F1 score is derived from precision and recall. Precision is the ratio of true positives to the sum of true positives and false positives. Recall is the ratio of true positives to the sum of true positives and false negatives. The F1 score is the harmonic mean of precision and recall.\n\n4. **Specificity Calculation**: Specificity is the ratio of true negatives to the sum of true negatives and false positives.\n\n5. **Negative Predictive Value Calculation**: NPV is the ratio of true negatives to the sum of true negatives and false negatives.\n\nEach metric is rounded to three decimal places before being returned as part of the result tuple. This approach ensures that we handle all edge cases, such as division by zero, gracefully and return appropriate values."}
{"task_id": 78, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates various descriptive statistics for a given dataset. The function should return these statistics in a dictionary format, with specific keys and values rounded to four decimal places where appropriate.\n\n### Approach\n1. **Problem Analysis**: The task requires computing several descriptive statistics, including mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR). These statistics help in understanding the distribution and central tendency of the data.\n\n2. **Intuition**: Each statistic provides different insights:\n   - **Mean** and **Median** give central tendency.\n   - **Mode** identifies the most frequent value.\n   - **Variance** and **Standard Deviation** measure spread.\n   - **Percentiles** and **IQR** help understand data distribution.\n\n3. **Algorithm Selection**: Using NumPy functions allows efficient computation of these statistics. For example, `np.mean` for mean, `np.median` for median, and `np.percentile` for percentiles. The mode is computed using `np.mode`.\n\n4. **Rounding**: Except for the mode, all computed statistics are rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    # Calculate each statistic\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.mode(data).mode[0]\n    variance = np.var(data, ddof=0)\n    std_dev = np.std(data, ddof=0)\n    q25 = np.percentile(data, 25)\n    q50 = np.percentile(data, 50)\n    q75 = np.percentile(data, 75)\n    iqr = q75 - q25\n\n    # Round the necessary values to four decimal places\n    rounded_mean = round(mean, 4)\n    rounded_median = round(median, 4)\n    rounded_variance = round(variance, 4)\n    rounded_std_dev = round(std_dev, 4)\n    rounded_q25 = round(q25, 4)\n    rounded_q50 = round(q50, 4)\n    rounded_q75 = round(q75, 4)\n    rounded_iqr = round(iqr, 4)\n\n    # Create the result dictionary\n    result = {\n        'mean': rounded_mean,\n        'median': rounded_median,\n        'mode': mode,\n        'variance': rounded_variance,\n        'standard_deviation': rounded_std_dev,\n        '25th_percentile': rounded_q25,\n        '50th_percentile': rounded_q50,\n        '75th_percentile': rounded_q75,\n        'interquartile_range': rounded_iqr\n    }\n\n    return result\n```\n\n### Explanation\n- **Mean**: Computed using `np.mean(data)`, representing the average value.\n- **Median**: Computed using `np.median(data)`, the middle value when data is sorted.\n- **Mode**: Computed using `np.mode(data).mode[0]`, the most frequent value in the dataset.\n- **Variance**: Computed using `np.var(data, ddof=0)`, the average of squared differences from the mean.\n- **Standard Deviation**: Computed using `np.std(data, ddof=0)`, the square root of variance.\n- **Percentiles**: Computed using `np.percentile(data, q)`, where `q` is 25, 50, or 75.\n- **Interquartile Range (IQR)**: The difference between 75th and 25th percentiles.\n\nEach computed statistic, except the mode, is rounded to four decimal places to ensure precision and meet the problem's requirements. The mode is kept as is since it is a value directly from the dataset. This approach ensures that the function efficiently computes and returns the required descriptive statistics in a structured format."}
{"task_id": 79, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with a probability p of success. This can be done using the binomial distribution formula.\n\n### Approach\nThe binomial distribution formula is given by:\n\\[ P = C(n, k) \\times p^k \\times (1-p)^{n-k} \\]\nwhere:\n- \\( C(n, k) \\) is the binomial coefficient, which represents the number of ways to choose k successes out of n trials.\n- \\( p^k \\) is the probability of achieving k successes.\n- \\( (1-p)^{n-k} \\) is the probability of achieving the remaining (n-k) failures.\n\nThe steps to compute this are:\n1. Calculate the binomial coefficient using the combination formula.\n2. Compute the probability of k successes, \\( p^k \\).\n3. Compute the probability of (n-k) failures, \\( (1-p)^{n-k} \\).\n4. Multiply these three values together to get the probability.\n5. Round the result to five decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    comb = math.comb(n, k)\n    prob = comb * (p ** k) * ((1 - p) ** (n - k))\n    return round(prob, 5)\n```\n\n### Explanation\n1. **Binomial Coefficient Calculation**: The binomial coefficient \\( C(n, k) \\) is calculated using `math.comb(n, k)`, which gives the number of ways to choose k successes from n trials.\n2. **Probability of Successes**: The term \\( p^k \\) is computed using `p ** k`.\n3. **Probability of Failures**: The term \\( (1-p)^{n-k} \\) is computed using `(1 - p) ** (n - k)`.\n4. **Combining Terms**: The product of the binomial coefficient, the probability of successes, and the probability of failures gives the desired probability.\n5. **Rounding**: The result is rounded to five decimal places using Python's `round` function to meet the output constraints.\n\nThis approach efficiently computes the required probability using the binomial distribution formula and ensures the result is precise to five decimal places."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability density function (PDF) of the normal distribution for a given value, mean, and standard deviation. The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is symmetric around the mean. The PDF gives the relative likelihood for a random variable to take a given value.\n\n### Approach\nThe approach to solve this problem involves using the mathematical formula for the normal distribution's PDF. The formula is:\n\n\\[ \\text{PDF}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\]\n\nWhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nThe steps to compute the PDF are as follows:\n1. Compute the exponent term: \\(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\).\n2. Calculate the exponential of this exponent term.\n3. Compute the denominator term: \\( \\sigma \\sqrt{2\\pi} \\).\n4. Divide the exponential value by the denominator to get the PDF.\n5. Round the result to 5 decimal places for precision.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * (std_dev ** 2))\n    exp_part = math.exp(exponent)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf = exp_part / denominator\n    return round(pdf, 5)\n```\n\n### Explanation\n1. **Exponent Calculation**: The exponent term is calculated using the formula \\(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\). This term measures how far the value \\( x \\) is from the mean in terms of standard deviations.\n2. **Exponential Function**: The exponential of the exponent term is computed using `math.exp()`, which gives the value of \\( e \\) raised to the power of the exponent.\n3. **Denominator Calculation**: The denominator is computed as \\( \\sigma \\sqrt{2\\pi} \\), which normalizes the PDF so that the total area under the curve is 1.\n4. **PDF Calculation**: The PDF is obtained by dividing the exponential value by the denominator.\n5. **Rounding**: The result is rounded to 5 decimal places to ensure precision and readability.\n\nThis approach efficiently computes the PDF using the mathematical properties of the normal distribution, ensuring accuracy and correctness for any valid input values."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution formula. The Poisson distribution is a probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space.\n\n### Approach\nThe Poisson probability formula is given by:\n\\[ P(k) = \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\]\nwhere:\n- \\( e \\) is the base of the natural logarithm,\n- \\( \\lambda \\) (lambda) is the average rate (mean) of occurrences,\n- \\( k \\) is the number of events.\n\nThe steps to compute this probability are as follows:\n1. Compute \\( e^{-\\lambda} \\) using the exponential function.\n2. Compute \\( \\lambda^k \\) using the power function.\n3. Compute the factorial of \\( k \\) using the factorial function.\n4. Multiply \\( e^{-\\lambda} \\) and \\( \\lambda^k \\), then divide the result by \\( k! \\) to get the probability.\n5. Round the result to 5 decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    prob = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(prob, 5)\n```\n\n### Explanation\nThe function `poisson_probability` takes two parameters: `k` (the number of events) and `lam` (the average rate of occurrences). It calculates the probability using the Poisson distribution formula. \n\n1. **Exponential Function**: `math.exp(-lam)` computes \\( e^{-\\lambda} \\).\n2. **Power Function**: `lam ** k` computes \\( \\lambda^k \\).\n3. **Factorial Function**: `math.factorial(k)` computes \\( k! \\).\n4. **Probability Calculation**: The product of \\( e^{-\\lambda} \\) and \\( \\lambda^k \\) is divided by \\( k! \\) to get the probability.\n5. **Rounding**: The result is rounded to 5 decimal places using `round(prob, 5)`.\n\nThis approach ensures that we accurately compute the Poisson probability for any given `k` and `lam`, providing a precise and efficient solution."}
{"task_id": 82, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the contrast of a grayscale image. The contrast is defined as the difference between the maximum and minimum pixel values in the image. This measure helps in understanding the range of intensities present in the image.\n\n### Approach\nThe approach to solve this problem is straightforward and involves the following steps:\n1. **Identify the Maximum Pixel Value**: Use the `np.max()` function to find the highest pixel value in the image.\n2. **Identify the Minimum Pixel Value**: Use the `np.min()` function to find the lowest pixel value in the image.\n3. **Compute the Contrast**: Subtract the minimum pixel value from the maximum pixel value to get the contrast.\n\nThis approach ensures that we efficiently compute the contrast using built-in numpy functions, which are optimized for performance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val\n```\n\n### Explanation\n- **Finding Maximum and Minimum Values**: The functions `np.max(img)` and `np.min(img)` efficiently traverse the entire image array to find the highest and lowest pixel values, respectively.\n- **Contrast Calculation**: The contrast is simply the difference between these two values, which gives a measure of the dynamic range of the image.\n\nThis solution is efficient and handles all valid grayscale images, including edge cases where all pixels have the same value, resulting in a contrast of zero."}
{"task_id": 83, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the dot product of two 1D vectors using Python and NumPy. The dot product is a fundamental operation in linear algebra and is computed as the sum of the products of corresponding entries of the two vectors.\n\n### Approach\nThe approach to solve this problem is straightforward:\n1. **Understand the Dot Product**: The dot product of two vectors is computed by multiplying corresponding entries of the vectors and then summing those products.\n2. **Use NumPy's Built-in Function**: NumPy provides a highly optimized function `np.dot()` that efficiently computes the dot product of two arrays. This function is both concise and efficient, making it ideal for our task.\n3. **Assumptions**: The problem assumes that the input vectors are compatible (i.e., they have the same length), so we do not need to handle cases where the vectors are of different lengths.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\n- **Import NumPy**: We start by importing NumPy, which is necessary for using the `np.dot()` function.\n- **Function Definition**: The function `calculate_dot_product` takes two arguments, `vec1` and `vec2`, which are 1D NumPy arrays.\n- **Compute Dot Product**: Inside the function, we use `np.dot(vec1, vec2)` to compute the dot product. This function efficiently handles the multiplication and summation required for the dot product.\n- **Return Result**: The result of the dot product is returned as a single number.\n\nThis approach ensures that the solution is both efficient and easy to understand, leveraging NumPy's optimized operations for numerical computations."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a Phi Transformation on a given list of numerical data to map it into a higher-dimensional space by generating polynomial features. This transformation allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features.\n\n### Approach\nThe Phi Transformation involves generating all possible polynomial features up to a specified degree for each data point. Each data point is transformed into a new set of features where each feature is a product of the original features raised to various powers, ranging from 0 up to the specified degree. \n\nThe steps to achieve this are as follows:\n1. **Check for Valid Degree**: If the degree is less than 0, return an empty list as no transformation is needed.\n2. **Generate Exponent Combinations**: For each data point, generate all possible combinations of exponents for each feature, where each exponent ranges from 0 to the specified degree.\n3. **Compute Polynomial Features**: For each combination of exponents, compute the product of each feature raised to its respective exponent.\n4. **Round Results**: Round each computed polynomial feature value to 8 decimal places.\n5. **Collect Results**: Collect all transformed features for each data point into a nested list.\n\n### Solution Code\n```python\nimport numpy as np\nimport itertools\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    transformed_data = []\n    for data_point in data:\n        m = len(data_point)\n        exponents = itertools.product(range(degree + 1), repeat=m)\n        features = []\n        for exponents_tuple in exponents:\n            product = 1.0\n            for i in range(m):\n                feature = data_point[i]\n                exponent = exponents_tuple[i]\n                product *= (feature ** exponent)\n            features.append(round(product, 8))\n        transformed_data.append(features)\n    return transformed_data\n```\n\n### Explanation\n- **Check for Valid Degree**: The function first checks if the degree is less than 0. If it is, an empty list is returned immediately.\n- **Generate Exponent Combinations**: Using `itertools.product`, we generate all possible combinations of exponents for each feature. Each exponent can range from 0 to the specified degree.\n- **Compute Polynomial Features**: For each combination of exponents, we compute the product of each feature raised to its respective exponent. This product is then rounded to 8 decimal places.\n- **Collect Results**: The computed features for each data point are collected into a nested list, which is returned as the final result.\n\nThis approach ensures that each data point is transformed into a higher-dimensional space, allowing linear models to capture nonlinear relationships in the data."}
{"task_id": 85, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the positional encoding for a given sequence length and model dimensionality using sine and cosine functions as specified in the Transformer architecture. The function should return -1 if the position is 0 or if the model dimensionality is less than or equal to 0.\n\n### Approach\nThe positional encoding in Transformers is designed to capture the position information of each token in a sequence. This is achieved using a combination of sine and cosine functions. The key steps to compute the positional encoding are as follows:\n\n1. **Check for Invalid Inputs**: If the position is 0 or the model dimensionality (d_model) is less than or equal to 0, return -1.\n2. **Compute Positional Encoding**: For each dimension from 0 to d_model-1:\n   - Calculate the exponent as (2 * dimension) / d_model.\n   - Compute the denominator as 10000 raised to the power of the exponent.\n   - Determine the argument for the sine or cosine function as position divided by the denominator.\n   - Use sine for even dimensions and cosine for odd dimensions.\n3. **Return the Result**: Convert the resulting array to a list of 16-bit floats and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = []\n    for j in range(d_model):\n        exponent = (2 * j) / d_model\n        denominator = 10000 ** exponent\n        argument = position / denominator\n        if j % 2 == 0:\n            value = np.sin(argument)\n        else:\n            value = np.cos(argument)\n        encoding.append(value)\n    encoding = np.array(encoding, dtype=np.float16).tolist()\n    return encoding\n```\n\n### Explanation\n- **Invalid Input Handling**: The function first checks if the position is 0 or if the model dimensionality is non-positive. If either condition is met, it returns -1.\n- **Positional Encoding Calculation**: For each dimension, the function computes the positional encoding using sine for even dimensions and cosine for odd dimensions. The argument for these trigonometric functions is derived from the position and a dynamically computed denominator based on the dimension.\n- **Result Conversion**: The computed values are converted to a numpy array of 16-bit floats and then converted to a list before being returned.\n\nThis approach ensures that the positional encoding is computed efficiently and correctly, adhering to the specifications of the Transformer architecture."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function should return 1 for overfitting, -1 for underfitting, and 0 for a good fit.\n\n### Approach\nThe approach to solve this problem involves checking two main conditions based on the training and test accuracies:\n\n1. **Overfitting**: This occurs when the training accuracy is significantly higher than the test accuracy by more than 0.2. If the difference between the training and test accuracies is greater than 0.2, the model is considered to be overfitting.\n\n2. **Underfitting**: This occurs when both the training and test accuracies are below 0.7. If both accuracies are below this threshold, the model is considered to be underfitting.\n\nIf neither of the above conditions is met, the model is considered to be performing well, and we return 0.\n\n### Solution Code\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    diff = training_accuracy - test_accuracy\n    if diff > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n1. **Calculate the Difference**: The difference between the training and test accuracies is computed to check for overfitting.\n2. **Check for Overfitting**: If the difference is greater than 0.2, the function returns 1, indicating overfitting.\n3. **Check for Underfitting**: If both accuracies are below 0.7, the function returns -1, indicating underfitting.\n4. **Good Fit**: If neither overfitting nor underfitting conditions are met, the function returns 0, indicating the model is performing well.\n\nThis approach ensures that we accurately determine the model's performance based on the given conditions."}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam optimizer update step function. The Adam optimizer combines the benefits of two other optimization techniques, AdaGrad and RMSProp, by using moving averages of the gradient and squared gradient to adjust the learning rate. The function should handle both scalar and array inputs and include bias correction for the moving averages.\n\n### Approach\n1. **Convert Inputs to Arrays**: Ensure that all input parameters (current parameter, gradient, first moment estimate, second moment estimate) are converted to numpy arrays to handle both scalar and array inputs uniformly.\n2. **Update Moving Averages**: Compute the new estimates for the first moment (m) and the second moment (v) using the given gradients and decay rates.\n3. **Bias Correction**: Apply bias correction to the updated moving averages to account for their initial values. This is done by dividing the moving averages by terms that depend on the current timestep.\n4. **Compute Step Size**: Calculate the step size for updating the parameter using the corrected moving averages and the given learning rate.\n5. **Update Parameter**: Adjust the current parameter by subtracting the computed step size.\n6. **Round and Convert to List**: Round the updated parameter, first moment, and second moment to 5 decimal places and convert them to lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Convert inputs to numpy arrays to handle both scalars and arrays\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    \n    # Update first moment estimate\n    new_m = beta1 * m + (1 - beta1) * grad\n    # Update second moment estimate\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias correction terms\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    m_corrected = new_m / (1 - beta1_power)\n    v_corrected = new_v / (1 - beta2_power)\n    \n    # Compute step\n    step = learning_rate * m_corrected / (np.sqrt(v_corrected + epsilon))\n    \n    # Update parameter\n    updated_parameter = parameter - step\n    \n    # Round to 5 decimal places and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    new_m = np.round(new_m, 5).tolist()\n    new_v = np.round(new_v, 5).tolist()\n    \n    return (updated_parameter, new_m, new_v)\n```\n\n### Explanation\n- **Convert Inputs**: The function starts by converting all inputs to numpy arrays to handle both scalar and array inputs uniformly.\n- **Update Moving Averages**: The first moment (m) and second moment (v) are updated using their respective decay rates and the current gradient.\n- **Bias Correction**: The moving averages are corrected for bias by dividing by terms that depend on the current timestep, ensuring that the estimates are not biased towards their initial values.\n- **Compute Step Size**: The step size is calculated using the corrected moving averages, learning rate, and a small constant for numerical stability.\n- **Update Parameter**: The current parameter is updated by subtracting the computed step size.\n- **Round and Convert**: The updated parameter, first moment, and second moment are rounded to 5 decimal places and converted to lists for the output.\n\nThis approach ensures that the Adam optimizer is implemented correctly, handling both scalar and array inputs efficiently and providing accurate updates to the parameters."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified version of the GPT-2 text generation model. The goal is to generate text based on a given prompt by incorporating key components of the GPT-2 architecture, such as token embeddings, positional embeddings, multi-head attention, a feed-forward network, and layer normalization.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Token Embeddings**: Convert input tokens into dense vector representations using pre-defined token embeddings.\n2. **Positional Embeddings**: Add positional information to the token embeddings to capture the sequence order.\n3. **Layer Normalization**: Normalize the embeddings to stabilize the training process.\n4. **Logit Computation**: Compute the logits for each token in the vocabulary using the normalized embeddings.\n5. **Token Sampling**: Sample the next token based on the computed logits.\n6. **Iterative Generation**: Repeat the process for the specified number of tokens to generate the text.\n\nThe implementation uses a helper function to load the model parameters and hyperparameters, which include token embeddings, positional embeddings, and layer normalization parameters.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt into tokens\n    tokens = encoder.encode(prompt)\n    generated_tokens = list(tokens)\n    \n    for _ in range(n_tokens_to_generate):\n        # Get the last n_ctx tokens, padding with zeros if necessary\n        if len(generated_tokens) >= hparams['n_ctx']:\n            input_tokens = generated_tokens[-hparams['n_ctx']:]\n        else:\n            input_tokens = generated_tokens + [0] * (hparams['n_ctx'] - len(generated_tokens))\n        \n        # Convert tokens to embeddings\n        token_embeddings = np.array([params['wte'][t] for t in input_tokens])\n        \n        # Add positional embeddings\n        positional_embeddings = params['wpe'][:len(input_tokens)]\n        embeddings = token_embeddings + positional_embeddings\n        \n        # Apply layer normalization\n        mean = np.mean(embeddings, axis=-1, keepdims=True)\n        var = np.var(embeddings, axis=-1, keepdims=True)\n        x_normalized = (embeddings - mean) / np.sqrt(var + 1e-8) * params['ln_f']['g'] + params['ln_f']['b']\n        \n        # Compute logits\n        logits = np.dot(x_normalized, params['wte'].T)\n        \n        # Get logits for the last token in the sequence\n        last_logits = logits[-1]\n        \n        # Sample the next token\n        next_token = np.argmax(last_logits)\n        \n        # Append the new token to the generated tokens\n        generated_tokens.append(next_token)\n    \n    # Decode the generated tokens into text\n    return encoder.decode(generated_tokens)\n```\n\n### Explanation\n1. **Token Embeddings**: The input tokens are converted into vectors using the token embedding matrix (`wte`).\n2. **Positional Embeddings**: Each token's position in the sequence is added to its embedding using the positional embedding matrix (`wpe`).\n3. **Layer Normalization**: The combined embeddings are normalized to stabilize the training process.\n4. **Logit Computation**: The normalized embeddings are used to compute logits, which represent the likelihood of each token in the vocabulary.\n5. **Token Sampling**: The token with the highest logit value is selected as the next token in the sequence.\n6. **Iterative Generation**: This process is repeated for the specified number of tokens to generate the complete text.\n\nThis approach provides a simplified yet functional implementation of the GPT-2 text generation model, demonstrating the core concepts involved in autoregressive text generation."}
{"task_id": 89, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified self-attention mechanism for a sequence of crystals, each marked by a unique numeric value. The goal is to compute the attention scores for each crystal based on their relationships and derive the final weighted patterns.\n\n### Approach\n1. **Compute Attention Scores**: For each crystal, compute the dot product of its value with every other crystal's value. This gives the attention scores which represent the relationship between each pair of crystals.\n2. **Softmax Function**: Convert these attention scores into probabilities using the softmax function. This ensures that the scores sum to 1, making them suitable for weighted sums.\n3. **Weighted Sum Calculation**: For each crystal, compute the weighted sum of all other crystals' values using the attention weights derived from the softmax function. This gives the final weighted pattern for each crystal.\n4. **Output**: Return the final weighted patterns as a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum()\n    \n    result = []\n    for i in range(n):\n        scores = []\n        for j in range(n):\n            # Compute the dot product between crystal i and j\n            dot_product = sum(a * b for a, b in zip(crystal_values[i], crystal_values[j]))\n            scores.append(dot_product)\n        # Compute the attention weights using softmax\n        weights = softmax(scores)\n        # Calculate the weighted sum of the vectors\n        weighted_sum = [0.0] * dimension\n        for j in range(n):\n            weight = weights[j]\n            for k in range(dimension):\n                weighted_sum[k] += crystal_values[j][k] * weight\n        # Sum all elements of the weighted sum to get a scalar\n        output = sum(weighted_sum)\n        result.append(round(output, 4))\n    return result\n```\n\n### Explanation\n1. **Softmax Function**: The softmax function is implemented to convert the attention scores into probabilities. This is done by exponentiating the scores, normalizing them, and then dividing by their sum.\n2. **Dot Product Calculation**: For each pair of crystals, the dot product is computed to measure their interaction. This is done using a loop over all pairs.\n3. **Weighted Sum Calculation**: Each crystal's value is weighted by the attention scores and summed up to produce the final pattern. This involves element-wise multiplication and summation across all dimensions.\n4. **Output Handling**: The final result is a list of floats, each representing the weighted sum for a crystal, rounded to four decimal places.\n\nThis approach ensures that each crystal's enhanced pattern is derived based on its interactions with all other crystals, providing a comprehensive and nuanced result."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the BM25 ranking function to calculate document scores for a given query in an information retrieval context. BM25 is an extension of TF-IDF that incorporates term frequency saturation, document length normalization, and a configurable penalty for document length effects.\n\n### Approach\nThe BM25 ranking function is designed to score documents based on their relevance to a given query. The approach involves the following steps:\n\n1. **Preprocess the Corpus**: Convert each document into a list of terms (tokens) and count the frequency of each term using a `Counter`.\n2. **Compute Document Lengths**: Calculate the length of each document and the average document length across the corpus.\n3. **Precompute Term Frequencies**: Create a dictionary to map each term to the number of documents that contain it.\n4. **Process the Query**: Split the query into individual terms.\n5. **Calculate Term Frequencies for Query Terms**: For each term in the query, determine how many documents contain that term.\n6. **Score Each Document**: For each document, compute the score based on the BM25 formula, which includes term frequency normalization, inverse document frequency, and document length normalization.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Preprocess the corpus into Counters\n    docs_terms = [Counter(doc.split()) for doc in corpus]\n    \n    # Compute L_d for each document\n    L_d_list = [len(doc.split()) for doc in corpus]\n    \n    # Compute L_avg\n    if L_d_list:\n        L_avg = sum(L_d_list) / len(L_d_list)\n    else:\n        L_avg = 0  # Handle empty corpus case\n    \n    # Precompute term_to_doc_count\n    term_to_doc_count = {}\n    for doc in docs_terms:\n        for term in doc:\n            term_to_doc_count[term] = term_to_doc_count.get(term, 0) + 1\n    \n    # Preprocess the query into terms\n    query_terms = query.split()\n    \n    # Compute n_t for each query term\n    n_t_dict = {term: term_to_doc_count.get(term, 0) for term in query_terms}\n    \n    # Now, compute the scores for each document\n    scores = []\n    N = len(corpus)\n    \n    for doc_terms, L_d in zip(docs_terms, L_d_list):\n        score = 0.0\n        for term in query_terms:\n            if term in doc_terms:\n                tf = doc_terms[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1\n                term_score = numerator / denominator\n                n_t = n_t_dict[term]\n                # Compute idf\n                if n_t == 0:\n                    idf = 0.0\n                else:\n                    idf = np.log((N - n_t + 1) / n_t)\n                score += term_score * idf\n        \n        # Compute length part\n        if L_avg == 0:\n            length_part = 0.0\n        else:\n            length_part = b * np.log(L_d / L_avg)\n        \n        total_score = score + length_part\n        scores.append(round(total_score, 3))\n    \n    return scores\n```\n\n### Explanation\n1. **Preprocessing the Corpus**: Each document is split into terms, and the frequency of each term is counted using `Counter`.\n2. **Document Lengths**: The length of each document is calculated, and the average length is computed.\n3. **Term Frequencies**: A dictionary is created to map each term to the number of documents that contain it, which helps in efficiently determining the inverse document frequency for query terms.\n4. **Query Processing**: The query is split into individual terms.\n5. **Scoring Documents**: For each document, the score is computed by summing contributions from each query term, considering term frequency normalization and inverse document frequency. The document length normalization is added to the score to adjust for document length effects.\n\nThis approach ensures that documents are scored based on their relevance to the query, taking into account both term frequency and document length, providing a robust ranking mechanism."}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the F1 score given the true and predicted labels. The F1 score is a measure of a model's accuracy that considers both precision and recall, providing a balanced metric that is useful for evaluating classification models.\n\n### Approach\nThe F1 score is the harmonic mean of precision and recall. Precision is the ratio of correctly predicted positive labels to the total predicted positive labels, while recall is the ratio of correctly predicted positive labels to the total actual positive labels. The F1 score is calculated as:\n\n\\[ F1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\]\n\nTo compute the F1 score, we follow these steps:\n1. **Count True Positives (TP), False Positives (FP), and False Negatives (FN)**: For each instance, if the true label matches the predicted label, it is a TP. If the predicted label is incorrect, it contributes to either FP or FN depending on whether the prediction was a false positive or false negative.\n2. **Sum the Counts**: Sum up all TP, FP, and FN across all instances.\n3. **Compute Precision and Recall**: Using the summed counts, compute precision and recall.\n4. **Calculate F1 Score**: Use the harmonic mean formula to compute the F1 score from precision and recall.\n\n### Solution Code\n```python\nfrom collections import defaultdict\n\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = defaultdict(int)\n    fp = defaultdict(int)\n    fn = defaultdict(int)\n    \n    for t, p in zip(y_true, y_pred):\n        if t == p:\n            tp[t] += 1\n        else:\n            fp[p] += 1\n            fn[t] += 1\n    \n    total_tp = sum(tp.values())\n    total_fp = sum(fp.values())\n    total_fn = sum(fn.values())\n    \n    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) != 0 else 0\n    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) != 0 else 0\n    \n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    \n    return round(f1, 3)\n```\n\n### Explanation\n1. **Initialization**: We use dictionaries to count TP, FP, and FN for each class.\n2. **Iterate Through Labels**: For each pair of true and predicted labels, we update the counts based on whether the prediction was correct or incorrect.\n3. **Sum Counts**: We sum up the counts of TP, FP, and FN across all classes.\n4. **Compute Precision and Recall**: These are calculated using the summed counts, ensuring we handle cases where division by zero might occur.\n5. **Calculate F1 Score**: Using the harmonic mean formula, we compute the F1 score and round it to three decimal places.\n\nThis approach efficiently computes the F1 score by considering all instances and classes, providing a balanced measure of a model's performance."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the power consumption for day 15 by removing a known daily fluctuation from the given data, fitting a linear regression model, and then adding back the fluctuation for day 15 with a 5% safety margin.\n\n### Approach\n1. **Subtract Daily Fluctuation**: The given data has a daily fluctuation that follows the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\). We subtract this fluctuation from each data point to get the detrended data.\n2. **Linear Regression**: Using the detrended data, we fit a linear regression model. This model helps us understand the underlying trend in the data.\n3. **Predict Day 15 Consumption**: Using the linear regression model, we predict the base consumption for day 15.\n4. **Add Fluctuation for Day 15**: The fluctuation for day 15 is calculated and added back to the predicted base consumption. For day 15, the fluctuation is zero.\n5. **Add Safety Margin**: We round the predicted value, add a 5% safety margin, and then round up to the nearest integer to ensure we meet future needs.\n\n### Solution Code\n```python\nimport math\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended = []\n    for i in range(10):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(consumption_data[i] - fluctuation)\n    \n    # Step 2: Perform linear regression on the detrended data\n    x = list(range(1, 11))  # Day numbers 1 through 10\n    y = detrended\n    \n    n = len(x)\n    mean_x = sum(x) / n\n    mean_y = sum(y) / n\n    \n    numerator = 0\n    denominator = 0\n    for xi, yi in zip(x, y):\n        numerator += (xi - mean_x) * (yi - mean_y)\n        denominator += (xi - mean_x) ** 2\n    \n    a = numerator / denominator if denominator != 0 else 0\n    b = mean_y - a * mean_x\n    \n    # Step 3: Predict day 15's base consumption\n    x_pred = 15\n    y_pred = a * x_pred + b\n    \n    # Step 4: Add the day 15 fluctuation (which is 0)\n    fluctuation_day15 = 10 * math.sin(2 * PI * 15 / 10)\n    y_pred += fluctuation_day15\n    \n    # Step 5: Round, add 5% safety margin, and round up\n    rounded = round(y_pred)\n    safety_added = rounded * 1.05\n    final_value = math.ceil(safety_added)\n    \n    return final_value\n```\n\n### Explanation\n1. **Subtract Daily Fluctuation**: For each day, we compute the fluctuation using the given formula and subtract it from the consumption data to get the detrended data.\n2. **Linear Regression**: We calculate the coefficients of the linear regression model using the means of the detrended data and the sum of products and squares of deviations.\n3. **Predict Day 15 Consumption**: Using the regression coefficients, we predict the base consumption for day 15.\n4. **Add Fluctuation for Day 15**: The fluctuation for day 15 is zero, so we only need to add this value back, which does not change the prediction.\n5. **Add Safety Margin**: We ensure the prediction is safe by rounding up after adding a 5% margin to the rounded prediction.\n\nThis approach efficiently removes known fluctuations, models the underlying trend, and ensures future needs are met with a safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. The MAE is a metric used to measure the average magnitude of errors in a set of predictions without considering their direction.\n\n### Approach\nThe approach to calculate the MAE involves the following steps:\n1. **Compute Absolute Differences**: For each element in the arrays, compute the absolute difference between the true value and the predicted value.\n2. **Sum the Differences**: Sum all these absolute differences.\n3. **Compute the Mean**: Divide the sum of absolute differences by the number of elements to get the mean.\n4. **Round the Result**: Round the resulting mean to three decimal places for the final output.\n\nThis approach ensures that we accurately measure the average magnitude of errors, providing a clear and concise metric for evaluating prediction accuracy.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Compute the absolute differences between true and predicted values\n    absolute_diffs = np.abs(y_true - y_pred)\n    # Calculate the mean of these absolute differences\n    mae_value = np.mean(absolute_diffs)\n    # Round the result to 3 decimal places\n    return round(mae_value, 3)\n```\n\n### Explanation\n1. **Compute Absolute Differences**: The function first calculates the absolute differences between each corresponding element in the `y_true` and `y_pred` arrays using `np.abs(y_true - y_pred)`.\n2. **Sum the Differences**: The absolute differences are then summed up.\n3. **Compute the Mean**: The mean of these absolute differences is computed using `np.mean()`, which gives the MAE.\n4. **Round the Result**: Finally, the computed MAE is rounded to three decimal places using the `round()` function to provide a clean and precise result.\n\nThis method efficiently computes the MAE using vectorized operations provided by NumPy, ensuring both accuracy and performance."}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the multi-head attention mechanism, a key component in the Transformer model. This involves three main functions: `compute_qkv`, `self_attention`, and `multi_head_attention`.\n\n### Approach\n1. **Compute Query, Key, and Value Vectors (`compute_qkv`)**:\n   - This function takes the input sequence `X` and the weight matrices for queries (`W_q`), keys (`W_k`), and values (`W_v`). It splits these weight matrices into submatrices for each head and computes the corresponding query, key, and value vectors for each head.\n\n2. **Self-Attention (`self_attention`)**:\n   - This function computes the attention scores for each head using the query and key vectors. It then applies a softmax function to these scores to get attention weights. Finally, it computes the output by applying these weights to the value vectors.\n\n3. **Multi-Head Attention (`multi_head_attention`)**:\n   - This function orchestrates the computation across all heads. It first computes the query, key, and value vectors for each head using `compute_qkv`. It then applies the `self_attention` function to each head's vectors and collects the outputs. The results are rounded to four decimal places and converted to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    d_model = X.shape[2]\n    d_head = W_q.shape[1] // n_heads\n    qkv_list = []\n    for h in range(n_heads):\n        Wq = W_q[:, h * d_head : (h + 1) * d_head]\n        Wk = W_k[:, h * d_head : (h + 1) * d_head]\n        Wv = W_v[:, h * d_head : (h + 1) * d_head]\n        q = X @ Wq\n        k = X @ Wk\n        v = X @ Wv\n        qkv_list.append((q, k, v))\n    return qkv_list\n\ndef self_attention(q, k, v):\n    d_head = q.shape[2]\n    scores = (q @ k.T) / np.sqrt(d_head)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=2, keepdims=True)\n    output = (v.T @ attention_weights.T).T\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    qkv_list = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    head_outputs = []\n    for q, k, v in qkv_list:\n        output = self_attention(q, k, v)\n        head_outputs.append(output)\n    result = []\n    for out in head_outputs:\n        rounded = np.round(out, 4)\n        result.append(rounded.tolist())\n    return result\n```\n\n### Explanation\n1. **Compute Query, Key, and Value Vectors**:\n   - The `compute_qkv` function splits the weight matrices into submatrices for each head. For each head, it computes the query, key, and value vectors by matrix multiplication of the input sequence with the respective submatrix.\n\n2. **Self-Attention**:\n   - The `self_attention` function calculates the attention scores by taking the dot product of the query and key vectors, scaled by the square root of the dimension of the key vectors. It then applies the softmax function to these scores to get attention weights. The output is computed by applying these weights to the value vectors.\n\n3. **Multi-Head Attention**:\n   - The `multi_head_attention` function uses `compute_qkv` to get the query, key, and value vectors for each head. It then applies the `self_attention` function to each head's vectors, collects the outputs, rounds them to four decimal places, and converts them to a list for the final result.\n\nThis approach ensures that each head processes the input sequence independently, allowing the model to focus on different parts of the input simultaneously, which is a key feature of the Transformer architecture."}
{"task_id": 95, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Phi coefficient, a measure of the correlation between two binary variables. The Phi coefficient is particularly useful for understanding the relationship between two binary variables, such as whether one variable can predict the value of another.\n\n### Approach\nThe Phi coefficient is calculated using a 2x2 contingency table, which counts the occurrences of each combination of the binary variables. The formula for the Phi coefficient is:\n\n\\[ \\Phi = \\frac{n_{11} \\cdot n_{00} - n_{10} \\cdot n_{01}}{\\sqrt{(n_{11} + n_{10}) \\cdot (n_{11} + n_{01}) \\cdot (n_{00} + n_{10}) \\cdot (n_{00} + n_{01})}} \\]\n\nWhere:\n- \\( n_{11} \\) is the count of cases where both variables are 1.\n- \\( n_{10} \\) is the count of cases where the first variable is 1 and the second is 0.\n- \\( n_{01} \\) is the count of cases where the first variable is 0 and the second is 1.\n- \\( n_{00} \\) is the count of cases where both variables are 0.\n\nThe steps to compute the Phi coefficient are:\n1. Construct the 2x2 contingency table by iterating through the input lists and counting the occurrences of each combination.\n2. Calculate the numerator and denominator of the Phi coefficient formula.\n3. Handle the special case where the denominator is zero, which indicates no variation in the data, resulting in a Phi coefficient of 0.\n4. Compute the Phi coefficient and round it to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n11 = 0\n    n10 = 0\n    n01 = 0\n    n00 = 0\n\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:\n            n00 += 1\n\n    row1 = n11 + n10\n    row2 = n01 + n00\n    col1 = n11 + n01\n    col2 = n10 + n00\n\n    denominator_squared = row1 * row2 * col1 * col2\n\n    if denominator_squared == 0:\n        return 0.0\n    else:\n        denominator = math.sqrt(denominator_squared)\n        numerator = (n11 * n00) - (n10 * n01)\n        phi = numerator / denominator\n\n    return round(phi, 4)\n```\n\n### Explanation\n1. **Contingency Table Construction**: The function iterates through each pair of values from the input lists `x` and `y`, updating the counts for each combination of binary values.\n2. **Numerator Calculation**: The numerator is computed as the difference between the product of the diagonal elements of the contingency table.\n3. **Denominator Calculation**: The denominator is the square root of the product of the sums of the rows and columns of the contingency table.\n4. **Special Case Handling**: If the denominator is zero, indicating no variation in the data, the function returns 0.0.\n5. **Phi Coefficient Calculation**: The Phi coefficient is computed and rounded to four decimal places before being returned.\n\nThis approach ensures that the Phi coefficient is accurately calculated and handles edge cases where the data might not vary, providing a robust solution for binary variable correlation analysis."}
{"task_id": 96, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Hard Sigmoid activation function, which is a computationally efficient approximation of the standard sigmoid function. The Hard Sigmoid function maps any input value to a value between 0 and 1 using a piecewise linear approach.\n\n### Approach\nThe Hard Sigmoid function is defined as follows:\n- If the input value \\( x \\) is less than or equal to -2.5, the function returns 0.\n- If the input value \\( x \\) is greater than or equal to 2.5, the function returns 1.\n- For values of \\( x \\) between -2.5 and 2.5, the function returns a linear interpolation between 0 and 1, calculated as \\( 0.2x + 0.5 \\).\n\nThis approach ensures that the function is efficient and straightforward, avoiding the computational cost of exponential operations found in the standard sigmoid function.\n\n### Solution Code\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n```\n\n### Explanation\nThe function `hard_sigmoid` takes a single input value \\( x \\) and returns the corresponding output based on the defined piecewise function:\n1. If \\( x \\) is less than or equal to -2.5, the function returns 0.\n2. If \\( x \\) is greater than or equal to 2.5, the function returns 1.\n3. For values of \\( x \\) between -2.5 and 2.5, the function computes the output using the linear formula \\( 0.2x + 0.5 \\).\n\nThis implementation efficiently maps any input value to a value between 0 and 1, making it suitable for use in neural networks where computational efficiency is crucial."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the ELU (Exponential Linear Unit) activation function. The ELU function is designed to mitigate the limitations of the ReLU function by providing negative outputs for negative inputs, which helps in stabilizing the training of neural networks.\n\n### Approach\nThe ELU function is defined as follows:\n- For non-negative inputs (x >= 0), the function returns the input value x.\n- For negative inputs (x < 0), the function returns a value based on the exponential function, specifically alpha multiplied by (e^x - 1), where alpha is a parameter that scales the negative output.\n\nThe steps to implement the ELU function are:\n1. Check if the input value x is non-negative. If it is, return x rounded to four decimal places.\n2. If x is negative, compute the result using the formula alpha * (e^x - 1) and round it to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```\n\n### Explanation\n- The function `elu` takes two parameters: `x` (the input value) and `alpha` (a scaling parameter for negative outputs, defaulting to 1.0).\n- If the input `x` is non-negative, the function returns `x` rounded to four decimal places.\n- If the input `x` is negative, the function computes the result using the formula `alpha * (math.exp(x) - 1)` and rounds it to four decimal places.\n- The use of `math.exp(x)` ensures that we correctly compute the exponential part of the formula for negative values of `x`.\n\nThis implementation efficiently handles both positive and negative inputs, providing the correct ELU activation value as required."}
{"task_id": 98, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the PReLU (Parametric ReLU) activation function. PReLU is a variant of the ReLU activation function that introduces a learnable parameter for negative inputs, allowing the model to learn a small slope for negative values instead of being zero.\n\n### Approach\nThe PReLU activation function is defined such that for positive input values, the output is the same as the input. For negative input values, the output is the product of the input and a learnable parameter alpha. This can be mathematically represented as:\n- If \\( x \\geq 0 \\): \\( \\text{PReLU}(x) = x \\)\n- If \\( x < 0 \\): \\( \\text{PReLU}(x) = \\alpha \\times x \\)\n\nTo efficiently compute this without using conditional statements, we can leverage the `max` function. The `max` function allows us to return the higher of the two values: \\( x \\) for positive inputs and \\( \\alpha \\times x \\) for negative inputs. This approach is both concise and efficient.\n\n### Solution Code\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(x, alpha * x)\n```\n\n### Explanation\nThe function `prelu` takes two parameters: `x` (the input value) and `alpha` (the slope parameter for negative inputs, with a default value of 0.25). The function computes the PReLU activation value by returning the maximum of `x` and `alpha * x`. This effectively applies the PReLU function as described, ensuring that positive inputs are unchanged while negative inputs are scaled by `alpha`. This approach is efficient and avoids the overhead of conditional checks, making it suitable for use in neural networks where performance is crucial."}
{"task_id": 99, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softplus activation function, which is a smooth approximation of the Rectified Linear Unit (ReLU) function. The Softplus function is defined as \\( \\text{Softplus}(x) = \\log(1 + e^x) \\). The challenge is to compute this function efficiently while handling edge cases to prevent numerical overflow or underflow.\n\n### Approach\nThe Softplus function can be computed directly for most values of \\( x \\), but we need to handle very large and very small values of \\( x \\) carefully to avoid numerical issues. Here's the approach:\n\n1. **For large positive values of \\( x \\) (greater than 20)**:\n   - Directly computing \\( e^x \\) can lead to overflow. Instead, we use the approximation \\( \\log(1 + e^x) \\approx x \\) for large \\( x \\). To maintain precision, we compute it as \\( x + \\exp(-x) \\).\n\n2. **For large negative values of \\( x \\) (less than -20)**:\n   - Directly computing \\( e^x \\) can lead to underflow. Instead, we use the approximation \\( \\log(1 + e^x) \\approx e^x \\) for very small \\( e^x \\).\n\n3. **For values of \\( x \\) between -20 and 20**:\n   - We compute the Softplus function directly using \\( \\log(1 + e^x) \\).\n\nAfter computing the Softplus value, we round the result to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 20:\n        value = x + math.exp(-x)\n    elif x < -20:\n        value = math.exp(x)\n    else:\n        value = math.log(1 + math.exp(x))\n    return round(value, 4)\n```\n\n### Explanation\n- **Handling Large Positive Values**: When \\( x \\) is greater than 20, we compute the result as \\( x + \\exp(-x) \\) to avoid overflow. This is because \\( e^x \\) becomes extremely large, making \\( \\log(1 + e^x) \\) approximately equal to \\( x \\).\n- **Handling Large Negative Values**: When \\( x \\) is less than -20, we compute the result as \\( \\exp(x) \\) because \\( e^x \\) is very small, making \\( \\log(1 + e^x) \\) approximately equal to \\( e^x \\).\n- **Direct Computation for Moderate Values**: For values of \\( x \\) between -20 and 20, we directly compute \\( \\log(1 + e^x) \\) as the numerical range is manageable and avoids both overflow and underflow issues.\n\nThis approach ensures that we handle all possible values of \\( x \\) efficiently and accurately, while also meeting the requirement to return the result rounded to four decimal places."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth activation function used in neural networks. The Softsign function is defined as \\( f(x) = \\frac{x}{1 + |x|} \\). This function ensures that the output is bounded between -1 and 1, making it suitable for various neural network applications.\n\n### Approach\nThe approach to implement the Softsign function involves the following steps:\n1. **Compute the Absolute Value**: Calculate the absolute value of the input \\( x \\).\n2. **Calculate the Denominator**: Add 1 to the absolute value of \\( x \\).\n3. **Compute the Softsign Value**: Divide the input \\( x \\) by the computed denominator.\n4. **Round the Result**: Round the result to four decimal places to meet the output constraints.\n\nThis approach ensures that the function is computed efficiently and the result is formatted correctly.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```\n\n### Explanation\n- **Compute the Absolute Value**: The absolute value of \\( x \\) is computed using `abs(x)`.\n- **Calculate the Denominator**: The denominator is \\( 1 + |x| \\), which ensures that the function approaches 1 as \\( x \\) becomes very large and approaches -1 as \\( x \\) becomes very negative.\n- **Compute the Softsign Value**: The result is obtained by dividing \\( x \\) by the denominator.\n- **Round the Result**: The result is rounded to four decimal places using Python's `round` function to meet the specified output format.\n\nThis implementation efficiently computes the Softsign function and ensures the output is within the required bounds and precision."}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the GRPO (Group Relative Policy Optimization) objective function used in reinforcement learning. The objective function combines a clipped policy gradient with a KL divergence penalty to maintain training stability.\n\n### Approach\nThe GRPO objective function is designed to optimize policy parameters by combining two key components:\n1. A clipped policy gradient to ensure stable updates.\n2. A KL divergence penalty to control the difference between the new policy and a reference policy.\n\nThe steps to compute the GRPO objective function are as follows:\n1. **Clipping the Likelihood Ratios**: The likelihood ratios (rhos) are clipped to ensure they stay within a specified range, which helps in maintaining stable updates.\n2. **Compute the Clipped Policy Gradient**: The clipped likelihood ratios are multiplied by the advantage estimates, and the average of these values is computed.\n3. **Compute the KL Divergence**: The KL divergence between the new policy and the reference policy is calculated. This involves summing the contributions from each state, which are derived from the likelihood ratios, old policy probabilities, and reference policy probabilities.\n4. **Combine the Components**: The clipped policy gradient and the KL divergence penalty are combined to form the total objective function value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Clip the likelihood ratios\n    clipped_rho = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    \n    # Compute the first term: average of clipped_rho * A\n    first_term = np.mean(clipped_rho * A)\n    \n    # Compute the KL divergence term\n    sum_kl = 0.0\n    for rho, pi_old, pi_ref in zip(rhos, pi_theta_old, pi_theta_ref):\n        if rho == 0 or pi_old == 0 or pi_ref == 0:\n            continue  # Avoid log(0), assuming inputs are valid\n        term = rho * pi_old * (np.log(rho) + np.log(pi_old) - np.log(pi_ref))\n        sum_kl += term\n    \n    # Apply KL penalty\n    kl_penalty = beta * sum_kl\n    \n    # Total objective\n    total = first_term + kl_penalty\n    \n    # Round to 6 decimal places\n    return round(total, 6)\n```\n\n### Explanation\n1. **Clipping the Likelihood Ratios**: The likelihood ratios are clipped to ensure they do not deviate too much from 1, which helps in preventing large updates that could destabilize training.\n2. **Clipped Policy Gradient**: The clipped likelihood ratios are multiplied by the advantage estimates, and the average is taken to form the first part of the objective function.\n3. **KL Divergence Calculation**: The KL divergence between the new policy and the reference policy is computed using the likelihood ratios, old policy probabilities, and reference policy probabilities. This ensures that the new policy does not diverge too much from the reference policy.\n4. **Combining Components**: The clipped policy gradient and the KL divergence penalty are combined to form the final objective function value, which is then rounded to six decimal places for precision.\n\nThis approach ensures that the policy updates are both stable and controlled, making the training process more efficient and reliable."}
{"task_id": 102, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Swish activation function, which is a self-gated activation function used in deep neural networks. The Swish function is defined as \\( \\text{Swish}(x) = x \\cdot \\sigma(\\beta x) \\), where \\( \\sigma \\) is the sigmoid function and \\( \\beta \\) is a parameter typically set to 1.0.\n\n### Approach\n1. **Understanding the Swish Function**: The Swish function combines the input value \\( x \\) with the sigmoid of \\( \\beta x \\). The sigmoid function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). For this implementation, we use \\( \\beta = 1.0 \\).\n2. **Compute the Sigmoid**: For a given input \\( x \\), compute the sigmoid value using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n3. **Apply Swish Function**: Multiply the input \\( x \\) by the computed sigmoid value.\n4. **Rounding**: Round the result to the nearest 4th decimal place to meet the output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    beta = 1.0\n    sigmoid = 1 / (1 + math.exp(-beta * x))\n    result = x * sigmoid\n    return round(result, 4)\n```\n\n### Explanation\n- **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This function maps any real number to a value between 0 and 1.\n- **Swish Function**: The Swish function is then calculated by multiplying the input \\( x \\) with the sigmoid value. This function is known for its smoothness and ability to handle both positive and negative values effectively.\n- **Rounding**: The result is rounded to four decimal places to ensure precision and meet the specified output constraints.\n\nThis approach ensures that the Swish activation function is implemented efficiently and correctly, providing the desired output for any given input value."}
{"task_id": 103, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the SELU (Scaled Exponential Linear Unit) activation function. SELU is a self-normalizing variant of the ELU activation function, designed to maintain mean and variance of the activations, which helps in training deep neural networks effectively.\n\n### Approach\nThe SELU activation function is defined as follows:\n- For non-negative input values (x \u2265 0), the function returns the input value scaled by a factor.\n- For negative input values (x < 0), the function returns a scaled version of the exponential function applied to the input, adjusted by specific constants to ensure self-normalization.\n\nThe specific formula for SELU is:\n- SELU(x) = scale * x, if x \u2265 0\n- SELU(x) = scale * \u03b1 * (e^x - 1), if x < 0\n\nHere, `alpha` and `scale` are constants with predefined values to ensure the self-normalizing property.\n\n### Solution Code\n```python\nimport math\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x >= 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Function Definition**: The function `selu` takes a single input `x` and returns the computed SELU value.\n2. **Constants**: The constants `alpha` and `scale` are predefined to ensure the self-normalizing property of the SELU function.\n3. **Condition Check**: The function checks if the input `x` is non-negative or negative.\n   - If `x` is non-negative, the result is simply `x` scaled by `scale`.\n   - If `x` is negative, the result is computed using the exponential function adjusted by `alpha` and `scale`.\n4. **Rounding**: The result is rounded to four decimal places to ensure numerical stability and precision.\n\nThis approach ensures that the SELU function is both efficient and numerically stable, making it suitable for use in deep neural networks."}
{"task_id": 104, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function for binary classification using logistic regression. The goal is to compute class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Approach\n1. **Compute the Linear Combination**: For each row in the feature matrix \\(X\\), compute the linear combination of the features and weights, then add the bias. This gives the log-odds.\n2. **Apply the Sigmoid Function**: Convert the log-odds into probabilities using the sigmoid function. The sigmoid function outputs values between 0 and 1, representing the probability of the positive class.\n3. **Threshold the Probabilities**: Compare each probability to 0.5. If the probability is greater than or equal to 0.5, predict 1; otherwise, predict 0.\n4. **Return the Result**: Convert the resulting predictions from a numpy array to a Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of features and weights plus bias\n    z = X.dot(weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    \n    # Threshold the probabilities to get binary predictions\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Convert numpy array to list and return\n    return predictions.tolist()\n```\n\n### Explanation\n1. **Linear Combination**: The linear combination is computed using matrix multiplication of the feature matrix \\(X\\) with the weights vector, followed by the addition of the bias term. This step transforms the input features into a linear score.\n2. **Sigmoid Function**: The sigmoid function is applied to the linear scores to convert them into probabilities. The sigmoid function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\), which maps any real number to a value between 0 and 1.\n3. **Thresholding**: Each probability is compared against a threshold of 0.5. Probabilities equal to or greater than 0.5 are classified as 1, while those below 0.5 are classified as 0.\n4. **Conversion to List**: The resulting predictions, which are in a numpy array format, are converted to a Python list to meet the output requirements.\n\nThis approach efficiently computes the binary predictions using the principles of logistic regression, ensuring that the output is both accurate and formatted correctly."}
{"task_id": 105, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient descent-based training algorithm for Softmax regression. The goal is to compute the model parameters using the Cross Entropy loss and return the optimized coefficients along with the collected loss values over iterations.\n\n### Approach\n1. **Initialization**: Start by initializing the weight matrix with small random values to avoid symmetry and ensure the model can learn effectively.\n2. **Forward Propagation**: Compute the logits by taking the dot product of the input features and the weight matrix. Apply the Softmax function to convert these logits into probabilities.\n3. **Loss Calculation**: Use the Cross Entropy loss to measure the error between the predicted probabilities and the actual labels.\n4. **Backward Propagation**: Compute the gradient of the loss with respect to the weights using the chain rule. This involves calculating the difference between the predicted probabilities and the true labels, then using this difference to update the weights.\n5. **Weight Update**: Adjust the weights using the computed gradient and the given learning rate.\n6. **Iteration**: Repeat the forward propagation, loss calculation, backward propagation, and weight update steps for the specified number of iterations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Get the number of features and classes\n    M = X.shape[1]\n    K = len(np.unique(y))\n    N = X.shape[0]\n    \n    # Initialize the weight matrix with small random values\n    B = np.random.randn(M, K) * 0.01\n    \n    # Precompute the one-hot encoding of the labels\n    y_one_hot = np.zeros((N, K))\n    y_one_hot[np.arange(N), y] = 1\n    \n    # List to store the losses\n    losses = []\n    \n    for _ in range(iterations):\n        # Forward pass\n        z = X.dot(B)\n        # Apply softmax with numerical stability\n        z -= np.max(z, axis=1, keepdims=True)\n        p = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n        \n        # Compute the loss\n        loss = -np.mean(np.log(p[np.arange(N), y]))\n        losses.append(round(loss, 4))\n        \n        # Backward pass\n        dz = p - y_one_hot\n        dW = (X.T.dot(dz)) / N\n        \n        # Update the weights\n        B -= learning_rate * dW\n    \n    # Round the final weights and convert to list\n    B_rounded = np.round(B, 4).tolist()\n    # Convert losses to list of rounded values\n    losses_rounded = [round(loss, 4) for loss in losses]\n    \n    return (B_rounded, losses_rounded)\n```\n\n### Explanation\n1. **Initialization**: The weight matrix `B` is initialized with small random values to break symmetry and ensure effective learning.\n2. **Forward Propagation**: The logits are computed using the dot product of the input features and the weights. The Softmax function is applied to convert these logits into probabilities.\n3. **Loss Calculation**: The Cross Entropy loss is computed by taking the mean of the negative logarithm of the predicted probabilities for the true classes.\n4. **Backward Propagation**: The gradient of the loss with respect to the weights is computed using the difference between the predicted probabilities and the true labels.\n5. **Weight Update**: The weights are updated using the computed gradient and the given learning rate.\n6. **Iteration**: The process is repeated for the specified number of iterations, with the loss values being collected and stored at each step.\n\nThis approach ensures that the model parameters are optimized using gradient descent, and the loss values are collected to monitor the training process."}
{"task_id": 106, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient descent-based training algorithm for logistic regression using the Binary Cross Entropy (BCE) loss function. The goal is to compute the model parameters and return the optimized coefficients along with the loss values collected over each iteration.\n\n### Approach\n1. **Initialize Weights**: Start with weights initialized to zero. The shape of the weights matches the number of features in the input data.\n2. **Compute Hypothesis**: Use the sigmoid function to compute the hypothesis, which represents the probability of the positive class.\n3. **Calculate Loss**: Use the BCE loss function to compute the loss, which measures the error between the predicted probabilities and the actual labels.\n4. **Compute Gradient**: Calculate the gradient of the loss with respect to each weight using the chain rule. This gradient indicates the direction in which the weights should be updated to minimize the loss.\n5. **Update Weights**: Adjust the weights by subtracting the product of the learning rate and the gradient. This step is repeated for the specified number of iterations.\n6. **Store Loss**: After each iteration, store the computed loss to track the convergence of the model.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    m = X.shape[0]  # Number of samples\n    n = X.shape[1]  # Number of features\n    \n    # Initialize weights\n    w = np.zeros(n)\n    \n    # List to store the loss after each iteration\n    losses = []\n    \n    for _ in range(iterations):\n        # Compute the hypothesis (a = sigmoid(Xw))\n        z = np.dot(X, w)\n        a = 1 / (1 + np.exp(-z))\n        \n        # Compute the loss\n        loss = (-y * np.log(a) - (1 - y) * np.log(1 - a)).mean()\n        losses.append(loss)\n        \n        # Compute the gradient\n        gradient = (X.T.dot(a - y)) / m\n        \n        # Update the weights\n        w = w - learning_rate * gradient\n    \n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    losses_rounded = [np.round(loss, 4) for loss in losses]\n    \n    return (w_rounded, losses_rounded)\n```\n\n### Explanation\n- **Initialization**: The weights are initialized to zero to start the optimization process.\n- **Hypothesis Calculation**: The sigmoid function is used to convert the linear combination of features and weights into a probability.\n- **Loss Calculation**: The BCE loss measures the discrepancy between the predicted probabilities and the actual labels.\n- **Gradient Calculation**: The gradient is computed using the chain rule, which helps in determining the direction of the weight update.\n- **Weight Update**: The weights are updated iteratively using the computed gradient and the given learning rate.\n- **Loss Tracking**: The loss after each iteration is stored to monitor the model's convergence.\n\nThis approach ensures that the model parameters are optimized using gradient descent, and the loss values are tracked to evaluate the training process."}
{"task_id": 107, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the masked self-attention mechanism used in sequence modeling tasks. This mechanism allows each position in a sequence to attend to all previous positions but not to future positions, which is crucial for tasks like text generation.\n\n### Approach\nThe approach involves several key steps:\n\n1. **Compute Query, Key, and Value Matrices**: Using the input sequence and given weight matrices, compute the Query (Q), Key (K), and Value (V) matrices.\n2. **Compute Attention Scores**: Calculate the attention scores using the dot product of Q and K transpose. This gives a matrix where each element represents the unnormalized attention score between two positions in the sequence.\n3. **Apply Masking**: Use a mask to ensure that positions which should not be attended to (future positions) are excluded. This is done by setting their scores to a very low value, effectively zeroing them out after the softmax operation.\n4. **Softmax Normalization**: Convert the attention scores into probabilities using the softmax function.\n5. **Compute Output**: Multiply the attention weights with the Value matrix to get the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores using einsum for batch matrix multiplication\n    scores = np.einsum('bnd,bmd->bnm', Q, K)\n    \n    # Scale the scores by the square root of the key dimension\n    d_k = K.shape[-1]\n    scores = scores / np.sqrt(d_k)\n    \n    # Reshape the mask to broadcast across the batch dimension\n    mask = mask[np.newaxis, :, :]\n    # Apply the mask by setting unallowed positions to a very low value\n    scores = scores + (1 - mask) * (-1e9)\n    \n    # Compute softmax to get attention weights\n    exp_scores = np.exp(scores)\n    sum_exp = np.sum(exp_scores, axis=2, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    \n    # Compute the output using einsum for batch matrix multiplication\n    output = np.einsum('bnm,bmd->bnd', attention_weights, V)\n    \n    # Return the output as a list\n    return output.tolist()\n```\n\n### Explanation\n1. **Compute Q, K, V**: The `compute_qkv` function takes the input sequence and weight matrices to produce the Q, K, and V matrices using matrix multiplication.\n2. **Attention Scores**: The `masked_attention` function uses `np.einsum` to compute the attention scores efficiently across the batch dimension.\n3. **Masking**: The mask is reshaped to match the batch dimensions and applied to the scores to exclude future positions.\n4. **Softmax**: The scores are normalized using softmax to convert them into attention probabilities.\n5. **Output**: The attention weights are multiplied with the Value matrix to produce the final output, which is then converted to a list for the result.\n\nThis approach ensures that each position in the sequence can only attend to previous positions, which is essential for tasks where future information should not influence the current processing step."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to measure the disorder in a basket of apples based on their colors. The disorder should be 0 if all apples are the same color and should increase as the variety of colors increases. \n\n### Approach\nThe approach to solve this problem involves using the concept of entropy from information theory. Entropy is a measure of disorder and is calculated based on the distribution of colors in the basket. The steps to compute the entropy are as follows:\n\n1. **Count Frequencies**: Count the frequency of each color in the basket using a dictionary or the `Counter` class from the `collections` module.\n2. **Calculate Probabilities**: For each color, compute the probability of that color occurring in the basket by dividing its frequency by the total number of apples.\n3. **Compute Entropy**: Use the formula for entropy, which is the sum of -p_i * log2(p_i) for each color, where p_i is the probability of the ith color. This formula quantifies the disorder based on the distribution of colors.\n4. **Round the Result**: Round the computed entropy to four decimal places for the final result.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    n = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        p = count / n\n        entropy += -p * math.log2(p)\n    return round(entropy, 4)\n```\n\n### Explanation\n- **Counting Frequencies**: The `Counter` class efficiently counts the occurrences of each color in the list.\n- **Calculating Probabilities**: Each color's probability is determined by dividing its count by the total number of apples.\n- **Entropy Calculation**: The entropy formula sums the product of the negative of each probability and its logarithm base 2. This measures the disorder, with higher entropy indicating more variety in colors.\n- **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the disorder is 0 when all apples are the same color and increases as the variety and distribution of colors become more diverse."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to perform Layer Normalization on a 3D input tensor. Layer Normalization is a technique used to stabilize the training of deep neural networks by normalizing the activations of each layer. This normalization is performed across the feature dimension for each sequence in the batch.\n\n### Approach\n1. **Compute the Mean**: For each element in the batch and each position in the sequence, compute the mean of the feature values.\n2. **Subtract the Mean**: Subtract this mean from each feature value to center the data around zero.\n3. **Compute the Variance**: Calculate the variance of the centered data across the feature dimension.\n4. **Add Epsilon**: To avoid division by zero, add a small value (epsilon) to the variance.\n5. **Compute the Standard Deviation**: Take the square root of the variance to get the standard deviation.\n6. **Normalize**: Divide the centered data by the standard deviation to normalize the features.\n7. **Apply Gamma and Beta**: Scale the normalized data using gamma and shift it using beta, which are learnable parameters.\n8. **Round and Convert**: Round the result to 5 decimal places and convert the numpy array to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute the mean across the feature dimension\n    mean = np.mean(X, axis=2, keepdims=True)\n    # Subtract the mean from each element\n    X_centered = X - mean\n    # Compute the variance across the feature dimension\n    var = np.mean(X_centered ** 2, axis=2, keepdims=True)\n    # Add a small epsilon to the variance to avoid division by zero\n    var += epsilon\n    # Compute the standard deviation\n    std = np.sqrt(var)\n    # Normalize the data\n    X_normalized = X_centered / std\n    # Apply the gamma and beta parameters\n    result = gamma * X_normalized + beta\n    # Round to 5 decimal places and convert to a list\n    return np.round(result, 5).tolist()\n```\n\n### Explanation\n- **Mean Calculation**: The mean is computed across the feature dimension (axis=2) to ensure that each feature is normalized independently for every position in the sequence.\n- **Centering**: Subtracting the mean centers the data, which helps in stabilizing the training process.\n- **Variance and Standard Deviation**: These are computed to scale the data. Adding epsilon ensures numerical stability.\n- **Normalization**: Dividing by the standard deviation normalizes the data, making the activations have a mean of 0 and a standard deviation of 1.\n- **Scaling and Shifting**: Gamma and beta parameters allow the model to learn the optimal scaling and shifting for each feature, which is crucial for maintaining the representational power of the layer.\n\nThis approach ensures that the data is normalized correctly across the feature dimension, and the use of gamma and beta allows the model to adapt the normalization to the specific needs of the task."}
{"task_id": 110, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the quality of machine translation using the METEOR score. The METEOR score considers both the accuracy of the translation and the word order, providing a comprehensive measure of translation quality.\n\n### Approach\nThe METEOR score is calculated using several components:\n1. **Unigram Matches**: Count the number of words in the candidate translation that match the reference translation, considering their frequency but not their order.\n2. **Precision and Recall**: Precision is the ratio of correctly matched words to the total words in the candidate, while recall is the ratio of correctly matched words to the total words in the reference.\n3. **F-mean**: A weighted harmonic mean of precision and recall, adjusted by parameters alpha and beta.\n4. **Word Order Penalty**: A penalty for words that are correctly matched but in the wrong order, calculated using the longest common subsequence (LCS) to determine the number of correctly ordered matches.\n\nThe METEOR score is computed as the F-mean minus a penalty term, which accounts for word order fragmentation.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    if not candidate or not reference:\n        return 0.0\n    \n    ref_words = reference.split()\n    candidate_words = candidate.split()\n    \n    ref_counts = Counter(ref_words)\n    candidate_counts = Counter(candidate_words)\n    \n    correct_matches = 0\n    for word in candidate_counts:\n        correct_matches += min(ref_counts.get(word, 0), candidate_counts[word])\n    \n    def lcs(a, b):\n        m = len(a)\n        n = len(b)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m):\n            for j in range(n):\n                if a[i] == b[j]:\n                    dp[i+1][j+1] = dp[i][j] + 1\n                else:\n                    dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n        return dp[m][n]\n    \n    correctly_ordered = lcs(candidate_words, ref_words)\n    \n    if correct_matches == 0:\n        penalty = 0.0\n    else:\n        penalty = (correct_matches - correctly_ordered) / correct_matches\n    \n    len_candidate = len(candidate_words)\n    len_ref = len(ref_words)\n    p = correct_matches / len_candidate if len_candidate > 0 else 0.0\n    r = correct_matches / len_ref if len_ref > 0 else 0.0\n    \n    if (alpha + beta) == 0:\n        f_mean = 0.0\n    else:\n        f_mean = (alpha * p + beta * r) / (alpha + beta)\n    \n    meteor = f_mean - gamma * penalty\n    \n    return round(meteor, 3)\n```\n\n### Explanation\n1. **Tokenization**: The reference and candidate translations are split into lists of words.\n2. **Correct Matches Calculation**: Using counters, we determine how many words in the candidate match the reference, considering their frequency.\n3. **Longest Common Subsequence (LCS)**: This step calculates the number of words that are correctly matched and in the correct order.\n4. **Penalty Calculation**: The penalty is the proportion of correct matches that are not in the correct order.\n5. **Precision and Recall**: These metrics are calculated based on the number of correct matches relative to the lengths of the candidate and reference.\n6. **F-mean Calculation**: A weighted harmonic mean of precision and recall, adjusted by parameters alpha and beta.\n7. **METEOR Score**: The final score is computed by subtracting the penalty from the F-mean, then rounding the result to three decimal places.\n\nThis approach ensures that the METEOR score provides a balanced evaluation of translation quality, considering both accuracy and word order."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\n### Approach\nThe Pointwise Mutual Information (PMI) is a measure used to determine the relationship between two events. It is calculated using the formula:\n\n\\[ PMI(x, y) = \\log_2\\left(\\frac{P(x, y)}{P(x) \\cdot P(y)}\\right) \\]\n\nWhere:\n- \\( P(x, y) \\) is the joint probability of events x and y.\n- \\( P(x) \\) is the marginal probability of event x.\n- \\( P(y) \\) is the marginal probability of event y.\n\nThe steps to compute PMI are as follows:\n1. Compute the joint probability \\( P(x, y) \\) as the ratio of joint counts to the total number of samples.\n2. Compute the marginal probabilities \\( P(x) \\) and \\( P(y) \\) as the ratios of their respective counts to the total number of samples.\n3. Calculate the ratio of the joint probability to the product of the marginal probabilities.\n4. Compute the base-2 logarithm of this ratio to get the PMI.\n5. Return the result rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Calculate the probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Avoid division by zero by checking if p_x or p_y is zero\n    if p_x == 0 or p_y == 0:\n        return 0.0  # or handle as appropriate, but per problem statement, this may not be necessary\n    \n    # Calculate the denominator\n    denominator = p_x * p_y\n    \n    # Calculate the ratio\n    ratio = p_xy / denominator\n    \n    # Calculate PMI\n    pmi = np.log2(ratio)\n    \n    # Round to 3 decimal places\n    return round(pmi, 3)\n```\n\n### Explanation\nThe function `compute_pmi` takes four parameters: `joint_counts`, `total_counts_x`, `total_counts_y`, and `total_samples`. It computes the probabilities of the joint occurrence and the individual occurrences of the events. Using these probabilities, it calculates the PMI using the given formula. The result is then rounded to three decimal places and returned. This approach ensures that we accurately measure the dependence between two events based on their occurrence counts."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1]. This normalization technique ensures that all features contribute equally to a model by scaling them to a common range.\n\n### Approach\nThe Min-Max Normalization technique scales each value in the list to a specified range, typically [0, 1], using the formula:\n\\[ \\text{normalized}(x_i) = \\frac{x_i - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} \\]\n\nHere are the steps involved in the approach:\n1. **Find the Minimum and Maximum Values**: Compute the minimum and maximum values of the input list.\n2. **Handle Edge Case**: If all values in the list are the same, the range is zero, and each value is normalized to 0.0.\n3. **Normalize Each Value**: For each value in the list, apply the Min-Max formula to scale it to the range [0, 1].\n4. **Round the Results**: Round each normalized value to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    if max_x == min_x:\n        return [0.0 for _ in x]\n    else:\n        normalized = [(xi - min_x) / (max_x - min_x) for xi in x]\n        return [round(num, 4) for num in normalized]\n```\n\n### Explanation\n1. **Finding Min and Max**: The function starts by determining the minimum and maximum values of the input list using Python's built-in `min` and `max` functions.\n2. **Edge Case Handling**: If all elements in the list are the same, the difference between the maximum and minimum values is zero. In this case, each element is normalized to 0.0 to avoid division by zero.\n3. **Normalization**: For each element in the list, the formula \\((x_i - \\text{min}(x)) / (\\text{max}(x) - \\text{min}(x))\\) is applied to scale the value to the range [0, 1].\n4. **Rounding**: Each normalized value is rounded to four decimal places to ensure the output meets the specified precision requirement.\n\nThis approach ensures that the input data is scaled appropriately, making it suitable for various machine learning models that require feature scaling."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple residual block using NumPy. The residual block processes an input array through two weight layers, applies ReLU activations, adds the original input via a shortcut connection, and then applies a final ReLU activation.\n\n### Approach\nThe approach to implement the residual block involves the following steps:\n\n1. **Main Path Processing**: The input array is passed through two weight layers using matrix multiplication. Each layer is followed by a ReLU activation function.\n2. **Shortcut Connection**: The original input array is added to the output of the second weight layer in the main path.\n3. **Final Activation**: The result from the shortcut addition is passed through another ReLU activation function.\n4. **Rounding and Conversion**: The final output is rounded to 4 decimal places and converted to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # Main path through two weight layers with ReLU\n    main = x.dot(w1)\n    main = np.maximum(0, main)  # ReLU activation\n    main = main.dot(w2)\n    main = np.maximum(0, main)  # ReLU activation\n    \n    # Add the original input (shortcut connection)\n    output = main + x\n    \n    # Final ReLU activation\n    output = np.maximum(0, output)\n    \n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n```\n\n### Explanation\n1. **Main Path Processing**:\n   - The input array `x` is first multiplied by the weight matrix `w1` to produce an intermediate result.\n   - This intermediate result is then passed through a ReLU activation function.\n   - The output from the first ReLU is then multiplied by the second weight matrix `w2`.\n   - This result is again passed through a ReLU activation function.\n\n2. **Shortcut Connection**:\n   - The original input array `x` is added to the output of the second ReLU activation. This step ensures that the original input features are preserved and can help in training by providing a direct path for gradient flow.\n\n3. **Final Activation**:\n   - The result from the shortcut addition is passed through another ReLU activation function to ensure all negative values are set to zero, maintaining the non-linear properties of the network.\n\n4. **Rounding and Conversion**:\n   - The final output is rounded to 4 decimal places to ensure precision and then converted to a list for the required output format.\n\nThis approach ensures that the residual block effectively combines the main path and the shortcut connection, providing a robust structure for deep neural networks."}
{"task_id": 114, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer. The goal is to compute the average of each feature map across all its spatial dimensions and return a 1D array where each element corresponds to the average of a feature map.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Reshape the Input Array**: Convert the 3D input array into a 2D array where each row represents a flattened version of a feature map. This is done by reshaping the array such that the first dimension becomes the product of the height and width, effectively flattening each feature map into a single row.\n\n2. **Compute the Mean Along the Flattened Dimension**: Once the array is reshaped, compute the mean of each feature map across all its elements. This is achieved by taking the mean along the first axis of the reshaped 2D array.\n\n3. **Return the Result**: The result is a 1D array where each element corresponds to the average of a feature map.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    # Reshape the input to (height * width, channels)\n    reshaped = x.reshape(-1, x.shape[2])\n    # Compute the mean along the first axis (across all elements of each channel)\n    return reshaped.mean(axis=0)\n```\n\n### Explanation\n- **Reshaping the Array**: The input array is reshaped from a 3D array (height, width, channels) to a 2D array (height*width, channels). This transformation allows us to treat each feature map as a single row in the 2D array.\n  \n- **Computing the Mean**: By computing the mean along the first axis of the reshaped array, we effectively calculate the average of all elements in each feature map. This results in a 1D array where each element is the average of the corresponding feature map.\n\nThis approach efficiently computes the global average pooling by leveraging NumPy's array manipulation and reduction functions, ensuring that the solution is both concise and performant."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format (batch, channels, height, width). The function will normalize the input across the batch and spatial dimensions for each channel, then apply scale (gamma) and shift (beta) parameters, ensuring numerical stability with a provided epsilon value.\n\n### Approach\n1. **Compute Mean and Variance**: For each channel, compute the mean and variance across the batch, height, and width dimensions. This is done using NumPy's `mean` and `var` functions with the appropriate axes.\n2. **Normalize the Input**: Subtract the mean from each element in the input array and divide by the square root of the variance plus epsilon. This step normalizes the input data.\n3. **Apply Scale and Shift**: Multiply the normalized data by gamma and add beta to each channel. This step adjusts the normalized data using learned parameters.\n4. **Round and Convert to List**: Round the result to 4 decimal places and convert the NumPy array to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute the mean across batch, height, and width for each channel\n    mean = np.mean(X, axis=(0, 2, 3))\n    \n    # Compute the variance across the same dimensions\n    var = np.var(X, axis=(0, 2, 3))\n    \n    # Normalize the input\n    X_normalized = (X - mean[:, None, None, None]) / np.sqrt(var[:, None, None, None] + epsilon)\n    \n    # Apply gamma (scale) and beta (shift)\n    output = gamma[:, None, None, None] * X_normalized + beta[:, None, None, None]\n    \n    # Round to 4 decimal places and convert to list\n    output = np.round(output, decimals=4)\n    return output.tolist()\n```\n\n### Explanation\n1. **Mean and Variance Calculation**: The mean and variance are computed for each channel across the batch, height, and width. This ensures that each channel is normalized independently.\n2. **Normalization**: Each element in the input array is normalized by subtracting the mean and dividing by the square root of the variance plus a small epsilon value to avoid division by zero.\n3. **Scaling and Shifting**: After normalization, the result is scaled by gamma and shifted by beta. These parameters allow the network to learn appropriate transformations for each channel.\n4. **Rounding and Conversion**: The final output is rounded to 4 decimal places and converted to a list to meet the specified output format.\n\nThis approach ensures that the batch normalization is performed efficiently and correctly, adhering to the constraints and requirements of the problem."}
{"task_id": 116, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`. The function should return the value of the derivative, rounded to four decimal places.\n\n### Approach\nThe derivative of a polynomial term `c * x^n` can be computed using the power rule from calculus. The power rule states that the derivative of `x^n` is `n * x^(n-1)`. Therefore, the derivative of `c * x^n` is `c * n * x^(n-1)`.\n\nHere are the steps to compute the derivative:\n1. **Check for a constant term**: If the exponent `n` is 0, the term is a constant, and its derivative is 0.\n2. **Compute the derivative**: For any other exponent `n`, compute the derivative using the formula `c * n * x^(n-1)`.\n3. **Round the result**: Return the result rounded to four decimal places.\n\n### Solution Code\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    if n == 0:\n        return 0.0\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n```\n\n### Explanation\n- **Constant Term Handling**: If `n` is 0, the term is a constant, and its derivative is 0. This is handled by checking if `n` is 0 and returning 0.0 immediately.\n- **Derivative Calculation**: For non-zero `n`, the derivative is calculated using the formula `c * n * x^(n-1)`. This formula is derived from the power rule in calculus.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the output constraints.\n\nThis approach ensures that the function correctly handles all valid inputs, including cases where `n` is a non-integer or `x` is zero, and returns the derivative value as required."}
{"task_id": 117, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. The goal is to return a list of orthonormal vectors that span the same subspace, rounded to 4 decimal places.\n\n### Approach\nThe Gram-Schmidt process is used to orthogonalize a set of vectors. The process involves taking each vector in turn and subtracting the projections of the previous orthogonal vectors onto it. Each resulting vector is then normalized to have unit length. Here are the key steps:\n\n1. **Initialization**: Start with an empty list to store the orthonormal basis vectors.\n2. **Processing Each Vector**: For each vector in the input list:\n   - Create a copy of the vector.\n   - Subtract the projections of all previously computed orthonormal vectors onto the current vector.\n   - Check if the resulting vector has a magnitude greater than a given tolerance. If it does, normalize the vector and add it to the basis.\n3. **Rounding and Conversion**: After processing all vectors, round each basis vector to 4 decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=np.float64)\n        u = v.copy()\n        for u_i in basis:\n            proj = np.dot(u, u_i) * u_i\n            u = u - proj\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            u = u / norm\n            basis.append(u)\n    # Round each vector to 4 decimal places and convert to list\n    result = []\n    for vec in basis:\n        rounded = np.round(vec, decimals=4)\n        result.append(rounded.tolist())\n    return result\n```\n\n### Explanation\n1. **Initialization**: The function starts with an empty list `basis` to store the orthonormal vectors.\n2. **Vector Processing**: For each vector in the input list:\n   - Convert the vector to a numpy array for easier manipulation.\n   - Create a copy of the vector to work with.\n   - For each vector already in the basis, compute the projection of the current vector onto the basis vector and subtract this projection from the current vector.\n   - Compute the norm of the resulting vector. If the norm is greater than the tolerance, normalize the vector and add it to the basis.\n3. **Rounding and Conversion**: After processing all vectors, each vector in the basis is rounded to 4 decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that the resulting vectors are orthonormal, meaning they are orthogonal to each other and each has a unit length. The use of the Gram-Schmidt process ensures that the basis vectors span the same subspace as the input vectors, while the tolerance parameter helps in handling numerical precision issues."}
{"task_id": 118, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product results in a vector that is perpendicular to both input vectors and follows the right-hand rule. This concept is fundamental in various fields such as physics, engineering, and 3D graphics.\n\n### Approach\nThe cross product of two vectors \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\) can be computed using the determinant of a matrix formed by the unit vectors \\( \\mathbf{i}, \\mathbf{j}, \\mathbf{k} \\) and the components of \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\). The formula for the cross product is:\n\n\\[\n\\mathbf{a} \\times \\mathbf{b} = \\left( a_2 b_3 - a_3 b_2, a_3 b_1 - a_1 b_3, a_1 b_2 - a_2 b_1 \\right)\n\\]\n\nUsing Python's `numpy` library, we can efficiently compute the cross product with the `np.cross` function. The result is then rounded to four decimal places and converted to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    rounded = np.round(cross, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Compute the Cross Product**: Using `np.cross(a, b)`, we compute the cross product of vectors `a` and `b`. This function handles both lists and numpy arrays, making it versatile for different input types.\n2. **Round the Result**: The result is rounded to four decimal places using `np.round(cross, 4)` to ensure precision.\n3. **Convert to List**: The rounded result is converted to a list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently computes the cross product and ensures the result is formatted correctly, making it suitable for various applications in physics, engineering, and 3D graphics."}
{"task_id": 119, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that uses Cramer's Rule to solve a system of linear equations given by the matrix equation \\(Ax = b\\). The function should return the solution vector \\(x\\) if the system has a unique solution, otherwise, it should return -1.\n\n### Approach\n1. **Check Determinant**: First, compute the determinant of the coefficient matrix \\(A\\). If the determinant is zero (or very close to zero within a small threshold), the system does not have a unique solution, and we return -1.\n2. **Cramer's Rule**: If the determinant is non-zero, we proceed to compute each element of the solution vector \\(x\\) using Cramer's Rule. For each element \\(x_i\\), we replace the \\(i\\)-th column of \\(A\\) with the vector \\(b\\) to form a new matrix \\(A_i\\), compute its determinant, and then divide by the determinant of \\(A\\).\n3. **Rounding**: Each element of the solution vector is rounded to four decimal places for precision.\n4. **Return Result**: The solution vector is returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    n = len(A)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x[i] = det_Ai / det_A\n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Determinant Check**: The determinant of matrix \\(A\\) is computed using `np.linalg.det(A)`. If the absolute value of the determinant is below a small threshold (1e-10), the function returns -1, indicating no unique solution.\n2. **Matrix Replacement**: For each column \\(i\\), a copy of matrix \\(A\\) is made, and the \\(i\\)-th column is replaced with vector \\(b\\) to form matrix \\(A_i\\).\n3. **Determinant Calculation**: The determinant of each modified matrix \\(A_i\\) is computed, and the corresponding element of the solution vector \\(x\\) is found by dividing this determinant by the determinant of \\(A\\).\n4. **Precision Handling**: The solution vector is rounded to four decimal places using `np.round` to ensure the required precision.\n5. **Result Conversion**: The solution vector is converted to a list using `tolist()` for the final output.\n\nThis approach efficiently leverages Cramer's Rule and numpy operations to solve the system of linear equations, ensuring both correctness and precision."}
{"task_id": 120, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Bhattacharyya distance between two discrete probability distributions. The Bhattacharyya distance is a measure of similarity between two probability distributions and is defined as the negative of the Bhattacharyya coefficient, which is the sum of the square roots of the product of corresponding probabilities from the two distributions.\n\n### Approach\n1. **Input Validation**: First, we check if the input lists `p` and `q` are valid. If their lengths are different or if either list is empty, we return 0.0.\n2. **Compute Bhattacharyya Coefficient**: For each pair of corresponding probabilities in `p` and `q`, we compute the square root of their product and sum these values.\n3. **Calculate Distance**: The Bhattacharyya distance is the negative of the computed sum.\n4. **Round Result**: Finally, we round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    sum_bhatta = 0.0\n    for pi, qi in zip(p, q):\n        sum_bhatta += np.sqrt(pi * qi)\n    distance = -sum_bhatta\n    return round(distance, 4)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the lengths of `p` and `q` are different or if either list is empty. If so, it returns 0.0 immediately.\n- **Sum Calculation**: Using a loop, the function computes the sum of the square roots of the products of corresponding elements from `p` and `q`.\n- **Distance Calculation**: The Bhattacharyya distance is derived by taking the negative of the computed sum.\n- **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the function handles invalid inputs gracefully and efficiently computes the Bhattacharyya distance for valid inputs."}
{"task_id": 121, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that computes the element-wise sum of two vectors. The function should return a new vector representing the resulting sum if the operation is valid, or -1 if the vectors have incompatible dimensions.\n\n### Approach\n1. **Check Vector Lengths**: The first step is to check if the two input vectors have the same length. If they do not, the function should return -1 as the operation is not valid.\n2. **Element-wise Sum**: If the vectors have the same length, the function will compute the sum of corresponding elements from each vector. This will be done using a loop that iterates through each index of the vectors, adds the elements at that index, and appends the result to a new list.\n3. **Return Result**: The function will return the resulting list of sums if the vectors are compatible. If not, it returns -1.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result\n```\n\n### Explanation\n- **Check Lengths**: The function starts by comparing the lengths of the two input vectors. If they are not equal, it immediately returns -1.\n- **Element-wise Addition**: If the lengths are equal, the function initializes an empty list `result`. It then iterates over each index of the vectors, computes the sum of the elements at each index, and appends this sum to `result`.\n- **Return Result**: Finally, the function returns the `result` list containing the element-wise sums of the two input vectors.\n\nThis approach ensures that the function handles both valid and invalid cases correctly, providing the expected output in each scenario."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the policy gradient estimator using the REINFORCE algorithm. The policy is parameterized by a 2D NumPy array `theta` of shape `(num_states, num_actions)`. The policy for each state is computed via softmax over `theta[s, :]`. Given a list of episodes (each a list of (state, action, reward) tuples), we compute the average gradient of the log-policy multiplied by the return at each time step.\n\n### Approach\n1. **Initialize Gradient**: Start with a zero-initialized gradient array of the same shape as `theta`.\n2. **Compute Returns**: For each episode, compute the returns for each step. The return for a step is the sum of rewards from that step onwards.\n3. **Process Each Step**: For each step in each episode, compute the probabilities of actions using the softmax function. Then, compute the gradient of the log probability of the taken action and multiply it by the corresponding return.\n4. **Accumulate Gradients**: Sum the gradients across all steps and episodes.\n5. **Average Gradient**: Divide the accumulated gradients by the total number of steps to get the average gradient.\n6. **Round and Convert**: Round the results to the nearest 4th decimal and convert the NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    total_steps = 0\n\n    for episode in episodes:\n        rewards = [r for (s, a, r) in episode]\n        n = len(rewards)\n        returns = [0.0] * n\n        returns[-1] = rewards[-1]\n        for t in range(n-2, -1, -1):\n            returns[t] = rewards[t] + returns[t+1]\n        \n        for t in range(n):\n            s, a, r = episode[t]\n            state_theta = theta[s, :]\n            max_theta = np.max(state_theta)\n            exp_theta = np.exp(state_theta - max_theta)\n            sum_exp = np.sum(exp_theta)\n            p = exp_theta / sum_exp\n            gradient_log = -p\n            gradient_log[a] += 1\n            gradient_contribution = gradient_log * returns[t]\n            gradient[s, :] += gradient_contribution\n        total_steps += n\n\n    if total_steps == 0:\n        return gradient.tolist()\n    \n    gradient /= total_steps\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()\n```\n\n### Explanation\n1. **Initialization**: The gradient array is initialized to zeros to store the accumulated gradients.\n2. **Return Calculation**: For each episode, the returns are computed starting from the end of the episode and moving backwards. This gives the cumulative reward for each step.\n3. **Softmax and Gradient Calculation**: For each step, the probabilities of actions are computed using the softmax function. The gradient of the log probability is then calculated, which is used to update the gradient array.\n4. **Gradient Accumulation**: The gradients are accumulated for each step, scaled by the corresponding return.\n5. **Averaging**: The accumulated gradients are divided by the total number of steps to get the average gradient.\n6. **Rounding and Conversion**: The final gradient is rounded to four decimal places and converted to a list for the output.\n\nThis approach ensures that we efficiently compute the policy gradient using the REINFORCE algorithm, taking into account the cumulative rewards and the probabilities of actions in each state."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer. The goal is to determine the percentage of computational cost savings achieved by using an MoE layer, given the number of experts, the number of active experts, and the input/output dimensions.\n\n### Approach\n1. **Understanding the Problem**: The task involves comparing the computational cost (in terms of floating-point operations, FLOPs) of an MoE layer with that of a dense layer. The MoE layer uses a subset of experts (sparsity) to reduce computational cost, while the dense layer uses all experts.\n\n2. **FLOPs Calculation**:\n   - For a dense layer, the number of FLOPs is calculated as the product of input dimension (d_in) and output dimension (d_out).\n   - For an MoE layer, the number of FLOPs is proportional to the number of active experts (k_active) relative to the total number of experts (n_experts). This is because only a subset of experts is active, reducing the computational load.\n\n3. **Efficiency Calculation**: The computational savings are derived by comparing the FLOPs of the dense layer with those of the MoE layer. The savings are calculated as a percentage reduction in FLOPs when using the MoE layer.\n\n### Solution Code\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = (k_active / n_experts) * flops_dense\n    saving = (flops_dense - flops_moe) / flops_dense * 100\n    return round(saving, 1)\n```\n\n### Explanation\n1. **FLOPs for Dense Layer**: The computational cost for a dense layer is given by the product of its input and output dimensions, `d_in * d_out`.\n\n2. **FLOPs for MoE Layer**: The computational cost for the MoE layer is calculated by considering the proportion of active experts. This is done by multiplying the FLOPs of the dense layer by the ratio of active experts to the total number of experts, `(k_active / n_experts)`.\n\n3. **Computational Savings**: The savings are computed as the difference between the FLOPs of the dense layer and the MoE layer, divided by the FLOPs of the dense layer, and then converted to a percentage. This value is rounded to the nearest tenth to provide the final result.\n\nThis approach efficiently quantifies the computational efficiency gain of using an MoE layer over a dense layer, providing a clear measure of how much computational cost is saved."}
{"task_id": 124, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Noisy Top-K gating function used in Mixture-of-Experts (MoE) models. This function helps in selecting which expert to use for each input by adding noise to the gating scores and then selecting the top K experts based on these noisy scores.\n\n### Approach\n1. **Compute Initial Gating Scores**: For each input vector, compute the initial gating scores using the input matrix and the gating weights.\n2. **Add Noise**: Add pre-sampled noise to these initial scores to create noisy scores. This step introduces stochasticity, which helps in training stability.\n3. **Select Top-K Experts**: For each input, determine the top K experts based on the noisy scores.\n4. **Compute Probabilities**: Normalize the noisy scores of the top K experts to form a probability distribution. Each probability is the ratio of the noisy score to the sum of the top K noisy scores.\n5. **Round and Convert to List**: Round the resulting probabilities to four decimal places and convert the numpy array to a Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute the initial gating scores\n    s = X.dot(W_g)\n    \n    # Add the pre-sampled noise to get the noisy scores\n    s_noisy = s + N\n    \n    batch_size = X.shape[0]\n    num_experts = W_g.shape[1]\n    \n    # Initialize the probabilities matrix\n    probs = np.zeros((batch_size, num_experts))\n    \n    for i in range(batch_size):\n        s_i = s_noisy[i]\n        # Get the indices of the scores in descending order\n        sorted_indices = np.argsort(s_i)[::-1]\n        top_k_indices = sorted_indices[:k]\n        \n        # Calculate the sum of the top k scores\n        sum_top = s_i[top_k_indices].sum()\n        \n        if sum_top == 0:\n            # If all top k scores are zero, assign uniform probability\n            probs[i, top_k_indices] = 1.0 / k\n        else:\n            # Normalize the top k scores to get probabilities\n            probs[i, top_k_indices] = s_i[top_k_indices] / sum_top\n    \n    # Round the probabilities to 4 decimal places\n    probs_rounded = np.round(probs, 4)\n    \n    # Convert the numpy array to a Python list\n    return probs_rounded.tolist()\n```\n\n### Explanation\n1. **Initial Gating Scores**: The initial scores are computed by multiplying the input matrix `X` with the gating weights matrix `W_g`. This gives a score for each expert for every input.\n2. **Noise Addition**: Pre-sampled noise `N` is added to these scores to create noisy scores. This step introduces randomness, which helps in preventing the model from being too deterministic.\n3. **Top-K Selection**: For each input, the scores are sorted in descending order, and the top K scores are selected. This helps in focusing on the most relevant experts.\n4. **Probability Calculation**: The selected top K scores are normalized to form a probability distribution. This ensures that the probabilities sum up to 1 for each input.\n5. **Rounding and Conversion**: The probabilities are rounded to four decimal places for precision, and the numpy array is converted to a Python list for the final output.\n\nThis approach efficiently combines deterministic gating scores with stochastic noise to create a robust and stable gating mechanism in MoE models."}
{"task_id": 125, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing. The goal is to compute the final MoE output by selecting the top-k experts per token, applying their transformations, and aggregating the results weighted by the normalized gating probabilities.\n\n### Approach\n1. **Compute Gating Scores**: For each token, compute the gating scores using the input tensor and the gating weight matrix. This gives a score for each expert indicating how much the expert should influence the output for that token.\n2. **Compute Probabilities**: Convert the gating scores into probabilities using the softmax function. These probabilities represent the weight each expert should have in the final output for each token.\n3. **Select Top-k Experts**: For each token, sort the experts based on their probabilities in descending order and select the top-k experts.\n4. **Compute Expert Outputs**: For each selected expert, compute the output by applying the expert's weight matrix to the token's input.\n5. **Aggregate Results**: Multiply each expert's output by the corresponding probability and sum these weighted outputs to get the final result for each token.\n6. **Round and Convert**: Round the final output to four decimal places and convert it to a list for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute gating scores\n    g = x.dot(Wg)\n    # Compute probabilities using softmax\n    p = np.exp(g) / np.exp(g).sum(axis=1, keepdims=True)\n    # Initialize output array\n    output_dim = We[0].shape[1]\n    output = np.zeros((x.shape[0], output_dim))\n    \n    for i in range(x.shape[0]):\n        expert_probs = p[i]\n        # Get indices sorted in descending order of probabilities\n        sorted_indices = np.argsort(expert_probs)[::-1]\n        top_k_indices = sorted_indices[:top_k]\n        # Select the corresponding expert weights\n        selected_We = We[top_k_indices]\n        # Compute the outputs from the selected experts\n        x_i = x[i]\n        expert_outputs = x_i.dot(selected_We)\n        # Get the probabilities for the selected experts\n        selected_probs = expert_probs[top_k_indices]\n        # Compute the weighted sum for the output\n        output[i] = (expert_outputs * selected_probs[:, np.newaxis]).sum(axis=0)\n    \n    # Round the output to 4 decimal places\n    output = np.round(output, 4)\n    # Convert to a Python list\n    return output.tolist()\n```\n\n### Explanation\n1. **Gating Scores Calculation**: The gating scores are computed using matrix multiplication of the input tensor with the gating weight matrix. This results in a score for each expert for each token.\n2. **Softmax Function**: The scores are converted into probabilities using the softmax function, ensuring that the probabilities sum to 1 for each token.\n3. **Top-k Selection**: For each token, the experts are sorted by their probabilities, and the top-k experts are selected to reduce computational complexity and focus on the most influential experts.\n4. **Expert Output Calculation**: For each selected expert, the output is computed by applying the expert's weight matrix to the token's input vector.\n5. **Weighted Sum**: The outputs from the selected experts are weighted by their respective probabilities and summed to produce the final output for each token.\n6. **Rounding and Conversion**: The final output is rounded to four decimal places and converted into a list for the result.\n\nThis approach efficiently combines the outputs of the most influential experts, leveraging the MoE technique to enhance model performance and flexibility."}
{"task_id": 126, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to perform Group Normalization on a 4D input tensor. Group Normalization is a technique where the input tensor is divided into smaller groups of channels, and each group is normalized separately. This approach helps in reducing the internal covariate shift and can be beneficial in training deeper networks.\n\n### Approach\n1. **Reshape the Input Tensor**: The input tensor is reshaped to group the channels into the specified number of groups. This allows us to process each group independently.\n2. **Compute Mean and Variance**: For each group, compute the mean and variance across all elements in the group. This includes the spatial dimensions (height and width) and the channels within the group.\n3. **Normalize Each Group**: Each group is normalized by subtracting the mean and dividing by the square root of the variance plus a small epsilon value to avoid division by zero.\n4. **Apply Scale and Shift**: After normalization, each group is scaled by gamma and shifted by beta. These parameters are learned during training and are applied per group.\n5. **Reshape Back and Round**: The normalized tensor is reshaped back to its original shape, and the results are rounded to the nearest 4th decimal place.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    C_per_group = C // num_groups\n    X_reshaped = X.reshape((B, num_groups, C_per_group, H, W))\n    \n    for i in range(num_groups):\n        group = X_reshaped[:, i, :, :, :]\n        mean = group.mean(axis=(1, 2, 3), keepdims=True)\n        var = group.var(axis=(1, 2, 3), keepdims=True)\n        normalized = (group - mean) / np.sqrt(var + epsilon)\n        gamma_slice = gamma[i * C_per_group : (i + 1) * C_per_group]\n        beta_slice = beta[i * C_per_group : (i + 1) * C_per_group]\n        gamma_reshaped = gamma_slice.reshape((1, 1, C_per_group, 1, 1))\n        beta_reshaped = beta_slice.reshape((1, 1, C_per_group, 1, 1))\n        normalized = normalized * gamma_reshaped + beta_reshaped\n        X_reshaped[:, i, :, :, :] = normalized\n    \n    X_normalized = X_reshaped.reshape((B, C, H, W))\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()\n```\n\n### Explanation\n1. **Reshaping the Tensor**: The input tensor `X` is reshaped to group the channels into `num_groups`. This allows us to process each group independently.\n2. **Mean and Variance Calculation**: For each group, the mean and variance are computed across all elements in the group, including the spatial dimensions (height and width) and the channels within the group.\n3. **Normalization**: Each group is normalized by subtracting the mean and dividing by the square root of the variance plus a small epsilon value to avoid division by zero.\n4. **Applying Gamma and Beta**: After normalization, each group is scaled by gamma and shifted by beta. These parameters are learned during training and are applied per group.\n5. **Reshaping and Rounding**: The normalized tensor is reshaped back to its original shape, and the results are rounded to the nearest 4th decimal place to meet the output constraints.\n\nThis approach ensures that each group of channels is normalized separately, which can help in stabilizing the training process and reducing the internal covariate shift."}
{"task_id": 127, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the minimum value of the function \\( f(x) = x^4 - 3x^3 + 2 \\) using an initial guess and an optimization technique. The goal is to determine the value of \\( x \\) where the function reaches its minimum, starting from any given initial position.\n\n### Approach\nThe approach to solve this problem involves using the gradient descent algorithm, which is an iterative optimization method used to find the minimum of a function. Here are the key steps:\n\n1. **Compute the Derivative**: The first step in gradient descent is to compute the derivative of the function. For \\( f(x) = x^4 - 3x^3 + 2 \\), the derivative is \\( f'(x) = 4x^3 - 9x^2 \\).\n\n2. **Update Rule**: Using the computed derivative, update the current value of \\( x \\) by subtracting the product of the learning rate and the derivative. This step moves \\( x \\) in the direction of the steepest descent.\n\n3. **Convergence Check**: Continue updating \\( x \\) until the change in \\( x \\) between iterations is smaller than a specified tolerance or the maximum number of iterations is reached. This ensures that the algorithm stops when it has converged sufficiently close to the minimum.\n\n4. **Return Result**: Once the algorithm converges, return the value of \\( x \\) rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        current_x = x\n        derivative = 4 * current_x**3 - 9 * current_x**2\n        x = current_x - learning_rate * derivative\n        if abs(x - current_x) < tolerance:\n            break\n    return round(x, 4)\n```\n\n### Explanation\n- **Initialization**: The function starts with an initial guess for \\( x \\) provided by the user.\n- **Derivative Calculation**: For each iteration, the derivative of the function at the current \\( x \\) is computed. This derivative indicates the slope of the function at that point.\n- **Update Step**: The current \\( x \\) is updated using the gradient descent formula, which moves \\( x \\) in the direction of the steepest descent.\n- **Convergence Check**: The algorithm checks if the change in \\( x \\) is below the tolerance level. If it is, the loop breaks early, saving unnecessary computations.\n- **Result**: The final value of \\( x \\) is rounded to four decimal places and returned as the result.\n\nThis approach ensures that we efficiently find the minimum of the function using gradient descent, starting from any initial position and adjusting the learning rate and tolerance as needed."}
{"task_id": 128, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Dynamic Tanh (DyT) function, which is a normalization-free transformation inspired by the standard Tanh function. The DyT function replaces layer normalization in Transformer architectures while preserving the squashing behavior and enabling stable training.\n\n### Approach\nThe Dynamic Tanh function is designed to be a scaled and shifted version of the standard Tanh function. The function uses three parameters: alpha, gamma, and beta. The approach involves the following steps:\n\n1. **Scaling the Input**: The input array `x` is scaled by a factor `gamma`.\n2. **Applying Tanh**: The scaled input is then passed through the standard Tanh function.\n3. **Scaling and Shifting the Output**: The output of the Tanh function is scaled by `alpha` and then shifted by `beta`.\n4. **Rounding and Conversion**: The resulting values are rounded to four decimal places and converted to a list of floats.\n\nThis approach ensures that the function dynamically adjusts the output based on the given parameters, providing a normalization-free transformation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    y = alpha * np.tanh(gamma * x) + beta\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()\n```\n\n### Explanation\n- **Scaling the Input**: The input array `x` is scaled by `gamma` to adjust the magnitude of the input values.\n- **Applying Tanh**: The scaled input is passed through the Tanh function, which maps the values to the range (-1, 1).\n- **Scaling and Shifting the Output**: The output of the Tanh function is then scaled by `alpha` to adjust the range and shifted by `beta` to center the values around a desired mean.\n- **Rounding and Conversion**: The final output is rounded to four decimal places to ensure precision and converted to a list of floats for the required output format.\n\nThis implementation ensures that the Dynamic Tanh function provides a normalization-free transformation while maintaining the squashing behavior of the standard Tanh function."}
