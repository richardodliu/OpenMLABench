{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the dot product of a matrix and a vector.\n    \n    Args:\n    a (list[list[int|float]]): The matrix to dot.\n    b (list[int|float]): The vector to dot.\n    \n    Returns:\n    list[int|float]: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((row[i] * b[i] for i in range(len(row))))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "from typing import List\ndef transpose_matrix(matrix: List[List[int | float]]) -> List[List[int | float]]:\n    \"\"\"\n    Transpose a given matrix.\n\n    Parameters:\n    matrix (List[List[int|float]]): The input matrix to be transposed.\n\n    Returns:\n    List[List[int|float]]: The transposed matrix.\n    \"\"\"\n    if not matrix or len(matrix) == 1:\n        return matrix\n    transposed = [[matrix[j][i] for j in range(len(matrix))] for i in range(len(matrix[0]))]\n    return transposed\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a given matrix into a specified shape.\n\n    Parameters:\n    a (list[list[int|float]]): The matrix to be reshaped.\n    new_shape (tuple[int, int]): The desired shape of the matrix.\n\n    Returns:\n    list[list[int|float]]: The reshaped matrix, or an empty list if not possible.\n    \"\"\"\n    if len(a) != len(new_shape[0]) or any((len(row) != new_shape[1] for row in a)):\n        return []\n    reshaped_matrix = [row[i:i + new_shape[1]] for i in range(len(a))]\n    return reshaped_matrix\nnew_shape = (2, 2)"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column, based on a given mode.\n\n    Parameters:\n    matrix (list[list[float]]): A 2D list representing the matrix.\n    mode (str): The mode to calculate the mean for, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means based on the specified mode.\n    \"\"\"\n    if mode == 'row':\n        row_means = [sum(row) / len(row) for row in matrix]\n        return row_means\n    elif mode == 'column':\n        col_means = [sum(col) / len(col) for col in zip(*matrix)]\n        return col_means\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    if not matrix or not all((isinstance(row, list) for row in matrix)):\n        raise ValueError('Input must be a non-empty list of lists.')\n    if not isinstance(scalar, (int, float)):\n        raise ValueError('Scalar must be a number.')\n    result = [[scalar * element for element in row] for row in matrix]\n    return result\nmatrix = [[1, 2], [3, 4]]\nscalar = 5\nresult = scalar_multiply(matrix, scalar)"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Parameters:\n    - matrix (list[list[float|int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    - list[float]: A list of the eigenvalues of the matrix sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eig(np_matrix)[0]\n    sorted_eigenvalues = np.sort(eigenvalues, reverse=True)\n    return sorted_eigenvalues\nmatrix = [[4, 1], [1, 5]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    if not np.linalg.det(T) or not np.linalg.det(S):\n        return -1\n    T_inv = np.linalg.inv(T)\n    transformed_A = np.dot(T_inv, A)\n    transformed_A = transformed_A.tolist()\n    return transformed_A\nA = [[1, 2], [3, 4]]\nT = [[5, 6], [7, 8]]\nS = [[9, 10], [11, 12]]"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists of floats.\n\n    Returns:\n    list[list[float]] or None: The inverse of the matrix or None if it's not invertible.\n    \"\"\"\n    if matrix[0][0] * matrix[1][1] != matrix[1][0] * matrix[0][1]:\n        return None\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inv_matrix = [[matrix[1][1] / det, -matrix[0][1] / det], [-matrix[0][0] / det, matrix[1][0] / det]]\n    return inv_matrix\nmatrix = [[2, 0], [0, 2]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0] * len(b[0]) for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nmatrix_a = [[1, 2], [3, 4]]\nmatrix_b = [[5, 6], [7, 8]]\nresult = matrixmul(matrix_a, matrix_b)"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Parameters:\n    vectors (list[list[float]]): A list of lists where each inner list represents a feature with its observations.\n\n    Returns:\n    list[list[float]]: A covariance matrix as a list of lists.\n    \"\"\"\n    data = np.array(vectors)\n    covariance_matrix = np.cov(data, rowvar=False)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n\n    Parameters:\n    - A (np.ndarray): Coefficient matrix.\n    - b (np.ndarray): Right-hand side vector.\n    - n (int): Number of iterations for the Jacobi method.\n\n    Returns:\n    - list: Approximate solution x.\n    \"\"\"\n    x = b.copy()\n    for _ in range(n):\n        dA_dx = np.dot(A.T, A)\n        x -= np.dot(dA_dx, x)\n        x = [round(x[i], 4) for i in range(len(x))]\n    return x.tolist()\nA = np.array([[2, 3], [1, 4]])\nn = 100"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    (eigenvalues, eigenvectors) = np.linalg.eig(A)\n    singular_values = eigenvalues\n    singular_values.sort(reverse=True)\n    return tuple((round(singular_value, 4) for singular_value in singular_values))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) == 1:\n        return matrix[0][0]\n    det_3x3 = determinant_4x4([[matrix[i][j] for j in range(1, len(matrix))] for i in range(1, len(matrix))])\n    det_2x2 = determinant_4x4([[matrix[i][j] for j in range(len(matrix) - 1)] for i in range(len(matrix) - 1)])\n    return det_3x3 * det_2x2\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_transpose = np.transpose(X)\n    dot_product = np.dot(X_transpose, y)\n    inverse_X_transpose = np.linalg.inv(X_transpose)\n    coefficient = dot_product / inverse_X_transpose.shape[1]\n    rounded_coefficient = round(coefficient, 4)\n    return rounded_coefficient\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 3, 4]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        gradients = np.dot(X.T, y - X @ coefficients)\n        coefficients -= alpha * gradients\n    return coefficients.tolist()\nX = np.array([[1], [2], [3], [4]])\ny = np.array([2, 5, 7, 11])\nalpha = 0.01\niterations = 1000\ncoefficients = linear_regression_gradient_descent(X, y, alpha, iterations)"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    std_data = np.std(data, axis=0)\n    normalized_data = (data - np.mean(data, axis=0)) / std_data\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data_min_max = (data - min_val) / (max_val - min_val)\n    return (normalized_data, normalized_data_min_max)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.random.choice(initial_centroids, size=k, replace=False)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array - centroids, axis=1)\n        cluster_labels = np.argmin(distances, axis=0)\n        new_centroids = []\n        for i in range(k):\n            cluster_points = points_array[cluster_labels == i]\n            if len(cluster_points) > 0:\n                new_centroids.append(np.mean(cluster_points, axis=0))\n            else:\n                new_centroids.append(centroids[i])\n        if np.allclose(centroids, new_centroids, atol=1e-06):\n            break\n        centroids = new_centroids\n    final_centroids = [round(centroid, 4) for centroid in centroids]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: numpy.ndarray, the feature matrix.\n    - y: numpy.ndarray, the target variable.\n    - k: int, the number of folds for cross-validation.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, optional, the seed for the random number generator.\n\n    Returns:\n    - A list of tuples, where each tuple contains a train-test split index for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of rows.'\n    train_indices = [i for i in range(len(X))]\n    train_indices = np.random.choice(train_indices, k=k, replace=False)\n    (X_train, X_test) = (X[train_indices], X[:train_indices])\n    (y_train, y_test) = (y[train_indices], y[:train_indices])\n    if shuffle:\n        np.random.shuffle(X_train)\n        np.random.shuffle(y_train)\n    return [(train_index, test_index) for (train_index, test_index) in zip(train_indices, X_test)]"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues, reverse=True)\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = []\n    for i in range(k):\n        principal_component = eigenvectors[i, :]\n        principal_component = principal_component * std_dev + mean\n        principal_components.append(principal_component.tolist())\n    return principal_components\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nk = 2\nprincipal_components = pca(data, k)"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implement a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Parameters:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str): The type of kernel ('linear' or 'RBF').\n    - lambda_val (float): The regularization parameter.\n    - iterations (int): The number of iterations for the algorithm.\n    - sigma (float): The standard deviation for the kernel.\n\n    Returns:\n    - list: A list containing the alpha coefficients and bias of the trained model.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0\n    if kernel == 'linear':\n\n        def kernel_function(x, y):\n            return np.dot(x, y)\n    elif kernel == 'RBF':\n\n        def kernel_function(x, y):\n            return np.exp(-0.5 * np.linalg.norm(x - y) ** 2)\n    for _ in range(iterations):\n        predicted_labels = np.sign(np.dot(data, alpha))\n        gradient = np.zeros(n_samples)\n        for i in range(n_samples):\n            gradient += alpha[i] * (predicted_labels[i] - labels[i]) * data[i]\n        alpha -= lambda_val * gradient / n_samples\n        bias -= lambda_val * np.sum(predicted_labels) / n_samples\n    alpha_list = alpha.tolist()\n    bias_list = [bias]\n    return (alpha_list, bias_list)\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nlabels = np.array([1, 1, 0])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n\n    Parameters:\n    - z (float): The input value for which to compute the sigmoid function.\n\n    Returns:\n    - float: The sigmoid activation of the input value, rounded to four decimal places.\n    \"\"\"\n    return round(math.exp(z) / (1 + math.exp(z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    exponentiated_scores = [math.exp(score) for score in scores]\n    sum_exponentiated = sum(exponentiated_scores)\n    softmax_values = [score / sum_exponentiated * 100 for score in exponentiated_scores]\n    return softmax_values\nscores = [0.1, 0.2, 0.3, 0.4]"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification,\n    handling multidimensional input features. The function takes a list of feature vectors\n    (each vector representing multiple features for an example), associated true binary labels,\n    and the neuron's weights (one for each feature) and bias as input. It returns the\n    predicted probabilities after sigmoid activation and the mean squared error between the\n    predicted probabilities and the true labels, both rounded to four decimal places.\n\n    :param features: List of lists of floats representing feature vectors for an example.\n    :param labels: List of integers representing true binary labels for an example.\n    :param weights: List of floats representing the weights for each feature.\n    :param bias: Float representing the bias term.\n    :return: Tuple containing the predicted probabilities and the mean squared error.\n    \"\"\"\n    X = np.array(features)\n    y = np.array(labels)\n    X = np.column_stack((X, np.ones(X.shape[0])))\n    Z = np.dot(X, weights) + bias\n    A = np.maximum(0, 1 / (1 + np.exp(-Z)))\n    mse = np.mean((A - y) ** 2)\n    predicted_probs = A.round()\n    mse_rounded = round(mse, 4)\n    return (predicted_probs, mse_rounded)\nfeatures = [[1, 2, 3], [4, 5, 6]]\nlabels = [0, 1]\nweights = [0.5, 0.3, 0.2]\nbias = 0.1"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    outputs = np.dot(features, weights) + bias\n    errors = labels - outputs\n    dweights = np.dot(errors.T, features)\n    dbias = np.sum(errors, axis=0)\n    weights -= learning_rate * dweights\n    bias -= learning_rate * dbias\n    mse_values = np.mean(errors ** 2)\n    mse_values_list = mse_values.tolist()\n    return (weights, bias, mse_values_list)"}
{"task_id": 26, "completion_id": 0, "solution": "import torch\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self):\n        if self._op == '+':\n            self.grad = 1.0\n        elif self._op == '*':\n            self.grad = self.data * self._prev[0].grad\n        elif self._op == 'ReLU':\n            self.grad = (self.data > 0) * 1.0\n        else:\n            raise ValueError(f'Unsupported operation: {self._op}')\n        for child in self._prev:\n            child.grad += self.grad\n\n    def __add__(self, other):\n        return Value(self.data + other.data, self._prev, f'+')\n\n    def __mul__(self, other):\n        return Value(self.data * other.data, self._prev, f'*')\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad}, op={self._op})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    if len(B) != len(C):\n        raise ValueError('Both bases must have the same dimension.')\n    if B[0].shape[0] != C[0].shape[1]:\n        raise ValueError('The number of columns in the first matrix must match the number of rows in the second matrix.')\n    P = np.dot(C.T, B)\n    P_rounded = P.round(4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[2, 3, 4], [5, 6, 7], [8, 9, 10]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix using eigen values & vectors.\n\n    Parameters:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V, \n           where U is orthogonal, S is a diagonal matrix of singular values, \n           and V is the columns of the matrix.\n    \"\"\"\n    A_T_A = np.dot(A.T, A)\n    (U, S, V) = np.linalg.eigh(A_T_A)\n    U = U.astype(np.float32)\n    V = V.astype(np.float32)\n    U = U.reshape(-1, 2)\n    V = V.reshape(-1, 2)\n    S = np.diag(S)\n    return (U, S, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Perform a random shuffle of the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Parameters:\n    - X (np.ndarray): The input data array.\n    - y (np.ndarray): The target array.\n    - seed (int, optional): The seed for reproducibility.\n\n    Returns:\n    - np.ndarray: A reshaped array containing the shuffled data and labels.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    permuted_indices = np.random.permutation(len(X))\n    X_shuffled = X[permuted_indices]\n    y_shuffled = y[permuted_indices]\n    return (X_shuffled, y_shuffled)"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yield batches of X and y from a numpy array X.\n    \n    Parameters:\n    - X: A numpy array of shape (n_samples, n_features)\n    - y: A numpy array of shape (n_samples, n_targets) or None if not provided\n    - batch_size: An integer specifying the number of samples per batch\n    \n    Returns:\n    - A generator yielding tuples of shape (batch_size, n_samples, n_features) and (batch_size, n_samples, n_targets) if y is provided, otherwise (batch_size, n_samples) for X\n    \"\"\"\n    idx = 0\n    while idx < X.shape[0]:\n        batch_X = X[idx:idx + batch_size]\n        batch_y = y[idx:idx + batch_size] if y is not None else None\n        yield (batch_X, batch_y)\n    if idx < X.shape[0]:\n        batch_X = X[idx:]\n        batch_y = y[idx:]\n        yield (batch_X, batch_y)"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on the condition that the value of feature_i\n    is greater than or equal to the threshold. The function returns two lists: one for\n    samples that meet the condition and another for those that do not.\n\n    :param X: A 2D numpy array representing the dataset.\n    :param feature_i: The index of the feature to check.\n    :param threshold: The threshold value to compare against.\n    :return: Two lists: one with samples where feature_i is greater than or equal to threshold,\n             and the other with samples where feature_i is less than threshold.\n    \"\"\"\n    features = X[:, feature_i]\n    labels = X[:, -1]\n    subset_meeting = []\n    subset_not_meeting = []\n    for i in range(features.shape[0]):\n        if features[i] >= threshold:\n            subset_meeting.append(features[i])\n        else:\n            subset_not_meeting.append(features[i])\n    return (np.array(subset_meeting).tolist(), np.array(subset_not_meeting).tolist())\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nfeature_i = 1\nthreshold = 5"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial features for a given dataset.\n\n    Parameters:\n    - X (np.ndarray): A 2D numpy array representing the input data.\n    - degree (int): The degree of the polynomial features to generate.\n\n    Returns:\n    - np.ndarray: A 2D numpy array containing the polynomial features.\n    \"\"\"\n    features = []\n    for combo in combinations_with_replacement(X, degree):\n        feature = np.array(combo)\n        features.append(feature)\n    features = np.array(features).reshape(-1, degree + 1)\n    return features\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    if replacements:\n        indices = np.random.choice(np.arange(len(X)), size=n_subsets, replace=True)\n        X_subset = X[indices]\n        y_subset = y[indices]\n    else:\n        X_subset = np.random.choice(X, size=n_subsets)\n        y_subset = np.random.choice(y, size=n_subsets)\n    subsets = [tuple(subset) for subset in zip(X_subset, y_subset)]\n    return subsets\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([0, 1, 2])\nn_subsets = 3\nreplacements = True\nseed = 42"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values in a 1D numpy array.\n\n    Parameters:\n    - x (np.array): A 1D numpy array of integer values.\n    - n_col (int, optional): The number of columns for the one-hot encoded array. If not provided,\n                            it will be automatically determined from the input array.\n\n    Returns:\n    - np.ndarray: A 2D numpy array where each row corresponds to a unique value from the input array,\n                  and each column contains the one-hot encoding of that value.\n    \"\"\"\n    if n_col is None:\n        n_col = len(np.unique(x))\n    encoder = OneHotEncoder(sparse=True)\n    one_hot_encoded = encoder.fit_transform(x.reshape(-1, 1)).toarray()\n    return one_hot_encoded\nx = np.array([1, 2, 3, 4, 5])\nn_col = 3"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diag(x)\nx = np.array([1, 2, 3])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    - y_true (np.ndarray): A 1D numpy array of true labels.\n    - y_pred (np.ndarray): A 1D numpy array of predicted labels.\n\n    Returns:\n    - float: The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    - X: A 2D numpy array representing the input data.\n    - Y: An optional 2D numpy array representing the reference dataset. If not provided,\n          the function calculates the correlation matrix of X with itself.\n\n    Returns:\n    - A 2D numpy array representing the correlation matrix.\n    \"\"\"\n    if Y is None:\n        Y = X.copy()\n    if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n        raise TypeError('Both X and Y must be numpy arrays.')\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError('X and Y must have the same number of rows.')\n    correlation_matrix = np.corrcoef(X, Y)\n    rounded_correlation_matrix = correlation_matrix.round(4)\n    result = rounded_correlation_matrix.tolist()\n    return result"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    sample_weights = np.ones_like(y)\n    best_thresholds = []\n    for feature_index in range(X.shape[1]):\n        min_support = np.sum(y == X[:, feature_index])\n        decision_boundary = np.mean(X[:, feature_index])\n        best_threshold = decision_boundary - min_support\n        updated_weights = (1 - sample_weights) * (y == X[:, feature_index]) + sample_weights * (y != X[:, feature_index])\n        best_thresholds.append((best_threshold, updated_weights))\n    return best_thresholds"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    exp_scores = np.exp(scores)\n    log_softmax_scores = exp_scores - np.max(exp_scores)\n    log_softmax_scores_rounded = log_softmax_scores.round(4)\n    return log_softmax_scores_rounded.tolist()\nscores = [1.0, 2.0, 3.0, 4.0]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n    '\\n    implement the Dense class based on the above structure\\n    '\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided.')\n        self.W = np.random.uniform(0, 1 / np.sqrt(self.input_shape[0]), (self.n_units, self.input_shape[0]))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer = 'adam'\n        self.learning_rate = 0.01\n        self.W_placeholder = np.random.uniform(0, 1 / np.sqrt(self.input_shape[0]), (self.n_units, self.input_shape[0]))\n        self.w0_placeholder = np.zeros((self.n_units,))\n\n    def parameters(self):\n        return self.W.shape + self.w0.shape\n\n    def forward_pass(self, X, training):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided.')\n        output = np.dot(X, self.W)\n        output += self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided.')\n        d_output = accum_grad\n        d_output = d_output.reshape(-1, self.n_units)\n        if self.trainable:\n            self.W -= self.learning_rate * np.dot(d_output.T, self.W)\n            self.w0 -= self.learning_rate * np.dot(d_output.T, self.w0)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    if not isinstance(input_matrix, np.ndarray) or input_matrix.ndim != 2:\n        raise ValueError('Input matrix must be a 2D array.')\n    if not isinstance(kernel, np.ndarray) or kernel.ndim != 2 or kernel.shape != input_matrix.shape:\n        raise ValueError('Kernel must be a 2D array and have the same shape as the input matrix.')\n    output_height = input_matrix.shape[0] - kernel.shape[0] + padding * 2\n    output_width = input_matrix.shape[1] - kernel.shape[1] + padding * 2\n    output = np.zeros((output_height, output_width), dtype=np.float32)\n    for i in range(output_height):\n        for j in range(output_width):\n            patch = input_matrix[i:i + kernel.shape[0], j:j + kernel.shape[1]]\n            output[i, j] = np.sum(patch * kernel)\n    output = output.reshape(-1, output.shape[1])\n    output = np.round(output, decimals=4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit activation function to a given float z.\n\n    Parameters:\n    - z: A float number for which the ReLU function will be applied.\n\n    Returns:\n    - The result of the ReLU function applied to the input z.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression Loss.\n\n    Args:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n    w (np.ndarray): Coefficients of the regression model.\n    y_true (np.ndarray): True labels of shape (n_samples).\n    alpha (float): Regularization parameter.\n\n    Returns:\n    float: Ridge loss value.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization_term = alpha * np.sum(w ** 2)\n    return mse + regularization_term\nX = np.array([[1, 2], [3, 4]])\nw = np.array([1, 0])\ny_true = np.array([2, 5])\nalpha = 0.1"}
{"task_id": 44, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function to a given input z.\n    \n    Parameters:\n    - z: A float representing the input value.\n    - alpha: A float representing the slope of the negative slope. Default is 0.01.\n    \n    Returns:\n    - A float or int after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n\n    Parameters:\n    - x1: A numpy array of shape (n,) representing the first input vector.\n    - x2: A numpy array of shape (n,) representing the second input vector.\n\n    Returns:\n    - A numpy array of shape (n,) representing the linear kernel between x1 and x2.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError('Input vectors must have the same length.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric between true and predicted binary labels.\n\n    Parameters:\n    - y_true (np.ndarray): An array of binary labels (0 or 1).\n    - y_pred (np.ndarray): An array of binary labels (0 or 1).\n\n    Returns:\n    - float: The precision of the predictions.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shapes of y_true and y_pred must be the same.')\n    tp = np.sum(y_true == 1)\n    fp = np.sum(y_true == 0) + np.sum(y_pred == 0)\n    precision = tp / (tp + fp)\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with Mean Squared Error (MSE) loss.\n\n    Parameters:\n    - X: Input features matrix (n_samples x n_features)\n    - y: Target values vector (n_samples x 1)\n    - weights: Initial weights vector (n_features x 1)\n    - learning_rate: Learning rate for the gradient descent\n    - n_iterations: Number of iterations for gradient descent\n    - batch_size: Size of each batch during training\n    - method: Type of gradient descent ('batch', 'sgd', 'mini-batch')\n\n    Returns:\n    - A list containing the weights and the MSE values at each iteration\n    \"\"\"\n    X = X.reshape(-1, X.shape[1])\n    mse_values = []\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradients = np.dot(X.T, y - X.dot(weights))\n        elif method == 'sgd':\n            gradients = np.dot(X.T, y - X.dot(weights) - learning_rate * np.dot(weights, X))\n        elif method == 'mini-batch':\n            gradients = np.dot(X.T, y - X.dot(weights) - learning_rate * np.dot(weights, X[:batch_size]))\n        weights -= learning_rate * gradients\n        mse_values.append((weights.flatten(), np.mean((y - X.dot(weights)) ** 2)))\n    return mse_values.tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    if np.linalg.det(matrix) == 0:\n        raise ValueError('The matrix is not reducible.')\n    n = len(matrix)\n    pivots = []\n    for i in range(n):\n        if matrix[i][i] != 0:\n            pivots.append(i)\n    rref_matrix = []\n    for i in range(n):\n        if i not in pivots:\n            new_row = [0] * n\n            new_row[i] = 1\n            rref_matrix.append(new_row)\n        else:\n            pivot_row = matrix[pivots[i]]\n            for j in range(n):\n                if j != pivots[i]:\n                    factor = pivot_row[j] / matrix[pivots[i]][j]\n                    matrix[pivots[i]] = [matrix[pivots[i]][j] - factor * matrix[j][pivots[i]] for j in range(n)]\n    return rref_matrix.tolist()\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nrref_matrix = rref(matrix)"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    g = np.dot(grad.T, x0)\n    for _ in range(num_iterations):\n        t = _ + 1\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        x0 -= learning_rate * m / np.sqrt(m ** 2 + v ** 2) * g\n    return x0.tolist()\nlearning_rate = 0.01\nnum_iterations = 1000"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    for _ in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        loss = np.sum((y - predictions) ** 2)\n        gradients = np.dot(X.T, predictions - y)\n        weights -= learning_rate * gradients / (2 * X.shape[1])\n        bias -= learning_rate * (predictions - y).mean()\n        if np.linalg.norm(gradients) < tol:\n            break\n    weights = weights.reshape(-1, 1)\n    bias = bias.reshape(-1, 1)\n    return (weights, bias.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([2, 4, 6])"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (n, m) = (len(source), len(target))\n    dp = np.zeros((n + 1, m + 1), dtype=int)\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1] + 1)\n    return dp[n][m]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n\n    Returns:\n    - Recall value rounded to three decimal places. If the denominator (TP + FN) is zero,\n      the recall should be 0.0 to avoid division by zero.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be the same.')\n    TP = np.sum(y_true == 1)\n    FP = np.sum(y_pred == 1)\n    if FP == 0:\n        return 0.0\n    recall_value = TP / (TP + FP)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism for a given input sequence X.\n    \n    Args:\n    X (numpy.ndarray): The input sequence to be processed.\n    W_q (numpy.ndarray): The query matrix for the attention mechanism.\n    W_k (numpy.ndarray): The key matrix for the attention mechanism.\n    W_v (numpy.ndarray): The value matrix for the attention mechanism.\n    \n    Returns:\n    numpy.ndarray: The self-attention output as a numpy array.\n    \"\"\"\n    scores = np.dot(X, W_q) + np.dot(X, W_k.T)\n    scores /= np.linalg.norm(scores, axis=1, keepdims=True)\n    attention_weights = np.exp(scores)\n    output = np.dot(attention_weights, W_v)\n    output = output.reshape(-1, X.shape[1])\n    return output.tolist()\nX = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nW_q = np.array([[0.05, 0.1, 0.2], [0.06, 0.15, 0.25]])\nW_k = np.array([[0.01, 0.02, 0.03], [0.04, 0.05, 0.06]])\nW_v = np.array([[0.07, 0.08, 0.09], [0.1, 0.11, 0.12]])\noutput = self_attention(X, W_q, W_k, W_v)"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.expand_dims(input_sequence, axis=0)\n    hidden_state = initial_hidden_state.copy()\n    for (i, input_vector) in enumerate(input_sequence):\n        z = np.dot(Wx[i], input_vector) + np.dot(Wh, hidden_state) + b\n        tanh_z = np.tanh(z)\n        hidden_state = tanh_z * hidden_state + Wx[i][0] * input_vector\n        hidden_states.append(hidden_state)\n    hidden_states = np.array(hidden_states).reshape(-1, len(input_sequence))\n    return hidden_states.tolist()\ninput_sequence = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nWx = [[1, 2], [3, 4], [5, 6]]\nWh = [[1, 2], [3, 4], [5, 6]]\nb = [0.1, 0.2, 0.3]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[tx, 0], [0, ty]])\n    points_array = np.array(points)\n    translated_points_array = np.dot(points_array, translation_matrix)\n    translated_points = translated_points_array.tolist()\n    return translated_points\npoints = [(1, 2), (3, 4), (5, 6)]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n    - mu_p: Mean of the first normal distribution\n    - sigma_p: Standard deviation of the first normal distribution\n    - mu_q: Mean of the second normal distribution\n    - sigma_q: Standard deviation of the second normal distribution\n\n    Returns:\n    - The KL divergence between the two normal distributions\n    \"\"\"\n    diff_mean = mu_p - mu_q\n    diff_var = sigma_p ** 2 - sigma_q ** 2\n    kl_div = 0.5 * np.sum(diff_mean * np.log(diff_mean / diff_var))\n    return kl_div\nmu_p = 0\nsigma_p = 1\nmu_q = 0\nsigma_q = 1"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x_ini = np.zeros_like(b)\n    (m, n) = A.shape\n    if x_ini is None:\n        x_ini = np.zeros(n)\n    for _ in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            for j in range(n):\n                dot_product = np.dot(A[i], x[i])\n                x_new[j] += dot_product * b[i]\n        if np.linalg.norm(x_new - x_ini) < 1e-06:\n            break\n    return x_new.tolist()\nA = np.array([[2, -3, 1], [-1, 2, -4], [1, 1, 1]])\nb = np.array([2, 0, 1])\nn = 3\nx_ini = np.zeros(3)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian Elimination on a given matrix A and vector b to find the solution.\n\n    Parameters:\n    A (numpy.ndarray): The coefficient matrix.\n    b (numpy.ndarray): The constant vector.\n\n    Returns:\n    numpy.ndarray: The solution vector x.\n    \"\"\"\n    n = A.shape[0]\n    for i in range(n):\n        if abs(A[i][i]) < 1e-06:\n            A[[i, i]] = A[[i, i], [i, -i]]\n            b[[i, i]] = b[[i, i], [i, -i]]\n        pivot = A[i][i]\n        A[i] /= pivot\n        b[i] /= pivot\n        for j in range(i + 1, n):\n            A[j] -= A[i] * A[j][i]\n            b[j] -= b[i] * A[j][i]\n    x = np.zeros(n)\n    x[-1] = b[-1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - np.dot(A[i + 1], x)) / A[i][i]\n    return x.tolist()\nA = np.array([[2, 3], [4, 5]])\nb = np.array([1, 2])"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        x = x.reshape(-1, self.input_size)\n        f = np.tanh(np.dot(self.Wf, x) + self.bi)\n        i = np.tanh(np.dot(self.Wi, x) + self.bi)\n        c = f * initial_cell_state + i * np.tanh(np.dot(self.Wc, x) + self.bi)\n        o = np.tanh(np.dot(self.Wo, c) + self.bo)\n        h_new = c * o\n        cell_new = np.tanh(np.dot(self.Wc, x) + self.bi) * c\n        return (h_new, cell_new)\ninitial_cell_state = np.zeros((1, self.hidden_size))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty.')\n    tf_idfs = []\n    for doc in corpus:\n        term_freqs = {}\n        for word in doc:\n            if word in term_freqs:\n                term_freqs[word] += 1\n            else:\n                term_freqs[word] = 1\n        if len(doc) > 0:\n            idf = np.log(len(corpus) / sum(term_freqs.values()))\n        else:\n            idf = 0\n        tfidf = idf * term_freqs.values()\n        tf_idfs.append(tfidf.round(4))\n    return tf_idfs\ncorpus = [['hello', 'world'], ['algorithm', 'programming'], ['data', 'science']]"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta=1):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('True labels and predicted labels must have the same length.')\n    precision = np.mean(y_true == y_pred)\n    recall = np.mean(y_true == 1)\n    f_score = 2 * precision * recall / (precision + recall)\n    if beta == 1:\n        return f_score\n    else:\n        return f_score * beta\nbeta = 0.5"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        for (i, x_t) in enumerate(x):\n            h_t = self.sigmoid(np.dot(self.W_xh, x_t) + self.b_h)\n            y_t = self.sigmoid(np.dot(self.W_hh, h_t) + self.b_y)\n            h = h_t\n        return y_t\n\n    def sigmoid(self, x):\n        \"\"\"\n        Sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Perform backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        loss = self.compute_loss(outputs, last_outputs, expected_output)\n        total_loss = np.sum(loss)\n        gradients = {}\n        for i in range(len(last_inputs)):\n            grad_hh = np.dot(W_hh.T, outputs[i] - expected_output[i])\n            grad_bh = np.sum(outputs[i] - expected_output[i], axis=0)\n            grad_yh = np.dot(W_hy.T, outputs[i] - expected_output[i])\n            grad_byy = np.sum(outputs[i] - expected_output[i], axis=0)\n            gradients['W_hh'] = grad_hh\n            gradients['b_h'] = grad_bh\n            gradients['W_hy'] = grad_yh\n            gradients['b_y'] = grad_byy\n            self.W_hh -= learning_rate * grad_hh\n            self.b_h -= learning_rate * grad_bh\n            self.W_hy -= learning_rate * grad_yh\n            self.b_y -= learning_rate * grad_byy\n        return total_loss\n\n    def compute_loss(self, outputs, last_outputs, expected_output):\n        \"\"\"\n        Compute the mean squared error loss.\n        \"\"\"\n        return np.mean((outputs - last_outputs) ** 2)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    for _ in range(n):\n        dot_product = np.dot(x, A)\n        step_size = -dot_product / np.dot(b, A)\n        x += step_size * b\n        if np.linalg.norm(x - x0) < tol:\n            break\n    return x.tolist()\nA = np.array([[2, 3], [1, 4]])\nb = np.array([5, 6])\nn = 1000\nx0 = np.zeros_like(b)\ntol = 1e-06"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_instances = len(y)\n    class_counts = {}\n    for label in y:\n        if label in class_counts:\n            class_counts[label] += 1\n        else:\n            class_counts[label] = 1\n    gini_impurity = 0.0\n    for (label, count) in class_counts.items():\n        p_label = count / n_instances\n        gini_impurity += p_label ** 2 * (1 - p_label)\n    return round(gini_impurity, 3)\ny = [1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5]"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    non_zero_elements = []\n    for (i, row) in enumerate(dense_matrix):\n        for (j, value) in enumerate(row):\n            if value != 0:\n                non_zero_elements.append((i, j))\n    values = [value for (_, _) in non_zero_elements]\n    row_pointers = [0] * len(dense_matrix)\n    current_row = 0\n    for (i, row) in enumerate(dense_matrix):\n        for (j, value) in enumerate(row):\n            if value != 0:\n                row_pointers[current_row] = i\n                current_row += 1\n    return (values, non_zero_elements, row_pointers)\ndense_matrix = [[1, 0, 0], [0, 2, 0], [0, 0, 3]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    magnitude_L = np.linalg.norm(L)\n    projection = dot_product / magnitude_L * L\n    projection_rounded = round(projection, 3)\n    return projection_rounded.tolist()\nv = [1, 2, 3]\nL = [2, 3, 4]\nprojection = orthogonal_projection(v, L)"}
{"task_id": 67, "completion_id": 0, "solution": "from typing import List\ndef compressed_col_sparse_matrix(dense_matrix: List[List[int]]) -> Tuple[List[int], List[int], List[int]]:\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = []\n    for (i, row) in enumerate(dense_matrix):\n        for (j, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                row_indices.append(i)\n                column_pointer.append(j)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[1, 0, 0], [0, 2, 0], [3, 0, 4]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space of a given matrix A.\n\n    Args:\n    A (np.ndarray): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    rref = np.linalg.matrix_rank(A)\n    if rref == 0:\n        raise ValueError('The matrix is singular, which cannot have a column space.')\n    rank = rref\n    column_space = []\n    for col in range(rank):\n        non_zero_index = np.where(A[:, col] != 0)[0][0]\n        column_space.append(A[:, non_zero_index])\n    return [v.tolist() for v in column_space]\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared for Regression Analysis.\n\n    Args:\n    - y_true (numpy.ndarray): True values of the dependent variable.\n    - y_pred (numpy.ndarray): Predicted values of the dependent variable.\n\n    Returns:\n    - float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The number of true values must be equal to the number of predicted values.')\n    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_residuals = np.sum((y_pred - np.mean(y_pred)) ** 2)\n    r_squared = 1 - ss_residuals / ss_total\n    return round(r_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "from typing import List\ndef calculate_brightness(img: List[List[int]]) -> float:\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n    img (List[List[int]]): A 2D list representing the grayscale image.\n\n    Returns:\n    float: The average brightness of the image rounded to two decimal places. Returns -1 if the image is empty or has inconsistent dimensions.\n    \"\"\"\n    if not img or len(img[0]) == 0:\n        return -1\n    total_brightness = sum((sum(row) for row in img))\n    num_pixels = len(img[0]) * len(img)\n    if num_pixels == 0:\n        return -1\n    brightness = total_brightness / num_pixels\n    return round(brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Parameters:\n    - y_true: A numpy array of shape (n,) containing the actual values.\n    - y_pred: A numpy array of shape (n,) containing the predicted values.\n\n    Returns:\n    - The RMSE value rounded to three decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Both y_true and y_pred must have the same shape.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Both y_true and y_pred cannot be empty.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return round(rmse, 3)\ny_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\ny_pred = np.array([1.1, 2.2, 3.3, 4.4, 5.5])"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    if len(y_true) != len(y_pred) or y_true == [] or y_pred == []:\n        return 0\n    intersection = np.intersect1d(y_true, y_pred)\n    union = np.union1d(y_true, y_pred)\n    jaccard_index = len(intersection) / len(union)\n    jaccard_index_rounded = round(jaccard_index, 3)\n    return jaccard_index_rounded\ny_true = np.array([0, 1, 0, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 0])"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    - y_true: A 1D numpy array of shape (n,) representing the true labels.\n    - y_pred: A 1D numpy array of shape (n,) representing the predicted labels.\n\n    Returns:\n    - The Dice Score as a float value rounded to 3 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Both arrays must have the same length.')\n    intersection = np.intersect1d(y_true, y_pred)\n    union = np.union1d(y_true, y_pred)\n    if not intersection.size:\n        return 0.0\n    dice_score = 2 * intersection.size / (union.size + intersection.size)\n    return round(dice_score, 3)\ny_true = np.array([1, 0, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 1, 0])"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary to ensure reproducibility.\n\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n\n    Parameters:\n    - row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = []\n    for (feature, value) in row.items():\n        hypervector = np.random.rand(dim)\n        composite_hv.append((feature, hypervector))\n    return list(composite_hv)\nrow = {'a': 1, 'b': 2, 'c': 3}\ndim = 4\nrandom_seeds = {'a': 100, 'b': 200, 'c': 300}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n\n    Parameters:\n    - data: A list of lists, where each inner list represents a pair\n    - [y_true, y_pred] for one observation. 'y_true' is the actual label, and 'y_pred' is the predicted label.\n\n    Returns:\n    - A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    true_count = Counter(y_true)\n    pred_count = Counter(y_pred)\n    correct_predictions = sum(true_count.values())\n    total_observations = len(data)\n    confusion_matrix = [[true_count[y_true[0]], pred_count[y_true[1]]], [pred_count[y_pred[0]], true_count[y_pred[1]]]]\n    return confusion_matrix\ndata = [[0, 1], [1, 0], [0, 0], [1, 1]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    v1 (np.ndarray): The first vector.\n    v2 (np.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity between the vectors, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Both vectors cannot be zero magnitude.')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == p))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a != p))\n    tn = len(actual) - tp - fn\n    fp = len(actual) - tp\n    confusion_matrix = [[tp, fn], [tn, fp]]\n    accuracy = tp / (tp + fn) if tp + fn > 0 else 0\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1_score = 2 * precision * recall / (precision + recall)\n    specificity = tn / (tn + fp)\n    negative_predictive_value = tn / (tn + tp)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Parameters:\n    - data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    - dict: A dictionary containing mean, median, mode, variance, standard deviation,\n             25th percentile, 50th percentile, 75th percentile, and IQR.\n    \"\"\"\n    if not isinstance(data, (list, np.ndarray)):\n        raise ValueError('Input must be a list or NumPy array.')\n    mean = np.mean(data)\n    median = np.median(data)\n    unique_values = np.unique(data)\n    mode = unique_values[0] if len(unique_values) > 1 else None\n    variance = np.var(data)\n    std_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n    mean = round(mean, 4)\n    median = round(median, 4)\n    mode = round(mode, 4)\n    variance = round(variance, 4)\n    std_deviation = round(std_deviation, 4)\n    percentile_25 = round(percentile_25, 4)\n    percentile_50 = round(percentile_50, 4)\n    percentile_75 = round(percentile_75, 4)\n    iqr = round(iqr, 4)\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': std_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\ndata = [1, 2, 3, 4, 5]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 3\np = 0.5"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    std_dev = math.sqrt(std_dev)\n    area = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    return round(area, 5)\nmean = 0\nstd_dev = 1\nx = 2.5"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or not isinstance(lam, float) or k < 0:\n        raise ValueError('Both k and lam must be non-negative integers.')\n    probability = math.exp(-lam * k)\n    return round(probability, 5)\nk = 3\nlam = 2"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image.\n    \"\"\"\n    max_value = np.max(img)\n    min_value = np.min(img)\n    contrast = (max_value - min_value) / 255\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Vectors must be of the same length.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    transformed_data = []\n    for point in data:\n        features = [np.power(point, i) for i in range(1, degree + 1)]\n        rounded_features = [round(feature, 8) for feature in features]\n        transformed_data.append(rounded_features)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int) -> np.ndarray:\n    \"\"\"\n    Calculate positional encodings for a sequence length (position) and model dimensionality (d_model).\n\n    Args:\n    position (int): The position in the sequence.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    np.ndarray: A -1 if position is 0, or if d_model is less than or equal to 0.\n    \"\"\"\n    if position == 0:\n        return -1.0 * np.ones(d_model)\n    pos_encoding = np.zeros((position, d_model))\n    sin = np.sin(np.arange(position) / d_model)\n    cos = np.cos(np.arange(position) / d_model)\n    pos_encoding[:, 0] = sin\n    pos_encoding[:, 1:] = cos\n    return pos_encoding.tolist()\nposition = 5\nd_model = 64"}
{"task_id": 86, "completion_id": 0, "solution": "import numpy as np\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    accuracy_difference = abs(training_accuracy - test_accuracy)\n    if accuracy_difference > 0.2:\n        return 1\n    elif accuracy_difference < 0.7:\n        return -1\n    else:\n        return 0\ntraining_accuracy = 0.85\ntest_accuracy = 0.9"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    squared_grad = np.square(grad)\n    if t == 0:\n        m = squared_grad\n    else:\n        m = (beta1 * m + (1 - beta1) * squared_grad) / (1 - beta1 ** t)\n    if t == 0:\n        v = squared_grad\n    else:\n        v = (beta2 * v + (1 - beta2) * squared_grad) / (1 - beta2 ** t)\n    learning_rate *= np.sqrt(1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m\n    return (updated_parameter.tolist(), m.tolist(), v.tolist())\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 10"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    model = torch.nn.Transformer(d_model=hparams['n_ctx'], n_heads=hparams['n_head'], num_layers=4, dropout=0.1, attention_dropout=0.1, layer_norm=True, pad_token_id=params['wpe'].shape[0] - 1)\n    outputs = model(input_ids)\n    hidden_states = outputs[0][:n_tokens_to_generate]\n    logits = outputs[1][:n_tokens_to_generate]\n    logits = logits.view(-1, n_tokens_to_generate)\n    logits = logits / params['ln_f']['g']\n    generated_text = []\n    for _ in range(n_tokens_to_generate):\n        (_, next_token) = torch.max(logits, dim=-1)\n        generated_text.append(params['wpe'][next_token])\n        logits[next_token] = 0\n    generated_text = [params['decode'](token) for token in generated_text]\n    return ' '.join(generated_text)\nprompt = 'Hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        return np.exp(values) / np.sum(np.exp(values))\n    relationship_matrix = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i + 1, n):\n            relationship_matrix[i, j] = softmax(crystal_values[i] + crystal_values[j])\n    attention_scores = relationship_matrix * crystal_values\n    result = [round(score, 4) for score in attention_scores]\n    return result\nn = 5\ncrystal_values = [1, 2, 3, 4, 5]"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    query_words = query.split()\n    scores = {}\n    for (doc_id, doc) in enumerate(corpus):\n        word_counts = Counter(doc.split())\n        tfidf_score = 0\n        for word in query_words:\n            if word in word_counts:\n                tfidf_score += word_counts[word] * (k1 + 1) * word_counts[word] / (len(word_counts) + k1)\n            else:\n                tfidf_score += 0\n        tfidf_score /= max(tfidf_score, 1e-10)\n        scores[doc_id] = tfidf_score\n    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\ncorpus = ['The quick brown fox jumps over the lazy dog.', 'This is a test document.', 'Another test document.']\nquery = 'quick brown fox'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "import numpy as np\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    precision = sum((1 for (label_true, label_pred) in zip(y_true, y_pred) if label_true == label_pred)) / len(y_true)\n    recall = sum((1 for (label_true, label_pred) in zip(y_true, y_pred) if label_true == 1)) / len(y_true)\n    f1_score = 2 * precision * recall / (precision + recall)\n    return round(f1_score, 3)\ny_true = [0, 1, 0, 1, 1, 0]\ny_pred = [0, 1, 1, 0, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    fluctuation = 10 * math.sin(2 * PI * math.floor(len(consumption_data) / 10))\n    n = len(consumption_data)\n    x = [i for i in range(1, n + 1)]\n    y = [consumption_data[i - 1] + fluctuation for i in range(1, n + 1)]\n    y_pred = sum((x[i] * y[i] for i in range(1, n + 1)))\n    y_pred += fluctuation\n    y_pred_rounded = round(y_pred * 1.05)\n    return y_pred_rounded\nconsumption_data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Arrays must be of the same length.')\n    abs_diff = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_diff)\n    return round(mae_value, 3)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.2, 3.3, 4.4, 5.5])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Multi-Head Attention mechanism.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_len, hidden_dim).\n    W_q (np.ndarray): Weight matrix for query vectors of shape (n_heads, hidden_dim).\n    W_k (np.ndarray): Weight matrix for key vectors of shape (n_heads, hidden_dim).\n    W_v (np.ndarray): Weight matrix for value vectors of shape (n_heads, hidden_dim).\n    n_heads (int): Number of heads in the multi-head attention mechanism.\n\n    Returns:\n    list: A list containing the output tensors of shape (batch_size, seq_len, n_heads * hidden_dim).\n    \"\"\"\n    (batch_size, seq_len, hidden_dim) = X.shape\n    (n_heads, hidden_dim) = (n_heads, hidden_dim)\n    n_heads *= 2\n    (q, k, v) = ([], [], [])\n    for i in range(n_heads):\n        q.append(X[:, i * hidden_dim:(i + 1) * hidden_dim].dot(W_q))\n        k.append(X[:, i * hidden_dim:(i + 1) * hidden_dim].dot(W_k))\n        v.append(X[:, i * hidden_dim:(i + 1) * hidden_dim].dot(W_v))\n    q = np.concatenate(q, axis=1)\n    k = np.concatenate(k, axis=1)\n    v = np.concatenate(v, axis=1)\n    q = q / np.sqrt(hidden_dim)\n    k = k / np.sqrt(hidden_dim)\n    v = v / np.sqrt(hidden_dim)\n    attn_output = np.matmul(q, k.transpose(1, 0)) * np.exp(-0.5 * np.matmul(v, k.transpose(1, 0)))\n    return attn_output.reshape(batch_size, seq_len, n_heads * hidden_dim)\nX = np.random.rand(8, 64, 64)\nW_q = np.random.rand(8, 64, 64)\nW_k = np.random.rand(8, 64, 64)\nW_v = np.random.rand(8, 64, 64)\nn_heads = 8\nattn_output = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 95, "completion_id": 0, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    product_sum = sum((x[i] * y[i] for i in range(len(x))))\n    sum_of_squares = sum((x[i] ** 2 for i in range(len(x))))\n    numerator = product_sum - sum_of_squares\n    denominator = math.sqrt(sum_of_squares)\n    phi_coefficient = numerator / denominator\n    return round(phi_coefficient, 4)\nx = [0, 1, 0, 1]\ny = [1, 0, 1, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "import numpy as np\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    x = np.clip(x, -1, 1)\n    return 1 / (1 + np.exp(-x))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * math.exp(x)\n    else:\n        return x"}
{"task_id": 98, "completion_id": 0, "solution": "import numpy as np\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    sign = np.sign(x)\n    if sign < 0:\n        return alpha * x\n    else:\n        return x\nx = 1.0\nalpha = 0.5"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < 0:\n        return -math.inf\n    softplus_value = math.log(1 + math.exp(x))\n    return round(softplus_value, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "import numpy as np\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    x = np.clip(x, -1, 1)\n    return 1 / (1 + x ** 2)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value rounded to the nearest 6th decimal.\n    \"\"\"\n    rhos = [max(epsilon, min(1 - epsilon, rho)) for rho in rhos]\n    sum_rho_A = sum((rho_i * A_i for (rho_i, A_i) in zip(rhos, A)))\n    expected_A = sum((pi_theta_old[i] * A_i for i in range(len(pi_theta_old))))\n    kl_divergence_penalty = beta * np.sum((np.log(pi_theta_old[i] / pi_theta_ref[i]) for i in range(len(pi_theta_old))))\n    return sum_rho_A - expected_A + kl_divergence_penalty\nrhos = [0.5, 0.3, 0.4]\nA = [1.0, 0.5, 0.7]\npi_theta_old = [0.8, 0.6, 0.9]\npi_theta_ref = [0.7, 0.6, 0.8]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 + math.exp(-x))"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return x * scale\n    else:\n        return alpha * (x + math.sqrt(x ** 2)) / (1 + math.sqrt(x ** 2))"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.special import logit\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    predictions = logit(X @ weights + bias)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    B = np.zeros(X.shape[1])\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, B)\n        A = np.exp(z)\n        loss = -np.sum(y * np.log(A))\n        dA = np.dot(A, X.T)\n        dA -= np.dot(y, dA)\n        B -= learning_rate * np.dot(X.T, dA)\n        losses.append(loss)\n    B = B.tolist()\n    losses = [loss for loss in losses]\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    w = np.zeros(X.shape[1])\n    b = 0\n    for _ in range(iterations):\n        y_pred = np.dot(X, w) + b\n        loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        gradients = np.dot(X.T, y_pred - y) / len(X)\n        w -= learning_rate * gradients\n        b -= learning_rate * (y_pred - y).sum()\n    return [w, b, loss]"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    Q_masked = Q * mask[:, None]\n    K_masked = K * mask[:, None]\n    Q_kv = np.matmul(Q_masked, K_masked)\n    return Q_kv.reshape(-1, Q.shape[-1], K.shape[-1])\nmask = np.random.rand(32, 1)"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    Parameters:\n    apples (list): A list of integers representing the colors of the apples.\n    \n    Returns:\n    float: The disorder of the apples.\n    \"\"\"\n    unique_colors = len(set(apples))\n    total_apples = len(apples)\n    disorder = unique_colors - total_apples\n    return round(disorder, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n\n    Parameters:\n    - X: A 3D numpy array representing batch size, sequence length, and feature dimensions.\n    - gamma: A 1D numpy array of weights for the normalization.\n    - beta: A 1D numpy array of biases for the normalization.\n    - epsilon: A small value to avoid division by zero in normalization.\n\n    Returns:\n    - A 3D numpy array where each sequence's features have been normalized.\n    \"\"\"\n    mean = np.mean(X, axis=2)\n    var = np.var(X, axis=2)\n    normalized = (X - mean) / np.sqrt(var + epsilon)\n    return normalized.tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower()\n    candidate = candidate.lower()\n    ref_counts = Counter(reference)\n    cand_counts = Counter(candidate)\n    correct_words = sum((ref_counts[word] == cand_counts[word] for word in ref_counts.keys()))\n    unmatched_words = len(ref_counts) - correct_words\n    unmatched_chars = sum((1 for word in ref_counts if word not in cand_counts))\n    precision = correct_words / unmatched_words\n    recall = correct_words / len(ref_counts)\n    f_mean = 2 * precision * recall / (precision + recall)\n    penalty = alpha * unmatched_chars * beta * gamma\n    score = f_mean + penalty\n    return round(score, 3)\nreference = 'This is an example sentence.'\ncandidate = 'This is another example sentence.'"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Args:\n    joint_counts (np.ndarray): A 2D array where each row represents the counts of events X and Y.\n    total_counts_x (int): The total count of event X.\n    total_counts_y (int): The total count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information as a float rounded to 3 decimal places.\n    \"\"\"\n    expected_count_X = np.sum(joint_counts, axis=0) / total_samples\n    expected_count_Y = np.sum(joint_counts, axis=1) / total_samples\n    pmi = np.sum(joint_counts * np.log(joint_counts * expected_count_X / (expected_count_Y * total_counts_x)), axis=1) / (total_samples * total_samples)\n    return round(pmi, 3)\njoint_counts = np.array([[1, 2], [3, 4]])\ntotal_counts_x = 5\ntotal_samples = 10"}
{"task_id": 112, "completion_id": 0, "solution": "import numpy as np\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    \n    Args:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats, scaled to the range [0, 1].\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    normalized_x = [(value - min_val) / (max_val - min_val) for value in x]\n    normalized_x_rounded = [round(value, 4) for value in normalized_x]\n    return normalized_x_rounded"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Create a simple residual block using NumPy.\n\n    Args:\n    x (np.ndarray): A 1D input array.\n    w1 (np.ndarray): First weight layer of the residual block.\n    w2 (np.ndarray): Second weight layer of the residual block.\n\n    Returns:\n    list: A list containing the processed input and the output of the residual block.\n    \"\"\"\n    x = np.maximum(0, x * w1)\n    x = x + x\n    x = np.maximum(0, x * w2)\n    return [x.tolist()]\nw1 = np.array([0.5, 0.5])\nw2 = np.array([0.2, 0.2])"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Parameters:\n    x (np.ndarray): A 3D NumPy array where the first two dimensions are the height and width of the feature maps,\n                     and the third dimension is the number of channels.\n\n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    avg = np.mean(x, axis=-1)\n    return avg"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n\n    Parameters:\n    - X: A 4D NumPy array (batch, channels, height, width).\n    - gamma: A 1D NumPy array (channels) containing the scale parameter.\n    - beta: A 1D NumPy array (channels) containing the shift parameter.\n    - epsilon: A float representing the epsilon value for numerical stability.\n\n    Returns:\n    - A 4D NumPy array (batch, channels, height, width) with normalized feature maps.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 1, 2))\n    var = np.var(X, axis=(0, 1, 2))\n    normalized = (X - mean) / np.sqrt(var + epsilon)\n    return normalized.tolist()\nX = np.random.rand(3, 4, 64, 64)"}
{"task_id": 116, "completion_id": 0, "solution": "import math\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float): A tolerance value to determine linear independence.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    normalized_vectors = [np.array(vector) / np.linalg.norm(vector) for vector in vectors]\n    for i in range(1, len(normalized_vectors)):\n        new_vector = normalized_vectors[i] - np.dot(normalized_vectors[:i], normalized_vectors[i])\n        normalized_vectors[i] = new_vector / np.linalg.norm(new_vector)\n    return normalized_vectors.tolist()\nvectors = [[1, 2], [3, 4]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n\n    Parameters:\n    a (np.array): A 3D vector.\n    b (np.array): Another 3D vector.\n\n    Returns:\n    np.array: The cross product of the two vectors, rounded to 4 decimal places.\n    \"\"\"\n    if a.ndim != 3 or b.ndim != 3:\n        raise ValueError('Both inputs must be 3D arrays.')\n    cross_product = np.cross(a, b)\n    return round(cross_product, 4)\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n\n    Returns:\n    np.array: The solution vector x, or -1 if the system has no unique solution.\n    \"\"\"\n    if np.linalg.det(A) == 0:\n        return -1\n    det_A = np.linalg.det(A)\n    x = np.zeros_like(b)\n    for i in range(len(A)):\n        new_A = np.delete(A, i, axis=0)\n        new_det_A = np.linalg.det(new_A)\n        coeff_x_i = det_A / new_det_A\n        x[i] = coeff_x_i * b[i]\n    return x.tolist()\nA = np.array([[2, 3], [4, 5]])\nb = np.array([6, 8])"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Parameters:\n    p (list[float]): A list of probabilities for the first distribution.\n    q (list[float]): A list of probabilities for the second distribution.\n\n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places.\n    \"\"\"\n    if len(p) != len(q):\n        return 0.0\n    if not p or not q:\n        return 0.0\n    numerator = np.sum((p - q) ** 2)\n    denominator = np.sum(p + q)\n    return np.sqrt(numerator / denominator)\np = [0.1, 0.3, 0.6]\nq = [0.2, 0.4, 0.8]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nresult = vector_sum(vector1, vector2)"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_episodes = len(episodes)\n    total_returns = [0] * num_episodes\n    total_rewards = [0] * num_episodes\n    total_log_likelihoods = [0] * num_episodes\n    for (episode_index, episode) in enumerate(episodes):\n        (state, action, reward) = episode[0]\n        log_policy = np.log(np.exp(theta[state, :]) / np.sum(theta[state, :]))\n        total_returns[episode_index] += reward\n        total_log_likelihoods[episode_index] += log_policy[action]\n        theta[state, :] -= 0.01 * np.mean(total_log_likelihoods[episode_index - 1] - total_log_likelihoods[episode_index])\n    total_returns = np.array(total_returns).reshape(-1, 1)\n    total_rewards = np.array(total_rewards).reshape(-1, 1)\n    total_log_likelihoods = np.array(total_log_likelihoods).reshape(-1, 1)\n    average_gradient = np.mean(total_returns * total_log_likelihoods, axis=1)\n    average_gradient = average_gradient.round(4)\n    return average_gradient.tolist()\ntheta = np.array([[0.5, 0.3], [0.4, 0.6]])\nepisodes = [[(1, 0, 1), (2, 0, 1), (3, 0, 1)], [(1, 1, 1), (2, 1, 1), (3, 1, 1)]]"}
{"task_id": 123, "completion_id": 0, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in ** 2 / n_experts\n    flops_moe = k_active * d_in ** 2 / n_experts\n    savings_percentage = flops_moe / flops_dense * 100\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return savings_percentage_rounded\nn_experts = 100\nk_active = 50\nd_in = 10\nd_out = 20\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the Noisy Top-K Gating Matrix for a given input matrix X.\n\n    Parameters:\n    - X: A 2D numpy array of shape (n_samples, n_features).\n    - W_g: A 2D numpy array of shape (n_features, k) representing the weights for each expert.\n    - W_noise: A 2D numpy array of shape (n_features, k) representing the noise weights for each expert.\n    - N: A 2D numpy array of shape (n_samples, k) representing the pre-sampled noise weights for each expert.\n    - k: An integer representing the number of experts to consider for gating.\n\n    Returns:\n    - A 2D numpy array of shape (n_samples, k) containing the gating probabilities for each expert.\n    \"\"\"\n    gating_probabilities = np.dot(W_g, N) + np.dot(W_noise, N)\n    gating_probabilities = np.round(gating_probabilities, 4)\n    return gating_probabilities.tolist()\nW_g = np.random.rand(5, 3)\nW_noise = np.random.rand(5, 3)\nN = np.random.rand(100, 3)"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    dot_product = np.dot(x, We)\n    gating_probabilities = np.exp(-np.dot(dot_product, Wg))\n    sorted_indices = np.argsort(gating_probabilities, axis=0)[::-1]\n    selected_indices = sorted_indices[:top_k]\n    transformed_indices = []\n    for index in selected_indices:\n        transformed_indices.append(np.dot(We[index], x))\n    weighted_sum = np.sum(transformed_indices, axis=0)\n    result = [round(weighted_sum[i], 4) for i in range(len(weighted_sum))]\n    return result.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    Args:\n    X (np.ndarray): Input tensor with shape (B, C, H, W).\n    gamma (np.ndarray): Scale factor for the learned normalization.\n    beta (np.ndarray): Shift factor for the learned normalization.\n    num_groups (int): Number of groups for each channel.\n    epsilon (float): Epsilon value used in the normalization.\n    \n    Returns:\n    np.ndarray: Normalized tensor.\n    \"\"\"\n    mean_channel = np.mean(X, axis=(0, 2, 3))\n    std_channel = np.std(X, axis=(0, 2, 3))\n    normalized_channel = (X - mean_channel) / (std_channel + epsilon)\n    normalized_channel = normalized_channel.reshape(-1, num_groups, X.shape[2], X.shape[3])\n    return normalized_channel.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.optimize import fsolve\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n    x_solution = fsolve(f, start_x, args=(learning_rate, tolerance), maxiter=max_iters)\n    return round(x_solution[0], 4)\nstart_x = 0.5"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh: Normalization-Free Transformer Activation\n\n    Args:\n    x (np.ndarray): Input tensor.\n    alpha (float): Scaling factor for the activation.\n    gamma (float): Slope factor for the activation.\n    beta (float): Bias factor for the activation.\n\n    Returns:\n    list[float]: Transformed tensor.\n    \"\"\"\n    squared_diff = x ** 2 - np.linalg.norm(x, axis=1)\n    tanh_output = np.tanh(squared_diff * alpha + gamma * beta)\n    transformed_output = tanh_output.reshape(-1, 1)\n    return transformed_output.tolist()\nalpha = 0.5\ngamma = 0.2\nbeta = 0.3"}
