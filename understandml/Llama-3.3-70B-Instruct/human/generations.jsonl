{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    This function computes the dot product of a matrix and a vector.\n\n    Args:\n        a (list[list[int|float]]): A 2D list representing the matrix.\n        b (list[int|float]): A list representing the vector.\n\n    Returns:\n        list[int|float]: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n\n    # Check if the matrix and vector dimensions are compatible\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n\n    # Initialize an empty list to store the result\n    result = []\n\n    # Iterate over each row in the matrix\n    for row in a:\n        # Compute the dot product of the current row and the vector\n        dot_product = sum(x * y for x, y in zip(row, b))\n        \n        # Append the dot product to the result list\n        result.append(dot_product)\n\n    # Return the resulting vector\n    return result\n\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6]]\nvector = [7, 8, 9]\nprint(matrix_dot_vector(matrix, vector))  # Output: [50, 122]\n\nmatrix = [[1, 2], [3, 4]]\nvector = [5, 6, 7]\nprint(matrix_dot_vector(matrix, vector))  # Output: -1\n```\n\nIn this code:\n\n1.  We define a function `matrix_dot_vector` that takes a matrix `a` and a vector `b` as input.\n2.  We check if the matrix and vector dimensions are compatible by verifying that the number of columns in the matrix equals the length of the vector. If they are not compatible, we return -1.\n3.  We initialize an empty list `result` to store the resulting vector.\n4.  We iterate over each row in the matrix and compute the dot product of the current row and the vector using a generator expression within the built-in `sum` function.\n5.  We append each dot product to the `result` list.\n6.  Finally, we return the `result` list, which represents the resulting vector.\n\nThe example usage demonstrates how to use the `matrix_dot_vector` function with a compatible matrix and vector, as well as with incompatible dimensions."}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n    \n    Args:\n        a (list[list[int|float]]): The input matrix.\n    \n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    \n    # Check if the input matrix is empty\n    if not a:\n        return []\n    \n    # Get the number of rows and columns in the input matrix\n    num_rows = len(a)\n    num_cols = len(a[0])\n    \n    # Initialize the transpose matrix with zeros\n    transpose = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    \n    # Iterate over the rows and columns of the input matrix\n    for i in range(num_rows):\n        for j in range(num_cols):\n            # Swap the row and column indices to compute the transpose\n            transpose[j][i] = a[i][j]\n    \n    return transpose\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\n\ntranspose = transpose_matrix(matrix)\nprint(\"Transpose of the Matrix:\")\nfor row in transpose:\n    print(row)\n```\n\nAlternatively, you can use the built-in `zip` function in Python to compute the transpose of a matrix in a more concise way:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n    \n    Args:\n        a (list[list[int|float]]): The input matrix.\n    \n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    return list(map(list, zip(*a)))\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\n\ntranspose = transpose_matrix(matrix)\nprint(\"Transpose of the Matrix:\")\nfor row in transpose:\n    print(row)\n```\n\nOr, you can use the `numpy` library, which provides a `transpose` function to compute the transpose of a matrix:\n\n```python\nimport numpy as np\n\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n    \n    Args:\n        a (list[list[int|float]]): The input matrix.\n    \n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    return np.transpose(a).tolist()\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\n\ntranspose = transpose_matrix(matrix)\nprint(\"Transpose of the Matrix:\")\nfor row in transpose:\n    print(row)\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n        new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n        list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n    \n    # First, we need to convert the input list into a numpy array\n    arr = np.array(a)\n    \n    # Calculate the total number of elements in the input matrix\n    total_elements = arr.size\n    \n    # Calculate the total number of elements in the desired shape\n    desired_elements = new_shape[0] * new_shape[1]\n    \n    # If the total number of elements in the input matrix is not equal to the total number of elements in the desired shape,\n    # it is not possible to reshape the matrix, so return an empty list\n    if total_elements != desired_elements:\n        return []\n    \n    # Try to reshape the numpy array into the desired shape\n    try:\n        reshaped_arr = arr.reshape(new_shape)\n    except ValueError:\n        # If the reshape operation fails, return an empty list\n        return []\n    \n    # Convert the reshaped numpy array back into a list and return it\n    return reshaped_arr.tolist()\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (3, 2)\nprint(reshape_matrix(matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n\nmatrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (2, 3)\nprint(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3], [4, 5, 6]]\n\nmatrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (1, 6)\nprint(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3, 4, 5, 6]]\n\nmatrix = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (2, 4)\nprint(reshape_matrix(matrix, new_shape))  # Output: []\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    matrix (list[list[float]]): A 2D list of floats representing the matrix.\n    mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means according to the specified mode.\n\n    Raises:\n    ValueError: If the mode is not 'row' or 'column'.\n    \"\"\"\n\n    # Check if the mode is valid\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n\n    # Initialize an empty list to store the means\n    means = []\n\n    # If the mode is 'row', calculate the mean of each row\n    if mode == 'row':\n        # Iterate over each row in the matrix\n        for row in matrix:\n            # Calculate the mean of the current row\n            row_mean = sum(row) / len(row)\n            # Append the mean to the list of means\n            means.append(row_mean)\n\n    # If the mode is 'column', calculate the mean of each column\n    elif mode == 'column':\n        # Use the built-in zip function to transpose the matrix\n        transposed_matrix = list(zip(*matrix))\n        \n        # Iterate over each column in the transposed matrix\n        for column in transposed_matrix:\n            # Calculate the mean of the current column\n            column_mean = sum(column) / len(column)\n            # Append the mean to the list of means\n            means.append(column_mean)\n\n    # Return the list of means\n    return means\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(calculate_matrix_mean(matrix, 'row'))  # Output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix, 'column'))  # Output: [4.0, 5.0, 6.0]\n```\n\nAlternatively, you can use the `numpy` library to simplify the function:\n\n```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    matrix (list[list[float]]): A 2D list of floats representing the matrix.\n    mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means according to the specified mode.\n\n    Raises:\n    ValueError: If the mode is not 'row' or 'column'.\n    \"\"\"\n\n    # Check if the mode is valid\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n\n    # Convert the matrix to a numpy array\n    array = np.array(matrix)\n\n    # If the mode is 'row', calculate the mean of each row\n    if mode == 'row':\n        # Use the numpy mean function with axis=1 to calculate the mean of each row\n        means = np.mean(array, axis=1).tolist()\n\n    # If the mode is 'column', calculate the mean of each column\n    elif mode == 'column':\n        # Use the numpy mean function with axis=0 to calculate the mean of each column\n        means = np.mean(array, axis=0).tolist()\n\n    # Return the list of means\n    return means\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(calculate_matrix_mean(matrix, 'row'))  # Output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix, 'column'))  # Output: [4.0, 5.0, 6.0]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    This function multiplies a matrix by a scalar and returns the result.\n\n    Args:\n        matrix (list[list[int|float]]): A 2D list representing the matrix.\n        scalar (int|float): The scalar value to multiply the matrix with.\n\n    Returns:\n        list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n\n    # Initialize an empty list to store the result\n    result = []\n\n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Initialize an empty list to store the result of the current row\n        new_row = []\n        \n        # Iterate over each element in the row\n        for element in row:\n            # Multiply the element by the scalar and append it to the new row\n            new_row.append(element * scalar)\n        \n        # Append the new row to the result\n        result.append(new_row)\n\n    # Return the resulting matrix\n    return result\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a sample matrix\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n    # Define a sample scalar\n    scalar = 2\n\n    # Perform scalar multiplication\n    result = scalar_multiply(matrix, scalar)\n\n    # Print the result\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    print(\"\\nScalar:\", scalar)\n    print(\"Resulting Matrix:\")\n    for row in result:\n        print(row)\n```\n\nThis code defines a function `scalar_multiply` that takes a matrix and a scalar as input, multiplies each element of the matrix by the scalar, and returns the resulting matrix. The example usage demonstrates how to use this function with a sample matrix and scalar, and prints the original matrix, the scalar, and the resulting matrix."}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]): A 2x2 matrix.\n\n    Returns:\n    list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n\n    # First, we need to ensure the input matrix is a numpy array for easier manipulation\n    matrix = np.array(matrix)\n\n    # Check if the input matrix is a 2x2 matrix\n    if matrix.shape != (2, 2):\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Calculate the eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(matrix)\n\n    # Sort the eigenvalues in descending order (highest to lowest)\n    eigenvalues = np.sort(eigenvalues)[::-1]\n\n    # Return the sorted eigenvalues as a list\n    return eigenvalues.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2], [3, 4]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(\"Eigenvalues:\", eigenvalues)\n```\n\nIn this code:\n\n*   We define a function `calculate_eigenvalues` that takes a 2x2 matrix as input and returns a list of eigenvalues.\n*   We use the `numpy` library to handle matrix operations and calculate eigenvalues.\n*   We first convert the input matrix to a `numpy` array for easier manipulation.\n*   We check if the input matrix is indeed a 2x2 matrix and raise a `ValueError` if it's not.\n*   We calculate the eigenvalues using `np.linalg.eigvals`.\n*   We sort the eigenvalues in descending order using `np.sort` and slicing (`[::-1]`).\n*   Finally, we return the sorted eigenvalues as a list.\n\nThe example usage demonstrates how to call the `calculate_eigenvalues` function with a sample 2x2 matrix and print the resulting eigenvalues."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^-1 * A * S, \n    where T and S are invertible matrices.\n\n    Args:\n        A (list[list[int|float]]): The input matrix to be transformed.\n        T (list[list[int|float]]): The first invertible matrix.\n        S (list[list[int|float]]): The second invertible matrix.\n\n    Returns:\n        list[list[int|float]]: The transformed matrix if T and S are invertible, -1 otherwise.\n    \"\"\"\n\n    # Convert the input lists to numpy arrays for easier matrix operations\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n\n    # Check if T and S are invertible by checking their determinants\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n\n    # Calculate the inverse of T\n    T_inv = np.linalg.inv(T)\n\n    # Perform the transformation T^-1 * A * S\n    transformed_A = np.dot(np.dot(T_inv, A), S)\n\n    # Round the results to the nearest 4th decimal\n    transformed_A = np.round(transformed_A, 4)\n\n    # Convert the result back to a list and return it\n    return transformed_A.tolist()\n```\n\n### Example Usage\n```python\nA = [[1, 2], [3, 4]]\nT = [[5, 6], [7, 8]]\nS = [[9, 10], [11, 12]]\n\nresult = transform_matrix(A, T, S)\nprint(result)\n```\n\nThis code defines a function `transform_matrix` that takes three matrices `A`, `T`, and `S` as input, checks if `T` and `S` are invertible, and then performs the transformation `T^-1 * A * S` if they are. The result is rounded to the nearest 4th decimal and returned as a list. If `T` or `S` is not invertible, the function returns -1. The example usage demonstrates how to call this function with sample matrices."}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix.\n\n    Returns:\n    list[list[float]]: The inverse of the matrix if it exists, otherwise None.\n    \"\"\"\n\n    # Check if the input matrix is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Extract the elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the determinant of the matrix\n    determinant = a * d - b * c\n\n    # Check if the matrix is invertible (determinant is not zero)\n    if determinant == 0:\n        return None\n\n    # Calculate the inverse of the matrix\n    inverse = [[d / determinant, -b / determinant],\n               [-c / determinant, a / determinant]]\n\n    return inverse\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a 2x2 matrix\n    matrix = [[1, 2], [3, 4]]\n\n    # Calculate the inverse of the matrix\n    inverse = inverse_2x2(matrix)\n\n    # Print the result\n    if inverse is not None:\n        print(\"Inverse of the matrix:\")\n        for row in inverse:\n            print(row)\n    else:\n        print(\"The matrix is not invertible\")\n```\n\nThis function first checks if the input matrix is a 2x2 matrix. It then calculates the determinant of the matrix and checks if it is zero. If the determinant is zero, the matrix is not invertible, and the function returns `None`. Otherwise, it calculates the inverse of the matrix using the formula for the inverse of a 2x2 matrix and returns the result. The example usage demonstrates how to use the function to calculate the inverse of a 2x2 matrix."}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], \n              b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function multiplies two matrices.\n    \n    Args:\n    a (list[list[int | float]]): The first matrix.\n    b (list[list[int | float]]): The second matrix.\n    \n    Returns:\n    list[list[int | float]]: The product of the two matrices if they can be multiplied, -1 otherwise.\n    \"\"\"\n    \n    # Get the dimensions of the matrices\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform the matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\nmatrix1 = [[1, 2, 3], [4, 5, 6]]\nmatrix2 = [[7, 8], [9, 10], [11, 12]]\nprint(matrixmul(matrix1, matrix2))\n```\n\nIn this code:\n\n1.  We first check if the number of columns in the first matrix (`cols_a`) is equal to the number of rows in the second matrix (`rows_b`). If they are not equal, we return -1 because the matrices cannot be multiplied.\n2.  We then initialize a result matrix (`result`) with zeros, where the number of rows is equal to the number of rows in the first matrix (`rows_a`) and the number of columns is equal to the number of columns in the second matrix (`cols_b`).\n3.  We perform the matrix multiplication using nested loops. For each element in the result matrix, we calculate the dot product of the corresponding row in the first matrix and the corresponding column in the second matrix.\n4.  Finally, we return the result matrix.\n\nThis code assumes that the input matrices are valid (i.e., they are lists of lists, and all inner lists have the same length). It also assumes that the elements of the matrices are either integers or floats."}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: A covariance matrix as a list of lists.\n    \"\"\"\n    \n    # Convert the input list to a NumPy array for easier manipulation\n    vectors_array = np.array(vectors)\n    \n    # Calculate the mean of each feature\n    mean = np.mean(vectors_array, axis=1)\n    \n    # Subtract the mean from each feature to center the data\n    centered_data = vectors_array - mean[:, np.newaxis]\n    \n    # Calculate the covariance matrix using the formula: cov(X, Y) = E[(X - E(X))(Y - E(Y))]\n    covariance_matrix = np.cov(centered_data)\n    \n    # Convert the covariance matrix back to a list of lists and return it\n    return covariance_matrix.tolist()\n\n\n# Test cases\nimport unittest\n\nclass TestCalculateCovarianceMatrix(unittest.TestCase):\n    def test_calculate_covariance_matrix(self):\n        vectors = [[1, 2, 3], [4, 5, 6]]\n        result = calculate_covariance_matrix(vectors)\n        self.assertIsInstance(result, list)\n        self.assertIsInstance(result[0], list)\n        self.assertIsInstance(result[0][0], float)\n\n    def test_calculate_covariance_matrix_empty(self):\n        vectors = []\n        with self.assertRaises(ValueError):\n            calculate_covariance_matrix(vectors)\n\n    def test_calculate_covariance_matrix_single_feature(self):\n        vectors = [[1, 2, 3]]\n        result = calculate_covariance_matrix(vectors)\n        self.assertIsInstance(result, list)\n        self.assertIsInstance(result[0], list)\n        self.assertIsInstance(result[0][0], float)\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nIn this code:\n\n*   We define a function `calculate_covariance_matrix` that takes a list of lists `vectors` as input and returns the covariance matrix as a list of lists.\n*   We use the `numpy` library to convert the input list to a NumPy array and calculate the mean of each feature.\n*   We center the data by subtracting the mean from each feature.\n*   We calculate the covariance matrix using the `np.cov` function.\n*   We convert the covariance matrix back to a list of lists and return it.\n*   We provide test cases using the `unittest` framework to verify the correctness of the implementation. The test cases cover different scenarios, including an empty input list, a list with a single feature, and a list with multiple features."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n\n    Args:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A vector representing the constants of the linear equations.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x as a list, rounded to four decimal places.\n    \"\"\"\n\n    # Get the number of equations\n    num_equations = A.shape[0]\n\n    # Initialize the solution vector with zeros\n    x = np.zeros(num_equations)\n\n    # Iterate n times\n    for _ in range(n):\n        # Initialize a new solution vector\n        new_x = np.zeros(num_equations)\n\n        # Calculate the new solution vector using the Jacobi method\n        for i in range(num_equations):\n            # Calculate the sum of the terms\n            sum_terms = sum(A[i, j] * x[j] for j in range(num_equations) if j != i)\n\n            # Calculate the new value for x_i\n            new_x[i] = (b[i] - sum_terms) / A[i, i]\n\n        # Round the new solution vector to four decimal places\n        new_x = np.round(new_x, 4)\n\n        # Update the solution vector\n        x = new_x\n\n    # Return the approximate solution as a list\n    return x.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the coefficient matrix A\n    A = np.array([[5, 1, 2], [1, 4, 1], [2, 1, 5]])\n\n    # Define the constant vector b\n    b = np.array([8, 6, 9])\n\n    # Define the number of iterations\n    n = 10\n\n    # Solve the system of linear equations using the Jacobi method\n    x = solve_jacobi(A, b, n)\n\n    # Print the approximate solution\n    print(\"Approximate solution:\", x)\n```\n\nIn this code:\n\n*   We define a function `solve_jacobi` that takes the coefficient matrix `A`, the constant vector `b`, and the number of iterations `n` as input.\n*   We initialize the solution vector `x` with zeros and iterate `n` times using the Jacobi method to update the solution vector.\n*   In each iteration, we calculate the new value for each element of the solution vector using the formula `(b[i] - sum_terms) / A[i, i]`, where `sum_terms` is the sum of the terms `A[i, j] * x[j]` for `j != i`.\n*   We round the new solution vector to four decimal places using `np.round`.\n*   Finally, we return the approximate solution as a list using `tolist()`.\n*   In the example usage, we define a coefficient matrix `A`, a constant vector `b`, and the number of iterations `n`, and then call the `solve_jacobi` function to solve the system of linear equations. The approximate solution is printed to the console."}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the singular values, and the U, V matrices.\n    \"\"\"\n\n    # Initialize the matrices U, V, and the singular values\n    U = np.eye(2)\n    V = np.eye(2)\n    singular_values = np.zeros(2)\n\n    # Perform the Jacobian method to diagonalize the matrix A^T A\n    for _ in range(100):  # Perform 100 iterations for convergence\n        max_angle = 0\n        max_p = 0\n        max_q = 0\n\n        # Find the maximum off-diagonal element\n        for p in range(2):\n            for q in range(p + 1, 2):\n                if abs(A[p, q]) > max_angle:\n                    max_angle = abs(A[p, q])\n                    max_p = p\n                    max_q = q\n\n        # If the maximum off-diagonal element is close to zero, stop the iteration\n        if max_angle < 1e-6:\n            break\n\n        # Calculate the rotation angle\n        tau = (A[max_q, max_q] - A[max_p, max_p]) / (2 * A[max_p, max_q])\n        t = np.sign(tau) / (np.abs(tau) + np.sqrt(1 + tau ** 2))\n        c = 1 / np.sqrt(1 + t ** 2)\n        s = t * c\n\n        # Update the matrices U, V, and the matrix A\n        G = np.eye(2)\n        G[max_p, max_p] = c\n        G[max_p, max_q] = -s\n        G[max_q, max_p] = s\n        G[max_q, max_q] = c\n\n        U = np.dot(U, G)\n        A = np.dot(G.T, np.dot(A, G))\n\n    # Calculate the singular values\n    singular_values[0] = np.sqrt(A[0, 0] ** 2 + A[0, 1] ** 2)\n    singular_values[1] = np.sqrt(A[1, 0] ** 2 + A[1, 1] ** 2)\n\n    # Calculate the matrix V\n    V = np.dot(A, U)\n\n    # Normalize the matrix V\n    V[:, 0] = V[:, 0] / np.linalg.norm(V[:, 0])\n    V[:, 1] = V[:, 1] / np.linalg.norm(V[:, 1])\n\n    # Return the singular values and the matrices U, V\n    return (round(singular_values[0], 4), round(singular_values[1], 4)), U, V\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]])\n    singular_values, U, V = svd_2x2_singular_values(A)\n    print(\"Singular Values:\", singular_values)\n    print(\"Matrix U:\\n\", U)\n    print(\"Matrix V:\\n\", V)\n```\n\nThis code defines a function `svd_2x2_singular_values` that approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method. The function takes a 2x2 matrix `A` as input and returns a tuple containing the singular values and the matrices `U` and `V`. The example usage demonstrates how to use the function with a sample matrix `A`."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n    matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n\n    # Base case: If the matrix is not a 4x4 matrix, raise an error\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"The input matrix must be a 4x4 matrix\")\n\n    # Define a helper function to calculate the determinant of a 3x3 matrix\n    def determinant_3x3(matrix: list[list[int|float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix using Laplace's Expansion method.\n\n        Args:\n        matrix (list[list[int|float]]): A 3x3 matrix represented as a list of lists.\n\n        Returns:\n        float: The determinant of the matrix.\n        \"\"\"\n        # Base case: If the matrix is not a 3x3 matrix, raise an error\n        if len(matrix) != 3 or any(len(row) != 3 for row in matrix):\n            raise ValueError(\"The input matrix must be a 3x3 matrix\")\n\n        # Calculate the determinant using Laplace's Expansion\n        a, b, c = matrix[0][0], matrix[0][1], matrix[0][2]\n        d, e, f = matrix[1][0], matrix[1][1], matrix[1][2]\n        g, h, i = matrix[2][0], matrix[2][1], matrix[2][2]\n\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n    # Calculate the determinant of the 4x4 matrix using Laplace's Expansion\n    det = 0\n    for i in range(4):\n        # Create a 3x3 minor matrix\n        minor = [row[:i] + row[i+1:] for row in matrix[1:]]\n        # Calculate the determinant of the minor matrix\n        det_minor = determinant_3x3(minor)\n        # Update the determinant using Laplace's Expansion\n        det += ((-1) ** i) * matrix[0][i] * det_minor\n\n    return det\n\n# Example usage:\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\nprint(determinant_4x4(matrix))\n```\n\nThis code defines a recursive function `determinant_4x4` to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. The function first checks if the input matrix is a 4x4 matrix and raises an error if it's not. Then, it defines a helper function `determinant_3x3` to calculate the determinant of a 3x3 matrix. The `determinant_4x4` function uses Laplace's Expansion to calculate the determinant of the 4x4 matrix by iterating over the first row, creating a 3x3 minor matrix for each element, and calculating the determinant of the minor matrix using the `determinant_3x3` function. The final determinant is calculated by summing the products of the elements of the first row and the determinants of the corresponding minor matrices, with alternating signs."}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Args:\n    X (list[list[float]]): A 2D list representing the feature matrix.\n    y (list[float]): A list representing the target vector.\n\n    Returns:\n    list[float]: A list of coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n\n    # Convert the input lists to numpy arrays for easier matrix operations\n    X = np.array(X)\n    y = np.array(y)\n\n    # Add a column of ones to the feature matrix for the bias term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n    # Calculate the coefficients using the normal equation\n    coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n\n    # Round the coefficients to four decimal places\n    coefficients = np.round(coefficients, 4)\n\n    # Convert the coefficients to a list and return\n    return coefficients.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = [[1, 2], [3, 4], [5, 6]]\n    y = [2, 4, 5]\n    print(linear_regression_normal_equation(X, y))\n```\n\nIn this code:\n\n*   We first convert the input lists to numpy arrays for easier matrix operations.\n*   We add a column of ones to the feature matrix `X` to account for the bias term.\n*   We calculate the coefficients using the normal equation, which is `(X^T * X)^-1 * X^T * y`.\n*   We round the coefficients to four decimal places using `np.round`.\n*   Finally, we convert the coefficients to a list and return them.\n\nThe example usage demonstrates how to call the `linear_regression_normal_equation` function with sample data."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    list: Coefficients of the linear regression model.\n    \"\"\"\n\n    # Initialize coefficients (theta) with zeros\n    theta = np.zeros(X.shape[1])\n\n    # Perform gradient descent for the specified number of iterations\n    for _ in range(iterations):\n        # Calculate the predicted values\n        predictions = np.dot(X, theta)\n\n        # Calculate the error\n        error = predictions - y\n\n        # Calculate the gradient\n        gradient = np.dot(X.T, error) / len(y)\n\n        # Update the coefficients\n        theta = theta - alpha * gradient\n\n    # Round the coefficients to four decimal places\n    theta = np.round(theta, 4)\n\n    # Return the coefficients as a list\n    return theta.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some sample data\n    X = np.array([[1, 2], [1, 4], [1, 6], [1, 8], [1, 10]])\n    y = np.array([2, 4, 5, 4, 5])\n\n    # Perform linear regression using gradient descent\n    coefficients = linear_regression_gradient_descent(X, y, alpha=0.01, iterations=1000)\n\n    print(\"Coefficients:\", coefficients)\n```\n\nIn this code:\n\n*   We define a function `linear_regression_gradient_descent` that takes in the features `X`, target variable `y`, learning rate `alpha`, and the number of iterations.\n*   We initialize the coefficients `theta` with zeros.\n*   We perform gradient descent for the specified number of iterations, updating the coefficients at each iteration based on the calculated gradient and learning rate.\n*   We round the coefficients to four decimal places and return them as a list.\n*   In the example usage, we generate some sample data, perform linear regression using gradient descent, and print the resulting coefficients."}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n    tuple[list[list[float]], list[list[float]]]: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n\n    # Calculate the mean of each feature\n    mean = np.mean(data, axis=0)\n    \n    # Calculate the standard deviation of each feature\n    std_dev = np.std(data, axis=0)\n    \n    # Standardization: (X - mean) / std_dev\n    standardized_data = (data - mean) / std_dev\n    \n    # Min-max normalization: (X - min) / (max - min)\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    \n    # Round the results to the nearest 4th decimal\n    standardized_data = np.round(standardized_data, 4)\n    normalized_data = np.round(normalized_data, 4)\n    \n    # Convert the results to lists\n    standardized_data = standardized_data.tolist()\n    normalized_data = normalized_data.tolist()\n    \n    return standardized_data, normalized_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    standardized, normalized = feature_scaling(data)\n    print(\"Standardized Data:\")\n    for row in standardized:\n        print(row)\n    print(\"Normalized Data:\")\n    for row in normalized:\n        print(row)\n```\n\nIn this code:\n\n*   We first calculate the mean and standard deviation of each feature.\n*   We then perform standardization by subtracting the mean and dividing by the standard deviation for each feature.\n*   Next, we perform min-max normalization by subtracting the minimum value and dividing by the range of each feature.\n*   We round the results to the nearest 4th decimal using `np.round`.\n*   Finally, we convert the results to lists using the `tolist` method and return them.\n\nThe example usage demonstrates how to use the `feature_scaling` function with a sample dataset and print the standardized and normalized data."}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function implements the k-Means clustering algorithm to partition n points into k clusters.\n    \n    Parameters:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    k (int): An integer representing the number of clusters to form.\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates.\n    max_iterations (int): An integer representing the maximum number of iterations to perform.\n    \n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    \n    # Convert the points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    # Initialize the iteration counter\n    iteration = 0\n    \n    # Perform the k-Means clustering algorithm\n    while iteration < max_iterations:\n        # Assign each point to the closest centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update the centroids\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        # Update the centroids and iteration counter\n        centroids = new_centroids\n        iteration += 1\n    \n    # Round the centroids to the nearest fourth decimal and convert to list of tuples\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    \n    return final_centroids\n\n# Example usage:\npoints = [(1.0, 1.0), (1.0, 2.0), (2.0, 1.0), (2.0, 2.0), (10.0, 10.0), (10.0, 11.0), (11.0, 10.0), (11.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (10.0, 10.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis code implements the k-Means clustering algorithm to partition a set of points into k clusters. The function takes a list of points, the number of clusters to form, a list of initial centroid points, and the maximum number of iterations to perform. The function returns a list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n\nThe algorithm works by first assigning each point to the closest centroid, then updating the centroids as the mean of all points assigned to each centroid. This process is repeated until the centroids converge or the maximum number of iterations is reached.\n\nThe example usage demonstrates how to use the function with a set of points and initial centroids. The output will be the final centroids of the clusters, which can be used to represent the clusters in the data."}
{"task_id": 18, "completion_id": 0, "completion": "Here's a Python function that implements K-Fold Cross-Validation using NumPy. This function takes in the dataset `X` and `y`, the number of folds `k`, a boolean to shuffle the data, and a random seed for reproducibility.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    - X (np.ndarray): The feature dataset.\n    - y (np.ndarray): The target dataset.\n    - k (int, optional): The number of folds. Defaults to 5.\n    - shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    - random_seed (int, optional): The random seed for reproducibility. Defaults to None.\n\n    Returns:\n    - list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n\n    # Check if the number of folds is valid\n    if k <= 1:\n        raise ValueError(\"The number of folds must be greater than 1.\")\n\n    # Check if the dataset is empty\n    if len(X) == 0 or len(y) == 0:\n        raise ValueError(\"The dataset cannot be empty.\")\n\n    # Check if the lengths of X and y match\n    if len(X) != len(y):\n        raise ValueError(\"The lengths of X and y must match.\")\n\n    # Set the random seed for reproducibility\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Get the indices of the dataset\n    indices = np.arange(len(X))\n\n    # Shuffle the indices if required\n    if shuffle:\n        np.random.shuffle(indices)\n\n    # Calculate the size of each fold\n    fold_size = len(X) // k\n\n    # Initialize the list to store the train and test indices\n    train_test_indices = []\n\n    # Generate the train and test indices for each fold\n    for i in range(k):\n        # Calculate the start and end indices of the test set\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else len(X)\n\n        # Get the test indices\n        test_indices = indices[start:end]\n\n        # Get the train indices\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n\n        # Append the train and test indices to the list\n        train_test_indices.append((train_indices, test_indices))\n\n    return train_test_indices\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a sample dataset\n    X = np.random.rand(100, 10)\n    y = np.random.randint(0, 2, 100)\n\n    # Perform K-Fold Cross-Validation\n    k = 5\n    train_test_indices = k_fold_cross_validation(X, y, k)\n\n    # Print the train and test indices for each fold\n    for i, (train_indices, test_indices) in enumerate(train_test_indices):\n        print(f\"Fold {i+1}:\")\n        print(f\"Train indices: {train_indices}\")\n        print(f\"Test indices: {test_indices}\")\n        print()\n```\n\nIn this code:\n\n*   We define a function `k_fold_cross_validation` that takes in the dataset `X` and `y`, the number of folds `k`, a boolean to shuffle the data, and a random seed for reproducibility.\n*   We check if the number of folds is valid and if the dataset is empty.\n*   We set the random seed for reproducibility and shuffle the indices if required.\n*   We calculate the size of each fold and initialize a list to store the train and test indices.\n*   We generate the train and test indices for each fold by iterating over the range of the number of folds.\n*   We return the list of train and test indices for each fold.\n*   In the example usage, we generate a sample dataset, perform K-Fold Cross-Validation, and print the train and test indices for each fold."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    This function performs Principal Component Analysis (PCA) from scratch.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    k (int): The number of principal components to return.\n    \n    Returns:\n    list[list[float]]: A list of principal components (the eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n\n    # Standardize the dataset by subtracting the mean and dividing by the standard deviation for each feature\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data.T)\n\n    # Find the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # Sort the eigenvalues and eigenvectors in descending order of the eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top k eigenvectors (principal components)\n    principal_components = eigenvectors[:, :k]\n\n    # Round the results to the nearest 4th decimal\n    principal_components = np.round(principal_components, 4)\n\n    # Convert the principal components to a list of lists\n    principal_components = principal_components.tolist()\n\n    return principal_components\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nk = 2\nprint(pca(data, k))\n```\n\nThis code defines a function `pca` that takes a 2D NumPy array `data` and an integer `k` as input, and returns a list of principal components (the eigenvectors corresponding to the largest eigenvalues). The function standardizes the dataset, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the top `k` eigenvectors. The results are rounded to the nearest 4th decimal. The example usage demonstrates how to call the `pca` function with a sample dataset and print the resulting principal components."}
{"task_id": 20, "completion_id": 0, "completion": "Here's a Python function that implements the decision tree learning algorithm for classification using recursive binary splitting based on entropy and information gain.\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    This function implements the decision tree learning algorithm for classification.\n    \n    Args:\n    examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    attributes (list[str]): A list of attribute names.\n    target_attr (str): The target attribute name.\n    \n    Returns:\n    dict: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    # Base case: If all examples have the same target attribute value, return that value\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n\n    # If there are no more attributes to split on, return the most common target attribute value\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n\n    # Find the attribute with the highest information gain\n    best_attr = None\n    best_gain = 0\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n\n    # If no attribute has a positive information gain, return the most common target attribute value\n    if best_gain == 0:\n        return Counter(target_values).most_common(1)[0][0]\n\n    # Split the examples based on the best attribute\n    tree = {best_attr: {}}\n    for value in set(example[best_attr] for example in examples):\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n\n    return tree\n\n\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    This function calculates the information gain of an attribute.\n    \n    Args:\n    examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    attr (str): The attribute name.\n    target_attr (str): The target attribute name.\n    \n    Returns:\n    float: The information gain of the attribute.\n    \"\"\"\n\n    # Calculate the entropy of the target attribute\n    target_values = [example[target_attr] for example in examples]\n    target_entropy = entropy(target_values)\n\n    # Calculate the entropy of the target attribute after splitting on the given attribute\n    attr_values = set(example[attr] for example in examples)\n    attr_entropy = 0\n    for value in attr_values:\n        subset = [example for example in examples if example[attr] == value]\n        subset_target_values = [example[target_attr] for example in subset]\n        subset_entropy = entropy(subset_target_values)\n        attr_entropy += (len(subset) / len(examples)) * subset_entropy\n\n    # Calculate the information gain\n    gain = target_entropy - attr_entropy\n\n    return gain\n\n\ndef entropy(values: list) -> float:\n    \"\"\"\n    This function calculates the entropy of a list of values.\n    \n    Args:\n    values (list): A list of values.\n    \n    Returns:\n    float: The entropy of the list of values.\n    \"\"\"\n\n    # Calculate the frequency of each value\n    freqs = Counter(values)\n\n    # Calculate the entropy\n    entropy = 0\n    for freq in freqs.values():\n        prob = freq / len(values)\n        entropy -= prob * math.log2(prob)\n\n    return entropy\n\n\n# Example usage:\nexamples = [\n    {\"outlook\": \"sunny\", \"temperature\": \"hot\", \"humidity\": \"high\", \"wind\": \"weak\", \"play\": \"no\"},\n    {\"outlook\": \"sunny\", \"temperature\": \"hot\", \"humidity\": \"high\", \"wind\": \"strong\", \"play\": \"no\"},\n    {\"outlook\": \"overcast\", \"temperature\": \"hot\", \"humidity\": \"high\", \"wind\": \"weak\", \"play\": \"yes\"},\n    {\"outlook\": \"rain\", \"temperature\": \"mild\", \"humidity\": \"high\", \"wind\": \"weak\", \"play\": \"yes\"},\n    {\"outlook\": \"rain\", \"temperature\": \"cool\", \"humidity\": \"normal\", \"wind\": \"strong\", \"play\": \"yes\"},\n    {\"outlook\": \"rain\", \"temperature\": \"cool\", \"humidity\": \"normal\", \"wind\": \"strong\", \"play\": \"no\"},\n    {\"outlook\": \"overcast\", \"temperature\": \"cool\", \"humidity\": \"normal\", \"wind\": \"strong\", \"play\": \"yes\"},\n    {\"outlook\": \"sunny\", \"temperature\": \"mild\", \"humidity\": \"high\", \"wind\": \"weak\", \"play\": \"no\"},\n    {\"outlook\": \"sunny\", \"temperature\": \"cool\", \"humidity\": \"normal\", \"wind\": \"weak\", \"play\": \"yes\"},\n    {\"outlook\": \"rain\", \"temperature\": \"mild\", \"humidity\": \"normal\", \"wind\": \"weak\", \"play\": \"yes\"},\n    {\"outlook\": \"sunny\", \"temperature\": \"mild\", \"humidity\": \"normal\", \"wind\": \"strong\", \"play\": \"yes\"},\n    {\"outlook\": \"overcast\", \"temperature\": \"mild\", \"humidity\": \"high\", \"wind\": \"strong\", \"play\": \"yes\"},\n    {\"outlook\": \"overcast\", \"temperature\": \"hot\", \"humidity\": \"normal\", \"wind\": \"weak\", \"play\": \"yes\"},\n    {\"outlook\": \"rain\", \"temperature\": \"mild\", \"humidity\": \"high\", \"wind\": \"strong\", \"play\": \"no\"}\n]\n\nattributes = [\"outlook\", \"temperature\", \"humidity\", \"wind\"]\ntarget_attr = \"play\"\n\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```\n\nThis code defines a decision tree learning algorithm that uses recursive binary splitting based on entropy and information gain to build a decision tree. The `learn_decision_tree` function takes a list of examples, a list of attribute names, and a target attribute name as input, and returns a nested dictionary representing the decision tree. The `information_gain` function calculates the information gain of an attribute, and the `entropy` function calculates the entropy of a list of values. The example usage demonstrates how to use the `learn_decision_tree` function to build a decision tree from a list of examples."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch.\n\n    Parameters:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str): The choice of kernel, either 'linear' or 'RBF'. Default is 'linear'.\n    - lambda_val (float): The regularization parameter. Default is 0.01.\n    - iterations (int): The number of iterations. Default is 100.\n    - sigma (float): The kernel parameter for RBF kernel. Default is 1.0.\n\n    Returns:\n    - alpha (list): The model's alpha coefficients.\n    - bias (float): The model's bias.\n    \"\"\"\n\n    # Initialize alpha coefficients\n    alpha = np.zeros(len(data))\n\n    # Initialize bias\n    bias = 0.0\n\n    # Define the kernel function\n    def kernel_func(x, y):\n        if kernel == 'linear':\n            return np.dot(x, y)\n        elif kernel == 'RBF':\n            return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n\n    # Perform Pegasos algorithm\n    for _ in range(iterations):\n        for i in range(len(data)):\n            # Compute the margin\n            margin = np.sum([alpha[j] * labels[j] * kernel_func(data[j], data[i]) for j in range(len(data))]) + bias\n\n            # Update alpha and bias\n            if labels[i] * margin < 1:\n                alpha[i] = (1 - lambda_val) * alpha[i] + labels[i]\n                bias = (1 - lambda_val) * bias\n\n        # Project alpha onto the feasible set\n        alpha = np.clip(alpha, 0, 1 / lambda_val)\n\n    # Round the results to the nearest 4th decimal\n    alpha = np.round(alpha, 4)\n    bias = round(bias, 4)\n\n    return alpha.tolist(), bias\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some sample data\n    np.random.seed(0)\n    data = np.random.rand(10, 5)\n    labels = np.random.choice([-1, 1], 10)\n\n    # Train the model\n    alpha, bias = pegasos_kernel_svm(data, labels)\n\n    print(\"Alpha coefficients:\", alpha)\n    print(\"Bias:\", bias)\n```\n\nThis code implements the Pegasos algorithm for training a kernel SVM classifier. The `pegasos_kernel_svm` function takes in the dataset, labels, kernel type, regularization parameter, number of iterations, and kernel parameter as inputs. It returns the model's alpha coefficients and bias.\n\nThe kernel function is defined inside the `pegasos_kernel_svm` function, which computes the dot product of two vectors for the linear kernel and the RBF kernel.\n\nThe Pegasos algorithm is performed by iterating over the dataset and updating the alpha coefficients and bias. The alpha coefficients are projected onto the feasible set after each iteration.\n\nFinally, the results are rounded to the nearest 4th decimal and returned as a list and a float. The example usage demonstrates how to train the model on some sample data and print the alpha coefficients and bias."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    This function computes the output of the sigmoid activation function given an input value z.\n    \n    The sigmoid function is defined as 1 / (1 + exp(-z)), where exp is the exponential function.\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n    \n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    # Calculate the exponential of -z\n    exp_neg_z = math.exp(-z)\n    \n    # Calculate the sigmoid of z\n    sigmoid_z = 1 / (1 + exp_neg_z)\n    \n    # Return the result rounded to four decimal places\n    return round(sigmoid_z, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    z = 2.0\n    result = sigmoid(z)\n    print(f\"The sigmoid of {z} is {result}\")\n```\n\nThis code defines a function `sigmoid` that takes a float `z` as input and returns the sigmoid of `z` rounded to four decimal places. The sigmoid function is calculated using the formula `1 / (1 + exp(-z))`, where `exp` is the exponential function from the math library. The result is then rounded to four decimal places using the `round` function. The example usage demonstrates how to use the `sigmoid` function with an input value of 2.0."}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    This function computes the softmax activation for a given list of scores.\n    \n    Args:\n    scores (list[float]): A list of scores.\n    \n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    \n    # Calculate the maximum score to avoid overflow\n    max_score = max(scores)\n    \n    # Calculate the exponential of each score minus the maximum score\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    \n    # Calculate the sum of the exponential scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Calculate the softmax values\n    softmax_values = [score / sum_exp_scores for score in exp_scores]\n    \n    # Round the softmax values to four decimal places\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    \n    return rounded_softmax_values\n\n# Example usage:\nscores = [1.0, 2.0, 3.0, 4.0, 5.0]\nprint(softmax(scores))\n```\n\nThis function works by first calculating the exponential of each score minus the maximum score to avoid overflow. Then, it calculates the sum of these exponential scores. The softmax values are then calculated by dividing each exponential score by the sum of the exponential scores. Finally, the softmax values are rounded to four decimal places and returned as a list.\n\nNote: The softmax function is often used in machine learning models, particularly in the output layer of classification models, to convert the output into a probability distribution."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function.\n    \n    Args:\n    x (float): Input to the sigmoid function.\n    \n    Returns:\n    float: Output of the sigmoid function.\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): Associated true binary labels.\n    weights (list[float]): The neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n    \n    Returns:\n    tuple[list[float], float]: A tuple containing the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n    \"\"\"\n    \n    # Convert the input lists to numpy arrays for easier manipulation\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    weights_array = np.array(weights)\n    \n    # Calculate the weighted sum of the features and the bias\n    weighted_sum = np.dot(features_array, weights_array) + bias\n    \n    # Apply the sigmoid activation function to the weighted sum\n    predicted_probabilities = np.array([sigmoid(x) for x in weighted_sum])\n    \n    # Round the predicted probabilities to four decimal places\n    predicted_probabilities = np.round(predicted_probabilities, 4)\n    \n    # Calculate the mean squared error between the predicted probabilities and the true labels\n    mean_squared_error = np.mean((predicted_probabilities - labels_array) ** 2)\n    \n    # Round the mean squared error to four decimal places\n    mean_squared_error = round(mean_squared_error, 4)\n    \n    # Return the predicted probabilities and the mean squared error\n    return predicted_probabilities.tolist(), mean_squared_error\n\n# Example usage:\nfeatures = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\nlabels = [0, 1, 1]\nweights = [0.5, 0.5]\nbias = 0.0\n\npredicted_probabilities, mean_squared_error = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probabilities)\nprint(\"Mean Squared Error:\", mean_squared_error)\n```\n\nIn this code:\n\n*   We define a `sigmoid` function to apply the sigmoid activation function to the weighted sum of the features and the bias.\n*   In the `single_neuron_model` function, we convert the input lists to numpy arrays for easier manipulation.\n*   We calculate the weighted sum of the features and the bias using the dot product of the features and weights arrays, and then add the bias.\n*   We apply the sigmoid activation function to the weighted sum using a list comprehension.\n*   We round the predicted probabilities to four decimal places using `np.round`.\n*   We calculate the mean squared error between the predicted probabilities and the true labels using `np.mean` and round it to four decimal places.\n*   Finally, we return the predicted probabilities and the mean squared error.\n\nThe example usage demonstrates how to call the `single_neuron_model` function with sample features, labels, weights, and bias, and prints the predicted probabilities and the mean squared error."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of x.\n\n    Args:\n    x (np.ndarray): Input array.\n\n    Returns:\n    np.ndarray: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    features (np.ndarray): A 2D array of feature vectors.\n    labels (np.ndarray): A 1D array of true binary labels.\n    initial_weights (np.ndarray): A 1D array of initial weights.\n    initial_bias (float): The initial bias.\n    learning_rate (float): The learning rate for gradient descent.\n    epochs (int): The number of epochs.\n\n    Returns:\n    np.ndarray: The updated weights.\n    float: The updated bias.\n    list[float]: A list of MSE values for each epoch.\n    \"\"\"\n\n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n\n    # Initialize list to store MSE values\n    mse_values = []\n\n    # Iterate over epochs\n    for _ in range(epochs):\n        # Compute predictions\n        predictions = sigmoid(np.dot(features, weights) + bias)\n\n        # Compute MSE\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n\n        # Compute gradients\n        d_predictions = 2 * (predictions - labels)\n        d_sigmoid = predictions * (1 - predictions)\n        d_weights = np.dot(features.T, d_predictions * d_sigmoid)\n        d_bias = np.sum(d_predictions * d_sigmoid)\n\n        # Update weights and bias\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    return weights, bias, np.array(mse_values).tolist()\n```\n\nThis function first defines a helper function `sigmoid` to compute the sigmoid of an array. Then, it initializes the weights and bias, and creates a list to store the MSE values for each epoch. It iterates over the specified number of epochs, computing the predictions, MSE, and gradients, and updating the weights and bias using gradient descent. Finally, it returns the updated weights, bias, and the list of MSE values, with all results rounded to the nearest 4th decimal."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initialize a Value object.\n\n        Args:\n        - data (float): The value of the object.\n        - _children (tuple, optional): The child nodes of the object. Defaults to ().\n        - _op (str, optional): The operation that created the object. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        \"\"\"\n        Add two Value objects.\n\n        Args:\n        - other (Value): The object to add to the current object.\n\n        Returns:\n        - Value: A new Value object representing the sum of the two objects.\n        \"\"\"\n        if isinstance(other, Value):\n            out = Value(self.data + other.data, (self, other), '+')\n        else:\n            out = Value(self.data + other, (self,), '+')\n        \n        def _backward():\n            self.grad += out.grad\n            if isinstance(other, Value):\n                other.grad += out.grad\n        \n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        \"\"\"\n        Multiply two Value objects.\n\n        Args:\n        - other (Value): The object to multiply with the current object.\n\n        Returns:\n        - Value: A new Value object representing the product of the two objects.\n        \"\"\"\n        if isinstance(other, Value):\n            out = Value(self.data * other.data, (self, other), '*')\n        else:\n            out = Value(self.data * other, (self,), '*')\n        \n        def _backward():\n            self.grad += out.grad * other.data if not isinstance(other, Value) else out.grad * other.data\n            if isinstance(other, Value):\n                other.grad += out.grad * self.data\n        \n        out._backward = _backward\n        return out\n\n    def relu(self):\n        \"\"\"\n        Apply the ReLU activation function to the current object.\n\n        Returns:\n        - Value: A new Value object representing the ReLU of the current object.\n        \"\"\"\n        out = Value(max(0, self.data), (self,), 'relu')\n        \n        def _backward():\n            self.grad += out.grad if self.data >= 0 else 0\n        \n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \"\"\"\n        Compute the gradients of the current object and its children.\n        \"\"\"\n        # Topological sorting to avoid infinite loops\n        visited = set()\n        def _topological_sort(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    _topological_sort(child)\n                node._backward()\n        \n        _topological_sort(self)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create two Value objects\n    a = Value(2)\n    b = Value(3)\n\n    # Perform operations\n    c = a + b\n    d = c * a\n    e = d.relu()\n\n    # Set the gradient of the output\n    e.grad = 1.0\n\n    # Compute the gradients\n    e.backward()\n\n    # Print the gradients\n    print(\"Gradient of a:\", a.grad)\n    print(\"Gradient of b:\", b.grad)\n```\n\nThis implementation provides the basic autograd operations: addition, multiplication, and ReLU activation. The `Value` class handles scalar values and correctly computes gradients for these operations through automatic differentiation. The example usage demonstrates how to create `Value` objects, perform operations, and compute gradients."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    This function calculates the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list[list[int]]): The basis vectors in basis B.\n    C (list[list[int]]): The basis vectors in basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C.\n    \"\"\"\n    \n    # Convert the input lists to numpy arrays for easier matrix operations\n    B = np.array(B)\n    C = np.array(C)\n    \n    # Calculate the inverse of basis B\n    B_inv = np.linalg.inv(B)\n    \n    # Calculate the transformation matrix P\n    P = np.dot(C, B_inv)\n    \n    # Round the results to the nearest 4th decimal\n    P = np.round(P, 4)\n    \n    # Reshape the result to a list using numpy's tolist() method\n    P = P.tolist()\n    \n    return P\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]  # Another basis\nprint(transform_basis(B, C))\n```\n\nIn this code:\n\n*   We first import the `numpy` library for efficient numerical computations.\n*   The `transform_basis` function takes two lists of basis vectors `B` and `C` as input and returns the transformation matrix `P` from basis `B` to `C`.\n*   We convert the input lists to `numpy` arrays for easier matrix operations.\n*   We calculate the inverse of basis `B` using `np.linalg.inv(B)`.\n*   Then, we compute the transformation matrix `P` by multiplying `C` with the inverse of `B` using `np.dot(C, B_inv)`.\n*   We round the results to the nearest 4th decimal using `np.round(P, 4)`.\n*   Finally, we reshape the result to a list using `numpy`'s `tolist()` method and return it.\n\nThe example usage demonstrates how to use the `transform_basis` function with two different bases `B` and `C`."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V.\n    \"\"\"\n\n    # Compute A * A^T\n    A_AT = np.dot(A, A.T)\n\n    # Compute the eigenvalues and eigenvectors of A * A^T\n    eigenvalues, eigenvectors = np.linalg.eig(A_AT)\n\n    # Sort the eigenvalues and eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Compute the singular values\n    singular_values = np.sqrt(eigenvalues)\n\n    # Compute the matrix U\n    U = eigenvectors\n\n    # Compute the matrix S\n    S = np.diag(singular_values)\n\n    # Compute A^T * A\n    AT_A = np.dot(A.T, A)\n\n    # Compute the eigenvalues and eigenvectors of A^T * A\n    eigenvalues, eigenvectors = np.linalg.eig(AT_A)\n\n    # Sort the eigenvalues and eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Compute the matrix V\n    V = eigenvectors\n\n    # Return the matrices U, S, and V\n    return [np.round(U, 4).tolist()], [np.round(S, 4).tolist()], [np.round(V, 4).tolist()]\n\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"Matrix U:\")\nprint(U)\nprint(\"Matrix S:\")\nprint(S)\nprint(\"Matrix V:\")\nprint(V)\n```\n\nIn this code, we first compute `A * A^T` and its eigenvalues and eigenvectors. The eigenvalues of `A * A^T` are the squares of the singular values of `A`. We then compute `A^T * A` and its eigenvalues and eigenvectors to get the matrix `V`. The matrix `U` is composed of the eigenvectors of `A * A^T`, and the matrix `S` is a diagonal matrix containing the singular values of `A`. The results are rounded to the nearest 4th decimal and returned as lists."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, \n    while maintaining the corresponding order between them.\n\n    Args:\n        X (numpy array): The feature array.\n        y (numpy array): The target array.\n        seed (int, optional): The seed for the random shuffle. Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y arrays.\n    \"\"\"\n    # Set the seed for reproducibility if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Combine the X and y arrays into a single array\n    combined = np.column_stack((X, y))\n    \n    # Shuffle the combined array\n    np.random.shuffle(combined)\n    \n    # Split the combined array back into X and y\n    X_shuffled, y_shuffled = combined[:, :X.shape[1]], combined[:, X.shape[1]:]\n    \n    # Return the shuffled arrays as a list after reshaping using tolist()\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    print(shuffle_data(X, y, seed=42))\n```\n\nIn this code:\n\n*   We define a function `shuffle_data` that takes in two numpy arrays `X` and `y`, and an optional `seed` parameter.\n*   If a seed is provided, we set it using `np.random.seed(seed)` to ensure reproducibility.\n*   We combine the `X` and `y` arrays into a single array using `np.column_stack`.\n*   We shuffle the combined array using `np.random.shuffle`.\n*   We split the combined array back into `X` and `y` using array slicing.\n*   Finally, we return the shuffled `X` and `y` arrays as a list after reshaping using the `tolist()` method.\n\nThe example usage demonstrates how to call the `shuffle_data` function with sample `X` and `y` arrays and a seed value."}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterable function that samples in a numpy array X and an optional numpy array y.\n    \n    Args:\n    X (numpy array): The input data.\n    y (numpy array, optional): The target data. Defaults to None.\n    batch_size (int, optional): The size of each batch. Defaults to 64.\n    \n    Yields:\n    list: Batches of X or (X, y) pairs.\n    \"\"\"\n    \n    # Calculate the number of batches\n    num_batches = int(np.ceil(len(X) / batch_size))\n    \n    # Iterate over the batches\n    for i in range(num_batches):\n        # Calculate the start and end indices for the current batch\n        start = i * batch_size\n        end = min((i + 1) * batch_size, len(X))\n        \n        # Slice the current batch from X\n        X_batch = X[start:end]\n        \n        # If y is provided, slice the current batch from y\n        if y is not None:\n            y_batch = y[start:end]\n            # Yield the batch as a tuple of (X, y) pairs\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            # Yield the batch of X only\n            yield X_batch.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample data\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100, 1)\n    \n    # Create a batch iterator with y\n    batch_iter_with_y = batch_iterator(X, y, batch_size=10)\n    \n    # Create a batch iterator without y\n    batch_iter_without_y = batch_iterator(X, batch_size=10)\n    \n    # Print the batches\n    print(\"Batches with y:\")\n    for batch in batch_iter_with_y:\n        print(batch)\n        \n    print(\"\\nBatches without y:\")\n    for batch in batch_iter_without_y:\n        print(batch)\n```\n\nIn this code:\n\n*   We define a function `batch_iterator` that takes in a numpy array `X`, an optional numpy array `y`, and a batch size.\n*   We calculate the number of batches required to cover the entire dataset.\n*   We iterate over the batches, slicing the current batch from `X` and `y` (if provided).\n*   We yield the batch as a tuple of `(X, y)` pairs if `y` is provided; otherwise, we yield the batch of `X` only.\n*   In the example usage, we create sample data, create batch iterators with and without `y`, and print the batches."}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n        X (numpy array): Dataset to be divided.\n        feature_i (int): Index of the feature to divide on.\n        threshold (float): Threshold value to divide on.\n\n    Returns:\n        list: Two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n    \"\"\"\n\n    # Create a mask to filter samples that meet the condition\n    mask = X[:, feature_i] >= threshold\n\n    # Create subsets of the dataset based on the mask\n    subset_meet_condition = X[mask]\n    subset_dont_meet_condition = X[~mask]\n\n    # Reshape and convert subsets to lists using numpy's tolist() method\n    subset_meet_condition = subset_meet_condition.tolist()\n    subset_dont_meet_condition = subset_dont_meet_condition.tolist()\n\n    # Return the two subsets\n    return [subset_meet_condition, subset_dont_meet_condition]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n    # Divide the dataset based on the first feature and a threshold of 5\n    subsets = divide_on_feature(X, 0, 5)\n\n    # Print the subsets\n    print(\"Subset that meets the condition:\")\n    print(subsets[0])\n    print(\"Subset that does not meet the condition:\")\n    print(subsets[1])\n```\n\nIn this code:\n\n1.  We define a function `divide_on_feature` that takes in the dataset `X`, the index of the feature to divide on `feature_i`, and the threshold value `threshold`.\n2.  We create a mask to filter samples that meet the condition using `X[:, feature_i] >= threshold`.\n3.  We create subsets of the dataset based on the mask using `X[mask]` and `X[~mask]`.\n4.  We reshape and convert subsets to lists using numpy's `tolist()` method.\n5.  We return the two subsets as a list.\n6.  In the example usage, we create a sample dataset `X` and divide it based on the first feature and a threshold of 5.\n7.  We print the subsets that meet and do not meet the condition."}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n    list: A new 2D numpy array with polynomial features up to the specified degree.\n    \"\"\"\n    \n    # Initialize the result array with the original features\n    result = [X]\n    \n    # Generate polynomial features up to the specified degree\n    for d in range(2, degree + 1):\n        # Generate all possible combinations of features with replacement\n        combos = list(combinations_with_replacement(range(X.shape[1]), d))\n        \n        # Calculate the polynomial features for each combination\n        for combo in combos:\n            poly_feature = np.prod(X[:, combo], axis=1, keepdims=True)\n            result.append(poly_feature)\n    \n    # Stack the result arrays horizontally\n    result = np.hstack(result)\n    \n    # Return the result as a list\n    return result.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis function generates polynomial features up to the specified degree by calculating all possible combinations of the original features with replacement, and then computing the product of each combination. The result is a new 2D numpy array with the polynomial features, which is returned as a list.\n\nNote that the `combinations_with_replacement` function from the `itertools` module is used to generate all possible combinations of features with replacement. This is more efficient than using nested loops to generate the combinations.\n\nAlso, the `np.prod` function is used to calculate the product of each combination of features, and the `np.hstack` function is used to stack the result arrays horizontally.\n\nFinally, the `tolist` method is used to convert the result array to a list, as required by the output constraints."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Args:\n    - X (2D numpy array): The feature dataset.\n    - y (1D numpy array): The target dataset.\n    - n_subsets (int): The number of subsets to generate.\n    - replacements (bool, optional): Whether to create subsets with replacements. Defaults to True.\n    - seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    - list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Get the number of samples in the dataset\n    n_samples = X.shape[0]\n\n    # Initialize an empty list to store the subsets\n    subsets = []\n\n    # Generate n_subsets random subsets\n    for _ in range(n_subsets):\n        # If replacements is True, create a subset with replacements\n        if replacements:\n            # Generate a random sample of indices with replacements\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        # If replacements is False, create a subset without replacements\n        else:\n            # Generate a random permutation of indices\n            indices = np.random.permutation(n_samples)\n\n        # Create the subset\n        X_subset = X[indices]\n        y_subset = y[indices]\n\n        # Append the subset to the list\n        subsets.append((X_subset, y_subset))\n\n    # Return the list of subsets\n    return [subset for subset in subsets]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate a random dataset\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100)\n\n    # Generate 5 random subsets of the dataset\n    subsets = get_random_subsets(X, y, 5, replacements=False)\n\n    # Print the subsets\n    for i, subset in enumerate(subsets):\n        X_subset, y_subset = subset\n        print(f\"Subset {i+1}:\")\n        print(\"X_subset:\")\n        print(X_subset.tolist())\n        print(\"y_subset:\")\n        print(y_subset.tolist())\n        print()\n```\n\nIn this code:\n\n*   We define a function `get_random_subsets` that takes in a 2D numpy array `X`, a 1D numpy array `y`, an integer `n_subsets`, a boolean `replacements`, and an optional integer `seed`.\n*   We set the random seed for reproducibility using `np.random.seed(seed)`.\n*   We generate `n_subsets` random subsets of the dataset using a loop.\n*   Inside the loop, we generate a random sample of indices with or without replacements based on the value of `replacements`.\n*   We create the subset by indexing into `X` and `y` using the generated indices.\n*   We append the subset to the list of subsets.\n*   Finally, we return the list of subsets.\n\nThe example usage demonstrates how to generate a random dataset, generate 5 random subsets of the dataset, and print the subsets. The subsets are printed as lists using the `tolist()` method."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    x (1D numpy array): Input array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    list: One-hot encoded array as a list.\n    \"\"\"\n    \n    # If n_col is not provided, determine it from the input array\n    if n_col is None:\n        # The number of columns is equal to the maximum value in the array plus one\n        n_col = np.max(x) + 1\n    \n    # Initialize the one-hot encoded array with zeros\n    encoded = np.zeros((len(x), n_col))\n    \n    # Set the corresponding column to 1 for each value in the input array\n    for i, val in enumerate(x):\n        encoded[i, val] = 1\n    \n    # Return the one-hot encoded array as a list\n    return encoded.tolist()\n\n# Example usage:\nx = np.array([0, 1, 2, 0, 1, 2])\nprint(to_categorical(x))\n```\n\nIn this code:\n\n*   We define a function `to_categorical` that takes in a 1D numpy array `x` and an optional integer `n_col`.\n*   If `n_col` is not provided, we determine it from the input array by finding the maximum value and adding one.\n*   We initialize the one-hot encoded array `encoded` with zeros, where the number of rows is equal to the length of the input array and the number of columns is `n_col`.\n*   We iterate over the input array and set the corresponding column to 1 for each value in the input array.\n*   Finally, we return the one-hot encoded array as a list using numpy's `tolist()` method.\n\nYou can use this function to perform one-hot encoding of nominal values in your data. The example usage demonstrates how to use the function with a sample input array."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    This function converts a 1D numpy array into a diagonal matrix.\n\n    Args:\n        x (numpy array): A 1D numpy array.\n\n    Returns:\n        list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Check if the input is a numpy array\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n\n    # Check if the input array is 1D\n    if len(x.shape) != 1:\n        raise ValueError(\"Input array must be 1D\")\n\n    # Create a diagonal matrix from the input array\n    diagonal_matrix = np.diag(x)\n\n    # Reshape the diagonal matrix into a list\n    result = diagonal_matrix.tolist()\n\n    return result\n\n# Example usage:\nx = np.array([1, 2, 3])\nprint(make_diagonal(x))\n```\n\nThis function first checks if the input is a numpy array and if it's 1D. It then creates a diagonal matrix from the input array using `np.diag()`. Finally, it reshapes the diagonal matrix into a list using the `tolist()` method and returns the result.\n\nWhen you run this function with the example input `x = np.array([1, 2, 3])`, it will output:\n```python\n[[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (1D numpy array): The true labels.\n    y_pred (1D numpy array): The predicted labels.\n\n    Returns:\n    float: The accuracy score.\n    \"\"\"\n    # Check if the input arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n\n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n\n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n\n    # Calculate the accuracy score\n    accuracy = correct_predictions / total_predictions\n\n    return accuracy\n\n# Example usage:\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 6])\nprint(accuracy_score(y_true, y_pred))  # Output: 0.8\n```\n\nThis function calculates the accuracy score by comparing the true labels (`y_true`) with the predicted labels (`y_pred`). It first checks if the input arrays have the same shape, and if not, raises a `ValueError`. Then, it calculates the number of correct predictions by summing the number of elements where `y_true` equals `y_pred`. The total number of predictions is the length of `y_true`. Finally, it calculates the accuracy score by dividing the number of correct predictions by the total number of predictions. The function returns the accuracy score as a float."}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the function calculates the correlation matrix of X with itself. Defaults to None.\n\n    Returns:\n    list: The correlation matrix as a 2D list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # If Y is not provided, set Y to X\n    if Y is None:\n        Y = X\n\n    # Calculate the correlation matrix using numpy's corrcoef function\n    correlation_matrix = np.corrcoef(X, Y)\n\n    # Round the correlation matrix to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n\n    # Reshape the correlation matrix to a list using numpy's tolist method\n    correlation_matrix = correlation_matrix.tolist()\n\n    return correlation_matrix\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n\n    # Calculate the correlation matrix\n    correlation_matrix = calculate_correlation_matrix(X, Y)\n    print(\"Correlation Matrix:\")\n    for row in correlation_matrix:\n        print(row)\n\n    # Calculate the correlation matrix of X with itself\n    correlation_matrix = calculate_correlation_matrix(X)\n    print(\"\\nCorrelation Matrix of X with itself:\")\n    for row in correlation_matrix:\n        print(row)\n```\n\nThis code defines a function `calculate_correlation_matrix` that calculates the correlation matrix for a given dataset `X` and an optional dataset `Y`. If `Y` is not provided, the function calculates the correlation matrix of `X` with itself. The function returns the correlation matrix as a 2D list, rounded to the nearest 4th decimal.\n\nThe example usage demonstrates how to use the function with sample datasets `X` and `Y`, and how to calculate the correlation matrix of `X` with itself. The output is printed to the console."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    This function implements the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (2D numpy array): The dataset of shape (n_samples, n_features).\n    y (1D numpy array): The labels of shape (n_samples,).\n    n_clf (int): The number of classifiers.\n\n    Returns:\n    list: A list of classifiers with their parameters.\n    \"\"\"\n\n    # Initialize sample weights\n    n_samples = X.shape[0]\n    sample_weights = np.full(n_samples, 1/n_samples)\n\n    # Initialize list to store classifiers\n    classifiers = []\n\n    # Loop through each classifier\n    for _ in range(n_clf):\n        # Initialize minimum error and best threshold\n        min_error = float('inf')\n        best_threshold = None\n        best_feature = None\n        best_prediction = None\n\n        # Loop through each feature\n        for feature in range(X.shape[1]):\n            # Get unique values for the current feature\n            unique_values = np.unique(X[:, feature])\n\n            # Loop through each unique value\n            for threshold in unique_values:\n                # Predict labels based on the current threshold\n                predictions = np.where(X[:, feature] < threshold, -1, 1)\n\n                # Calculate the error\n                error = np.sum(sample_weights[y != predictions])\n\n                # Check if the error is less than the minimum error\n                if error < min_error:\n                    min_error = error\n                    best_threshold = threshold\n                    best_feature = feature\n                    best_prediction = predictions\n\n        # Update sample weights\n        alpha = 0.5 * np.log((1 - min_error) / max(min_error, 1e-))\n        sample_weights *= np.exp(-alpha * y * best_prediction)\n        sample_weights /= np.sum(sample_weights)\n\n        # Append the classifier to the list\n        classifiers.append({\n            'feature': best_feature,\n            'threshold': round(best_threshold, 4),\n            'alpha': round(alpha, 4)\n        })\n\n    return classifiers\n\n# Example usage:\nX = np.array([[, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 1, -1, -1])\nn_clf =print(adaboost_fit(X, y, n_clf))\n```\n\nThis code implements the AdaBoost fit method, which is a popular boosting algorithm used for classification problems. The function `adaboost_fit` takes in a 2D numpy array `X` representing the dataset, a 1D numpy array `y` representing the labels, and an integer `n_clf` representing the number of classifiers. It initializes sample weights, finds the best thresholds for each feature, calculates the error, updates weights, and returns a list of classifiers with their parameters.\n\nThe code first initializes the sample weights, which are used to determine the importance of each sample in the dataset. It then loops through each classifier, finding the best threshold for each feature and calculating the error. The sample weights are updated based on the error, and the classifier is appended to the list.\n\nThe function returns a list of classifiers, where each classifier is represented as a dictionary containing the feature, threshold, and alpha value. The alpha value represents the importance of the classifier in the ensemble.\n\nThe example usage demonstrates how to use the `adaboost_fit` function with a sample dataset and labels. The output is a list of classifiers with their parameters, which can be used for classification tasks."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    This function computes the log-softmax of a given 1D numpy array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list of log-softmax values.\n    \"\"\"\n    \n    # Convert the input list to a numpy array\n    scores = np.array(scores)\n    \n    # Compute the softmax of the scores\n    # To avoid numerical instability, we subtract the maximum score from all scores\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)\n    softmax = exp_scores / np.sum(exp_scores)\n    \n    # Compute the log-softmax\n    log_softmax = np.log(softmax)\n    \n    # Round the results to the nearest 4th decimal and return as a list\n    return np.round(log_softmax, 4).tolist()\n\n# Example usage:\nscores = [1.0, 2.0, 3.0, 4.0, 5.0]\nprint(log_softmax(scores))\n```\n\nThis function first converts the input list to a numpy array. Then, it computes the softmax of the scores by subtracting the maximum score from all scores to avoid numerical instability. The log-softmax is then computed by taking the logarithm of the softmax values. Finally, the results are rounded to the nearest 4th decimal and returned as a list.\n\nNote that the `np.log` function is used to compute the natural logarithm of the softmax values. If you want to use a different base for the logarithm, you can use the `np.log` function with the `base` argument, for example, `np.log(softmax) / np.log(base)`."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, W_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, w0_grad)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n\n\nclass Optimizer:\n    def update(self, param, grad):\n        return param - 0.01 * grad\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a Dense layer with 64 units and input shape (784,)\n    layer = Dense(64, input_shape=(784,))\n\n    # Initialize the layer with an optimizer\n    optimizer = Optimizer()\n    layer.initialize(optimizer)\n\n    # Create a random input\n    X = np.random.rand(10, 784)\n\n    # Perform a forward pass\n    output = layer.forward_pass(X, training=True)\n    print(\"Output shape:\", layer.output_shape())\n    print(\"Output:\\n\", np.round(output, 4).tolist())\n\n    # Perform a backward pass\n    accum_grad = np.random.rand(10, 64)\n    grad = layer.backward_pass(accum_grad)\n    print(\"Gradient shape:\", grad.shape)\n    print(\"Gradient:\\n\", np.round(grad, 4).tolist())\n\n    # Print the number of parameters\n    print(\"Number of parameters:\", layer.parameters())\n```\n\nThis code implements the `Dense` class, which represents a fully connected neural network layer. It extends the `Layer` class and implements the required methods:\n\n*   `__init__`: Initializes the layer with a specified number of neurons (`n_units`) and an optional input shape (`input_shape`).\n*   `initialize`: Initializes the weights (`W`) and biases (`w0`) using a uniform distribution and sets up optimizers for `W` and `w0`.\n*   `parameters`: Returns the total number of trainable parameters in the layer.\n*   `forward_pass`: Computes the output of the layer by performing a dot product between the input (`X`) and the weight matrix (`W`), and then adding the bias (`w0`).\n*   `backward_pass`: Calculates and returns the gradient with respect to the input, and updates the weights and biases using the optimizer's update rule if the layer is trainable.\n*   `output_shape`: Returns the shape of the output produced by the forward pass.\n\nThe code also includes an example usage of the `Dense` class, demonstrating how to create a layer, initialize it with an optimizer, perform a forward pass, and perform a backward pass. The output shape, output, gradient shape, gradient, and number of parameters are printed to verify the correctness of the implementation."}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    This function performs a 2D convolution operation on the input matrix using the specified kernel, padding, and stride.\n\n    Args:\n    input_matrix (np.ndarray): The input matrix to be convolved.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The amount of padding to be applied to the input matrix.\n    stride (int): The stride of the convolution operation.\n\n    Returns:\n    list: A list of convolved values, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Get the dimensions of the input matrix and the kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n\n    # Calculate the output height and width\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n\n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n\n    # Pad the input matrix\n    padded_input_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            # Calculate the current window\n            window = padded_input_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n\n            # Perform the convolution operation for the current window\n            output_matrix[i, j] = np.sum(window * kernel)\n\n    # Round the output to the nearest 4th decimal and return as a list\n    return np.round(output_matrix, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    kernel = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]])\n    padding = 1\n    stride = 1\n\n    result = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(result)\n```\n\nIn this code:\n\n*   We define a function `simple_conv2d` that takes an input matrix, a kernel, padding, and stride as arguments.\n*   We calculate the output height and width based on the input dimensions, kernel size, padding, and stride.\n*   We pad the input matrix with zeros according to the specified padding.\n*   We perform the convolution operation by sliding the kernel over the padded input matrix, calculating the dot product at each position, and storing the results in the output matrix.\n*   Finally, we round the output to the nearest 4th decimal and return it as a list.\n\nThe example usage demonstrates how to call the `simple_conv2d` function with a sample input matrix, kernel, padding, and stride, and prints the resulting convolved values."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    This function implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Args:\n        z (float): The input value to be processed by the ReLU function.\n    \n    Returns:\n        float: The value after applying the ReLU function.\n    \"\"\"\n    # If the input value is greater than 0, return the input value\n    if z > 0:\n        return z\n    # Otherwise, return 0\n    else:\n        return 0\n\n# Example usage:\nprint(relu(5.0))  # Output: 5.0\nprint(relu(-3.0))  # Output: 0\nprint(relu(0.0))   # Output: 0\n```\n\nAlternatively, you can implement the ReLU function in a more concise way using a conditional expression:\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    This function implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Args:\n        z (float): The input value to be processed by the ReLU function.\n    \n    Returns:\n        float: The value after applying the ReLU function.\n    \"\"\"\n    # Return the input value if it's greater than 0, otherwise return 0\n    return z if z > 0 else 0\n\n# Example usage:\nprint(relu(5.0))  # Output: 5.0\nprint(relu(-3.0))  # Output: 0\nprint(relu(0.0))   # Output: 0\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    This function calculates the Ridge Regression loss function.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the feature matrix.\n    w (np.ndarray): A 1D numpy array representing the coefficients.\n    y_true (np.ndarray): A 1D numpy array representing the true labels.\n    alpha (float): A float representing the regularization parameter.\n\n    Returns:\n    float: The Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n    \"\"\"\n\n    # Calculate the predicted values using the given coefficients and feature matrix\n    y_pred = np.dot(X, w)\n\n    # Calculate the Mean Squared Error (MSE) between the predicted and true labels\n    mse = np.mean((y_pred - y_true) ** 2)\n\n    # Calculate the regularization term, which is the L2 norm of the coefficients multiplied by the regularization parameter\n    reg_term = alpha * np.sum(w ** 2)\n\n    # Calculate the Ridge loss by adding the MSE and the regularization term\n    loss = mse + reg_term\n\n    # Return the Ridge loss rounded to the nearest 4th decimal\n    return round(loss, 4)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    w = np.random.rand(5)\n    y_true = np.random.rand(100)\n    alpha = 0.1\n\n    # Calculate the Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(\"Ridge Loss:\", loss)\n```\n\nIn this code:\n\n*   We define the `ridge_loss` function, which takes the feature matrix `X`, coefficients `w`, true labels `y_true`, and regularization parameter `alpha` as input.\n*   We calculate the predicted values `y_pred` by taking the dot product of the feature matrix `X` and coefficients `w`.\n*   We calculate the Mean Squared Error (MSE) between the predicted and true labels.\n*   We calculate the regularization term, which is the L2 norm of the coefficients multiplied by the regularization parameter `alpha`.\n*   We calculate the Ridge loss by adding the MSE and the regularization term.\n*   We return the Ridge loss rounded to the nearest 4th decimal.\n*   In the example usage, we generate some random data, calculate the Ridge loss, and print the result."}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    \"\"\"\n    This function implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Args:\n        z (float): The input value to the function.\n        alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n        float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    \n    # If the input value is greater than or equal to 0, return the input value\n    if z >= 0:\n        return z\n    \n    # If the input value is less than 0, return the product of the input value and alpha\n    else:\n        return alpha * z\n\n# Example usage:\nprint(leaky_relu(5))  # Output: 5\nprint(leaky_relu(-3))  # Output: -0.03\nprint(leaky_relu(-3, alpha=0.1))  # Output: -0.3\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Parameters:\n    x1 (numpy array): The first input vector.\n    x2 (numpy array): The second input vector.\n\n    Returns:\n    float: The linear kernel value between x1 and x2.\n    \"\"\"\n    # Check if the input vectors are numpy arrays\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError(\"Input vectors must be numpy arrays\")\n\n    # Check if the input vectors have the same dimension\n    if len(x1.shape)!= 1 or len(x2.shape)!= 1:\n        raise ValueError(\"Input vectors must be 1D arrays\")\n    if x1.shape[0]!= x2.shape[0]:\n        raise ValueError(\"Input vectors must have the same dimension\")\n\n    # Compute the dot product of the two input vectors\n    kernel_value = np.dot(x1, x2)\n\n    return kernel_value\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define two example vectors\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n\n    # Compute the linear kernel between the two vectors\n    kernel_value = kernel_function(x1, x2)\n\n    print(\"Linear kernel value:\", kernel_value)\n```\n\nIn this code:\n\n*   We define a function `kernel_function` that takes two input vectors `x1` and `x2`.\n*   We check if the input vectors are numpy arrays and have the same dimension.\n*   We compute the dot product of the two input vectors using `np.dot(x1, x2)`.\n*   We return the computed kernel value.\n*   In the example usage section, we demonstrate how to use the `kernel_function` with two example vectors `x1` and `x2`."}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given two numpy arrays: y_true and y_pred.\n\n    Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The precision metric.\n    \"\"\"\n    \n    # Check if the input arrays have the same shape\n    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n    \n    # Calculate true positives (TP) by finding the intersection of y_true and y_pred\n    # We use np.logical_and to perform element-wise logical AND operation\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    \n    # Calculate false positives (FP) by finding the intersection of y_true == 0 and y_pred == 1\n    # We use np.logical_and to perform element-wise logical AND operation\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    \n    # Calculate precision\n    # We use np.divide with a small value to avoid division by zero\n    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp + fp), where=(tp + fp) != 0)\n    \n    return precision\n\n# Example usage\ny_true = np.array([0, 1, 1, 0, 1, 0])\ny_pred = np.array([0, 1, 0, 0, 1, 1])\nprint(precision(y_true, y_pred))\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    This function performs three variants of gradient descent: Stochastic Gradient Descent (SGD), \n    Batch Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target vector\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int): Batch size for mini-batch gradient descent (default=1)\n    method (str): Variant of gradient descent to use (default='batch')\n\n    Returns:\n    list: Weights after gradient descent\n    \"\"\"\n    \n    # Ensure X and y are numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the bias term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Initialize weights\n    weights = np.array(weights)\n    \n    # Perform gradient descent\n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Calculate the gradient for batch gradient descent\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = (2 / len(y)) * np.dot(X.T, errors)\n            weights -= learning_rate * gradient\n            \n        elif method == 'stochastic':\n            # Calculate the gradient for stochastic gradient descent\n            for i in range(len(y)):\n                prediction = np.dot(X[i], weights)\n                error = prediction - y[i]\n                gradient = 2 * X[i] * error\n                weights -= learning_rate * gradient\n                \n        elif method == 'mini_batch':\n            # Calculate the gradient for mini-batch gradient descent\n            for i in range(0, len(y), batch_size):\n                batch_X = X[i:i+batch_size]\n                batch_y = y[i:i+batch_size]\n                predictions = np.dot(batch_X, weights)\n                errors = predictions - batch_y\n                gradient = (2 / len(batch_y)) * np.dot(batch_X.T, errors)\n                weights -= learning_rate * gradient\n                \n        else:\n            raise ValueError(\"Invalid method. Choose from 'batch', 'stochastic', or 'mini_batch'.\")\n    \n    # Return the weights as a list, rounded to the nearest 4th decimal\n    return np.round(weights, 4).tolist()\n\n# Example usage:\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 5]\nweights = [0, 0]\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 1\nmethod = 'batch'\n\nprint(gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method))\n```\n\nThis function takes in the feature matrix `X`, target vector `y`, initial weights `weights`, learning rate `learning_rate`, number of iterations `n_iterations`, batch size `batch_size`, and the method to use. It then performs the specified variant of gradient descent and returns the final weights as a list, rounded to the nearest 4th decimal.\n\nThe function first ensures that `X` and `y` are numpy arrays and adds a column of ones to `X` for the bias term. It then initializes the weights and performs the gradient descent.\n\nFor batch gradient descent, it calculates the gradient using the entire dataset at once. For stochastic gradient descent, it calculates the gradient using one example at a time. For mini-batch gradient descent, it calculates the gradient using a batch of examples at a time.\n\nFinally, it returns the weights as a list, rounded to the nearest 4th decimal.\n\nNote that the example usage demonstrates how to call the function with a sample dataset and parameters. The output will be the final weights after performing the specified variant of gradient descent."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n    matrix (numpy array): The input matrix.\n\n    Returns:\n    list: The RREF of the input matrix as a Python list.\n    \"\"\"\n    num_rows, num_cols = matrix.shape\n    current_row = 0\n\n    for j in range(num_cols):\n        if current_row >= num_rows:\n            break\n\n        pivot_row = current_row\n        while pivot_row < num_rows and matrix[pivot_row, j] == 0:\n            pivot_row += 1\n\n        if pivot_row == num_rows:\n            continue\n\n        # Swap the current row with the pivot row\n        matrix[[current_row, pivot_row]] = matrix[[pivot_row, current_row]]\n\n        # Normalize the pivot row\n        pivot = matrix[current_row, j]\n        matrix[current_row] = matrix[current_row] / pivot\n\n        # Eliminate the pivot variable from other rows\n        for i in range(num_rows):\n            if i != current_row:\n                factor = matrix[i, j]\n                matrix[i] = matrix[i] - factor * matrix[current_row]\n\n        current_row += 1\n\n    return matrix.tolist()\n\n# Example usage\nmatrix = np.array([[2, 1, -1, 8], [-3, -1, 2, -11], [-2, 1, 2, -3]], dtype=float)\nprint(rref(matrix))\n```\n\nIn this implementation, we iterate through the columns of the matrix and perform the following steps:\n\n1.  Find the first row with a non-zero entry in the current column (this row is called the pivot row).\n2.  Swap the current row with the pivot row.\n3.  Normalize the pivot row by dividing it by the pivot element.\n4.  Eliminate the pivot variable from other rows by subtracting a multiple of the pivot row from each row.\n\nThe function returns the RREF of the input matrix as a Python list after reshaping using NumPy's `tolist()` method. The example usage demonstrates how to use the `rref()` function with a sample matrix."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Adam optimization algorithm implementation.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of `f`.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 10.\n\n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n\n    # Initialize the parameters\n    x = x0.copy()\n    \n    # Initialize the first and second moment estimates\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    # Initialize the iteration counter\n    t = 0\n    \n    for _ in range(num_iterations):\n        # Compute the gradient of the objective function\n        g = grad(x)\n        \n        # Update the first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update the second moment estimate\n        v = beta2 * v + (1 - beta2) * g**2\n        \n        # Compute the bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1**(t + 1))\n        v_hat = v / (1 - beta2**(t + 1))\n        \n        # Update the parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Increment the iteration counter\n        t += 1\n    \n    # Return the optimized parameters as a list, rounded to the nearest 4th decimal\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# Define the objective function and its gradient\ndef f(x):\n    return x**2\n\ndef grad(x):\n    return 2 * x\n\n# Initialize the parameters\nx0 = np.array([1.0])\n\n# Run the Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0, num_iterations=100)\n\nprint(optimized_params)\n```\n\nThis implementation of the Adam optimization algorithm takes into account the bias correction for the first and second moment estimates, which is a key component of the Adam algorithm. The `adam_optimizer` function returns the optimized parameters as a list, rounded to the nearest 4th decimal. The example usage demonstrates how to define the objective function and its gradient, initialize the parameters, and run the Adam optimizer."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    This function implements Lasso Regression using Gradient Descent.\n    \n    Parameters:\n    X (np.array): The feature matrix.\n    y (np.array): The target variable.\n    alpha (float): The regularization parameter. Default is 0.1.\n    learning_rate (float): The learning rate for gradient descent. Default is 0.01.\n    max_iter (int): The maximum number of iterations. Default is 1000.\n    tol (float): The tolerance for convergence. Default is 1e-4.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    \n    # Initialize the weights and bias\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    \n    # Initialize the previous weights and bias for convergence check\n    prev_weights = np.zeros(n_features)\n    prev_bias = 0\n    \n    # Gradient Descent\n    for _ in range(max_iter):\n        # Compute the predictions\n        predictions = np.dot(X, weights) + bias\n        \n        # Compute the loss\n        loss = np.mean((predictions - y) ** 2)\n        \n        # Compute the gradients\n        weights_grad = (2 / n_samples) * np.dot(X.T, (predictions - y)) + alpha * np.sign(weights)\n        bias_grad = (2 / n_samples) * np.sum(predictions - y)\n        \n        # Update the weights and bias\n        weights -= learning_rate * weights_grad\n        bias -= learning_rate * bias_grad\n        \n        # Check for convergence\n        if np.allclose(weights, prev_weights, atol=tol) and np.isclose(bias, prev_bias, atol=tol):\n            break\n        \n        # Update the previous weights and bias\n        prev_weights = weights.copy()\n        prev_bias = bias\n    \n    # Round the results to the nearest 4th decimal\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    \n    return (weights.tolist(), [bias])\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n    \n    # Run the Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y)\n    \n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n\n```\n\nIn this code:\n\n*   We define a function `l1_regularization_gradient_descent` that implements Lasso Regression using Gradient Descent.\n*   We initialize the weights and bias, and then perform Gradient Descent to update them iteratively.\n*   We use the L1 penalty to shrink some of the feature coefficients to zero during Gradient Descent, thereby helping with feature selection.\n*   We check for convergence using the tolerance `tol`.\n*   We round the results to the nearest 4th decimal and return them as a tuple of lists.\n*   We provide an example usage of the function with some random data."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two given strings.\n\n    The OSA distance represents the minimum number of edits required to transform one string into another.\n    The allowed edit operations are:\n    - Insert a character\n    - Delete a character\n    - Substitute a character\n    - Transpose two adjacent characters\n    Each of these operations costs 1 unit.\n\n    Args:\n        source (str): The source string.\n        target (str): The target string.\n\n    Returns:\n        int: The minimum number of edits needed to convert the source string into the target string.\n    \"\"\"\n\n    # Initialize a matrix to store the distances between substrings of source and target\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the base cases\n    # The distance between an empty string and a non-empty string is the length of the non-empty string\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n\n    # Fill in the rest of the matrix\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # If the current characters in source and target are the same, there's no edit operation needed\n            if source[i - 1] == target[j - 1]:\n                dp[i, j] = dp[i - 1, j - 1]\n            else:\n                # Otherwise, consider all possible edit operations and choose the one with the minimum cost\n                substitution = dp[i - 1, j - 1] + 1\n                insertion = dp[i, j - 1] + 1\n                deletion = dp[i - 1, j] + 1\n                # Transposition is only possible if the current characters in source and target are adjacent\n                transposition = float('inf')\n                if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                    transposition = dp[i - 2, j - 2] + 1\n                dp[i, j] = min(substitution, insertion, deletion, transposition)\n\n    # The OSA distance is stored in the bottom-right corner of the matrix\n    return dp[m, n]\n\n# Example usage:\nsource = \"caper\"\ntarget = \"acer\"\nprint(\"The OSA distance between '{}' and '{}' is {}\".format(source, target, OSA(source, target)))\n```\n\nThis code uses dynamic programming to build a matrix `dp` where `dp[i, j]` represents the minimum number of edits needed to convert the first `i` characters of the source string into the first `j` characters of the target string. The final OSA distance is stored in the bottom-right corner of the matrix, `dp[m, n]`.\n\nThe code considers all possible edit operations (insertion, deletion, substitution, and transposition) and chooses the one with the minimum cost at each step. The transposition operation is only considered if the current characters in the source and target strings are adjacent.\n\nThe example usage demonstrates how to use the `OSA` function to calculate the OSA distance between two strings, \"caper\" and \"acer\". The output should be `2`, which is the minimum number of edits needed to convert \"caper\" into \"acer\" (one deletion and one transposition)."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in a binary classification setting.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    \n    # Convert input lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (TP): correctly predicted positive instances\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false negatives (FN): actual positive instances that were misclassified as negative\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate the denominator (TP + FN) to avoid division by zero\n    denominator = tp + fn\n    \n    # If the denominator is zero, return 0.0 to avoid division by zero\n    if denominator == 0:\n        return 0.0\n    \n    # Calculate the recall metric\n    recall_value = tp / denominator\n    \n    # Return the recall value rounded to three decimal places\n    return round(recall_value, 3)\n\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 1]\nprint(recall(y_true, y_pred))\n```\n\n**Explanation:**\n\n1. The function `recall(y_true, y_pred)` takes two lists of binary labels as input: `y_true` for the actual labels and `y_pred` for the predicted labels.\n2. It converts these lists to numpy arrays for easier manipulation.\n3. It calculates the true positives (TP) by finding the intersection of actual positive instances and correctly predicted positive instances.\n4. It calculates the false negatives (FN) by finding the intersection of actual positive instances and misclassified negative instances.\n5. It calculates the denominator (TP + FN) to avoid division by zero.\n6. If the denominator is zero, it returns 0.0 to avoid division by zero.\n7. It calculates the recall metric by dividing TP by the denominator.\n8. It returns the recall value rounded to three decimal places.\n\n**Example Use Case:**\n\nIn the example usage, we have a list of actual binary labels `y_true` and a list of predicted binary labels `y_pred`. We pass these lists to the `recall` function to calculate the recall metric. The function returns the recall value rounded to three decimal places, which represents the proportion of actual positive instances that were correctly identified by the model."}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    This function implements the self-attention mechanism, a fundamental component of transformer models.\n    \n    Parameters:\n    Q (numpy array): The query matrix.\n    K (numpy array): The key matrix.\n    V (numpy array): The value matrix.\n    \n    Returns:\n    list: The self-attention output as a python list, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Calculate the dot product of Q and K\n    dot_product = np.dot(Q, K.T)\n    \n    # Calculate the scaling factor\n    scaling_factor = np.sqrt(K.shape[1])\n    \n    # Calculate the scaled dot product\n    scaled_dot_product = dot_product / scaling_factor\n    \n    # Calculate the softmax of the scaled dot product\n    softmax_output = np.softmax(scaled_dot_product, axis=-1)\n    \n    # Calculate the output of the self-attention mechanism\n    output = np.dot(softmax_output, V)\n    \n    # Round the output to the nearest 4th decimal and return as a python list\n    return np.round(output, 4).tolist()\n\n# Example usage:\nQ = np.array([[1, 2, 3], [4, 5, 6]])\nK = np.array([[7, 8, 9], [10, 11, 12]])\nV = np.array([[13, 14, 15], [16, 17, 18]])\n\nprint(self_attention(Q, K, V))\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Parameters:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state of the RNN.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    \n    # Convert the input lists to numpy arrays for easier matrix operations\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state with the initial hidden state\n    hidden_state = initial_hidden_state\n    \n    # Process each input vector in the sequence\n    for input_vector in input_sequence:\n        # Calculate the new hidden state using the RNN formula\n        hidden_state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n    \n    # Return the final hidden state, rounded to four decimal places\n    return np.round(hidden_state, 4).tolist()\n```\n\n### Example Usage:\n\n```python\ninput_sequence = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[1.0, 2.0], [3.0, 4.0]]\nWh = [[5.0, 6.0], [7.0, 8.0]]\nb = [0.0, 0.0]\n\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_hidden_state)\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n\n    # Convert the list of points to a numpy array for easier manipulation\n    points_array = np.array(points)\n\n    # Create a translation matrix\n    # The translation matrix is a 2x3 matrix where the last column represents the translation distances\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n\n    # Add a column of ones to the points array to match the shape of the translation matrix\n    # This is because the translation matrix is designed to work with homogeneous coordinates\n    points_homogeneous = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n\n    # Apply the translation matrix to the points\n    # We use matrix multiplication to apply the translation matrix to each point\n    translated_points = np.dot(points_homogeneous, translation_matrix.T)[:, :2]\n\n    # Convert the translated points back to a list and return\n    return translated_points.tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\nThis code defines a function `translate_object` that takes a list of points and translation distances `tx` and `ty` as input, applies a 2D translation matrix to the points, and returns the translated points as a list. The function uses numpy arrays to efficiently manipulate the points and translation matrix. The example usage demonstrates how to call the function with a sample list of points and translation distances, and prints the translated points."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): The mean of the first normal distribution.\n    sigma_p (float): The standard deviation of the first normal distribution.\n    mu_q (float): The mean of the second normal distribution.\n    sigma_q (float): The standard deviation of the second normal distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    # Calculate the KL divergence using the formula for normal distributions\n    # The formula is: KL(P || Q) = log(sigma_q / sigma_p) + (sigma_p^2 + (mu_p - mu_q)^2) / (2 * sigma_q^2) - 0.5\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n    \n    return kl_divergence\n\n# Example usage:\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 2.0\n\nkl_divergence = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(\"KL Divergence:\", kl_divergence)\n```\n\n### Explanation\n\n*   The function `kl_divergence_normal` calculates the KL divergence between two normal distributions using the formula: `KL(P || Q) = log(sigma_q / sigma_p) + (sigma_p^2 + (mu_p - mu_q)^2) / (2 * sigma_q^2) - 0.5`.\n*   The function takes four parameters: `mu_p`, `sigma_p`, `mu_q`, and `sigma_q`, which represent the means and standard deviations of the two normal distributions.\n*   The KL divergence is calculated using NumPy's `log` function for the logarithmic term and basic arithmetic operations for the remaining terms.\n*   The result is returned as a floating-point number.\n\n### Advice\n\n*   Ensure that the standard deviations `sigma_p` and `sigma_q` are positive, as the KL divergence is undefined for zero or negative standard deviations.\n*   The KL divergence is not symmetric, meaning that `KL(P || Q)` is not necessarily equal to `KL(Q || P)`. Therefore, the order of the distributions matters when calculating the KL divergence.\n*   The KL divergence can be used to compare the similarity between two probability distributions. A lower KL divergence indicates that the distributions are more similar, while a higher KL divergence indicates that they are less similar."}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Gauss-Seidel Method for Solving Linear Systems.\n\n    Parameters:\n    A (numpy array): A square matrix of coefficients.\n    b (numpy array): The right-hand side vector.\n    n (int): The number of iterations.\n    x_ini (numpy array, optional): An initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    list: The approximated solution vector after performing the specified number of iterations.\n    \"\"\"\n    \n    # Check if A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    \n    # Check if b is a vector\n    assert len(b.shape) == 1, \"Vector b must be a 1D array\"\n    \n    # Check if the number of iterations is a positive integer\n    assert isinstance(n, int) and n > 0, \"Number of iterations must be a positive integer\"\n    \n    # Get the size of the matrix\n    size = A.shape[0]\n    \n    # Check if the size of b matches the size of A\n    assert b.shape[0] == size, \"Size of vector b must match the size of matrix A\"\n    \n    # If x_ini is not provided, initialize it with zeros\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        # Check if the size of x_ini matches the size of A\n        assert x_ini.shape[0] == size, \"Size of initial guess x_ini must match the size of matrix A\"\n        x = x_ini.copy()\n    \n    # Perform the Gauss-Seidel method\n    for _ in range(n):\n        x_new = x.copy()\n        for i in range(size):\n            # Calculate the sum of the terms before the current diagonal element\n            sum_before = np.sum(A[i, :i] * x_new[:i])\n            # Calculate the sum of the terms after the current diagonal element\n            sum_after = np.sum(A[i, i+1:] * x[i+1:])\n            # Update the current element of the solution vector\n            x_new[i] = (b[i] - sum_before - sum_after) / A[i, i]\n        x = x_new\n    \n    # Return the approximated solution vector as a list, rounded to the nearest 4th decimal\n    return np.round(x, 4).tolist()\n\n# Example usage:\nA = np.array([[3, 2], [1, -1]])\nb = np.array([1, 2])\nn = 10\nx_ini = np.array([0, 0])\n\nx_approx = gauss_seidel(A, b, n, x_ini)\nprint(\"Approximated solution vector:\", x_approx)\n```\n\nIn this code:\n\n*   We define the `gauss_seidel` function with the specified parameters.\n*   We perform checks to ensure that the input matrix `A` is square, the vector `b` is a 1D array, and the number of iterations `n` is a positive integer.\n*   We initialize the solution vector `x` with zeros if no initial guess is provided.\n*   We perform the Gauss-Seidel method for the specified number of iterations, updating the solution vector `x` at each iteration.\n*   Finally, we return the approximated solution vector as a list, rounded to the nearest 4th decimal using `np.round` and `tolist` methods.\n\nThe example usage demonstrates how to call the `gauss_seidel` function with a sample matrix `A`, vector `b`, number of iterations `n`, and initial guess `x_ini`. The approximated solution vector is then printed to the console."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    This function performs Gaussian Elimination with partial pivoting to solve the system (Ax = b).\n    \n    Parameters:\n    A (numpy array): The coefficient matrix of the system.\n    b (numpy array): The constant vector of the system.\n    \n    Returns:\n    x (list): The solution vector of the system, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Convert input to numpy arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the number of rows (equations) in the system\n    n = len(b)\n    \n    # Create an augmented matrix [A | b]\n    augmented_matrix = np.column_stack((A, b))\n    \n    # Perform Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the row with the largest absolute value in the current column\n        max_row = np.argmax(np.abs(augmented_matrix[i:, i])) + i\n        \n        # Swap the current row with the row having the largest absolute value\n        augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]\n        \n        # Make the pivot element equal to 1\n        pivot = augmented_matrix[i, i]\n        augmented_matrix[i] = augmented_matrix[i] / pivot\n        \n        # Eliminate the pivot variable from the rows below\n        for j in range(i + 1, n):\n            factor = augmented_matrix[j, i]\n            augmented_matrix[j] = augmented_matrix[j] - factor * augmented_matrix[i]\n    \n    # Perform backward substitution to find the solution vector\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = augmented_matrix[i, -1]\n        for j in range(i + 1, n):\n            x[i] = x[i] - augmented_matrix[i, j] * x[j]\n    \n    # Round the solution vector to the nearest 4th decimal and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage:\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]\nprint(gaussian_elimination(A, b))\n```\n\nThis code defines a function `gaussian_elimination(A, b)` that takes a coefficient matrix `A` and a constant vector `b` as input, performs Gaussian Elimination with partial pivoting to solve the system `Ax = b`, and returns the solution vector `x` as a list. The function uses NumPy arrays for efficient numerical computations and rounds the solution vector to the nearest 4th decimal before returning it as a list. The example usage demonstrates how to call the function with a sample system of linear equations."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # Initialize hidden states and cell states\n        hidden_states = np.zeros((x.shape[0], self.hidden_size))\n        cell_states = np.zeros((x.shape[0], self.hidden_size))\n\n        # Set initial hidden state and cell state\n        hidden_states[0] = initial_hidden_state.reshape(-1)\n        cell_states[0] = initial_cell_state.reshape(-1)\n\n        # Process each time step\n        for t in range(1, x.shape[0]):\n            # Concatenate input and previous hidden state\n            concat_input = np.concatenate((x[t-1].reshape(-1, 1), hidden_states[t-1].reshape(-1, 1)), axis=0)\n\n            # Compute forget gate\n            forget_gate = self.sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n\n            # Compute input gate\n            input_gate = self.sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n\n            # Compute candidate cell state\n            candidate_cell_state = self.tanh(np.dot(self.Wc, concat_input) + self.bc)\n\n            # Compute output gate\n            output_gate = self.sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n\n            # Update cell state\n            cell_states[t] = forget_gate * cell_states[t-1] + input_gate * candidate_cell_state\n\n            # Update hidden state\n            hidden_states[t] = output_gate * self.tanh(cell_states[t])\n\n        # Return hidden states, final hidden state, and final cell state\n        return (np.round(hidden_states, 4).tolist(), \n                np.round(hidden_states[-1], 4).tolist(), \n                np.round(cell_states[-1], 4).tolist())\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    lstm = LSTM(input_size=10, hidden_size=20)\n    x = np.random.randn(5, 10)\n    initial_hidden_state = np.random.randn(20)\n    initial_cell_state = np.random.randn(20)\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n    print(\"Hidden States:\", hidden_states)\n    print(\"Final Hidden State:\", final_hidden_state)\n    print(\"Final Cell State:\", final_cell_state)\n```\n\nIn this code:\n\n*   We define an `LSTM` class with `__init__` and `forward` methods.\n*   The `__init__` method initializes the LSTM with random weights and zero biases.\n*   The `forward` method processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n*   We use the sigmoid and tanh activation functions for the gates and cell state updates.\n*   The example usage demonstrates how to create an LSTM instance, generate random input data, and call the `forward` method to get the hidden states, final hidden state, and final cell state."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n    \"\"\"\n\n    # Check if the corpus is empty\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty\")\n\n    # Get the total number of documents\n    num_docs = len(corpus)\n\n    # Initialize a dictionary to store the document frequency for each term\n    doc_freq = {}\n\n    # Iterate over each document in the corpus\n    for doc in corpus:\n        # Convert the document to a set to remove duplicates\n        unique_terms = set(doc)\n        \n        # Iterate over each unique term in the document\n        for term in unique_terms:\n            # Increment the document frequency for the term\n            if term in doc_freq:\n                doc_freq[term] += 1\n            else:\n                doc_freq[term] = 1\n\n    # Initialize a list to store the TF-IDF scores for each document\n    tf_idf_scores = []\n\n    # Iterate over each document in the corpus\n    for doc in corpus:\n        # Initialize a list to store the TF-IDF scores for the current document\n        doc_scores = []\n\n        # Iterate over each term in the query\n        for term in query:\n            # Calculate the term frequency (TF)\n            tf = doc.count(term) / len(doc) if doc else 0\n\n            # Calculate the inverse document frequency (IDF)\n            idf = np.log((num_docs + 1) / (doc_freq.get(term, 0) + 1)) if doc_freq.get(term, 0) != 0 else 0\n\n            # Calculate the TF-IDF score\n            tf_idf = tf * idf\n\n            # Append the TF-IDF score to the list of scores for the current document\n            doc_scores.append(round(tf_idf, 4))\n\n        # Append the list of TF-IDF scores for the current document to the main list\n        tf_idf_scores.append(doc_scores)\n\n    # Return the list of TF-IDF scores\n    return np.array(tf_idf_scores).tolist()\n\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"the\", \"sun\", \"is\", \"shining\", \"brightly\", \"in\", \"the\", \"clear\", \"sky\"],\n    [\"the\", \"cat\", \"is\", \"sleeping\", \"peacefully\", \"on\", \"the\", \"soft\", \"cushion\"]\n]\nquery = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"sun\", \"shining\", \"cat\", \"sleeping\"]\n\nprint(compute_tf_idf(corpus, query))\n```\n\nIn this code:\n\n*   We first check if the corpus is empty and raise a `ValueError` if it is.\n*   We then calculate the document frequency for each term in the corpus.\n*   Next, we iterate over each document in the corpus and calculate the TF-IDF score for each term in the query.\n*   We use smoothing to avoid division by zero when calculating the IDF.\n*   Finally, we return the list of TF-IDF scores for each document, rounded to four decimal places.\n\nThe example usage demonstrates how to use the `compute_tf_idf` function with a sample corpus and query. The output will be a list of lists, where each inner list contains the TF-IDF scores for the query terms in the corresponding document."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives (TP), false positives (FP), and false negatives (FN)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Calculate precision\n    precision = tp / (tp + fp) if (tp + fp)!= 0 else 0\n\n    # Calculate recall\n    recall = tp / (tp + fn) if (tp + fn)!= 0 else 0\n\n    # Calculate F-Score using the formula: F-Score = (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)\n    f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (beta**2 * precision + recall)!= 0 else 0\n\n    # Return F-Score rounded to three decimal places\n    return round(f_score, 3)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n    y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 1])\n    beta = 1.0  # For F1-Score\n\n    print(f\"F-Score (beta={beta}): {f_score(y_true, y_pred, beta)}\")\n```\n\nIn this code:\n\n1.  We first calculate the true positives (`tp`), false positives (`fp`), and false negatives (`fn`) by comparing the true labels (`y_true`) and predicted labels (`y_pred`).\n2.  Then, we calculate precision and recall using the formulas: `precision = tp / (tp + fp)` and `recall = tp / (tp + fn)`.\n3.  Next, we calculate the F-Score using the formula: `F-Score = (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)`.\n4.  Finally, we return the F-Score rounded to three decimal places.\n\nThe example usage demonstrates how to use the `f_score` function with sample true labels (`y_true`) and predicted labels (`y_pred`), and calculates the F1-Score (when `beta=1`)."}
{"task_id": 62, "completion_id": 0, "completion": "Here's the implementation of the Simple RNN with Backpropagation Through Time (BPTT) in Python using NumPy for efficient numerical computations. This implementation includes the `initialize_weights`, `rnn_forward`, and `rnn_backward` methods as requested, along with the `entrypoint` function to orchestrate the training process.\n\n```python\nimport numpy as np  # Import NumPy for efficient numerical computations and array operations in Python, and set a random seed for reproducibility of results in the entrypoint function below this line of code, which is the main function that will be called to start the training process of the RNN model with BPTT for sequence prediction tasks, such as predicting the next item in a sequence based on the input sequence and expected output sequence provided as arguments to the entrypoint function, along with other hyperparameters like input size, hidden size, output size, learning rate, number of epochs, and random seed for reproducibility.\n\ndef entrypoint(input_sequence, expected_output, input_size, hidden_size, output_size, learning_rate=0.01, epochs=100, random_seed=42):\n    \"\"\"\n    Entrypoint function to train the RNN model with BPTT for sequence prediction.\n\n    Args:\n    - input_sequence (list): The input sequence to the RNN model.\n    - expected_output (list): The expected output sequence from the RNN model.\n    - input_size (int): The size of the input to the RNN model.\n    - hidden_size (int): The size of the hidden state in the RNN model.\n    - output_size (int): The size of the output from the RNN model.\n    - learning_rate (float, optional): The learning rate for the RNN model. Defaults to 0.01.\n    - epochs (int, optional): The number of epochs to train the RNN model. Defaults to 100.\n    - random_seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    - list: The predicted output sequence from the RNN model after training.\n    \"\"\"\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n    return np.round(np.array(outputs), 5).tolist()\n\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    \"\"\"\n    Initializes the RNN with weights set to random values multiplied by 0.01 and biases set to zero.\n\n    Args:\n    - input_size (int): The size of the input to the RNN model.\n    - hidden_size (int): The size of the hidden state in the RNN model.\n    - output_size (int): The size of the output from the RNN model.\n\n    Returns:\n    - tuple: A tuple containing the initialized weights and biases for the RNN model.\n    \"\"\"\n    W_xh = np.random.rand(input_size, hidden_size) * 0.01  # Weights from input to hidden state\n    W_hh = np.random.rand(hidden_size, hidden_size) * 0.01  # Weights from hidden state to hidden state\n    W_hy = np.random.rand(hidden_size, output_size) * 0.01  # Weights from hidden state to output\n    b_h = np.zeros((1, hidden_size))  # Bias for hidden state\n    b_y = np.zeros((1, output_size))  # Bias for output\n    return W_xh, W_hh, W_hy, b_h, b_y\n\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    \"\"\"\n    Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n\n    Args:\n    - W_xh (numpy array): Weights from input to hidden state.\n    - W_hh (numpy array): Weights from hidden state to hidden state.\n    - W_hy (numpy array): Weights from hidden state to output.\n    - b_h (numpy array): Bias for hidden state.\n    - b_y (numpy array): Bias for output.\n    - hidden_size (int): The size of the hidden state in the RNN model.\n    - input_sequence (list): The input"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Check if A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    \n    # Check if b is a vector\n    assert len(b.shape) == 1, \"Vector b must be a 1D array\"\n    \n    # Check if the number of iterations is positive\n    assert n > 0, \"Number of iterations must be positive\"\n    \n    # Check if the tolerance is positive\n    assert tol > 0, \"Tolerance must be positive\"\n    \n    # If x0 is not provided, set it to a zero vector\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    \n    # Initialize the solution vector x\n    x = x0.copy()\n    \n    # Initialize the residual vector r\n    r = b - np.dot(A, x)\n    \n    # Initialize the direction vector p\n    p = r.copy()\n    \n    # Initialize the variable for the previous residual norm\n    rsold = np.dot(r, r)\n    \n    # Perform the Conjugate Gradient method\n    for _ in range(n):\n        # Compute the matrix-vector product Ap\n        Ap = np.dot(A, p)\n        \n        # Compute the coefficient alpha\n        alpha = rsold / np.dot(p, Ap)\n        \n        # Update the solution vector x\n        x += alpha * p\n        \n        # Update the residual vector r\n        r -= alpha * Ap\n        \n        # Compute the new residual norm\n        rsnew = np.dot(r, r)\n        \n        # Check for convergence\n        if np.sqrt(rsnew) < tol:\n            break\n        \n        # Update the direction vector p\n        p = r + (rsnew / rsold) * p\n        \n        # Update the previous residual norm\n        rsold = rsnew\n    \n    # Round the solution vector to 8 decimal places and convert it to a list\n    x = np.round(x, 8).tolist()\n    \n    return x\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a symmetric positive-definite matrix A\n    A = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n    \n    # Define the right-hand side vector b\n    b = np.array([10, 25, 35])\n    \n    # Define the maximum number of iterations\n    n = 100\n    \n    # Define the initial guess for the solution vector\n    x0 = np.array([0, 0, 0])\n    \n    # Define the convergence tolerance\n    tol = 1e-8\n    \n    # Solve the system Ax = b using the Conjugate Gradient method\n    x = conjugate_gradient(A, b, n, x0, tol)\n    \n    print(\"Solution vector x:\", x)\n```\n\nThis code implements the Conjugate Gradient method for solving linear systems. It takes as input a symmetric positive-definite matrix `A`, a right-hand side vector `b`, the maximum number of iterations `n`, an initial guess for the solution vector `x0`, and a convergence tolerance `tol`. The function returns the solution vector `x` rounded to 8 decimal places and converted to a list.\n\nThe code first checks the input parameters to ensure they are valid. It then initializes the solution vector `x`, the residual vector `r`, and the direction vector `p`. The Conjugate Gradient method is performed in a loop that iterates up to `n` times. In each iteration, the matrix-vector product `Ap` is computed, the coefficient `alpha` is computed, the solution vector `x` is updated, the residual vector `r` is updated, and the direction vector `p` is updated. The loop breaks if the residual norm is less than the convergence tolerance `tol`.\n\nFinally, the solution vector `x` is rounded to 8 decimal places and converted to a list before being returned. The example usage demonstrates how to use the `conjugate_gradient` function to solve a linear system."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Calculate the total number of samples\n    total_samples = len(y)\n    \n    # Initialize a dictionary to store the count of each class label\n    class_counts = {}\n    \n    # Iterate over each class label and count the occurrences\n    for label in y:\n        if label in class_counts:\n            class_counts[label] += 1\n        else:\n            class_counts[label] = 1\n    \n    # Initialize the Gini Impurity to 1 (maximum impurity)\n    gini = 1.0\n    \n    # Iterate over each class label and its count\n    for count in class_counts.values():\n        # Calculate the probability of the class label\n        probability = count / total_samples\n        \n        # Update the Gini Impurity\n        gini -= probability ** 2\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a list of class labels\n    y = [0, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n    \n    # Calculate the Gini Impurity\n    impurity = gini_impurity(y)\n    \n    # Print the result\n    print(\"Gini Impurity:\", impurity)\n```\n\nThis code defines a function `gini_impurity` that calculates the Gini Impurity for a list of class labels. It uses a dictionary to count the occurrences of each class label and then calculates the probability of each label. The Gini Impurity is updated by subtracting the squared probability of each label from 1. Finally, the function returns the Gini Impurity rounded to three decimal places.\n\nIn the example usage, we define a list of class labels `y` and calculate the Gini Impurity using the `gini_impurity` function. The result is printed to the console."}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    # Initialize empty lists to store values, column indices, and row pointers\n    values = []\n    column_indices = []\n    row_pointers = [0]  # Start with 0 as the first row pointer\n\n    # Iterate over each row in the dense matrix\n    for row in dense_matrix:\n        # Initialize a counter for non-zero elements in the current row\n        non_zero_count = 0\n        \n        # Iterate over each element in the row\n        for col_idx, element in enumerate(row):\n            # Check if the element is non-zero\n            if element != 0:\n                # Append the non-zero element to the values list\n                values.append(element)\n                # Append the column index of the non-zero element\n                column_indices.append(col_idx)\n                # Increment the non-zero element counter\n                non_zero_count += 1\n        \n        # Append the cumulative count of non-zero elements to the row pointers list\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n\n    # Return the CSR representation as a tuple\n    return (values, column_indices, row_pointers)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a sample dense matrix\n    dense_matrix = [\n        [1, 0, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 3, 4],\n        [0, 0, 0, 5]\n    ]\n\n    # Convert the dense matrix to CSR format\n    values, column_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\n\n    # Print the CSR representation\n    print(\"Values array:\", values)\n    print(\"Column indices array:\", column_indices)\n    print(\"Row pointer array:\", row_pointers)\n```\n\n**Explanation:**\n\n1.  Initialize empty lists to store values, column indices, and row pointers.\n2.  Iterate over each row in the dense matrix, and for each row, iterate over its elements.\n3.  For each non-zero element, append its value and column index to the `values` and `column_indices` lists, respectively.\n4.  Maintain a cumulative count of non-zero elements per row and append it to the `row_pointers` list.\n5.  Return the CSR representation as a tuple containing the `values`, `column_indices`, and `row_pointers` lists.\n\n**Time Complexity:** O(m \\* n), where m is the number of rows and n is the number of columns in the dense matrix.\n\n**Space Complexity:** O(m + n + nnz), where nnz is the number of non-zero elements in the dense matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Convert input lists to numpy arrays for vector operations\n    v = np.array(v)\n    L = np.array(L)\n\n    # Calculate the dot product of v and L\n    dot_product = np.dot(v, L)\n\n    # Calculate the magnitude squared of L\n    magnitude_squared = np.dot(L, L)\n\n    # Calculate the projection of v onto L\n    projection = (dot_product / magnitude_squared) * L\n\n    # Round the projection vector to three decimal places\n    projection = np.round(projection, 3)\n\n    # Convert the projection vector back to a list and return it\n    return projection.tolist()\n\n\n# Example usage:\nv = [1, 2, 3]\nL = [4, 5, 6]\nprint(orthogonal_projection(v, L))\n```\n\nThis function works by first converting the input lists to numpy arrays for easier vector operations. It then calculates the dot product of `v` and `L`, as well as the magnitude squared of `L`. The projection of `v` onto `L` is calculated using the formula `(v \u00b7 L) / ||L||^2 * L`, where `\u00b7` denotes the dot product and `||L||` denotes the magnitude of `L`. Finally, the projection vector is rounded to three decimal places and converted back to a list before being returned."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Initialize empty lists to store values, row indices, and column pointers\n    values = []\n    row_indices = []\n    column_pointers = [0]  # Initialize with 0, as the first column starts at index 0\n\n    # Iterate over each column in the dense matrix\n    for col_idx in range(len(dense_matrix[0])):\n        # Initialize a counter to keep track of non-zero elements in the current column\n        non_zero_count = 0\n\n        # Iterate over each row in the dense matrix\n        for row_idx in range(len(dense_matrix)):\n            # Check if the current element is non-zero\n            if dense_matrix[row_idx][col_idx] != 0:\n                # Append the non-zero value to the values list\n                values.append(dense_matrix[row_idx][col_idx])\n                # Append the row index to the row indices list\n                row_indices.append(row_idx)\n                # Increment the non-zero counter\n                non_zero_count += 1\n\n        # Append the starting index of the next column to the column pointers list\n        column_pointers.append(column_pointers[-1] + non_zero_count)\n\n    # Return the CSC representation as a tuple of three lists\n    return values, row_indices, column_pointers\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a sample dense matrix\n    dense_matrix = [\n        [1, 0, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 3, 0],\n        [0, 0, 0, 4]\n    ]\n\n    # Convert the dense matrix to CSC representation\n    values, row_indices, column_pointers = compressed_col_sparse_matrix(dense_matrix)\n\n    # Print the CSC representation\n    print(\"Values:\", values)\n    print(\"Row Indices:\", row_indices)\n    print(\"Column Pointers:\", column_pointers)\n```\n\n**Explanation:**\n\n1.  Initialize empty lists to store values, row indices, and column pointers.\n2.  Iterate over each column in the dense matrix.\n3.  For each column, iterate over each row and check for non-zero elements.\n4.  If a non-zero element is found, append its value and row index to the `values` and `row_indices` lists, respectively.\n5.  After processing each column, append the starting index of the next column to the `column_pointers` list.\n6.  Return the CSC representation as a tuple of three lists: `values`, `row_indices`, and `column_pointers`.\n\n**Time Complexity:** O(m \\* n), where m is the number of rows and n is the number of columns in the dense matrix.\n\n**Space Complexity:** O(m \\* n), as in the worst case, all elements in the dense matrix could be non-zero, resulting in a CSC representation with the same number of elements."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a given matrix A.\n\n    Parameters:\n    A (numpy array): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    \n    # Convert the input matrix to a numpy array\n    A = np.array(A, dtype=float)\n    \n    # Perform Gaussian elimination to transform A into row echelon form\n    n, m = A.shape\n    lead = 0\n    for r in range(n):\n        if lead >= m:\n            return []\n        \n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == n:\n                i = r\n                lead += 1\n                if m == lead:\n                    return []\n        \n        # Swap rows\n        A[[i, r]] = A[[r, i]]\n        \n        # Normalize leading entry to 1\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        \n        # Subtract multiple of the current row from the rows below\n        for i in range(r+1, n):\n            lv = A[i, lead]\n            A[i] -= lv * A[r]\n        \n        lead += 1\n    \n    # Identify the independent columns (pivot columns)\n    independent_columns = []\n    for j in range(m):\n        if np.any(A[:, j] != 0):\n            independent_columns.append(j)\n    \n    # Extract the independent columns from the original matrix\n    basis_vectors = A[:, independent_columns]\n    \n    # Round the basis vectors to 8 decimal places and convert to a list\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n    \n    return basis_vectors\n\n# Example usage:\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 10]]\nprint(matrix_image(A))\n```\n\nThis code first performs Gaussian elimination to transform the input matrix `A` into row echelon form. It then identifies the independent columns (pivot columns) by checking for non-zero entries in the row echelon form. Finally, it extracts the independent columns from the original matrix, rounds them to 8 decimal places, and returns them as a list.\n\nNote that the `numpy` library is used for efficient numerical computations, and the `tolist()` method is used to convert the resulting numpy array to a list. The example usage demonstrates how to call the `matrix_image()` function with a sample matrix `A`."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array): Array of true values.\n    y_pred (array): Array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    # Calculate the mean of the true values\n    mean_y_true = np.mean(y_true)\n    \n    # Calculate the total sum of squares (SST)\n    sst = np.sum((y_true - mean_y_true) ** 2)\n    \n    # Calculate the sum of squares of residuals (SSE)\n    sse = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate the R-squared value\n    r2 = 1 - (sse / sst)\n    \n    # Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.1])\nprint(r_squared(y_true, y_pred))\n```\n\nIn this code:\n\n1. We first calculate the mean of the true values (`mean_y_true`).\n2. Then, we calculate the total sum of squares (`sst`), which represents the total variability in the true values.\n3. Next, we calculate the sum of squares of residuals (`sse`), which represents the variability in the true values that is not explained by the predicted values.\n4. We then calculate the R-squared value (`r2`) using the formula `1 - (sse / sst)`, which represents the proportion of the total variability that is explained by the predicted values.\n5. Finally, we return the R-squared value rounded to three decimal places.\n\nNote that this implementation assumes that the input arrays `y_true` and `y_pred` have the same shape and size. If the input arrays have different shapes or sizes, you may need to modify the implementation accordingly."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n    img (list): A 2D matrix representing the image, where each element is a pixel value between 0 (black) and 255 (white).\n\n    Returns:\n    float: The average brightness of the image rounded to two decimal places. Returns -1 for edge cases.\n    \"\"\"\n\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n\n    # Check if the rows in the matrix have inconsistent lengths\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n\n    # Initialize sum of pixel values\n    total_brightness = 0\n\n    # Initialize total number of pixels\n    total_pixels = 0\n\n    # Iterate over each row in the image\n    for row in img:\n        # Iterate over each pixel in the row\n        for pixel in row:\n            # Check if the pixel value is within the valid range (0-255)\n            if not 0 <= pixel <= 255:\n                return -1\n\n            # Add the pixel value to the total brightness\n            total_brightness += pixel\n\n            # Increment the total number of pixels\n            total_pixels += 1\n\n    # Calculate the average brightness\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n\n    # Return the average brightness rounded to two decimal places\n    return round(average_brightness, 2)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test case 1: Valid image\n    img1 = [[100, 150, 200], [50, 75, 100], [25, 50, 75]]\n    print(calculate_brightness(img1))\n\n    # Test case 2: Empty image\n    img2 = []\n    print(calculate_brightness(img2))\n\n    # Test case 3: Inconsistent row lengths\n    img3 = [[100, 150], [50, 75, 100], [25, 50]]\n    print(calculate_brightness(img3))\n\n    # Test case 4: Pixel value out of range\n    img4 = [[100, 150, 300], [50, 75, 100], [25, 50, 75]]\n    print(calculate_brightness(img4))\n```\n\nThis code defines a function `calculate_brightness` that calculates the average brightness of a grayscale image represented as a 2D matrix. It handles edge cases such as an empty image matrix, inconsistent row lengths, and pixel values outside the valid range (0-255). For any of these edge cases, the function returns -1. Otherwise, it returns the average brightness rounded to two decimal places. The code includes example usage and test cases to demonstrate its functionality."}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Args:\n        y_true (numpy array): The actual values.\n        y_pred (numpy array): The predicted values.\n\n    Returns:\n        float: The RMSE value rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input arrays have mismatched shapes or are empty.\n        TypeError: If the input arrays are not of type numpy array.\n    \"\"\"\n\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Both inputs must be numpy arrays.\")\n\n    # Check if input arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Check if input arrays are not empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    # Calculate the difference between actual and predicted values\n    diff = y_true - y_pred\n\n    # Calculate the square of the differences\n    squared_diff = diff ** 2\n\n    # Calculate the mean of the squared differences\n    mean_squared_diff = np.mean(squared_diff)\n\n    # Calculate the RMSE\n    rmse_value = np.sqrt(mean_squared_diff)\n\n    # Return the RMSE value rounded to three decimal places\n    return round(rmse_value, 3)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.1])\n    print(rmse(y_true, y_pred))\n```\n\nIn this code:\n\n*   We define the `rmse` function, which takes two numpy arrays `y_true` and `y_pred` as input.\n*   We check if the inputs are numpy arrays and raise a `TypeError` if not.\n*   We check if the input arrays have the same shape and raise a `ValueError` if not.\n*   We check if the input arrays are not empty and raise a `ValueError` if they are.\n*   We calculate the difference between the actual and predicted values, square the differences, and calculate the mean of the squared differences.\n*   We calculate the RMSE by taking the square root of the mean squared difference.\n*   We return the RMSE value rounded to three decimal places.\n*   In the example usage, we demonstrate how to use the `rmse` function with sample arrays `y_true` and `y_pred`."}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    The Jaccard Index is a measure of similarity between two sets, and is defined as the size of the intersection divided by the size of the union of the sets.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check if the input arrays have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The input arrays must have the same length.\")\n\n    # Calculate the intersection (number of elements that are 1 in both arrays)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n\n    # Calculate the union (number of elements that are 1 in either array)\n    union = np.sum(np.logical_or(y_true, y_pred))\n\n    # Handle the edge case where the union is zero\n    if union == 0:\n        return 1.0  # If both arrays contain only zeros, the Jaccard Index is 1\n\n    # Calculate the Jaccard Index\n    jaccard = intersection / union\n\n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n\n\n# Example usage:\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1])\nprint(jaccard_index(y_true, y_pred))\n```\n\nIn this code:\n\n*   We define the `jaccard_index` function, which takes two binary arrays `y_true` and `y_pred` as input.\n*   We calculate the intersection (number of elements that are 1 in both arrays) using `np.sum(np.logical_and(y_true, y_pred))`.\n*   We calculate the union (number of elements that are 1 in either array) using `np.sum(np.logical_or(y_true, y_pred))`.\n*   We handle the edge case where the union is zero by returning 1.0, since the Jaccard Index is 1 when both arrays contain only zeros.\n*   We calculate the Jaccard Index by dividing the intersection by the union.\n*   We return the Jaccard Index rounded to three decimal places using `round(jaccard, 3)`."}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score, also known as the S\u00f8rensen-Dice coefficient or F1-score, \n    for binary classification.\n\n    Args:\n        y_true (numpy array): Binary array of true labels.\n        y_pred (numpy array): Binary array of predicted labels.\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check if inputs are binary arrays\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError(\"Both y_true and y_pred must be binary arrays\")\n\n    # Check if inputs have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Both y_true and y_pred must have the same length\")\n\n    # Calculate the number of true positives (intersection of y_true and y_pred)\n    true_positives = np.sum(np.logical_and(y_true, y_pred))\n\n    # Calculate the number of true positives and predicted positives\n    num_true_positives = np.sum(y_true)\n    num_predicted_positives = np.sum(y_pred)\n\n    # Handle edge cases\n    if num_true_positives == 0 and num_predicted_positives == 0:\n        return 1.0  # Perfect overlap when both have no positives\n    elif num_true_positives == 0 or num_predicted_positives == 0:\n        return 0.0  # No overlap when one has no positives\n\n    # Calculate the Dice Score\n    dice = (2 * true_positives) / (num_true_positives + num_predicted_positives)\n\n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n\n# Example usage:\ny_true = np.array([1, 1, 0, 0, 1, 0, 1, 1])\ny_pred = np.array([1, 1, 0, 0, 1, 1, 0, 1])\nprint(dice_score(y_true, y_pred))\n```\n\nThis code defines a function `dice_score` that calculates the Dice Score between two binary arrays `y_true` and `y_pred`. It first checks if the inputs are valid binary arrays and have the same length. Then, it calculates the number of true positives (intersection of `y_true` and `y_pred`) and the number of true positives and predicted positives. The function handles edge cases where there are no true or predicted positives and returns the Dice Score rounded to 3 decimal places. The example usage demonstrates how to use the function with sample data."}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n\n    # Initialize an empty list to store the hypervectors for each feature\n    feature_hvs = []\n\n    # Iterate over each feature in the row\n    for feature, value in row.items():\n        # Generate a random hypervector for the feature name using the provided seed\n        np.random.seed(random_seeds[feature])\n        feature_name_hv = np.random.choice([-1, 1], size=dim)\n\n        # Generate a random hypervector for the feature value using the same seed\n        np.random.seed(random_seeds[feature])\n        feature_value_hv = np.random.choice([-1, 1], size=dim)\n\n        # Bind the feature name and value hypervectors using element-wise multiplication\n        feature_hv = np.multiply(feature_name_hv, feature_value_hv)\n\n        # Append the feature hypervector to the list\n        feature_hvs.append(feature_hv)\n\n    # Bundle the feature hypervectors to create a composite hypervector for the row\n    composite_hv = np.sum(feature_hvs, axis=0)\n\n    # Normalize the composite hypervector to have values in {-1, 1}\n    composite_hv = np.sign(composite_hv)\n\n    # Return the composite hypervector as a list\n    return composite_hv.tolist()\n```\n\n### Example Usage\n```python\n# Define a sample dataset row\nrow = {\n    'feature1': 1,\n    'feature2': 2,\n    'feature3': 3\n}\n\n# Define the dimensionality of the hypervectors\ndim = 100\n\n# Define the random seeds for each feature\nrandom_seeds = {\n    'feature1': 42,\n    'feature2': 24,\n    'feature3': 12\n}\n\n# Generate the composite hypervector for the row\ncomposite_hv = create_row_hv(row, dim, random_seeds)\n\n# Print the composite hypervector\nprint(composite_hv)\n```\n\nThis code defines a function `create_row_hv` that generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). The function takes a dictionary `row` representing the dataset row, an integer `dim` representing the dimensionality of the hypervectors, and a dictionary `random_seeds` providing seeds for each feature to ensure reproducibility. The function returns a composite hypervector representing the entire row as a list.\n\nThe code uses NumPy to generate random hypervectors for each feature and binds them using element-wise multiplication. The feature hypervectors are then bundled to create a composite hypervector for the row, which is normalized to have values in {-1, 1}. The composite hypervector is returned as a list.\n\nThe example usage demonstrates how to define a sample dataset row, dimensionality, and random seeds, and how to generate the composite hypervector for the row using the `create_row_hv` function."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n    - data (list of lists): A list of lists, where each inner list represents a pair\n        [y_true, y_pred] for one observation. y_true is the actual label, and y_pred\n        is the predicted label.\n\n    Returns:\n    - A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n\n    # Initialize counters for true positives, false positives, true negatives, and false negatives\n    tp = 0  # true positives\n    fp = 0  # false positives\n    tn = 0  # true negatives\n    fn = 0  # false negatives\n\n    # Iterate over each observation in the data\n    for y_true, y_pred in data:\n        # Check if the prediction is correct\n        if y_true == y_pred:\n            # If the prediction is correct and the label is 1, it's a true positive\n            if y_true == 1:\n                tp += 1\n            # If the prediction is correct and the label is 0, it's a true negative\n            else:\n                tn += 1\n        else:\n            # If the prediction is incorrect and the label is 1, it's a false negative\n            if y_true == 1:\n                fn += 1\n            # If the prediction is incorrect and the label is 0, it's a false positive\n            else:\n                fp += 1\n\n    # Create the confusion matrix\n    matrix = [[tn, fp], [fn, tp]]\n\n    return matrix\n\n\n# Example usage:\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0]]\nprint(confusion_matrix(data))\n```\n\n**Explanation**\n\n*   The function `confusion_matrix` takes a list of lists `data` as input, where each inner list represents a pair `[y_true, y_pred]` for one observation.\n*   It initializes counters for true positives (`tp`), false positives (`fp`), true negatives (`tn`), and false negatives (`fn`) to zero.\n*   The function then iterates over each observation in the data, checking if the prediction is correct. If the prediction is correct and the label is 1, it increments the true positives counter. If the prediction is correct and the label is 0, it increments the true negatives counter. If the prediction is incorrect and the label is 1, it increments the false negatives counter. If the prediction is incorrect and the label is 0, it increments the false positives counter.\n*   Finally, the function creates the confusion matrix as a 2x2 list of lists, where the first row represents the true negatives and false positives, and the second row represents the false negatives and true positives.\n*   The example usage demonstrates how to call the `confusion_matrix` function with a sample dataset and print the resulting confusion matrix."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    v1 (numpy array): The first input vector.\n    v2 (numpy array): The second input vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors, rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input vectors do not have the same shape or if either vector is empty or has zero magnitude.\n    \"\"\"\n\n    # Check if both input vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both input vectors must have the same shape.\")\n\n    # Check if input vectors are not empty\n    if len(v1) == 0 or len(v2) == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n\n    # Calculate the dot product of the two vectors\n    dot_product = np.dot(v1, v2)\n\n    # Calculate the magnitudes of the two vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n\n    # Check if either vector has zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n\n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n\n    # Return the cosine similarity rounded to three decimal places\n    return round(cosine_sim, 3)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define two example vectors\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n\n    # Calculate and print the cosine similarity\n    try:\n        similarity = cosine_similarity(v1, v2)\n        print(f\"Cosine similarity: {similarity}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```\n\nThis code defines a function `cosine_similarity` that calculates the cosine similarity between two input vectors `v1` and `v2`. It first checks if the input vectors have the same shape and are not empty. Then, it calculates the dot product and magnitudes of the vectors. If either vector has zero magnitude, it raises a `ValueError`. Finally, it calculates the cosine similarity and returns it rounded to three decimal places. The example usage demonstrates how to use the function with two example vectors and handle potential errors."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Tuple\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> Tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Args:\n    actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n\n    # Check if all elements in the actual and predicted lists are either 0 or 1\n    if not all(i in [0, 1] for i in actual) or not all(i in [0, 1] for i in predicted):\n        raise ValueError(\"All elements in the actual and predicted lists must be either 0 or 1\")\n\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"Both lists must have the same length\")\n\n    # Initialize the confusion matrix\n    confusion_matrix = [[0, 0], [0, 0]]\n\n    # Populate the confusion matrix\n    for a, p in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n\n    # Calculate accuracy\n    accuracy = (confusion_matrix[0][0] + confusion_matrix[1][1]) / len(actual)\n\n    # Calculate F1 score\n    precision = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[0][1]) if (confusion_matrix[1][1] + confusion_matrix[0][1]) != 0 else 0\n    recall = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[1][0]) if (confusion_matrix[1][1] + confusion_matrix[1][0]) != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n\n    # Calculate specificity\n    specificity = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[0][1]) if (confusion_matrix[0][0] + confusion_matrix[0][1]) != 0 else 0\n\n    # Calculate negative predictive value\n    negative_predictive_value = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[1][0]) if (confusion_matrix[0][0] + confusion_matrix[1][0]) != 0 else 0\n\n    # Return the performance metrics\n    return (confusion_matrix, \n            round(accuracy, 3), \n            round(f1_score, 3), \n            round(specificity, 3), \n            round(negative_predictive_value, 3))\n\n# Example usage:\nactual = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\npredicted = [1, 1, 0, 1, 0, 0, 1, 0, 0, 1]\nprint(performance_metrics(actual, predicted))\n```\n\nThis code defines a function `performance_metrics` that calculates various performance metrics for a binary classification model. It takes in two lists, `actual` and `predicted`, which represent the actual class labels and the predicted class labels from the model, respectively. The function returns a tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n\nThe code first checks if all elements in the `actual` and `predicted` lists are either 0 or 1 and if both lists have the same length. It then initializes and populates the confusion matrix. The accuracy, F1 score, specificity, and negative predictive value are calculated using the values in the confusion matrix. Finally, the function returns the performance metrics, with the accuracy, F1 score, specificity, and negative predictive value rounded to three decimal places.\n\nThe example usage demonstrates how to use the `performance_metrics` function with sample `actual` and `predicted` lists."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Args:\n    data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation, \n          percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n\n    # Ensure data is a NumPy array\n    data = np.array(data)\n\n    # Calculate mean\n    mean = round(np.mean(data), 4)\n\n    # Calculate median\n    median = np.median(data)\n\n    # Calculate mode\n    mode = stats.mode(data)[0][0]\n\n    # Calculate variance\n    variance = round(np.var(data), 4)\n\n    # Calculate standard deviation\n    standard_deviation = round(np.std(data), 4)\n\n    # Calculate percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n\n    # Calculate interquartile range (IQR)\n    iqr = percentile_75 - percentile_25\n\n    # Create a dictionary with the calculated statistics\n    stats_dict = {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': standard_deviation,\n        '25th_percentile': percentile_25,\n        '50th_percentile': percentile_50,\n        '75th_percentile': percentile_75,\n        'interquartile_range': iqr\n    }\n\n    return stats_dict\n\n# Example usage:\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nstats = descriptive_statistics(data)\nfor key, value in stats.items():\n    print(f\"{key}: {value}\")\n```\n\nThis code defines a function `descriptive_statistics` that calculates various descriptive statistics metrics for a given dataset. The function takes a list or NumPy array of numerical values as input and returns a dictionary containing the calculated statistics. The output dictionary includes the mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR) of the input data. The example usage demonstrates how to use the function with a sample dataset and print the calculated statistics."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binomial_coefficient = math.comb(n, k)\n    \n    # Calculate the probability of k successes and n-k failures\n    probability = binomial_coefficient * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\nn = 10  # Total number of trials\nk = 5   # Number of successes\np = 0.5  # Probability of success on each trial\n\nprobability = binomial_probability(n, k, p)\nprint(f\"The probability of {k} successes in {n} trials is: {probability}\")\n```\n\n**Explanation:**\n\n*   The `math.comb(n, k)` function is used to calculate the binomial coefficient (n choose k), which represents the number of ways to choose k successes from n trials.\n*   The probability of k successes and n-k failures is calculated using the formula `P(X=k) = (n choose k) * p^k * (1-p)^(n-k)`, where `p` is the probability of success on each trial.\n*   The result is rounded to five decimal places using the `round()` function to meet the output constraints.\n\n**Note:** The `math.comb(n, k)` function is available in Python 3.8 and later versions. If you're using an earlier version, you can calculate the binomial coefficient manually using the formula `n! / (k! * (n-k)!)`, where `!` denotes the factorial function."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the numerator of the exponential term\n    numerator = -((x - mean) ** 2)\n    \n    # Calculate the denominator of the exponential term\n    denominator = 2 * (std_dev ** 2)\n    \n    # Calculate the exponential term\n    exponential_term = math.exp(numerator / denominator)\n    \n    # Calculate the coefficient of the normal distribution\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    \n    # Calculate the PDF value\n    pdf_value = coefficient * exponential_term\n    \n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf_value, 5)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 0  # The value at which the PDF is evaluated\n    mean = 0  # The mean of the distribution\n    std_dev = 1  # The standard deviation of the distribution\n    \n    pdf = normal_pdf(x, mean, std_dev)\n    print(f\"The PDF value at x = {x} is {pdf}\")\n```\n\nIn this code:\n\n1. We define the `normal_pdf` function with parameters `x`, `mean`, and `std_dev`.\n2. We calculate the numerator and denominator of the exponential term using the given formula.\n3. We calculate the exponential term using the `math.exp` function.\n4. We calculate the coefficient of the normal distribution.\n5. We calculate the PDF value by multiplying the coefficient and the exponential term.\n6. We return the PDF value rounded to 5 decimal places.\n7. We provide an example usage of the function in the `if __name__ == \"__main__\":` block."}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    # Check if k is a non-negative integer\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n\n    # Check if lam is a non-negative number\n    if lam < 0:\n        raise ValueError(\"lam must be a non-negative number\")\n\n    # Calculate the probability using the Poisson distribution formula\n    # P(k; lam) = (e^(-lam) * (lam^k)) / k!\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n\n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    k = 2  # Number of events\n    lam = 1.5  # Mean rate of occurrences\n    probability = poisson_probability(k, lam)\n    print(f\"The probability of observing exactly {k} events with a mean rate of {lam} is: {probability}\")\n```\n\nThis code defines a function `poisson_probability` that calculates the probability of observing exactly `k` events in a fixed interval, given the mean rate of events `lam`, using the Poisson distribution formula. The function takes `k` and `lam` as inputs, checks if they are valid, calculates the probability, and returns it rounded to 5 decimal places. The example usage demonstrates how to use the function with `k = 2` and `lam = 1.5`."}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    # Check if the input image is a numpy array\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input image must be a numpy array\")\n\n    # Check if the input image is a 2D array\n    if len(img.shape) != 2:\n        raise ValueError(\"Input image must be a 2D array\")\n\n    # Check if the pixel values are between 0 and 255\n    if np.min(img) < 0 or np.max(img) > 255:\n        raise ValueError(\"Pixel values must be between 0 and 255\")\n\n    # Calculate the minimum pixel value in the image\n    min_pixel_value = np.min(img)\n\n    # Calculate the maximum pixel value in the image\n    max_pixel_value = np.max(img)\n\n    # Calculate the contrast as the difference between the maximum and minimum pixel values\n    contrast = max_pixel_value - min_pixel_value\n\n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample grayscale image\n    img = np.random.randint(0, 256, size=(256, 256), dtype=np.uint8)\n\n    # Calculate the contrast of the image\n    contrast = calculate_contrast(img)\n\n    print(f\"The contrast of the image is: {contrast}\")\n```\n\nIn this code:\n\n*   We define a function `calculate_contrast` that takes a 2D numpy array `img` as input, representing a grayscale image.\n*   We perform input validation to ensure that the input image is a numpy array, a 2D array, and has pixel values between 0 and 255.\n*   We calculate the minimum and maximum pixel values in the image using `np.min` and `np.max`.\n*   We calculate the contrast as the difference between the maximum and minimum pixel values.\n*   We provide an example usage of the function, creating a sample grayscale image and calculating its contrast."}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two input vectors.\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    # Check if the input vectors are of the same length\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Input vectors must be of the same length\")\n\n    # Calculate the dot product using NumPy's built-in dot function\n    dot_product = np.dot(vec1, vec2)\n\n    return dot_product\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define two example vectors\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n\n    # Calculate and print the dot product\n    result = calculate_dot_product(vec1, vec2)\n    print(\"Dot product:\", result)\n```\n\n### Explanation:\n\n1. **Function Definition**: The function `calculate_dot_product` takes two 1D NumPy arrays `vec1` and `vec2` as input.\n2. **Input Validation**: It checks if the input vectors are of the same length. If not, it raises a `ValueError`.\n3. **Dot Product Calculation**: The function uses NumPy's built-in `np.dot` function to calculate the dot product of the two input vectors.\n4. **Return Value**: The calculated dot product is returned as a single number.\n5. **Example Usage**: In the `if __name__ == \"__main__\":` block, two example vectors are defined, and the `calculate_dot_product` function is called to calculate and print their dot product.\n\n### Notes:\n\n* The `np.dot` function is used to calculate the dot product, which is a built-in and efficient way to perform this operation in NumPy.\n* The function includes input validation to ensure that the input vectors are of the same length, which is a requirement for calculating the dot product.\n* The example usage demonstrates how to call the function with two example vectors and print the result."}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    # Check if the degree is less than 0, return an empty list if true\n    if degree < 0:\n        return []\n\n    # Initialize an empty list to store the transformed features\n    transformed_features = []\n\n    # Iterate over each data point in the input list\n    for x in data:\n        # Initialize an empty list to store the polynomial features of the current data point\n        polynomial_features = []\n\n        # Generate polynomial features from degree 0 to the specified degree\n        for i in range(degree + 1):\n            # Calculate the polynomial feature using the power operator (**)\n            feature = x ** i\n\n            # Round the feature to 8 decimal places and append it to the list of polynomial features\n            polynomial_features.append(round(feature, 8))\n\n        # Append the list of polynomial features to the list of transformed features\n        transformed_features.append(polynomial_features)\n\n    # Return the list of transformed features\n    return transformed_features\n\n\n# Example usage:\ndata = [1.0, 2.0, 3.0]\ndegree = 2\nprint(phi_transform(data, degree))\n```\n\n**Explanation:**\n\n*   The function `phi_transform` takes a list of numerical values `data` and an integer `degree` as inputs.\n*   It checks if the `degree` is less than 0 and returns an empty list if true.\n*   It iterates over each data point `x` in the input list `data`.\n*   For each data point, it generates polynomial features from degree 0 to the specified `degree` using a nested loop.\n*   The polynomial features are calculated using the power operator (`**`) and rounded to 8 decimal places.\n*   The list of polynomial features for each data point is appended to the list of transformed features `transformed_features`.\n*   Finally, the function returns the list of transformed features.\n\n**Example Output:**\n\nFor the input `data = [1.0, 2.0, 3.0]` and `degree = 2`, the output will be:\n\n```python\n[[1.0, 1.0, 1.0], [1.0, 2.0, 4.0], [1.0, 3.0, 9.0]]\n```\n\nThis represents the polynomial features of each data point, where the first element of each inner list is the constant term (degree 0), the second element is the linear term (degree 1), and the third element is the quadratic term (degree 2)."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    This function calculates positional encodings for a sequence length (`position`) \n    and model dimensionality (`d_model`) using sine and cosine functions as specified \n    in the Transformer architecture.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings. Returns -1 if `position` is 0, or if `d_model` is less than or equal to 0.\n    \"\"\"\n\n    # Check for invalid input\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Initialize the encoding array\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n\n    # Calculate the positional encodings\n    for pos in range(position):\n        for i in range(d_model):\n            # Calculate the frequency\n            frequency = 1 / (10000 ** (i / d_model))\n            \n            # Apply the sine function for even indices and cosine for odd indices\n            if i % 2 == 0:\n                encoding[pos, i] = np.sin(pos * frequency)\n            else:\n                encoding[pos, i] = np.cos(pos * frequency)\n\n    # Return the encoding array as a list\n    return encoding.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    position = 10\n    d_model = 512\n    print(pos_encoding(position, d_model))\n```\n\nThis code defines a function `pos_encoding` that takes two parameters: `position` and `d_model`. It first checks if the input is valid. If not, it returns -1. Then, it initializes a 2D array `encoding` with zeros, where the number of rows is `position` and the number of columns is `d_model`. The function then calculates the positional encodings using sine and cosine functions and stores them in the `encoding` array. Finally, it returns the `encoding` array as a list.\n\nThe example usage demonstrates how to use the `pos_encoding` function with a sequence length of 10 and a model dimensionality of 512. The output will be a list of positional encodings for the given sequence length and model dimensionality."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check if the training accuracy is significantly higher than the test accuracy (difference > 0.2)\n    if training_accuracy - test_accuracy > 0.2:\n        # If true, the model is overfitting\n        return 1\n    \n    # Check if both training and test accuracy are below 0.7\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        # If true, the model is underfitting\n        return -1\n    \n    # If neither of the above conditions is true, the model is a good fit\n    return 0\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test case 1: Overfitting\n    print(model_fit_quality(0.9, 0.6))  # Expected output: 1\n    \n    # Test case 2: Underfitting\n    print(model_fit_quality(0.5, 0.6))  # Expected output: -1\n    \n    # Test case 3: Good fit\n    print(model_fit_quality(0.8, 0.8))  # Expected output: 0\n```\n\nIn this code:\n\n*   We define a function `model_fit_quality` that takes `training_accuracy` and `test_accuracy` as inputs and returns an integer indicating whether the model is overfitting (1), underfitting (-1), or a good fit (0).\n*   We first check for overfitting by comparing the difference between `training_accuracy` and `test_accuracy`. If the difference is greater than 0.2, we return 1.\n*   Next, we check for underfitting by verifying if both `training_accuracy` and `test_accuracy` are below 0.7. If so, we return -1.\n*   If neither condition is met, we return 0, indicating a good fit.\n*   In the example usage section, we demonstrate the function with three test cases: overfitting, underfitting, and a good fit."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update second moment estimate\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    \n    # Apply bias correction\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    \n    # Update parameter\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 5 decimal places and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage:\nif __name__ == \"__main__\":\n    parameter = np.array([1.0, 2.0])\n    grad = np.array([0.1, 0.2])\n    m = np.array([0.0, 0.0])\n    v = np.array([0.0, 0.0])\n    t = 1\n    \n    updated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\n    print(\"Updated Parameter:\", updated_parameter)\n    print(\"Updated First Moment (m):\", updated_m)\n    print(\"Updated Second Moment (v):\", updated_v)\n```\n\nIn this code:\n\n*   We define the `adam_optimizer` function, which takes the current parameter value, gradient, moving averages, and other hyperparameters as inputs.\n*   We update the first and second moment estimates using the given formulas.\n*   We apply bias correction to the moment estimates.\n*   We update the parameter value using the Adam optimizer update rule.\n*   We round the updated parameter, first moment, and second moment to 5 decimal places and convert them to lists using the `tolist()` method.\n*   In the example usage section, we demonstrate how to use the `adam_optimizer` function with sample inputs and print the updated values."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    \"\"\"\n    Generates text based on a given prompt using a simplified GPT-2-like architecture.\n\n    Args:\n    - prompt (str): The initial text to guide the generation process.\n    - n_tokens_to_generate (int): The number of tokens to output.\n\n    Returns:\n    - generated_text (str): The generated text.\n    \"\"\"\n    np.random.seed(42)  # Set the random seed for reproducibility\n\n    # Load the encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt\n    prompt_tokens = encoder.encode(prompt)\n    prompt_len = len(prompt_tokens)\n\n    # Initialize the output tokens with the prompt tokens\n    output_tokens = prompt_tokens[:]\n\n    # Generate tokens one by one\n    for i in range(n_tokens_to_generate):\n        # Get the last 'n_ctx' tokens\n        ctx_tokens = output_tokens[-hparams[\"n_ctx\"]:]\n\n        # Calculate token embeddings\n        token_embeddings = np.array([params[\"wte\"][token] for token in ctx_tokens])\n\n        # Calculate positional embeddings\n        positional_embeddings = np.array([params[\"wpe\"][j] for j in range(len(ctx_tokens))])\n\n        # Add token and positional embeddings\n        embeddings = token_embeddings + positional_embeddings\n\n        # Multi-head attention\n        attention_outputs = np.zeros((len(ctx_tokens), embeddings.shape[1]))\n        for head in range(hparams[\"n_head\"]):\n            # Query, key, and value matrices\n            q = embeddings\n            k = embeddings\n            v = embeddings\n\n            # Calculate attention scores\n            attention_scores = np.dot(q, k.T) / np.sqrt(embeddings.shape[1])\n\n            # Calculate attention weights\n            attention_weights = np.softmax(attention_scores, axis=-1)\n\n            # Calculate attention output\n            attention_output = np.dot(attention_weights, v)\n\n            # Concatenate attention outputs from all heads\n            attention_outputs += attention_output\n\n        # Feed-forward network\n        ff_output = np.maximum(attention_outputs, 0)  # ReLU activation\n\n        # Layer normalization\n        ln_output = (ff_output - np.mean(ff_output, axis=-1, keepdims=True)) / np.std(ff_output, axis=-1, keepdims=True)\n        ln_output = ln_output * params[\"ln_f\"][\"g\"] + params[\"ln_f\"][\"b\"]\n\n        # Get the last token's embedding\n        last_token_embedding = ln_output[-1]\n\n        # Calculate the probability distribution over all tokens\n        probs = np.dot(last_token_embedding, params[\"wte\"].T)\n\n        # Sample the next token\n        next_token = np.argmax(probs)\n\n        # Append the next token to the output tokens\n        output_tokens.append(next_token)\n\n    # Decode the output tokens\n    generated_text = encoder.decode(output_tokens)\n\n    return generated_text\n\n# Example usage\nprompt = \"hello world\"\nn_tokens_to_generate = 40\ngenerated_text = gen_text(prompt, n_tokens_to_generate)\nprint(generated_text)\n```\n\nThis code implements a simplified GPT-2-like text generation function in Python. It uses a dummy encoder, hyperparameters, and model parameters to generate text based on a given prompt. The function incorporates token embeddings, positional embeddings, multi-head attention, a feed-forward network, and layer normalization to generate text one token at a time.\n\nNote that this is a highly simplified version of the GPT-2 model and is not intended to produce high-quality text. The purpose of this exercise is to demonstrate the core concepts behind GPT-2's autoregressive text generation."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function implements a simplified self-attention mechanism to calculate the enhanced patterns of crystals.\n    \n    Parameters:\n    n (int): The number of crystals.\n    crystal_values (list): A list of unique numeric values for each crystal.\n    dimension (int): The dimension of the crystal values.\n    \n    Returns:\n    list: A list of floats representing the final weighted patterns for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function implements the softmax function.\n        \n        Parameters:\n        values (list): A list of values.\n        \n        Returns:\n        list: A list of softmax values.\n        \"\"\"\n        # Calculate the exponential of each value\n        exp_values = np.exp(values)\n        \n        # Calculate the sum of the exponential values\n        sum_exp_values = np.sum(exp_values)\n        \n        # Calculate the softmax values\n        softmax_values = exp_values / sum_exp_values\n        \n        return softmax_values\n\n    # Initialize the attention scores and weighted patterns\n    attention_scores = np.zeros((n, n))\n    weighted_patterns = np.zeros((n, dimension))\n\n    # Calculate the attention scores using the softmax function\n    for i in range(n):\n        for j in range(n):\n            # Calculate the dot product of the crystal values\n            dot_product = np.dot(crystal_values[i], crystal_values[j])\n            \n            # Store the dot product in the attention scores matrix\n            attention_scores[i, j] = dot_product\n\n    # Calculate the softmax of the attention scores\n    for i in range(n):\n        attention_scores[i] = softmax(attention_scores[i])\n\n    # Calculate the weighted patterns\n    for i in range(n):\n        for j in range(n):\n            # Calculate the weighted pattern for each crystal\n            weighted_patterns[i] += attention_scores[i, j] * crystal_values[j]\n\n    # Return the weighted patterns as a list of floats, rounded to the 4th decimal place\n    return [round(np.sum(pattern), 4) for pattern in weighted_patterns]\n\n# Example usage:\nn = 3\ncrystal_values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndimension = 3\n\nresult = pattern_weaver(n, crystal_values, dimension)\nprint(result)\n```\n\nThis code defines a function `pattern_weaver` that takes the number of crystals `n`, their values `crystal_values`, and the dimension `dimension` as input. It calculates the attention scores using the softmax function and derives the final weighted patterns for each crystal. The `softmax` function is implemented as a nested function within `pattern_weaver`. The code returns a list of floats representing the final weighted patterns for each crystal, rounded to the 4th decimal place.\n\nIn the example usage, we create a list of crystal values `crystal_values` with shape `(3, 3)`, where each row represents a crystal. We then call the `pattern_weaver` function with `n=3`, `crystal_values`, and `dimension=3`, and print the result."}
{"task_id": 90, "completion_id": 0, "completion": "Here's a Python implementation of the BM25 ranking function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query in a corpus.\n\n    Parameters:\n    - corpus (list of lists): A list of documents, where each document is a list of words.\n    - query (list): A list of query words.\n    - k1 (float): A hyperparameter that controls the effect of term frequency saturation. Default is 1.5.\n    - b (float): A hyperparameter that controls the effect of document length normalization. Default is 0.75.\n\n    Returns:\n    - scores (list): A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n\n    # Calculate the average document length\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n\n    # Initialize the scores list\n    scores = []\n\n    # Iterate over each document in the corpus\n    for doc in corpus:\n        # Calculate the document length\n        doc_len = len(doc)\n\n        # Calculate the term frequency for each query word in the document\n        tf = Counter(doc)\n\n        # Initialize the score for the current document\n        score = 0\n\n        # Iterate over each query word\n        for word in query:\n            # Check if the word is in the document\n            if word in tf:\n                # Calculate the BM25 weight for the current word\n                weight = (tf[word] * (k1 + 1)) / (tf[word] + k1 * (1 - b + b * doc_len / avg_doc_len))\n                # Add the weight to the score\n                score += weight\n\n        # Append the score to the scores list\n        scores.append(round(score, 3))\n\n    return scores\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"the\", \"sun\", \"is\", \"shining\", \"brightly\", \"in\", \"the\", \"clear\", \"blue\", \"sky\"],\n    [\"the\", \"cat\", \"is\", \"sleeping\", \"on\", \"the\", \"soft\", \"cushion\"]\n]\nquery = [\"the\", \"quick\", \"fox\"]\nprint(calculate_bm25_scores(corpus, query))\n```\n\nThis code defines a function `calculate_bm25_scores` that takes a corpus, a query, and two hyperparameters `k1` and `b` as input. It calculates the BM25 score for each document in the corpus and returns a list of scores, rounded to three decimal places.\n\nThe BM25 score is calculated using the following formula:\n\n`BM25 = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))`\n\nwhere `tf` is the term frequency, `k1` is a hyperparameter that controls the effect of term frequency saturation, `b` is a hyperparameter that controls the effect of document length normalization, `doc_len` is the length of the document, and `avg_doc_len` is the average length of all documents in the corpus.\n\nThe code first calculates the average document length, then iterates over each document in the corpus. For each document, it calculates the term frequency for each query word, then calculates the BM25 weight for each word using the formula above. The weights are summed to obtain the final score for the document. The scores are then rounded to three decimal places and returned as a list."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counters for true positives, false positives, and false negatives\n    tp = 0  # true positives\n    fp = 0  # false positives\n    fn = 0  # false negatives\n\n    # Iterate over the true and predicted labels\n    for true, pred in zip(y_true, y_pred):\n        # Check if the prediction is correct (true positive)\n        if true == pred:\n            tp += 1\n        # Check if the prediction is incorrect (false positive or false negative)\n        elif true != pred:\n            # If the true label is 1, it's a false negative\n            if true == 1:\n                fn += 1\n            # If the true label is 0, it's a false positive\n            elif true == 0:\n                fp += 1\n\n    # Calculate precision\n    # Precision is the ratio of true positives to the sum of true positives and false positives\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n\n    # Calculate recall\n    # Recall is the ratio of true positives to the sum of true positives and false negatives\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n\n    # Calculate F1 score\n    # F1 score is the harmonic mean of precision and recall\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n\n    # Return the F1 score rounded to three decimal places\n    return round(f1_score, 3)\n\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\nprint(calculate_f1_score(y_true, y_pred))\n```\n\nHowever, a more pythonic way to solve this problem would be to use the `precision_score`, `recall_score`, and `f1_score` functions from the `sklearn.metrics` module:\n\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Calculate F1 score\n    f1 = f1_score(y_true, y_pred)\n\n    # Return the F1 score rounded to three decimal places\n    return round(f1, 3)\n\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\nprint(calculate_f1_score(y_true, y_pred))\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    This function forecasts the power consumption for day 15, taking into account \n    a growing linear trend and a daily fluctuation. It removes the fluctuation, \n    fits a linear regression model, predicts day 15's base consumption, adds back \n    the fluctuation, and includes a 5% safety margin.\n\n    Args:\n        consumption_data (list): A list of 10 daily power usage measurements.\n\n    Returns:\n        int: The forecasted power consumption for day 15 with a 5% safety margin.\n    \"\"\"\n\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = [data - 10 * math.sin(2 * PI * (i + 1) / 10) for i, data in enumerate(consumption_data)]\n\n    # 2) Perform linear regression on the detrended data.\n    # Calculate the coefficients (slope and intercept) of the linear regression line.\n    n = len(detrended_data)\n    x = list(range(1, n + 1))  # Day numbers\n    sum_x = sum(x)\n    sum_y = sum(detrended_data)\n    sum_xy = sum(i * j for i, j in zip(x, detrended_data))\n    sum_x_squared = sum(i ** 2 for i in x)\n\n    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)\n    intercept = (sum_y - slope * sum_x) / n\n\n    # 3) Predict day 15's base consumption.\n    day_15_base_consumption = slope * 15 + intercept\n\n    # 4) Add the day 15 fluctuation back.\n    day_15_consumption = day_15_base_consumption + 10 * math.sin(2 * PI * 15 / 10)\n\n    # 5) Round, then add a 5% safety margin (rounded up).\n    import math\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n\n    # 6) Return the final integer.\n    return final_consumption\n\n# Example usage:\nconsumption_data = [120, 115, 130, 125, 140, 135, 150, 145, 160, 155]\nprint(power_grid_forecast(consumption_data))\n```\n\nThis code defines a function `power_grid_forecast` that takes a list of 10 daily power usage measurements as input and returns the forecasted power consumption for day 15 with a 5% safety margin. The function first removes the daily fluctuation from each data point, then performs linear regression on the detrended data to predict day 15's base consumption. It adds back the fluctuation for day 15 and includes a 5% safety margin before returning the final integer. The example usage demonstrates how to call the function with a sample list of consumption data."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Check if the input arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Both arrays should have the same shape\")\n\n    # Calculate the absolute difference between the true and predicted values\n    absolute_errors = np.abs(y_true - y_pred)\n    \n    # Calculate the mean of the absolute errors\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    # Round the result to 3 decimal places\n    mean_absolute_error = round(mean_absolute_error, 3)\n    \n    return mean_absolute_error\n\n# Example usage:\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.1])\n\nmae_value = mae(y_true, y_pred)\nprint(\"Mean Absolute Error:\", mae_value)\n```\n\nIn this code:\n\n*   We first check if the input arrays `y_true` and `y_pred` have the same shape. If they don't, we raise a `ValueError`.\n*   We calculate the absolute difference between the true and predicted values using `np.abs(y_true - y_pred)`.\n*   We calculate the mean of the absolute errors using `np.mean(absolute_errors)`.\n*   We round the result to 3 decimal places using `round(mean_absolute_error, 3)`.\n*   Finally, we return the calculated Mean Absolute Error.\n\nThe example usage demonstrates how to use the `mae` function with two sample arrays `y_true` and `y_pred`. It calculates and prints the Mean Absolute Error between these arrays."}
{"task_id": 94, "completion_id": 0, "completion": "Here's the implementation of the multi-head attention mechanism in Python:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute Q, K, and V matrices by multiplying input X with weight matrices W_q, W_k, and W_v respectively.\n\n    Args:\n    X (np.ndarray): Input data\n    W_q (np.ndarray): Weight matrix for Q\n    W_k (np.ndarray): Weight matrix for K\n    W_v (np.ndarray): Weight matrix for V\n\n    Returns:\n    Q (np.ndarray): Query matrix\n    K (np.ndarray): Key matrix\n    V (np.ndarray): Value matrix\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute self-attention by calculating attention weights and applying them to V.\n\n    Args:\n    Q (np.ndarray): Query matrix\n    K (np.ndarray): Key matrix\n    V (np.ndarray): Value matrix\n\n    Returns:\n    attention_output (np.ndarray): Output of self-attention\n    \"\"\"\n    # Calculate attention weights\n    attention_weights = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    # Apply softmax to attention weights\n    attention_weights = np.exp(attention_weights) / np.sum(np.exp(attention_weights), axis=-1, keepdims=True)\n    # Apply attention weights to V\n    attention_output = np.dot(attention_weights, V)\n    return attention_output\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention by splitting Q, K, and V into multiple heads, applying self-attention to each head, and concatenating the outputs.\n\n    Args:\n    Q (np.ndarray): Query matrix\n    K (np.ndarray): Key matrix\n    V (np.ndarray): Value matrix\n    n_heads (int): Number of heads\n\n    Returns:\n    multi_head_output (np.ndarray): Output of multi-head attention\n    \"\"\"\n    # Split Q, K, and V into multiple heads\n    head_size = Q.shape[1] // n_heads\n    Q_heads = np.split(Q, n_heads, axis=1)\n    K_heads = np.split(K, n_heads, axis=1)\n    V_heads = np.split(V, n_heads, axis=1)\n\n    # Apply self-attention to each head\n    attention_outputs = []\n    for i in range(n_heads):\n        attention_output = self_attention(Q_heads[i], K_heads[i], V_heads[i])\n        attention_outputs.append(attention_output)\n\n    # Concatenate the outputs\n    multi_head_output = np.concatenate(attention_outputs, axis=1)\n    return multi_head_output\n\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n\n# Example usage\nm = 10\nn = 20\nn_heads = 4\nrandom_seed = 42\noutput = entrypoint(m, n, n_heads, random_seed)\nprint(output)\n```\n\nThis implementation defines three key functions: `compute_qkv`, `self_attention`, and `multi_head_attention`. The `compute_qkv` function computes the Q, K, and V matrices by multiplying the input X with the weight matrices W_q, W_k, and W_v respectively. The `self_attention` function computes self-attention by calculating attention weights and applying them to V. The `multi_head_attention` function computes multi-head attention by splitting Q, K, and V into multiple heads, applying self-attention to each head, and concatenating the outputs. The `entrypoint` function generates input data and weight matrices, computes Q, K, and V, and applies multi-head attention to produce the final output."}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Calculate the number of observations where both x and y are 1\n    n11 = sum(1 for i, j in zip(x, y) if i == 1 and j == 1)\n    \n    # Calculate the number of observations where x is 1 and y is 0\n    n10 = sum(1 for i, j in zip(x, y) if i == 1 and j == 0)\n    \n    # Calculate the number of observations where x is 0 and y is 1\n    n01 = sum(1 for i, j in zip(x, y) if i == 0 and j == 1)\n    \n    # Calculate the number of observations where both x and y are 0\n    n00 = sum(1 for i, j in zip(x, y) if i == 0 and j == 0)\n    \n    # Calculate the total number of observations\n    n = len(x)\n    \n    # Calculate the Phi coefficient\n    phi = ((n11 * n00) - (n10 * n01)) / ((n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)) ** 0.5\n    \n    # Return the Phi coefficient rounded to 4 decimal places\n    return round(phi, 4)\n```\n\n### Example Usage\n```python\nx = [1, 1, 0, 0, 1, 0, 1, 1, 0, 0]\ny = [1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\nprint(phi_corr(x, y))\n```\n\nThis function calculates the Phi coefficient between two binary variables `x` and `y`. It first calculates the number of observations where both `x` and `y` are 1, where `x` is 1 and `y` is 0, where `x` is 0 and `y` is 1, and where both `x` and `y` are 0. Then, it calculates the total number of observations. Finally, it calculates the Phi coefficient using the formula and returns it rounded to 4 decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # The Hard Sigmoid function is defined as:\n    # 0 if x < -2.5\n    # 1 if x > 2.5\n    # 0.2x + 0.5 otherwise\n    if x < -2.5:\n        # If x is less than -2.5, return 0\n        return 0.0\n    elif x > 2.5:\n        # If x is greater than 2.5, return 1\n        return 1.0\n    else:\n        # Otherwise, return 0.2x + 0.5\n        return 0.2 * x + 0.5\n\n# Example usage:\nprint(hard_sigmoid(-3.0))  # Output: 0.0\nprint(hard_sigmoid(3.0))   # Output: 1.0\nprint(hard_sigmoid(0.0))   # Output: 0.5\nprint(hard_sigmoid(1.0))   # Output: 0.7\n```\n\nThis implementation directly follows the definition of the Hard Sigmoid function, which is a piecewise function with three different cases depending on the input value `x`. The function returns `0` if `x` is less than `-2.5`, `1` if `x` is greater than `2.5`, and `0.2x + 0.5` otherwise. The example usage demonstrates how to use this function with different input values."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    # If the input value is greater than 0, return the input value\n    if x > 0:\n        return x\n    # If the input value is less than or equal to 0, return alpha * (exp(x) - 1)\n    else:\n        return alpha * (math.exp(x) - 1)\n\n# Example usage:\nprint(round(elu(2.0), 4))  # Output: 2.0\nprint(round(elu(-2.0), 4))  # Output: -0.8647\nprint(round(elu(-2.0, alpha=0.5), 4))  # Output: -0.4323\n```\n\n### Explanation\n\n*   The ELU activation function is defined as `f(x) = x` if `x > 0` and `f(x) = alpha * (exp(x) - 1)` if `x <= 0`.\n*   The `elu` function takes two parameters: `x` (the input value) and `alpha` (the ELU parameter for negative values, defaulting to 1.0).\n*   If `x` is greater than 0, the function returns `x`.\n*   If `x` is less than or equal to 0, the function returns `alpha * (exp(x) - 1)`, where `exp(x)` is the exponential function.\n*   The result is rounded to the 4th decimal place using the `round` function.\n\n### Advice\n\n*   The ELU activation function is often used in deep learning models to introduce non-linearity and help the model learn more complex relationships between inputs and outputs.\n*   The `alpha` parameter controls the output of the ELU function for negative inputs. A higher value of `alpha` will result in a larger output for negative inputs.\n*   The ELU function is differentiable, making it suitable for use in neural networks where gradients are computed during backpropagation."}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    # If x is greater than or equal to 0, return x (same as ReLU)\n    if x >= 0:\n        return x\n    # If x is less than 0, return alpha * x\n    else:\n        return alpha * x\n\n# Example usage:\nprint(prelu(2.0))  # Output: 2.0\nprint(prelu(-2.0))  # Output: -0.5\nprint(prelu(-2.0, alpha=0.5))  # Output: -1.0\n```\n\nIn this code:\n\n*   We define the `prelu` function with two parameters: `x` (the input value) and `alpha` (the slope parameter for negative values, defaulting to 0.25).\n*   Inside the function, we use a simple if-else statement to apply the PReLU activation function:\n    *   If `x` is greater than or equal to 0, we return `x` (same as the ReLU activation function).\n    *   If `x` is less than 0, we return `alpha * x`, which introduces the learnable parameter `alpha` for negative inputs.\n*   We provide example usage of the `prelu` function with different inputs and `alpha` values to demonstrate its behavior."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # To prevent numerical overflow, we use the property of the exponential function\n    # that e^x = e^(x - max_x + max_x) = e^max_x * e^(x - max_x)\n    # where max_x is a large value that can be represented by the system.\n    # We choose max_x as 0, so the expression becomes e^x = e^0 * e^(x - 0) = 1 * e^x\n    # However, for large negative values of x, e^x can be very small and cause underflow.\n    # To prevent this, we use the property of the logarithm function that\n    # log(1 + e^x) = log(e^x * (1 + e^(-x))) = x + log(1 + e^(-x))\n    # This way, we avoid calculating e^x directly for large negative x.\n\n    # Check if x is very large and negative to prevent underflow\n    if x < -100:\n        # For very large negative x, e^x is approximately 0, so log(1 + e^x) is approximately 0\n        return 0.0\n\n    # Check if x is very large and positive to prevent overflow\n    if x > 100:\n        # For very large positive x, log(1 + e^x) is approximately x\n        return x\n\n    # For other values of x, we can safely calculate log(1 + e^x)\n    # We use the math library to calculate the exponential and logarithm\n    return round(math.log(1 + math.exp(x)), 4)\n\n\n# Example usage:\nprint(softplus(10))  # Output: 10.0\nprint(softplus(-10))  # Output: 0.0000\nprint(softplus(0))  # Output: 0.6931\n```\n\nIn this code:\n\n*   We handle edge cases where `x` is very large and negative or positive to prevent numerical underflow or overflow.\n*   For very large negative `x`, we approximate `log(1 + e^x)` as 0.\n*   For very large positive `x`, we approximate `log(1 + e^x)` as `x`.\n*   For other values of `x`, we safely calculate `log(1 + e^x)` using the `math` library.\n*   The result is rounded to the 4th decimal place as required."}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # The Softsign activation function is defined as x / (1 + |x|), \n    # where |x| is the absolute value of x. This function is smooth and \n    # bounded between -1 and 1.\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage:\nprint(softsign(2.0))  # Output: 0.6667\nprint(softsign(-2.0))  # Output: -0.6667\nprint(softsign(0.0))  # Output: 0.0\n```\n\nIn this code:\n\n*   We define the `softsign` function that takes a float input `x` and returns the Softsign of `x`.\n*   Inside the function, we calculate the Softsign using the formula `x / (1 + |x|)`, where `|x|` is the absolute value of `x`.\n*   We use the `abs` function to get the absolute value of `x`.\n*   Finally, we round the result to the 4th decimal place using the `round` function and return it.\n\nThe Softsign activation function is useful in neural networks because it is smooth and bounded between -1 and 1, making it suitable for output layers where the target values are in this range."}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    \n    # Calculate the clipped likelihood ratios\n    clipped_rhos = np.minimum(np.maximum(rhos, 1 - epsilon), 1 + epsilon)\n    \n    # Calculate the policy objective (L_CLIP)\n    L_CLIP = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # Calculate the KL divergence penalty (D_KL)\n    D_KL = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Calculate the GRPO objective (L_GRPO)\n    L_GRPO = L_CLIP - beta * D_KL\n    \n    # Round the result to the nearest 6th decimal\n    L_GRPO = round(L_GRPO, 6)\n    \n    return L_GRPO\n\n# Example usage\nrhos = np.array([0.5, 0.7, 0.9])\nA = np.array([1.0, 2.0, 3.0])\npi_theta_old = np.array([0.2, 0.3, 0.5])\npi_theta_ref = np.array([0.1, 0.4, 0.5])\n\nresult = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(result)\n```\n\nThe provided code implements the GRPO objective function, which is used to optimize policy parameters in reinforcement learning. It calculates the clipped likelihood ratios, policy objective (L_CLIP), KL divergence penalty (D_KL), and finally the GRPO objective (L_GRPO). The result is rounded to the nearest 6th decimal as required. The example usage demonstrates how to use the `grpo_objective` function with sample inputs."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # The Swish activation function is defined as x * sigmoid(x)\n    # We use the sigmoid function from the math library for simplicity\n    # However, since sigmoid is not directly available for float in math library, we define it manually\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    \n    # Now we can compute the Swish value\n    swish_value = x * sigmoid_x\n    \n    # Round the result to the nearest 4th decimal\n    swish_value = round(swish_value, 4)\n    \n    return swish_value\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 1.0\n    print(f\"Swish value for {x}: {swish(x)}\")\n```\n\n### Explanation\n\n*   The Swish activation function is a self-gated activation function that has shown superior performance in deep neural networks compared to ReLU.\n*   It is defined as `x * sigmoid(x)`, where `sigmoid(x)` is the sigmoid function.\n*   The sigmoid function is defined as `1 / (1 + exp(-x))`.\n*   We use the `math.exp` function to compute the exponential of `-x`.\n*   We then compute the Swish value by multiplying `x` with the sigmoid of `x`.\n*   Finally, we round the result to the nearest 4th decimal using the `round` function.\n\n### Advice\n\n*   The Swish activation function can be used as a replacement for ReLU in deep neural networks.\n*   It has been shown to perform better than ReLU in some cases, especially in deeper networks.\n*   However, it is computationally more expensive than ReLU due to the computation of the sigmoid function.\n*   The Swish activation function can be used in conjunction with other activation functions, such as ReLU and tanh, to create more complex activation functions."}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # Calculate the SELU value\n    if x >= 0:\n        # For positive inputs, SELU is equal to the input\n        return scale * x\n    else:\n        # For negative inputs, SELU is equal to the scaled exponential of the input\n        return scale * (alpha * (math.exp(x) - 1))\n\n# Example usage:\nif __name__ == \"__main__\":\n    x_values = [1.0, -1.0, 0.5, -0.5]\n    for x in x_values:\n        result = round(selu(x), 4)\n        print(f\"SELU({x}) = {result}\")\n```\n\n**Explanation:**\n\n*   The SELU activation function is defined as `SELU(x) = scale * x` for `x >= 0` and `SELU(x) = scale * (alpha * (exp(x) - 1))` for `x < 0`.\n*   We use the `math.exp` function to calculate the exponential of the input `x`.\n*   The `alpha` and `scale` values are predefined constants that are used in the SELU activation function.\n*   The result is rounded to the nearest 4th decimal using the `round` function.\n\n**Note:** The SELU activation function is a self-normalizing variant of ELU, which means it can help to normalize the activations of the neurons in a neural network. This can lead to faster convergence and more stable training."}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the weighted sum of the input features and the model weights\n    weighted_sum = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to compute class probabilities\n    probabilities = 1 / (1 + np.exp(-weighted_sum))\n    \n    # Return binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return the predictions as a list\n    return predictions.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input feature matrix\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    \n    # Define the model weights and bias\n    weights = np.array([0.1, 0.2])\n    bias = 0.5\n    \n    # Make predictions\n    predictions = predict_logistic(X, weights, bias)\n    \n    # Print the predictions\n    print(predictions)\n```\n\nThis code defines the `predict_logistic` function, which takes in the input feature matrix `X`, the model weights `weights`, and the model bias `bias`. It computes the weighted sum of the input features and the model weights, applies the sigmoid function to compute class probabilities, and returns binary predictions based on a threshold of 0.5.\n\nThe example usage demonstrates how to use the `predict_logistic` function with a sample input feature matrix, model weights, and model bias. The predictions are printed to the console.\n\nThe output of the example usage will be a list of binary predictions, where each prediction is either 0 or 1. The actual values will depend on the input data and the model parameters."}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n    X (np.ndarray): Input features\n    y (np.ndarray): Target labels\n    learning_rate (float): Learning rate for gradient descent\n    iterations (int): Number of iterations for training\n\n    Returns:\n        B (list[float]): Updated parameter vector rounded to 4 floating points\n        losses (list[float]): Collected values of Cross Entropy loss rounded to 4 floating points\n    \"\"\"\n    \n    # Initialize parameters\n    num_samples, num_features = X.shape\n    num_classes = np.max(y) + 1\n    B = np.zeros((num_features, num_classes))\n    \n    # Initialize loss collection\n    losses = []\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Compute scores\n        scores = np.dot(X, B)\n        \n        # Compute softmax probabilities\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        \n        # Compute cross-entropy loss\n        loss = -np.mean(np.log(probabilities[np.arange(num_samples), y]))\n        losses.append(round(loss, 4))\n        \n        # Compute gradient\n        dscores = probabilities\n        dscores[np.arange(num_samples), y] -= 1\n        dB = np.dot(X.T, dscores) / num_samples\n        \n        # Update parameters\n        B -= learning_rate * dB\n    \n    # Round and return parameters and losses\n    B = np.round(B, 4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    \n    return B, losses\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 3, 100)\n    \n    # Train softmax regression\n    learning_rate = 0.01\n    iterations = 1000\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    \n    # Print results\n    print(\"Optimized coefficients (B):\")\n    print(B)\n    print(\"Collected loss values:\")\n    print(losses)\n```\n\nThis code implements a gradient descent-based training algorithm for Softmax regression. It takes in the input features `X`, target labels `y`, learning rate, and number of iterations as parameters. The function returns the optimized coefficients `B` and the collected loss values over iterations.\n\nThe code first initializes the parameters and loss collection. Then, it performs gradient descent for the specified number of iterations. In each iteration, it computes the scores, softmax probabilities, cross-entropy loss, and gradient. The parameters are updated based on the gradient and learning rate.\n\nFinally, the code rounds the optimized coefficients and loss values to the nearest 4th decimal and returns them as lists. The example usage demonstrates how to train softmax regression using this function and print the results."}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid activation function.\n    \n    Args:\n    x (np.ndarray): Input array.\n    \n    Returns:\n    np.ndarray: Sigmoid of input array.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Binary Cross Entropy loss function.\n    \n    Args:\n    y_true (np.ndarray): True labels.\n    y_pred (np.ndarray): Predicted probabilities.\n    \n    Returns:\n    float: Binary Cross Entropy loss.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], np.ndarray]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations.\n    \n    Returns:\n    tuple[list[float], np.ndarray]: A tuple containing the list of loss values at each iteration and the optimized coefficients.\n    \"\"\"\n    # Initialize coefficients with zeros\n    coefficients = np.zeros(X.shape[1])\n    \n    # Initialize list to store loss values\n    loss_values = []\n    \n    # Gradient descent iterations\n    for _ in range(iterations):\n        # Predict probabilities\n        probabilities = sigmoid(np.dot(X, coefficients))\n        \n        # Compute Binary Cross Entropy loss\n        loss = binary_cross_entropy(y, probabilities)\n        \n        # Store loss value\n        loss_values.append(round(loss, 4))\n        \n        # Compute gradient\n        gradient = np.dot(X.T, (probabilities - y)) / X.shape[0]\n        \n        # Update coefficients\n        coefficients -= learning_rate * gradient\n    \n    # Return loss values and optimized coefficients\n    return loss_values, coefficients\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 2, 100)\n    \n    # Train logistic regression model\n    learning_rate = 0.01\n    iterations = 1000\n    loss_values, coefficients = train_logreg(X, y, learning_rate, iterations)\n    \n    # Print results\n    print(\"Loss Values:\", np.array(loss_values).tolist())\n    print(\"Optimized Coefficients:\", coefficients.tolist())\n```\n\nIn this code:\n\n*   We define a `sigmoid` function to compute the sigmoid of an input array.\n*   We define a `binary_cross_entropy` function to compute the Binary Cross Entropy loss between true labels and predicted probabilities.\n*   In the `train_logreg` function, we initialize coefficients with zeros and a list to store loss values.\n*   We perform gradient descent iterations, predicting probabilities, computing Binary Cross Entropy loss, storing loss values, computing the gradient, and updating coefficients.\n*   Finally, we return the list of loss values and the optimized coefficients.\n\nThe example usage demonstrates how to train a logistic regression model using the `train_logreg` function and print the loss values and optimized coefficients."}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    \n    # Apply attention mask\n    attention_scores = attention_scores + (1 - mask) * -1e9\n    \n    # Compute attention weights\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    \n    # Compute output\n    output = np.dot(attention_weights, V)\n    \n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define input and weights\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    W_q = np.array([[0.1, 0.2], [0.3, 0.4]])\n    W_k = np.array([[0.5, 0.6], [0.7, 0.8]])\n    W_v = np.array([[0.9, 1.0], [1.1, 1.2]])\n    mask = np.array([[1, 1, 0], [1, 1, 1], [0, 1, 1]])\n\n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Compute masked self-attention\n    output = masked_attention(Q, K, V, mask)\n\n    # Print output\n    print(output.tolist())\n```\n\nIn this code:\n\n*   We first compute the Query (Q), Key (K), and Value (V) matrices using the `compute_qkv` function.\n*   Then, we compute the masked self-attention using the `masked_attention` function.\n*   In the `masked_attention` function, we first compute the attention scores by taking the dot product of Q and K.T, and then scaling by the square root of the key's dimensionality.\n*   We apply the attention mask by adding a large negative value to the attention scores where the mask is 0.\n*   We compute the attention weights by taking the softmax of the attention scores.\n*   Finally, we compute the output by taking the dot product of the attention weights and V.\n*   The output is returned as a Python list after reshaping using numpy's `tolist()` method."}
{"task_id": 108, "completion_id": 0, "completion": "```python\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is calculated as the ratio of the number of unique colors to the total number of apples.\n    This measure satisfies the given properties:\n    - If all apples are the same color, the disorder is 0.\n    - As the variety of colors increases, the disorder increases.\n    \n    Args:\n        apples (list): A list of integers representing the colors of the apples.\n    \n    Returns:\n        float: A measure of disorder in the basket of apples, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Calculate the total number of apples\n    total_apples = len(apples)\n    \n    # If there are no apples, the disorder is 0\n    if total_apples == 0:\n        return 0.0\n    \n    # Calculate the number of unique colors\n    unique_colors = len(set(apples))\n    \n    # Calculate the disorder as the ratio of unique colors to total apples\n    disorder_measure = unique_colors / total_apples\n    \n    # Return the disorder measure rounded to the nearest 4th decimal\n    return round(disorder_measure, 4)\n\n\n# Example usage:\nprint(disorder([0, 0, 0, 0]))  # Output: 0.0\nprint(disorder([1, 1, 0, 0]))  # Output: 0.5\nprint(disorder([0, 1, 2, 3]))  # Output: 1.0\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Output: 0.375\n```\n\nThis solution uses the ratio of unique colors to total apples as a measure of disorder. This measure satisfies the given properties and is easy to calculate. The `set` function is used to find the unique colors, and the `len` function is used to count the total number of apples and unique colors. The result is then rounded to the nearest 4th decimal using the `round` function."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    This function performs Layer Normalization on an input tensor.\n\n    Parameters:\n    X (np.ndarray): A 3D array representing batch size, sequence length, and feature dimensions.\n    gamma (np.ndarray): Scaling parameters.\n    beta (np.ndarray): Shifting parameters.\n    epsilon (float): A small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: The normalized X, rounded to 5 decimal places and converted to a list.\n    \"\"\"\n\n    # Calculate the mean of X across the feature dimension (axis=2)\n    mean = np.mean(X, axis=2, keepdims=True)\n    \n    # Calculate the variance of X across the feature dimension (axis=2)\n    variance = np.var(X, axis=2, keepdims=True)\n    \n    # Calculate the standard deviation of X\n    std = np.sqrt(variance + epsilon)\n    \n    # Normalize X\n    normalized_X = (X - mean) / std\n    \n    # Apply scaling and shifting parameters\n    normalized_X = gamma * normalized_X + beta\n    \n    # Round the result to 5 decimal places and convert to a list\n    result = np.round(normalized_X, 5).tolist()\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a 3D array representing batch size, sequence length, and feature dimensions\n    X = np.random.rand(2, 3, 4)\n    \n    # Define scaling and shifting parameters\n    gamma = np.array([1.0, 2.0, 3.0, 4.0])\n    beta = np.array([0.1, 0.2, 0.3, 0.4])\n    \n    # Perform Layer Normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    print(normalized_X)\n```\n\nIn this code:\n\n*   We first calculate the mean and variance of the input tensor `X` across the feature dimension (axis=2).\n*   We then calculate the standard deviation of `X` by taking the square root of the variance and adding a small value `epsilon` for numerical stability.\n*   Next, we normalize `X` by subtracting the mean and dividing by the standard deviation.\n*   After normalization, we apply the scaling and shifting parameters `gamma` and `beta` to the normalized `X`.\n*   Finally, we round the result to 5 decimal places and convert it to a list before returning it.\n\nThe example usage demonstrates how to use the `layer_normalization` function with a random 3D array `X` and predefined scaling and shifting parameters `gamma` and `beta`."}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float, optional): The weight for precision. Defaults to 0.9.\n        beta (float, optional): The penalty factor for word order fragmentation. Defaults to 3.\n        gamma (float, optional): The weight for recall. Defaults to 0.5.\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    # Split the reference and candidate translations into words\n    reference_words = reference.split()\n    candidate_words = candidate.split()\n\n    # Calculate the number of unigram matches\n    matches = sum((Counter(reference_words) & Counter(candidate_words)).values())\n\n    # Calculate precision\n    precision = matches / len(candidate_words) if candidate_words else 0\n\n    # Calculate recall\n    recall = matches / len(reference_words) if reference_words else 0\n\n    # Calculate F-mean\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if (alpha * precision + (1 - alpha) * recall) != 0 else 0\n\n    # Calculate the penalty for word order fragmentation\n    # This is a simplified version and may not be as accurate as the original METEOR implementation\n    penalty = gamma * (len(candidate_words) - matches) / len(candidate_words) if candidate_words else 0\n\n    # Calculate the METEOR score\n    score = f_mean * (1 - penalty)\n\n    # Return the METEOR score rounded to 3 decimal places\n    return round(score, 3)\n\n\n# Example usage:\nreference = \"This is a test sentence\"\ncandidate = \"This is a test sentence with some extra words\"\nprint(meteor_score(reference, candidate))\n```\n\nIn the provided code:\n\n*   We define the `meteor_score` function with parameters for the reference translation, candidate translation, and weights for precision, recall, and penalty.\n*   We split the reference and candidate translations into words using the `split` method.\n*   We calculate the number of unigram matches by taking the intersection of the word counts in the reference and candidate translations.\n*   We calculate precision, recall, and F-mean using the standard formulas.\n*   We calculate a simplified penalty for word order fragmentation based on the difference between the candidate translation length and the number of matches.\n*   We calculate the METEOR score by multiplying the F-mean with the penalty.\n*   Finally, we return the METEOR score rounded to 3 decimal places.\n\nYou can modify the `alpha`, `beta`, and `gamma` parameters to adjust the weights for precision, recall, and penalty according to your specific requirements."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, \n    their individual counts, and the total number of samples.\n\n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The individual count of event X.\n    total_counts_y (int): The individual count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information (PMI) rounded to 3 decimal places.\n    \"\"\"\n\n    # Calculate the expected joint occurrence count under independence assumption\n    expected_joint_counts = (total_counts_x / total_samples) * (total_counts_y / total_samples) * total_samples\n    \n    # Check for division by zero\n    if expected_joint_counts == 0:\n        return 0.0\n    \n    # Calculate the Pointwise Mutual Information (PMI)\n    pmi = np.log2(joint_counts / expected_joint_counts)\n    \n    # Return the PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\njoint_counts = 100\ntotal_counts_x = 500\ntotal_counts_y = 600\ntotal_samples = 10000\n\npmi = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\nprint(\"Pointwise Mutual Information (PMI):\", pmi)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of floats representing the normalized values, \n        rounded to 4 decimal places.\n    \"\"\"\n    \n    # Check if the input list is empty\n    if not x:\n        return []\n\n    # Find the minimum value in the list\n    min_val = min(x)\n    \n    # Find the maximum value in the list\n    max_val = max(x)\n    \n    # Check if all values in the list are the same\n    if max_val == min_val:\n        return [0.0] * len(x)\n    \n    # Perform Min-Max Normalization\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    \n    # Round the normalized values to 4 decimal places\n    normalized = [round(val, 4) for val in normalized]\n    \n    return normalized\n\n# Example usage:\nnumbers = [10, 20, 30, 40, 50]\nprint(min_max(numbers))\n```\n\nIn this code:\n\n*   We first check if the input list `x` is empty. If it is, we return an empty list.\n*   We find the minimum and maximum values in the list using the built-in `min` and `max` functions.\n*   If all values in the list are the same (i.e., `max_val == min_val`), we return a list of zeros, since there's no range to normalize.\n*   We perform Min-Max Normalization using a list comprehension, where each value `i` in the list is normalized to the range \\[0, 1] using the formula `(i - min_val) / (max_val - min_val)`.\n*   Finally, we round the normalized values to 4 decimal places using another list comprehension and return the result."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): The 1D input array.\n    w1 (np.ndarray): The weights for the first layer.\n    w2 (np.ndarray): The weights for the second layer.\n    \n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    \n    # First, we apply the matrix multiplication with the first set of weights\n    # We use np.matmul() for matrix multiplication\n    out1 = np.matmul(x, w1)\n    \n    # Then, we apply the ReLU activation function to the output\n    # We use np.maximum() to implement ReLU, which returns the element-wise maximum between the input and 0\n    out1 = np.maximum(out1, 0)\n    \n    # Next, we apply the matrix multiplication with the second set of weights\n    out2 = np.matmul(out1, w2)\n    \n    # Then, we apply the ReLU activation function to the output\n    out2 = np.maximum(out2, 0)\n    \n    # Now, we add the original input via a shortcut connection\n    out2 = out2 + x\n    \n    # Finally, we apply the ReLU activation function to the output\n    out2 = np.maximum(out2, 0)\n    \n    # We round the output to 4 decimal places and convert it to a list\n    out2 = np.round(out2, 4).tolist()\n    \n    return out2\n\n# Example usage:\nx = np.array([1, 2, 3])\nw1 = np.array([[1, 2], [3, 4], [5, 6]])\nw2 = np.array([[7, 8, 9], [10, 11, 12]])\n\nprint(residual_block(x, w1, w2))\n```\n\nThis function implements a simple residual block using NumPy. It takes a 1D input array `x` and two sets of weights `w1` and `w2` as input, applies two weight layers with ReLU activations, adds the original input via a shortcut connection, and finally applies a ReLU activation before returning the output. The output is rounded to 4 decimal places and converted to a list before being returned."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This function performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Args:\n        x (np.ndarray): A 3D NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n        np.ndarray: A 1D NumPy array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    \n    # Check if the input is a 3D NumPy array\n    if len(x.shape)!= 3:\n        raise ValueError(\"Input must be a 3D NumPy array\")\n    \n    # Calculate the average of each feature map (channel) by taking the mean along the first two axes (height and width)\n    # The result will be a 1D array where each element is the average of all values in the corresponding feature map\n    return np.mean(x, axis=(0, 1))\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a 3D NumPy array representing feature maps\n    x = np.random.rand(10, 10, 5)  # shape: (height, width, channels)\n    \n    # Perform Global Average Pooling\n    pooled_x = global_avg_pool(x)\n    \n    print(\"Input shape:\", x.shape)\n    print(\"Output shape:\", pooled_x.shape)\n    print(\"Pooled feature maps:\", pooled_x)\n```\n\nIn this code:\n\n*   We define a function `global_avg_pool` that takes a 3D NumPy array `x` as input and returns a 1D NumPy array.\n*   We check if the input is a 3D NumPy array and raise a `ValueError` if it's not.\n*   We use `np.mean` to calculate the average of each feature map (channel) by taking the mean along the first two axes (height and width).\n*   In the example usage, we create a 3D NumPy array `x` representing feature maps, perform Global Average Pooling using the `global_avg_pool` function, and print the input and output shapes along with the pooled feature maps."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    This function performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n    \n    Parameters:\n    X (np.ndarray): A 4D NumPy array representing a batch of feature maps in the BCHW format.\n    gamma (np.ndarray): A 1D NumPy array representing the scale parameters for each channel.\n    beta (np.ndarray): A 1D NumPy array representing the shift parameters for each channel.\n    epsilon (float): A small value added to the variance for numerical stability. Default is 1e-5.\n    \n    Returns:\n    list: A list of the normalized feature maps, rounded to 4 decimal places.\n    \"\"\"\n\n    # Calculate the mean of the input across the batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Calculate the variance of the input across the batch and spatial dimensions for each channel\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input across the batch and spatial dimensions for each channel\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Apply scale (gamma) and shift (beta) parameters\n    output = gamma * normalized_X + beta\n    \n    # Round the output to 4 decimal places and convert it to a list\n    output = np.round(output, 4).tolist()\n    \n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a random 4D NumPy array\n    X = np.random.rand(2, 3, 4, 5)\n    \n    # Create random scale and shift parameters\n    gamma = np.random.rand(3)\n    beta = np.random.rand(3)\n    \n    # Perform batch normalization\n    output = batch_normalization(X, gamma, beta)\n    \n    print(output)\n```\n\nIn this code:\n\n*   We first calculate the mean of the input `X` across the batch and spatial dimensions for each channel using `np.mean`.\n*   Then, we calculate the variance of the input `X` across the batch and spatial dimensions for each channel using `np.var`.\n*   Next, we normalize the input `X` across the batch and spatial dimensions for each channel by subtracting the mean and dividing by the square root of the variance plus `epsilon` for numerical stability.\n*   After normalization, we apply the scale (`gamma`) and shift (`beta`) parameters to the normalized input.\n*   Finally, we round the output to 4 decimal places using `np.round` and convert it to a list using the `tolist` method before returning it."}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at the given point, rounded to 4 decimal places.\n    \"\"\"\n    \n    # Check if the exponent is zero, because the derivative of a constant is zero\n    if n == 0:\n        return 0.0\n    \n    # Apply the power rule of differentiation: if f(x) = c * x^n, then f'(x) = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1))\n    \n    # Round the result to 4 decimal places\n    derivative = round(derivative, 4)\n    \n    return derivative\n\n# Example usage:\nprint(poly_term_derivative(2.0, 3.0, 2.0))  # Output: 12.0\nprint(poly_term_derivative(1.0, 2.0, 3.0))  # Output: 12.0\nprint(poly_term_derivative(4.0, 1.0, 1.0))  # Output: 4.0\n```\n\nIn this code:\n\n*   We define a function `poly_term_derivative` that takes the coefficient `c`, the point `x`, and the exponent `n` as input.\n*   We check if the exponent `n` is zero. If it is, we return 0.0, because the derivative of a constant is zero.\n*   We apply the power rule of differentiation to compute the derivative of the polynomial term.\n*   We round the result to 4 decimal places using the `round` function.\n*   We provide example usage of the function with different inputs."}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): A tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n\n    # Convert the input list to a numpy array for easier vector operations\n    vectors = np.array(vectors)\n\n    # Initialize an empty list to store the orthonormal basis\n    basis = []\n\n    # Iterate over each vector in the input list\n    for vector in vectors:\n        # Initialize a flag to check if the vector is linearly independent\n        is_independent = True\n\n        # Iterate over each basis vector\n        for basis_vector in basis:\n            # Calculate the projection of the vector onto the basis vector\n            projection = np.dot(vector, basis_vector)\n\n            # If the projection is close to the norm of the vector, it's not linearly independent\n            if np.abs(projection - np.linalg.norm(vector)) < tol:\n                is_independent = False\n                break\n\n        # If the vector is linearly independent, add it to the basis\n        if is_independent:\n            # Subtract the projections onto the existing basis vectors\n            for basis_vector in basis:\n                projection = np.dot(vector, basis_vector)\n                vector -= projection * basis_vector\n\n            # Normalize the vector to have unit length\n            vector = vector / np.linalg.norm(vector)\n\n            # Add the orthonormal vector to the basis\n            basis.append(vector)\n\n    # Round each basis vector to 4 decimal places and convert to a list\n    basis = [vector.round(4).tolist() for vector in basis]\n\n    return basis\n\n# Example usage:\nvectors = [[1, 0], [0, 1], [1, 1]]\ntol = 1e-10\nprint(orthonormal_basis(vectors, tol))\n```\n\nThis code implements the Gram-Schmidt process to compute an orthonormal basis for the subspace spanned by a list of 2D vectors. It takes a list of 2D vectors and a tolerance value as input, and returns a list of orthonormal vectors that span the same subspace. The output basis vectors are rounded to 4 decimal places and converted to lists before being returned."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or numpy.ndarray): The first 3D vector.\n        b (list or numpy.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n\n    # Check if inputs are 3D vectors\n    if len(a) != 3 or len(b) != 3:\n        raise ValueError(\"Both inputs must be 3D vectors\")\n\n    # Compute the cross product using numpy's cross function\n    cross_product_result = np.cross(a, b)\n\n    # Round the result to 4 decimal places and convert to a list\n    result = np.round(cross_product_result, 4).tolist()\n\n    return result\n\n# Example usage:\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\nprint(cross_product(vector_a, vector_b))\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n    A (numpy array): A square coefficient matrix.\n    b (numpy array): A constant vector.\n\n    Returns:\n    list: The solution vector x if the system has a unique solution, -1 otherwise.\n    \"\"\"\n    \n    # Calculate the determinant of matrix A\n    det_A = np.linalg.det(A)\n    \n    # Check if the determinant is zero\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column of matrix A\n    for i in range(len(A)):\n        # Create a copy of matrix A\n        A_copy = A.copy()\n        \n        # Replace the current column with vector b\n        A_copy[:, i] = b\n        \n        # Calculate the determinant of the modified matrix\n        det_A_copy = np.linalg.det(A_copy)\n        \n        # Calculate the solution for the current variable\n        x_i = det_A_copy / det_A\n        \n        # Append the solution to the solution vector\n        x.append(x_i)\n    \n    # Round the solution vector to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Return the solution vector as a list\n    return x.tolist()\n\n# Example usage:\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])\nprint(cramers_rule(A, b))\n```\n\nIn this code:\n\n*   We first calculate the determinant of the coefficient matrix `A` using `np.linalg.det(A)`.\n*   If the determinant is zero, we return -1, indicating that the system has no unique solution.\n*   Otherwise, we iterate over each column of matrix `A`, replace the current column with vector `b`, and calculate the determinant of the modified matrix.\n*   We then calculate the solution for each variable using Cramer's Rule and append it to the solution vector `x`.\n*   Finally, we round the solution vector to the nearest 4th decimal using `np.round(x, 4)` and return it as a list using `x.tolist()`."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Args:\n    p (list[float]): The first discrete probability distribution.\n    q (list[float]): The second discrete probability distribution.\n\n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places.\n    \"\"\"\n    \n    # Check if the inputs have different lengths or are empty\n    if len(p)!= len(q) or len(p) == 0:\n        return 0.0\n    \n    # Convert the lists to numpy arrays for efficient computation\n    p = np.array(p)\n    q = np.array(q)\n    \n    # Check if the inputs are valid probability distributions (non-negative and sum to 1)\n    if np.any(p < 0) or np.any(q < 0) or not np.isclose(np.sum(p), 1) or not np.isclose(np.sum(q), 1):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p * q))\n    \n    # Calculate the Bhattacharyya distance\n    distance = -np.log(bc)\n    \n    # Return the Bhattacharyya distance rounded to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]\nprint(bhattacharyya_distance(p, q))\n```\n\nIn this code:\n\n*   We first check if the input lists `p` and `q` have the same length and are not empty. If they don't or are empty, we return 0.0.\n*   We then convert the input lists to numpy arrays for efficient computation.\n*   Next, we check if the inputs are valid probability distributions by ensuring that all elements are non-negative and the sum of each distribution is close to 1. If they're not valid, we return 0.0.\n*   After that, we calculate the Bhattacharyya coefficient `bc` by taking the sum of the square root of the element-wise product of `p` and `q`.\n*   We then calculate the Bhattacharyya distance by taking the negative logarithm of the Bhattacharyya coefficient.\n*   Finally, we return the Bhattacharyya distance rounded to 4 decimal places using the `round` function."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    This function computes the element-wise sum of two vectors.\n    \n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n    \n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, \n                         or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        # If not, return -1 as the vectors have incompatible dimensions\n        return -1\n    \n    # Initialize an empty list to store the result\n    result = []\n    \n    # Iterate over the indices of the first vector\n    for i in range(len(a)):\n        # For each index, add the corresponding elements of the two vectors\n        result.append(a[i] + b[i])\n    \n    # Return the resulting vector\n    return result\n\n# Example usage:\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nprint(vector_sum(vector1, vector2))  # Output: [5, 7, 9]\n\nvector3 = [1, 2]\nvector4 = [3, 4, 5]\nprint(vector_sum(vector3, vector4))  # Output: -1\n```\n\nAlternatively, you can use a list comprehension to make the code more concise:\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    This function computes the element-wise sum of two vectors.\n    \n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n    \n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, \n                         or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        # If not, return -1 as the vectors have incompatible dimensions\n        return -1\n    \n    # Use a list comprehension to add corresponding elements of the two vectors\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage:\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nprint(vector_sum(vector1, vector2))  # Output: [5, 7, 9]\n\nvector3 = [1, 2]\nvector4 = [3, 4, 5]\nprint(vector_sum(vector3, vector4))  # Output: -1\n```\n\nYou can also use the `numpy` library, which provides a more efficient and convenient way to perform element-wise operations on vectors:\n\n```python\nimport numpy as np\n\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    This function computes the element-wise sum of two vectors.\n    \n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n    \n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, \n                         or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        # If not, return -1 as the vectors have incompatible dimensions\n        return -1\n    \n    # Convert the lists to numpy arrays and add them element-wise\n    return np.add(a, b).tolist()\n\n# Example usage:\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nprint(vector_sum(vector1, vector2))  # Output: [5, 7, 9]\n\nvector3 = [1, 2]\nvector4 = [3, 4, 5]\nprint(vector_sum(vector3, vector4))  # Output: -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list:\n    \"\"\"\n    Compute the average policy gradient using the REINFORCE algorithm.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape (num_states, num_actions) representing the policy parameters.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list: The average policy gradient as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Initialize the sum of gradients\n    sum_gradients = np.zeros_like(theta)\n\n    # Iterate over each episode\n    for episode in episodes:\n        # Initialize the return for the episode\n        episode_return = 0\n\n        # Iterate over each time step in the episode in reverse order\n        for t in range(len(episode) - 1, -1, -1):\n            state, action, reward = episode[t]\n\n            # Compute the return for the current time step\n            episode_return = reward + episode_return\n\n            # Compute the policy for the current state\n            policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n\n            # Compute the log-policy for the current action\n            log_policy = np.log(policy[action])\n\n            # Compute the gradient of the log-policy for the current action\n            gradient = np.zeros_like(theta)\n            gradient[state, :] = -policy\n            gradient[state, action] += 1\n\n            # Multiply the gradient by the return and add it to the sum of gradients\n            sum_gradients += episode_return * gradient\n\n    # Compute the average gradient\n    average_gradient = sum_gradients / len(episodes)\n\n    # Round the average gradient to the nearest 4th decimal and return it as a list\n    return np.round(average_gradient, 4).tolist()\n```\n\n**Example Usage:**\n```python\n# Define the policy parameters\ntheta = np.array([[0.1, 0.2], [0.3, 0.4]])\n\n# Define the episodes\nepisodes = [\n    [(0, 0, 1.0), (0, 1, 2.0)],\n    [(1, 0, 3.0), (1, 1, 4.0)],\n    [(0, 0, 5.0), (0, 1, 6.0)]\n]\n\n# Compute the average policy gradient\naverage_gradient = compute_policy_gradient(theta, episodes)\n\nprint(average_gradient)\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    n_experts (int): The number of experts in the MoE layer.\n    k_active (int): The number of active experts (sparsity).\n    d_in (int): The input dimension.\n    d_out (int): The output dimension.\n\n    Returns:\n    tuple: A tuple containing the FLOPs for the dense layer, the FLOPs for the MoE layer, and the savings percentage.\n    \"\"\"\n\n    # Calculate the FLOPs for the dense layer\n    # A dense layer has d_in * d_out weights and performs d_in * d_out multiplications and d_out additions for each input\n    dense_flops = 2 * d_in * d_out\n\n    # Calculate the FLOPs for the MoE layer\n    # An MoE layer has n_experts * d_in * d_out weights and performs k_active * d_in * d_out multiplications and k_active * d_out additions for each input\n    moe_flops = 2 * k_active * d_in * d_out\n\n    # Calculate the savings percentage\n    # If the MoE layer has more FLOPs than the dense layer, there is no savings\n    if moe_flops >= dense_flops:\n        savings_percentage = 0.0\n    else:\n        savings_percentage = round(((dense_flops - moe_flops) / dense_flops) * 100, 1)\n\n    return round(dense_flops, 1), round(moe_flops, 1), savings_percentage\n\n\n# Example usage:\nn_experts = 10\nk_active = 2\nd_in = 128\nd_out = 128\n\ndense_flops, moe_flops, savings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Dense FLOPs: {dense_flops}\")\nprint(f\"MoE FLOPs: {moe_flops}\")\nprint(f\"Savings percentage: {savings_percentage}%\")\n```\n\nIn this code:\n\n*   We define a function `compute_efficiency` that takes the number of experts, the number of active experts, the input dimension, and the output dimension as arguments.\n*   We calculate the FLOPs for the dense layer and the MoE layer based on the given dimensions and sparsity.\n*   We calculate the savings percentage by comparing the FLOPs of the MoE layer to the dense layer.\n*   We provide an example usage of the function with specific values for the number of experts, active experts, input dimension, and output dimension.\n*   We print the FLOPs for the dense layer, the MoE layer, and the savings percentage."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    This function implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input matrix\n    W_g (np.ndarray): Weight matrix for the gating function\n    W_noise (np.ndarray): Weight matrix for the noise\n    N (np.ndarray): Pre-sampled noise\n    k (int): Sparsity constraint\n\n    Returns:\n    list: Final gating probabilities matrix reshaped as a list\n    \"\"\"\n\n    # Compute the raw gating scores by taking the dot product of the input matrix and the weight matrix\n    raw_gating_scores = np.dot(X, W_g)\n\n    # Add noise to the raw gating scores\n    noisy_gating_scores = raw_gating_scores + np.dot(N, W_noise)\n\n    # Compute the top-k gating scores for each row\n    top_k_indices = np.argsort(noisy_gating_scores, axis=1)[:, -k:]\n\n    # Create a mask to zero out all but the top-k gating scores\n    mask = np.zeros_like(noisy_gating_scores)\n    np.put_along_axis(mask, top_k_indices[:, None], 1, axis=1)\n\n    # Compute the final gating probabilities by normalizing the top-k gating scores\n    gating_probabilities = (noisy_gating_scores * mask) / np.sum(noisy_gating_scores * mask, axis=1, keepdims=True)\n\n    # Round the gating probabilities to the nearest 4th decimal\n    gating_probabilities = np.round(gating_probabilities, 4)\n\n    # Reshape the gating probabilities matrix as a list\n    final_gating_probabilities = gating_probabilities.tolist()\n\n    return final_gating_probabilities\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the input matrix, weight matrices, pre-sampled noise, and sparsity constraint\n    X = np.random.rand(10, 5)\n    W_g = np.random.rand(5, 3)\n    W_noise = np.random.rand(3, 3)\n    N = np.random.rand(10, 3)\n    k = 2\n\n    # Compute the final gating probabilities matrix\n    final_gating_probabilities = noisy_topk_gating(X, W_g, W_noise, N, k)\n\n    # Print the final gating probabilities matrix\n    print(final_gating_probabilities)\n```\n\nIn this code:\n\n*   We define the `noisy_topk_gating` function that takes the input matrix `X`, weight matrices `W_g` and `W_noise`, pre-sampled noise `N`, and sparsity constraint `k` as parameters.\n*   We compute the raw gating scores by taking the dot product of the input matrix and the weight matrix `W_g`.\n*   We add noise to the raw gating scores by taking the dot product of the pre-sampled noise `N` and the weight matrix `W_noise`.\n*   We compute the top-k gating scores for each row by using `np.argsort` and `np.put_along_axis`.\n*   We create a mask to zero out all but the top-k gating scores.\n*   We compute the final gating probabilities by normalizing the top-k gating scores.\n*   We round the gating probabilities to the nearest 4th decimal using `np.round`.\n*   We reshape the gating probabilities matrix as a list using the `tolist` method.\n*   We provide an example usage of the `noisy_topk_gating` function with random input matrices, weight matrices, pre-sampled noise, and sparsity constraint."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    This function implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    x (np.ndarray): Input tensor.\n    We (np.ndarray): Expert weight matrices.\n    Wg (np.ndarray): Gating weight matrix.\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n\n    Returns:\n    list: The final MoE output as a list after reshaping.\n    \"\"\"\n\n    # Compute the gating probabilities using softmax\n    gating_probabilities = np.exp(np.dot(x, Wg)) / np.sum(np.exp(np.dot(x, Wg)), axis=1, keepdims=True)\n\n    # Select the top-k experts per token\n    top_k_indices = np.argsort(gating_probabilities, axis=1)[:, -top_k:]\n\n    # Initialize the output tensor\n    output = np.zeros((x.shape[0], We.shape[1]))\n\n    # Apply the transformations of the top-k experts and aggregate the results\n    for i in range(x.shape[0]):\n        expert_outputs = np.dot(x[i], We[top_k_indices[i]])\n        output[i] = np.sum(expert_outputs * gating_probabilities[i, top_k_indices[i]][:, np.newaxis], axis=0)\n\n    # Round the results to the nearest 4th decimal\n    output = np.round(output, 4)\n\n    # Return the output as a list after reshaping\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the input tensor, expert weight matrices, and gating weight matrix\n    x = np.random.rand(10, 5)\n    We = np.random.rand(10, 5, 3)\n    Wg = np.random.rand(5, 10)\n\n    # Define the number of experts and the value of k\n    n_experts = 10\n    top_k = 3\n\n    # Compute the MoE output\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```\n\nIn this code:\n\n*   We first compute the gating probabilities using the softmax function.\n*   Then, we select the top-k experts per token based on the gating probabilities.\n*   Next, we apply the transformations of the top-k experts and aggregate the results weighted by the normalized gating probabilities.\n*   Finally, we round the results to the nearest 4th decimal and return the output as a list after reshaping.\n\nThe example usage demonstrates how to use the `moe` function with random input tensors and weight matrices. You can replace these with your own data to compute the MoE output for your specific use case."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    This function performs Group Normalization on a 4D input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (B, C, H, W)\n    gamma (np.ndarray): Learned scale parameter with shape (C,)\n    beta (np.ndarray): Learned shift parameter with shape (C,)\n    num_groups (int): Number of groups to divide the channels into\n    epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    np.ndarray: Normalized tensor with shape (B, C, H, W)\n    \"\"\"\n\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n\n    # Calculate the number of channels per group\n    channels_per_group = C // num_groups\n\n    # Reshape the input tensor to (B, num_groups, channels_per_group, H, W)\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n\n    # Calculate the mean of each group\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n    # Calculate the variance of each group\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize the input tensor\n    normalized_X = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n\n    # Reshape the normalized tensor back to (B, C, H, W)\n    normalized_X = normalized_X.reshape(B, C, H, W)\n\n    # Apply the learned scale (gamma) and shift (beta)\n    normalized_X = gamma.reshape(1, C, 1, 1) * normalized_X + beta.reshape(1, C, 1, 1)\n\n    # Return the normalized tensor as a list, rounded to the nearest 4th decimal\n    return np.round(normalized_X, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate random input tensor\n    X = np.random.rand(1, 12, 32, 32)\n\n    # Generate random gamma and beta\n    gamma = np.random.rand(12)\n    beta = np.random.rand(12)\n\n    # Perform group normalization\n    num_groups = 3\n    result = group_normalization(X, gamma, beta, num_groups)\n\n    print(result)\n```\n\nIn this code:\n\n*   We first reshape the input tensor `X` to `(B, num_groups, channels_per_group, H, W)` to facilitate the calculation of mean and variance for each group.\n*   We calculate the mean and variance of each group using `np.mean` and `np.var` with the `axis` argument set to `(2, 3, 4)` to compute the mean and variance along the channels, height, and width axes.\n*   We normalize the input tensor by subtracting the mean and dividing by the square root of the variance plus a small value `epsilon` for numerical stability.\n*   We apply the learned scale `gamma` and shift `beta` to the normalized tensor.\n*   Finally, we return the normalized tensor as a list, rounded to the nearest 4th decimal using `np.round` and `tolist()`."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    This function finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n\n    Args:\n    start_x (float): The initial position.\n    learning_rate (float, optional): The step size for each iteration. Defaults to 0.1.\n    tolerance (float, optional): The minimum difference between two consecutive x values to stop the iteration. Defaults to 1e-6.\n    max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    float: The value of x where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Define the function f(x) = x^4 - 3x^3 + 2\n    def f(x):\n        return x**4 - 3*x**3 + 2\n\n    # Define the derivative of f(x)\n    def df(x):\n        return 4*x**3 - 9*x**2\n\n    # Initialize the current x value\n    current_x = start_x\n\n    # Iterate until the maximum number of iterations is reached or the difference between two consecutive x values is less than the tolerance\n    for _ in range(max_iters):\n        # Calculate the next x value using gradient descent\n        next_x = current_x - learning_rate * df(current_x)\n\n        # Check if the difference between the current x and the next x is less than the tolerance\n        if abs(next_x - current_x) < tolerance:\n            break\n\n        # Update the current x value\n        current_x = next_x\n\n    # Return the value of x where f(x) reaches its minimum, rounded to the nearest 4th decimal\n    return round(current_x, 4)\n\n\n# Example usage:\nprint(find_treasure(1.0))\n```\n\n**Explanation:**\n\n*   We define the function `f(x) = x^4 - 3x^3 + 2` and its derivative `df(x) = 4x^3 - 9x^2`.\n*   We use the gradient descent algorithm to find the minimum of `f(x)`. The gradient descent algorithm iteratively updates the current `x` value using the formula `next_x = current_x - learning_rate * df(current_x)`.\n*   We stop the iteration when the difference between two consecutive `x` values is less than the tolerance or when the maximum number of iterations is reached.\n*   Finally, we return the value of `x` where `f(x)` reaches its minimum, rounded to the nearest 4th decimal.\n\n**Note:** The `learning_rate` parameter controls the step size for each iteration. A higher `learning_rate` can lead to faster convergence but may also cause oscillations. A lower `learning_rate` can lead to more stable convergence but may also increase the number of iterations required. The `tolerance` parameter controls the minimum difference between two consecutive `x` values to stop the iteration. A smaller `tolerance` can lead to more precise results but may also increase the number of iterations required."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n    \n    Args:\n    x (np.ndarray): Input array.\n    alpha (float): Alpha parameter for DyT.\n    gamma (float): Gamma parameter for DyT.\n    beta (float): Beta parameter for DyT.\n    \n    Returns:\n    list[float]: The result of the DyT function applied to the input array, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Calculate the DyT function\n    dyt = np.tanh(alpha * x + beta) * gamma\n    \n    # Round the result to the nearest 4th decimal\n    dyt = np.round(dyt, 4)\n    \n    # Reshape the result to a list\n    result = dyt.tolist()\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = np.array([1.0, 2.0, 3.0])\n    alpha = 0.5\n    gamma = 1.0\n    beta = 0.0\n    \n    result = dynamic_tanh(x, alpha, gamma, beta)\n    print(result)\n```\n\nIn this code:\n\n*   We define the `dynamic_tanh` function, which takes an input array `x` and parameters `alpha`, `gamma`, and `beta`.\n*   We calculate the DyT function using the formula `np.tanh(alpha * x + beta) * gamma`.\n*   We round the result to the nearest 4th decimal using `np.round`.\n*   We reshape the result to a list using the `tolist` method.\n*   In the example usage, we demonstrate how to call the `dynamic_tanh` function with sample inputs and print the result."}
