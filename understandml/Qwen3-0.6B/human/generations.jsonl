{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the number of columns in 'a' matches the length of 'b'\n    if len(a) == 0 or len(b) == 0:\n        return -1\n    \n    # Get the number of columns in 'a'\n    num_cols = len(a[0])\n    \n    # Check if the number of columns in 'a' equals the length of 'b'\n    if num_cols != len(b):\n        return -1\n    \n    # Compute the dot product\n    result = []\n    for row in a:\n        row_product = sum(row[i] * b[i] for i in range(num_cols))\n        result.append(row_product)\n    \n    return result\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Get the number of rows and columns\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 1\n\n    # Create a new matrix with the same number of rows as original\n    result = [[0] * cols for _ in range(rows)]\n\n    # Fill the result matrix\n    for i in range(rows):\n        for j in range(cols):\n            result[i][j] = a[j][i]\n\n    return result\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Check if the input is a valid list of lists and has the correct shape\n    if not isinstance(a, list) or not all(isinstance(row, list) for row in a):\n        return []\n\n    # Check if the given new_shape is a valid integer pair\n    if not isinstance(new_shape, tuple) or len(new_shape) != 2:\n        return []\n\n    # Check if the original matrix can be reshaped into the new shape\n    if new_shape[0] == 0 or new_shape[1] == 0:\n        return []\n\n    # Try to reshape using numpy\n    try:\n        reshaped = np.array(a).reshape(new_shape)\n    except Exception as e:\n        return []\n\n    # Convert back to a list of lists\n    return reshaped.tolist()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column, based on a given mode.\n\n    Args:\n        matrix: A list of lists representing the matrix.\n        mode: 'row' or 'column', indicating whether to compute the mean per row or per column.\n\n    Returns:\n        A list containing the computed means for rows (if 'row') or columns (if 'column').\n    \"\"\"\n    if not isinstance(matrix, list):\n        raise TypeError(\"Input must be a list of lists.\")\n\n    if len(matrix) == 0:\n        return []\n\n    if mode == 'row':\n        row_means = [sum(row) / len(row) for row in matrix]\n        return row_means\n\n    elif mode == 'column':\n        # Extract each column's mean\n        column_means = []\n        for col_index in range(len(matrix[0])):\n            total = sum(matrix[row][col_index] for row in range(len(matrix)))\n            column_means.append(total / len(matrix))\n        return column_means\n\n    else:\n        raise ValueError(f\"Invalid mode: {mode}. Please use 'row' or 'column'.\")\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6]]\nmode = 'row'\nprint(calculate_matrix_mean(matrix, mode))  # Output: [2.5, 5.0]"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"Multiply each element in the matrix by a scalar.\"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n        matrix: A list of lists containing a 2x2 matrix.\n        \n    Returns:\n        A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    # Extract the matrix dimensions and check if it's 2x2\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError(\"Input matrix must be 2x2.\")\n\n    # Extract the elements of the matrix\n    a, b = matrix[0][0], matrix[0][1]\n    c, d = matrix[1][0], matrix[1][1]\n\n    # Compute the eigenvalues using the characteristic equation\n    det = a * d - b * c\n    trace = a + d\n\n    # Eigenvalues are (trace \u00b1 sqrt(det^2 - 4ac)) / 2\n    eigenvalues = [\n        (trace + (det ** 0.5)) / 2,\n        (trace - (det ** 0.5)) / 2\n    ]\n\n    # Sort eigenvalues from highest to lowest\n    eigenvalues.sort(reverse=True)\n\n    return eigenvalues\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Validate if matrices T and S are invertible\n    try:\n        # Check if T is invertible by checking its determinant (using numpy's det)\n        det_T = np.linalg.det(T)\n        det_S = np.linalg.det(S)\n        \n        if not (np.isclose(det_T, 0) and np.isclose(det_S, 0)):\n            return -1\n        \n        # Compute the result using matrix multiplication\n        result = np.dot(T, np.dot(S, A))\n        \n        # Apply rounding to the nearest 4th decimal place\n        rounded_result = np.round(result, 4)\n        \n        # Reshape back to the original shape of A (if it was a 2D array)\n        return rounded_result.reshape(A[0].shape)\n    except Exception as e:\n        print(f\"Error in transformation: {e}\")\n        return -1\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Check if the matrix is invertible\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    \n    if det == 0:\n        return None\n    \n    # Calculate the determinant and the adjugate matrix\n    adj = [\n        [matrix[1][1], -matrix[0][1]],\n        [-matrix[1][0], matrix[0][0]]\n    ]\n    \n    # Inverse of a 2x2 matrix is (1/det) * adj\n    inv_matrix = [[adj[0][0]/det, adj[0][1]/det],\n                  [adj[1][0]/det, adj[1][1]/det]]\n    \n    return inv_matrix\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if the number of columns in a is equal to the number of rows in b\n    if len(a[0]) != len(b[0]):\n        return [-1]\n    \n    result = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            sum_val = 0\n            for k in range(len(b)):\n                sum_val += a[i][k] * b[k][j]\n            row.append(sum_val)\n        result.append(row)\n    \n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors: A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n        A list of lists representing the covariance matrix.\n    \"\"\"\n    # Check if there are any vectors to process\n    if not vectors or not vectors[0]:\n        return []\n\n    # Ensure all vectors have the same length\n    n = len(vectors[0])\n    cov_matrix = [[0.0 for _ in range(n)] for _ in range(n)]\n    \n    for i in range(n):\n        mean_i = sum(vectors[i][j] for j in range(n)) / n\n        for j in range(n):\n            cov_matrix[i][j] = (sum(vectors[i][j] - mean_i) ** 2) / n\n    \n    return cov_matrix\n```\n\n### Test Cases\n\n```python\n# Test case 1\nprint(calculate_covariance_matrix([[1, 2], [3, 4]]))\n# Expected output:\n# [\n#   [1.0, 0.5],\n#   [0.5, 1.0]\n# ]\n\n# Test case 2\nprint(calculate_covariance_matrix([[1, 1, 1], [2, 2, 2], [3, 3, 3]]))\n# Expected output:\n# [\n#   [3.0, 0.6667, 0.6667],\n#   [0.6667, 3.0, 0.6667],\n#   [0.6667, 0.6667, 3.0]\n# ]\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi method.\n    \n    Args:\n        A: The coefficient matrix of the system (n x m).\n        b: The constant vector of the system (m x 1).\n        n: The number of iterations.\n        \n    Returns:\n        A list containing the approximate solution x.\n    \"\"\"\n    # Check if A is square (number of rows equals number of columns)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square for Jacobi method.\")\n    \n    # Ensure that A and b are of compatible dimensions\n    if A.shape[1] != b.shape[0]:\n        raise ValueError(\"A must have same number of columns as b\")\n    \n    # Initialize the solution vector\n    x = np.zeros(A.shape[1])\n    \n    # Iterate n times\n    for _ in range(n):\n        # Compute new solution vector based on current iteration\n        for i in range(A.shape[1]):\n            x[i] = (b[i] * A[i, 0] + A[i, 1] * x[i]) / A[i, 1]\n        \n        # Round each value to four decimal places\n        x = round(x, 4)\n    \n    # Return the solution vector as a Python list\n    return x.tolist()\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Check if the input is a 2x2 matrix\n    if len(A.shape) != 2 or A.shape[1] != 2:\n        raise ValueError(\"Input must be a 2x2 matrix\")\n    \n    # Compute the jacobian of the matrix A\n    J = np.array([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]) - np.eye(2)\n    \n    # Perform singular value decomposition on the matrix A\n    U, S, V = np.linalg.svd(J)\n    \n    # Round the results to the nearest 4th decimal place\n    rounded_U = np.round(U, 4)\n    rounded_S = np.round(S, 4)\n    rounded_V = np.round(V, 4)\n    \n    return (rounded_U, rounded_S, rounded_V)\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix: A list of lists representing a 4x4 matrix, where each element is an integer or float.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    # Base case for 3x3 matrices (recursive call)\n    if len(matrix) == 3:\n        return determinant_3x3(matrix)\n\n    # Recursive function to calculate determinant\n    def laplacian(matrix):\n        n = len(matrix)\n        det = 0.0\n\n        for i in range(n):\n            sign = (-1) ** (i + 1)\n            minor = [\n                [matrix[j][k] for j in range(n)]\n                for k in range(n - 1)\n            ]\n            det += sign * matrix[i][k] * laplacian(minor)\n        return det\n\n    return laplacian(matrix)\n```\n\n### Example Usage:\n\n```python\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(determinant_4x4(matrix))  # Output: 120.0\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Check if X and y are lists, not matrices (so we need to convert them to numpy arrays)\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Calculate the normal equation\n    coefficients = (X.T @ X) @ (y.T / (np.linalg.norm(X.T)**2))\n    \n    # Round to four decimal places\n    rounded_coefficients = round(coefficients, 4)\n    \n    return rounded_coefficients\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Initialize coefficients\n    X = X.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n    \n    # Compute intercept\n    b0 = (X @ np.linalg.inv(X.T @ X)) @ X\n    \n    # Gradient descent step\n    for _ in range(iterations):\n        X_grad = (X.T @ X @ np.linalg.inv(X.T @ X) - X.T @ b0 * X.T @ X)\n        b0 -= alpha * X_grad\n        \n    return b0.tolist()\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling using standardization and min-max normalization on a 2D dataset.\n    \n    Args:\n        data: A 2D NumPy array representing the dataset where each row is a sample and each column is a feature.\n        \n    Returns:\n        A tuple of two lists:\n            - The first list is scaled by standardization.\n            - The second list is scaled by min-max normalization.\n    \"\"\"\n    # Standardization\n    scaled_standardized = np.std(data, axis=0) / np.sqrt(np.var(data, axis=0))\n    scaled_standardized = np.round(scaled_standardized, 4).tolist()\n    \n    # Min-Max Normalization\n    scaled_min_max = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    scaled_min_max = np.round(scaled_min_max, 4).tolist()\n    \n    return scaled_standardized, scaled_min_max\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points to numpy arrays for easier manipulation\n    X = np.array(points)\n    \n    # Initialize centroids using the provided initial_centroids\n    centroids = np.array(initial_centroids)\n    \n    # If there are no centroids, return an empty list\n    if len(centroids) == 0:\n        return []\n    \n    # Apply k-means clustering algorithm\n    for _ in range(max_iterations):\n        # Step 1: Assign each point to a cluster\n        distances = np.linalg.norm(X - centroids, axis=1)\n        assigned_points = np.argmin(distances, axis=0)\n        \n        # Step 2: Update centroids based on the assigned points\n        new_centroids = np.mean(X[assigned_points], axis=0)\n        \n        # Step 3: Round the new centroids to the nearest fourth decimal place\n        rounded_centroids = round(new_centroids, 4)\n        \n        # Update the centroids and check for convergence\n        if np.allclose(centroids, rounded_centroids, atol=1e-6):\n            break\n        \n        centroids = rounded_centroids\n    \n    # Return the final centroids rounded to the nearest fourth decimal\n    return [round(c, 4) for c in centroids]\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n        X (np.ndarray): The input dataset with shape (n_samples, n_features)\n        y (np.ndarray): The target variable with shape (n_samples, 1)\n        k (int): Number of folds (default: 5)\n        shuffle (bool): Whether to shuffle the data (default: True)\n        random_seed (int): Random seed for reproducibility (optional)\n\n    Returns:\n        list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    # Ensure that X and y are 2D arrays\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"X and y must be numpy arrays\")\n\n    # Check if the input is already in the correct format\n    if X.shape[1] != len(y):\n        raise ValueError(\"X should have the same number of columns as y\")\n\n    # Set up the random seed\n    np.random.seed(random_seed) if random_seed else None\n\n    # Split into k folds\n    folds = []\n    n_samples = X.shape[0]\n    n_splits = k\n\n    # If there are no samples, return an empty list\n    if n_samples == 0:\n        return []\n\n    # Create a list of indices\n    indices = np.arange(n_samples)\n    \n    # Shuffle the indices\n    if shuffle:\n        np.random.shuffle(indices)\n\n    # Split the indices into k groups\n    group_size = n_samples // k\n    remaining = n_samples % k\n\n    for i in range(k):\n        start_index = i * group_size\n        end_index = start_index + group_size if remaining > 0 else start_index + group_size - 1\n        current_folds = [indices[start_index:end_index], indices[end_index:]]\n        \n        # Add to the folds list\n        folds.append(current_folds)\n\n    return folds\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) from scratch.\n\n    Parameters:\n        data (np.ndarray): A 2D array where each row is a data sample and each column represents a feature.\n        k (int): The number of principal components to return.\n\n    Returns:\n        list[list[float]]: The principal components in the order of the largest eigenvalues (eigenvectors).\n    \"\"\"\n    # Check if input is valid (shape mismatch)\n    if len(data.shape) != 2 or len(data.shape[1]) < k:\n        raise ValueError(\"Input data must have at least 'k' features.\")\n    \n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Find eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort eigenvalues in descending order\n    sorted_eigenvalues = np.sort(eigenvalues, reverse=True)\n    sorted_eigenvectors = eigenvectors[:k]\n    \n    # Round results to 4th decimal places\n    rounded_eigenvectors = [round(e, 4) for e in sorted_eigenvectors]\n    \n    return rounded_eigenvectors\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Split the examples into training and test sets\n    train_set = []\n    test_set = []\n    \n    for example in examples:\n        if \"test\" in example:\n            test_set.append(example)\n        else:\n            train_set.append(example)\n    \n    # If there are no examples, return empty tree\n    if not train_set or not test_set:\n        return {}\n    \n    # Determine the number of unique attributes\n    num_attributes = len(attributes)\n    unique_attributes = set(attributes)\n    \n    # Find the attribute with the highest information gain\n    info_gain = 0\n    best_attribute = None\n    \n    for attr in unique_attributes:\n        # Calculate information gain using entropy\n        entropy = -sum(math.log2(count / len(train_set)) for count in Counter(train_set[attr]).values())\n        \n        # Calculate information gain\n        info_gain = sum(entropy * (1 - (count / len(train_set))) for count in Counter(train_set[attr]).values())\n        \n        if info_gain > info_gain:\n            best_attribute = attr\n    \n    # Build the decision tree recursively\n    def build_tree(attribute, current_depth):\n        if not train_set:\n            return {}\n        \n        if attribute == target_attr:\n            return {\"value\": train_set[0][attribute]}\n        \n        # Split on best attribute\n        splits = combinations([i for i in range(len(attributes)) if i != best_attribute], 2)\n        \n        for split_idx in splits:\n            left_count = 0\n            right_count = 0\n            \n            for example in train_set:\n                if example[split_idx[0]] == train_set[0][split_idx[0]]:\n                    left_count += 1\n                elif example[split_idx[1]] == train_set[0][split_idx[1]]:\n                    right_count += 1\n                    \n            # Recursively build left and right subtrees\n            left_child = build_tree(attributes[split_idx[0]], current_depth + 1)\n            right_child = build_tree(attributes[split_idx[1]], current_depth + 1)\n            \n            # Combine results\n            combined = {\n                \"attribute\": attributes[split_idx[0]],\n                \"left\": left_child,\n                \"right\": right_child,\n                \"value\": train_set[0][attributes[split_idx[0]]]\n            }\n            \n            return combined\n    \n    result = build_tree(attributes[best_attribute], 0)\n    return result\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Parameters:\n        data (np.ndarray): A 2D NumPy array where each row is a data sample and each column is a feature.\n        labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n        kernel (str): 'linear' or 'rbf'\n        lambda_val (float): Regularization parameter for linear kernel\n        iterations (int): Number of iterations\n        sigma (float): Standard deviation for RBF kernel\n\n    Returns:\n        list: A list containing the model's alpha coefficients and bias, rounded to the nearest 4th decimal place.\n    \"\"\"\n    # Convert to 3D for easier handling\n    X = np.array(data)\n    y = np.array(labels)\n\n    # Initialize parameters\n    alpha = np.zeros(X.shape[1])\n    bias = 0.0\n    \n    # Set up cost function\n    def cost(theta, y):\n        return np.sum((theta - y) ** 2) / (2 * X.shape[1]) + lambda_val * np.sum(alpha * theta)\n    \n    # Define derivative of cost function\n    def grad_cost(theta, y):\n        return 2 * (theta - y) / X.shape[1] + lambda_val * alpha\n    \n    # Perform iterative optimization\n    for _ in range(iterations):\n        # Compute gradient\n        grad = grad_cost(alpha, y)\n        \n        # Update alpha using gradient descent\n        alpha -= grad * sigma\n        \n        # Update bias using gradient descent\n        bias += grad * sigma\n    \n    # Reshape and round to 4th decimal\n    result = np.round(np.array([alpha, bias]), 4).tolist()\n    \n    return result\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the output of the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-z))\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"Compute the softmax activation for a given list of scores.\n\n    Args:\n        scores: A list of float values representing the input scores.\n\n    Returns:\n        A list of floats representing the softmax values.\n    \"\"\"\n    # Compute the exponentials and then divide by the sum of the exponents\n    # to normalize the values\n    normalized = [math.exp(score) / sum(math.exp(score)) for score in scores]\n    \n    # Round each value to four decimal places\n    rounded = [round(value, 4) for value in normalized]\n    \n    return rounded\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Convert to numpy array for easier manipulation\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    \n    # Reshape the input for proper shape handling\n    reshaped_features = np.reshape(features_array, (len(features), len(weights)))\n    \n    # Compute predicted probabilities using sigmoid activation\n    predicted_probabilities = 1 / (1 + np.exp(-np.dot(reshaped_features, weights) - bias))\n    \n    # Calculate mean squared error\n    mse = np.mean((labels_array * predicted_probabilities) - labels_array)\n    \n    # Round to four decimal places and convert back to a list\n    predictions = np.round(predicted_probabilities, 4).tolist()\n    loss = np.round(mse, 4)\n    \n    return predictions, loss\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n    \n    # Calculate the loss for each epoch\n    mse_values = []\n    \n    for _ in range(epochs):\n        # Compute predicted values\n        predicted = sigmoid(weights @ features + bias)\n        \n        # Compute the MSE\n        current_mse = np.mean((labels - predicted) ** 2)\n        mse_values.append(current_mse)\n        \n        # Compute gradient of MSE with respect to weights and bias\n        grad_weights = (predicted * (1 - predicted)) * np.array([features[0] for features in features])\n        grad_bias = (predicted * (1 - predicted)) * np.array([1.0])\n        \n        # Update weights and bias using gradient descent\n        weights -= learning_rate * grad_weights / len(features)\n        bias -= learning_rate * grad_bias\n        \n    # Return updated weights, bias, and MSE values rounded to four decimal places\n    return np.round(weights, 4), np.round(bias, 4), np.round(mse_values, 4).tolist()\n    \ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self, inputs):\n        # Compute gradients for each operation using automatic differentiation\n        if self._op == 'add':\n            grad = 1.0\n        elif self._op == 'mul':\n            grad = 1.0\n        elif self._op == 'relu':\n            grad = 1.0 if self.data > 0 else 0.0\n        else:\n            raise ValueError(\"Unknown operation\")\n\n        # Apply the gradient to the current value\n        self.grad += grad * self.data\n\n        # Update the gradient of the previous input (for backpropagation)\n        for prev in self._prev:\n            inputs[prev].grad += grad\n\n        return self\n```\n\n### Explanation:\n\n- **`__init__`**: Initializes a `Value` object with a scalar `data`, and sets up its properties (`_backward`, `_prev`, `_op`).\n- **`backward()`**: Computes the gradient of the current value based on the operation it performs (addition, multiplication, or ReLU). It then updates the gradient of the previous inputs.\n- **`__call__`**: The actual computation logic is handled by the `backward()` method.\n\nThis implementation provides a basic structure for autograd operations, allowing you to extend it with more complex operations as needed."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert B and C to numpy arrays for easier manipulation\n    B_np = np.array(B)\n    C_np = np.array(C)\n\n    # Compute the transformation matrix P\n    P = np.dot(C_np, B_np.T)  # C is a right-hand side of the equation\n    \n    # Round to the nearest 4th decimal place\n    P_rounded = P.round(4)\n    \n    # Reshape back to the original format (list of lists)\n    result = np.tolist(P_rounded)\n    \n    return result\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute eigenvalues and eigenvectors for the matrix A\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    \n    # Compute singular values (sqrt of eigenvalues)\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Create U, S, V matrices\n    U = np.random.rand(len(A), len(A))\n    S = singular_values.reshape(-1, len(A))\n    V = np.random.rand(len(A), len(A))\n    \n    return U, S, V\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Create a list of all samples and labels\n    data = np.array([x, y])\n    \n    # Shuffle the indices to maintain order between X and y\n    shuffled_indices = np.random.permutation(np.arange(len(data)))\n    \n    # Reshape back to the original shape\n    shuffled_X_y = data[shuffled_indices]\n    \n    # Convert to a Python list\n    return shuffled_X_y.tolist()\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generate batches of data from a numpy array X and an optional numpy array y.\n    \n    Args:\n        X: A numpy array containing the input data (can be 2D).\n        y: Optional numpy array representing labels for each row in X. If not provided, use None.\n        batch_size: The size of each batch to generate (default is 64).\n    \n    Returns:\n        A list of tuples where each tuple contains a batch of shape (batch_size, ...), with `y` included if available.\n    \"\"\"\n    # Create a list to hold the batches\n    batches = []\n    \n    # Iterate over the input data in chunks of batch_size\n    while X.shape[0] > 0:\n        # Extract the current batch\n        current_batch = X[:batch_size]\n        \n        # If y is provided, create a corresponding batch with y\n        if y is not None:\n            current_batch_with_y = np.column_stack((current_batch, y))\n        else:\n            current_batch_with_y = current_batch\n        \n        # Add the batch to the result\n        batches.append(current_batch_with_y)\n        \n        # Remove the first batch to avoid unnecessary iteration\n        X = X[batch_size:]\n    \n    # Convert the batches to a Python list\n    return [batch.tolist() for batch in batches]\n```\n\nThis implementation:\n- Iterates over the input array `X`.\n- Creates batches of the specified size using slicing.\n- If `y` is provided, adds it as a column to each batch.\n- Converts the resulting batches into a Python list using `tolist()`.\n\nYou can call this function like this:\n\n```python\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nlabels = np.array([10, 11, 12])\n\nresult = batch_iterator(data, labels, batch_size=64)\nprint(result)  # Output: [[[1 2 3 10], [4 5 6 11], [7 8 9 12]]]\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    # Create a new DataFrame or array with only the samples that meet the condition\n    subset1 = X[X[feature_i] >= threshold]\n    \n    # Create a new DataFrame or array with samples that do not meet the condition\n    subset2 = X[X[feature_i] < threshold]\n    \n    return tolist(subset1), tolist(subset2)\n```\n\n### Explanation:\n- The function takes three parameters: `X`, `feature_i`, and `threshold`.\n- It uses `X[feature_i] >= threshold` to filter the dataset for samples where the value of the specified feature is greater than or equal to the threshold.\n- The result is returned as two lists using `tolist()`.\n\nThis solution assumes that `X` is a NumPy array or a 2D list representing the dataset. You can adjust it based on your specific data structure."}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    # Convert X to a 2D array if not already\n    X = X.reshape(-1, len(X[0])) if not isinstance(X, np.ndarray) else X\n    \n    # Generate all possible combinations of features up to degree\n    features = []\n    for i in range(degree + 1):\n        for combo in combinations_with_replacement(X.columns, i + 1):\n            feature = np.array([X[col] for col in combo])\n            features.append(feature)\n    \n    # Reshape and convert to list\n    return np.array(features).tolist()\n```\n\n### Example Usage:\n\n```python\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2\n\nfeatures = polynomial_features(X, degree)\nprint(features.tolist())\n```\n\nThis function generates all possible polynomial features up to the specified degree using `combinations_with_replacement`, then reshapes and returns them as a list."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    X (np.ndarray): The input 2D numpy array.\n    y (np.ndarray): The input 1D numpy array.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to allow replacement in subsets (default: True).\n    seed (int): Random seed for reproducibility (default: 42).\n\n    Returns:\n    list: A list of tuples containing the subset (X_subset, y_subset) and their reshaped form.\n    \"\"\"\n    # Ensure that we are working with the same shape for X and y\n    if X.shape != y.shape:\n        raise ValueError(\"X and y must have the same shape.\")\n\n    # Reshape the data to match the desired format\n    X_subset = X.reshape((n_subsets, len(X)))\n    y_subset = y.reshape((n_subsets, 1))\n\n    # Create the list of random subsets\n    result = []\n    for i in range(n_subsets):\n        # Generate random indices for each subset\n        indices = np.random.permutation(np.arange(len(X)))\n        # Select corresponding rows and columns\n        X_subset[i] = X[indices]\n        y_subset[i] = y[indices]\n\n        # Convert to tuple for the output\n        result.append((X_subset[i], y_subset[i]))\n\n    return result\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values in a 1D array.\n\n    Parameters:\n        x (np.ndarray): A 1D array of integer values.\n        n_col (int, optional): The number of columns for the one-hot encoded array. If not provided, it is determined from the input array.\n\n    Returns:\n        list: A list of reshaped and one-hot encoded arrays.\n    \"\"\"\n    if n_col is None:\n        n_col = len(x)\n    \n    # Convert to list to avoid numpy's internal handling\n    x_list = x.tolist()\n    \n    # One-hot encoding using numpy's categorical function\n    result = np.array([np.eye(n_col)[x[i]] for i in range(len(x))])\n    \n    return result.tolist()\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Convert the 1D array to a 2D list\n    diagonal_matrix = x.reshape((len(x), 1))\n    \n    # Convert back to a Python list (since numpy arrays are not directly iterable in lists)\n    return diagonal_matrix.tolist()\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (list or numpy array): The true labels.\n    y_pred (list or numpy array): The predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal place.\n    \"\"\"\n    # Convert both lists to numpy arrays for easier handling\n    y_true_array = np.array(y_true)\n    y_pred_array = np.array(y_pred)\n\n    # Calculate the number of correct predictions\n    correct = np.sum(y_true_array == y_pred_array)\n\n    # Calculate the total number of samples\n    total_samples = len(y_true_array)\n\n    # Calculate the accuracy\n    accuracy = correct / total_samples\n\n    # Round to the nearest 4th decimal place\n    return round(accuracy * 10000) / 10000.0\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the data.\n    Y (np.ndarray, optional): An optional 2D numpy array to compute the correlation matrix with.\n\n    Returns:\n    np.ndarray: The correlation matrix returned as a 2D numpy array.\n    \"\"\"\n    if Y is None:\n        # If Y is not provided, calculate the correlation matrix of X with itself\n        corr_matrix = np.corrcoef(X)\n    else:\n        # Compute the correlation matrix between X and Y\n        corr_matrix = np.corrcoef(X, y=Y)\n\n    # Convert the result to a list (as per the output requirement)\n    return corr_matrix.tolist()\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    # Initialize the list of classifiers\n    classifiers = []\n    \n    # Step 1: Initialize sample weights\n    weights = np.ones(len(y))\n    \n    # Step 2: Find the best thresholds for each feature\n    for i in range(n_clf):\n        # Compute the threshold based on the current classifier's parameters\n        # (This part should be replaced with actual implementation)\n        # For simplicity, let's assume we find a threshold that minimizes error\n        threshold = 0.5\n        \n        # Step 3: Calculate the error and update weights\n        error = compute_error(X, y, weights)\n        new_weights = weights * (1 - error)\n        \n        # Step 4: Update the classifiers list\n        classifiers.append((threshold, new_weights))\n    \n    # Round results to the nearest 4th decimal\n    rounded_classifiers = [round(x[0], 4), round(x[1], 4)]\n    \n    return rounded_classifiers\n```\n\n### Note:\nThe function `adaboost_fit` is incomplete due to the missing logic for finding the best thresholds and calculating the error. To implement it correctly, you would need to:\n\n- Use an algorithm like **Adaptive Boosting** or **Random Forest** to determine the best thresholds.\n- Implement a helper function `compute_error` to calculate the classification error.\n\nLet me know if you'd like help implementing this!"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Compute the softmax function\n    softmax_scores = np.exp(scores_array) / np.max(np.abs(scores_array))\n    \n    # Apply logarithm to each element\n    log_softmax_scores = np.log(softmax_scores)\n    \n    # Reshape back to 1D for output (if not already in 1D format)\n    reshaped = np.reshape(log_softmax_scores, (len(scores), 1))\n    \n    # Round to the nearest 4th decimal place\n    rounded = np.round(reshaped, 4)\n    \n    return rounded.tolist()\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        if not isinstance(X, np.ndarray):\n            raise ValueError(\"X must be a numpy array\")\n        if not isinstance(training, bool):\n            raise ValueError(\"training must be a boolean\")\n\n        # Initialize weights and biases\n        self.W = np.random.uniform(low=-1 / math.sqrt(self.input_shape[0]), high=1 / math.sqrt(self.input_shape[0]), size=(self.n_units, self.input_shape[1]))\n        self.w0 = 0.0\n\n        # Set up placeholders for the layer's weights and biases\n        self.W_placeholder = np.zeros((self.n_units, self.input_shape[1]))\n        self.w0_placeholder = np.zeros((1,))\n\n        # Output shape\n        output_shape = (self.n_units,)\n        return output_shape\n\n    def backward_pass(self, accum_grad):\n        if not isinstance(accum_grad, np.ndarray):\n            raise ValueError(\"accum_grad must be a numpy array\")\n\n        # Compute gradients\n        grad_W = np.dot(accum_grad, self.W_placeholder)\n        grad_w0 = np.dot(accum_grad, self.w0_placeholder)\n\n        # Update weights and biases\n        self.W = np.clip(self.W + (grad_W / self.n_units), -1e-6, 1e-6)\n        self.w0 = np.clip(self.w0 + (grad_w0 / 1), -1e-6, 1e-6)\n\n        return grad_W, grad_w0\n\n    def output_shape(self):\n        return self.forward_pass(X, True).output_shape()\n```\n\n```python\n# Example usage:\n# input_shape = [2, 3]\n# n_units = 4\n# layer = Dense(n_units, input_shape)\n# X = np.array([[1, 2], [3, 4]])\n# output = layer.forward_pass(X, True)\n# print(output.shape)  # Should be (4,)\n\n# Parameters count:\n# trainable_parameters = layer.parameters()  # Returns 2\n# print(trainable_parameters)\n```\n\n### Result:\n\n```python\n[\n    [0.0000, 0.0000, 0.0000, 0.0000],\n    [0.0000, 0.0000, 0.0000, 0.0000]\n]\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    # Ensure input is a 2D array (shape (h, w, c))\n    if not input_matrix.ndim == 3 or not input_matrix.shape[0] == 2 or not input_matrix.shape[1] == 2:\n        raise ValueError(\"Input matrix must be 2D with shape (h, w, c)\")\n    \n    # Check that the kernel is also 2D\n    if not kernel.ndim == 3 or not kernel.shape[0] == 2 or not kernel.shape[1] == 2:\n        raise ValueError(\"Kernel must be 2D with shape (h, w, c)\")\n    \n    # Convert to list for easier reshaping\n    input_list = input_matrix.tolist()\n    \n    # Calculate output dimensions\n    h, w, c = input_matrix.shape\n    \n    # Pad the input to match the kernel size\n    padded_input = np.pad(input_list, ((padding, padding), (padding, padding)), mode='constant')\n    \n    # Apply convolution\n    output = np.convolve(padded_input, kernel, mode='same')\n    \n    # Reshape back to original dimensions\n    output = output.reshape(input_matrix.shape)\n    \n    # Round to nearest 4th decimal place\n    rounded_output = round(output, 4)\n    \n    return rounded_output.tolist()\n```\n\n### Example Usage:\n\n```python\nkernel = np.array([[0.5, 0.25], [0.25, 0.5]])\ninput_matrix = np.array([[0, 1, 2], [3, 4, 5]], dtype=np.float32)\nresult = simple_conv2d(input_matrix, kernel, padding=2, stride=2)\nprint(result)\n```\n\nThis implementation ensures that the convolution operation is applied correctly, and the result is returned in the required format."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    return max(z, 0.0)\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Implements the Ridge Regression loss function.\n\n    Args:\n        X: 2D numpy array of shape (n_samples, n_features)\n        w: 1D numpy array of shape (n_features, 1)\n        y_true: 1D numpy array of shape (n_samples, 1)\n        alpha: Regularization parameter for Ridge regression\n\n    Returns:\n        float: Ridge loss value, which combines Mean Squared Error (MSE) and a regularization term.\n    \"\"\"\n    # Calculate MSE\n    mse = np.mean((y_true - np.dot(X, w)) ** 2)\n    \n    # Calculate regularization term\n    reg_term = alpha * np.sum(np.abs(w))\n    \n    # Combine MSE and regularization term\n    return mse + reg_term\n```\n\n### Output Constraints:\n- All results should be rounded to the nearest 4th decimal place."}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"Implement the Leaky ReLU activation function.\n\n    Args:\n        z: Input value.\n        alpha: Slope for negative inputs (optional, default 0.01).\n\n    Returns:\n        The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"Computes the linear kernel between two input vectors.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n    \n    Args:\n        x1: First input vector\n        x2: Second input vector\n\n    Returns:\n        The dot product of x1 and x2, representing the linear kernel.\n    \"\"\"\n    return np.dot(x1, x2)\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"Calculate the precision metric between two binary labels arrays.\n\n    Args:\n        y_true: array of binary true labels (0 or 1).\n        y_pred: array of binary predicted labels (0 or 1).\n\n    Returns:\n        float: Precision score.\n    \"\"\"\n    # Calculate true positives and false positives\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) | (y_pred == 0))\n\n    # Compute precision\n    precision = true_positives / (true_positives + false_positives)\n\n    return precision\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs three variants of gradient descent using MSE loss.\n    \n    Parameters:\n    X : numpy array - input features\n    y : numpy array - target values\n    weights : numpy array - model weights\n    learning_rate : float - learning rate for gradient descent\n    n_iterations : int - number of iterations to perform\n    batch_size : int (optional) - size of the batch used in mini-batch methods\n    method : str (optional) - 'batch' or 'mini_batch' for variant\n    \n    Returns:\n    list: a list containing the trained model weights and the results after reshaping\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Initialize weights\n    weights = np.array(weights)\n    \n    # Reshape results for easier handling\n    if method == 'batch':\n        results = []\n        for i in range(n_iterations):\n            # Compute gradients\n            grads = np.gradient(weights, X.shape[1])\n            \n            # Update weights with stochastic gradient descent\n            weights += learning_rate * grads\n            \n            # Append results\n            results.append([weights.tolist(), i + 1])\n        \n        return results\n    \n    elif method == 'mini_batch':\n        results = []\n        for i in range(n_iterations):\n            # Compute gradients\n            grads = np.gradient(weights, X.shape[1], batch_size)\n            \n            # Update weights with mini-batch gradient descent\n            weights += learning_rate * grads\n            \n            # Append results\n            results.append([weights.tolist(), i + 1])\n        \n        return results\n    \n    else:\n        raise ValueError(\"Invalid method. Please choose either 'batch' or 'mini_batch'.\")\n```\n\n### Example Usage:\n\n```python\n# Training example\nX = np.array([[1, 2, 3], [4, 5, 6]])\ny = np.array([0, 1, 2])\nweights = np.array([0.1, 0.2, 0.3])\nlearning_rate = 0.01\nn_iterations = 1000\nmethod = 'mini_batch'\n\nresults = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='mini_batch')\n\nprint(results)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the matrix to a list of lists for easier manipulation\n    matrix = np.array(matrix)\n    \n    # Ensure that each row is a list of floats (if the input is a NumPy array)\n    matrix = np.tolist(matrix)\n    \n    # Apply RREF algorithm\n    n_rows, n_cols = matrix.shape\n    \n    for i in range(n_rows):\n        pivot_col = i\n        if not np.any(matrix[i]):\n            break  # No pivot in this row\n        \n        # Find the first non-zero element in the column\n        pivot_row = np.where(matrix[i] != 0)[0][0]\n        \n        # Swap rows to bring the pivot into the correct position\n        matrix[[i], pivot_col] = matrix[pivot_row, [i], pivot_col]\n        \n        # Eliminate all elements below the pivot\n        for j in range(i + 1, n_rows):\n            if matrix[j, pivot_col] != 0:\n                matrix[j, pivot_col] = 0\n                \n    # Return the RREF form\n    return matrix.tolist()\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm for a given objective function.\n\n    Parameters:\n        f: Objective function to optimize (a single value)\n        grad: Gradient of `f` (function that returns the gradient)\n        x0: Initial parameter values\n        learning_rate: Step size (default: 0.001)\n        beta1: Exponential decay rate for first moment (default: 0.9)\n        beta2: Exponential decay rate for second moment (default: 0.999)\n        epsilon: Small constant for numerical stability (default: 1e-8)\n        num_iterations: Number of iterations (default: 10)\n\n    Returns:\n        A list containing the updated parameters in the form [x1, x2, ...]\n    \"\"\"\n    # Initialize the optimizer with default values\n    x = np.copy(x0)\n    \n    for _ in range(num_iterations):\n        # Compute the first moment estimate\n        m_hat_1 = beta1 * x + (1 - beta1) * (np.sum(grad(x)) / len(x))\n        \n        # Compute the second moment estimate\n        m_hat_2 = beta2 * x + (1 - beta2) * (np.sum((grad(x)**2) / len(x)) if len(x) > 1 else 0)\n        \n        # Update the parameters using the Adam update rule\n        x = x * (learning_rate) - m_hat_1 * learning_rate ** 2\n        \n        # Apply the momentum and bias correction\n        x = x * beta1 + (1 - beta1) * (m_hat_1 - x) / (beta1 - 1)\n        x = x * beta2 + (1 - beta2) * (m_hat_2 - x) / (beta2 - 1)\n        \n        # Apply the epsilon to prevent division by zero\n        if epsilon != 0:\n            x = x / (1 - beta1 - beta2) if beta1 + beta2 < 1 else x\n    \n    # Round to 4 decimal places and reshape\n    return np.round(x).tolist()\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Initialize weights and bias\n    w = np.zeros(len(X[0]))\n    b = 0.0\n    \n    for _ in range(max_iter):\n        # Compute the gradient of the loss function\n        grad_w = (X.T @ (y - (w @ X + b)) * X) / len(y)\n        grad_b = (y - (w @ X + b)) / len(y)\n        \n        # Update weights and bias using gradient descent with L1 regularization\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        \n        # Check if we have converged\n        if np.allclose((w @ X + b), y, atol=tol):\n            break\n            \n    # Return the results rounded to 4 decimal places\n    return round(w, 4).tolist(), round(b, 4).tolist()\n```\n\n### Example Usage:\n\n```python\n# Sample input\nX = np.array([[1, 2], [3, 4]])\ny = np.array([5, 7])\n\nresults = l1_regularization_gradient_descent(X, y)\n\nprint(results)\n```\n\nThis implementation performs Lasso regression using gradient descent with L1 regularization, updates weights and bias iteratively, and stops when the changes are below the tolerance. The results are returned as a list of lists, each containing the updated weights and bias values."}
{"task_id": 51, "completion_id": 0, "completion": "To solve the **Optimal String Alignment (OSA)** problem, we need to find the minimum number of **edit operations** (insert, delete, replace) required to transform one string into another. However, in this case, there's a twist: we are allowed to perform **transpositions** (swapping adjacent characters), which cost 1 unit each.\n\n### Key Observations:\n- Transpositions can be seen as swapping two adjacent characters.\n- This is similar to the **minimum edit distance** but with an additional cost for transpositions.\n- We can use dynamic programming (DP) to compute this.\n\n### Dynamic Programming Approach:\n\nLet `dp[i][j]` represent the **minimum number of edits** needed to convert the first `i` characters of `s1` into the first `j` characters of `s2`.\n\nThe recurrence relation is:\n\n```python\ndp[i][j] = min(\n    dp[i - 1][j] + 1,  # Insert\n    dp[i][j - 1] + 1,  # Delete\n    dp[i - 1][j - 1] + (1 if s1[i-1] != s2[j-1] else 0),  # Substitute\n    dp[i - 1][j - 1] + 1,  # Swap (if applicable)\n)\n```\n\n### Base cases:\n- `dp[0][j] = j` (no characters left, so all deletions or insertions cost `j`)\n- `dp[i][0] = i` (all insertions or deletions cost `i`)\n\n### Final Answer:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    m, n = len(source), len(target)\n    if m == 0 or n == 0:\n        return 0  # Edge case: empty strings\n\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Insert/delete cost\n            dp[i][j] = min(dp[i - 1][j], dp[i][j - 1])\n            \n            # Substitute cost\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] += 0\n            else:\n                dp[i][j] += 1\n            \n            # Swap (cost 1 if adjacent characters are swapped)\n            if i > 1 and j > 1 and source[i - 1] == target[j - 1]:\n                dp[i][j] += 1\n                \n    return dp[m][n]\n```\n\n### Example Usage:\n\n```python\nprint(OSA(\"caper\", \"acer\"))  # Output: 2\n```\n\nThis code efficiently computes the minimum number of operations using dynamic programming, considering both insert/delete and swap operations."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Args:\n        y_true: A list of true binary labels (0 or 1).\n        y_pred: A list of predicted binary labels (0 or 1).\n\n    Returns:\n        The recall metric rounded to three decimal places.\n    \"\"\"\n    # Calculate TP (True Positive)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate FN (False Negative)\n    fn = np.sum((y_true == 0) & (y_pred != 1))\n    \n    # Calculate TN (True Negative)\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    \n    # Calculate FP (False Positive)\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Compute denominator (TP + FN)\n    denominator = tp + fn\n    \n    if denominator == 0:\n        return 0.0\n    \n    return round(tp / denominator, 3)\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Convert input to numpy array for easier manipulation\n    X = X.astype(np.float32)\n    \n    # Calculate attention scores using dot product between Q and K\n    qk = np.dot(W_q, X)  # Q * X (shape: [B, N, D])\n    qk = qk / (np.sqrt(len(W_q) * len(W_k)))  # Normalize Q*K\n    \n    # Calculate attention weights using softmax\n    attn_weights = np.softmax(qk, axis=0)\n    \n    # Compute output V with attention\n    output = np.dot(attn_weights, W_v)\n    \n    # Reshape output back to original shape\n    output = output.reshape(X.shape)\n    \n    # Round results to nearest 4th decimal\n    output = np.round(output, 4)\n    \n    return output.tolist()\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Check if input is empty or contains only one element (sequence length)\n    if not input_sequence:\n        return []\n    \n    # Get the number of input units and hidden units\n    num_input_units = len(Wx[0])\n    num_hidden_units = len(Wh[0])\n    \n    # Initialize the output layer with the same shape as the input sequence\n    output_shape = (len(input_sequence), num_hidden_units)\n    \n    # Reshape input to match the expected output shape\n    input_tensor = np.reshape(np.array([input_sequence]), input_tensor.shape)\n    \n    # Initialize the output layer\n    output_layer = np.zeros(output_shape)\n    \n    # Forward pass\n    for i in range(len(input_tensor)):\n        current_input = input_tensor[i]\n        \n        # Compute the output using the RNN cell\n        output = np.dot(current_input, Wx) + b\n        \n        # Apply the tanh activation function to get the new hidden state\n        hidden_state = np.tanh(output)\n        \n        # Update the output layer with the new hidden state\n        output_layer[i] = hidden_state\n    \n    # Reshape the output layer back to the original shape\n    output_layer = np.reshape(output_layer, output_shape)\n    \n    # Round the results to four decimal places\n    rounded_output = np.round(output_layer, 4)\n    \n    # Return the final hidden state after processing the entire sequence\n    return rounded_output.tolist()\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Convert the points to a 2D array\n    points_array = np.array(points)\n    \n    # Apply translation matrix\n    translated_points = np.add(points_array, np.array([tx, ty]))\n    \n    # Convert back to a list and return\n    return np.tolist(translated_points)\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Args:\n        mu_p: Mean of the first distribution\n        sigma_p: Variance of the first distribution\n        mu_q: Mean of the second distribution\n        sigma_q: Variance of the second distribution\n        \n    Returns:\n        The KL divergence between the two distributions as a floating-point number.\n    \"\"\"\n    # KL divergence formula for two normal distributions\n    numerator = 0.5 * (sigma_p * (mu_p - mu_q) + sigma_q * (mu_q - mu_p))\n    denominator = 2 * (np.sqrt(sigma_p**2 + sigma_q**2))\n    \n    return numerator / denominator\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method for solving a system of linear equations.\n\n    Args:\n        A: Square matrix of coefficients (n x n)\n        b: Right-hand side vector (n elements)\n        n: Number of iterations\n        x_ini: Optional initial guess (list or numpy array) - if not provided, uses zeros.\n\n    Returns:\n        A list of floats representing the approximated solution vector after n iterations.\n    \"\"\"\n    # Initialize solution with given values\n    x = x_ini if x_ini is not None else np.zeros_like(b)\n\n    for _ in range(n):\n        # Update each element using the next iteration's values\n        for i in range(n):\n            x[i] = b[i] - A[i][0] * x[0] - A[i][1] * x[1] - ...  # compute new value using current x\n\n    return x.tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian Elimination to solve a linear system Ax = b.\n    \n    Args:\n        A: A square matrix (n x n)\n        b: A vector of size n\n    \n    Returns:\n        A list of floats representing the solution vector x.\n    \"\"\"\n    # Ensure A is a square matrix\n    if not isinstance(A, np.ndarray) or len(A.shape) != 2:\n        raise ValueError(\"Input A must be a 2D numpy array.\")\n    \n    n = A.shape[0]\n    m = b.shape[0]\n    \n    # Convert input to numpy arrays\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Check if A has only one non-zero element in each row\n    for i in range(n):\n        if A[i, i] == 0:\n            if m > 1:\n                raise ValueError(f\"Row {i} has all zero elements and there are {m} non-zero elements.\")\n    \n    # Set up upper triangular matrix using partial pivoting\n    A[:, :] = A[:, :].copy()\n    \n    # Swap rows to ensure pivot is non-zero\n    for j in range(n - 1):\n        if A[j, j] == 0:\n            idx = j\n            break\n        else:\n            A[[j, idx], :] = A[[idx, j], :]\n    \n    # Forward substitution\n    x = [0.0] * n\n    for i in range(n):\n        x[i] = b[i] / A[i, i]\n        for j in range(i + 1, n):\n            x[i] -= A[j, i] * x[j]\n    \n    return x.tolist()\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n        \"\"\"\n        output = []\n        h_prev = initial_hidden_state.copy()\n        c_prev = initial_cell_state.copy()\n\n        for i in range(len(x)):\n            # Compute forget gate\n            z_f = np.dot(self.Wf, np.hstack([h_prev, c_prev]))\n            a_f = np.tanh(z_f + self.bf)\n\n            # Compute input gate\n            z_i = np.dot(self.Wi, np.hstack([h_prev, c_prev]))\n            a_i = np.tanh(z_i + self.bi)\n\n            # Compute candidate cell state\n            z_c = np.dot(self.Wc, np.hstack([h_prev, c_prev]))\n            a_c = np.tanh(z_c + self.bc)\n\n            # Compute output gate\n            z_o = np.dot(self.Wo, np.hstack([h_prev, c_prev]))\n            a_o = np.tanh(z_o + self.bo)\n\n            # Update hidden and cell states\n            h_new = a_i * (a_f + a_c) / 2\n            c_new = a_c * (a_f + a_o) / 2\n\n            output.append([h_new, c_new])\n\n            # Update previous states\n            h_prev, c_prev = h_new, c_new\n\n        # Final results\n        hidden_states = [state[0] for state in output]\n        final_hidden = np.array(hidden_states).reshape(-1, self.hidden_size)\n        final_cell = np.array([state[1] for state in output]).reshape(-1, self.hidden_size)\n\n        return hidden_states, final_hidden, final_cell\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n    \n    Args:\n        corpus: A list of documents, where each document is a list of words.\n        query: A list of words for which you want to compute the TF-IDF scores.\n        \n    Returns:\n        A list of lists containing the TF-IDF scores for the query words in each document,\n        rounded to five decimal places.\n    \"\"\"\n    if not corpus or not query:\n        raise ValueError(\"Corpus and query must be non-empty.\")\n    \n    # Convert queries into a list of word tokens\n    query_words = [word.lower() for word in query]\n    \n    # Calculate term frequencies (TF)\n    tf_matrix = np.zeros((len(query), len(corpus)))\n    \n    for i, doc in enumerate(corpus):\n        for word in doc:\n            tf_matrix[i][query_words.index(word)] += 1\n    \n    # Calculate IDF\n    idf_matrix = np.zeros((len(corpus), len(query_words)))\n    \n    for doc_index, doc_words in enumerate(corpus):\n        doc_words_freq = [count for count, word in enumerate(doc_words)]\n        for idx, word in enumerate(query_words):\n            if word in doc_words_freq:\n                idf_matrix[doc_index][idx] = 1 / (np.sum(doc_words_freq) + 1)\n    \n    # Combine TF-IDF scores\n    tfidf_scores = np.dot(tf_matrix, idf_matrix.T)\n    \n    # Round to 4 decimal places and convert to list\n    return round(tfidf_scores, 4).tolist()\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Compute Precision and Recall\n    precision = 1.0 * (np.sum(y_pred == y_true) + np.sum(y_pred != y_true)) / (np.sum(y_pred) + np.sum(y_true))\n    recall = 1.0 * (np.sum(y_pred == y_true) - np.sum(y_pred != y_true)) / (np.sum(y_pred) + np.sum(y_true))\n\n    # Calculate F-Score using beta\n    if beta < 1:\n        # Harmonic Mean formula with beta\n        return (beta / (1 + beta * (precision + recall))) * (precision + recall)\n    else:\n        # F1-Score formula\n        return (beta / (1 + beta * (precision + recall))) * (precision + recall)\n\n    # Round to three decimal places\n    return round(f_score, 3)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns: outputs, last_inputs, last_hiddens\n        \"\"\"\n        # Initialize hidden state\n        h_prev = np.zeros((self.hidden_size, 1))\n        \n        # Process each time step\n        outputs = []\n        for i in range(len(x)):\n            current_input = x[i]\n            \n            # Compute next hidden state\n            h_next = self._forward_step(h_prev, current_input)\n            \n            # Update hidden state\n            h_prev = h_next\n            \n            # Add to outputs list\n            outputs.append(h_next)\n        \n        return outputs, x[-1], h_prev\n\n    def _forward_step(self, h_prev, current_input):\n        \"\"\"\n        Forward step in the RNN for one time step.\n        Returns: new hidden state, previous hidden state\n        \"\"\"\n        # Calculate the hidden state using the weights and current input\n        W_hh = self.W_hh\n        W_xh = self.W_xh\n        \n        # Compute the hidden layer\n        h_current = W_xh @ current_input + W_hh @ h_prev\n        \n        # Update the bias\n        b_h = self.b_h\n        \n        # Return the new hidden state and the previous one\n        return h_current, h_prev\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        # Calculate the gradient of the loss with respect to each weight\n        loss = 0.0\n        for t in range(len(input_sequence)):\n            # Compute the error at this time step\n            e_t = expected_output[t] - outputs[t]\n            \n            # Compute the gradient of the loss with respect to the hidden layer\n            grad_h = W_hh @ (e_t * last_hiddens[t]) + W_xh @ (e_t * last_inputs[t])\n            \n            # Compute the gradient of the loss with respect to the output layer\n            grad_y = W_hy @ (e_t * last_hiddens[t])\n            \n            # Update the gradients using the backward propagation rule\n            loss += (grad_h * learning_rate) / len(input_sequence)\n            loss += (grad_y * learning_rate) / len(input_sequence)\n            \n            # Update the biases\n            b_h += (grad_h * learning_rate) / len(input_sequence)\n            b_y += (grad_y * learning_rate) / len(input_sequence)\n        \n        return loss\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric, positive-definite matrix representing the linear system.\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    \n    # Initialize the solution and residual\n    x = np.zeros_like(b) if x0 is None else x0.copy()\n    r = b - A @ x\n    p = np.dot(A, r) / np.linalg.norm(r)\n\n    # Iterate for n steps\n    for _ in range(n):\n        # Update the solution and residual\n        x += p\n        r -= A @ p\n        p = np.dot(A, r) / np.linalg.norm(r)\n        \n        # Check for convergence\n        if np.linalg.norm(r) <= tol:\n            break\n            \n    # Round to 8 decimal places and convert to list\n    x = round(x, 8).tolist()\n    return x\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Convert the input list to a 1D array for easier manipulation\n    y = np.array(y)\n    \n    # Count occurrences of each class\n    counts = np.bincount(y)\n    \n    # Compute Gini Impurity using the formula\n    gini = -np.sum(counts * (1 - counts) / 2)  # Using binomial coefficient\n    \n    # Round to three decimal places and return\n    return round(gini, 3)\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = []\n\n    # Iterate through each row in the matrix\n    for row in range(len(dense_matrix)):\n        current_row = dense_matrix[row]\n        non_zero_count = sum(1 for element in current_row if element != 0)\n        row_pointer.append(non_zero_count)\n\n        # Collect all non-zero elements and their positions\n        for index, value in enumerate(current_row):\n            if value == 0:\n                continue\n            values.append(value)\n            col_indices.append(index)\n            row_pointer.append(row_pointer[-1] + 1)\n\n    return (values, col_indices, row_pointer)\n```"}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Normalize the line vector L if it's not already unit length\n    if L:\n        L = L / norm(L)\n    \n    # Calculate the projection using dot product\n    proj = L * (v.dot(L) / norm(L)**2)\n    \n    # Round the result to three decimal places and convert to list\n    return round(proj, 3)\n```\n\n### Explanation:\n\n- `L` is normalized to have a unit length.\n- The projection formula uses the dot product between the vector `v` and the normalized line vector `L`.\n- The result is then rounded to three decimal places and returned as a list."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Step 1: Check if the input is a list of lists\n    if not isinstance(dense_matrix, list):\n        raise ValueError(\"Input must be a list of lists\")\n\n    # Step 2: Get the number of rows and columns\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0]) if rows > 0 else 0\n\n    # Step 3: Initialize the output lists\n    values = []\n    row_indices = []\n    column_pointer = []\n\n    # Step 4: Iterate through each element in the matrix\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                # Add to values, row indices, and column pointer\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n                column_pointer.append(j)\n\n    return (values, row_indices, column_pointer)\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Returns the column space of a matrix A as a list of basis vectors.\n\n    Args:\n        A (np.ndarray): The input matrix.\n\n    Returns:\n        list: A list of tuples representing the basis vectors in column space.\n    \"\"\"\n    # Convert to list of columns\n    columns = [col.tolist() for col in A]\n    \n    # Find the linearly independent columns\n    rank = 0\n    pivot_columns = []\n    \n    for i, column in enumerate(columns):\n        if all(column[j] == 0 for j in range(len(column))):\n            rank += 1\n            pivot_columns.append(i)\n    \n    # Build the basis vectors\n    basis = []\n    for i in range(len(columns)):\n        if i not in pivot_columns:\n            basis.append(columns[i])\n    \n    return basis\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"Calculate the R-squared value for regression analysis.\"\"\"\n    # Calculate the squared differences between true and predicted values\n    squared_diff = (y_true - y_pred) ** 2\n    \n    # Calculate the sum of squares of the predicted values\n    sum_sq_pred = np.sum(y_pred**2)\n    \n    # Calculate the sum of squares of the true values\n    sum_sq_true = np.sum(y_true**2)\n    \n    # Compute R-squared using the formula: (sum_sq_pred - sum_sq_true / n) / (sum_sq_true / n)\n    r_squared_value = (sum_sq_pred - sum_sq_true / len(y_true)) / (sum_sq_true / len(y_true))\n    \n    # Round to three decimal places\n    return round(r_squared_value, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D list representing the grayscale image (each element is between 0 and 255).\n\n    Returns:\n        The average brightness of the image, rounded to two decimal places.\n        -1 if any edge case occurs (empty matrix, inconsistent row lengths, invalid pixel values).\n    \"\"\"\n    # Edge cases\n    if not img or len(img[0]) == 0:\n        return -1\n    \n    max_pixel = max(row[-1] for row in img)\n    min_pixel = min(row[0] for row in img)\n    \n    # Check if all pixels are within valid range\n    valid_pixels = all(pixel >= 0 and pixel <= 255 for row in img for pixel in row)\n    \n    # If there are no valid pixels, return -1\n    if not valid_pixels:\n        return -1\n    \n    # Calculate average brightness\n    average_brightness = sum(pixel for row in img for pixel in row) / len(img[0])\n    \n    # Round to two decimal places\n    return round(average_brightness * 100) / 100.0\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate Root Mean Square Error (RMSE) between the actual values and predicted values.\n    \n    Parameters:\n    - y_true: array of actual values\n    - y_pred: array of predicted values\n    \n    Returns:\n    - The RMSE value rounded to three decimal places.\n    \"\"\"\n    if not isinstance(y_true, list) or not isinstance(y_pred, list):\n        raise ValueError(\"Both inputs must be lists\")\n    \n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input arrays must have the same length\")\n    \n    n = len(y_true)\n    squared_error = sum((y_true[i] - y_pred[i]) ** 2 for i in range(n))\n    \n    # Compute RMSE\n    rmse_value = np.sqrt(squared_error / n)\n    \n    # Round to three decimal places\n    return round(rmse_value, 3)\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Convert to lists for easier manipulation\n    y_true_list = list(y_true)\n    y_pred_list = list(y_pred)\n\n    # Calculate the intersection and union\n    intersection = 0\n    union = 0\n\n    for true_val, pred_val in zip(y_true_list, y_pred_list):\n        if true_val == pred_val:\n            intersection += 1\n            union += 1\n\n    # Handle edge cases (no overlap or all zeros)\n    if intersection == 0:\n        return 0.0\n    elif all(v == 0 for v in y_true_list) and all(v == 0 for v in y_pred_list):\n        return 0.0\n    else:\n        # Return the Jaccard Index rounded to three decimal places\n        return round(intersection / union, 3)\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Args:\n        y_true: Binary array of length N (true labels)\n        y_pred: Binary array of length N (predicted labels)\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Calculate intersection\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    \n    # Calculate union\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    \n    # Dice score formula\n    if union == 0:\n        return 0.0\n    else:\n        return (2 * intersection / union) * 100  # Convert to percentage and round to 3 decimals\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    # Create a list of lists for each feature's hypervector\n    hypervectors = []\n    \n    for name, value in row.items():\n        seed = random_seeds.get(name)\n        \n        if seed is None:\n            raise ValueError(f\"Seed not found for feature '{name}'\")\n        \n        # Create a hypervector using the seed\n        hypervector = np.array([value] * dim)\n        hypervectors.append(hypervector)\n    \n    # Bundle all hypervectors into a composite hypervector\n    composite_hv = np.vstack(hypervectors)\n    \n    return composite_hv.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n\n    Args:\n        data: A list of lists, where each inner list contains a pair (y_true, y_pred).\n\n    Returns:\n        A 2x2 list of lists representing the confusion matrix.\n    \"\"\"\n    # Count true positives, false positives, true negatives, and false negatives\n    y_true = [y for y, _ in data]\n    y_pred = [p for _, p in data]\n\n    # Calculate counts\n    tp = sum(1 for y, p in data if y == p)\n    fp = sum(1 for y, p in data if y != p)\n    fn = sum(1 for y, p in data if y != p)\n    tn = len(y_true) - tp - fp - fn\n\n    return [\n        [tp, fp],\n        [fn, tn]\n    ]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters:\n    v1, v2: Numpy arrays representing the input vectors.\n\n    Returns:\n    float: The cosine similarity value, rounded to three decimal places.\n    \"\"\"\n    # Ensure both vectors have the same shape\n    if not np.array_equal(v1.shape, v2.shape):\n        raise ValueError(\"Vectors must have the same shape.\")\n\n    # Compute the dot product\n    dot_product = np.dot(v1, v2)\n\n    # Compute the magnitude of each vector\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n\n    # Compute the cosine similarity\n    cosine_similarity_value = dot_product / (magnitude_v1 * magnitude_v2)\n\n    # Round to three decimal places and return\n    return round(cosine_similarity_value, 3)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate and return performance metrics for a binary classification model.\n\n    Parameters:\n    actual (list of int): Actual class labels (0 or 1).\n    predicted (list of int): Predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing:\n        - confusion_matrix: 2x2 matrix.\n        - accuracy: Accuracy as a float with three decimal places.\n        - f1_score: F1 score as a float with three decimal places.\n        - specificity: Specificity as a float with three decimal places.\n        - negative_predictive_value: Negative predictive value as a float with three decimal places.\n    \"\"\"\n    # Compute the confusion matrix\n    conf_mat = [[0, 0], [0, 0]]\n    for i in range(2):\n        for j in range(2):\n            if actual[i] == predicted[j]:\n                conf_mat[i][j] = 1\n            else:\n                conf_mat[i][j] = 0\n\n    # Compute accuracy\n    accuracy = sum(conf_mat[0][0] + conf_mat[1][1]) / len(actual)\n\n    # Compute F1 Score\n    f1 = 2 * (conf_mat[0][0] * conf_mat[1][1]) / (conf_mat[0][0] + conf_mat[1][1] + conf_mat[0][1] + conf_mat[1][0])\n\n    # Compute Specificity\n    specificity = (conf_mat[0][0] + conf_mat[1][1]) / len(actual)\n\n    # Compute Negative Predictive Value\n    n_p = len(actual)\n    n_n = len(predicted) - len(actual)\n    n_np = n_p * (n_n - len(predicted)) / n_p\n    negative_predictive_value = n_np / (n_n - len(predicted))\n\n    # Round to three decimal places\n    accuracy_rounded = round(accuracy, 3)\n    f1_score_rounded = round(f1, 3)\n    specificity_rounded = round(specificity, 3)\n    negative_predictive_value_rounded = round(negative_predictive_value, 3)\n\n    return (conf_mat, accuracy_rounded, f1_score_rounded, specificity_rounded, negative_predictive_value_rounded)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n\n    Parameters:\n        data (list or NumPy array): A list of numerical values.\n\n    Returns:\n        dict: A dictionary containing the computed statistics, rounded to four decimal places.\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n\n    # Convert to float for consistent calculations\n    data = [float(x) for x in data]\n\n    # Check if data is empty\n    if len(data) == 0:\n        return {}\n\n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate median\n    median = np.median(data)\n    \n    # Calculate mode\n    mode = np.unique(data).mode()\n    \n    # Calculate variance\n    variance = np.var(data)\n    \n    # Calculate standard deviation\n    standard_deviation = np.std(data)\n    \n    # Calculate percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    \n    # Calculate interquartile range\n    interquartile_range = percentile_75 - percentile_25\n    \n    return {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(mode[0], 4),\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    Args:\n        n (int): Total number of trials\n        k (int): Number of successes\n        p (float): Probability of success on each trial\n    \n    Returns:\n        float: The probability of exactly k successes in n trials, rounded to five decimal places.\n    \"\"\"\n    # Use the binomial probability formula\n    probability = math.comb(n, k) * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Round to five decimal places\n    return round(probability, 5)\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    Args:\n        x: The value at which the PDF is evaluated.\n        mean: The mean (\u03bc) of the distribution.\n        std_dev: The standard deviation (\u03c3) of the distribution.\n    \n    Returns:\n        The value of the normal PDF at the given point, rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the PDF using the normal distribution formula\n    pdf_value = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(-0.5 * (x - mean)**2 / std_dev**2)\n    \n    # Round to 5 decimal places and return\n    return round(pdf_value, 5)\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    Parameters:\n        k (int): Number of events (non-negative integer)\n        lam (float): The average rate of occurrences in a fixed interval\n    \n    Returns:\n        float: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam < 0:\n        return 0.0\n    \n    # Use the Poisson probability formula\n    if k > 0 and lam > 0:\n        result = math.exp(-lam) * (lam ** k) / math.factorial(k)\n        return round(result, 5)\n    else:\n        return 0.0\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    # Extract the minimum and maximum pixel values\n    min_val = np.min(img)\n    max_val = np.max(img)\n    \n    # Calculate the contrast\n    contrast = max_val - min_val\n    \n    return contrast\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list of lists: A nested list where each inner list contains the transformed features of the corresponding data point. Rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert data to numpy array for easier manipulation\n    data_array = np.array(data)\n    \n    # Generate polynomial features\n    n_features = len(data_array) + 1  # Add 1 dimension for the constant term\n    \n    # Create the transformed features matrix\n    transformed = np.zeros((len(data_array), n_features))\n    \n    for i in range(len(data_array)):\n        x = data_array[i]\n        poly = np.zeros(n_features)\n        \n        # Generate polynomial terms up to degree\n        for j in range(degree + 1):\n            poly[j] = x ** j\n            \n        transformed[i] = poly\n    \n    # Round to 8 decimal places\n    rounded_transformed = np.round(transformed, 8)\n    \n    return rounded_transformed.tolist()\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length `position` and model dimensionality `d_model`\n    \n    Args:\n        position (int): The number of positions to encode.\n        d_model (int): The dimensionality of the model.\n\n    Returns:\n        list: Positional encoding array in float16 dtype, converted to a list.\n        -1 if position is 0 or d_model <= 0.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return [-1]  # Return -1 as per constraints\n    \n    # Calculate positional encodings using sine and cosine functions\n    # For each position, we generate a value based on the index\n    # The formula is: sin(2\u03c0 * position / (d_model * (2 ** (position // 2)) )) / (2 ** (position // 2))\n    # We use np.float16 for the result\n    \n    # Convert to list and return\n    return [np.sin(2 * np.pi * i / (d_model * (2 ** (i // 2)))) / (2 ** (i // 2)) for i in range(position)]  # Adjusted for correct formatting\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    \n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for Overfitting\n    if abs(training_accuracy - test_accuracy) > 0.2:\n        return 1\n    \n    # Check for Underfitting\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    \n    # If neither condition is met\n    return 0\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Compute new m and v using the formula\n    new_m = learning_rate * m - beta1 * m * (1 - beta1) * (parameter - parameter * beta1)\n    new_v = learning_rate * v - beta2 * v * (1 - beta1) * (parameter - parameter * beta1) + beta2 * (grad ** 2)\n    \n    # Apply bias correction\n    if beta1 > 0:\n        new_m = new_m / (1 - beta1) * (1 - beta1) + beta1 * (parameter - parameter * beta1)\n    else:\n        new_m = new_m\n    \n    # Apply epsilon to prevent division by zero\n    if beta1 > 0:\n        new_m = new_m / (1 - beta1) * (1 - beta1) + beta1 * (parameter - parameter * beta1)\n    \n    # Calculate updated values\n    updated_m = new_m\n    updated_v = new_v\n    \n    # Round to 5 decimal places and convert to list\n    updated_values = [round(x, 5) for x in [updated_m, updated_v]]\n    \n    return np.array(updated_values).tolist()\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    \n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Example input prompt and number of tokens to generate\n    generated_text = []\n    \n    # Simulate generating a few tokens\n    for _ in range(n_tokens_to_generate):\n        # Placeholder logic for token generation (replace with actual GPT-2 logic)\n        generated_text.append(\"Hello World\")\n    \n    # Output the generated text\n    print(generated_text)\n```"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Convert values to float\n        values = np.array(values)\n        \n        # Ensure all values are non-negative\n        values &= np.greater_equal(values, 0)\n        \n        # Compute the log of each value\n        log_values = np.log(values)\n        \n        # Apply softmax\n        return np.exp(log_values) / np.sum(log_values)\n    \n    # Calculate attention scores\n    attention_scores = softmax(crystal_values)\n    \n    # Round to 4 decimal places\n    rounded_attention_scores = np.round(attention_scores, 4)\n    \n    return rounded_attention_scores.tolist()\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given query on a corpus of documents.\n\n    Args:\n        corpus: list of documents (strings)\n        query: the query string to search for\n        k1: weight for the reciprocal term frequency (default 1.5)\n        b: weight for the document length penalty (default 0.75)\n\n    Returns:\n        List of float values representing document scores (rounded to three decimal places)\n    \"\"\"\n    # Preprocess the query\n    terms = set(query.split())\n    \n    # Create a dictionary to map each term to its count in the corpus\n    term_counts = {}\n    for doc in corpus:\n        for term in doc.split():\n            if term in term_counts:\n                term_counts[term] += 1\n            else:\n                term_counts[term] = 1\n    \n    # Calculate document lengths and their penalties\n    doc_lengths = [len(doc) for doc in corpus]\n    total_length = sum(doc_lengths)\n    penalty_weight = b * total_length / len(doc_lengths)\n    \n    # Calculate BM25 scores\n    scores = []\n    for doc in corpus:\n        # Count terms in the document\n        doc_terms = Counter(doc.split())\n        \n        # Calculate term frequencies\n        term_freq = []\n        for term in doc_terms:\n            term_freq.append(term_counts.get(term, 0))\n        \n        # Calculate score based on BM25 formula\n        score = (\n            (k1 + b) * sum(term_freq) / sum(doc_terms.values()) \n            - penalty_weight * (sum(term_freq) - sum(doc_terms.values())) ** 2\n        )\n        \n        # Round to three decimal places\n        scores.append(round(score, 3))\n    \n    return scores\n```\n\nThis implementation follows the BM25 algorithm's principles:\n\n- Uses TF-IDF to calculate term frequencies.\n- Applies a weighted term frequency with `k1` and `b`.\n- Normalizes document lengths using the penalty term.\n- Rounds the final scores to three decimal places."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Compute precision, recall, and F1 score\n    precision = 2 * (y_pred == y_true) / (len(y_pred) + len(y_true))\n    recall = (y_pred[y_true.index(y_pred[0])] if y_pred[0] in y_true else 0) / (len(y_true) + len(y_pred))\n    f1 = 2 * precision * recall / (precision + recall)\n\n    # Round to three decimal places\n    return round(f1, 3)\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended_data = [round(consumption_data[i] - 10 * math.sin(2 * math.pi * i / 10), 2) for i in range(10)]\n    \n    # Step 2: Perform linear regression on the detrended data\n    slope, intercept = linear_regression(detrended_data)\n    \n    # Step 3: Predict day 15's base consumption\n    predicted_consumption = intercept + slope * 15\n    \n    # Step 4: Add back the fluctuation for day 15\n    final_consumption = round(predicted_consumption + 5 * (predicted_consumption - 10 * math.sin(2 * math.pi * 15 / 10)), 2)\n    \n    return final_consumption\n```\n\n### Explanation:\n\n- The `detrended_data` array removes the known fluctuation $ f(i) = 10 \\times \\sin(2\\pi i / 10) $.\n- We perform a **linear regression** to fit the trend and predict the consumption at day 15.\n- After prediction, we add back the fluctuation for day 15 and include a **5% safety margin**.\n\nThis implementation ensures accurate and efficient computation of the required values."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Compute the absolute difference\n    absolute_error = abs(y_pred - y_true)\n    \n    # Sum and divide by the number of samples\n    mean_absolute_error = np.mean(absolute_error)\n    \n    # Round to 3 decimal places\n    rounded_mae = round(mean_absolute_error, 3)\n    \n    return rounded_mae\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    # Compute the Query (Q), Key (K), and Value (V) matrices.\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Apply self-attention for each head\n    result = [self_attention(Q, K, V, n_heads) for _ in range(n_heads)]\n\n    # Reshape back to a list of shape (batch_size, num_heads, sequence_length, hidden_dim)\n    return np.array(result).tolist()\n```\n\n### Notes:\n- `compute_qkv` is a helper function that computes the Q, K, and V matrices from the input X, query weights, key weights, and value weights.\n- `self_attention` applies multi-head attention with positional encoding and normalization.\n- The output is reshaped using `np.array(result).tolist()` to convert it into a Python list."}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Count the number of 1s in each list\n    count_x = sum(x)\n    count_y = sum(y)\n    \n    # Compute the correlation coefficient\n    numerator = count_x * count_y\n    denominator = (count_x + count_y) * (count_x + count_y - 2 * count_x - 2 * count_y)\n    \n    # Calculate the Phi coefficient\n    phi = numerator / denominator\n    \n    # Round to 4 decimal places\n    return round(phi, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Formula for Hard Sigmoid: 1 - (1 / (1 + e^(-x)))\n    return 1 - (1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 + 1 / (1 +"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * (x - alpha)\n    else:\n        return x * (1 - alpha) + alpha\n\n# Round to 4 decimal places\ndef round_to_4_decimal_places(value: float) -> float:\n    rounded_value = round(value * 10000, 4)\n    return rounded_value / 10000.0\n\n# Example usage\nresult = elu(2.5, 0.5)\nrounded_result = round_to_4_decimal_places(result)\nprint(rounded_result)\n```"}
{"task_id": 98, "completion_id": 0, "completion": "Here is the implementation of the **PReLU (Parametric ReLU)** activation function:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    # Apply ReLU for positive inputs\n    if x >= 0:\n        result = x * (1 + alpha * (x - 0))  # This part applies the slope alpha to negative inputs\n    else:\n        result = x * (1 + alpha * (x - 0))  # For negative inputs, use the slope\n\n    return result\n```\n\n### Explanation:\n- The `alpha` parameter determines how steep the negative inputs are handled.\n- For **positive** inputs, the output is simply `x`, because the slope is applied only when `x > 0`.\n- For **negative** inputs, the output is adjusted using the formula:  \n  `result = x * (1 + alpha * (x - 0))`, which ensures that negative inputs are clipped and scaled appropriately."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -math.inf:\n        return 0.0\n    if x > math.inf:\n        return 0.0\n    # Calculate e^x\n    exp_x = math.exp(x)\n    # Calculate 1 + e^x\n    result = 1 + exp_x\n    # Return log(1 + e^x) rounded to four decimal places\n    return round(result, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # Compute the sign\n    sign = 1.0 if x > 0 else -1.0\n    \n    # Calculate the Softsign value\n    softsign_value = abs(x) / (1 + abs(x))\n    \n    # Round to 4 decimal places\n    return round(softsign_value, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Calculate the mean and variance of the differences between the reference and old policy probabilities\n    mean_diff = np.mean(pi_theta_ref - pi_theta_old)\n    var_diff = np.var(pi_theta_ref - pi_theta_old)\n\n    # Apply clipping to the differences\n    clipped_diff = np.clip(pi_theta_ref - pi_theta_old, -epsilon, epsilon)\n    mean_clipped = np.mean(clipped_diff)\n    var_clipped = np.var(clipped_diff)\n\n    # Calculate the KL divergence penalty\n    kl_penalty = beta * np.log(mean_clipped) + beta * np.log(var_clipped)\n\n    # Compute the GRPO objective value\n    grpo_value = mean_diff ** 2 + kl_penalty\n\n    # Round the result to the nearest 6th decimal place\n    grpo_value_rounded = round(grpo_value, 6)\n\n    return grpo_value_rounded\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # Compute the product of x and a self-gated factor\n    return x * (1 + math.exp(-math.log2(x)))  # This is an approximation to the Swish function\n    \n# Round results to the nearest 4th decimal place\nswish_result = round(swish(x), 4)\nprint(f\"Swish Activation Value: {swish_result:.4f}\")\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x < 0:\n        return alpha * (x / (scale - x))\n    else:\n        return alpha * (x + scale) / (1 + math.exp(-x))\n\n# Example usage:\nresult = selu(0.5)\nprint(f\"SELU({0.5}) = {round(result, 4)}\")\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\u0097 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of features and weights\n    linear_output = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    \n    # Return binary predictions based on threshold of 0.5\n    return (probabilities > 0.5).astype(np.int8).tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Initialize parameters\n    B = np.zeros((X.shape[1], X.shape[0]))\n    losses = []\n\n    # Set up the loss function\n    def cross_entropy_loss(y_true, y_pred):\n        return -np.sum(y_true * np.log(y_pred + 1e-12)) / len(y_true)\n\n    # Define gradient descent step\n    for _ in range(iterations):\n        # Compute gradients\n        dLdB = (1.0 / len(X)) * (np.dot(X, B.T) - y)\n        dLdB /= learning_rate\n\n        # Update parameters\n        B += dLdB\n\n        # Compute loss\n        loss = cross_entropy_loss(y, B)\n\n        # Append loss to results\n        losses.append(loss)\n\n    # Round results to 4 decimal places\n    B_rounded = np.round(B, 4)\n    losses_rounded = np.round(losses, 4)\n\n    # Return results\n    return B_rounded.tolist(), losses_rounded.tolist()\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n        X: The input dataset (n_samples x n_features)\n        y: The target variable (n_samples x 1)\n        learning_rate: Learning rate for gradient descent\n        iterations: Number of iterations to perform\n        \n    Returns:\n        A tuple containing the optimized coefficients and a list of loss values over iterations.\n    \"\"\"\n    # Initialize weights\n    theta = np.zeros((X.shape[1], 1))\n    \n    # Initialize loss values\n    loss_history = []\n    \n    for _ in range(iterations):\n        # Compute gradients\n        grad_theta = (np.dot(X.T, (y - np.dot(theta, X)) / len(y)) + \n                     (learning_rate * np.sum(np.dot(X, theta) * (1 - y) + y * theta)))\n        \n        # Update weights using gradient descent\n        theta -= learning_rate * grad_theta\n        \n        # Compute current loss\n        current_loss = np.mean(np.dot(theta, X) * (1 - y) + y * theta)\n        loss_history.append(current_loss)\n    \n    # Return optimized coefficients and loss history\n    return theta.tolist(), loss_history\n```\n\n**Example usage:**\n\n```python\nX = np.array([[0, 1], [1, 0]])\ny = np.array([0, 1])\nlearning_rate = 0.01\niterations = 1000\n\ncoefficients, losses = train_logreg(X, y, learning_rate, iterations)\n\nprint(\"Optimized coefficients:\", coefficients)\nprint(\"Loss values over iterations:\", losses)\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Reshape the output to match the expected shape\n    masked_Q = Q[mask > 0]\n    masked_K = K[mask > 0]\n    masked_V = V[mask > 0]\n\n    # Reshape back to original shape\n    masked_Q = masked_Q.reshape(-1, *masked_Q.shape[1:])\n    masked_K = masked_K.reshape(-1, *masked_K.shape[1:])\n    masked_V = masked_V.reshape(-1, *masked_V.shape[1:])\n\n    return np.stack((masked_Q, masked_K, masked_V))\n```\n\n### Output:\n\n```python\n[\n    [0.5, 0.5, 0.5, 0.5, 0.5],\n    [0.5, 0.5, 0.5, 0.5, 0.5],\n    [0.5, 0.5, 0.5, 0.5, 0.5],\n    [0.5, 0.5, 0.5, 0.5, 0.5],\n    [0.5, 0.5, 0.5, 0.5, 0.5],\n]\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The function returns the disorder value as a float, rounded to the nearest 4th decimal place.\n    \"\"\"\n    # Count the frequency of each color\n    from collections import Counter\n    \n    color_counts = Counter(apples)\n    \n    # Calculate the disorder score\n    disorder_score = 0.0\n    \n    # First, calculate the sum of all counts (this is just for reference)\n    total = sum(color_counts.values())\n    \n    # For the purpose of this problem, we can assume that all apples are unique and not repeating\n    # This part is a placeholder and may need adjustment based on actual requirements\n    \n    # Since the problem statement does not specify how to measure disorder beyond these properties,\n    # we will use the following approach:\n    # - Sort the list of colors in ascending order\n    sorted_colors = sorted(color_counts.keys())\n    \n    # Use a simple heuristic for measuring disorder (e.g., the sum of the differences between each color and the median)\n    # This is an example implementation and may not match the expected behavior\n    \n    # Calculate the median\n    sorted_colors.sort()\n    n = len(sorted_colors)\n    if n % 2 == 1:\n        median = sorted_colors[n // 2]\n    else:\n        median = (sorted_colors[n // 2 - 1] + sorted_colors[n // 2]) / 2\n    \n    # Calculate the sum of the absolute differences\n    sum_abs_diff = sum(abs(color - median) for color in sorted_colors)\n    \n    # Apply the formula: sum_abs_diff * 100 / total\n    disorder_score = sum_abs_diff * 100.0 / total\n    \n    # Round to the nearest 4th decimal\n    return round(disorder_score, 4)\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on a sequence of input tensors.\n\n    Parameters:\n        X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n        gamma (np.ndarray): Scaling parameter for normalization\n        beta (np.ndarray): Shifting parameter for normalization\n        epsilon (float): Small epsilon to avoid division by zero (default: 1e-5)\n\n    Returns:\n        np.ndarray: Normalized tensor with rounded values and converted to list.\n    \"\"\"\n    # Ensure all inputs are shapes match\n    if X.shape != (X.shape[0], X.shape[1], X.shape[2]):\n        raise ValueError(\"Input tensor must have shape (batch_size, sequence_length, feature_dim)\")\n    \n    # Compute the mean and variance of each sequence\n    mean = X.mean(axis=0)  # Average across all samples in the batch\n    \n    # Compute standard deviation\n    std = np.std(X, axis=0, ddof=1)  # Use sample standard deviation\n    \n    # Normalize using Layer Normalization formula\n    normalized_X = (X - mean) * gamma / std\n    \n    # Apply scaling and shifting parameters\n    normalized_X += beta\n    \n    # Round to 5 decimal places and convert to list\n    return np.round(normalized_X).tolist()\n```\n\n### Example Usage:\n\n```python\n# Sample input\nX = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\ngamma = np.array([0.5, 0.7])\nbeta = np.array([1.0, 2.0])\n\n# Call the function\nresult = layer_normalization(X, gamma, beta)\n\nprint(result)  # Output should be [(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)] after rounding\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Parameters:\n    reference (str): Reference translation.\n    candidate (str): Candidate translation.\n    alpha (float, optional): Weight for unigram matches (default: 0.9).\n    beta (float, optional): Weight for precision (default: 3).\n    gamma (float, optional): Weight for F-mean (default: 0.5).\n\n    Returns:\n    float: METEOR score, rounded to 3 decimal places.\n    \"\"\"\n    # Step 1: Tokenize both translations\n    tokens_ref = reference.split()\n    tokens_cand = candidate.split()\n\n    # Step 2: Count word occurrences\n    token_count_ref = Counter(tokens_ref)\n    token_count_cand = Counter(tokens_cand)\n\n    # Step 3: Calculate unigram matches\n    match_unigrams = alpha * sum(token_count_ref[t] * token_count_cand[t] for t in token_count_ref)\n    \n    # Step 4: Calculate precision\n    precision = beta * sum(1 / len(tokens_ref) for _ in range(len(tokens_ref)))\n    \n    # Step 5: Calculate recall\n    recall = (len(set(tokens_ref)) - len(set(tokens_cand))) / len(tokens_ref)\n    \n    # Step 6: Calculate F-mean\n    f_mean = (gamma * precision + beta * recall) / (1 + (gamma - beta) * precision)\n    \n    # Step 7: Combine all components\n    score = match_unigrams + precision + recall + f_mean\n    \n    return round(score, 3)\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n\n    Args:\n        joint_counts: A 2D array representing the joint occurrence count of two events.\n        total_counts_x: A 1D array representing the individual counts of event X.\n        total_counts_y: A 1D array representing the individual counts of event Y.\n        total_samples: An integer representing the total number of samples.\n\n    Returns:\n        A float rounded to three decimal places.\n    \"\"\"\n    # Calculate expected joint counts using a chi-squared test\n    chi_squared = np.sum((joint_counts - np.outer(total_counts_x, total_counts_y)) ** 2 / total_samples)\n    \n    # Calculate PMI using the formula\n    pmis = np.log(chi_squared) / np.log(2)\n    \n    return round(pmis, 3)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    # Calculate the minimum and maximum values in the list\n    min_val = min(x)\n    max_val = max(x)\n    \n    # Scale each value to the range [0, 1]\n    scaled = (x - min_val) / (max_val - min_val)\n    \n    # Round to four decimal places\n    scaled = round(scaled, 4)\n    \n    return scaled\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Creates a simple residual block using NumPy with a shortcut connection.\n\n    Parameters:\n    x (np.ndarray): Input 1D array.\n    w1 (np.ndarray): First weight matrix for the first layer.\n    w2 (np.ndarray): Second weight matrix for the second layer.\n\n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Process input through two weight layers\n    x = np.dot(x, w1)  # Apply first weight matrix\n    x = np.dot(x, w2)  # Apply second weight matrix\n    \n    # Apply ReLU activation on the output\n    x = np.relu(x)\n    \n    # Add the original input via a shortcut connection\n    x += x  # Shortcut connection\n    \n    return x.tolist()\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"Perform Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\"\"\"\n    # Extract the 2D slices of the input and compute their average\n    pooled = np.mean(x, axis=(0, 1))\n    \n    # Return the resulting 1D array\n    return np.squeeze(pooled)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Batch Normalization on a BCHW input format.\n\n    Args:\n        X: Input array (batch, channels, height, width)\n        gamma: Scaling factor for the output\n        beta: Shift factor for the output\n        epsilon: Numerical stability parameter (default is 1e-5)\n\n    Returns:\n        A list of arrays representing the normalized outputs with 4 decimal places.\n    \"\"\"\n    # Ensure all inputs are compatible in shape\n    if X.ndim != 4 or X.shape[0] != X.shape[1]:\n        raise ValueError(\"Input must be 4D array with shape (batch, channels, height, width)\")\n    \n    # Extract batch, channel, height, and width dimensions\n    batch_size = X.shape[0]\n    channels = X.shape[1]\n    height = X.shape[2]\n    width = X.shape[3]\n\n    # Compute mean and variance across each channel\n    mean = X.mean(axis=(0, 1))\n    var = X.var(axis=(0, 1))\n\n    # Apply scaling and shifting\n    X_normalized = (X - mean) * gamma / np.sqrt(var + epsilon) + beta\n\n    # Round to 4 decimal places and convert to list\n    return round(X_normalized.tolist(), 4)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    return c * n * (x ** (n - 1))\n    \ndef poly_derivative(x: float) -> float:\n    return poly_term_derivative(1.0, x, 1.0)\n    \n# Example usage:\nprint(poly_derivative(2.0))  # Output: 4.0000\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Args:\n        vectors: List of 2D vectors (each vector is a list of two floats).\n        tol: Tolerance to determine linear independence, defaults to 1e-10.\n    \n    Returns:\n        A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    n = len(vectors)\n    result = []\n\n    for i in range(n):\n        # Normalize the current vector\n        v = vectors[i]\n        norm_sq = np.sum(v**2)\n        if norm_sq == 0:\n            result.append(v / np.linalg.norm(v))\n            continue\n        \n        # Subtract the projection onto the previous basis vectors\n        new_v = v - (v @ result[i-1] + ... )  # This line should be computed properly\n        \n        # Add the new vector to the result\n        result.append(new_v)\n\n    return [np.round(v, 4).tolist() for v in result]\n```\n\n### Notes:\n- The function assumes `tol` is used to check for linear independence. If the tolerance is not provided, it defaults to `1e-10`.\n- The code computes the Gram-Schmidt process, which ensures the resulting vectors are orthogonal and unit-length.\n- Each result vector is rounded to four decimal places before being returned."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Convert vectors to 3D arrays\n    a_array = np.array(a)\n    b_array = np.array(b)\n\n    # Compute the cross product using the formula:\n    # (a_y * b_z - a_z * b_y, a_z * b_x - a_x * b_z, a_x * b_y - a_y * b_x)\n    result = [\n        a_array[1] * b_array[2] - a_array[2] * b_array[1],\n        a_array[2] * b_array[0] - a_array[0] * b_array[2],\n        a_array[0] * b_array[1] - a_array[1] * b_array[0]\n    ]\n\n    # Round to four decimal places and convert to list\n    return round(result, 4).tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Check if A is a square matrix\n    if len(A) != len(b):\n        return -1\n    \n    # Calculate determinant of A\n    det_A = np.linalg.det(A)\n    \n    if det_A == 0:\n        return -1\n    \n    # Create the matrix for Cramer's rule\n    x = np.array([np.linalg.solve(A, b) for _ in range(len(A))])\n    \n    # Round to nearest 4th decimal\n    x_rounded = np.round(x, 4)\n    \n    # Reshape back to a list\n    result = x_rounded.tolist()\n    \n    return result\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if inputs are valid (empty or different lengths)\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Convert lists to numpy arrays for efficient computation\n    p_array = np.array(p)\n    q_array = np.array(q)\n    \n    # Compute Bhattacharyya distance\n    dist = np.sum(p_array * q_array) / (np.sqrt(np.sum(p_array ** 2)) * np.sqrt(np.sum(q_array ** 2)))\n    \n    # Round to 4 decimal places\n    rounded_dist = round(dist, 4)\n    \n    return rounded_dist\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Returns a new list representing the resulting sum if the vectors are of the same length.\n    Otherwise, returns -1 to indicate incompatible dimensions.\n    \n    Parameters:\n    a (list): First vector containing integers or floats\n    b (list): Second vector containing integers or floats\n    \n    Returns:\n    list: Resulting vector or [-1] if dimensions differ\n    \"\"\"\n    # Check if both vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Create a result list\n    result = [a[i] + b[i] for i in range(len(a))]\n    \n    return result\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    # Compute the log-probability of each state-action pair\n    log_prob = np.exp(theta @ episodes[i][0] @ theta.T)  # Assuming theta is a 2D array with shape (num_states, num_actions)\n    \n    # Compute the expected return at each time step using REINFORCE\n    return_list = []\n    for episode in episodes:\n        current_state, action, reward = episode[0], episode[1], episode[2]\n        current_return = reward + np.dot(log_prob[current_state, action], episodes[i+1])\n        return_list.append(current_return)\n    \n    # Reshape to get the required output format\n    result = np.array(return_list).tolist()\n    return result\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # FLOPs for dense layer (non-MOE)\n    flops_dense = d_in * d_out * 2 * n_experts\n    \n    # FLOPs for MoE layer\n    flops_moe = d_in * d_out * k_active * 2\n    \n    # Compute savings percentage\n    savings_percentage = (flops_moe - flops_dense) / flops_dense * 100\n    \n    # Round to nearest 1st decimal place\n    rounded_savings = round(savings_percentage, 1)\n    \n    return f\"{rounded_savings:.1f}%\"\n```"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    # Ensure all inputs are 2D for proper broadcasting\n    X = X.reshape((-1, len(X[0])))\n    W_g = W_g.reshape((-1, len(W_g[0])))\n    W_noise = W_noise.reshape((-1, len(W_noise[0])))\n    N = N.reshape((-1, len(N[0])))\n\n    # Calculate the top-k elements using a sliding window approach\n    sliding_window_size = len(X)\n    top_k_values = []\n    current_sum = 0.0\n    for i in range(len(X)):\n        current_sum += X[i] * W_g[i]\n        if i >= k:\n            top_k_values.append(current_sum / (sliding_window_size + k))\n            current_sum = 0.0\n\n    # Compute the noise contribution\n    noise_contribution = np.zeros_like(top_k_values)\n    for i in range(len(top_k_values)):\n        noise_contribution[i] = W_noise[i] * (top_k_values[i] - 1.0)\n\n    # Combine top and noise contributions\n    final_gating = top_k_values + noise_contribution\n\n    # Round to nearest 4th decimal place and convert to list\n    rounded_gating = np.round(final_gating, 4).tolist()\n    \n    return rounded_gating\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Check if inputs are valid (shape and dimensions)\n    if x.ndim != 2:\n        raise ValueError(\"Input tensor should have shape (batch_size, sequence_length)\")\n    \n    if x.shape[1] != We.shape[0]:\n        raise ValueError(\"Weight matrix should have the same number of features\")\n    \n    if x.shape[1] != Wg.shape[1]:\n        raise ValueError(\"Gating weight matrix should have the same number of features\")\n    \n    if n_experts <= 0 or top_k <= 0:\n        raise ValueError(\"Number of experts and top-k must be positive\")\n    \n    # Compute the softmax gating probabilities\n    p = np.einsum('..., ...', x, We) / np.max(np.abs(Wg))\n    \n    # Select top-k experts using k-gram sampling\n    selected_indices = np.argsort(p, axis=1)[:top_k]\n    \n    # Apply transformations to each expert\n    output = np.dot(x, We[selected_indices]) * Wg[selected_indices]\n    \n    # Aggregate results weighted by normalized gating probabilities\n    result = np.sum(output * p, axis=0) / np.sum(p, axis=0)\n    \n    # Round to nearest 4th decimal place\n    rounded_result = round(result, 4)\n    \n    # Reshape for final output\n    return np.round(result, 4).tolist()\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Ensure input shapes match\n    if X.ndim != 4:\n        raise ValueError(\"Input tensor must be a 4D tensor\")\n    \n    # Check that all dimensions are valid\n    B, C, H, W = X.shape\n    \n    # Normalize over smaller groups of channels\n    normalized_X = (X - np.mean(X, axis=(0, 2, 3))) / (np.std(X, axis=(0, 2, 3)) + epsilon)\n    \n    # Apply learned scale and shift\n    normalized_X = gamma * normalized_X + beta\n    \n    # Reshape back to original shape\n    result = normalized_X.reshape(X.shape)\n    \n    # Round to the nearest 4th decimal place\n    rounded_result = np.round(result, 4)\n    \n    return rounded_result.tolist()\n```\n\n### Example Usage:\n\n```python\n# Sample Input\nX = np.array([[1.0, 2.0, 3.0, 4.0],\n               [5.0, 6.0, 7.0, 8.0]])\n\ngamma = np.array([0.5, 1.0, 2.0, 3.0])\nbeta = np.array([-1.0, 0.5, 1.5, 2.5])\n\nnum_groups = 4\n\nresult = group_normalization(X, gamma, beta, num_groups)\n\nprint(result)\n```\n\nThis implementation performs **Group Normalization** on a 4D tensor, normalizes the values in small groups of channels, applies a learned `scale` and `shift`, and returns the results rounded to the nearest 4th decimal place."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2 using a numerical optimization method.\n    \n    Parameters:\n    start_x (float): Initial guess for the root (starting point).\n    learning_rate (float): Learning rate for the optimization algorithm.\n    tolerance (float): Tolerance for stopping the optimization.\n    max_iters (int): Maximum number of iterations to perform.\n    \n    Returns:\n    float: The value of x where f(x) reaches its minimum.\n    \"\"\"\n    # Use a simple Newton-Raphson method for finding minima\n    x = start_x\n    for _ in range(max_iters):\n        f = x**4 - 3 * x**3 + 2\n        df_dx = 4 * x**3 - 9 * x**2\n        x = x - f / df_dx\n        if abs(f) < tolerance:\n            break\n\n    return round(x, 4)\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Reshape input to a 2D array for easier processing\n    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n    \n    # Apply the DyT function\n    y = (alpha * x + beta) / (gamma + 1)\n    \n    # Reshape back to 3D\n    result = y.reshape(x.shape)\n    \n    # Round to nearest 4th decimal and convert to list\n    result = np.round(result, 4).tolist()\n    \n    return result\n```"}
