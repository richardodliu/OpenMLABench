{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing a matrix.\n    b (list[int|float]): A 1D list representing a vector.\n\n    Returns:\n    list[int|float] | int: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a:\n        return -1\n    num_cols_a = len(a[0])\n    if num_cols_a != len(b):\n        return -1\n    if not all((len(row) == num_cols_a for row in a)):\n        raise ValueError(\"All rows in the matrix 'a' must have the same number of columns\")\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing the input matrix.\n\n    Returns:\n    list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    if not isinstance(a, list) or not all((isinstance(row, list) for row in a)):\n        raise ValueError('Input must be a list of lists')\n    if not a:\n        return []\n    num_cols = len(a[0])\n    if not all((len(row) == num_cols for row in a)):\n        raise ValueError('All rows must have the same number of columns')\n    return [list(row) for row in zip(*a)]\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n    new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n    list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    reshaped_list = reshaped_array.tolist()\n    return reshaped_list\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    - matrix (list[list[float]]): A 2D list of numbers.\n    - mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    - list[float]: A list of means according to the specified mode.\n\n    Raises:\n    - ValueError: If the mode is neither 'row' nor 'column'.\n    - ValueError: If the matrix is empty or not a list of lists.\n    - ZeroDivisionError: If a row or column is empty.\n    \"\"\"\n    if not matrix or not all((isinstance(row, list) for row in matrix)):\n        raise ValueError('Matrix must be a non-empty list of lists')\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        row_lengths = set(map(len, matrix))\n        if len(row_lengths) > 1:\n            raise ValueError('All rows must have the same length for column mode')\n        return [sum(col) / len(col) for col in zip(*matrix)]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    This function performs scalar multiplication on a given matrix.\n\n    Args:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix with.\n\n    Returns:\n    list[list[int|float]]: The result of the scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\nfrom typing import Union, List\ndef calculate_eigenvalues(matrix: List[List[Union[float, int]]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (List[List[Union[float, int]]]): A 2x2 matrix.\n\n    Returns:\n    List[float]: A list containing the eigenvalues in descending order.\n\n    Raises:\n    ValueError: If the input matrix is not a 2x2 matrix.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^{-1}AS, \n    where T and S are invertible matrices.\n\n    Args:\n    A (list[list[int|float]]): The input matrix to be transformed.\n    T (list[list[int|float]]): The first invertible matrix.\n    S (list[list[int|float]]): The second invertible matrix.\n\n    Returns:\n    list[list[int|float]]: The transformed matrix, or -1 if the transformation is not possible.\n    \"\"\"\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    if A.shape[0] != T.shape[0] or A.shape[1] != S.shape[0]:\n        return -1\n    try:\n        T_inv = np.linalg.inv(T)\n        transformed_A = np.dot(np.dot(T_inv, A), S)\n        transformed_A = np.round(transformed_A, 4)\n        return transformed_A.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[list[float]]: The inverse of the input matrix, or None if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices.\n\n    Args:\n    a (list[list[int | float]]): The first matrix.\n    b (list[list[int | float]]): The second matrix.\n\n    Returns:\n    list[list[int | float]]: The product of the two matrices. Returns -1 if the matrices are incompatible for multiplication.\n    \"\"\"\n    if not a or not b:\n        return -1\n    num_rows_a = len(a)\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    num_cols_b = len(b[0])\n    if num_cols_a != num_rows_b:\n        return -1\n    result = [[0 for _ in range(num_cols_b)] for _ in range(num_rows_a)]\n    for i in range(num_rows_a):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport numpy as np\nfrom typing import List\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    vector_array = np.array(vectors)\n    covariance_matrix = np.cov(vector_array, rowvar=False)\n    return covariance_matrix.tolist()\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n\n    Args:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A vector representing the constants on the right-hand side of the equations.\n    n (int): The number of iterations to perform.\n\n    Returns:\n    list: The approximate solution x after n iterations.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert b.shape[0] == A.shape[0], 'Vector b must have the same number of rows as matrix A'\n    assert n > 0 and isinstance(n, int), 'n must be a positive integer'\n    num_equations = A.shape[0]\n    x = np.zeros(num_equations)\n    for _ in range(n):\n        new_x = np.zeros(num_equations)\n        for i in range(num_equations):\n            sum_terms = np.sum([A[i, j] * x[j] for j in range(num_equations) if j != i])\n            new_x[i] = (b[i] - sum_terms) / A[i, i]\n        x = np.round(new_x, 4)\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the left singular vectors (U), singular values, and right singular vectors (V).\n    \"\"\"\n    assert isinstance(A, np.ndarray) and A.shape == (2, 2), 'Input must be a 2x2 NumPy array'\n    A_T_A = np.dot(A.T, A)\n    V = np.eye(2)\n    for _ in range(10):\n        if abs(A_T_A[0, 1]) > abs(A_T_A[1, 0]):\n            (p, q) = (0, 1)\n        else:\n            (p, q) = (1, 0)\n        if A_T_A[p, p] == A_T_A[q, q]:\n            theta = np.pi / 4\n        else:\n            theta = 0.5 * np.arctan2(2 * A_T_A[p, q], A_T_A[p, p] - A_T_A[q, q])\n        c = np.cos(theta)\n        s = np.sin(theta)\n        J = np.array([[c, -s], [s, c]])\n        A_T_A = np.dot(J.T, np.dot(A_T_A, J))\n        V = np.dot(V, J)\n    singular_values = np.sqrt(np.diag(A_T_A))\n    U = np.dot(A, np.dot(V, np.diag(1 / singular_values)))\n    if np.linalg.det(U) < 0:\n        U[:, 0] *= -1\n        singular_values[0] *= -1\n    if np.linalg.det(V) < 0:\n        V[:, 0] *= -1\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    V = np.round(V, 4)\n    return (U, singular_values, V)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_3x3(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 3x3 matrix.\n\n    Args:\n    matrix: A 3x3 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n    \"\"\"\n    (a, b, c) = (matrix[0][0], matrix[0][1], matrix[0][2])\n    (d, e, f) = (matrix[1][0], matrix[1][1], matrix[1][2])\n    (g, h, i) = (matrix[2][0], matrix[2][1], matrix[2][2])\n    return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n\n    Args:\n    matrix: A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input matrix must be a 4x4 matrix')\n    det = 0.0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det += (-1) ** i * matrix[0][i] * determinant_3x3(minor)\n    return det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Args:\n    X (list[list[float]]): A 2D list representing the feature matrix.\n    y (list[float]): A list representing the target vector.\n\n    Returns:\n    list[float]: A list of coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    list: Coefficients of the linear regression model.\n    \"\"\"\n    m = X.shape[0]\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        gradient = 1 / m * np.dot(X.T, np.dot(X, coefficients) - y)\n        coefficients = coefficients - alpha * gradient\n    return np.round(coefficients, 4).tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a given dataset using both standardization and min-max normalization.\n\n    Args:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n    tuple[list[list[float]], list[list[float]]]: Two 2D lists, the first one scaled by standardization and the second by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / np.where(std_dev == 0, 1, std_dev)\n    standardized_data = np.where(std_dev == 0, 0, standardized_data)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    data_min = np.min(data, axis=0)\n    data_max = np.max(data, axis=0)\n    min_max_normalized_data = (data - data_min) / np.where(data_max - data_min == 0, 1, data_max - data_min)\n    min_max_normalized_data = np.where(data_max - data_min == 0, 0.5, min_max_normalized_data)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data, min_max_normalized_data)\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function performs k-Means clustering on a given set of points.\n\n    Args:\n    - points: A list of points, where each point is a tuple of coordinates.\n    - k: An integer representing the number of clusters to form.\n    - initial_centroids: A list of initial centroid points, each a tuple of coordinates.\n    - max_iterations: An integer representing the maximum number of iterations to perform.\n\n    Returns:\n    - A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    - X (np.ndarray): Feature dataset.\n    - y (np.ndarray): Target variable.\n    - k (int, optional): Number of folds. Defaults to 5.\n    - shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    - random_seed (int, optional): Seed for the random number generator. Defaults to None.\n\n    Returns:\n    - list: A list of tuples containing train-test indices for each fold.\n    \"\"\"\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    splits = []\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = (i + 1) * fold_size\n        if i == k - 1:\n            test_end = n_samples\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n        splits.append((train_indices, test_indices))\n    return splits\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the given dataset.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - list[list[float]]: The principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k].T\n    principal_components = np.round(principal_components, 4).tolist()\n    return principal_components\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using recursive binary splitting based on entropy and information gain.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attributes (list[str]): A list of attribute names.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - dict: A nested dictionary representing the decision tree.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attr: {}}\n    attr_values = set((example[best_attr] for example in examples))\n    for value in attr_values:\n        subtree_examples = [example for example in examples if example[best_attr] == value]\n        subtree_attributes = [attr for attr in attributes if attr != best_attr]\n        tree[best_attr][value] = learn_decision_tree(subtree_examples, subtree_attributes, target_attr)\n    return tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculates the entropy of a list of examples with respect to a target attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The entropy of the examples with respect to the target attribute.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    value_counts = Counter(target_values)\n    total = len(target_values)\n    return -sum((count / total * math.log2(count / total) for count in value_counts.values()))\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the information gain of splitting a list of examples on an attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attr (str): The name of the attribute to split on.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The information gain of splitting the examples on the attribute.\n    \"\"\"\n    total_entropy = entropy(examples, target_attr)\n    attr_values = set((example[attr] for example in examples))\n    weighted_entropy = sum((len([example for example in examples if example[attr] == value]) / len(examples) * entropy([example for example in examples if example[attr] == value], target_attr) for value in attr_values))\n    return total_entropy - weighted_entropy\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str, optional): The choice of kernel. Defaults to 'linear'. Options: 'linear', 'rbf'.\n    - lambda_val (float, optional): The regularization parameter. Defaults to 0.01.\n    - iterations (int, optional): The number of iterations. Defaults to 100.\n    - sigma (float, optional): The sigma value for the RBF kernel. Defaults to 1.0.\n\n    Returns:\n    - alpha (list): The model's alpha coefficients.\n    - bias (float): The model's bias.\n    \"\"\"\n    assert data.shape[0] == labels.shape[0], 'Data and labels must have the same number of samples'\n    if np.any(labels != -1) and np.any(labels != 1):\n        labels = np.where(labels > 0, 1, -1)\n    num_samples = data.shape[0]\n    alpha = np.zeros(num_samples)\n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (t * lambda_val)\n        if kernel == 'linear':\n            predictions = np.dot(data, data.T).dot(alpha)\n        elif kernel == 'rbf':\n            pairwise_distances = np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n            kernel_matrix = np.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n            predictions = kernel_matrix.dot(alpha)\n        else:\n            raise ValueError(\"Invalid kernel. Options: 'linear', 'rbf'.\")\n        hinge_loss = np.maximum(0, 1 - labels * (predictions + alpha.dot(np.zeros(num_samples)) - alpha.dot(alpha * labels)))\n        alpha = alpha - learning_rate * lambda_val * alpha\n        alpha += learning_rate * (labels * (hinge_loss > 0))\n    if kernel == 'linear':\n        bias = np.mean(labels - np.dot(data, data.T).dot(alpha * labels))\n    elif kernel == 'rbf':\n        pairwise_distances = np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n        kernel_matrix = np.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n        bias = np.mean(labels - kernel_matrix.dot(alpha * labels))\n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n    return (alpha.tolist(), bias)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n    z (float): The input to the sigmoid function.\n\n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    return round(sigmoid_output, 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n    scores (list[float]): A list of scores.\n\n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Computes the sigmoid of x.\n    \n    Args:\n    x (float): Input to the sigmoid function.\n    \n    Returns:\n    float: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): Associated true binary labels.\n    weights (list[float]): The neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n\n    Returns:\n    tuple[list[float], float]: A tuple containing the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n    \"\"\"\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    labels_array = np.array(labels)\n    linear_combination = np.dot(features_array, weights_array) + bias\n    predicted_probabilities = np.vectorize(sigmoid)(linear_combination)\n    mean_squared_error = np.mean((predicted_probabilities - labels_array) ** 2)\n    return (np.round(predicted_probabilities, 4).tolist(), round(mean_squared_error, 4))\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"The sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    - features (np.ndarray): A 2D array where each row is a feature vector.\n    - labels (np.ndarray): A 1D array of true binary labels associated with the feature vectors.\n    - initial_weights (np.ndarray): The initial weights of the neuron.\n    - initial_bias (float): The initial bias of the neuron.\n    - learning_rate (float): The learning rate for gradient descent.\n    - epochs (int): The number of epochs to train the neuron.\n\n    Returns:\n    - updated_weights (np.ndarray): The updated weights of the neuron.\n    - updated_bias (float): The updated bias of the neuron.\n    - mse_values (list[float]): A list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_predictions = 2 * (predictions - labels)\n        d_linear_combination = d_predictions * predictions * (1 - predictions)\n        d_weights = np.dot(features.T, d_linear_combination)\n        d_bias = np.sum(d_linear_combination)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (np.round(weights, 4), round(bias, 4), mse_values)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initialize a Value object.\n\n        Args:\n        - data (float): The scalar value.\n        - _children (tuple, optional): The children nodes. Defaults to ().\n        - _op (str, optional): The operation that created this node. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the Value object.\n        \"\"\"\n        return f'Value(data={self.data}, grad={self.grad})'\n\n    def __add__(self, other):\n        \"\"\"\n        Implement addition operation.\n\n        Args:\n        - other (Value or float): The other operand.\n\n        Returns:\n        - Value: The result of the addition.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        \"\"\"\n        Implement multiplication operation.\n\n        Args:\n        - other (Value or float): The other operand.\n\n        Returns:\n        - Value: The result of the multiplication.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        \"\"\"\n        Implement ReLU activation.\n\n        Returns:\n        - Value: The result of the ReLU activation.\n        \"\"\"\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            if self.data > 0:\n                self.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \"\"\"\n        Compute gradients through automatic differentiation.\n        \"\"\"\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    This function computes the transformation matrix P from basis B to C.\n\n    Args:\n    B (list[list[int]]): Basis vectors in basis B.\n    C (list[list[int]]): Basis vectors in basis C.\n\n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C.\n    \"\"\"\n    B_array = np.array(B, dtype=float)\n    C_array = np.array(C, dtype=float)\n    C_inv = np.linalg.inv(C_array)\n    P = np.dot(C_inv, B_array)\n    P = np.round(P, 4)\n    return P.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[2, 0, 0], [0, 3, 0], [0, 0, 4]]\nP = transform_basis(B, C)\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V^T.\n    \"\"\"\n    A = np.array(A)\n    assert A.shape == (2, 2), 'Input matrix must be 2x2'\n    A_T_A = np.dot(A.T, A)\n    (eigen_values, V) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigen_values)[::-1]\n    eigen_values = eigen_values[idx]\n    V = V[:, idx]\n    if np.linalg.det(V) < 0:\n        V[:, 1] *= -1\n    singular_values = np.sqrt(eigen_values)\n    S = np.diag(singular_values)\n    U = np.dot(A, np.dot(V, np.linalg.inv(S)))\n    if np.linalg.det(U) < 0:\n        U[:, 1] *= -1\n        S[1, 1] *= -1\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Args:\n        X (numpy array): The feature dataset.\n        y (numpy array): The target dataset.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y datasets.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same length'\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return [X_shuffled_list, y_shuffled_list]\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 2])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates a batch iterator for the given numpy array X and optional numpy array y.\n\n    Args:\n    - X (numpy array): The input data.\n    - y (numpy array, optional): The target data. Defaults to None.\n    - batch_size (int, optional): The size of each batch. Defaults to 64.\n\n    Yields:\n    - batches of X if y is None, otherwise batches of (X, y) pairs.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise TypeError('X must be a numpy array')\n    if y is not None and (not isinstance(y, np.ndarray)):\n        raise TypeError('y must be a numpy array or None')\n    if y is not None and len(X) != len(y):\n        raise ValueError('X and y must have the same length')\n    num_batches = int(np.ceil(len(X) / batch_size))\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(X))\n        X_batch = X[start_idx:end_idx]\n        y_batch = y[start_idx:end_idx] if y is not None else None\n        if y_batch is not None:\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset into two subsets based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n    - X (numpy array): The input dataset.\n    - feature_i (int): The index of the feature to check.\n    - threshold (float): The threshold value.\n\n    Returns:\n    - subset1 (list): Samples where the feature value is greater than or equal to the threshold.\n    - subset2 (list): Samples where the feature value is less than the threshold.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise ValueError('Input dataset must be a numpy array')\n    if feature_i < 0 or feature_i >= X.shape[1]:\n        raise IndexError('Feature index out of range')\n    subset1 = X[X[:, feature_i] >= threshold].tolist()\n    subset2 = X[X[:, feature_i] < threshold].tolist()\n    return (subset1, subset2)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): Degree of polynomial features.\n\n    Returns:\n    list: A list of polynomial features up to the specified degree.\n    \"\"\"\n    n_features = X.shape[1]\n    poly_features = []\n    for d in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), d):\n            feature = np.prod(X[:, combo], axis=1)\n            poly_features.append(feature)\n    poly_features = np.array(poly_features).T\n    return poly_features.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (2D numpy array): Feature dataset.\n    y (1D numpy array): Target variable.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements. Default is True.\n    seed (int): Random seed for reproducibility. Default is 42.\n\n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": false, "result": "failed: n_col must be a positive integer", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encoding of nominal values.\n\n    Parameters:\n    x (numpy.ndarray): A 1D numpy array of integer values.\n    n_col (int, optional): The number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    list: A Python list representing the one-hot encoded array after reshaping.\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError('Input must be a numpy array')\n    if len(x.shape) != 1:\n        raise ValueError('Input array must be 1D')\n    if n_col is None:\n        n_col = np.max(x) + 1\n    if not isinstance(n_col, int) or n_col <= 0:\n        raise ValueError('n_col must be a positive integer')\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Args:\n    x (numpy.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D Python list representing the diagonal matrix.\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError('Input must be a numpy array')\n    if len(x.shape) != 1:\n        raise ValueError('Input array must be 1D')\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy array): 1D numpy array containing the true labels.\n    y_pred (numpy array): 1D numpy array containing the predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    assert y_true.shape == y_pred.shape, 'Input arrays must have the same shape'\n    correct_predictions = y_true == y_pred\n    num_correct = np.sum(correct_predictions)\n    total_labels = len(y_true)\n    accuracy = num_correct / total_labels\n    return round(accuracy, 4)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Args:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): An optional 2D numpy array. Defaults to None.\n\n    Returns:\n    list: The correlation matrix as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    assert X.shape[0] == Y.shape[0], 'X and Y must have the same number of observations'\n    correlation_matrix = np.corrcoef(X.T, Y.T)[:X.shape[1], X.shape[1]:]\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (numpy array): 2D array of shape (n_samples, n_features) representing the dataset.\n    y (numpy array): 1D array of shape (n_samples,) representing the labels.\n    n_clf (int): Number of classifiers.\n\n    Returns:\n    list: A list of classifiers with their parameters.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = None\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                predictions = np.where(X[:, feature] <= threshold, -1, 1)\n                error = np.sum(w[y != predictions])\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'error': min_error}\n                predictions = np.where(X[:, feature] > threshold, -1, 1)\n                error = np.sum(w[y != predictions])\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'error': min_error, 'invert': True}\n        alpha = 0.5 * math.log((1 - best_clf['error']) / best_clf['error'])\n        predictions = np.where(X[:, best_clf['feature']] <= best_clf['threshold'], -1, 1)\n        if 'invert' in best_clf:\n            predictions = np.where(X[:, best_clf['feature']] > best_clf['threshold'], -1, 1)\n        w *= np.exp(-alpha * y * predictions)\n        w /= np.sum(w)\n        classifiers.append({'feature': best_clf['feature'], 'threshold': best_clf['threshold'], 'alpha': round(alpha, 4), 'invert': 'invert' in best_clf})\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    This function computes the log-softmax of a given 1D array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list representing the log-softmax of the input scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    log_softmax_values = stable_scores - np.log(np.sum(np.exp(stable_scores)))\n    return np.round(log_softmax_values, 4).tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_optimizer = copy.copy(optimizer)\n        self.w0_optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_wrt_input = np.dot(accum_grad, self.W.T)\n        grad_wrt_W = np.dot(self.layer_input.T, accum_grad)\n        grad_wrt_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, grad_wrt_W)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_wrt_w0)\n        return grad_wrt_input\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    Applies a 2D convolution operation on the input matrix using the given kernel, padding, and stride.\n\n    Args:\n    - input_matrix (np.ndarray): The input matrix to be convolved.\n    - kernel (np.ndarray): The convolutional kernel.\n    - padding (int): The number of pixels to pad the input matrix with.\n    - stride (int): The stride of the convolution operation.\n\n    Returns:\n    - list: The result of the convolution operation as a Python list, rounded to the nearest 4th decimal.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    return np.round(output_matrix, 4).tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n    z (float): The input value.\n\n    Returns:\n    float: The value after applying the ReLU function.\n    \"\"\"\n    return max(0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    This function calculates the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the feature matrix.\n    w (np.ndarray): A 1D numpy array representing the coefficients.\n    y_true (np.ndarray): A 1D numpy array representing the true labels.\n    alpha (float): A float representing the regularization parameter.\n\n    Returns:\n    float: The Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    n = len(y_true)\n    mse = np.sum((y_pred - y_true) ** 2) / n\n    reg_term = alpha * np.sum(w ** 2)\n    ridge_loss = mse + reg_term\n    return round(ridge_loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n    z (float): The input to the Leaky ReLU function.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n    float: The value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Args:\n        x1 (numpy.ndarray): The first input vector.\n        x2 (numpy.ndarray): The second input vector.\n\n    Returns:\n        float: The linear kernel value between x1 and x2.\n\n    Raises:\n        ValueError: If the input vectors are not of the same shape.\n    \"\"\"\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError('Input vectors must be numpy arrays')\n    if x1.shape != x2.shape:\n        raise ValueError('Input vectors must have the same shape')\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): Array containing true binary labels.\n    y_pred (numpy array): Array containing predicted binary labels.\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    assert y_true.shape == y_pred.shape, 'y_true and y_pred must have the same shape'\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    precision_value = tp / (tp + fp)\n    return precision_value\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    This function performs three variants of gradient descent - Stochastic Gradient Descent (SGD), \n    Batch Gradient Descent, and Mini-Batch Gradient Descent - using Mean Squared Error (MSE) as the loss function.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target variable\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations for gradient descent\n    batch_size (int): Size of the batch for Mini-Batch Gradient Descent (default is 1)\n    method (str): Type of gradient descent to perform - 'batch', 'stochastic', or 'mini-batch' (default is 'batch')\n\n    Returns:\n    list: The optimized weights after gradient descent\n    \"\"\"\n    if method not in ['batch', 'stochastic', 'mini-batch']:\n        raise ValueError(\"Invalid method. Please choose from 'batch', 'stochastic', or 'mini-batch'.\")\n    weights = np.array(weights)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            gradient = 2 / len(X) * np.dot(X.T, predictions - y)\n            weights = weights - learning_rate * gradient\n        elif method == 'stochastic':\n            indices = np.arange(len(X))\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n            for i in range(len(X)):\n                prediction = np.dot(X[i], weights)\n                gradient = 2 * (prediction - y[i]) * X[i]\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            indices = np.arange(len(X))\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n            for i in range(0, len(X), batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                predictions = np.dot(X_batch, weights)\n                gradient = 2 / len(X_batch) * np.dot(X_batch.T, predictions - y_batch)\n                weights = weights - learning_rate * gradient\n    return np.round(weights, 4).tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n    matrix (list): A 2D list representing the input matrix.\n\n    Returns:\n    list: The RREF of the input matrix as a Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (num_rows, num_cols) = matrix.shape\n    current_row = 0\n    current_col = 0\n    while current_row < num_rows and current_col < num_cols:\n        max_row = np.argmax(np.abs(matrix[current_row:, current_col])) + current_row\n        if matrix[max_row, current_col] == 0:\n            current_col += 1\n            continue\n        matrix[[current_row, max_row]] = matrix[[max_row, current_row]]\n        pivot = matrix[current_row, current_col]\n        matrix[current_row] /= pivot\n        for row in range(num_rows):\n            if row != current_row:\n                factor = matrix[row, current_col]\n                matrix[row] -= factor * matrix[current_row]\n        current_row += 1\n        current_col += 1\n    return matrix.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam Optimization Algorithm.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of `f`.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 1000.\n\n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    x = x0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.square(g)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\nx0 = np.array([10.0, 20.0])\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    This function implements Lasso Regression using Gradient Descent.\n\n    Parameters:\n    X (np.array): Feature matrix\n    y (np.array): Target variable\n    alpha (float): Regularization parameter (default = 0.1)\n    learning_rate (float): Learning rate for gradient descent (default = 0.01)\n    max_iter (int): Maximum number of iterations (default = 1000)\n    tol (float): Tolerance for convergence (default = 1e-4)\n\n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        weights_grad = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        bias_grad = -2 / n_samples * np.sum(y - y_pred)\n        new_weights = weights - learning_rate * weights_grad\n        new_bias = bias - learning_rate * bias_grad\n        if np.linalg.norm(weights - new_weights) < tol and np.abs(bias - new_bias) < tol:\n            break\n        weights = new_weights\n        bias = new_bias\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights.tolist(), bias)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n\n    The OSA distance is the minimum number of operations (insertions, deletions, \n    substitutions, and transpositions of adjacent characters) required to transform \n    one string into another.\n\n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n\n    Returns:\n    int: The OSA distance between source and target.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    d = np.zeros((m + 1, n + 1), dtype=int)\n    d[:, 0] = np.arange(m + 1)\n    d[0, :] = np.arange(n + 1)\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            substitution_cost = 0 if source[i - 1] == target[j - 1] else 1\n            d[i, j] = min(d[i - 1, j] + 1, d[i, j - 1] + 1, d[i - 1, j - 1] + substitution_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + 1)\n    return d[m, n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in a binary classification setting.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        recall_value = 0.0\n    else:\n        recall_value = tp / denominator\n    recall_value = round(recall_value, 3)\n    return recall_value\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention output for the given input X and weight matrices W_q, W_k, W_v.\n\n    Args:\n    X (numpy array): Input sequence.\n    W_q (numpy array): Weight matrix for query.\n    W_k (numpy array): Weight matrix for key.\n    W_v (numpy array): Weight matrix for value.\n\n    Returns:\n    list: Self-attention output as a list after reshaping.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    self_attention_output = np.dot(attention_weights, V)\n    self_attention_output = np.round(self_attention_output, 4)\n    return self_attention_output.reshape(-1).tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Args:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for input_vector in input_sequence:\n        hidden_state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n    return np.round(hidden_state, 4).tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n    points (list): A list of [x, y] coordinates.\n    tx (float): The translation distance in the x direction.\n    ty (float): The translation distance in the y direction.\n\n    Returns:\n    list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    return translated_points[:, :2].tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n\n    Raises:\n    ValueError: If sigma_p or sigma_q is non-positive.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method to solve a system of linear equations Ax = b.\n\n    Args:\n    - A (numpy array): A square matrix of coefficients.\n    - b (numpy array): The right-hand side vector.\n    - n (int): The number of iterations.\n    - x_ini (numpy array, optional): Initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    - list: The approximated solution vector after n iterations, rounded to the nearest 4th decimal.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square.'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b must have compatible dimensions.'\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x_ini.astype(float)\n    for _ in range(n):\n        x_new = x.copy()\n        for i in range(A.shape[0]):\n            s1 = sum((A[i, j] * x_new[j] for j in range(i)))\n            s2 = sum((A[i, j] * x[j] for j in range(i + 1, A.shape[0])))\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).reshape(-1, 1).tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n\n    Args:\n    A (numpy.ndarray): The coefficient matrix.\n    b (numpy.ndarray): The constant vector.\n\n    Returns:\n    list: The solution vector x.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b must have compatible dimensions'\n    n = A.shape[0]\n    augmented_matrix = np.column_stack((A, b))\n    for i in range(n - 1):\n        max_row = np.argmax(np.abs(augmented_matrix[i:, i])) + i\n        augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]\n        assert augmented_matrix[i, i] != 0, 'Matrix A is singular'\n        for j in range(i + 1, n):\n            factor = augmented_matrix[j, i] / augmented_matrix[i, i]\n            augmented_matrix[j] -= factor * augmented_matrix[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (augmented_matrix[i, -1] - np.dot(augmented_matrix[i, i + 1:-1], x[i + 1:])) / augmented_matrix[i, i]\n    return np.round(x, 4).tolist()\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        \"\"\"Tanh activation function\"\"\"\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n\n        Args:\n        x (numpy array): Input sequence of shape (sequence_length, input_size, 1)\n        initial_hidden_state (numpy array): Initial hidden state of shape (hidden_size, 1)\n        initial_cell_state (numpy array): Initial cell state of shape (hidden_size, 1)\n\n        Returns:\n        hidden_states (list): List of hidden states at each time step\n        final_hidden_state (list): Final hidden state\n        final_cell_state (list): Final cell state\n        \"\"\"\n        sequence_length = x.shape[0]\n        hidden_states = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(sequence_length):\n            x_t = x[t]\n            concat_input = np.concatenate((x_t, hidden_state), axis=0)\n            forget_gate = self.sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n            input_gate = self.sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, concat_input) + self.bc)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            output_gate = self.sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n        final_hidden_state = hidden_state\n        final_cell_state = cell_state\n        hidden_states = np.round(np.array(hidden_states), 4).reshape(-1, self.hidden_size).tolist()\n        final_hidden_state = np.round(final_hidden_state, 4).reshape(1, -1).tolist()[0]\n        final_cell_state = np.round(final_cell_state, 4).reshape(1, -1).tolist()[0]\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document, rounded to four decimal places.\n\n    Raises:\n    - ValueError: If the corpus is empty.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty')\n    num_docs = len(corpus)\n    df = {}\n    for doc in corpus:\n        unique_words = set(doc)\n        for word in unique_words:\n            df[word] = df.get(word, 0) + 1\n    tf_idf_scores = []\n    for doc in corpus:\n        tf = {}\n        for word in doc:\n            tf[word] = tf.get(word, 0) + 1\n        max_tf = max(tf.values(), default=1)\n        doc_tf_idf_scores = []\n        for word in query:\n            word_tf = tf.get(word, 0) / max_tf if max_tf != 0 else 0\n            word_df = df.get(word, 0)\n            word_idf = np.log((num_docs + 1) / (word_df + 1)) + 1\n            tf_idf_score = word_tf * word_idf\n            doc_tf_idf_scores.append(tf_idf_score)\n        tf_idf_scores.append(doc_tf_idf_scores)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores\ncorpus = [['this', 'is', 'a', 'test', 'document'], ['this', 'document', 'is', 'just', 'a', 'test'], ['another', 'test', 'document']]\nquery = ['test', 'document', 'example']\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if precision + recall != 0 else 0\n    return round(f_score_value, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        \n        Returns:\n        outputs (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        last_inputs (numpy array): A 3D numpy array with shape (sequence_length, input_size + hidden_size, batch_size)\n        last_hiddens (numpy array): A 3D numpy array with shape (sequence_length + 1, hidden_size, batch_size)\n        \"\"\"\n        sequence_length = input_sequence.shape[0]\n        batch_size = input_sequence.shape[2]\n        outputs = np.zeros((sequence_length, self.output_size, batch_size))\n        last_inputs = np.zeros((sequence_length, self.input_size + self.hidden_size, batch_size))\n        last_hiddens = np.zeros((sequence_length + 1, self.hidden_size, batch_size))\n        for t in range(sequence_length):\n            if t == 0:\n                hidden_state_prev = np.zeros((self.hidden_size, batch_size))\n            else:\n                hidden_state_prev = last_hiddens[t]\n            x_t = input_sequence[t]\n            hidden_state = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, hidden_state_prev) + self.b_h)\n            output = np.dot(self.W_hy, hidden_state) + self.b_y\n            outputs[t] = output\n            last_inputs[t, :self.input_size, :] = x_t\n            last_inputs[t, self.input_size:, :] = hidden_state_prev\n            last_hiddens[t + 1] = hidden_state\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        expected_output (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        outputs (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        last_inputs (numpy array): A 3D numpy array with shape (sequence_length, input_size + hidden_size, batch_size)\n        last_hiddens (numpy array): A 3D numpy array with shape (sequence_length + 1, hidden_size, batch_size)\n        learning_rate (float): The learning rate for weight updates\n        \n        Returns:\n        dW_xh, dW_hh, dW_hy, db_h, db_y (numpy arrays): The gradients of the loss with respect to the weights and biases\n        \"\"\"\n        sequence_length = input_sequence.shape[0]\n        batch_size = input_sequence.shape[2]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dhidden_state_next = np.zeros((self.hidden_size, batch_size))\n        for t in reversed(range(sequence_length)):\n            output_error = outputs[t] - expected_output[t]\n            doutput = output_error\n            dW_hy += np.dot(doutput, last_hiddens[t + 1].T)\n            db_y += np.sum(doutput, axis=1, keepdims=True)\n            dhidden_state = np.dot(self.W_hy.T, doutput) + dhidden_state_next\n            dhidden_state_raw = dhidden_state * (1 - last_hiddens[t + 1] ** 2)\n            db_h += np.sum(dhidden_state_raw, axis=1, keepdims=True)\n            dW_xh += np.dot(dhidden_state_raw, last_inputs[t, :self.input_size, :].T)\n            dW_hh += np.dot(dhidden_state_raw, last_inputs[t, self.input_size:, :].T)\n            dhidden_state_next = np.dot(self.W_hh.T, dhidden_state_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return (dW_xh, dW_hh, dW_hy, db_h, db_y)\n\n    def train(self, input_sequence, expected_output, learning_rate):\n        \"\"\"\n        Train the RNN on a given sequence of inputs and expected outputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        expected_output (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        learning_rate (float): The learning rate for weight updates\n        \"\"\"\n        (outputs, last_inputs, last_hiddens) = self.forward(input_sequence)\n        self.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    A = np.asarray(A)\n    b = np.asarray(b)\n    size = A.shape[0]\n    if x0 is None:\n        x = np.zeros(size)\n    else:\n        x = np.asarray(x0)\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    y = np.array(y)\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(np.square(probabilities))\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_ptr = [0]\n    for row in dense_matrix:\n        row_non_zero_count = 0\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n                row_non_zero_count += 1\n        row_ptr.append(row_ptr[-1] + row_non_zero_count)\n    return (values, column_indices, row_ptr)\ndense_matrix = [[1, 0, 2], [0, 3, 0], [4, 5, 6]]\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product_v_L = sum((a * b for (a, b) in zip(v, L)))\n    dot_product_L_L = sum((i ** 2 for i in L))\n    if dot_product_L_L == 0:\n        return [0.0] * len(v)\n    scalar_factor = dot_product_v_L / dot_product_L_L\n    projection_vector = [scalar_factor * i for i in L]\n    rounded_projection_vector = [round(i, 3) for i in projection_vector]\n    return rounded_projection_vector\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_ptr[col + 1] = len(values)\n    return (values, row_indices, col_ptr)\ndense_matrix = [[1, 0, 2], [0, 3, 0], [4, 0, 5]]\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (num_rows, num_cols) = A.shape\n    ref_A = A.copy()\n    pivot_cols = []\n    row = 0\n    for col in range(num_cols):\n        if row >= num_rows:\n            break\n        max_row = np.argmax(np.abs(ref_A[row:, col])) + row\n        if ref_A[max_row, col] == 0:\n            continue\n        ref_A[[row, max_row]] = ref_A[[max_row, row]]\n        ref_A[row] /= ref_A[row, col]\n        for i in range(row + 1, num_rows):\n            ref_A[i] -= ref_A[i, col] * ref_A[row]\n        pivot_cols.append(col)\n        row += 1\n    basis_vectors = A[:, pivot_cols]\n    return np.round(basis_vectors, 8).tolist()\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    mean_y_true = np.mean(y_true)\n    ss_tot = np.sum((y_true - mean_y_true) ** 2)\n    if ss_tot == 0:\n        return np.nan\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    if not img:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    else:\n        average_brightness = total_brightness / total_pixels\n        return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input arrays are not of a valid type.\n    \"\"\"\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise TypeError('Inputs must be of type list or numpy.ndarray')\n    try:\n        y_true = np.array(y_true, dtype=float)\n        y_pred = np.array(y_pred, dtype=float)\n    except ValueError:\n        raise TypeError('Inputs must be convertible to numpy arrays of dtype float')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Input arrays cannot be empty')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape')\n    residuals = y_true - y_pred\n    sum_squared_residuals = np.sum(residuals ** 2)\n    mean_squared_residuals = sum_squared_residuals / len(y_true)\n    rmse_value = np.sqrt(mean_squared_residuals)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    assert y_true.shape == y_pred.shape, 'y_true and y_pred must have the same shape'\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Ground truth binary labels.\n    y_pred (numpy.ndarray): Predicted binary labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not np.all(np.logical_or(y_true == 0, y_true == 1)) or not np.all(np.logical_or(y_pred == 0, y_pred == 1)):\n        raise ValueError('Inputs must be binary arrays')\n    true_positives = np.sum(y_true * y_pred)\n    sum_of_elements = np.sum(y_true) + np.sum(y_pred)\n    if sum_of_elements == 0:\n        return 1.0\n    dice = 2.0 * true_positives / sum_of_elements\n    return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    This function generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        np.random.seed(random_seeds[feature])\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        value_hv = np.random.choice([-1, 1], size=dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.where(composite_hv > 0, 1, -1)\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    (tn, fp, fn, tp) = (0, 0, 0, 0)\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n    return [[tn, fp], [fn, tp]]\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    This function calculates the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (numpy array): The first vector.\n    v2 (numpy array): The second vector.\n    \n    Returns:\n    float: The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    This function calculates various performance metrics for a binary classification model.\n\n    Args:\n    actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"Both 'actual' and 'predicted' lists must have the same length.\")\n    if not all((i in [0, 1] for i in actual)) or not all((i in [0, 1] for i in predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    counts = Counter(zip(actual, predicted))\n    tn = counts[0, 0]\n    fp = counts[0, 1]\n    fn = counts[1, 0]\n    tp = counts[1, 1]\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3) if tp + tn + fp + fn != 0 else 0\n    f1_score = round(2 * tp / (2 * tp + fp + fn), 3) if 2 * tp + fp + fn != 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp != 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn != 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: invalid index to scalar variable.", "solution": "import numpy as np\nimport numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Args:\n    - data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    - dict: A dictionary containing mean, median, mode, variance, standard deviation,\n            percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.array(data)\n    if len(data) == 0:\n        raise ValueError('Input data is empty')\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(interquartile_range, 4)}\n    return stats_dict\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not (isinstance(n, int) and isinstance(k, int) and isinstance(p, (int, float))):\n        raise TypeError(\"Input types are not valid. 'n' and 'k' should be integers, 'p' should be a number.\")\n    if n < 0 or k < 0:\n        raise ValueError(\"'n' and 'k' should be non-negative integers.\")\n    if k > n:\n        raise ValueError(\"'k' cannot be greater than 'n'.\")\n    if p < 0 or p > 1:\n        raise ValueError(\"'p' should be between 0 and 1.\")\n    binomial_coefficient = math.comb(n, k)\n    probability = binomial_coefficient * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev == 0:\n        raise ValueError('Standard deviation cannot be zero.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n\n    Returns:\n        int: The contrast of the grayscale image, calculated as the difference between the maximum and minimum pixel values.\n\n    Raises:\n        ValueError: If the input image is not a 2D numpy array or if pixel values are not between 0 and 255.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D numpy array')\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError('Pixel values must be between 0 and 255')\n    min_pixel = np.min(img)\n    max_pixel = np.max(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n        TypeError: If the input vectors are not NumPy arrays.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError('Both inputs must be NumPy arrays')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays')\n    if len(vec1) != len(vec2):\n        raise ValueError('Input vectors must be of the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for point in data:\n        transformed_data.append(np.round(np.power(point, np.arange(degree + 1)).tolist(), 8))\n    return transformed_data\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length and model dimensionality.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings if position and d_model are valid, -1 otherwise.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                pe[pos, i] = np.sin(pos / np.power(10000, 2 * i / d_model))\n            else:\n                pe[pos, i] = np.cos(pos / np.power(10000, 2 * (i - 1) / d_model))\n    return pe.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    updated_m = beta1 * m + (1 - beta1) * grad\n    updated_v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_hat = updated_m / (1 - np.power(beta1, t + 1))\n    v_hat = updated_v / (1 - np.power(beta2, t + 1))\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(updated_m, 5).tolist()\n    updated_v = np.round(updated_v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: cannot reshape array of size 10 into shape (1,12,0)", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12, 'n_embd': 10}\n    params = {'wte': np.random.rand(3, hparams['n_embd']), 'wpe': np.random.rand(hparams['n_ctx'], hparams['n_embd']), 'blocks': [{'ln_1': {'g': np.ones(hparams['n_embd']), 'b': np.zeros(hparams['n_embd'])}, 'ln_2': {'g': np.ones(hparams['n_embd']), 'b': np.zeros(hparams['n_embd'])}, 'attn': {'c_attn': {'w': np.random.rand(hparams['n_embd'], 3 * hparams['n_embd']), 'b': np.zeros(3 * hparams['n_embd'])}, 'c_proj': {'w': np.random.rand(hparams['n_embd'], hparams['n_embd']), 'b': np.zeros(hparams['n_embd'])}}, 'mlp': {'c_fc': {'w': np.random.rand(hparams['n_embd'], 4 * hparams['n_embd']), 'b': np.zeros(4 * hparams['n_embd'])}, 'c_proj': {'w': np.random.rand(4 * hparams['n_embd'], hparams['n_embd']), 'b': np.zeros(hparams['n_embd'])}}}], 'ln_f': {'g': np.ones(hparams['n_embd']), 'b': np.zeros(hparams['n_embd'])}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    \"\"\"Layer normalization.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\ndef softmax(x, axis=-1):\n    \"\"\"Softmax function.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\ndef multi_head_attention(q, k, v, n_head):\n    \"\"\"Multi-head attention.\"\"\"\n    d_k = q.shape[-1] // n_head\n    q = np.reshape(q, (*q.shape[:-1], n_head, d_k))\n    k = np.reshape(k, (*k.shape[:-1], n_head, d_k))\n    v = np.reshape(v, (*v.shape[:-1], n_head, d_k))\n    qk = np.matmul(np.transpose(q, (0, 1, 3, 2)), np.transpose(k, (0, 1, 2, 3))) / np.sqrt(d_k)\n    attention_weights = softmax(qk)\n    output = np.matmul(attention_weights, np.transpose(v, (0, 1, 3, 2)))\n    output = np.transpose(output, (0, 1, 3, 2))\n    return np.reshape(output, (*output.shape[:-2], -1))\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    generated_ids = input_ids.copy()\n    for _ in range(n_tokens_to_generate):\n        input_ids = np.array(generated_ids[-hparams['n_ctx']:])\n        token_embeddings = params['wte'][input_ids]\n        positional_embeddings = params['wpe'][np.arange(len(input_ids))]\n        x = token_embeddings + positional_embeddings\n        for block in params['blocks']:\n            x = layer_norm(x, **block['ln_1'])\n            qkv = np.matmul(x, block['attn']['c_attn']['w']) + block['attn']['c_attn']['b']\n            (q, k, v) = np.split(qkv, 3, axis=-1)\n            attention_output = multi_head_attention(q, k, v, hparams['n_head'])\n            attention_output = np.matmul(attention_output, block['attn']['c_proj']['w']) + block['attn']['c_proj']['b']\n            x = x + attention_output\n            x = layer_norm(x, **block['ln_2'])\n            mlp_output = np.maximum(np.matmul(x, block['mlp']['c_fc']['w']) + block['mlp']['c_fc']['b'], 0)\n            mlp_output = np.matmul(mlp_output, block['mlp']['c_proj']['w']) + block['mlp']['c_proj']['b']\n            x = x + mlp_output\n        x = layer_norm(x, **params['ln_f'])\n        logits = np.matmul(x[-1], params['wte'].T)\n        next_token_id = np.argmax(logits)\n        generated_ids.append(next_token_id)\n    generated_text = encoder.decode(generated_ids)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function implements a simplified self-attention mechanism.\n    \n    Parameters:\n    n (int): The number of crystals.\n    crystal_values (list): A list of crystal values.\n    dimension (int): The dimension of the crystal values.\n    \n    Returns:\n    list: A list of floats representing the final weighted pattern for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function computes the softmax of a given set of values.\n        \n        Parameters:\n        values (numpy array): The input values.\n        \n        Returns:\n        numpy array: The softmax of the input values.\n        \"\"\"\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum(axis=-1, keepdims=True)\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n    weights = np.random.rand(dimension, dimension)\n    queries = np.dot(crystal_values, weights)\n    keys = np.dot(crystal_values, weights)\n    values = crystal_values\n    attention_scores = np.dot(queries, keys.T) / np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_weights, values)\n    return [round(np.sum(weighted_pattern), 4) for weighted_pattern in weighted_patterns]\nn = 3\ncrystal_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ndimension = 3\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given corpus and query.\n\n    Parameters:\n    - corpus (list of list of str): A list of documents where each document is a list of terms.\n    - query (list of str): A list of query terms.\n    - k1 (float, optional): Term frequency saturation parameter. Defaults to 1.5.\n    - b (float, optional): Document length normalization parameter. Defaults to 0.75.\n\n    Returns:\n    - list of float: A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    df = Counter((term for doc in corpus for term in set(doc)))\n    N = len(corpus)\n    avgdl = sum((len(doc) for doc in corpus)) / N\n    bm25_scores = []\n    for doc in corpus:\n        dl = len(doc)\n        tf = Counter(doc)\n        score = 0\n        for term in query:\n            if term in tf:\n                freq = tf[term]\n                idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1)\n                score += idf * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * dl / avgdl))\n        bm25_scores.append(round(score, 3))\n    return bm25_scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'Both lists should have the same length'\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    fp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 0 and pred == 1))\n    fn = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nPI = 3.14159\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    n = len(detrended_data)\n    x_sum = sum(range(1, n + 1))\n    y_sum = sum(detrended_data)\n    xy_sum = sum((i * detrended_data[i - 1] for i in range(1, n + 1)))\n    x_squared_sum = sum((i ** 2 for i in range(1, n + 1)))\n    m = (n * xy_sum - x_sum * y_sum) / (n * x_squared_sum - x_sum ** 2)\n    b = (y_sum - m * x_sum) / n\n    day_15_base = m * 15 + b\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base + day_15_fluctuation\n    day_15_consumption = round(day_15_consumption)\n    safety_margin = math.ceil(day_15_consumption * 0.05)\n    final_consumption = day_15_consumption + safety_margin\n    return final_consumption\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Both inputs must be numpy arrays')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\nfrom scipy.special import softmax\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_q (np.ndarray): Weight matrix for Query.\n    W_k (np.ndarray): Weight matrix for Key.\n    W_v (np.ndarray): Weight matrix for Value.\n\n    Returns:\n    tuple: Q, K, V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform self-attention on the given Q, K, and V matrices.\n\n    Args:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n\n    Returns:\n    np.ndarray: Output of self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = softmax(attention_scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_q (np.ndarray): Weight matrix for Query.\n    W_k (np.ndarray): Weight matrix for Key.\n    W_v (np.ndarray): Weight matrix for Value.\n    n_heads (int): Number of heads.\n\n    Returns:\n    list: Output of multi-head attention as a list.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    (n_samples, seq_len, d_model) = Q.shape\n    head_size = d_model // n_heads\n    Q_heads = Q.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    K_heads = K.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    V_heads = V.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    outputs = []\n    for i in range(n_heads):\n        output = self_attention(Q_heads[:, i, :, :], K_heads[:, i, :, :], V_heads[:, i, :, :])\n        outputs.append(output)\n    outputs = np.array(outputs).transpose(1, 2, 0, 3).reshape(n_samples, seq_len, d_model)\n    outputs = np.round(outputs, 4)\n    return outputs.tolist()\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of the same length')\n    if not all((i in [0, 1] for i in x)) or not all((i in [0, 1] for i in y)):\n        raise ValueError('Input lists must contain only binary values (0 or 1)')\n    (n11, n00, n10, n01) = (0, 0, 0, 0)\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        else:\n            n01 += 1\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    if x > 2.5:\n        return 1.0\n    return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        exp_x = math.exp(x)\n        return round(alpha * (exp_x - 1), 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 709.78:\n        return round(x, 4)\n    elif x < -37:\n        return round(math.exp(x), 4)\n    else:\n        result = math.log(1 + math.exp(x))\n        return round(result, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    obj = np.minimum(rhos * A, clipped_rhos * A)\n    kl_divergence = pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref))\n    kl_divergence_penalty = beta * kl_divergence\n    grpo_obj_value = np.mean(obj - kl_divergence_penalty)\n    return round(grpo_obj_value, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of z.\n\n    Args:\n        z: Input to the sigmoid function.\n\n    Returns:\n        Sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = np.max(y) + 1\n    B = np.zeros((C, M))\n    losses = []\n    y_onehot = np.eye(C)[y]\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        probs = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs /= np.sum(probs, axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-15), axis=1))\n        losses.append(round(loss, 4))\n        dB = np.dot((probs - y_onehot).T, X) / N\n        B -= learning_rate * dB\n    return (B.round(4).tolist(), losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of x.\n    \n    Args:\n    x (np.ndarray): Input array.\n    \n    Returns:\n    np.ndarray: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations for gradient descent.\n    \n    Returns:\n    tuple[list[float], ...]: A tuple containing the optimized coefficients and the collected loss values over iterations.\n    \"\"\"\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    coefficients = np.zeros(X.shape[1])\n    coefficient_history = [coefficients.tolist()]\n    loss_history = []\n    for _ in range(iterations):\n        predictions = sigmoid(np.dot(X, coefficients))\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss = round(loss, 4)\n        loss_history.append(loss)\n        gradient = np.dot(X.T, predictions - y) / X.shape[0]\n        coefficients -= learning_rate * gradient\n        coefficient_history.append(coefficients.tolist())\n    return (coefficient_history[-1], loss_history)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'softmax'", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_scores = np.where(mask == 0, -np.inf, attention_scores)\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    output_list = output.tolist()\n    return output_list\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nimport numpy as np\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = {}\n    for color in apples:\n        if color in counts:\n            counts[color] += 1\n        else:\n            counts[color] = 1\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in counts.values()]\n    entropy = -sum([p * np.log(p) for p in probabilities if p > 0])\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Applies Layer Normalization to the input data X.\n\n    Args:\n    - X (np.ndarray): Input 3D array of shape (batch_size, sequence_length, feature_dim).\n    - gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    - beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    - epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    - normalized_X (list): Normalized X, rounded to 5 decimal places, and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    normalized_X = gamma * normalized_X + beta\n    normalized_X = np.round(normalized_X, 5).tolist()\n    return normalized_X\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float, optional): The parameter for calculating the F-mean. Defaults to 0.9.\n    - beta (float, optional): The parameter for calculating the penalty. Defaults to 3.\n    - gamma (float, optional): The parameter for calculating the penalty. Defaults to 0.5.\n\n    Returns:\n    - float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n    reference_counter = Counter(reference_tokens)\n    candidate_counter = Counter(candidate_tokens)\n    matches = sum((min(reference_counter[token], candidate_counter[token]) for token in candidate_counter))\n    precision = matches / len(candidate_tokens) if candidate_tokens else 0\n    recall = matches / len(reference_tokens) if reference_tokens else 0\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall) if precision + recall > 0 else 0\n    m = matches\n    ch = 0\n    prev_match = False\n    for token in candidate_tokens:\n        if token in reference_counter and reference_counter[token] > 0:\n            if not prev_match:\n                ch += 1\n            prev_match = True\n            reference_counter[token] -= 1\n        else:\n            prev_match = False\n    penalty = gamma * (ch / m) ** beta if m > 0 else 0\n    score = f_mean * (1 - penalty)\n    return round(score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    This function computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, \n    their individual counts, and the total number of samples.\n\n    Args:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total count of event x.\n    total_counts_y (int): The total count of event y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information rounded to 3 decimal places.\n    \"\"\"\n    prob_joint = joint_counts / total_samples\n    if prob_joint == 0:\n        return 0.0\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n    pmi = np.log2(prob_joint / (prob_x * prob_y))\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1] and returns a list of floats \n    rounded to 4 decimal places.\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of normalized floats rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if max_val == min_val:\n        return [0.0] * len(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    normalized = [round(val, 4) for val in normalized]\n    return normalized\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n    list: The output of the residual block after final ReLU activation, rounded to 4 decimal places, and converted to a list.\n    \"\"\"\n    hidden_layer = np.maximum(np.dot(x, w1), 0)\n    output_layer = np.dot(hidden_layer, w2)\n    output_layer += x\n    output_layer = np.maximum(output_layer, 0)\n    output = np.round(output_layer, 4).tolist()\n    return output\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps.\n\n    Args:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    assert len(x.shape) == 3, 'Input must be a 3D NumPy array'\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    This function performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n\n    Args:\n    X (np.ndarray): A 4D NumPy array representing a batch of feature maps in the BCHW format.\n    gamma (np.ndarray): A 1D NumPy array representing the scale parameters for each channel.\n    beta (np.ndarray): A 1D NumPy array representing the shift parameters for each channel.\n    epsilon (float): A small value added to the denominator for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: The batch normalized output, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n    gamma_reshaped = gamma.reshape(1, -1, 1, 1)\n    beta_reshaped = beta.reshape(1, -1, 1, 1)\n    output = gamma_reshaped * normalized_X + beta_reshaped\n    output = np.round(output, 4).tolist()\n    return output\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_basis = []\n    for v in vectors:\n        orthogonalized_v = v.copy()\n        for basis_v in orthonormal_basis:\n            basis_v = np.array(basis_v)\n            orthogonalized_v = orthogonalized_v - np.dot(orthogonalized_v, basis_v) * basis_v\n        norm = np.linalg.norm(orthogonalized_v)\n        if norm > tol:\n            orthonormal_basis.append((orthogonalized_v / norm).tolist())\n    orthonormal_basis = [[round(x, 4) for x in v] for v in orthonormal_basis]\n    return orthonormal_basis\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n    a (numpy.ndarray): The first 3D vector.\n    b (numpy.ndarray): The second 3D vector.\n\n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both inputs must be 3-dimensional vectors')\n    result = np.cross(a, b)\n    result = np.round(result, 4).tolist()\n    return result\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b must have compatible dimensions'\n    assert len(b.shape) == 1, 'Vector b must be a 1D array'\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = np.zeros_like(b, dtype=float)\n    for i in range(A.shape[1]):\n        A_i = A.copy()\n        A_i[:, i] = b\n        x[i] = np.linalg.det(A_i) / det_A\n    return np.round(x, 4).reshape(-1).tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    This function calculates the Bhattacharyya distance between two discrete probability distributions p and q.\n    \n    Args:\n    p (list[float]): The first discrete probability distribution.\n    q (list[float]): The second discrete probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance between p and q rounded to 4 decimal places. If p and q have different lengths or are empty, returns 0.0.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    sum_sqrt_pq = np.sum(np.sqrt(p_array * q_array))\n    if sum_sqrt_pq > 0:\n        distance = -np.log(sum_sqrt_pq)\n        return round(distance, 4)\n    else:\n        return 0.0\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import Union, List\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n\n    Returns:\n    list[int|float] or int: A new vector representing the element-wise sum if the operation is valid; -1 otherwise.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n    \"\"\"\n    This function computes the average gradient of the log-policy multiplied by the return at each time step.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape `(num_states, num_actions)` parameterizing the policy.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list[float]: The average gradient of the log-policy multiplied by the return at each time step, reshaped into a list.\n    \"\"\"\n    total_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = np.zeros(len(episode))\n        returns[-1] = episode[-1][2]\n        for i in range(len(episode) - 2, -1, -1):\n            returns[i] = episode[i][2] + returns[i + 1]\n        for (t, (state, action, _)) in enumerate(episode):\n            policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            log_policy_grad = np.zeros_like(theta)\n            log_policy_grad[state, :] = -policy\n            log_policy_grad[state, action] += 1\n            total_gradient += log_policy_grad * returns[t]\n    avg_gradient = total_gradient / len(episodes)\n    return np.round(avg_gradient, 4).flatten().tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    - n_experts (int): The total number of experts in the MoE layer.\n    - k_active (int): The number of active experts in the MoE layer.\n    - d_in (int): The input dimension of the layer.\n    - d_out (int): The output dimension of the layer.\n\n    Returns:\n    - savings_percentage (float): The percentage of computational cost savings.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out - d_out\n    moe_flops = 2 * d_in * n_experts - n_experts + k_active * (2 * d_in * d_out - d_out)\n    if dense_flops == 0:\n        savings_percentage = 0\n    else:\n        savings_percentage = round((dense_flops - moe_flops) / dense_flops * 100, 1)\n    return savings_percentage\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 512\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    This function implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for gating.\n    W_noise (np.ndarray): Weight matrix for noise.\n    N (np.ndarray): Pre-sampled noise.\n    k (int): Sparsity constraint.\n\n    Returns:\n    list: Final gating probabilities matrix reshaped as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n    gating_scores = np.matmul(X, W_g)\n    noise = np.matmul(N, W_noise)\n    noisy_gating_scores = gating_scores + noise\n    top_k_scores = np.sort(noisy_gating_scores, axis=-1)[:, -k:]\n    mask = noisy_gating_scores >= np.min(top_k_scores, axis=-1, keepdims=True)\n    gating_probabilities = np.exp(noisy_gating_scores) * mask / np.sum(np.exp(noisy_gating_scores) * mask, axis=-1, keepdims=True)\n    gating_probabilities = np.nan_to_num(gating_probabilities)\n    gating_probabilities = np.round(gating_probabilities, 4)\n    gating_probabilities_list = gating_probabilities.tolist()\n    return gating_probabilities_list\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Mixture-of-Experts (MoE) layer.\n\n    Args:\n    - x (np.ndarray): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n    - We (np.ndarray): Expert weight matrices of shape (n_experts, embedding_dim, embedding_dim).\n    - Wg (np.ndarray): Gating weight matrix of shape (embedding_dim, n_experts).\n    - n_experts (int): Number of experts.\n    - top_k (int): Number of top experts to select per token.\n\n    Returns:\n    - A python list representing the MoE output after reshaping.\n    \"\"\"\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    gating_probs = np.round(gating_probs, 4)\n    top_k_indices = np.argsort(-gating_probs, axis=-1)[:, :, :top_k]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    top_k_probs = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    top_k_probs = np.round(top_k_probs, 4)\n    moe_output = np.zeros_like(x)\n    for k in range(top_k):\n        expert_indices = top_k_indices[:, :, k]\n        expert_weights = We[expert_indices]\n        expert_outputs = np.einsum('bse,bsed->bsd', x, expert_weights)\n        moe_output += np.expand_dims(top_k_probs[:, :, k], -1) * expert_outputs\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n    - X (np.ndarray): Input tensor with shape (B, C, H, W)\n    - gamma (np.ndarray): Learned scale with shape (C,)\n    - beta (np.ndarray): Learned shift with shape (C,)\n    - num_groups (int): Number of groups to divide the channels into\n    - epsilon (float): Small value added for numerical stability (default=1e-5)\n\n    Returns:\n    - normalized_X (list): Normalized tensor reshaped and returned as a list, rounded to 4 decimal places\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    normalized_X_reshaped = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X_reshaped.reshape(B, C, H, W)\n    normalized_X = gamma[None, :, None, None] * normalized_X + beta[None, :, None, None]\n    return np.round(normalized_X, 4).tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    This function finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Args:\n    - start_x (float): The initial value of x to start the search from.\n    - learning_rate (float, optional): The step size for each gradient descent iteration. Defaults to 0.1.\n    - tolerance (float, optional): The convergence criterion. Defaults to 1e-6.\n    - max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    - float: The value of x where f(x) is minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        new_x = x - learning_rate * gradient\n        if np.abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n\n    Args:\n    x (np.ndarray): Input array.\n    alpha (float): Scaling factor.\n    gamma (float): Hyperparameter for DyT.\n    beta (float): Shift parameter.\n\n    Returns:\n    list[float]: DyT transformed values as a Python list, rounded to the nearest 4th decimal.\n    \"\"\"\n    dyt = alpha * np.tanh(gamma * x + beta)\n    dyt_flat = dyt.flatten()\n    result = np.round(dyt_flat, 4).tolist()\n    return result\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
