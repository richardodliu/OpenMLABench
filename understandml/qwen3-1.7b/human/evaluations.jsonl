{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Args:\n        a: A list of lists representing the matrix.\n        b: A list representing the vector.\n        \n    Returns:\n        A list where each element is the dot product of a row in 'a' with 'b'.\n        Returns -1 if the matrix and vector dimensions are incompatible.\n        \n    Raises:\n        ValueError: If the matrix is empty or the vector is empty.\n    \"\"\"\n    if not a:\n        return -1\n    if not b:\n        return -1\n    num_rows = len(a)\n    num_cols = len(a[0]) if num_rows > 0 else 0\n    vector_length = len(b)\n    if num_cols != vector_length:\n        return -1\n    result = []\n    for i in range(num_rows):\n        dot_product = 0\n        for j in range(num_cols):\n            dot_product += a[i][j] * b[j]\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": false, "result": "failed: name 'transpose_matrix' is not defined", "solution": "\n\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    (rows, cols) = new_shape\n    if rows * cols != len(a) * len(a[0]):\n        return []\n    matrix = np.array(a)\n    reshaped = matrix.reshape(rows, cols).tolist()\n    return reshaped\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Parameters:\n        matrix (list[list[float]]): A 2D list representing the matrix.\n        mode (str): Specifies whether to calculate the mean by row or column.\n            Must be either 'row' or 'column'.\n\n    Returns:\n        list[float]: A list of means, where each element corresponds to the mean of\n            a row (if mode is 'row') or a column (if mode is 'column').\n\n    Raises:\n        ValueError: If the mode is not 'row' or 'column'.\n    \"\"\"\n    if mode not in ('row', 'column'):\n        raise ValueError(\"Mode must be 'row' or 'column'.\")\n    result = []\n    if mode == 'row':\n        for row in matrix:\n            mean = sum(row) / len(row)\n            result.append(mean)\n    elif mode == 'column':\n        columns = list(zip(*matrix))\n        for col in columns:\n            mean = sum(col) / len(col)\n            result.append(mean)\n    return result\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a matrix by a given scalar.\n\n    Parameters:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix by.\n\n    Returns:\n    list[list[int|float]]: A new matrix where each element is the original element multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[1][1]\n    trace = a + b\n    determinant = a * b - matrix[0][1] * matrix[1][0]\n    eigenvalues = [(trace + abs(trace ** 2 - 4 * determinant)) / 2, (trace - abs(trace ** 2 - 4 * determinant)) / 2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=np.float64)\n    T_np = np.array(T, dtype=np.float64)\n    S_np = np.array(S, dtype=np.float64)\n    try:\n        det_T = np.linalg.det(T_np)\n        det_S = np.linalg.det(S_np)\n        if abs(det_T) < 1e-10 or abs(det_S) < 1e-10:\n            return [-1]\n    except np.linalg.LinAlgError:\n        return [-1]\n    result = np.dot(np.linalg.inv(T_np), A_np)\n    result = np.dot(result, S_np)\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d, -b], [-c, a]]\n    inverse[0][0] /= determinant\n    inverse[0][1] /= determinant\n    inverse[1][0] /= determinant\n    inverse[1][1] /= determinant\n    return inverse\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    result = [[0] * cols_b for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            total = 0\n            for k in range(cols_a):\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: list index out of range", "solution": "\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors (list[list[float]]): A list of lists where each inner list represents a feature\n                                   with its observations.\n\n    Returns:\n        list[list[float]]: A 2D list representing the covariance matrix.\n    \"\"\"\n    if not vectors:\n        return []\n    n_samples = len(vectors[0])\n    n_features = len(vectors)\n    mean = [sum(feature) / n_samples for feature in vectors]\n    centered = [[v[i] - mean[i] for i in range(n_features)] for v in vectors]\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            sum_ij = 0.0\n            for k in range(n_samples):\n                sum_ij += centered[k][i] * centered[k][j]\n            cov_matrix[i][j] = sum_ij / (n_samples - 1)\n    return cov_matrix\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix (n x n)\n        b (np.ndarray): Right-hand side vector (n x 1)\n        n (int): Number of iterations to perform\n    \n    Returns:\n        list: Approximate solution vector x (n x 1), rounded to 4 decimal places\n    \"\"\"\n    A = A.astype(float)\n    b = b.astype(float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            for j in range(A.shape[0]):\n                if j != i:\n                    x_new[i] += -(A[i, j] * x[j]) / A[i, i]\n            x_new[i] += b[i] / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method.\n    Returns the singular values rounded to the nearest 4th decimal place.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b, c, d) = (A[0, 0], A[0, 1], A[1, 0], A[1, 1])\n    det = a * d - b * c\n    trace = a + d\n    eigenvalues = [(trace + np.sqrt(trace ** 2 - 4 * det)) / 2, (trace - np.sqrt(trace ** 2 - 4 * det)) / 2]\n    singular_values = np.sqrt(eigenvalues)\n    rounded_singular_values = np.round(singular_values, 4)\n    return tuple(rounded_singular_values)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n        matrix: A 4x4 list of lists containing integers or floats.\n        \n    Returns:\n        The determinant of the 4x4 matrix as a float.\n    \"\"\"\n    if len(matrix) == 1:\n        return matrix[0][0]\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    n = len(matrix)\n    det = 0\n    for i in range(n):\n        minor = [[matrix[j][k] for k in range(n)] for j in range(n) if j != i]\n        sign = (-1) ** i\n        minor_det = determinant_4x4(minor)\n        det += matrix[i][i] * sign * minor_det\n    return det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_matrix = np.array(X)\n    y_vector = np.array(y)\n    X_with_intercept = np.hstack((np.ones((X_matrix.shape[0], 1)), X_matrix))\n    coefficients = np.linalg.solve(X_with_intercept.T @ X_with_intercept, X_with_intercept.T @ y_vector)\n    rounded_coefficients = np.round(coefficients, 4)\n    return rounded_coefficients.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X @ theta\n        gradient = X.T @ (predictions - y) / len(y)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized = (data - min_val) / (max_val - min_val) if max_val - min_val != 0 else data\n    scaled_standardization = np.round(standardized, 4).tolist()\n    scaled_min_max = np.round(normalized, 4).tolist()\n    return (scaled_standardization, scaled_min_max)\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: list indices must be integers or slices, not tuple", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = initial_centroids.copy()\n    for _ in range(max_iterations):\n        assignments = np.argmin(np.linalg.norm(points_array - centroids[:, np.newaxis], axis=2), axis=1)\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            cluster_points = points_array[assignments == i]\n            if len(cluster_points) > 0:\n                new_centroids[i] = np.mean(cluster_points, axis=0)\n        if np.allclose(centroids, new_centroids, atol=1e-06):\n            break\n        centroids = new_centroids\n    final_centroids = np.round(centroids, 4).astype(int)\n    return list(final_centroids)\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,) (8,) ", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Perform K-Fold Cross-Validation on the given dataset.\n\n    Args:\n        X: 2D array of shape (n_samples, n_features)\n        y: 1D array of shape (n_samples,)\n        k: Number of folds (default: 5)\n        shuffle: Whether to shuffle the data before splitting (default: True)\n        random_seed: Random seed for reproducibility (default: None)\n\n    Returns:\n        A list of lists, where each sublist contains the indices of the training samples for that fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    if not shuffle:\n        indices = np.arange(X.shape[0])\n    else:\n        indices = np.random.permutation(X.shape[0])\n    folds = []\n    n_samples = X.shape[0]\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size\n        if i < remainder:\n            end += 1\n        train_indices = indices[start:end]\n        test_indices = indices[end:]\n        folds.append((train_indices, test_indices))\n    return folds\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a 2D dataset.\n\n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n\n    Returns:\n    - A list of lists, where each sublist contains the coordinates of the principal component.\n    - All results are rounded to the nearest 4th decimal place.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    n_samples = data.shape[0]\n    cov_matrix = standardized_data.T @ standardized_data / (n_samples - 1)\n    (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    top_k_eigenvalues = eigenvalues[sorted_indices][:k]\n    top_k_eigenvectors = eigenvectors[:, sorted_indices][:k]\n    principal_components = top_k_eigenvectors.tolist()\n    rounded_components = [[round(val, 4) for val in component] for component in principal_components]\n    return rounded_components\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: name 'choose_best_split' is not defined", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learns a decision tree using recursive binary splitting based on entropy and information gain.\n    \n    Parameters:\n        examples (list[dict]): List of examples, each example is a dictionary of attribute-value pairs.\n        attributes (list[str]): List of attribute names.\n        target_attr (str): The target attribute to classify on.\n    \n    Returns:\n        dict: A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples or len(examples) == 0:\n        return {}\n    if len(set((examples[i][target_attr] for i in range(len(examples))))) == 1:\n        return {target_attr: examples[0][target_attr]}\n    if not attributes:\n        return {target_attr: Counter([example[target_attr] for example in examples]).most_common(1)[0][0]}\n    best_attr = choose_best_split(examples, attributes, target_attr)\n    root = {best_attr: {}}\n    for value in get_unique_values(examples, best_attr):\n        subset = split_examples(examples, best_attr, value)\n        child = learn_decision_tree(subset, attributes.copy(), target_attr)\n        root[best_attr][value] = child\n    return root\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: Data and labels must have the same number of features.", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    if data.shape[1] != len(labels):\n        raise ValueError('Data and labels must have the same number of features.')\n    (n_samples, n_features) = data.shape\n    n_classes = len(np.unique(labels))\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        K = np.dot(data, data.T)\n        gamma = sigma / np.sqrt(n_features)\n        K = K * gamma ** 2\n    else:\n        raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n    for _ in range(iterations):\n        for i in range(n_samples):\n            dot_product = K[i, :] @ labels\n            alpha[i] = alpha[i] + lambda_val * labels[i] * (dot_product - bias)\n            bias += lambda_val * labels[i] * (dot_product - bias)\n        if kernel == 'linear':\n            weights = alpha * data\n        elif kernel == 'rbf':\n            weights = alpha[:, np.newaxis] * np.outer(data, data)\n            weights = weights * (sigma ** 2) ** (1 / 2)\n        else:\n            raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n        if kernel == 'linear':\n            decision_boundary = weights @ data + bias\n        elif kernel == 'rbf':\n            decision_boundary = np.dot(weights, data) + bias\n        weights = np.clip(weights, -1000, 1000)\n    return [np.round(alpha, 4).tolist(), np.round(bias, 4)]\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    return round(math.exp(z) / (1 + math.exp(z)), 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    total = sum(scores)\n    exp_scores = [math.exp(score) for score in scores]\n    normalized = [exp_score / total for exp_score in exp_scores]\n    rounded = [round(value, 4) for value in normalized]\n    return rounded\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features_array = np.array(features).reshape(len(features), -1)\n    z = np.dot(features_array, weights) + bias\n    predicted_probabilities = 1 / (1 + np.exp(-z))\n    predicted_probabilities_rounded = np.round(predicted_probabilities, 4)\n    mse = np.mean((predicted_probabilities_rounded - labels) ** 2)\n    return (predicted_probabilities_rounded.tolist(), mse)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (3,) (3,2) ", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Trains a single neuron with sigmoid activation using backpropagation.\n    \n    Parameters:\n        features: A 2D array of shape (n_samples, n_features)\n        labels: A 1D array of shape (n_samples,)\n        initial_weights: A 1D array of shape (n_features,)\n        initial_bias: A float (bias term)\n        learning_rate: A float (learning rate for gradient descent)\n        epochs: An integer (number of training epochs)\n        \n    Returns:\n        A tuple containing:\n        - updated_weights: The final weights after training\n        - updated_bias: The final bias after training\n        - mse_history: A list of MSE values for each epoch, rounded to 4 decimal places\n    \"\"\"\n    if features.ndim != 2 or features.shape[1] != len(initial_weights):\n        raise ValueError('Features must have the same number of columns as the initial_weights')\n    features = np.array(features)\n    labels = np.array(labels)\n    mse_history = []\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(x):\n        return sigmoid(x) * (1 - sigmoid(x))\n    weights = initial_weights.copy()\n    bias = initial_bias\n    for epoch in range(epochs):\n        predicted = (weights.dot(features.T) + bias).ravel()\n        mse = np.mean((predicted - labels) ** 2)\n        mse_history.append(round(mse, 4))\n        d_loss_w = 2 * (predicted - labels) * sigmoid_derivative(predicted) * features\n        d_loss_b = 2 * (predicted - labels) * sigmoid_derivative(predicted)\n        weights -= learning_rate * d_loss_w\n        bias -= learning_rate * d_loss_b\n    return (weights.tolist(), bias, mse_history)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": false, "result": "timed out", "solution": "\nimport torch\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad}, op='{self._op}')\"\n\n    def __add__(self, other):\n        new_value = Value(data=self.data + other.data if isinstance(other, Value) else self.data + other, _children={self, other}, _op='+')\n        return new_value\n\n    def __mul__(self, other):\n        new_value = Value(data=self.data * other.data if isinstance(other, Value) else self.data * other, _children={self, other}, _op='*')\n        return new_value\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return Value(-self.data, _children={}, _op='-')\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other / self\n\n    def backward(self):\n        if self._op == '+':\n            self._grad_from_children([self])\n        elif self._op == '*':\n            self._grad_from_children([self, self._prev[0]])\n        elif self._op == '-':\n            self._grad_from_children([self, self._prev[0]])\n        elif self._op == '/':\n            self._grad_from_children([self, self._prev[0]])\n        elif self._op == 'ReLU':\n            self._grad_from_children([self])\n        else:\n            pass\n\n    def _grad_from_children(self, children):\n        for child in children:\n            if self._op == '+':\n                child.grad += self.data * self.grad\n            elif self._op == '*':\n                child.grad += self.data * self.grad\n                child.grad += self.grad * child.data\n            elif self._op == '-':\n                child.grad += self.data * self.grad\n            elif self._op == '/':\n                child.grad += self.data * self.grad\n                child.grad -= self.grad * child.data * child.data ** 2\n            elif self._op == 'ReLU':\n                if self.data > 0:\n                    child.grad += self.grad\n                else:\n                    child.grad += 0\n            else:\n                pass\n\n    def zero_grad(self):\n        self.grad = 0\n\n    def __setattr__(self, key, value):\n        if key in ['data', '_prev', '_op', '_backward']:\n            super().__setattr__(key, value)\n        elif isinstance(value, Value):\n            self._prev.add(value)\n        else:\n            self._prev.add(Value(value))\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_np = np.array(B)\n    C_np = np.array(C)\n    P = np.dot(np.linalg.inv(C_np), B_np)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using the method from the blog post.\n    \n    Args:\n        A: A 2x2 numpy array\n        \n    Returns:\n        A tuple (U, S, V) where:\n        - U is a 2x2 orthogonal matrix\n        - S is a 2x2 diagonal matrix with singular values on the diagonal\n        - V is a 2x2 orthogonal matrix\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix')\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    U = eigenvectors[:, :2]\n    V = A[:, :2] / np.linalg.norm(A[:, :2], axis=0)\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in X and y while maintaining the corresponding order.\n    \n    Parameters:\n        X (np.ndarray): Input feature matrix.\n        y (np.ndarray): Target labels.\n        seed (int, optional): Random seed for reproducibility.\n    \n    Returns:\n        list: A list of lists containing shuffled data.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of X or (X, y) pairs based on whether y is provided.\n    \n    Parameters:\n        X (np.ndarray): Input data array.\n        y (np.ndarray, optional): Target labels array.\n        batch_size (int): Size of each batch.\n        \n    Returns:\n        list: A list of batches, where each batch is either a numpy array or a tuple (X, y).\n    \"\"\"\n    if y is None:\n        for i in range(0, X.shape[0], batch_size):\n            yield X[i:i + batch_size].tolist()\n    else:\n        for i in range(0, X.shape[0], batch_size):\n            x_batch = X[i:i + batch_size]\n            y_batch = y[i:i + batch_size]\n            yield (x_batch.tolist(), y_batch.tolist())\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: cannot reshape array of size 3 into shape (2)", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of the\n    feature at index feature_i is greater than or equal to the threshold.\n\n    Parameters:\n    X (np.ndarray): Input dataset, shape (n_samples, n_features)\n    feature_i (int): Index of the feature to evaluate\n    threshold (float): Threshold value for comparison\n\n    Returns:\n    list: List of samples where feature_i >= threshold\n    list: List of samples where feature_i < threshold\n    \"\"\"\n    feature = X[:, feature_i]\n    above_threshold = feature >= threshold\n    below_threshold = feature < threshold\n    above_list = np.where(above_threshold)[0].reshape(-1, X.shape[1])\n    below_list = np.where(below_threshold)[0].reshape(-1, X.shape[1])\n    return (above_list.tolist(), below_list.tolist())\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,3)", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n    \n    Parameters:\n        X (np.ndarray): 2D array of shape (n_samples, n_features)\n        degree (int): Maximum degree of the polynomial features to generate\n    \n    Returns:\n        np.ndarray: 2D array of shape (n_samples, sum_{k=1}^degree (n_features * k))\n            containing all polynomial features up to the specified degree.\n    \"\"\"\n    X = np.array(X, dtype=np.float64)\n    if degree == 0:\n        return X\n    (n_samples, n_features) = X.shape\n    result = np.zeros((n_samples, 1))\n    for k in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), k):\n            feature = np.ones((n_samples, 1))\n            for i in range(k):\n                idx = indices[i]\n                feature *= X[:, idx] ** (i + 1)\n            result = np.hstack((result, feature))\n    return result.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of the dataset (X, y), with or without replacement.\n\n    Parameters:\n    - X: 2D numpy array (n_samples, n_features)\n    - y: 1D numpy array (n_samples,)\n    - n_subsets: Number of subsets to generate\n    - replacements: Whether to allow sampling with replacement (True) or without (False)\n    - seed: Seed for reproducibility (default is 42)\n\n    Returns:\n    - A list of n_subsets tuples, each containing a subset of (X_subset, y_subset)\n    \"\"\"\n    np.random.seed(seed)\n    X = np.array(X)\n    y = np.array(y)\n    if len(X) < n_subsets:\n        raise ValueError('Number of samples must be at least n_subsets.')\n    indices = np.arange(len(X))\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            subset_indices = np.random.choice(indices, size=len(X), replace=True)\n        else:\n            subset_indices = np.random.choice(indices, size=len(X), replace=False)\n        X_subset = X[subset_indices]\n        y_subset = y[subset_indices]\n        subsets.append((X_subset, y_subset))\n    return subsets.tolist()\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": false, "result": "failed: index 4 is out of bounds for axis 0 with size 3", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = len(np.unique(x))\n    x = np.array(x, dtype=np.int64)\n    one_hot = np.eye(n_col, dtype=np.float32)[x]\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D NumPy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.ndarray): A 1D NumPy array.\n    \n    Returns:\n    list: A Python list containing the elements of the diagonal matrix.\n    \"\"\"\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true labels (y_true) and predicted labels (y_pred).\n    \n    Parameters:\n    y_true (np.ndarray): True labels (shape: (n_samples,))\n    y_pred (np.ndarray): Predicted labels (shape: (n_samples,))\n    \n    Returns:\n    float: Accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct = (y_true == y_pred).sum()\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n        X (np.ndarray): 2D numpy array of shape (n_samples, n_features)\n        Y (np.ndarray, optional): 2D numpy array of shape (n_samples, n_features)\n        \n    Returns:\n        np.ndarray: Correlation matrix of shape (n_features, n_features)\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X)\n    else:\n        corr_matrix = np.corrcoef(X, Y)\n    rounded_corr_matrix = np.round(corr_matrix, 4)\n    return rounded_corr_matrix.tolist()\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: name 'find_best_feature_and_threshold' is not defined", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the AdaBoost classifier fit method.\n    \n    Parameters:\n        X: 2D numpy array of shape (n_samples, n_features)\n        y: 1D numpy array of shape (n_samples,)\n        n_clf: Number of classifiers to use in AdaBoost\n    \n    Returns:\n        list: List of classifiers with their parameters (decision trees)\n    \"\"\"\n    n_samples = X.shape[0]\n    weights = np.ones(n_samples) * (1.0 / n_samples)\n    classifiers = []\n    for iter in range(n_clf):\n        (best_idx, best_threshold) = find_best_feature_and_threshold(X, y, weights)\n        clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n        clf.fit(X[:, best_idx], y)\n        y_pred = clf.predict(X)\n        error = np.sum(weights * (y != y_pred)) / np.sum(weights)\n        weights = weights * (1 - error)\n        weights = weights / np.sum(weights)\n        classifiers.append((clf, best_idx, best_threshold))\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: axis 1 is out of bounds for array of dimension 1", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores, dtype=np.float64)\n    log_softmax_array = np.log(np.exp(scores_array - np.max(scores_array, axis=1, keepdims=True)) + 1e-12)\n    rounded_log_softmax = np.round(log_softmax_array, 4)\n    return rounded_log_softmax.tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Dense.initialize() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n        self.optimizer_state = None\n\n    def initialize(self):\n        input_size = self.input_shape[0] if self.input_shape else 0\n        self.W = np.random.uniform(-1 / np.sqrt(input_size), 1 / np.sqrt(input_size), (self.n_units, input_size))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer = 'adam'\n        self.optimizer_state = {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08}\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be specified for dense layer')\n        X_reshaped = X.reshape(-1, self.input_shape[0])\n        self.output = np.dot(X_reshaped, self.W) + self.w0\n        return self.output\n\n    def backward_pass(self, accum_grad):\n        if not self.trainable:\n            return np.zeros_like(accum_grad)\n        dW = np.dot(accum_grad, self.layer_input.T)\n        dw0 = accum_grad\n        if self.optimizer == 'adam':\n            t = np.array([np.sum(np.square(self.optimizer_state['beta_1'] * self.optimizer_state['v'] + (1 - self.optimizer_state['beta_1']) * self.layer_input))])\n            v = self.optimizer_state['v']\n            s = self.optimizer_state['s']\n            self.optimizer_state['v'] = self.optimizer_state['beta_1'] * self.optimizer_state['v'] + (1 - self.optimizer_state['beta_1']) * self.layer_input\n            self.optimizer_state['s'] = self.optimizer_state['beta_2'] * self.optimizer_state['s'] + (1 - self.optimizer_state['beta_2']) * np.square(self.layer_input)\n            learning_rate = self.optimizer_state['learning_rate']\n            beta_1 = self.optimizer_state['beta_1']\n            beta_2 = self.optimizer_state['beta_2']\n            epsilon = self.optimizer_state['epsilon']\n            self.W -= learning_rate * (dW / np.sqrt(v + epsilon))\n            self.w0 -= learning_rate * (dw0 / np.sqrt(s + epsilon))\n        return np.dot(accum_grad, self.layer_input.T)\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,0) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.array(input_matrix)\n    if input_matrix.ndim != 2:\n        raise ValueError('Input matrix must be 2D')\n    if kernel.ndim != 2:\n        raise ValueError('Kernel must be 2D')\n    if kernel.shape[0] < 1 or kernel.shape[1] < 1:\n        raise ValueError('Kernel must have at least 1 row and 1 column')\n    if padding < 0 or padding > input_matrix.shape[0] // 2:\n        raise ValueError('Padding must be between 0 and half the height of the input matrix')\n    if stride < 1 or stride > input_matrix.shape[0] // 1:\n        raise ValueError('Stride must be between 1 and the height of the input matrix')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width), dtype=np.float32)\n    for i in range(output_height):\n        for j in range(output_width):\n            start_i = max(0, i * stride)\n            start_j = max(0, j * stride)\n            region = input_matrix[start_i:start_i + kernel_height, start_j:start_j + kernel_width]\n            conv_result = np.sum(region * kernel)\n            output[i, j] = conv_result\n    output = np.round(output, 4)\n    return output.tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss.\n    \n    Parameters:\n        X: 2D numpy array of shape (n_samples, n_features)\n        w: 1D numpy array of shape (n_features,)\n        y_true: 1D numpy array of shape (n_samples,)\n        alpha: Regularization parameter (float)\n    \n    Returns:\n        float: The Ridge Regression loss rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.dot(w, w)\n    total_loss = mse + regularization\n    return round(total_loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function to a given input z.\n    \n    Leaky ReLU allows for a small, non-zero gradient when the input is negative,\n    which helps with the initialization of neural networks.\n    \n    Parameters:\n        z (float): The input value to apply the Leaky ReLU to.\n        alpha (float): The slope for negative inputs (default: 0.01).\n        \n    Returns:\n        float: The result of applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate precision metric.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('y_true and y_pred must be numpy arrays.')\n    if y_true.shape != (y_pred.shape[0],):\n        raise ValueError('y_true and y_pred must have the same number of samples.')\n    tp = (y_true == 1) & (y_pred == 1).sum()\n    fp = (y_true == 0) & (y_pred == 1).sum()\n    precision = tp / (tp + fp)\n    return precision\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with different methods (SGD, Batch, Mini-Batch) using MSE.\n\n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples, 1)\n    - weights: initial weights (array of shape (n_features, 1))\n    - learning_rate: learning rate for gradient descent\n    - n_iterations: number of iterations to perform\n    - batch_size: size of the batch for mini-batch gradient descent\n    - method: 'batch', 'sgd', or 'mini_batch' to specify the method\n\n    Returns:\n    - result: list of lists, each sublist contains the weights after each iteration\n    \"\"\"\n    (n_samples, n_features) = (X.shape[0], X.shape[1])\n    result = []\n    if method == 'batch':\n        for _ in range(n_iterations):\n            predictions = np.dot(X, weights)\n            loss = (predictions - y) ** 2\n            gradient = 2 * np.dot(X.T, predictions - y) / n_samples\n            weights -= learning_rate * gradient\n            result.append(weights.tolist())\n    elif method == 'sgd':\n        for _ in range(n_iterations):\n            idx = np.random.randint(n_samples)\n            x_sample = X[idx, :]\n            y_sample = y[idx, :]\n            prediction = np.dot(x_sample, weights)\n            loss = (prediction - y_sample) ** 2\n            gradient = 2 * np.dot(x_sample.T, prediction - y_sample) / 1\n            weights -= learning_rate * gradient\n            result.append(weights.tolist())\n    elif method == 'mini_batch':\n        for _ in range(n_iterations):\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices, :]\n            y_shuffled = y[indices, :]\n            for i in range(0, n_samples, batch_size):\n                batch_X = X_shuffled[i:i + batch_size, :]\n                batch_y = y_shuffled[i:i + batch_size, :]\n                predictions = np.dot(batch_X, weights)\n                loss = (predictions - batch_y) ** 2\n                gradient = 2 * np.dot(batch_X.T, predictions - batch_y) / batch_size\n                weights -= learning_rate * gradient\n            result.append(weights.tolist())\n    else:\n        raise ValueError(\"Invalid method: must be 'batch', 'sgd', or 'mini_batch'\")\n    return result\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    if matrix.size == 0 or np.all(matrix == 0):\n        return []\n    (num_rows, num_cols) = matrix.shape\n    matrix = matrix.copy()\n    pivot_positions = []\n    for col in range(num_cols):\n        pivot = None\n        for row in range(num_rows):\n            if matrix[row, col] != 0:\n                pivot = (row, col)\n                break\n        if pivot is not None:\n            pivot_positions.append(pivot)\n    (pivot_row, pivot_col) = pivot_positions[0]\n    if matrix[pivot_row, pivot_col] != 1:\n        matrix[pivot_row, :] /= matrix[pivot_row, pivot_col]\n    for row in range(num_rows):\n        if row != pivot_row and matrix[row, pivot_col] != 0:\n            factor = matrix[row, pivot_col]\n            matrix[row, :] -= factor * matrix[pivot_row, :]\n    for row in range(num_rows):\n        if row != pivot_row and matrix[row, pivot_col] != 0:\n            factor = matrix[row, pivot_col]\n            matrix[row, :] -= factor * matrix[pivot_row, :]\n    for row in range(pivot_row + 1, num_rows):\n        if matrix[row, pivot_col] != 0:\n            factor = matrix[row, pivot_col]\n            matrix[row, :] -= factor * matrix[pivot_row, :]\n    return matrix.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: <lambda>() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm to optimize a function.\n\n    Parameters:\n    - f: Objective function to be minimized.\n    - grad: Gradient of the objective function.\n    - x0: Initial parameter values.\n    - learning_rate: Learning rate for the optimizer (default: 0.001).\n    - beta1: Exponential decay rate for the first moment estimates (default: 0.9).\n    - beta2: Exponential decay rate for the second moment estimates (default: 0.999).\n    - epsilon: Small constant for numerical stability (default: 1e-8).\n    - num_iterations: Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n    - Optimized parameters as a list, rounded to the nearest 4th decimal place.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    beta1_t = beta1\n    beta2_t = beta2\n    for _ in range(num_iterations):\n        g = grad(f, x)\n        m = beta1_t * m + (1 - beta1_t) * g\n        v = beta2_t * v + (1 - beta2_t) * g ** 2\n        m_corrected = m / (1 - beta1_t)\n        v_corrected = v / (1 - beta2_t)\n        x = x - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n        beta1_t *= 1 - beta1_t\n        beta2_t *= 1 - beta2_t\n    return np.round(x.tolist(), 4)\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> list:\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise ValueError('X and y must be NumPy arrays')\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        predictions = np.dot(X, w) + b\n        gradient_w = np.dot(X.T, predictions - y) / n_samples - alpha * np.sign(w)\n        gradient_b = np.sum(predictions - y) / n_samples\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        if abs(gradient_w).sum() < tol:\n            break\n    return w.tolist() + [b].tolist()\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    The OSA distance is the minimum number of edit operations (insert, delete, substitute, transpose)\n    required to transform the source string into the target string.\n    \n    Parameters:\n        source (str): The source string.\n        target (str): The target string.\n    \n    Returns:\n        int: The OSA distance.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                cost = 0\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n    return dp[m][n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        return round(tp / (tp + fn), 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    (batch_size, seq_len, embed_dim) = X.shape\n    Q = np.dot(X, W_q.T)\n    K = np.dot(X, W_k.T)\n    V = np.dot(X, W_v.T)\n    attention_scores = np.dot(Q, K.T)\n    attention_weights = np.softmax(attention_scores, axis=2)\n    attended_values = np.dot(attention_weights, V)\n    return attended_values.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: object of type 'float' has no len()", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    seq_len = len(input_sequence)\n    batch_size = len(input_sequence[0]) if input_sequence else 0\n    input_dim = len(input_sequence[0][0]) if input_sequence else 0\n    input_sequence = np.array(input_sequence).reshape(seq_len, batch_size, input_dim)\n    hidden_state = np.copy(initial_hidden_state)\n    for t in range(seq_len):\n        hidden_state_t = np.tanh(np.dot(input_sequence[t], Wx) + np.dot(hidden_state, Wh) + b)\n        hidden_state = hidden_state_t\n    return np.round(hidden_state, 4).tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (3,2) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of points.\n    \n    Parameters:\n        points (list of lists): List of [x, y] coordinates.\n        tx (float): Translation distance in the x-direction.\n        ty (float): Translation distance in the y-direction.\n        \n    Returns:\n        list: A new list of translated points.\n    \"\"\"\n    points_array = np.array(points, dtype=np.float64)\n    translated = np.add(points_array, np.array([[tx, ty], [0, 0]], dtype=np.float64))\n    return translated.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first distribution.\n    sigma_p (float): Standard deviation of the first distribution.\n    mu_q (float): Mean of the second distribution.\n    sigma_q (float): Standard deviation of the second distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    log_ratio_std = np.log(sigma_q / sigma_p)\n    term1 = (mu_p ** 2 - mu_q ** 2) / (2 * sigma_p ** 2)\n    term2 = -sigma_p ** 2 / (2 * sigma_q ** 2)\n    term3 = 0.5 * log_ratio_std\n    kl_divergence = term1 + term2 + term3\n    return kl_divergence\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: shapes (0,) and (3,) not aligned: 0 (dim 0) != 3 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Gauss-Seidel Iterative Method for Solving Linear Systems\n    \n    Parameters:\n        A (np.ndarray): Square matrix of coefficients.\n        b (np.ndarray): Right-hand side vector.\n        n (int): Number of iterations.\n        x_ini (np.ndarray, optional): Initial guess for the solution vector. Defaults to zeros.\n    \n    Returns:\n        np.ndarray: Approximated solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    x = x_ini if x_ini is not None else np.zeros_like(b)\n    if not isinstance(b, np.ndarray):\n        b = np.array(b)\n    if b.ndim != 1:\n        b = b.reshape(-1, 1)\n    if A.ndim != 2:\n        A = A.reshape(-1, -1)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[1]):\n            if A[i, i] == 0:\n                continue\n            sum_term = np.dot(A[i, :i], x) + np.dot(A[i, i + 1:], x_new)\n            x_new[i] = (b[i] - sum_term) / A[i, i]\n        x = x_new\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    A_np = np.array(A, dtype=float)\n    b_np = np.array(b, dtype=float)\n    n = A_np.shape[0]\n    for col in range(n):\n        max_row = col\n        for i in range(col, n):\n            if abs(A_np[i, col]) > abs(A_np[max_row, col]):\n                max_row = i\n        (A_np[col], A_np[max_row]) = (A_np[max_row], A_np[col])\n        (b_np[col], b_np[max_row]) = (b_np[max_row], b_np[col])\n        for row in range(col + 1, n):\n            factor = A_np[row, col] / A_np[col, col]\n            for j in range(col, n):\n                A_np[row, j] -= factor * A_np[col, j]\n            b_np[row] -= factor * b_np[col]\n    x = [0.0] * n\n    for i in range(n - 1, -1, -1):\n        x[i] = b_np[i] / A_np[i, i]\n        for j in range(i + 1, n):\n            x[i] -= A_np[i, j] * x[j]\n    return np.round(x, 4).tolist()\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Parameters:\n            x (np.ndarray): Input sequence of shape (num_sequences, seq_len, input_size)\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size,)\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size,)\n            \n        Returns:\n            list: List containing:\n                - hidden_states (list of np.ndarrays): List of hidden states at each time step\n                - final_hidden_state (np.ndarray): Final hidden state after processing all inputs\n                - final_cell_state (np.ndarray): Final cell state after processing all inputs\n        \"\"\"\n        (num_sequences, seq_len, input_size) = x.shape\n        batch_size = num_sequences\n        hidden_states = []\n        current_hidden = initial_hidden_state\n        current_cell = initial_cell_state\n        for t in range(seq_len):\n            x_t = x[:, t, :]\n            f = np.tanh(np.dot(x_t, self.Wf) + np.dot(current_hidden, self.Wf) + self.bf)\n            i = np.tanh(np.dot(x_t, self.Wi) + np.dot(current_hidden, self.Wi) + self.bi)\n            c = np.tanh(np.dot(x_t, self.Wc) + np.dot(current_hidden, self.Wc) + self.bc)\n            o = np.tanh(np.dot(x_t, self.Wo) + np.dot(current_hidden, self.Wo) + self.bo)\n            new_cell = c * f + i * c\n            new_hidden = o * np.tanh(new_cell)\n            hidden_states.append(new_hidden)\n            current_hidden = new_hidden\n            current_cell = new_cell\n        hidden_states = [h.tolist() for h in hidden_states]\n        final_hidden_state = current_hidden.tolist()\n        final_cell_state = current_cell.tolist()\n        return (hidden_states, np.array(final_hidden_state), np.array(final_cell_state))\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: unhashable type: 'list'", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    query_terms = set(query)\n    query_counts = {}\n    for term in query_terms:\n        query_counts[term] = 0\n        for doc in corpus:\n            if term in doc:\n                query_counts[term] += 1\n    tf = {}\n    for doc in corpus:\n        tf_doc = {}\n        for term in doc:\n            if term in query_terms:\n                tf_doc[term] = doc.count(term) / len(doc)\n        tf[doc] = tf_doc\n    df = {}\n    for term in query_terms:\n        df[term] = 0\n        for doc in corpus:\n            if term in doc:\n                df[term] += 1\n    idf = {}\n    for term in query_terms:\n        if df[term] == 0:\n            idf[term] = 1.0\n        else:\n            idf[term] = np.log(len(corpus) / df[term] + 1)\n    tf_idf_scores = []\n    for doc in corpus:\n        score = []\n        for term in query_terms:\n            if term in doc:\n                tf_score = tf[doc][term]\n                idf_score = idf[term]\n                score.append(round(tf_score * idf_score, 4))\n            else:\n                score.append(0.0)\n        tf_idf_scores.append(score)\n    return np.array(tf_idf_scores).tolist()\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('y_true and y_pred must be numpy arrays')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    y_true = y_true.astype(int)\n    y_pred = y_pred.astype(int)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp + np.float64(np.nan)) if tp + fp + np.nan != 0 else 0.0\n    recall = tp / (tp + fn + np.float64(np.nan)) if tp + fn + np.nan != 0 else 0.0\n    if beta == 1:\n        f_score_value = 2 * precision * recall / (precision + recall)\n    else:\n        numerator = (1 + beta ** 2) * precision * recall\n        denominator = (1 + beta ** 2) * (precision + recall) - beta ** 2 * precision * recall\n        f_score_value = numerator / denominator\n    return round(f_score_value, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: shapes (5,1) and (5,5) not aligned: 1 (dim 1) != 5 (dim 0)", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n        self.input_size = input_size\n        self.output_size = output_size\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns:\n            outputs: Output of the RNN for each time step.\n            last_inputs: The last input used for the final time step.\n            last_hiddens: The last hidden state used for the final time step.\n        \"\"\"\n        (batch_size, seq_len) = x.shape\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for t in range(seq_len):\n            h_current = np.dot(h_prev, self.W_hh) + self.b_h\n            h_current = np.maximum(0, h_current)\n            y_current = np.dot(h_current, self.W_hy) + self.b_y\n            y_current = np.exp(y_current) / np.sum(np.exp(y_current), axis=0)\n            outputs.append(y_current)\n            h_prev = h_current\n        return (np.array(outputs), x[-1, :], h_prev)\n\n    def backward(self, x, y, learning_rate=0.01):\n        \"\"\"\n        Backpropagation through time (BPTT) to update the weights.\n        Args:\n            x: Input sequence of shape (batch_size, seq_len, input_size)\n            y: Target sequence of shape (batch_size, seq_len, output_size)\n            learning_rate: Learning rate for weight updates\n        \"\"\"\n        (batch_size, seq_len) = (x.shape[0], x.shape[1])\n        (outputs, last_input, last_hidden) = self.forward(x)\n        loss = np.sum((outputs - y) ** 2) / (2 * seq_len)\n        dW_xh = np.zeros_like(self.W_xh)\n        dB_h = np.zeros_like(self.b_h)\n        dW_hh = np.zeros_like(self.W_hh)\n        dB_y = np.zeros_like(self.b_y)\n        dW_hy = np.zeros_like(self.W_hy)\n        for t in reversed(range(seq_len)):\n            dy = outputs[t] - y[t]\n            dW_hy += np.dot(last_hidden, dy.T) * (1 / seq_len)\n            dB_y += dy\n            dh = np.dot(dy, self.W_hy.T) * (1 / seq_len)\n            dh = np.dot(dh, self.W_hh.T) + np.dot(np.zeros((self.hidden_size, 1)), dh.T)\n            dW_hh += np.dot(dh, last_hidden.T) * (1 / seq_len)\n            dB_h += dh\n            if t > 0:\n                dx = np.dot(dh, self.W_hh.T) * (1 / seq_len)\n                dW_xh += np.dot(dx, x[t, :, :].T) * (1 / seq_len)\n                dB_h += dx\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * dB_h\n        self.b_y -= learning_rate * dB_y\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for the solution vector (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    A = np.array(A, dtype=np.float64)\n    b = np.array(b, dtype=np.float64)\n    x = x0 if x0 is not None else np.zeros_like(b)\n    r = b - A @ x\n    p = r.copy()\n    err = np.dot(r, r)\n    for iter in range(n):\n        alpha = err / np.dot(p, A @ p)\n        x += alpha * p\n        r = b - A @ x\n        beta = np.dot(r, r) / np.dot(p, p)\n        p = r + beta * p\n        if np.abs(err) <= tol:\n            break\n        err = np.dot(r, r)\n        if iter == n - 1:\n            break\n    x_rounded = np.round(x, 8)\n    return x_rounded.tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = np.unique(y, return_counts=True)\n    total_samples = len(y)\n    probabilities = class_counts[1] / total_samples\n    gini = 1 - sum((probabilities * probabilities for probabilities in class_counts[1]))\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    (rows, cols) = (len(dense_matrix), len(dense_matrix[0]))\n    values = []\n    indices = []\n    ptr = 0\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                indices.append(j)\n                ptr += 1\n    row_pointer = [0] * (rows + 1)\n    for i in range(rows):\n        row_pointer[i + 1] = ptr\n    return (values, indices, row_pointer)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    if not L:\n        raise ValueError('Line vector L cannot be empty.')\n    if all((x == 0 for x in L)):\n        raise ValueError('Line vector L cannot be the zero vector.')\n    dot_product = sum((v[i] * L[i] for i in range(len(v))))\n    magnitude_L_squared = sum((x ** 2 for x in L))\n    projection_scalar = dot_product / magnitude_L_squared\n    projection_vector = [v[i] * projection_scalar for i in range(len(v))]\n    return [round(x, 3) for x in projection_vector]\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row_indices, column_pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    if rows == 0:\n        return ([], [], [])\n    cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(cols):\n        current_values = []\n        current_rows = []\n        for row in range(rows):\n            if dense_matrix[row][col] != 0:\n                current_values.append(dense_matrix[row][col])\n                current_rows.append(row)\n        if len(current_values) > 0:\n            values.extend(current_values)\n            row_indices.extend(current_rows)\n            column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    A_np = np.array(A)\n    rref = np.linalg.matrix_rank(A_np)\n    rank = rref\n    identity_matrix = np.eye(A_np.shape[1])\n    A_rolled = A_np.copy()\n    for i in range(A_rolled.shape[0]):\n        if abs(A_rolled[i, i]) < 1e-10:\n            continue\n        for j in range(i + 1, A_rolled.shape[0]):\n            factor = A_rolled[j, i] / A_rolled[i, i]\n            A_rolled[j] -= factor * A_rolled[i]\n    pivot_columns = [i for i in range(A_rolled.shape[1]) if abs(A_rolled[i, i]) > 1e-10]\n    basis_vectors = []\n    for col in pivot_columns:\n        basis_vectors.append(A_np[:, col].tolist())\n    basis_vectors = [[round(val, 8) for val in vec] for vec in basis_vectors]\n    return basis_vectors\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    sst = np.sum((y_true - y_mean) ** 2)\n    ssr = np.sum((y_true - y_pred) ** 2)\n    r_squared = 1 - ssr / sst\n    return round(r_squared, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_brightness(img):\n    if not img:\n        return -1\n    row_lengths = set((len(row) for row in img))\n    if len(row_lengths) > 1:\n        return -1\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n    total_brightness = sum((pixel for row in img for pixel in row)) / len(img[0])\n    return round(total_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are not compatible (e.g., mismatched shapes, empty arrays).\n    \"\"\"\n    if not (isinstance(y_true, (list, tuple, np.ndarray)) and isinstance(y_pred, (list, tuple, np.ndarray))):\n        raise ValueError('Input must be a list, tuple, or numpy array.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must have the same length.')\n    if np.isscalar(y_true) and np.isscalar(y_pred):\n        if y_true.size == 0 or y_pred.size == 0:\n            raise ValueError('Cannot compute RMSE for empty arrays.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): True labels (binary array)\n    y_pred (np.ndarray): Predicted labels (binary array)\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places\n    \"\"\"\n    if y_true.size == 0 or y_pred.size == 0:\n        return 0.0\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if np.sum(y_true) == 0 or np.sum(y_pred) == 0:\n        return 0.0\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    dice_score = 2 * intersection / (union + 1e-09)\n    return round(dice_score, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n        row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n        dim (int): The dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n        list: A list representing the composite hypervector.\n    \"\"\"\n    hypervectors = []\n    for (feature_name, value) in row.items():\n        seed = random_seeds.get(feature_name, 0)\n        hypervector = np.random.default_rng(seed).uniform(0, 1, size=dim)\n        hypervectors.append(hypervector.tolist())\n    composite_hv = np.array(hypervectors).tolist()\n    return composite_hv\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n\n    Args:\n        data: A list of lists, where each inner list contains [y_true, y_pred].\n\n    Returns:\n        A 2x2 list of lists representing the confusion matrix.\n    \"\"\"\n    y_true = [row[0] for row in data]\n    y_pred = [row[1] for row in data]\n    true_pos = sum((1 for (t, p) in zip(y_true, y_pred) if t == p == 1))\n    false_pos = sum((1 for (t, p) in zip(y_true, y_pred) if t == p == 0))\n    false_neg = sum((1 for (t, p) in zip(y_true, y_pred) if t == 0 and p == 1))\n    true_neg = sum((1 for (t, p) in zip(y_true, y_pred) if t == p == 0))\n    confusion_matrix = [[true_pos, false_neg], [false_pos, true_neg]]\n    return confusion_matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters:\n    v1 (np.ndarray): First input vector.\n    v2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: Cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_similarity_value = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity_value, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: local variable 'precision' referenced before assignment", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('Actual and predicted lists must have the same length.')\n    if not all((x in (0, 1) for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted must be 0 or 1.')\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + fp + tn + fn)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    result = (tuple(map(round, confusion_matrix)), round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))\n    return result\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or np.ndarray): A list or NumPy array of numerical values.\n    \n    Returns:\n    dict: A dictionary containing all the requested descriptive statistics.\n    \"\"\"\n    data_array = np.array(data)\n    mean = np.mean(data_array)\n    median = np.median(data_array)\n    try:\n        mode = np.unique(data_array).mean()\n    except ValueError:\n        mode = data_array[0]\n    variance = np.var(data_array)\n    standard_deviation = np.std(data_array)\n    percentiles = np.percentile(data_array, [25, 50, 75])\n    iqr = percentiles[1] - percentiles[0]\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(iqr, 4)}\n    return result\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if k < 0 or k > n:\n        raise ValueError('k must be between 0 and n')\n    if p < 0 or p > 1:\n        raise ValueError('p must be between 0 and 1')\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    The PDF of the normal distribution is given by:\n    f(x) = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of occurrences lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative float')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    The contrast is defined as the difference between the maximum and minimum pixel values in the image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        \n    Returns:\n        float: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        img = np.array(img)\n    if img.ndim != 2:\n        raise ValueError('Input must be a 2D array.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the input arrays are not 1D or have different lengths.\n    \"\"\"\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise ValueError('Both inputs must be NumPy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Vectors must have the same length.')\n    return np.dot(vec1, vec2)\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n        If the degree is less than 0, returns an empty list.\n    \"\"\"\n    if degree < 0:\n        return []\n    X = np.array(data, dtype=np.float64)\n    if len(X) == 0:\n        return []\n    from sklearn.preprocessing import PolynomialFeatures\n    poly = PolynomialFeatures(degree=degree)\n    transformed = poly.fit_transform(X)\n    transformed_rounded = np.round(transformed, 8)\n    return transformed_rounded.tolist()\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return [-1.0]\n    num_positions = position + 1\n    num_dimensions = d_model\n    pos_enc = np.zeros((num_positions, num_dimensions), dtype=np.float16)\n    for i in range(num_positions):\n        for j in range(num_dimensions):\n            freq = 1.0 / np.power(1024.0, 2 * j / num_dimensions)\n            pos_enc[i, j] = np.sin(i / freq) / freq\n            pos_enc[i, j] += np.cos(i / freq) / freq\n    return pos_enc.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    \n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    accuracy_diff = abs(training_accuracy - test_accuracy)\n    if accuracy_diff > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter, dtype=np.float64)\n    grad = np.array(grad, dtype=np.float64)\n    bias_correction1 = 1.0 / (1.0 - np.power(beta1, t))\n    bias_correction2 = 1.0 / (1.0 - np.power(beta2, t))\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * np.square(grad)\n    m_t_corrected = m_t / bias_correction1\n    v_t_corrected = v_t / bias_correction2\n    parameter = parameter - learning_rate * m_t_corrected / np.sqrt(v_t_corrected + epsilon)\n    m = m_t_corrected\n    v = v_t_corrected\n    updated_parameter = parameter.tolist()\n    updated_m = m.tolist()\n    updated_v = v.tolist()\n    return (updated_parameter, updated_m, updated_v)\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: axis 1 is out of bounds for array of dimension 1", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_tokens = encoder.encode(prompt)\n    prompt_length = len(prompt_tokens)\n    generated_tokens = prompt_tokens.copy()\n    for _ in range(n_tokens_to_generate):\n        pos_emb = np.sin(np.arange(1024) / (1024 / hparams['n_ctx']))\n        pos_emb = pos_emb.reshape(1, 1024, 1)\n        x = np.concatenate((generated_tokens, pos_emb), axis=1)\n        wte = params['wte']\n        x = np.dot(x, wte)\n        attention_weights = np.random.rand(x.shape[1], x.shape[1])\n        x = np.dot(x, attention_weights)\n        ff = np.random.rand(x.shape[0], x.shape[1])\n        x = np.dot(x, ff)\n        ln_f = params['ln_f']\n        x = (x - ln_f['g']) / ln_f['b']\n        generated_tokens = np.concatenate((generated_tokens, x), axis=1)\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: cannot reshape array of size 3 into shape (3,2)", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        if np.allclose(values, 0):\n            return np.zeros_like(values)\n        exp_values = np.exp(values) / np.sum(np.exp(values), axis=1, keepdims=True)\n        return exp_values\n    crystal_matrix = np.array(crystal_values).reshape(n, dimension)\n    attention_scores = np.dot(crystal_matrix, crystal_matrix.T)\n    attention_weights = softmax(attention_scores)\n    weighted_pattern = np.dot(attention_weights, crystal_matrix)\n    return [round(val, 4) for val in weighted_pattern.flatten()]\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "timed out", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given query against a corpus of documents.\n    \n    Parameters:\n        corpus (list of str): List of documents (strings).\n        query (str): The search query.\n        k1 (float): A parameter that controls the saturation effect of term frequency.\n        b (float): A parameter that controls the impact of document length on score.\n        \n    Returns:\n        list: A list of scores (floats) for each document in the corpus, rounded to 3 decimal places.\n    \"\"\"\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    if not hasattr(stopwords, 'words'):\n        import nltk\n        nltk.download('stopwords')\n        nltk.download('punkt')\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(query.lower())\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n    query_terms = Counter(filtered_tokens)\n    corpus_terms = {}\n    for doc in corpus:\n        words = word_tokenize(doc.lower())\n        filtered_words = [word for word in words if word not in stop_words]\n        doc_terms = Counter(filtered_words)\n        for (term, count) in doc_terms.items():\n            if term in corpus_terms:\n                corpus_terms[term] += count\n            else:\n                corpus_terms[term] = count\n    scores = []\n    for doc in corpus:\n        words = word_tokenize(doc.lower())\n        filtered_words = [word for word in words if word not in stop_words]\n        doc_terms = Counter(filtered_words)\n        tf = sum((doc_terms.get(term, 0) for term in query_terms)) / len(filtered_words)\n        idf = np.log(len(corpus) / (len(corpus_terms) - sum((1 for count in corpus_terms.values() if count == 0)) + 1))\n        df = sum((1 for count in corpus_terms.values() if count >= query_terms[term]))\n        score = k1 * tf * (1 + k1 * df) / (1 + k1 * df) * idf * (1 - b + b * len(words))\n        scores.append(round(score, 3))\n    return scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    import numpy as np\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must have the same length.')\n    tp = np.sum(np.logical_and(y_true == y_pred, y_true == 1))\n    fp = np.sum(np.logical_and(y_true != y_pred, y_true == 1))\n    fn = np.sum(np.logical_and(y_true != y_pred, y_true == 0))\n    precision = tp / (tp + fp + np.nan_to_num(np.zeros_like(fp)))\n    recall = tp / (tp + fn + np.nan_to_num(np.zeros_like(fn)))\n    f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption - 10 * math.sin(2 * math.pi * i / 10) for (i, consumption) in enumerate(consumption_data)]\n    n = len(detrended_data)\n    x = [i for i in range(n)]\n    y = detrended_data\n    mean_x = sum(x) / n\n    mean_y = sum(y) / n\n    numerator = sum(((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)))\n    denominator = sum(((x[i] - mean_x) ** 2 for i in range(n)))\n    m = numerator / denominator\n    b = mean_y - m * mean_x\n    prediction = m * 15 + b\n    fluctuation = 10 * math.sin(2 * math.pi * 15 / 10)\n    predicted_with_fluctuation = prediction + fluctuation\n    rounded_value = round(predicted_with_fluctuation)\n    safety_margin = 0.05 * rounded_value\n    final_value = round(rounded_value + safety_margin)\n    return final_value\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error (MAE) between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\nimport numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    X_reshaped = X.reshape(-1, X.shape[2])\n    Q = X_reshaped @ W_q.T\n    K = X_reshaped @ W_k.T\n    V = X_reshaped @ W_v.T\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, scale: float=1.0) -> np.ndarray:\n    \"\"\"\n    Compute self-attention using the dot product of Q and K.\n    \"\"\"\n    attention_scores = Q @ K.T / (scale * np.sqrt(K.shape[1]))\n    attention_weights = np.softmax(attention_scores, axis=1)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Multi-head attention mechanism.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_reshaped = Q.reshape(-1, n_heads, Q.shape[1])\n    K_reshaped = K.reshape(-1, n_heads, K.shape[1])\n    V_reshaped = V.reshape(-1, n_heads, V.shape[1])\n    outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q_reshaped[:, i], K_reshaped[:, i], V_reshaped[:, i])\n        outputs.append(head_output)\n    final_output = np.concatenate(outputs, axis=1)\n    final_output_rounded = np.round(final_output, 4)\n    return final_output_rounded.tolist()\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": false, "result": "failed: name 'math' is not defined", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of the same length.')\n    count_0_x = x.count(0)\n    count_1_x = len(x) - count_0_x\n    count_0_y = y.count(0)\n    count_1_y = len(y) - count_0_y\n    contingency_table = [[count_1_x, count_0_x], [count_1_y, count_0_y]]\n    row_sums = [contingency_table[i][0] + contingency_table[i][1] for i in range(2)]\n    col_sums = [contingency_table[0][i] + contingency_table[1][i] for i in range(2)]\n    total = row_sums[0] + row_sums[1]\n    numerator = total * sum((i * j for (i, j) in zip(row_sums, col_sums))) - sum(row_sums) * sum(col_sums)\n    denominator = math.sqrt(total * sum((i * i for i in row_sums)) - sum(row_sums) ** 2) * math.sqrt(total * sum((j * j for j in col_sums)) - sum(col_sums) ** 2)\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": false, "result": "failed: name 'hard_sigmoid' is not defined", "solution": "\n\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x < 0:\n        return alpha * (math.exp(x) - 1)\n    else:\n        return math.exp(x)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(x, x * alpha)\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return math.log(1 + math.exp(x))\n    elif x < -100:\n        return math.log(1 + math.exp(x))\n    else:\n        return math.log(1 + math.exp(x))\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / 1.0, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    grpo = np.sum(rhos * A) - beta * np.sum(np.log(pi_theta_ref) - np.log(pi_theta_old))\n    clipped_grpo = np.clip(grpo, -epsilon, epsilon)\n    return round(clipped_grpo, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": false, "result": "failed: test case failed: swish(1)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * math.exp(x) / (1 + math.exp(x))\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    return scale * np.exp(alpha * x) if x >= 0 else scale * np.exp(alpha * x) * (1 + np.exp(-alpha * x))\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_model = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_model))\n    predictions = [1 if prob > 0.5 else 0 for prob in probabilities]\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Parameters:\n        X: Input features (NxM)\n        y: True labels (NxK)\n        learning_rate: Learning rate for gradient descent\n        iterations: Number of iterations to perform\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_classes = y.shape[1]\n    num_features = X.shape[1]\n    B = np.zeros((num_classes, num_features))\n    losses = []\n\n    def compute_loss(X, y, B):\n        scores = X @ B.T\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y * np.log(probabilities)) / X.shape[0]\n        return loss\n    for i in range(iterations):\n        scores = X @ B.T\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        softmax_probs = probabilities\n        grads = np.zeros_like(B)\n        for j in range(num_classes):\n            prob_j = softmax_probs[:, j]\n            delta = prob_j - y[:, j]\n            grads[j] = X.T @ delta\n        B = B - learning_rate * grads\n        loss = compute_loss(X, y, B)\n        losses.append(round(loss, 4))\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_rounded, losses_rounded)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n        X: Input features (shape: (n_samples, n_features))\n        y: Target labels (shape: (n_samples,))\n        learning_rate: Learning rate for gradient descent\n        iterations: Number of iterations to perform gradient descent\n        \n    Returns:\n        A tuple containing:\n        - A list of optimized coefficients (shape: (n_features,))\n        - A list of loss values (shape: (iterations,))\n    \"\"\"\n    if X.ndim != 2:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    coefficients = np.zeros(n_features)\n    loss_history = []\n    for iteration in range(iterations):\n        predictions = 1 / (1 + np.exp(-np.dot(X, coefficients)))\n        loss = -np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions)) / n_samples\n        loss_history.append(round(loss, 4))\n        gradient = np.dot(X.T, predictions - y) / n_samples\n        coefficients -= learning_rate * gradient\n        if iteration > 0 and abs(loss_history[iteration] - loss_history[iteration - 1]) < 1e-06:\n            break\n    coefficients_list = coefficients.tolist()\n    loss_list = [round(loss, 4) for loss in loss_history]\n    return (coefficients_list, loss_list)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'softmax'", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    The mask is a 2D array with shape (batch_size, seq_len), where\n    mask[i][j] = 1 if position i, j is valid (i.e., not padding),\n    and 0 if it is padding.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    masked_scores = attention_scores * mask\n    attention_weights = np.softmax(masked_scores, axis=1)\n    result = np.dot(attention_weights, V)\n    return result.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is 0 if all apples are the same color and increases as the variety\n    of colors increases. The function returns the disorder rounded to the nearest\n    4th decimal place.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    if len(color_counts) == 1:\n        return 0.0\n    num_colors = len(color_counts)\n    import math\n    if len(apples) == 0:\n        return 0.0\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    probabilities = [count / total for count in color_counts.values()]\n    entropy = 0.0\n    for p in probabilities:\n        if p > 0:\n            entropy += p * math.log2(p)\n    disorder = -entropy\n    return round(disorder, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: type list doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Applies Layer Normalization to the input tensor X.\n    \n    Parameters:\n        X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n        gamma (np.ndarray): Scaling parameter (shape: (feature_dim,))\n        beta (np.ndarray): Shifting parameter (shape: (feature_dim,))\n        epsilon (float): Small value added to the denominator to avoid division by zero.\n        \n    Returns:\n        list: Normalized X with rounded values to 5 decimal places.\n    \"\"\"\n    X = X.copy()\n    (batch_size, seq_len, feat_dim) = X.shape\n    mean = X.mean(axis=2, keepdims=True)\n    var = ((X - mean) ** 2).mean(axis=2, keepdims=True)\n    std = np.sqrt(var + epsilon)\n    normalized = (X - mean) / std\n    normalized = normalized * gamma + beta\n    return [round(val, 5) for val in normalized.tolist()]\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Parameters:\n        reference (list of str): List of reference sentences.\n        candidate (list of str): List of candidate translations.\n        alpha (float): Weight for precision (default: 0.9)\n        beta (float): Weight for recall (default: 3)\n        gamma (float): Weight for the penalty for word order fragmentation (default: 0.5)\n\n    Returns:\n        float: METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    def preprocess(text):\n        text = text.lower()\n        text = text.replace(',', '').replace('.', '').replace('?', '').replace(':', '')\n        text = text.replace(';', '').replace('-', '').replace('!', '').replace('\"', '')\n        text = text.replace('(', '').replace(')', '')\n        return text\n    reference_processed = [preprocess(sent) for sent in reference]\n    candidate_processed = [preprocess(sent) for sent in candidate]\n    reference_words = Counter((word for sent in reference_processed for word in sent.split()))\n    candidate_words = Counter((word for sent in candidate_processed for word in sent.split()))\n    common_words = reference_words & candidate_words\n    total_words = len(reference_words) + len(candidate_words) - len(common_words)\n    precision = len(common_words) / len(candidate_words) if len(candidate_words) > 0 else 0.0\n    recall = len(common_words) / len(reference_words) if len(reference_words) > 0 else 0.0\n    f_mean = 2 * precision * recall / (alpha * precision + beta * recall)\n    penalty = 0.0\n    if len(reference_processed) > 0 and len(candidate_processed) > 0:\n        similarity = sum((1 for (r, c) in zip(reference_processed, candidate_processed) if r == c))\n        penalty = (1 - similarity / len(reference_processed)) * gamma\n    meteor = alpha * precision + beta * recall + gamma * penalty\n    return round(meteor, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: name 'pmi' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    expected_joint = joint_counts / total_samples * (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    actual_joint = joint_counts / total_samples\n    pm_i = np.log2(actual_joint / expected_joint)\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    return [(value - min_val) / (max_val - min_val) for value in x]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x = np.dot(x, w1)\n    x = np.maximum(0, x)\n    x = np.dot(x, w2)\n    x = np.maximum(0, x)\n    x = x + x\n    return np.round(x, 4).tolist()\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D input array.\n\n    Parameters:\n    x (np.ndarray): Input 3D array with shape (height, width, channels)\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,), where each element is the average of the corresponding feature map.\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise ValueError('Input must be a NumPy array.')\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array.')\n    (height, width, channels) = x.shape\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D NumPy array in BCHW format.\n\n    Args:\n        X: Input 4D array with shape (batch_size, channels, height, width)\n        gamma: Scale parameter for normalization (shape: (channels,))\n        beta: Shift parameter for normalization (shape: (channels,))\n        epsilon: Small value to add to denominator for numerical stability.\n\n    Returns:\n        Output after batch normalization, rounded to 4 decimal places, as a list.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('Input must be a 4D array.')\n    if X.shape[0] != X.shape[1] or X.shape[2] != X.shape[3]:\n        raise ValueError('Input must have equal height and width dimensions.')\n    (batch_size, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = normalized * gamma + beta\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    Args:\n        vectors: A list of 2D vectors (each vector is a list of 2 floats).\n        tol: Tolerance for checking linear independence (used in Gram-Schmidt process).\n        \n    Returns:\n        A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    vecs = np.array(vectors)\n    orthonormal_basis = []\n    for (i, vec) in enumerate(vecs):\n        e = vec.copy()\n        for j in range(i):\n            proj = np.dot(e, orthonormal_basis[j]) * orthonormal_basis[j]\n            e -= proj\n        e /= np.linalg.norm(e)\n        orthonormal_basis.append(np.round(e, 4))\n    return orthonormal_basis.tolist()\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (np.array): A 1D numpy array of shape (3,)\n    b (np.array): A 1D numpy array of shape (3,)\n    \n    Returns:\n    np.array: A 1D numpy array of shape (3,) with the cross product, rounded to 4 decimal places.\n    \"\"\"\n    result = np.cross(a, b)\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'shape'", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != len(b):\n        raise ValueError('Number of equations must match the length of vector b.')\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    x = np.zeros_like(b)\n    for i in range(A.shape[1]):\n        A_i = np.column_stack((A[:, :i], b))\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    bhattacharyya_coeff = np.dot(p_array, q_array.T) / (np.sqrt(np.dot(p_array, p_array.T)) * np.sqrt(np.dot(q_array, q_array.T)))\n    bhattacharyya_distance = -np.log(bhattacharyya_coeff)\n    return round(bhattacharyya_distance, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = []\n    for (x, y) in zip(a, b):\n        result.append(x + y)\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states = theta.shape[0]\n    num_actions = theta.shape[1]\n    total_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        state_seq = []\n        action_seq = []\n        reward_seq = []\n        for (state, action, reward) in episode:\n            state_seq.append(state)\n            action_seq.append(action)\n            reward_seq.append(reward)\n        policy = np.exp(theta[state_seq] - np.max(theta[state_seq, :], axis=1, keepdims=True)) / np.sum(np.exp(theta[state_seq] - np.max(theta[state_seq, :], axis=1, keepdims=True)), axis=1)\n        log_policy = np.log(policy)\n        gamma = 1.0\n        returns = []\n        current_return = 0.0\n        for t in reversed(range(len(reward_seq))):\n            current_return = reward_seq[t] + gamma * current_return\n            returns.insert(0, current_return)\n        for t in range(len(reward_seq)):\n            state = state_seq[t]\n            action = action_seq[t]\n            log_p = log_policy[t, action]\n            return_t = returns[t]\n            grad = log_p * return_t\n            total_gradient[state, action] += grad\n    average_gradient = total_gradient / len(episodes)\n    rounded_gradient = np.round(average_gradient, 4).tolist()\n    return rounded_gradient\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = 2 * d_in * d_out * n_experts\n    flops_moe = k_active * d_in * d_out\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    savings = round(savings, 1)\n    return {'flops_dense': flops_dense, 'flops_moe': flops_moe, 'savings_percentage': savings}\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: N must have the same number of columns as X.", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the Noisy Top-K Gating probabilities for an MoE model.\n\n    Parameters:\n    - X: Input matrix (shape: (n_samples, n_features))\n    - W_g: Gate weights matrix (shape: (n_features, n_experts))\n    - W_noise: Noise matrix (shape: (n_features, n_experts))\n    - N: Shape of the input matrix (for broadcasting)\n    - k: Sparsity constraint (number of top elements to consider)\n\n    Returns:\n    - A list of lists representing the final gating probabilities matrix.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError('X must be a 2D array.')\n    if W_g.ndim != 2 or W_noise.ndim != 2:\n        raise ValueError('W_g and W_noise must be 2D arrays.')\n    if N.ndim != 1 or N.shape[0] != X.shape[1]:\n        raise ValueError('N must have the same number of columns as X.')\n    X_reshaped = X.reshape(-1, 1)\n    W_g_reshaped = W_g.reshape(1, -1)\n    W_noise_reshaped = W_noise.reshape(1, -1)\n    top_k_values = np.dot(X_reshaped, W_g_reshaped) + np.dot(X_reshaped, W_noise_reshaped)\n    top_k_indices = np.argpartition(top_k_values, axis=1, stable=True)[:, :k]\n    top_k_values = np.take_along_axis(top_k_values, axis=1, mask=top_k_indices)\n    probabilities = top_k_values / top_k_values.sum(axis=1, keepdims=True)\n    probabilities_rounded = np.round(probabilities, 4)\n    result = probabilities_rounded.reshape(N.shape[0], -1)\n    return result.tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: Input must be 2D (batch x sequence)", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    x = np.array(x, dtype=np.float32)\n    if x.ndim != 2:\n        raise ValueError('Input must be 2D (batch x sequence)')\n    if x.shape[1] != n_experts:\n        raise ValueError('Input shape must match number of experts')\n    Wg_normalized = Wg / np.linalg.norm(Wg, axis=1, keepdims=True)\n    gate_probs = np.dot(x, Wg_normalized)\n    gate_probs = np.exp(gate_probs) / np.sum(np.exp(gate_probs), axis=1, keepdims=True)\n    top_k_indices = np.argsort(gate_probs, axis=1)[:, :top_k]\n    output = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        selected_experts = top_k_indices[i]\n        output[i] = np.dot(x[i, :], We[selected_experts])\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    X = np.array(X, dtype=np.float32)\n    if X.ndim != 4:\n        raise ValueError('Input must be a 4D array (B, C, H, W)')\n    if num_groups <= 0:\n        raise ValueError('num_groups must be greater than 0')\n    (B, C, H, W) = X.shape\n    group_size = C // num_groups\n    groups = []\n    for i in range(num_groups):\n        start = i * group_size\n        end = start + group_size\n        group = X[:, start:end, :, :]\n        groups.append(group)\n    normalized = []\n    for group in groups:\n        mean = np.mean(group, axis=(0, 2, 3), keepdims=True)\n        var = np.var(group, axis=(0, 2, 3), keepdims=True)\n        norm = (group - mean) / np.sqrt(var + epsilon)\n        normalized_group = norm * gamma + beta\n        normalized.append(normalized_group)\n    normalized_tensor = np.concatenate(normalized, axis=1)\n    normalized_tensor = np.round(normalized_tensor, 4)\n    return normalized_tensor.tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": false, "result": "failed: local variable 'previous_x' referenced before assignment", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the minimum of the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n    \n    Args:\n        start_x: Initial guess for x.\n        learning_rate: Step size for each iteration.\n        tolerance: Stopping condition (when the change in x is less than tolerance).\n        max_iters: Maximum number of iterations to perform.\n        \n    Returns:\n        The x value where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        df_dx = 4 * x ** 3 - 9 * x ** 2\n        x -= learning_rate * df_dx\n        if abs(x - previous_x) < tolerance:\n            break\n        previous_x = x\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) function to the input array.\n    \n    The DyT function is a normalization-free transformation that preserves the squashing behavior of the tanh function.\n    It is inspired by the tanh function and is used in Transformer architectures for activation.\n    \n    Parameters:\n        x (np.ndarray): Input array of shape (batch_size, sequence_length, features)\n        alpha (float): Scaling factor for the dynamic tanh\n        gamma (float): Shift factor for the dynamic tanh\n        beta (float): Additional scaling factor for the dynamic tanh\n    \n    Returns:\n        list[float]: Output list of floats after applying the dynamic tanh function.\n    \"\"\"\n    y = alpha * (np.tanh(gamma * x + beta) + 1) / 2\n    return y.tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
