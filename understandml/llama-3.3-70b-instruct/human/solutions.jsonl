{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    This function computes the dot product of a matrix and a vector.\n\n    Args:\n        a (list[list[int|float]]): A 2D list representing the matrix.\n        b (list[int|float]): A list representing the vector.\n\n    Returns:\n        list[int|float]: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n    \n    Args:\n        a (list[list[int|float]]): The input matrix.\n    \n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    if not a:\n        return []\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transpose = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transpose[j][i] = a[i][j]\n    return transpose\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntranspose = transpose_matrix(matrix)"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n        new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n        list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n    arr = np.array(a)\n    total_elements = arr.size\n    desired_elements = new_shape[0] * new_shape[1]\n    if total_elements != desired_elements:\n        return []\n    try:\n        reshaped_arr = arr.reshape(new_shape)\n    except ValueError:\n        return []\n    return reshaped_arr.tolist()\nnew_shape = (2, 4)"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    matrix (list[list[float]]): A 2D list of floats representing the matrix.\n    mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means according to the specified mode.\n\n    Raises:\n    ValueError: If the mode is not 'row' or 'column'.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    means = []\n    if mode == 'row':\n        for row in matrix:\n            row_mean = sum(row) / len(row)\n            means.append(row_mean)\n    elif mode == 'column':\n        transposed_matrix = list(zip(*matrix))\n        for column in transposed_matrix:\n            column_mean = sum(column) / len(column)\n            means.append(column_mean)\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    This function multiplies a matrix by a scalar and returns the result.\n\n    Args:\n        matrix (list[list[int|float]]): A 2D list representing the matrix.\n        scalar (int|float): The scalar value to multiply the matrix with.\n\n    Returns:\n        list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = []\n        for element in row:\n            new_row.append(element * scalar)\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]): A 2x2 matrix.\n\n    Returns:\n    list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    matrix = np.array(matrix)\n    if matrix.shape != (2, 2):\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    eigenvalues = np.linalg.eigvals(matrix)\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    return eigenvalues.tolist()"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^-1 * A * S, \n    where T and S are invertible matrices.\n\n    Args:\n        A (list[list[int|float]]): The input matrix to be transformed.\n        T (list[list[int|float]]): The first invertible matrix.\n        S (list[list[int|float]]): The second invertible matrix.\n\n    Returns:\n        list[list[int|float]]: The transformed matrix if T and S are invertible, -1 otherwise.\n    \"\"\"\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    transformed_A = np.dot(np.dot(T_inv, A), S)\n    transformed_A = np.round(transformed_A, 4)\n    return transformed_A.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix.\n\n    Returns:\n    list[list[float]]: The inverse of the matrix if it exists, otherwise None.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function multiplies two matrices.\n    \n    Args:\n    a (list[list[int | float]]): The first matrix.\n    b (list[list[int | float]]): The second matrix.\n    \n    Returns:\n    list[list[int | float]]: The product of the two matrices if they can be multiplied, -1 otherwise.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List\nimport unittest\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: A covariance matrix as a list of lists.\n    \"\"\"\n    vectors_array = np.array(vectors)\n    mean = np.mean(vectors_array, axis=1)\n    centered_data = vectors_array - mean[:, np.newaxis]\n    covariance_matrix = np.cov(centered_data)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n\n    Args:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A vector representing the constants of the linear equations.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x as a list, rounded to four decimal places.\n    \"\"\"\n    num_equations = A.shape[0]\n    x = np.zeros(num_equations)\n    for _ in range(n):\n        new_x = np.zeros(num_equations)\n        for i in range(num_equations):\n            sum_terms = sum((A[i, j] * x[j] for j in range(num_equations) if j != i))\n            new_x[i] = (b[i] - sum_terms) / A[i, i]\n        new_x = np.round(new_x, 4)\n        x = new_x\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the singular values, and the U, V matrices.\n    \"\"\"\n    U = np.eye(2)\n    V = np.eye(2)\n    singular_values = np.zeros(2)\n    for _ in range(100):\n        max_angle = 0\n        max_p = 0\n        max_q = 0\n        for p in range(2):\n            for q in range(p + 1, 2):\n                if abs(A[p, q]) > max_angle:\n                    max_angle = abs(A[p, q])\n                    max_p = p\n                    max_q = q\n        if max_angle < 1e-06:\n            break\n        tau = (A[max_q, max_q] - A[max_p, max_p]) / (2 * A[max_p, max_q])\n        t = np.sign(tau) / (np.abs(tau) + np.sqrt(1 + tau ** 2))\n        c = 1 / np.sqrt(1 + t ** 2)\n        s = t * c\n        G = np.eye(2)\n        G[max_p, max_p] = c\n        G[max_p, max_q] = -s\n        G[max_q, max_p] = s\n        G[max_q, max_q] = c\n        U = np.dot(U, G)\n        A = np.dot(G.T, np.dot(A, G))\n    singular_values[0] = np.sqrt(A[0, 0] ** 2 + A[0, 1] ** 2)\n    singular_values[1] = np.sqrt(A[1, 0] ** 2 + A[1, 1] ** 2)\n    V = np.dot(A, U)\n    V[:, 0] = V[:, 0] / np.linalg.norm(V[:, 0])\n    V[:, 1] = V[:, 1] / np.linalg.norm(V[:, 1])\n    return ((round(singular_values[0], 4), round(singular_values[1], 4)), U, V)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n    matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input matrix must be a 4x4 matrix')\n\n    def determinant_3x3(matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix using Laplace's Expansion method.\n\n        Args:\n        matrix (list[list[int|float]]): A 3x3 matrix represented as a list of lists.\n\n        Returns:\n        float: The determinant of the matrix.\n        \"\"\"\n        if len(matrix) != 3 or any((len(row) != 3 for row in matrix)):\n            raise ValueError('The input matrix must be a 3x3 matrix')\n        (a, b, c) = (matrix[0][0], matrix[0][1], matrix[0][2])\n        (d, e, f) = (matrix[1][0], matrix[1][1], matrix[1][2])\n        (g, h, i) = (matrix[2][0], matrix[2][1], matrix[2][2])\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det_minor = determinant_3x3(minor)\n        det += (-1) ** i * matrix[0][i] * det_minor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Args:\n    X (list[list[float]]): A 2D list representing the feature matrix.\n    y (list[float]): A list representing the target vector.\n\n    Returns:\n    list[float]: A list of coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    list: Coefficients of the linear regression model.\n    \"\"\"\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / len(y)\n        theta = theta - alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n    tuple[list[list[float]], list[list[float]]]: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data = np.round(standardized_data, 4)\n    normalized_data = np.round(normalized_data, 4)\n    standardized_data = standardized_data.tolist()\n    normalized_data = normalized_data.tolist()\n    return (standardized_data, normalized_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function implements the k-Means clustering algorithm to partition n points into k clusters.\n    \n    Parameters:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    k (int): An integer representing the number of clusters to form.\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates.\n    max_iterations (int): An integer representing the maximum number of iterations to perform.\n    \n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    iteration = 0\n    while iteration < max_iterations:\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n        iteration += 1\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\npoints = [(1.0, 1.0), (1.0, 2.0), (2.0, 1.0), (2.0, 2.0), (10.0, 10.0), (10.0, 11.0), (11.0, 10.0), (11.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (10.0, 10.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    - X (np.ndarray): The feature dataset.\n    - y (np.ndarray): The target dataset.\n    - k (int, optional): The number of folds. Defaults to 5.\n    - shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    - random_seed (int, optional): The random seed for reproducibility. Defaults to None.\n\n    Returns:\n    - list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if k <= 1:\n        raise ValueError('The number of folds must be greater than 1.')\n    if len(X) == 0 or len(y) == 0:\n        raise ValueError('The dataset cannot be empty.')\n    if len(X) != len(y):\n        raise ValueError('The lengths of X and y must match.')\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    indices = np.arange(len(X))\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = len(X) // k\n    train_test_indices = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else len(X)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        train_test_indices.append((train_indices, test_indices))\n    return train_test_indices"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    This function performs Principal Component Analysis (PCA) from scratch.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    k (int): The number of principal components to return.\n    \n    Returns:\n    list[list[float]]: A list of principal components (the eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    principal_components = principal_components.tolist()\n    return principal_components\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nk = 2"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    This function implements the decision tree learning algorithm for classification.\n    \n    Args:\n    examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    attributes (list[str]): A list of attribute names.\n    target_attr (str): The target attribute name.\n    \n    Returns:\n    dict: A nested dictionary representing the decision tree.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n    best_attr = None\n    best_gain = 0\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    if best_gain == 0:\n        return Counter(target_values).most_common(1)[0][0]\n    tree = {best_attr: {}}\n    for value in set((example[best_attr] for example in examples)):\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n    return tree\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    This function calculates the information gain of an attribute.\n    \n    Args:\n    examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    attr (str): The attribute name.\n    target_attr (str): The target attribute name.\n    \n    Returns:\n    float: The information gain of the attribute.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    target_entropy = entropy(target_values)\n    attr_values = set((example[attr] for example in examples))\n    attr_entropy = 0\n    for value in attr_values:\n        subset = [example for example in examples if example[attr] == value]\n        subset_target_values = [example[target_attr] for example in subset]\n        subset_entropy = entropy(subset_target_values)\n        attr_entropy += len(subset) / len(examples) * subset_entropy\n    gain = target_entropy - attr_entropy\n    return gain\ndef entropy(values: list) -> float:\n    \"\"\"\n    This function calculates the entropy of a list of values.\n    \n    Args:\n    values (list): A list of values.\n    \n    Returns:\n    float: The entropy of the list of values.\n    \"\"\"\n    freqs = Counter(values)\n    entropy = 0\n    for freq in freqs.values():\n        prob = freq / len(values)\n        entropy -= prob * math.log2(prob)\n    return entropy\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch.\n\n    Parameters:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str): The choice of kernel, either 'linear' or 'RBF'. Default is 'linear'.\n    - lambda_val (float): The regularization parameter. Default is 0.01.\n    - iterations (int): The number of iterations. Default is 100.\n    - sigma (float): The kernel parameter for RBF kernel. Default is 1.0.\n\n    Returns:\n    - alpha (list): The model's alpha coefficients.\n    - bias (float): The model's bias.\n    \"\"\"\n    alpha = np.zeros(len(data))\n    bias = 0.0\n\n    def kernel_func(x, y):\n        if kernel == 'linear':\n            return np.dot(x, y)\n        elif kernel == 'RBF':\n            return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n    for _ in range(iterations):\n        for i in range(len(data)):\n            margin = np.sum([alpha[j] * labels[j] * kernel_func(data[j], data[i]) for j in range(len(data))]) + bias\n            if labels[i] * margin < 1:\n                alpha[i] = (1 - lambda_val) * alpha[i] + labels[i]\n                bias = (1 - lambda_val) * bias\n        alpha = np.clip(alpha, 0, 1 / lambda_val)\n    alpha = np.round(alpha, 4)\n    bias = round(bias, 4)\n    return (alpha.tolist(), bias)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    This function computes the output of the sigmoid activation function given an input value z.\n    \n    The sigmoid function is defined as 1 / (1 + exp(-z)), where exp is the exponential function.\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n    \n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    exp_neg_z = math.exp(-z)\n    sigmoid_z = 1 / (1 + exp_neg_z)\n    return round(sigmoid_z, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    This function computes the softmax activation for a given list of scores.\n    \n    Args:\n    scores (list[float]): A list of scores.\n    \n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [score / sum_exp_scores for score in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values\nscores = [1.0, 2.0, 3.0, 4.0, 5.0]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Sigmoid activation function.\n    \n    Args:\n    x (float): Input to the sigmoid function.\n    \n    Returns:\n    float: Output of the sigmoid function.\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): Associated true binary labels.\n    weights (list[float]): The neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n    \n    Returns:\n    tuple[list[float], float]: A tuple containing the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n    \"\"\"\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    weights_array = np.array(weights)\n    weighted_sum = np.dot(features_array, weights_array) + bias\n    predicted_probabilities = np.array([sigmoid(x) for x in weighted_sum])\n    predicted_probabilities = np.round(predicted_probabilities, 4)\n    mean_squared_error = np.mean((predicted_probabilities - labels_array) ** 2)\n    mean_squared_error = round(mean_squared_error, 4)\n    return (predicted_probabilities.tolist(), mean_squared_error)\nfeatures = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\nlabels = [0, 1, 1]\nweights = [0.5, 0.5]\nbias = 0.0"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of x.\n\n    Args:\n    x (np.ndarray): Input array.\n\n    Returns:\n    np.ndarray: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    features (np.ndarray): A 2D array of feature vectors.\n    labels (np.ndarray): A 1D array of true binary labels.\n    initial_weights (np.ndarray): A 1D array of initial weights.\n    initial_bias (float): The initial bias.\n    learning_rate (float): The learning rate for gradient descent.\n    epochs (int): The number of epochs.\n\n    Returns:\n    np.ndarray: The updated weights.\n    float: The updated bias.\n    list[float]: A list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_predictions = 2 * (predictions - labels)\n        d_sigmoid = predictions * (1 - predictions)\n        d_weights = np.dot(features.T, d_predictions * d_sigmoid)\n        d_bias = np.sum(d_predictions * d_sigmoid)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights, bias, np.array(mse_values).tolist())"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initialize a Value object.\n\n        Args:\n        - data (float): The value of the object.\n        - _children (tuple, optional): The child nodes of the object. Defaults to ().\n        - _op (str, optional): The operation that created the object. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        \"\"\"\n        Add two Value objects.\n\n        Args:\n        - other (Value): The object to add to the current object.\n\n        Returns:\n        - Value: A new Value object representing the sum of the two objects.\n        \"\"\"\n        if isinstance(other, Value):\n            out = Value(self.data + other.data, (self, other), '+')\n        else:\n            out = Value(self.data + other, (self,), '+')\n\n        def _backward():\n            self.grad += out.grad\n            if isinstance(other, Value):\n                other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        \"\"\"\n        Multiply two Value objects.\n\n        Args:\n        - other (Value): The object to multiply with the current object.\n\n        Returns:\n        - Value: A new Value object representing the product of the two objects.\n        \"\"\"\n        if isinstance(other, Value):\n            out = Value(self.data * other.data, (self, other), '*')\n        else:\n            out = Value(self.data * other, (self,), '*')\n\n        def _backward():\n            self.grad += out.grad * other.data if not isinstance(other, Value) else out.grad * other.data\n            if isinstance(other, Value):\n                other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        \"\"\"\n        Apply the ReLU activation function to the current object.\n\n        Returns:\n        - Value: A new Value object representing the ReLU of the current object.\n        \"\"\"\n        out = Value(max(0, self.data), (self,), 'relu')\n\n        def _backward():\n            self.grad += out.grad if self.data >= 0 else 0\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \"\"\"\n        Compute the gradients of the current object and its children.\n        \"\"\"\n        visited = set()\n\n        def _topological_sort(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    _topological_sort(child)\n                node._backward()\n        _topological_sort(self)"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    This function calculates the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list[list[int]]): The basis vectors in basis B.\n    C (list[list[int]]): The basis vectors in basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C.\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    B_inv = np.linalg.inv(B)\n    P = np.dot(C, B_inv)\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V.\n    \"\"\"\n    A_AT = np.dot(A, A.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_AT)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    U = eigenvectors\n    S = np.diag(singular_values)\n    AT_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(AT_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    V = eigenvectors\n    return ([np.round(U, 4).tolist()], [np.round(S, 4).tolist()], [np.round(V, 4).tolist()])\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, \n    while maintaining the corresponding order between them.\n\n    Args:\n        X (numpy array): The feature array.\n        y (numpy array): The target array.\n        seed (int, optional): The seed for the random shuffle. Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y arrays.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    combined = np.column_stack((X, y))\n    np.random.shuffle(combined)\n    (X_shuffled, y_shuffled) = (combined[:, :X.shape[1]], combined[:, X.shape[1]:])\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterable function that samples in a numpy array X and an optional numpy array y.\n    \n    Args:\n    X (numpy array): The input data.\n    y (numpy array, optional): The target data. Defaults to None.\n    batch_size (int, optional): The size of each batch. Defaults to 64.\n    \n    Yields:\n    list: Batches of X or (X, y) pairs.\n    \"\"\"\n    num_batches = int(np.ceil(len(X) / batch_size))\n    for i in range(num_batches):\n        start = i * batch_size\n        end = min((i + 1) * batch_size, len(X))\n        X_batch = X[start:end]\n        if y is not None:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n        X (numpy array): Dataset to be divided.\n        feature_i (int): Index of the feature to divide on.\n        threshold (float): Threshold value to divide on.\n\n    Returns:\n        list: Two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n    \"\"\"\n    mask = X[:, feature_i] >= threshold\n    subset_meet_condition = X[mask]\n    subset_dont_meet_condition = X[~mask]\n    subset_meet_condition = subset_meet_condition.tolist()\n    subset_dont_meet_condition = subset_dont_meet_condition.tolist()\n    return [subset_meet_condition, subset_dont_meet_condition]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n    list: A new 2D numpy array with polynomial features up to the specified degree.\n    \"\"\"\n    result = [X]\n    for d in range(2, degree + 1):\n        combos = list(combinations_with_replacement(range(X.shape[1]), d))\n        for combo in combos:\n            poly_feature = np.prod(X[:, combo], axis=1, keepdims=True)\n            result.append(poly_feature)\n    result = np.hstack(result)\n    return result.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Args:\n    - X (2D numpy array): The feature dataset.\n    - y (1D numpy array): The target dataset.\n    - n_subsets (int): The number of subsets to generate.\n    - replacements (bool, optional): Whether to create subsets with replacements. Defaults to True.\n    - seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    - list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [subset for subset in subsets]"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    x (1D numpy array): Input array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    list: One-hot encoded array as a list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    encoded = np.zeros((len(x), n_col))\n    for (i, val) in enumerate(x):\n        encoded[i, val] = 1\n    return encoded.tolist()\nx = np.array([0, 1, 2, 0, 1, 2])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    This function converts a 1D numpy array into a diagonal matrix.\n\n    Args:\n        x (numpy array): A 1D numpy array.\n\n    Returns:\n        list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError('Input must be a numpy array')\n    if len(x.shape) != 1:\n        raise ValueError('Input array must be 1D')\n    diagonal_matrix = np.diag(x)\n    result = diagonal_matrix.tolist()\n    return result\nx = np.array([1, 2, 3])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (1D numpy array): The true labels.\n    y_pred (1D numpy array): The predicted labels.\n\n    Returns:\n    float: The accuracy score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return accuracy\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 6])"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the function calculates the correlation matrix of X with itself. Defaults to None.\n\n    Returns:\n    list: The correlation matrix as a 2D list, rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    correlation_matrix = correlation_matrix.tolist()\n    return correlation_matrix"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    This function computes the log-softmax of a given 1D numpy array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list of log-softmax values.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)\n    softmax = exp_scores / np.sum(exp_scores)\n    log_softmax = np.log(softmax)\n    return np.round(log_softmax, 4).tolist()\nscores = [1.0, 2.0, 3.0, 4.0, 5.0]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, W_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, w0_grad)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    This function performs a 2D convolution operation on the input matrix using the specified kernel, padding, and stride.\n\n    Args:\n    input_matrix (np.ndarray): The input matrix to be convolved.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The amount of padding to be applied to the input matrix.\n    stride (int): The stride of the convolution operation.\n\n    Returns:\n    list: A list of convolved values, rounded to the nearest 4th decimal.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    padded_input_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    for i in range(output_height):\n        for j in range(output_width):\n            window = padded_input_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(window * kernel)\n    return np.round(output_matrix, 4).tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    This function implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Args:\n        z (float): The input value to be processed by the ReLU function.\n    \n    Returns:\n        float: The value after applying the ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    This function calculates the Ridge Regression loss function.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the feature matrix.\n    w (np.ndarray): A 1D numpy array representing the coefficients.\n    y_true (np.ndarray): A 1D numpy array representing the true labels.\n    alpha (float): A float representing the regularization parameter.\n\n    Returns:\n    float: The Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    This function implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Args:\n        z (float): The input value to the function.\n        alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n        float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Parameters:\n    x1 (numpy array): The first input vector.\n    x2 (numpy array): The second input vector.\n\n    Returns:\n    float: The linear kernel value between x1 and x2.\n    \"\"\"\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError('Input vectors must be numpy arrays')\n    if len(x1.shape) != 1 or len(x2.shape) != 1:\n        raise ValueError('Input vectors must be 1D arrays')\n    if x1.shape[0] != x2.shape[0]:\n        raise ValueError('Input vectors must have the same dimension')\n    kernel_value = np.dot(x1, x2)\n    return kernel_value"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given two numpy arrays: y_true and y_pred.\n\n    Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The precision metric.\n    \"\"\"\n    assert y_true.shape == y_pred.shape, 'y_true and y_pred must have the same shape'\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp + fp), where=tp + fp != 0)\n    return precision\ny_true = np.array([0, 1, 1, 0, 1, 0])\ny_pred = np.array([0, 1, 0, 0, 1, 1])"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    This function performs three variants of gradient descent: Stochastic Gradient Descent (SGD), \n    Batch Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target vector\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int): Batch size for mini-batch gradient descent (default=1)\n    method (str): Variant of gradient descent to use (default='batch')\n\n    Returns:\n    list: Weights after gradient descent\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    weights = np.array(weights)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 / len(y) * np.dot(X.T, errors)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for i in range(len(y)):\n                prediction = np.dot(X[i], weights)\n                error = prediction - y[i]\n                gradient = 2 * X[i] * error\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            for i in range(0, len(y), batch_size):\n                batch_X = X[i:i + batch_size]\n                batch_y = y[i:i + batch_size]\n                predictions = np.dot(batch_X, weights)\n                errors = predictions - batch_y\n                gradient = 2 / len(batch_y) * np.dot(batch_X.T, errors)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose from 'batch', 'stochastic', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 5]\nweights = [0, 0]\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 1\nmethod = 'batch'"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n    matrix (numpy array): The input matrix.\n\n    Returns:\n    list: The RREF of the input matrix as a Python list.\n    \"\"\"\n    (num_rows, num_cols) = matrix.shape\n    current_row = 0\n    for j in range(num_cols):\n        if current_row >= num_rows:\n            break\n        pivot_row = current_row\n        while pivot_row < num_rows and matrix[pivot_row, j] == 0:\n            pivot_row += 1\n        if pivot_row == num_rows:\n            continue\n        matrix[[current_row, pivot_row]] = matrix[[pivot_row, current_row]]\n        pivot = matrix[current_row, j]\n        matrix[current_row] = matrix[current_row] / pivot\n        for i in range(num_rows):\n            if i != current_row:\n                factor = matrix[i, j]\n                matrix[i] = matrix[i] - factor * matrix[current_row]\n        current_row += 1\n    return matrix.tolist()\nmatrix = np.array([[2, 1, -1, 8], [-3, -1, 2, -11], [-2, 1, 2, -3]], dtype=float)"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Adam optimization algorithm implementation.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of `f`.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 10.\n\n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n    x = x0.copy()\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** (t + 1))\n        v_hat = v / (1 - beta2 ** (t + 1))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n        t += 1\n    return np.round(x, 4).tolist()\ndef grad(x):\n    return 2 * x"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    This function implements Lasso Regression using Gradient Descent.\n    \n    Parameters:\n    X (np.array): The feature matrix.\n    y (np.array): The target variable.\n    alpha (float): The regularization parameter. Default is 0.1.\n    learning_rate (float): The learning rate for gradient descent. Default is 0.01.\n    max_iter (int): The maximum number of iterations. Default is 1000.\n    tol (float): The tolerance for convergence. Default is 1e-4.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_weights = np.zeros(n_features)\n    prev_bias = 0\n    for _ in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        loss = np.mean((predictions - y) ** 2)\n        weights_grad = 2 / n_samples * np.dot(X.T, predictions - y) + alpha * np.sign(weights)\n        bias_grad = 2 / n_samples * np.sum(predictions - y)\n        weights -= learning_rate * weights_grad\n        bias -= learning_rate * bias_grad\n        if np.allclose(weights, prev_weights, atol=tol) and np.isclose(bias, prev_bias, atol=tol):\n            break\n        prev_weights = weights.copy()\n        prev_bias = bias\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights.tolist(), [bias])"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two given strings.\n\n    The OSA distance represents the minimum number of edits required to transform one string into another.\n    The allowed edit operations are:\n    - Insert a character\n    - Delete a character\n    - Substitute a character\n    - Transpose two adjacent characters\n    Each of these operations costs 1 unit.\n\n    Args:\n        source (str): The source string.\n        target (str): The target string.\n\n    Returns:\n        int: The minimum number of edits needed to convert the source string into the target string.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i, j] = dp[i - 1, j - 1]\n            else:\n                substitution = dp[i - 1, j - 1] + 1\n                insertion = dp[i, j - 1] + 1\n                deletion = dp[i - 1, j] + 1\n                transposition = float('inf')\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    transposition = dp[i - 2, j - 2] + 1\n                dp[i, j] = min(substitution, insertion, deletion, transposition)\n    return dp[m, n]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in a binary classification setting.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    recall_value = tp / denominator\n    return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 1]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    This function implements the self-attention mechanism, a fundamental component of transformer models.\n    \n    Parameters:\n    Q (numpy array): The query matrix.\n    K (numpy array): The key matrix.\n    V (numpy array): The value matrix.\n    \n    Returns:\n    list: The self-attention output as a python list, rounded to the nearest 4th decimal.\n    \"\"\"\n    dot_product = np.dot(Q, K.T)\n    scaling_factor = np.sqrt(K.shape[1])\n    scaled_dot_product = dot_product / scaling_factor\n    softmax_output = np.softmax(scaled_dot_product, axis=-1)\n    output = np.dot(softmax_output, V)\n    return np.round(output, 4).tolist()\nQ = np.array([[1, 2, 3], [4, 5, 6]])\nV = np.array([[13, 14, 15], [16, 17, 18]])"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Parameters:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state of the RNN.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for input_vector in input_sequence:\n        hidden_state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n    return np.round(hidden_state, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n    points_homogeneous = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    translated_points = np.dot(points_homogeneous, translation_matrix.T)[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): The mean of the first normal distribution.\n    sigma_p (float): The standard deviation of the first normal distribution.\n    mu_q (float): The mean of the second normal distribution.\n    sigma_q (float): The standard deviation of the second normal distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 2.0\nkl_divergence = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Gauss-Seidel Method for Solving Linear Systems.\n\n    Parameters:\n    A (numpy array): A square matrix of coefficients.\n    b (numpy array): The right-hand side vector.\n    n (int): The number of iterations.\n    x_ini (numpy array, optional): An initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    list: The approximated solution vector after performing the specified number of iterations.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert len(b.shape) == 1, 'Vector b must be a 1D array'\n    assert isinstance(n, int) and n > 0, 'Number of iterations must be a positive integer'\n    size = A.shape[0]\n    assert b.shape[0] == size, 'Size of vector b must match the size of matrix A'\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        assert x_ini.shape[0] == size, 'Size of initial guess x_ini must match the size of matrix A'\n        x = x_ini.copy()\n    for _ in range(n):\n        x_new = x.copy()\n        for i in range(size):\n            sum_before = np.sum(A[i, :i] * x_new[:i])\n            sum_after = np.sum(A[i, i + 1:] * x[i + 1:])\n            x_new[i] = (b[i] - sum_before - sum_after) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()\nA = np.array([[3, 2], [1, -1]])\nb = np.array([1, 2])\nn = 10\nx_ini = np.array([0, 0])"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    This function performs Gaussian Elimination with partial pivoting to solve the system (Ax = b).\n    \n    Parameters:\n    A (numpy array): The coefficient matrix of the system.\n    b (numpy array): The constant vector of the system.\n    \n    Returns:\n    x (list): The solution vector of the system, rounded to the nearest 4th decimal.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    augmented_matrix = np.column_stack((A, b))\n    for i in range(n):\n        max_row = np.argmax(np.abs(augmented_matrix[i:, i])) + i\n        augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]\n        pivot = augmented_matrix[i, i]\n        augmented_matrix[i] = augmented_matrix[i] / pivot\n        for j in range(i + 1, n):\n            factor = augmented_matrix[j, i]\n            augmented_matrix[j] = augmented_matrix[j] - factor * augmented_matrix[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = augmented_matrix[i, -1]\n        for j in range(i + 1, n):\n            x[i] = x[i] - augmented_matrix[i, j] * x[j]\n    return np.round(x, 4).tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = np.zeros((x.shape[0], self.hidden_size))\n        cell_states = np.zeros((x.shape[0], self.hidden_size))\n        hidden_states[0] = initial_hidden_state.reshape(-1)\n        cell_states[0] = initial_cell_state.reshape(-1)\n        for t in range(1, x.shape[0]):\n            concat_input = np.concatenate((x[t - 1].reshape(-1, 1), hidden_states[t - 1].reshape(-1, 1)), axis=0)\n            forget_gate = self.sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n            input_gate = self.sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, concat_input) + self.bc)\n            output_gate = self.sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n            cell_states[t] = forget_gate * cell_states[t - 1] + input_gate * candidate_cell_state\n            hidden_states[t] = output_gate * self.tanh(cell_states[t])\n        return (np.round(hidden_states, 4).tolist(), np.round(hidden_states[-1], 4).tolist(), np.round(cell_states[-1], 4).tolist())"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty')\n    num_docs = len(corpus)\n    doc_freq = {}\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            if term in doc_freq:\n                doc_freq[term] += 1\n            else:\n                doc_freq[term] = 1\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            tf = doc.count(term) / len(doc) if doc else 0\n            idf = np.log((num_docs + 1) / (doc_freq.get(term, 0) + 1)) if doc_freq.get(term, 0) != 0 else 0\n            tf_idf = tf * idf\n            doc_scores.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_scores)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'sun', 'is', 'shining', 'brightly', 'in', 'the', 'clear', 'sky'], ['the', 'cat', 'is', 'sleeping', 'peacefully', 'on', 'the', 'soft', 'cushion']]\nquery = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'sun', 'shining', 'cat', 'sleeping']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall != 0 else 0\n    return round(f_score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert len(b.shape) == 1, 'Vector b must be a 1D array'\n    assert n > 0, 'Number of iterations must be positive'\n    assert tol > 0, 'Tolerance must be positive'\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    x = np.round(x, 8).tolist()\n    return x"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    total_samples = len(y)\n    class_counts = {}\n    for label in y:\n        if label in class_counts:\n            class_counts[label] += 1\n        else:\n            class_counts[label] = 1\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, column_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    magnitude_squared = np.dot(L, L)\n    projection = dot_product / magnitude_squared * L\n    projection = np.round(projection, 3)\n    return projection.tolist()\nv = [1, 2, 3]\nL = [4, 5, 6]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointers = [0]\n    for col_idx in range(len(dense_matrix[0])):\n        non_zero_count = 0\n        for row_idx in range(len(dense_matrix)):\n            if dense_matrix[row_idx][col_idx] != 0:\n                values.append(dense_matrix[row_idx][col_idx])\n                row_indices.append(row_idx)\n                non_zero_count += 1\n        column_pointers.append(column_pointers[-1] + non_zero_count)\n    return (values, row_indices, column_pointers)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a given matrix A.\n\n    Parameters:\n    A (numpy array): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (n, m) = A.shape\n    lead = 0\n    for r in range(n):\n        if lead >= m:\n            return []\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == n:\n                i = r\n                lead += 1\n                if m == lead:\n                    return []\n        A[[i, r]] = A[[r, i]]\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        for i in range(r + 1, n):\n            lv = A[i, lead]\n            A[i] -= lv * A[r]\n        lead += 1\n    independent_columns = []\n    for j in range(m):\n        if np.any(A[:, j] != 0):\n            independent_columns.append(j)\n    basis_vectors = A[:, independent_columns]\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n    return basis_vectors\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 10]]"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array): Array of true values.\n    y_pred (array): Array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    mean_y_true = np.mean(y_true)\n    sst = np.sum((y_true - mean_y_true) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - sse / sst\n    return round(r2, 3)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.1])"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n    img (list): A 2D matrix representing the image, where each element is a pixel value between 0 (black) and 255 (white).\n\n    Returns:\n    float: The average brightness of the image rounded to two decimal places. Returns -1 for edge cases.\n    \"\"\"\n    if not img:\n        return -1\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Args:\n        y_true (numpy array): The actual values.\n        y_pred (numpy array): The predicted values.\n\n    Returns:\n        float: The RMSE value rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input arrays have mismatched shapes or are empty.\n        TypeError: If the input arrays are not of type numpy array.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Both inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    diff = y_true - y_pred\n    squared_diff = diff ** 2\n    mean_squared_diff = np.mean(squared_diff)\n    rmse_value = np.sqrt(mean_squared_diff)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    The Jaccard Index is a measure of similarity between two sets, and is defined as the size of the intersection divided by the size of the union of the sets.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('The input arrays must have the same length.')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1])"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score, also known as the S\u00f8rensen-Dice coefficient or F1-score, \n    for binary classification.\n\n    Args:\n        y_true (numpy array): Binary array of true labels.\n        y_pred (numpy array): Binary array of predicted labels.\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Both y_true and y_pred must be binary arrays')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Both y_true and y_pred must have the same length')\n    true_positives = np.sum(np.logical_and(y_true, y_pred))\n    num_true_positives = np.sum(y_true)\n    num_predicted_positives = np.sum(y_pred)\n    if num_true_positives == 0 and num_predicted_positives == 0:\n        return 1.0\n    elif num_true_positives == 0 or num_predicted_positives == 0:\n        return 0.0\n    dice = 2 * true_positives / (num_true_positives + num_predicted_positives)\n    return round(dice, 3)\ny_true = np.array([1, 1, 0, 0, 1, 0, 1, 1])\ny_pred = np.array([1, 1, 0, 0, 1, 1, 0, 1])"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    feature_hvs = []\n    for (feature, value) in row.items():\n        np.random.seed(random_seeds[feature])\n        feature_name_hv = np.random.choice([-1, 1], size=dim)\n        np.random.seed(random_seeds[feature])\n        feature_value_hv = np.random.choice([-1, 1], size=dim)\n        feature_hv = np.multiply(feature_name_hv, feature_value_hv)\n        feature_hvs.append(feature_hv)\n    composite_hv = np.sum(feature_hvs, axis=0)\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n    - data (list of lists): A list of lists, where each inner list represents a pair\n        [y_true, y_pred] for one observation. y_true is the actual label, and y_pred\n        is the predicted label.\n\n    Returns:\n    - A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    tp = 0\n    fp = 0\n    tn = 0\n    fn = 0\n    for (y_true, y_pred) in data:\n        if y_true == y_pred:\n            if y_true == 1:\n                tp += 1\n            else:\n                tn += 1\n        elif y_true == 1:\n            fn += 1\n        else:\n            fp += 1\n    matrix = [[tn, fp], [fn, tp]]\n    return matrix\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    v1 (numpy array): The first input vector.\n    v2 (numpy array): The second input vector.\n\n    Returns:\n    float: The cosine similarity between the two vectors, rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input vectors do not have the same shape or if either vector is empty or has zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if len(v1) == 0 or len(v2) == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Tuple\ndef performance_metrics(actual: list[int], predicted: list[int]) -> Tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Args:\n    actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    if not all((i in [0, 1] for i in actual)) or not all((i in [0, 1] for i in predicted)):\n        raise ValueError('All elements in the actual and predicted lists must be either 0 or 1')\n    if len(actual) != len(predicted):\n        raise ValueError('Both lists must have the same length')\n    confusion_matrix = [[0, 0], [0, 0]]\n    for (a, p) in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n    accuracy = (confusion_matrix[0][0] + confusion_matrix[1][1]) / len(actual)\n    precision = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[0][1]) if confusion_matrix[1][1] + confusion_matrix[0][1] != 0 else 0\n    recall = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[1][0]) if confusion_matrix[1][1] + confusion_matrix[1][0] != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    specificity = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[0][1]) if confusion_matrix[0][0] + confusion_matrix[0][1] != 0 else 0\n    negative_predictive_value = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[1][0]) if confusion_matrix[0][0] + confusion_matrix[1][0] != 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))\nactual = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\npredicted = [1, 1, 0, 1, 0, 0, 1, 0, 0, 1]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Args:\n    data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation, \n          percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.array(data)\n    mean = round(np.mean(data), 4)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = round(np.var(data), 4)\n    standard_deviation = round(np.std(data), 4)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    stats_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\n    return stats_dict\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coefficient = math.comb(n, k)\n    probability = binomial_coefficient * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 5\np = 0.5\nprobability = binomial_probability(n, k, p)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    numerator = -(x - mean) ** 2\n    denominator = 2 * std_dev ** 2\n    exponential_term = math.exp(numerator / denominator)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * exponential_term\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('Input image must be a numpy array')\n    if len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D array')\n    if np.min(img) < 0 or np.max(img) > 255:\n        raise ValueError('Pixel values must be between 0 and 255')\n    min_pixel_value = np.min(img)\n    max_pixel_value = np.max(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two input vectors.\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Input vectors must be of the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_features = []\n    for x in data:\n        polynomial_features = []\n        for i in range(degree + 1):\n            feature = x ** i\n            polynomial_features.append(round(feature, 8))\n        transformed_features.append(polynomial_features)\n    return transformed_features\ndata = [1.0, 2.0, 3.0]\ndegree = 2"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    This function calculates positional encodings for a sequence length (`position`) \n    and model dimensionality (`d_model`) using sine and cosine functions as specified \n    in the Transformer architecture.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings. Returns -1 if `position` is 0, or if `d_model` is less than or equal to 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            frequency = 1 / 10000 ** (i / d_model)\n            if i % 2 == 0:\n                encoding[pos, i] = np.sin(pos * frequency)\n            else:\n                encoding[pos, i] = np.cos(pos * frequency)\n    return encoding.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generates text based on a given prompt using a simplified GPT-2-like architecture.\n\n    Args:\n    - prompt (str): The initial text to guide the generation process.\n    - n_tokens_to_generate (int): The number of tokens to output.\n\n    Returns:\n    - generated_text (str): The generated text.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_tokens = encoder.encode(prompt)\n    prompt_len = len(prompt_tokens)\n    output_tokens = prompt_tokens[:]\n    for i in range(n_tokens_to_generate):\n        ctx_tokens = output_tokens[-hparams['n_ctx']:]\n        token_embeddings = np.array([params['wte'][token] for token in ctx_tokens])\n        positional_embeddings = np.array([params['wpe'][j] for j in range(len(ctx_tokens))])\n        embeddings = token_embeddings + positional_embeddings\n        attention_outputs = np.zeros((len(ctx_tokens), embeddings.shape[1]))\n        for head in range(hparams['n_head']):\n            q = embeddings\n            k = embeddings\n            v = embeddings\n            attention_scores = np.dot(q, k.T) / np.sqrt(embeddings.shape[1])\n            attention_weights = np.softmax(attention_scores, axis=-1)\n            attention_output = np.dot(attention_weights, v)\n            attention_outputs += attention_output\n        ff_output = np.maximum(attention_outputs, 0)\n        ln_output = (ff_output - np.mean(ff_output, axis=-1, keepdims=True)) / np.std(ff_output, axis=-1, keepdims=True)\n        ln_output = ln_output * params['ln_f']['g'] + params['ln_f']['b']\n        last_token_embedding = ln_output[-1]\n        probs = np.dot(last_token_embedding, params['wte'].T)\n        next_token = np.argmax(probs)\n        output_tokens.append(next_token)\n    generated_text = encoder.decode(output_tokens)\n    return generated_text\nprompt = 'hello world'\nn_tokens_to_generate = 40\ngenerated_text = gen_text(prompt, n_tokens_to_generate)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function implements a simplified self-attention mechanism to calculate the enhanced patterns of crystals.\n    \n    Parameters:\n    n (int): The number of crystals.\n    crystal_values (list): A list of unique numeric values for each crystal.\n    dimension (int): The dimension of the crystal values.\n    \n    Returns:\n    list: A list of floats representing the final weighted patterns for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function implements the softmax function.\n        \n        Parameters:\n        values (list): A list of values.\n        \n        Returns:\n        list: A list of softmax values.\n        \"\"\"\n        exp_values = np.exp(values)\n        sum_exp_values = np.sum(exp_values)\n        softmax_values = exp_values / sum_exp_values\n        return softmax_values\n    attention_scores = np.zeros((n, n))\n    weighted_patterns = np.zeros((n, dimension))\n    for i in range(n):\n        for j in range(n):\n            dot_product = np.dot(crystal_values[i], crystal_values[j])\n            attention_scores[i, j] = dot_product\n    for i in range(n):\n        attention_scores[i] = softmax(attention_scores[i])\n    for i in range(n):\n        for j in range(n):\n            weighted_patterns[i] += attention_scores[i, j] * crystal_values[j]\n    return [round(np.sum(pattern), 4) for pattern in weighted_patterns]\nn = 3\ncrystal_values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query in a corpus.\n\n    Parameters:\n    - corpus (list of lists): A list of documents, where each document is a list of words.\n    - query (list): A list of query words.\n    - k1 (float): A hyperparameter that controls the effect of term frequency saturation. Default is 1.5.\n    - b (float): A hyperparameter that controls the effect of document length normalization. Default is 0.75.\n\n    Returns:\n    - scores (list): A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    scores = []\n    for doc in corpus:\n        doc_len = len(doc)\n        tf = Counter(doc)\n        score = 0\n        for word in query:\n            if word in tf:\n                weight = tf[word] * (k1 + 1) / (tf[word] + k1 * (1 - b + b * doc_len / avg_doc_len))\n                score += weight\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'sun', 'is', 'shining', 'brightly', 'in', 'the', 'clear', 'blue', 'sky'], ['the', 'cat', 'is', 'sleeping', 'on', 'the', 'soft', 'cushion']]\nquery = ['the', 'quick', 'fox']"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == pred:\n            tp += 1\n        elif true != pred:\n            if true == 1:\n                fn += 1\n            elif true == 0:\n                fp += 1\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    This function forecasts the power consumption for day 15, taking into account \n    a growing linear trend and a daily fluctuation. It removes the fluctuation, \n    fits a linear regression model, predicts day 15's base consumption, adds back \n    the fluctuation, and includes a 5% safety margin.\n\n    Args:\n        consumption_data (list): A list of 10 daily power usage measurements.\n\n    Returns:\n        int: The forecasted power consumption for day 15 with a 5% safety margin.\n    \"\"\"\n    detrended_data = [data - 10 * math.sin(2 * PI * (i + 1) / 10) for (i, data) in enumerate(consumption_data)]\n    n = len(detrended_data)\n    x = list(range(1, n + 1))\n    sum_x = sum(x)\n    sum_y = sum(detrended_data)\n    sum_xy = sum((i * j for (i, j) in zip(x, detrended_data)))\n    sum_x_squared = sum((i ** 2 for i in x))\n    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)\n    intercept = (sum_y - slope * sum_x) / n\n    day_15_base_consumption = slope * 15 + intercept\n    day_15_consumption = day_15_base_consumption + 10 * math.sin(2 * PI * 15 / 10)\n    import math\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [120, 115, 130, 125, 140, 135, 150, 145, 160, 155]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Both arrays should have the same shape')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    mean_absolute_error = round(mean_absolute_error, 3)\n    return mean_absolute_error\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.1])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute self-attention by calculating attention weights and applying them to V.\n\n    Args:\n    Q (np.ndarray): Query matrix\n    K (np.ndarray): Key matrix\n    V (np.ndarray): Value matrix\n\n    Returns:\n    attention_output (np.ndarray): Output of self-attention\n    \"\"\"\n    attention_weights = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    attention_weights = np.exp(attention_weights) / np.sum(np.exp(attention_weights), axis=-1, keepdims=True)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention by splitting Q, K, and V into multiple heads, applying self-attention to each head, and concatenating the outputs.\n\n    Args:\n    Q (np.ndarray): Query matrix\n    K (np.ndarray): Key matrix\n    V (np.ndarray): Value matrix\n    n_heads (int): Number of heads\n\n    Returns:\n    multi_head_output (np.ndarray): Output of multi-head attention\n    \"\"\"\n    head_size = Q.shape[1] // n_heads\n    Q_heads = np.split(Q, n_heads, axis=1)\n    K_heads = np.split(K, n_heads, axis=1)\n    V_heads = np.split(V, n_heads, axis=1)\n    attention_outputs = []\n    for i in range(n_heads):\n        attention_output = self_attention(Q_heads[i], K_heads[i], V_heads[i])\n        attention_outputs.append(attention_output)\n    multi_head_output = np.concatenate(attention_outputs, axis=1)\n    return multi_head_output\nn_heads = 4"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n11 = sum((1 for (i, j) in zip(x, y) if i == 1 and j == 1))\n    n10 = sum((1 for (i, j) in zip(x, y) if i == 1 and j == 0))\n    n01 = sum((1 for (i, j) in zip(x, y) if i == 0 and j == 1))\n    n00 = sum((1 for (i, j) in zip(x, y) if i == 0 and j == 0))\n    n = len(x)\n    phi = (n11 * n00 - n10 * n01) / ((n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -100:\n        return 0.0\n    if x > 100:\n        return x\n    return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.minimum(np.maximum(rhos, 1 - epsilon), 1 + epsilon)\n    L_CLIP = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    D_KL = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    L_GRPO = L_CLIP - beta * D_KL\n    L_GRPO = round(L_GRPO, 6)\n    return L_GRPO\nrhos = np.array([0.5, 0.7, 0.9])\nA = np.array([1.0, 2.0, 3.0])\npi_theta_old = np.array([0.2, 0.3, 0.5])\npi_theta_ref = np.array([0.1, 0.4, 0.5])"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    swish_value = round(swish_value, 4)\n    return swish_value"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * (alpha * (math.exp(x) - 1))"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    weighted_sum = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-weighted_sum))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n    X (np.ndarray): Input features\n    y (np.ndarray): Target labels\n    learning_rate (float): Learning rate for gradient descent\n    iterations (int): Number of iterations for training\n\n    Returns:\n        B (list[float]): Updated parameter vector rounded to 4 floating points\n        losses (list[float]): Collected values of Cross Entropy loss rounded to 4 floating points\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_classes = np.max(y) + 1\n    B = np.zeros((num_features, num_classes))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.mean(np.log(probabilities[np.arange(num_samples), y]))\n        losses.append(round(loss, 4))\n        dscores = probabilities\n        dscores[np.arange(num_samples), y] -= 1\n        dB = np.dot(X.T, dscores) / num_samples\n        B -= learning_rate * dB\n    B = np.round(B, 4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid activation function.\n    \n    Args:\n    x (np.ndarray): Input array.\n    \n    Returns:\n    np.ndarray: Sigmoid of input array.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Binary Cross Entropy loss function.\n    \n    Args:\n    y_true (np.ndarray): True labels.\n    y_pred (np.ndarray): Predicted probabilities.\n    \n    Returns:\n    float: Binary Cross Entropy loss.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], np.ndarray]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations.\n    \n    Returns:\n    tuple[list[float], np.ndarray]: A tuple containing the list of loss values at each iteration and the optimized coefficients.\n    \"\"\"\n    coefficients = np.zeros(X.shape[1])\n    loss_values = []\n    for _ in range(iterations):\n        probabilities = sigmoid(np.dot(X, coefficients))\n        loss = binary_cross_entropy(y, probabilities)\n        loss_values.append(round(loss, 4))\n        gradient = np.dot(X.T, probabilities - y) / X.shape[0]\n        coefficients -= learning_rate * gradient\n    return (loss_values, coefficients)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    attention_scores = attention_scores + (1 - mask) * -1000000000.0\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is calculated as the ratio of the number of unique colors to the total number of apples.\n    This measure satisfies the given properties:\n    - If all apples are the same color, the disorder is 0.\n    - As the variety of colors increases, the disorder increases.\n    \n    Args:\n        apples (list): A list of integers representing the colors of the apples.\n    \n    Returns:\n        float: A measure of disorder in the basket of apples, rounded to the nearest 4th decimal.\n    \"\"\"\n    total_apples = len(apples)\n    if total_apples == 0:\n        return 0.0\n    unique_colors = len(set(apples))\n    disorder_measure = unique_colors / total_apples\n    return round(disorder_measure, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    This function performs Layer Normalization on an input tensor.\n\n    Parameters:\n    X (np.ndarray): A 3D array representing batch size, sequence length, and feature dimensions.\n    gamma (np.ndarray): Scaling parameters.\n    beta (np.ndarray): Shifting parameters.\n    epsilon (float): A small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: The normalized X, rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    std = np.sqrt(variance + epsilon)\n    normalized_X = (X - mean) / std\n    normalized_X = gamma * normalized_X + beta\n    result = np.round(normalized_X, 5).tolist()\n    return result"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float, optional): The weight for precision. Defaults to 0.9.\n        beta (float, optional): The penalty factor for word order fragmentation. Defaults to 3.\n        gamma (float, optional): The weight for recall. Defaults to 0.5.\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    reference_words = reference.split()\n    candidate_words = candidate.split()\n    matches = sum((Counter(reference_words) & Counter(candidate_words)).values())\n    precision = matches / len(candidate_words) if candidate_words else 0\n    recall = matches / len(reference_words) if reference_words else 0\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall) if alpha * precision + (1 - alpha) * recall != 0 else 0\n    penalty = gamma * (len(candidate_words) - matches) / len(candidate_words) if candidate_words else 0\n    score = f_mean * (1 - penalty)\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, \n    their individual counts, and the total number of samples.\n\n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The individual count of event X.\n    total_counts_y (int): The individual count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information (PMI) rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_counts = total_counts_x / total_samples * (total_counts_y / total_samples) * total_samples\n    if expected_joint_counts == 0:\n        return 0.0\n    pmi = np.log2(joint_counts / expected_joint_counts)\n    return round(pmi, 3)\njoint_counts = 100\ntotal_counts_x = 500\ntotal_counts_y = 600\ntotal_samples = 10000\npmi = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of floats representing the normalized values, \n        rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if max_val == min_val:\n        return [0.0] * len(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    normalized = [round(val, 4) for val in normalized]\n    return normalized"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): The 1D input array.\n    w1 (np.ndarray): The weights for the first layer.\n    w2 (np.ndarray): The weights for the second layer.\n    \n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    out1 = np.matmul(x, w1)\n    out1 = np.maximum(out1, 0)\n    out2 = np.matmul(out1, w2)\n    out2 = np.maximum(out2, 0)\n    out2 = out2 + x\n    out2 = np.maximum(out2, 0)\n    out2 = np.round(out2, 4).tolist()\n    return out2\nx = np.array([1, 2, 3])\nw1 = np.array([[1, 2], [3, 4], [5, 6]])\nw2 = np.array([[7, 8, 9], [10, 11, 12]])"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This function performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Args:\n        x (np.ndarray): A 3D NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n        np.ndarray: A 1D NumPy array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    if len(x.shape) != 3:\n        raise ValueError('Input must be a 3D NumPy array')\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    This function performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n    \n    Parameters:\n    X (np.ndarray): A 4D NumPy array representing a batch of feature maps in the BCHW format.\n    gamma (np.ndarray): A 1D NumPy array representing the scale parameters for each channel.\n    beta (np.ndarray): A 1D NumPy array representing the shift parameters for each channel.\n    epsilon (float): A small value added to the variance for numerical stability. Default is 1e-5.\n    \n    Returns:\n    list: A list of the normalized feature maps, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * normalized_X + beta\n    output = np.round(output, 4).tolist()\n    return output"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at the given point, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative = c * n * x ** (n - 1)\n    derivative = round(derivative, 4)\n    return derivative"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): A tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    vectors = np.array(vectors)\n    basis = []\n    for vector in vectors:\n        is_independent = True\n        for basis_vector in basis:\n            projection = np.dot(vector, basis_vector)\n            if np.abs(projection - np.linalg.norm(vector)) < tol:\n                is_independent = False\n                break\n        if is_independent:\n            for basis_vector in basis:\n                projection = np.dot(vector, basis_vector)\n                vector -= projection * basis_vector\n            vector = vector / np.linalg.norm(vector)\n            basis.append(vector)\n    basis = [vector.round(4).tolist() for vector in basis]\n    return basis\nvectors = [[1, 0], [0, 1], [1, 1]]\ntol = 1e-10"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or numpy.ndarray): The first 3D vector.\n        b (list or numpy.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if len(a) != 3 or len(b) != 3:\n        raise ValueError('Both inputs must be 3D vectors')\n    cross_product_result = np.cross(a, b)\n    result = np.round(cross_product_result, 4).tolist()\n    return result"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n    A (numpy array): A square coefficient matrix.\n    b (numpy array): A constant vector.\n\n    Returns:\n    list: The solution vector x if the system has a unique solution, -1 otherwise.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(len(A)):\n        A_copy = A.copy()\n        A_copy[:, i] = b\n        det_A_copy = np.linalg.det(A_copy)\n        x_i = det_A_copy / det_A\n        x.append(x_i)\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Args:\n    p (list[float]): The first discrete probability distribution.\n    q (list[float]): The second discrete probability distribution.\n\n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    if np.any(p < 0) or np.any(q < 0) or (not np.isclose(np.sum(p), 1)) or (not np.isclose(np.sum(q), 1)):\n        return 0.0\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    This function computes the element-wise sum of two vectors.\n    \n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n    \n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, \n                         or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list:\n    \"\"\"\n    Compute the average policy gradient using the REINFORCE algorithm.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape (num_states, num_actions) representing the policy parameters.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list: The average policy gradient as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n    sum_gradients = np.zeros_like(theta)\n    for episode in episodes:\n        episode_return = 0\n        for t in range(len(episode) - 1, -1, -1):\n            (state, action, reward) = episode[t]\n            episode_return = reward + episode_return\n            policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            log_policy = np.log(policy[action])\n            gradient = np.zeros_like(theta)\n            gradient[state, :] = -policy\n            gradient[state, action] += 1\n            sum_gradients += episode_return * gradient\n    average_gradient = sum_gradients / len(episodes)\n    return np.round(average_gradient, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    n_experts (int): The number of experts in the MoE layer.\n    k_active (int): The number of active experts (sparsity).\n    d_in (int): The input dimension.\n    d_out (int): The output dimension.\n\n    Returns:\n    tuple: A tuple containing the FLOPs for the dense layer, the FLOPs for the MoE layer, and the savings percentage.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = 2 * k_active * d_in * d_out\n    if moe_flops >= dense_flops:\n        savings_percentage = 0.0\n    else:\n        savings_percentage = round((dense_flops - moe_flops) / dense_flops * 100, 1)\n    return (round(dense_flops, 1), round(moe_flops, 1), savings_percentage)\nk_active = 2\nd_in = 128\nd_out = 128"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    This function implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input matrix\n    W_g (np.ndarray): Weight matrix for the gating function\n    W_noise (np.ndarray): Weight matrix for the noise\n    N (np.ndarray): Pre-sampled noise\n    k (int): Sparsity constraint\n\n    Returns:\n    list: Final gating probabilities matrix reshaped as a list\n    \"\"\"\n    raw_gating_scores = np.dot(X, W_g)\n    noisy_gating_scores = raw_gating_scores + np.dot(N, W_noise)\n    top_k_indices = np.argsort(noisy_gating_scores, axis=1)[:, -k:]\n    mask = np.zeros_like(noisy_gating_scores)\n    np.put_along_axis(mask, top_k_indices[:, None], 1, axis=1)\n    gating_probabilities = noisy_gating_scores * mask / np.sum(noisy_gating_scores * mask, axis=1, keepdims=True)\n    gating_probabilities = np.round(gating_probabilities, 4)\n    final_gating_probabilities = gating_probabilities.tolist()\n    return final_gating_probabilities"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    This function implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    x (np.ndarray): Input tensor.\n    We (np.ndarray): Expert weight matrices.\n    Wg (np.ndarray): Gating weight matrix.\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n\n    Returns:\n    list: The final MoE output as a list after reshaping.\n    \"\"\"\n    gating_probabilities = np.exp(np.dot(x, Wg)) / np.sum(np.exp(np.dot(x, Wg)), axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probabilities, axis=1)[:, -top_k:]\n    output = np.zeros((x.shape[0], We.shape[1]))\n    for i in range(x.shape[0]):\n        expert_outputs = np.dot(x[i], We[top_k_indices[i]])\n        output[i] = np.sum(expert_outputs * gating_probabilities[i, top_k_indices[i]][:, np.newaxis], axis=0)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    This function performs Group Normalization on a 4D input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (B, C, H, W)\n    gamma (np.ndarray): Learned scale parameter with shape (C,)\n    beta (np.ndarray): Learned shift parameter with shape (C,)\n    num_groups (int): Number of groups to divide the channels into\n    epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    np.ndarray: Normalized tensor with shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = X.shape\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    normalized_X = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    normalized_X = normalized_X.reshape(B, C, H, W)\n    normalized_X = gamma.reshape(1, C, 1, 1) * normalized_X + beta.reshape(1, C, 1, 1)\n    return np.round(normalized_X, 4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    This function finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n\n    Args:\n    start_x (float): The initial position.\n    learning_rate (float, optional): The step size for each iteration. Defaults to 0.1.\n    tolerance (float, optional): The minimum difference between two consecutive x values to stop the iteration. Defaults to 1e-6.\n    max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    float: The value of x where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        next_x = current_x - learning_rate * df(current_x)\n        if abs(next_x - current_x) < tolerance:\n            break\n        current_x = next_x\n    return round(current_x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n    \n    Args:\n    x (np.ndarray): Input array.\n    alpha (float): Alpha parameter for DyT.\n    gamma (float): Gamma parameter for DyT.\n    beta (float): Beta parameter for DyT.\n    \n    Returns:\n    list[float]: The result of the DyT function applied to the input array, rounded to the nearest 4th decimal.\n    \"\"\"\n    dyt = np.tanh(alpha * x + beta) * gamma\n    dyt = np.round(dyt, 4)\n    result = dyt.tolist()\n    return result"}
