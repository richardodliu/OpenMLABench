{"task_id": 0, "completion_id": 0, "completion": "To solve this problem, we need to implement the closed-form solution for Ridge (L2-regularised) Linear Regression. The goal is to compute the weight vector that minimizes the sum of squared errors while incorporating a regularization term to prevent overfitting. The solution involves matrix operations and inversion, which requires careful handling to ensure numerical stability.\n\n### Approach\n1. **Extract Features and Target**: Separate the input data into features (X) and target (y). The target is the last column of the input data, and the features are all preceding columns.\n2. **Add Bias Term**: Append a column of ones to the feature matrix to include the bias term in the model.\n3. **Compute X Transpose and XTX**: Calculate the transpose of the feature matrix and then compute the product of this transpose with the feature matrix (X^T X).\n4. **Construct Regularized Matrix**: Add a scaled identity matrix to XTX, where the scaling factor is the regularization strength (alpha). This forms the matrix A.\n5. **Check Invertibility**: Compute the inverse of matrix A. If the matrix is singular (non-invertible), return -1.\n6. **Compute Weights**: Multiply the inverse matrix by the product of X transpose and the target vector to get the weight vector.\n7. **Round Weights**: Round each element of the weight vector to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Ridge regression using closed-form solution.\"\"\"\n    # Extract features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add a bias term\n    n_samples = X.shape[0]\n    X = np.hstack((X, np.ones((n_samples, 1)))\n    \n    # Compute X transpose and XTX\n    X_transpose = X.T\n    XTX = X_transpose.dot(X)\n    \n    # Create identity matrix of appropriate size\n    n_features = X.shape[1]\n    I = np.identity(n_features)\n    \n    # Construct the matrix A\n    A = XTX + alpha * I\n    \n    # Check if matrix A is invertible\n    try:\n        A_inv = np.linalg.inv(A)\n    except np.linalg.LinAlgError:\n        return [-1]\n    \n    # Compute the weights\n    w = A_inv.dot(X_transpose.dot(y))\n    \n    # Round to 4 decimal places\n    w_rounded = [round(weight, 4) for weight in w.ravel()]\n    \n    return w_rounded\n```\n\n### Explanation\n- **Extracting Features and Target**: The input data is split into features (X) and target (y). The target is the last column, and features are all other columns.\n- **Bias Term Addition**: A column of ones is added to the feature matrix to include the intercept term in the model.\n- **Matrix Operations**: The product of the transpose of the feature matrix with itself (XTX) is computed, which is a key step in forming the regularized matrix.\n- **Regularization**: The identity matrix scaled by alpha is added to XTX to form matrix A, which is then inverted if possible.\n- **Inverse Check**: The inverse of matrix A is computed, and if it is not invertible (determinant zero), the function returns -1.\n- **Weight Calculation**: The weights are computed using the inverse matrix and the product of X transpose and y.\n- **Rounding**: The resulting weights are rounded to four decimal places for the final output.\n\nThis approach ensures that the solution is both efficient and numerically stable, leveraging matrix operations to find the optimal weights for the linear regression model with L2 regularization."}
{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the dual-form perceptron learning algorithm, which is a step towards kernel methods. The algorithm learns a linear classifier in the dual form, where the weight vector is expressed as a linear combination of training samples.\n\n### Approach\nThe dual-form perceptron algorithm is based on the following steps:\n1. **Initialization**: Start with zero weights and bias.\n2. **Training Loop**: Iterate through the training samples. For each sample, compute the activation and check if it is misclassified.\n3. **Update Parameters**: If a sample is misclassified, update the dual parameters (\u03b1) and the bias (b).\n4. **Early Termination**: Stop the training if no updates occur after a full pass through the data or if the maximum number of iterations is reached.\n5. **Compute Primal Weights**: After training, convert the dual parameters into the primal weight vector.\n\nThe algorithm uses the Gram matrix to handle inner products, which allows it to work with kernel methods.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples = X_train.shape[0]\n    alpha = np.zeros(n_samples, dtype=np.float64)\n    b = 0.0\n    max_iter = n_iter\n    for _ in range(max_iter):\n        updated = False\n        for i in range(n_samples):\n            activation = 0.0\n            for j in range(n_samples):\n                x_j = X_train[j]\n                y_j = y_train[j]\n                x_i = X_train[i]\n                k = np.dot(x_j, x_i)\n                activation += alpha[j] * y_j * k\n            margin = y_train[i] * (activation + b)\n            if margin <= 0:\n                alpha[i] += eta\n                b += eta * y_train[i]\n                updated = True\n                break  # Restart the loop\n        if not updated:\n            break\n    # Compute the primal weight vector\n    w = np.zeros(n_samples, dtype=np.float64)\n    for j in range(n_samples):\n        yj = y_train[j]\n        xj = X_train[j]\n        w += alpha[j] * yj * xj\n    # Round to four decimal places\n    w_rounded = [round(wi, 4) for wi in w]\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)\n```\n\n### Explanation\n1. **Initialization**: The dual parameters (\u03b1) and bias (b) are initialized to zero.\n2. **Training Loop**: The outer loop runs for a maximum number of iterations. For each sample, the activation is computed using the Gram matrix (inner product of samples). If the sample is misclassified, the dual parameters and bias are updated.\n3. **Early Termination**: The loop breaks early if no updates occur after a full pass through the data.\n4. **Primal Weights**: After training, the primal weight vector is computed by summing the contributions of each training sample scaled by their dual parameters and labels.\n\nThis approach ensures that the algorithm efficiently learns the linear classifier in the dual form, leveraging the Gram matrix for kernel methods."}
{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to implement the standard link functions for Generalized Linear Models (GLMs) which include the identity, log, and logit links. Each link function must provide the link, its inverse, and its derivative, all of which should work with both numpy scalars and arrays.\n\n### Approach\n1. **Identity Link**: The identity function is straightforward, where the link function is the same as the response variable, its inverse is also the same, and the derivative is 1.\n2. **Log Link**: The log function is used for Poisson-distributed data. The inverse of the log function is the exponential function, and the derivative is the reciprocal of the response variable.\n3. **Logit Link**: The logit function is used for Bernoulli or Binomial-distributed data. The inverse of the logit function is the logit function itself, and the derivative is the reciprocal of the product of the response variable and its complement.\n\nEach function is implemented as a lambda to ensure it works with both scalars and numpy arrays, leveraging numpy's element-wise operations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {}\n    \n    # Identity link\n    links['identity'] = {\n        'link': lambda x: x,\n        'inv_link': lambda x: x,\n        'derivative': lambda x: 1\n    }\n    \n    # Log link\n    links['log'] = {\n        'link': lambda x: np.log(x),\n        'inv_link': lambda x: np.exp(x),\n        'derivative': lambda x: 1.0 / x\n    }\n    \n    # Logit link\n    links['logit'] = {\n        'link': lambda x: np.log(x / (1 - x)),\n        'inv_link': lambda x: np.log(x / (1 - x)),\n        'derivative': lambda x: 1.0 / (x * (1 - x))\n    }\n    \n    return links\n```\n\n### Explanation\n- **Identity Link**: The identity function simply returns the input, its inverse is the same, and the derivative is 1.\n- **Log Link**: The log function transforms the response variable, its inverse is the exponential function, and the derivative is the reciprocal of the response variable.\n- **Logit Link**: The logit function transforms the response variable, its inverse is the same function, and the derivative is the reciprocal of the product of the response variable and its complement.\n\nThis implementation ensures that each function works seamlessly with both scalars and numpy arrays, making it versatile for various applications in GLM analysis."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to compute the depth of the leaf reached by each sample in a random isolation tree. An isolation tree is a binary tree where each node splits the samples into left and right children based on a randomly chosen feature and a split value. The goal is to determine the depth of the leaf each sample ends up in after traversing the tree.\n\n### Approach\n1. **Set Random Seed**: To ensure reproducibility, we set the random seed to 0.\n2. **Recursive Tree Construction**: We use a helper function to recursively build the tree. Each node in the tree processes a subset of samples.\n3. **Leaf Node Check**: If a node has fewer than three samples, it is a leaf node, and we record the current depth for all samples in this node.\n4. **Splitting**: For non-leaf nodes, we choose a feature uniformly at random, determine the split value, and split the samples into left and right children.\n5. **Depth Assignment**: As we build the tree, we recursively process each child node and assign the appropriate depth to each sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    depths = [0] * n_samples\n\n    def helper(samples, depth):\n        if len(samples) < 3:\n            for s in samples:\n                depths[s] = depth\n        else:\n            n_features = data.shape[1]\n            f = np.random.randint(n_features)\n            f_values = data[samples, f]\n            down = min(f_values)\n            up = max(f_values)\n            v = np.random.uniform(down, up)\n            left = [s for s in samples if data[s, f] <= v]\n            right = [s for s in samples if data[s, f] > v]\n            helper(left, depth + 1)\n            helper(right, depth + 1)\n\n    helper(list(range(n_samples)), 0\n    return depths\n```\n\n### Explanation\n1. **Initialization**: The function starts by setting the random seed to ensure reproducibility.\n2. **Helper Function**: The helper function is defined to process each node recursively. It takes the current samples and the current depth as arguments.\n3. **Leaf Node Handling**: If a node has fewer than three samples, it is a leaf, and the depth of each sample in this node is set to the current depth.\n4. **Splitting Process**: For non-leaf nodes, a feature is chosen uniformly at random. The samples are split into left and right based on the chosen feature value.\n5. **Recursive Processing**: The helper function is called recursively on the left and right children, incrementing the depth for each subsequent level.\n\nThis approach ensures that each sample's depth is accurately recorded as it traverses the tree from the root to its respective leaf. The solution efficiently builds the tree and assigns depths using a recursive strategy, ensuring correctness and reproducibility."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to identify all frequent itemsets in a list of transactions based on a given minimum support threshold. An itemset is considered frequent if it appears in enough transactions, specifically at least the ceiling of the minimum support multiplied by the total number of transactions.\n\n### Approach\n1. **Preprocessing Transactions**: Convert each transaction into a set to facilitate quick lookups.\n2. **Calculate Required Support**: Determine the minimum number of transactions an itemset must appear in to be considered frequent. This is calculated as the ceiling of the minimum support threshold multiplied by the total number of transactions.\n3. **Generate Combinations**: For each possible size of item combinations (from 1 to the number of unique items), generate all possible combinations of items.\n4. **Count Support for Each Combination**: For each combination, count how many transactions include all items in the combination.\n5. **Check Frequency**: If a combination meets or exceeds the required support, add it to the result with its support count.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    if not transactions:\n        return {}\n    \n    N = len(transactions)\n    required_support = math.ceil(minsup * N)\n    \n    # Preprocess each transaction into a set for quick lookups\n    transactions_sets = [set(t) for t in transactions]\n    \n    # Collect all unique items across all transactions\n    all_items = set()\n    for t in transactions:\n        all_items.update(t)\n    all_items = list(all_items)\n    \n    result = {}\n    \n    # Iterate over all possible combination sizes\n    for k in range(1, len(all_items) + 1):\n        for combo in combinations(all_items, k):\n            count = 0\n            for t_set in transactions_sets:\n                if all(item in t_set for item in combo):\n                    count += 1\n            if count >= required_support:\n                sorted_combo = tuple(sorted(combo))\n                result[sorted_combo] = count\n    \n    return result\n```\n\n### Explanation\n1. **Preprocessing**: Each transaction is converted into a set to allow efficient membership checking.\n2. **Required Support Calculation**: The minimum number of transactions an itemset must appear in is determined using the formula `ceil(minsup * N)`, where `N` is the total number of transactions.\n3. **Combination Generation**: Using itertools, we generate all possible combinations of items of each possible size.\n4. **Support Counting**: For each combination, we count how many transactions include all items in the combination. If this count meets or exceeds the required support, the combination is added to the result.\n5. **Result Construction**: The result is a dictionary where each key is a tuple of sorted items, and the value is the support count.\n\nThis approach ensures that we efficiently find all frequent itemsets while adhering to the constraints and requirements of the problem."}
{"task_id": 11, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to cluster the given data into `k` groups, where each group is represented by a center point. The algorithm should repeatedly assign samples to the nearest center, recompute the centers as the mean of the samples in each cluster, and continue until the centers converge or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples as the initial cluster centers.\n2. **Assignment**: For each sample, determine the nearest cluster center based on the squared Euclidean distance.\n3. **Reinitialization**: If any cluster becomes empty during the assignment step, reinitialize its center with a random sample from the dataset.\n4. **Recompute Centers**: Calculate new cluster centers as the mean of the samples assigned to each cluster.\n5. **Convergence Check**: Stop the algorithm if the maximum change in any center between iterations is less than a specified threshold (`epsilon`) or if the maximum number of iterations (`max_iter`) is reached.\n6. **Rounding**: After convergence, round each coordinate of the centers to 4 decimal places and return the final centers along with the cluster labels for each sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n\n    if k < 1 or k > n_samples:\n        raise ValueError(\"k must be between 1 and the number of samples\")\n\n    old_centers = data[:k]\n\n    for _ in range(max_iter):\n        clusters = np.zeros(len(data), dtype=int)\n        for i, sample in enumerate(data):\n            min_dist = float('inf')\n            min_center = 0\n            for j, center in enumerate(old_centers):\n                dist = np.sum((sample - center) ** 2)\n                if dist < min_dist:\n                    min_dist = dist\n                    min_center = j\n            clusters[i] = min_center\n\n        # Check for empty clusters and reinitialize if necessary\n        empty_clusters = []\n        for j in range(k):\n            if clusters.count(j) == 0:\n                empty_clusters.append(j)\n\n        for j in empty_clusters:\n            idx = np.random.randint(len(data))\n            old_centers[j] = data[idx]\n\n        # Compute new centers as the mean of the samples in each cluster\n        new_centers = []\n        for j in range(k):\n            cluster_samples = data[clusters == j]\n            new_centers.append(np.mean(cluster_samples, axis=0))\n\n        # Check for convergence\n        max_diff = 0\n        for i in range(k):\n            diff = np.abs(old_centers[i] - new_centers[i])\n            if diff > max_diff:\n                max_diff = diff\n\n        if max_diff < epsilon:\n            break\n\n        old_centers = new_centers\n\n    # Round the centers to 4 decimal places\n    rounded_centers = [[round(c, 4) for c in center] for center in old_centers]\n\n    # Get the cluster labels for each sample\n    cluster_labels = clusters\n\n    return (rounded_centers, cluster_labels)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first `k` samples from the dataset.\n2. **Assignment**: Each sample is assigned to the nearest cluster center based on the squared Euclidean distance.\n3. **Reinitialization**: If any cluster becomes empty, its center is reinitialized with a random sample from the dataset to ensure the algorithm can continue.\n4. **Recompute Centers**: The new cluster centers are calculated as the mean of the samples in each cluster.\n5. **Convergence Check**: The algorithm stops if the maximum change in any center is less than `epsilon` or the maximum number of iterations is reached.\n6. **Rounding**: The final centers are rounded to 4 decimal places, and the cluster labels for each sample are returned along with the centers."}
{"task_id": 13, "completion_id": 0, "completion": "To solve this problem, we need to implement an item-based k-nearest neighbor (k-NN) collaborative filtering recommender. The goal is to predict the most attractive items for a given active user based on their ratings and the similarity between items. The similarity can be measured using either cosine similarity or Pearson correlation.\n\n### Approach\n1. **Build an Item-Item Similarity Matrix**: For each pair of items, compute their similarity based on the ratings provided by the users. The similarity is computed using either cosine similarity or Pearson correlation depending on the specified criterion.\n2. **Collect Rated Items**: Identify the items that the active user has already rated.\n3. **Compute Predicted Attractiveness**: For each unrated item, compute a score that represents its predicted attractiveness based on the similarity matrix and the ratings of the active user.\n4. **Sort and Return Recommendations**: Sort the unrated items based on their predicted attractiveness in descending order. In case of ties, the item with the smaller index is chosen first. Return the top k items.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_user, n_item = data.shape\n    if n_user == 0:\n        return []\n\n    # Build item-user sets\n    item_users = [set() for _ in range(n_item)]\n    for user in range(n_user):\n        for i in range(n_item):\n            if data[user, i] > 0:\n                item_users[i].add(user)\n\n    # Initialize similarity matrix\n    S = np.zeros((n_item, n_item))\n\n    # Compute similarity matrix\n    for i in range(n_item):\n        for j in range(i + 1, n_item):\n            common_users = item_users[i].intersection(item_users[j])\n            if not common_users:\n                S[i, j] = 0.0\n                S[j, i] = 0.0\n            else:\n                v1 = [data[u, i] for u in common_users]\n                v2 = [data[u, j] for u in common_users]\n                if criterion == 'cosine':\n                    mean_v1 = np.mean(v1)\n                    mean_v2 = np.mean(v2)\n                    if np.std(v1) > 1e-3:\n                        v1_centered = v1 - mean_v1\n                    else:\n                        v1_centered = v1\n                    if np.std(v2) > 1e-3:\n                        v2_centered = v2 - mean_v2\n                    else:\n                        v2_centered = v2\n                    dot = np.dot(v1_centered, v2_centered)\n                    len_v1 = np.linalg.norm(v1_centered)\n                    len_v2 = np.linalg.norm(v2_centered)\n                    if len_v1 == 0 or len_v2 == 0:\n                        sim = 0.0\n                    else:\n                        sim = dot / (len_v1 * len_v2)\n                    S[i, j] = sim\n                    S[j, i] = sim\n                elif criterion == 'pearson':\n                    correlation = np.corrcoef(v1, v2)[0, 0]\n                    S[i, j] = correlation\n                    S[j, i] = correlation\n\n    # Get rated items for the active user\n    rated_items = np.where(data[user_ind, :] > 0)[0]\n    if len(rated_items) == n_item:\n        return []\n\n    r = data[user_ind, rated_items]\n    unrated_items = [t for t in range(n_item) if t not in rated_items]\n\n    # Compute scores for unrated items\n    scores = []\n    for t in unrated_items:\n        sum_product = 0.0\n        sum_abs = 0.0\n        for i in rated_items:\n            sum_product += r[i] * S[t, i]\n            sum_abs += abs(S[t, i])\n        if sum_abs == 0:\n            score = 0.0\n        else:\n            score = sum_product / sum_abs\n        scores.append((score, t))\n\n    # Sort by descending score, then ascending item index\n    scores.sort(key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in scores[:k]]\n\n    return result\n```\n\n### Explanation\n1. **Building the Similarity Matrix**: The code first constructs a similarity matrix `S` where each entry `S[i, j]` represents the similarity between item `i` and item `j`. This is done by finding common users who rated both items and computing the similarity based on the specified criterion.\n2. **Collecting Rated Items**: The code identifies which items the active user has rated and extracts their ratings.\n3. **Computing Predicted Attractiveness**: For each unrated item, the code computes a score by summing the product of the item's similarity with each rated item's rating, normalized by the sum of the absolute similarities.\n4. **Sorting and Returning Recommendations**: The unrated items are sorted by their predicted scores in descending order, and the top `k` items are returned. In case of ties, the item with the smaller index is chosen first.\n\nThis approach ensures that the recommendations are based on the collaborative filtering technique, leveraging the similarity between items to predict the most attractive items for the active user."}
{"task_id": 20, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes both the sigmoid activation and its gradient for a given input. The function should handle various input types, including scalars, lists, and NumPy arrays, and return the results rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a scalar, a list, or a NumPy array. We first convert it into a NumPy array to handle it uniformly.\n2. **Compute Sigmoid Function**: The sigmoid function is defined as \\( s(x) = \\frac{1}{1 + e^{-x}} \\). We compute this for the input array.\n3. **Compute Gradient**: The gradient of the sigmoid function is given by \\( s'(x) = s(x) \\times (1 - s(x)) \\). Using the result from the sigmoid function, we compute the gradient.\n4. **Round Results**: Both the sigmoid values and gradients are rounded to four decimal places.\n5. **Return Appropriate Data Types**: If the input was a scalar, return the results as floats. If the input was an array, return the results as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    x = np.asarray(x)\n    sig_x = 1 / (1 + np.exp(-x))\n    grad_x = sig_x * (1 - sig_x)\n    rounded_sig = np.round(sig_x, 4)\n    rounded_grad = np.round(grad_x, 4)\n    \n    if x.ndim == 0:\n        return (rounded_sig.item(), rounded_grad.item())\n    else:\n        return (rounded_sig.tolist(), rounded_grad.tolist())\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to handle both scalar and array inputs uniformly.\n- **Sigmoid Calculation**: Using the formula \\( s(x) = \\frac{1}{1 + e^{-x}} \\), we compute the sigmoid values for each element in the input array.\n- **Gradient Calculation**: The gradient is computed using the derivative of the sigmoid function, which is \\( s'(x) = s(x) \\times (1 - s(x)) \\).\n- **Rounding**: Both the sigmoid values and gradients are rounded to four decimal places to meet the problem requirements.\n- **Return Type Handling**: The function checks if the input was a scalar (0-dimensional array) and returns the results as floats. For arrays, it returns the results as lists.\n\nThis approach ensures that the function works efficiently for various input types and returns the results in the required format."}
{"task_id": 25, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that predicts labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function \u2013 RBF) kernel. The function should compute the decision function for each test sample and return the predicted labels as a list of integers.\n\n### Approach\nThe decision function for an SVM with a Gaussian kernel is given by:\n\\[ g(z) = \\sum_{i=1}^{n} \\alpha_i y_i \\exp(-\\gamma ||x_i - z||^2) + b \\]\nwhere \\( z \\) is a test sample, \\( \\alpha \\) are the Lagrange multipliers, \\( y \\) are the class labels, \\( \\gamma \\) is the Gaussian kernel hyper-parameter, and \\( b \\) is the bias term. The predicted class is determined by the sign of \\( g(z) \\).\n\nThe steps to implement this function are as follows:\n1. **Compute the Kernel Matrix**: The kernel matrix \\( K \\) is computed where each element \\( K_{ij} \\) is the Gaussian kernel between the \\( i \\)-th training sample and the \\( j \\)-th test sample. This is done using the formula \\( K_{ij} = \\exp(-\\gamma ||x_i - z_j||^2) \\).\n2. **Compute Scores**: For each test sample, compute the score by taking the dot product of the vector of Lagrange multipliers multiplied by their corresponding class labels and the kernel vector, then add the bias term.\n3. **Predict Labels**: Convert the scores to predicted labels by taking the sign of each score, resulting in 1 or -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters:\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns:\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly 1 or -1.\n    \"\"\"\n    # Compute the kernel matrix K\n    kernel = np.exp(-gamma * (X_train - X_test) @ (X_train - X_test).T)\n    # Compute the scores for each test sample\n    scores = (alpha * y_train) @ kernel + b\n    # Determine the predicted labels\n    predicted_labels = np.sign(scores).astype(int)\n    return predicted_labels.tolist()\n```\n\n### Explanation\n1. **Kernel Matrix Calculation**: The kernel matrix is computed using the formula \\( K = \\exp(-\\gamma (X_{\\text{train}} - X_{\\text{test}})^T (X_{\\text{train}} - X_{\\text{test}})) \\). This matrix efficiently captures the Gaussian similarity between each training and test sample.\n2. **Score Calculation**: For each test sample, the score is computed by multiplying the vector of Lagrange multipliers (scaled by their class labels) with the corresponding kernel vector and adding the bias term.\n3. **Prediction**: The predicted label for each test sample is determined by the sign of the computed score, resulting in either 1 or -1.\n\nThis approach ensures that the function is efficient and vectorized, leveraging numpy operations to handle large datasets effectively."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to compute the optimal rank-k reconstruction of a given matrix using the truncated Singular Value Decomposition (SVD). This reconstruction is equivalent to the best linear autoencoder with k latent units. The task involves checking the validity of the rank k, computing the SVD, reconstructing the matrix, and calculating the mean squared error (MSE).\n\n### Approach\n1. **Check Validity of k**: Ensure that the given rank k is within the valid range, i.e., 1 \u2264 k \u2264 min(m, n), where m and n are the dimensions of the input matrix X. If k is invalid, return (-1, -1).\n2. **Compute SVD**: Use numpy's SVD function to decompose the matrix X into three matrices: U, S, and V.\n3. **Reconstruct the Matrix**: Using the top k singular values and corresponding singular vectors from U and V, reconstruct the matrix. The reconstruction is done by multiplying the first k columns of U with the first k singular values and then with the transpose of the first k rows of V.\n4. **Calculate MSE**: Compute the mean squared error between the original matrix X and the reconstructed matrix X_hat.\n5. **Round Results**: Round both the reconstructed matrix and the MSE to four decimal places and convert the matrix back to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns (-1, -1).\n    \"\"\"\n    # Check if k is valid\n    m = len(X)\n    if m == 0:\n        return (-1, -1)\n    n = len(X[0])\n    if k < 1 or k > min(m, n):\n        return (-1, -1)\n    \n    # Convert X to a numpy array for SVD computation\n    X_array = np.array(X)\n    \n    # Compute SVD\n    U, S, V = np.svd(X_array)\n    \n    # Extract the first k components\n    U_k = U[:, :k]\n    S_k = S[:k, :]  # S is k x k, so slicing as is\n    V_k = V  # V is k x n\n    \n    # Compute the rank-k reconstruction\n    X_hat = np.dot(U_k, np.dot(S_k, V.T))\n    \n    # Calculate the mean squared error\n    mse = np.mean(np.square(X_array - X_hat))\n    \n    # Round to four decimal places\n    X_hat_rounded = np.round(X_hat, 4)\n    mse_rounded = np.round(mse, 4)\n    \n    # Convert X_hat to a list of lists\n    X_hat_list = X_hat_rounded.tolist()\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if k is within the valid range. If not, it returns (-1, -1).\n2. **SVD Computation**: The SVD of the input matrix X is computed using numpy's `svd` function, which returns the matrices U, S, and V.\n3. **Matrix Reconstruction**: The top k singular values and vectors are extracted from U, S, and V. The reconstruction is performed by multiplying these components in the order U_k @ S_k @ V.T.\n4. **MSE Calculation**: The mean squared error between the original and reconstructed matrices is computed and rounded to four decimal places.\n5. **Result Conversion**: The reconstructed matrix is converted back to a list of lists and both the matrix and MSE are returned as a tuple, rounded to four decimal places."}
{"task_id": 29, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a one-dimensional gradient boosting algorithm using decision stumps. The goal is to fit a model on training data and predict values for a new point based on the learned stumps.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Sort the Training Data**: Combine the training features and targets into a list of tuples and sort them based on the feature values.\n2. **Generate Candidate Splits**: Compute midpoints between consecutive feature values to create potential split points.\n3. **Iteratively Add Stumps**: For each candidate split, compute the mean residuals on either side of the split. Add the stump that minimizes the residual sum of squares (RSS) to the ensemble until the RSS is below a specified tolerance.\n4. **Update Residuals**: After adding each stump, update the residuals to reflect the new predictions.\n5. **Predict for a New Point**: For the query point, sum the contributions from all learned stumps based on where the point falls relative to each split.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x = np.array(x_train)\n    y = np.array(y_train)\n    \n    # Sort the training data by the feature value\n    sorted_indices = np.argsort(x)\n    sorted_x = x[sorted_indices]\n    sorted_y = y[sorted_indices]\n    \n    # Generate candidate split points\n    candidate_splits = []\n    for i in range(len(sorted_x) - 1):\n        s = (sorted_x[i] + sorted_x[i + 1]) / 2\n        candidate_splits.append(s)\n    \n    # Initialize residuals with the target values\n    residuals = sorted_y.copy()\n    ensemble = []\n    \n    while True:\n        min_rss = float('inf')\n        best_split = None\n        best_c1 = 0.0\n        best_c2 = 0.0\n        \n        for s in candidate_splits:\n            # Compute c1 and c2 for the current split\n            c1 = 0.0\n            c2 = 0.0\n            for i in range(len(sorted_x)):\n                if sorted_x[i] <= s:\n                    c1 += residuals[i]\n                else:\n                    c2 += residuals[i]\n            \n            # Compute new residuals for this candidate split\n            new_residuals = []\n            for i in range(len(sorted_x)):\n                if sorted_x[i] <= s:\n                    new_r = residuals[i] - c1\n                else:\n                    new_r = residuals[i] - c2\n                new_residuals.append(new_r)\n            \n            # Calculate the RSS for this candidate split\n            rss = np.sum(new_residuals ** 2)\n            if rss < min_rss:\n                min_rss = rss\n                best_split = s\n                best_c1 = c1\n                best_c2 = c2\n        \n        # Add the best split to the ensemble\n        ensemble.append((best_split, best_c1, best_c2))\n        residuals = new_residuals\n        \n        # Check if we should stop\n        current_rss = np.sum(residuals ** 2)\n        if current_rss <= epsilon:\n            break\n    \n    # Calculate the prediction for the query point\n    prediction = 0.0\n    for split, c1, c2 in ensemble:\n        if x_query <= split:\n            prediction += c1\n        else:\n            prediction += c2\n    \n    # Return the prediction rounded to four decimal places\n    return round(prediction, 4)\n```\n\n### Explanation\n1. **Sorting Data**: The training data is sorted based on the feature values to facilitate the creation of candidate splits.\n2. **Candidate Splits**: Midpoints between consecutive feature values are computed to create potential split points.\n3. **Stump Addition**: For each candidate split, the algorithm computes the mean residuals on either side of the split. The split that minimizes the RSS is added to the ensemble.\n4. **Residual Updates**: After each split is added, the residuals are updated to reflect the new predictions.\n5. **Prediction**: For a new query point, the prediction is computed by summing the contributions from all learned stumps based on where the point falls relative to each split.\n\nThis approach ensures that the model is built iteratively, adding stumps that provide the maximum reduction in RSS until the model's performance meets the specified tolerance. The prediction for a new point is efficient and based on the learned stumps."}
{"task_id": 34, "completion_id": 0, "completion": "To solve this problem, we need to generate a NumPy tensor filled with 0.0s and 1.0s based on the specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and handle edge cases such as invalid sparsity values.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return a float array filled with -1.0 of the specified shape.\n2. **Seed Handling**: If a seed is provided, set it using `np.random.seed(seed)` to ensure reproducibility.\n3. **Random Array Generation**: Generate a random array using `np.random.rand(*shape)` to create an array of random floats in the specified shape.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity` to determine which values will be set to 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where values greater than or equal to the threshold are set to 1.0 and others to 0.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generate a binary tensor with 0.0 and 1.0 values based on the given shape and sparsity.\n\n    Parameters:\n        shape (tuple[int, ...]): The shape of the output tensor.\n        sparsity (float, optional): Probability of generating 1.0. Must be in [0, 1]. Default is 0.5.\n        seed (int, optional): Seed for reproducibility. If provided, np.random.seed is called. Default is None.\n\n    Returns:\n        numpy.ndarray: A tensor of float dtype with 0.0 and 1.0 values. If sparsity is invalid, returns a float array with -1.0 in the same shape.\n    \"\"\"\n    # Check if sparsity is within the valid range\n    if not (0.0 <= sparsity <= 1.0):\n        return np.full(shape, -1.0, dtype=np.float64)\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate the random array\n    rand_arr = np.random.rand(*shape)\n    \n    # Calculate the threshold for determining 1.0 values\n    threshold = 1.0 - sparsity\n    \n    # Create the binary array\n    binary_arr = np.where(rand_arr >= threshold, 1.0, 0.0)\n    \n    return binary_arr\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns a float array filled with -1.0 of the specified shape.\n- **Seed Handling**: If a seed is provided, it initializes the random number generator to ensure consistent results.\n- **Random Array Generation**: Using `np.random.rand(*shape)`, the function generates an array of random values between 0 and 1.\n- **Threshold Calculation**: The threshold is computed as `1 - sparsity`, determining which values are set to 1.0.\n- **Binary Conversion**: The function converts the random array into a binary array using `np.where`, setting values above the threshold to 1.0 and others to 0.0.\n\nThis approach ensures that the function efficiently generates the required binary tensor while handling edge cases and ensuring reproducibility when a seed is provided."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs a median split on a dataset, which is a fundamental operation used in constructing a KD-Tree. The function will split the dataset into three parts: the pivot (the row with the median value), the left part (rows with values strictly less than the median), and the right part (rows with values strictly greater than the median).\n\n### Approach\n1. **Extract the Column**: Extract the specified column from the dataset.\n2. **Sort the Column**: Use `numpy.argpartition` to sort the column and get the indices of the sorted values.\n3. **Find the Median**: Determine the median value from the sorted column. For an even number of samples, the median is the element at the middle index.\n4. **Identify the Pivot**: The pivot is the row index of the median value in the original dataset.\n5. **Split the Data**: Create two lists, `left` and `right`, containing the indices of rows with values strictly less than and greater than the median, respectively.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger values.\n    \"\"\"\n    n = data.shape[0]\n    sorted_indices = np.argpartition(data[:, d])\n    median_value = data[sorted_indices[n//2]]\n    pivot = sorted_indices[n//2]\n    \n    left = []\n    right = []\n    for i in range(n):\n        if data[i][d] < median_value:\n            left.append(i)\n        elif data[i][d] > median_value:\n            right.append(i)\n    \n    return (pivot, left, right)\n```\n\n### Explanation\n1. **Extract the Column**: The specified column `d` is extracted from the dataset using slicing.\n2. **Sort the Column**: `numpy.argpartition` is used to sort the column and return the indices of the sorted values. This helps in efficiently finding the median without sorting the entire array.\n3. **Find the Median**: The median is found at the middle index of the sorted column. For an even number of samples, this is the element at `n//2`.\n4. **Identify the Pivot**: The pivot is the row index of the median value in the original dataset.\n5. **Split the Data**: By iterating through the dataset, we collect indices of rows with values less than and greater than the median into `left` and `right` lists, respectively. These lists are naturally sorted as we iterate from the start to the end of the dataset.\n\nThis approach ensures that the function runs in O(n) time, making it efficient for large datasets. The resulting lists are sorted increasingly, ensuring deterministic output."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that converts a one-dimensional signal into overlapping frames using NumPy. Each frame is a window of a specified width, and consecutive frames are separated by a given stride. The function should return a view on the original array, ensuring efficient memory usage.\n\n### Approach\n1. **Check Pre-Conditions**: Ensure the input array is one-dimensional, the stride is positive, and the array is long enough to form the frames.\n2. **Calculate Number of Frames**: Determine the number of frames by using the formula `(len(x) - frame_width) // stride + 1`, which accounts for the number of complete frames that can be formed.\n3. **Create Overlapping Frames**: Use NumPy's `as_strided` function to create a view of the original array. This function allows us to efficiently create the frames without copying the data, thus maintaining memory efficiency.\n4. **Set Writeable Flag**: Adjust the writeable flag of the resulting view based on the provided `writeable` parameter.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    if x.ndim != 1:\n        raise AssertionError(\"x must be 1-D\")\n    if stride < 1:\n        raise AssertionError(\"stride must be >= 1\")\n    if x.size < frame_width:\n        raise AssertionError(\"x must be at least frame_width in length\")\n\n    # Calculate the number of frames\n    n_frames = (x.size - frame_width) // stride + 1\n\n    # Create the view using as_strided\n    view = as_strided(x, shape=(n_frames, frame_width), strides=(stride, 1))\n\n    # Set the writeable flag\n    view.flags.writeable = writeable\n\n    return view\n```\n\n### Explanation\n- **Pre-Conditions Check**: The function first ensures that the input array is one-dimensional, the stride is positive, and the array is long enough to form the frames. If any of these conditions are not met, an `AssertionError` is raised.\n- **Frame Calculation**: The number of frames is calculated to ensure we only consider complete frames, dropping any trailing samples that do not form a complete frame.\n- **View Creation**: Using `as_strided`, the function efficiently creates a view of the original array, which is memory efficient as it does not require copying the data.\n- **Writeable Flag**: The resulting view's writeable flag is set based on the provided `writeable` parameter, ensuring the returned array is either writeable or read-only as required.\n\nThis approach ensures that the function is both efficient and meets the problem requirements, providing a clear and concise solution."}
{"task_id": 48, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given matrix is row-stochastic. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1, within a small numerical tolerance.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check if the input is a numpy array or a list of lists**: The function should handle both cases uniformly.\n2. **Check if the matrix is two-dimensional**: For numpy arrays, this is done by checking the number of dimensions. For lists, ensure each element is a list and all rows have the same length.\n3. **Check if all elements are valid probabilities**: Each element must be between 0 and 1 inclusive.\n4. **Check if each row sums to 1 within a tolerance**: The sum of each row should be approximately 1, allowing for a small numerical error (1e-8).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        # Check if the matrix is at least 2D\n        if X.ndim < 2:\n            return False\n        # Check if all elements are between 0 and 1\n        if not np.all((X >= 0) & (X <= 1)):\n            return False\n        # Check if each row sums to 1 within a tolerance\n        row_sums = np.sum(X, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-8):\n            return False\n        return True\n    else:\n        # X is a list of lists\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Check if all rows have the same length\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        # Check if the matrix is not empty\n        if len(X) == 0:\n            return False\n        # Convert to numpy array\n        try:\n            X_np = np.array(X)\n        except:\n            return False\n        # Check if all elements are numeric and of float type\n        if not np.issubdtype(X_np.dtype, np.floating):\n            return False\n        # Check if all elements are between 0 and 1\n        if not np.all((X_np >= 0) & (X_np <= 1)):\n            return False\n        # Check if each row sums to 1 within a tolerance\n        row_sums = np.sum(X_np, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-8):\n            return False\n        return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a numpy array or a list of lists. This ensures that the function can handle both types uniformly.\n2. **Dimension Check**: For numpy arrays, it checks if the array is at least 2D. For lists, it ensures each element is a list and all rows have the same length.\n3. **Element Check**: The function verifies that all elements are within the range [0, 1]. This is done using numpy's array operations for efficiency.\n4. **Row Sum Check**: The function calculates the sum of each row and checks if each sum is approximately 1, allowing for a small numerical tolerance (1e-8) to account for floating-point precision issues.\n\nThis approach ensures that the function efficiently and accurately determines if the given matrix is row-stochastic, covering all edge cases and ensuring numerical stability."}
{"task_id": 55, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least squares (OLS) linear regression. The goal is to find the weight vector that minimizes the mean squared error between the predicted values and the actual target values. The RMSprop algorithm is an optimization technique that efficiently updates the weights by adjusting them in the direction opposite to the gradient of the loss function, scaled by a learning rate and a decay factor for the squared gradients.\n\n### Approach\n1. **Problem Analysis**: The task is to minimize the mean squared error (MSE) between the predicted values and the actual target values. The loss function is given by \\( L(w) = \\frac{1}{2n} \\| Xw - y \\|_2^2 \\), where \\( X \\) is the feature matrix, \\( y \\) is the target vector, and \\( w \\) is the weight vector.\n\n2. **Gradient Calculation**: The gradient of the loss function with respect to \\( w \\) is computed as \\( g = \\frac{1}{n} X^T (Xw - y) \\), where \\( X^T \\) is the transpose of the feature matrix.\n\n3. **RMSprop Update Rule**: The RMSprop algorithm updates the weights using the following steps:\n   - Compute the squared gradients and maintain a running average using a decay factor \\( \\rho \\).\n   - Update the weights by moving in the direction opposite to the gradient, scaled by a learning rate \\( \\eta \\) and divided by the square root of the running average of squared gradients, plus a small constant to avoid division by zero.\n\n4. **Batch Processing**: The data is processed in mini-batches. If the number of samples \\( n \\) is less than the batch size, the entire dataset is used as one batch. Otherwise, successive blocks of rows are processed, wrapping around to the beginning when the end of the matrix is reached.\n\n5. **Stopping Conditions**: The optimization stops when the norm of the gradient becomes smaller than a given threshold \\( \\epsilon \\) or the maximum number of iterations \\( \\text{max\\_iter} \\) is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n = X.shape[0]\n    d = X.shape[1]\n    w = np.zeros(d, dtype=np.float64)\n    s = np.zeros(d, dtype=np.float64)\n    current_pos = 0\n\n    for iteration in range(max_iter):\n        if n < batch_size:\n            batch_x = X\n            batch_y = y\n        else:\n            start = current_pos\n            end = start + batch_size\n            if end > n:\n                take_end = n - start\n                take_start = batch_size - take_end\n                if take_start < 0:\n                    take_start = 0\n                batch_x = X[start: start + take_end] + X[:take_start]\n                batch_y = y[start: start + take_end] + y[:take_start]\n            else:\n                batch_x = X[start:end]\n                batch_y = y[start:end]\n            current_pos = end % n\n\n        # Compute Xw\n        Xw = batch_x @ w\n        residuals = Xw - batch_y\n        # Compute gradient\n        g = (1.0 / n) * np.dot(X.T, residuals)\n        # Compute squared gradient\n        g_squared = g ** 2\n        # Update s\n        s = rho * s + (1 - rho) * g_squared\n        # Compute denominator\n        sqrt_s = np.sqrt(s)\n        denominator = sqrt_s + eps_station\n        # Update weights\n        w = w - eta * g / denominator\n        # Check stopping conditions\n        norm_g = np.linalg.norm(g)\n        if norm_g < epsilon:\n            break\n        if iteration == max_iter - 1:\n            break\n\n    # Round to four decimal places\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The weights \\( w \\) and the squared gradient average \\( s \\) are initialized to zero. The current position for batch processing is set to the start of the data.\n- **Batch Processing**: For each iteration, the function processes a batch of data. If the batch size is larger than the number of samples, the entire dataset is used. Otherwise, successive blocks of rows are processed, wrapping around to the beginning when the end is reached.\n- **Gradient Calculation**: The gradient \\( g \\) is computed using the current batch, and the squared gradient is used to update the running average \\( s \\).\n- **Weight Update**: The weights are updated using the RMSprop rule, which adjusts the learning rate by the gradient and the decay factor.\n- **Stopping Conditions**: The optimization stops when the gradient's norm is below a threshold or the maximum number of iterations is reached.\n- **Result**: The final weights are rounded to four decimal places and returned as a list."}
{"task_id": 56, "completion_id": 0, "completion": "To solve this problem, we need to implement the SoftPlus activation function, which is widely used in deep learning models. The function should be able to compute both the SoftPlus value and its derivative (which is the sigmoid function) based on the input parameters. The solution must handle both scalar and array inputs, ensuring numerical stability for very large positive or negative values.\n\n### Approach\nThe SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\). The derivative of this function is the sigmoid function, \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). \n\nTo handle numerical stability, we use a piece-wise approach:\n1. For positive values of \\( x \\), compute \\( \\text{SoftPlus}(x) \\) as \\( x + \\log(1 + e^{-x}) \\) to avoid overflow.\n2. For non-positive values of \\( x \\), compute \\( \\text{SoftPlus}(x) \\) as \\( \\log(1 + e^x) \\).\n\nFor the derivative (sigmoid function), we use the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\), which is computed efficiently using a piece-wise approach to avoid numerical issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function and its derivative (sigmoid).\"\"\"\n    if isinstance(x, (int, float)):\n        # Handle scalar input\n        if deriv:\n            return round(1.0 / (1 + np.exp(-x)), 4)\n        else:\n            if x > 0:\n                return round(x + np.log(1 + np.exp(-x)), 4)\n            else:\n                return round(np.log(1 + np.exp(x)), 4)\n    else:\n        # Convert list or array to numpy array\n        x = np.array(x)\n        if deriv:\n            # Compute sigmoid\n            sig = 1 / (1 + np.exp(-x))\n            sig_rounded = np.round(sig, 4)\n            return sig_rounded.tolist()\n        else:\n            # Compute SoftPlus\n            exp_x = np.exp(x)\n            log_val = np.where(x > 0, x + np.log(1 + np.exp(-x)), np.log(1 + exp_x))\n            log_val_rounded = np.round(log_val, 4)\n            return log_val_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Scalar Input**: If the input `x` is a scalar (int or float), the function directly computes the result using the appropriate formula based on whether the derivative is required.\n2. **Handling Array Input**: If the input `x` is an array or list, it is converted to a numpy array for efficient computation. The function then computes the SoftPlus or sigmoid values element-wise.\n3. **Numerical Stability**: The function uses `np.exp` and `np.log` functions to handle large values, ensuring numerical stability by avoiding overflow through a piece-wise approach.\n4. **Rounding**: The results are rounded to four decimal places and returned as a list for array inputs or as a single float for scalar inputs.\n\nThis approach ensures that the function efficiently and accurately computes both the SoftPlus activation and its derivative, handling various input types and ensuring numerical stability."}
{"task_id": 58, "completion_id": 0, "completion": "To solve this problem, we need to compute the unsmoothed Maximum-Likelihood N-gram log-probability of a given sequence using a training corpus. The solution involves building an N-gram model from the corpus and then using this model to compute the log probability of the target sequence.\n\n### Approach\n1. **Building the N-gram Model**: \n   - For N=1 (unigram), we count the occurrences of each word in the corpus.\n   - For N>1, we build a nested dictionary where each key is an (N-1)-gram, and the value is another dictionary of N-grams that start with that (N-1)-gram, with their counts.\n\n2. **Computing the Log Probability**:\n   - For each window of size N in the sequence, compute the log probability using the precomputed N-gram model.\n   - For N=1, the probability of each word is its count divided by the total number of words in the corpus.\n   - For N>1, the probability of each N-gram is the count of the N-gram divided by the count of its (N-1)-gram prefix.\n   - Sum the log probabilities of all valid windows in the sequence. If any required count is zero, return negative infinity.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    The function builds an N-gram count table from the *corpus* and returns the total log-probability\n    of the *sequence* under the resulting unsmoothed language model. If any required count is zero,\n    it returns ``float('-inf')``.\n\n    Args:\n        corpus: List of word tokens.\n        sequence: List of word tokens to compute the log-probability for.\n        N: Order of the N-gram model.\n\n    Returns:\n        The total log-probability rounded to 4 decimal places, or ``float('-inf')`` if any required count is zero.\n    \"\"\"\n    if N == 0:\n        return float('-inf')  # N must be at least 1\n\n    # Build the ngram counts\n    if N == 1:\n        ngram_counts = defaultdict(int)\n        for word in corpus:\n            ngram_counts[word] += 1\n    else:\n        ngram_counts = defaultdict(lambda: defaultdict(int))\n        for i in range(len(corpus) - N + 1):\n            current_gram = tuple(corpus[i:i+N])\n            prefix = tuple(corpus[i:i+N-1])\n            ngram_counts[prefix][current_gram] += 1\n\n    total = 0.0\n\n    if N == 1:\n        total_words = len(corpus)\n        for word in sequence:\n            if word not in ngram_counts or ngram_counts[word] == 0:\n                return float('-inf')\n            prob = math.log(ngram_counts[word] / total_words)\n            total += prob\n    else:\n        for i in range(len(sequence) - N + 1):\n            window = tuple(sequence[i:i+N])\n            prefix = tuple(sequence[i:i+N-1])\n            if prefix not in ngram_counts or window not in ngram_counts[prefix]:\n                return float('-inf')\n            count_window = ngram_counts[prefix][window]\n            count_prefix = sum(ngram_counts[prefix].values())\n            if count_window == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = math.log(count_window / count_prefix)\n            total += prob\n\n    return round(total, 4)\n```\n\n### Explanation\n1. **Building the N-gram Model**:\n   - For N=1, we use a flat dictionary to count word occurrences.\n   - For N>1, we use a nested dictionary to count occurrences of each N-gram and their respective (N-1)-gram prefixes.\n\n2. **Computing Log Probabilities**:\n   - For each window of size N in the sequence, we check if the window and its prefix exist in the model.\n   - If any window or prefix is missing, we return negative infinity.\n   - For each valid window, we compute the log probability and sum these values.\n   - The result is rounded to four decimal places and returned.\n\nThis approach ensures that we efficiently compute the log probability using the Maximum-Likelihood estimation for the given sequence under the unsmoothed N-gram model."}
{"task_id": 62, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a univariate Lasso regression model that uses polynomial feature expansion and optimizes the objective function using coordinate descent. The goal is to minimize a combination of squared error and L1 regularization, which helps in preventing overfitting by encouraging sparsity in the model.\n\n### Approach\n1. **Problem Analysis**: The task is to fit a polynomial regression model with L1 regularization. The objective function combines the squared error term and the L1 regularization term. The challenge is to optimize this function efficiently.\n\n2. **Design Matrix Construction**: The design matrix is constructed to include a column of ones (for the bias term) followed by polynomial features up to the specified degree. This matrix helps in transforming the input data into a form that can be used with polynomial terms.\n\n3. **Coordinate Descent Optimization**: This method updates each coefficient one by one, minimizing the objective function. The update for the bias term (w0) is done separately, while each polynomial coefficient (wj) is updated using a specific formula derived from the Lasso objective function.\n\n4. **Soft Threshold Function**: This function is used to compute the new value of each coefficient, ensuring that the model remains sparse by encouraging coefficients to become zero if their magnitude is below the regularization strength.\n\n5. **Early Stopping**: The optimization process stops when the largest absolute change in any coefficient is below a specified tolerance or after a given number of iterations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\"\"\"\n    # Check if X and y are compatible\n    m = len(X)\n    if len(y) != m:\n        raise ValueError(\"X and y must have the same length.\")\n    \n    # Build design matrix\n    design_matrix = np.zeros((m, degree + 1))\n    design_matrix[:, 0] = 1.0\n    for j in range(1, degree + 1):\n        design_matrix[:, j] = np.array([x ** j for x in X], dtype=np.float64)\n    \n    # Initialize weights\n    current_weights = np.zeros(degree + 1)\n    \n    # Early stopping\n    for _ in range(n_iterations):\n        max_change = 0.0\n        \n        # Update w0\n        X_without_w0 = design_matrix[:, 1:]\n        residual_without_w0 = y - (X_without_w0 @ current_weights)\n        w0_new = np.mean(residual_without_w0)\n        change_w0 = abs(w0_new - current_weights[0])\n        max_change = max(max_change, change_w0)\n        current_weights[0] = w0_new\n        \n        # Update other coefficients\n        for j in range(1, degree + 1):\n            xj = design_matrix[:, j]\n            # Compute residual_j = y - (X * current_weights) + current_weights[j] * xj\n            residual_j = y - (design_matrix @ current_weights) + current_weights[j] * xj\n            rho = xj.T @ residual_j\n            denominator = xj.T @ xj\n            if denominator == 0:\n                new_wj = 0.0\n            else:\n                if rho == 0:\n                    new_wj = 0.0\n                else:\n                    sign_rho = np.sign(rho)\n                    abs_rho = np.abs(rho)\n                    threshold = max(abs_rho - reg_factor, 0.0)\n                    new_wj = sign_rho * threshold\n                new_wj = new_wj / denominator\n            change = abs(new_wj - current_weights[j])\n            if change > max_change:\n                max_change = change\n            current_weights[j] = new_wj\n        \n        if max_change < tol:\n            break\n    \n    # Return coefficients rounded to 4 decimals\n    return [round(coeff, 4) for coeff in current_weights]\n```\n\n### Explanation\n1. **Design Matrix Construction**: The design matrix is constructed to include a column of ones and polynomial features up to the specified degree. This allows the model to capture polynomial trends in the data.\n\n2. **Coordinate Descent Optimization**: The algorithm updates each coefficient iteratively. The bias term (w0) is updated first using the mean of the residuals. Each polynomial coefficient (wj) is updated using a formula that incorporates the current residuals and the regularization strength.\n\n3. **Soft Threshold Function**: This function ensures that each coefficient is adjusted to be zero if its magnitude is below the regularization strength, promoting sparsity in the model.\n\n4. **Early Stopping**: The algorithm stops iterating when the change in coefficients falls below a specified tolerance, ensuring efficient convergence.\n\nThis approach efficiently minimizes the Lasso objective function, providing a robust model that balances accuracy and regularization."}
{"task_id": 65, "completion_id": 0, "completion": "To solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The HMM is defined by its transition matrix `A`, emission matrix `B`, and initial distribution `pi`. The backward algorithm efficiently computes this probability by recursively calculating intermediate values known as beta values.\n\n### Approach\n1. **Check for Empty Inputs**: If any of the input lists (A, B, pi, or obs) are empty, return 0.0 immediately as no valid probability can be computed.\n2. **Initialize Beta Values**: The beta values represent the probability of the remaining part of the observation sequence given the current state. For the last observation, the beta value for each state is initialized to 1.0.\n3. **Compute Beta Values Recursively**: Starting from the second-to-last observation and moving backwards, compute the beta values for each state using the transition matrix `A` and emission matrix `B`.\n4. **Compute Final Probability**: After processing all observations, compute the final probability by summing the contributions from each state, considering the initial distribution `pi` and the computed beta values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to the nearest 6th decimal.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n    N = len(A)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n\n    # Initialize beta_prev for t = T-1\n    beta_prev = [1.0 for _ in range(N)]\n\n    for t in range(T-2, -1, -1):\n        current_obs = obs[t+1]\n        beta_current = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                beta_current[i] += A[i][j] * B[j][current_obs] * beta_prev[j]\n        beta_prev = beta_current\n\n    # Calculate the final probability\n    result = 0.0\n    for i in range(N):\n        result += pi[i] * B[i][obs[0]] * beta_prev[i]\n    return round(result, 6)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if any input list is empty and returns 0.0 if so. It then initializes the beta values for the last observation to 1.0.\n2. **Backward Calculation**: For each observation from the second last to the first, the function updates the beta values using the transition and emission probabilities. This is done iteratively, updating the beta values for each state based on the next observation.\n3. **Final Probability**: After processing all observations, the function computes the final probability by summing the contributions from each state, considering the initial distribution and the computed beta values. The result is then rounded to six decimal places.\n\nThis approach efficiently computes the probability using dynamic programming, ensuring that the solution is both time and space efficient."}
{"task_id": 69, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward algorithm for Hidden Markov Models (HMMs) to compute the likelihood of a given sequence of observations. The HMM is defined by its initial state probabilities, transition matrix, and emission matrix. The goal is to determine the probability that the model generates the exact sequence of observations provided.\n\n### Approach\n1. **Input Validation**: \n   - Ensure the initial state vector `S` is a valid probability vector.\n   - Ensure the transition matrix `A` is a valid square matrix.\n   - Ensure the emission matrix `B` is a valid matrix with the correct dimensions.\n   - Ensure the observations list is non-empty and each observation is within the valid range.\n\n2. **Forward Algorithm**:\n   - Initialize the current state vector with the initial state probabilities.\n   - For each observation in the sequence, update the state vector by first transitioning to the next state using the transition matrix `A`, then emitting the observation using the emission matrix `B`.\n   - After processing all observations, the sum of the current state vector gives the total probability of the sequence.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)\n    if n == 0:\n        return -1\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    m = len(B[0]) if B else 0\n    for row in B:\n        if len(row) != m:\n            return -1\n\n    # Validate observations\n    for obs in observations:\n        if not isinstance(obs, int):\n            return -1\n        if obs < 0 or obs >= m:\n            return -1\n\n    if len(observations) == 0:\n        return -1  # Empty sequence is invalid\n\n    # Initialize the current state vector\n    current_state = np.array(S, dtype=np.float64)\n\n    # Process each observation\n    for obs in observations:\n        current_state = current_state.dot(A)\n        current_state = current_state.dot(B)\n\n    # Return the total probability rounded to 4 decimal places\n    total_probability = np.sum(current_state)\n    return round(total_probability, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices and vectors have the correct dimensions and if the observations are valid. If any check fails, it returns -1.\n2. **State Initialization**: The initial state vector `current_state` is initialized with the given probabilities.\n3. **Processing Observations**: For each observation, the state vector is updated by first transitioning to the next state using the transition matrix `A` and then emitting the observation using the emission matrix `B`.\n4. **Result Calculation**: After processing all observations, the sum of the current state vector is computed, which represents the total probability of the sequence. This value is then rounded to four decimal places and returned.\n\nThis approach ensures that the function correctly computes the likelihood of the given sequence using the forward algorithm for HMMs, handling both valid and invalid inputs appropriately."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The goal is to predict class labels for each sample in an unlabeled test set based on the majority vote of the k closest labeled training samples. The solution must handle edge cases such as invalid values of k and ensure that ties in class labels are resolved by selecting the smallest label.\n\n### Approach\n1. **Input Validation**: Check if the value of k is a positive integer and does not exceed the number of training samples. If not, return -1.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. Using squared distance avoids the computational cost of taking square roots while preserving the order of distances.\n3. **Nearest Neighbors**: For each test sample, identify the k nearest neighbors based on the computed distances.\n4. **Majority Vote**: Collect the class labels of these k nearest neighbors and determine the most frequent label. In case of a tie, select the smallest label.\n5. **Result Compilation**: Store the predicted class labels for each test sample and return the results.\n\n### Solution Code\n```python\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is a positive integer and within valid range\n    if not isinstance(k, int) or k < 1 or k > len(X_train):\n        return -1\n    if len(X_train) == 0:\n        return -1  # No training samples to use\n    \n    result = []\n    for x_test in X_test:\n        distances = []\n        for x_train, y in zip(X_train, y_train):\n            # Compute squared Euclidean distance\n            dist = sum((x_test[i] - x_train[i])**2 for i in range(len(x_test)))\n            distances.append((dist, y))\n        # Sort the samples by distance\n        distances.sort()\n        # Take the first k samples\n        k_neighbors = distances[:k]\n        # Extract the class labels\n        y_values = [y for (d, y) in k_neighbors]\n        # Count frequency of each class\n        freq = {}\n        for y in y_values:\n            freq[y] = freq.get(y, 0) + 1\n        # Find the maximum frequency\n        max_freq = max(freq.values())\n        # Collect all classes with max frequency\n        candidates = [y for y in freq if freq[y] == max_freq]\n        # Choose the smallest class in case of tie\n        prediction = min(candidates)\n        result.append(prediction)\n    return result\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if k is a positive integer and within the valid range. If not, it returns -1.\n2. **Distance Calculation**: For each test sample, the function computes the squared Euclidean distance to each training sample, which is efficient and avoids the computational overhead of square root operations.\n3. **Nearest Neighbors**: The distances are sorted, and the k nearest neighbors are selected.\n4. **Majority Vote**: The class labels of these neighbors are collected, and the most frequent label is determined. In case of a tie, the smallest label is chosen.\n5. **Result Compilation**: The predicted labels for all test samples are compiled into a list and returned.\n\nThis approach ensures that the solution is both efficient and correct, handling all edge cases as specified."}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to compute the unnormalised categorical cross-entropy loss for a batch of one-hot encoded targets. The cross-entropy loss is a common loss function used in machine learning, especially in classification tasks. It measures the difference between the predicted probabilities and the actual labels, providing a way to train models to minimize this loss.\n\n### Approach\nThe categorical cross-entropy loss is calculated using the formula:\n\\[ L = -\\sum_{i=1}^{n\\_samples} \\sum_{j=1}^{n\\_classes} y_{ij} \\cdot \\log(y_{ij} + \\epsilon) \\]\nwhere \\( y \\) is the true class labels (one-hot encoded) and \\( y_{pred} \\) are the predicted probabilities. Here, \\( \\epsilon \\) is a small value added to avoid taking the logarithm of zero.\n\nThe steps to compute the loss are as follows:\n1. Convert the input arrays \\( y \\) and \\( y_{pred} \\) to numpy arrays if they are not already.\n2. Compute the sum of the product of each element in \\( y \\) with the logarithm of the corresponding element in \\( y_{pred} \\), adding a small epsilon to each element of \\( y_{pred} \\) to avoid taking the logarithm of zero.\n3. Sum all these values to get the total loss.\n4. Multiply the total loss by -1 to get the negative of the sum, which is the cross-entropy loss.\n5. Return the result rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    epsilon = 1e-10\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    sum_loss = np.sum(y * np.log(y_pred + epsilon))\n    L = -sum_loss\n    return round(L, 4)\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: The function first converts the input lists or numpy arrays to numpy arrays to handle them uniformly.\n- **Logarithm and Summation**: It computes the sum of the product of each true label (from the one-hot encoded matrix) with the logarithm of the corresponding predicted probabilities. This step ensures that each term in the sum contributes only when the true label is 1, effectively summing the log probabilities of the correct predictions.\n- **Negative Sum**: The sum is then negated to match the standard cross-entropy loss formula, which is a positive value.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the cross-entropy loss using vectorized operations, making it suitable for large batches of data."}
{"task_id": 77, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward propagation for an L-layer neural network. The network uses ReLU activation for all hidden layers and the sigmoid function for the output layer. The goal is to compute the activation values for each layer and return the final activation of the output layer.\n\n### Approach\n1. **Initialization**: Start with the input matrix `X` and initialize the activation values `A_prev` to `X`.\n2. **Iterate through each layer**: For each layer from 1 to L:\n   - Compute the linear transformation using the current layer's weights (`W`) and bias (`b`).\n   - Apply the appropriate activation function: ReLU for hidden layers and sigmoid for the output layer.\n   - Store the computed activation values and the activation function used for each layer in a cache.\n3. **Update the activation values**: After processing each layer, update `A_prev` to the activation values of the current layer.\n4. **Return the final activation**: After processing all layers, return the activation values of the output layer, rounded to 4 decimal places and converted to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    L = len(parameters) // 2  # Number of layers\n    A_prev = X\n    caches = []\n    \n    for i in range(1, L + 1):\n        W = parameters[f'W{i}']\n        b = parameters[f'b{i}']\n        Z = np.dot(W, A_prev) + b\n        \n        if i < L:\n            A = np.maximum(0, Z)\n            caches.append((Z, 'ReLU'))\n        else:\n            A = 1 / (1 + np.exp(-Z))\n            caches.append((Z, 'sigmoid'))\n        \n        A_prev = A\n    \n    AL = np.round(A, 4)\n    return AL.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by determining the number of layers `L` from the parameters dictionary. The input `X` is the initial activation values for the first layer.\n- **Layer Processing**: For each layer, the linear transformation is computed using the current weights and biases. The activation function is applied based on whether the layer is a hidden layer (ReLU) or the output layer (sigmoid).\n- **Caching**: Each layer's pre-activation values (`Z`) and the activation function used are stored in a cache for later use in backpropagation.\n- **Update and Return**: After processing all layers, the final activation values are rounded to 4 decimal places and converted to a list of lists to be returned as the result.\n\nThis approach ensures that the forward propagation is correctly computed for each layer, preparing the necessary information for the subsequent backpropagation step."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy (log-loss) cost between two NumPy arrays, A2 (predicted probabilities) and Y (ground-truth labels). The binary cross-entropy cost is a common loss function used in classification tasks, and it measures the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clipping Values**: To avoid taking the logarithm of zero or one, we first clip the values of A2 to the interval [\u03b5, 1-\u03b5], where \u03b5 is a very small value (1e-15). This ensures that no element in A2 is exactly 0 or 1.\n2. **Compute Logarithms**: For each element in A2, compute the logarithm of the clipped values. Multiply these logarithms by the corresponding elements in Y.\n3. **Compute the Remaining Terms**: For elements where Y is 0, compute the logarithm of (1 - A2) and multiply by 1 (since 1 - Y will be 1 in these cases).\n4. **Sum the Costs**: Sum all the computed terms to get the total cost.\n5. **Round the Result**: Finally, round the total cost to six decimal places for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    y_part = Y * np.log(A2_clipped)\n    one_minus_y_part = (1 - Y) * np.log(1 - A2_clipped)\n    cost = np.sum(y_part + one_minus_y_part)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The `np.clip` function ensures that all values in A2 are within the range [\u03b5, 1-\u03b5], preventing any log(0) or log(1) operations.\n- **Logarithms and Multiplication**: Using NumPy's `log` function, we compute the logarithms of the clipped values and multiply them by the corresponding elements in Y. For elements where Y is 0, we compute the logarithm of (1 - A2) and multiply by 1.\n- **Summing Costs**: The sum of these terms gives the total cost, which is then rounded to six decimal places for the final result.\n\nThis approach efficiently computes the binary cross-entropy cost while handling potential numerical issues by clipping the probabilities, ensuring the function works robustly for all valid inputs."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to implement an \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy (best) action. The policy is defined by specific probabilities based on the Q-values of the actions and an exploration parameter \u03b5.\n\n### Approach\nThe \u03b5-soft exploration policy is designed to balance exploration and exploitation. The key steps to implement this policy are as follows:\n\n1. **Identify the Greedy Action**: Determine the action with the highest Q-value. If there are multiple actions with the same highest Q-value, select the first occurrence (smallest index).\n2. **Calculate Base Probability**: Compute the base probability for non-greedy actions using the formula `base = \u03b5 / n`, where `n` is the number of actions.\n3. **Compute Probabilities**: For the greedy action, the probability is calculated as `1 - \u03b5 + base`. For all other actions, the probability is `base`.\n4. **Round Probabilities**: Each probability is rounded to four decimal places to ensure precision and meet the problem requirements.\n\n### Solution Code\n```python\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:"}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to aggregate predictions from multiple decision trees using a majority vote mechanism. Each decision tree provides a prediction for every sample in the dataset, and the final prediction for each sample is determined by the majority vote across all trees. In case of a tie, the smallest label is chosen based on its type (numeric or string).\n\n### Approach\n1. **Input Structure**: The input is a 2D list where each inner list represents the predictions of one decision tree for all samples.\n2. **Iterate Over Samples**: For each sample, collect predictions from all trees.\n3. **Count Votes**: For each sample, count the occurrences of each label.\n4. **Determine Majority**: Identify the label(s) with the highest count. If there's a tie, break it by choosing the smallest label:\n   - For numeric labels, choose the smallest numeric value.\n   - For string labels, use lexicographic order.\n5. **Construct Result**: Append the determined label to the result list for each sample.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    \n    num_samples = len(predictions[0])\n    result = []\n    \n    for i in range(num_samples):\n        sample = [tree[i] for tree in predictions]\n        label_type = type(sample[0])\n        counts = {}\n        for label in sample:\n            counts[label] = counts.get(label, 0) + 1\n        \n        max_count = max(counts.values())\n        tied_labels = [label for label, cnt in counts.items() if cnt == max_count]\n        \n        if len(tied_labels) == 1:\n            chosen_label = tied_labels[0]\n        else:\n            if isinstance(label_type, (int, float)):\n                # Sort numerically\n                sorted_labels = sorted(tied_labels, key=lambda x: float(x))\n            else:\n                # Sort lexicographically\n                sorted_labels = sorted(tied_labels)\n            chosen_label = sorted_labels[0]\n        \n        result.append(chosen_label)\n    \n    return result\n```\n\n### Explanation\n1. **Input Handling**: The function starts by checking if the input is empty and returns an empty list if true.\n2. **Sample Processing**: For each sample, it collects predictions from all trees and counts the occurrences of each label.\n3. **Vote Aggregation**: It determines the label(s) with the highest count. If there's a tie, it sorts the tied labels appropriately (numerically or lexicographically) and selects the smallest one.\n4. **Result Construction**: The chosen label for each sample is appended to the result list, which is returned at the end.\n\nThis approach ensures that the final prediction is the majority vote, with ties broken by the smallest label as specified."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the softplus activation function, which is a smooth approximation of the ReLU activation function. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). To avoid numerical issues such as overflow for very large positive numbers and underflow for very small negative numbers, we use the numerically stable helper function `np.logaddexp` provided by NumPy.\n\n### Approach\n1. **Problem Analysis**: The softplus function can be computed using the identity \\( \\text{softplus}(z) = \\ln(e^0 + e^z) \\), which can be efficiently calculated using `np.logaddexp(0.0, z)` to avoid overflow and underflow issues.\n2. **Handling Different Input Types**: The function should handle scalar, list, and NumPy array inputs. For scalar inputs, it returns a single float. For list or array inputs, it returns a list of the same shape.\n3. **Numerical Stability**: Using `np.logaddexp` ensures that the computation is numerically stable for both very large and very small values of `z`.\n4. **Rounding**: The result is rounded to 4 decimal places to meet the problem requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # Scalar case\n        result = np.logaddexp(0.0, z)\n        return round(result, 4)\n    else:\n        # Array-like case\n        z_array = np.asarray(z)\n        result = np.logaddexp(0.0, z_array)\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Input Handling**: If the input `z` is a scalar (int or float), the function computes the softplus value using `np.logaddexp(0.0, z)` and returns the result rounded to 4 decimal places as a float.\n2. **Array-like Input Handling**: If the input `z` is a list or a NumPy array, it is converted to a NumPy array. The softplus function is computed using `np.logaddexp(0.0, z_array)`, the result is rounded to 4 decimal places, and the output is converted to a list using `tolist()`.\n3. **Numerical Stability**: The use of `np.logaddexp` ensures that the computation remains stable for both very large and very small values of `z`, avoiding potential overflow or underflow issues.\n\nThis approach ensures that the function efficiently and accurately computes the softplus activation for a wide range of input types, providing the required precision and numerical stability."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate how good a policy's current estimates are by computing the mean-squared error (MSE) between the true expected payouts of each arm and the policy's estimates. The MSE is a measure of the difference between values predicted by a model and the actual values observed.\n\n### Approach\n1. **Extract Data**: Extract the true expected values from the bandit and the policy's estimates.\n2. **Check for Estimates**: Ensure the policy has provided estimates for at least one arm. If not, return `numpy.nan`.\n3. **Sort Indices**: Sort the arm indices based on the policy's estimates to ensure correct pairing with the bandit's true values.\n4. **Compute Squared Errors**: For each arm, compute the squared error between the true value and the policy's estimate.\n5. **Calculate MSE**: Compute the mean of these squared errors and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to the nearest 4th decimal place. If the policy\n        does not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Extract the true expected values from the bandit\n    true_values = bandit['arm_evs']\n    \n    # Extract the policy's estimates\n    estimates = policy['ev_estimates']\n    \n    # Check if the policy has provided any estimates\n    if not isinstance(estimates, dict) or len(estimates) == 0:\n        return np.nan\n    \n    # Get the sorted arm indices from the policy's estimates\n    sorted_indices = sorted(estimates.keys())\n    \n    # Calculate the squared errors for each corresponding arm\n    squared_errors = []\n    for idx in sorted_indices:\n        true_val = true_values[idx]\n        est_val = estimates[idx]\n        squared_errors.append((est_val - true_val) ** 2)\n    \n    # Compute the mean of the squared errors\n    mse_value = np.mean(squared_errors)\n    \n    # Round to four decimal places\n    return round(mse_value, 4)\n```\n\n### Explanation\n1. **Extract Data**: The function starts by extracting the true expected values from the bandit and the policy's estimates.\n2. **Check for Estimates**: It checks if the policy has provided any estimates. If not, it returns `numpy.nan`.\n3. **Sort Indices**: The arm indices from the policy's estimates are sorted to ensure they match the order of the bandit's true values.\n4. **Compute Squared Errors**: For each arm, the squared error between the true value and the policy's estimate is computed and stored.\n5. **Calculate MSE**: The mean of these squared errors is computed and rounded to four decimal places to get the final MSE value.\n\nThis approach ensures that we accurately measure how well the policy's estimates match the true expected values, providing a clear metric for evaluation."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement an N-gram language model that computes the additive-smoothed log-probability of a given sentence using a provided corpus. The solution involves building frequency tables for all possible N-grams, applying additive smoothing, and then computing the log probability of the target sentence.\n\n### Approach\n1. **Collect Unique Words**: Extract all unique words from the provided corpus and include an additional \"<UNK>\" token to handle unseen words.\n2. **Build N-gram Frequency Tables**: For each sentence in the corpus, generate all possible 1-gram, 2-gram, ..., N-gram sequences and count their occurrences.\n3. **Process the Target Sentence**: For each contiguous N-gram in the target sentence, compute its probability using additive smoothing. Sum the log probabilities of all these N-grams to get the final result.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Collect all unique words in the corpus\n    all_words = set()\n    for sentence in corpus:\n        words = sentence.split()\n        all_words.update(words)\n    \n    # Add the unknown word token\n    vocabulary = all_words.union({'<UNK>')\n    vocab_size = len(vocabulary)\n    \n    # Build n-gram frequency tables\n    counters = [defaultdict(int) for _ in range(N+1)]\n    for sentence in corpus:\n        words = sentence.split()\n        for n in range(1, N+1):\n            for i in range(len(words) - n + 1):\n                ngram = ' '.join(words[i:i+n])\n                counters[n][ngram] += 1\n    \n    # Process the target sequence\n    target_words = sequence.split()\n    if len(target_words) < N:\n        return 0.0\n    \n    total_log = 0.0\n    for i in range(len(target_words) - N + 1):\n        ngram = ' '.join(target_words[i:i+N])\n        count = counters[N].get(ngram, 0)\n        prob = (count + K) / (count + K * vocab_size)\n        total_log += math.log(prob)\n    \n    return round(total_log, 4)\n```\n\n### Explanation\n1. **Collect Unique Words**: The code first gathers all unique words from the provided corpus and adds the \"<UNK>\" token to handle unseen words.\n2. **Build N-gram Frequency Tables**: For each sentence, the code generates all possible N-grams and updates their counts in respective frequency tables.\n3. **Process Target Sentence**: The target sentence is split into words. For each N-gram in the sentence, the code retrieves its count from the frequency table, applies additive smoothing, computes the log probability, and accumulates the total log probability.\n4. **Return Result**: The accumulated log probability is rounded to four decimal places and returned.\n\nThis approach ensures that we efficiently compute the additive-smoothed log probability of the target sentence using the provided N-gram model."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to factorize a given matrix \\( X \\) into two low-rank matrices \\( W \\) and \\( H \\) using the regularized Alternating Least Squares (ALS) algorithm. The goal is to minimize the regularized Frobenius reconstruction loss, which includes both the reconstruction error and a regularization term to prevent overfitting.\n\n### Approach\nThe regularized ALS algorithm works by alternating between updating \\( W \\) and \\( H \\) to minimize the loss function. The loss function is defined as:\n\\[ \\|X - WH\\|_F^2 + \\alpha(\\|W\\|_F^2 + \\|H\\|_F^2) \\]\nwhere \\( \\| \\cdot \\|_F \\) denotes the Frobenius norm, and \\( \\alpha \\) is a regularization weight.\n\n1. **Initialization**: We start by initializing the matrices \\( W \\) and \\( H \\) with random values, using a fixed random seed for reproducibility.\n2. **Iterative Update**: We iteratively update \\( W \\) and \\( H \\) in alternating fashion:\n   - For each update, we solve a least squares problem with the regularization term included.\n   - When updating \\( W \\), we fix \\( H \\) and solve for \\( W \\) using the matrix equation derived from the loss function.\n   - When updating \\( H \\), we fix \\( W \\) and solve for \\( H \\) similarly.\n3. **Stopping Condition**: The algorithm stops when the loss drops below a specified tolerance or the maximum number of iterations is reached.\n4. **Reconstruction**: After the algorithm converges, we reconstruct the matrix \\( X \\) using the best obtained \\( W \\) and \\( H \\), and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    K = int(K)\n    \n    # Initialize W and H\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n    \n    # Compute initial loss\n    X_hat_initial = W @ H\n    initial_loss = np.sum((X - X_hat_initial)**2) + alpha * (np.sum(W**2) + np.sum(H**2))\n    best_loss = initial_loss\n    best_W = W.copy()\n    best_H = H.copy()\n    \n    for iteration in range(max_iter):\n        # Update W\n        A = H @ H.T + alpha * np.identity(K)\n        b = X @ H.T\n        W_new = np.linalg.lstsq(A, b, rcond=None)[0]\n        \n        # Update H\n        A = W @ W.T + alpha * np.identity(K)\n        b = W @ X\n        H_new = np.linalg.lstsq(A, b, rcond=None)[0]\n        \n        # Compute new loss\n        X_hat = W_new @ H_new\n        new_loss = np.sum((X - X_hat)**2) + alpha * (np.sum(W_new**2) + np.sum(H_new**2))\n        \n        # Check if this is the best so far\n        if new_loss < best_loss:\n            best_loss = new_loss\n            best_W = W_new\n            best_H = H_new\n            if best_loss < tol:\n                break\n        else:\n            # No improvement, update W and H for next iteration\n            W = W_new\n            H = H_new\n    \n    # Compute the best reconstruction\n    X_hat = best_W @ best_H\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    \n    return X_hat_rounded\n```\n\n### Explanation\n1. **Initialization**: The matrices \\( W \\) and \\( H \\) are initialized with random values, ensuring reproducibility by fixing the random seed.\n2. **Iterative Update**: In each iteration, we update \\( W \\) and \\( H \\) alternately. For each update, we solve a least squares problem with the regularization term included to ensure the solution is stable.\n3. **Loss Calculation**: The loss is computed after each update to check for convergence. If the loss drops below the specified tolerance, the algorithm stops early.\n4. **Reconstruction**: After the algorithm converges, the best \\( W \\) and \\( H \\) are used to reconstruct \\( X \\), and the result is rounded to 4 decimal places for the final output.\n\nThis approach ensures that the factorization minimizes the regularized Frobenius loss, providing a robust and stable solution to the matrix factorization problem."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means++ clustering algorithm with K-Means++ initialization. The goal is to cluster a given dataset into K clusters and return the centroids of these clusters after the algorithm completes. The centroids should be rounded to four decimal places and sorted in ascending order by their first coordinate.\n\n### Approach\n1. **Set Random Seeds**: Both Python's random and NumPy's random generators are set using the provided random_state to ensure reproducibility.\n2. **Initialize Centroids**: Using the K-Means++ procedure, we initialize the centroids. This involves selecting the sample with the maximum distance to all other samples for the first centroid, and for subsequent centroids, selecting the sample with the maximum distance to all existing centroids.\n3. **Perform Lloyd's Iterations**: This involves repeatedly assigning each sample to the nearest centroid and updating each centroid to be the mean of the samples in its cluster. This process continues until either the cluster assignments stop changing or the maximum number of iterations is reached.\n4. **Handle Empty Clusters**: If a cluster becomes empty during the update step, its centroid remains unchanged.\n5. **Return Centroids**: After all iterations, the centroids are rounded to four decimal places and sorted in ascending order by their first coordinate, with the entire centroid tuple used as a secondary key for tie-breaking.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    # Set random seeds for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    \n    # Initialize centroids using K-Means++ procedure\n    centroids = []\n    for _ in range(K):\n        if not centroids:\n            # Choose the sample with the maximum distance to all others\n            distances = np.linalg.norm(X - X[0], axis=1)\n            selected = np.argmax(distances)\n        else:\n            # Choose the sample with the maximum distance to all current centroids\n            distances = np.linalg.norm(X - centroids, axis=1)\n            selected = np.argmax(distances)\n        centroids.append(X[selected, :])\n    \n    # Perform Lloyd's iterations\n    previous_assignments = None\n    for _ in range(max_iter):\n        # Compute the distance from each sample to current centroids\n        distances = np.linalg.norm(X - centroids, axis=1)\n        assignments = np.argmin(distances, axis=0)\n        \n        # Check if assignments have changed\n        if previous_assignments is not None:\n            if np.array_equal(assignments, previous_assignments):\n                break\n        previous_assignments = assignments\n        \n        # Compute new centroids\n        new_centroids = []\n        for j in range(K):\n            mask = (assignments == j)\n            if np.sum(mask) == 0:\n                new_centroids.append(centroids[j])\n            else:\n                selected_samples = X[mask]\n                new_centroid = np.mean(selected_samples, axis=0)\n                new_centroids.append(new_centroid)\n        \n        # Check if centroids have changed\n        if np.array_equal(new_centroids, centroids):\n            break\n        centroids = new_centroids\n    \n    # Round and sort the centroids\n    rounded = [list(np.round(centroid, 4)) for centroid in centroids]\n    rounded.sort(key=lambda x: x)\n    \n    return rounded\n```\n\n### Explanation\n1. **Random Seeds**: The function sets the random seeds for both Python's random module and NumPy's random generator using the provided random_state to ensure that the results are reproducible.\n2. **Centroid Initialization**: The K-Means++ initialization is implemented by iteratively selecting the sample with the maximum distance to all existing centroids until K centroids are chosen.\n3. **Lloyd's Iterations**: The algorithm repeatedly assigns each sample to the nearest centroid and updates each centroid to be the mean of the samples in its cluster. This continues until the assignments do not change or the maximum number of iterations is reached.\n4. **Empty Clusters Handling**: If a cluster becomes empty during the update step, its centroid remains unchanged.\n5. **Rounding and Sorting**: The final centroids are rounded to four decimal places and sorted in ascending order by their first coordinate, with the entire centroid tuple used as a secondary key for tie-breaking.\n\nThis approach ensures that the K-Means++ algorithm is implemented efficiently and correctly, providing the desired clustering result."}
{"task_id": 111, "completion_id": 0, "completion": "To solve this problem, we need to create three functions for character-level sequence encoding and decoding. These functions will convert raw text into a fixed-length list of integer token IDs, process these IDs through a neural model, and then convert the predicted IDs back into a human-readable string.\n\n### Approach\n1. **string_to_int**: This function converts each character of the input string into an integer using a provided vocabulary. The output is a fixed-length list of integers, padded or truncated to the specified length (TIME_STEPS). Characters not found in the vocabulary are replaced with a padding token (0).\n\n2. **int_to_string**: This function converts a list of integer IDs back into a string, ignoring any padding tokens (0s). It uses an inverse vocabulary to map each integer to its corresponding character.\n\n3. **run_example**: This function combines the encoding and decoding processes. It encodes the input text, runs it through a neural model, decodes the predicted IDs, and returns the resulting string.\n\n### Solution Code\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a raw text string into a fixed-length list of integer token ids.\"\"\"\n    encoded = []\n    for char in text:\n        if char in vocabulary:\n            encoded.append(vocabulary[char])\n        else:\n            encoded.append(0)\n    # Pad or truncate to TIME_STEPS\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    filtered_indices = [i for i in indices if i != 0]\n    return ''.join([inverse_vocab[i] for i in filtered_indices])\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on the input text and returns the decoded prediction.\"\"\"\n    # Encode the input text into integer tokens\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Reshape the encoded list to (1, TIME_STEPS) for model input\n    encoded_array = np.array(encoded, dtype=np.int64).reshape(1, -1)\n    # Get model predictions\n    predicted = model.predict(encoded_array)\n    # Extract the predicted indices for each time step\n    predicted_ids = np.argmax(predicted, axis=1)\n    # Decode the predicted indices into a string, ignoring padding tokens\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n1. **string_to_int**:\n   - Iterates through each character of the input string.\n   - Converts each character to its corresponding integer using the vocabulary. If a character is not found, it uses 0.\n   - Ensures the output list is exactly TIME_STEPS long by padding with zeros if necessary or truncating if too long.\n\n2. **int_to_string**:\n   - Takes a list of integers and converts them back into characters using the inverse vocabulary.\n   - Ignores any padding tokens (0s) to form the final string.\n\n3. **run_example**:\n   - Encodes the input text into integer tokens.\n   - Converts the encoded list into a numpy array with the correct shape for the model.\n   - Predicts the model's output for the encoded input.\n   - Converts the predicted integer IDs back into a string, ignoring any padding tokens, to get the final decoded text."}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to create a function that automates the process of running a trained model on multiple input strings and collecting the predictions. This is particularly useful in sequence-to-sequence or language generation projects where we want to quickly see the model's predictions on various inputs.\n\n### Approach\nThe function `run_examples` is designed to handle the following tasks:\n1. **Input Handling**: The function takes a model, input vocabulary, inverse output vocabulary, and an iterable of input strings. If the input strings are not provided, it falls back to a global constant `EXAMPLES`.\n2. **Model Execution**: For each input string, the function calls another helper function `run_example` to get the model's predictions.\n3. **Prediction Processing**: The predictions from `run_example` are processed into a single string.\n4. **Output Collection**: Each prediction is printed in the format \"input: [input string] and output: [predicted string]\", and the predicted string is collected into a list.\n5. **Return Result**: The list of predicted strings is returned.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    outputs = []\n    for example in examples:\n        prediction_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        prediction = ''.join(prediction_chars)\n        print(f\"input: {example}\")\n        print(f\"output: {prediction}\")\n        outputs.append(prediction)\n    return outputs\n```\n\n### Explanation\n1. **Function Definition**: The function `run_examples` is defined with four parameters: `model`, `input_vocabulary`, `inv_output_vocabulary`, and `examples`. The `examples` parameter defaults to `None` if not provided.\n2. **Fallback to Global Constant**: If `examples` is `None`, the function uses the global constant `EXAMPLES`.\n3. **Loop Through Examples**: For each input string in the examples, the function calls `run_example` to get the model's predictions.\n4. **Process Predictions**: The predictions are converted from a list of characters into a single string.\n5. **Print and Collect**: Each prediction is printed in the specified format, and the predicted string is added to the output list.\n6. **Return Result**: The list of predicted strings is returned after processing all examples.\n\nThis approach ensures that the function efficiently processes each example, provides clear output, and returns the results in the required format."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute both the average binary cross-entropy loss and its gradient for logistic regression given a feature matrix, a binary target vector, and a weight vector. The logistic model predicts the probability that a sample belongs to the positive class, and we use this probability to compute the loss and its gradient.\n\n### Approach\n1. **Compute the Probability**: For each sample, compute the probability `p` using the logistic function. This probability is given by `p = 1 / (1 + exp(-z))`, where `z` is the dot product of the feature vector and the weight vector.\n2. **Handle Numerical Issues**: To avoid numerical issues when `p` is exactly 0 or 1, replace these values with a small constant `\u03b5 = 1e-20`.\n3. **Compute Loss**: The average binary cross-entropy loss is computed using the formula `J(w) = -1/m * \u03a3 [y \u00b7 ln(p) + (1 \u2013 y) \u00b7 ln(1 \u2013 p)]`, where `m` is the number of samples.\n4. **Compute Gradient**: The gradient of the loss with respect to the weights is given by `(1/m) * X\u1d40 (p \u2013 y)`, where `X\u1d40` is the transpose of the feature matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    \n    # Compute loss\n    log_p = np.log(p)\n    log_1m_p = np.log(1 - p)\n    loss = (-1/m) * (np.sum(y * log_p + (1 - y) * log_1m_p))\n    \n    # Compute gradient\n    gradient = (X.T.dot(p - y)) / m\n    \n    # Round and convert to required format\n    loss_rounded = round(loss, 4)\n    gradient_rounded = np.round(gradient, 4)\n    gradient_list = gradient_rounded.reshape(-1, 1).tolist()\n    \n    return (loss_rounded, gradient_list)\n```\n\n### Explanation\n1. **Probability Calculation**: The probability `p` for each sample is computed using the logistic function. This probability is adjusted to avoid numerical issues by clipping values close to 0 or 1.\n2. **Loss Calculation**: The loss is computed by summing the contributions from each sample and then averaging over all samples. This ensures that the loss is on a scale that is easier to optimize.\n3. **Gradient Calculation**: The gradient is computed using the transpose of the feature matrix to ensure that each weight update correctly reflects the contribution of all samples. This gradient is then scaled by the number of samples to maintain the average gradient.\n\nThis approach efficiently computes both the loss and gradient for logistic regression, ensuring numerical stability and correct optimization."}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost is an ensemble method that iteratively builds a set of weak classifiers (decision stumps) to improve classification performance. Each decision stump is a one-level decision tree, which is a simple threshold-based classifier.\n\n### Approach\n1. **Initialization**: Start with equal weights for all training examples.\n2. **Iterative Training**: For each iteration, find the decision stump that minimizes the weighted classification error. This involves:\n   - Sorting the training data based on each feature.\n   - Trying all possible splits for each feature to find the one that results in the minimum error.\n   - Selecting the feature with the smallest error and computing the corresponding weight (alpha_t) using the formula: \n     \\[\n     \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n     \\]\n   - Updating the weights based on the error of the selected stump.\n3. **Prediction**: After training all stumps, predict the class for each test sample by summing the weighted outputs of all stumps and taking the sign of the sum.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf: Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    m = X_train.shape[0]\n    if m == 0:\n        return []\n    n = X_train.shape[1]\n    \n    # Initialize weights\n    weights = np.ones(m) / m\n    classifiers = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature = -1\n        best_split = 0\n        \n        for j in range(n):\n            # Sort the data based on the current feature\n            sorted_indices = np.argsort(X_train[:, j])\n            sorted_weights = [weights[i] for i in sorted_indices]\n            \n            # Try all possible splits for the current feature\n            min_error_j = float('inf')\n            best_split_j = 0\n            for i in range(len(sorted_weights) - 1):\n                # Split after the i-th element\n                left_y = y_train[sorted_indices[:i+1]]\n                left_weights = sorted_weights[:i+1]\n                right_y = y_train[sorted_indices[i+1:]]\n                right_weights = sorted_weights[i+1:]\n                \n                # Calculate the error for this split\n                error = 0.0\n                for k in range(i+1):\n                    if left_y[k] == 1:\n                        error += left_weights[k]\n                for k in range(len(right_weights)):\n                    if right_y[k] == -1:\n                        error += right_weights[k]\n                \n                if error < min_error_j:\n                    min_error_j = error\n                    best_split_j = i + 1\n            \n            # Update the best feature and split if current feature is better\n            if min_error_j < min_error:\n                min_error = min_error_j\n                best_feature = j\n                best_split = best_split_j\n        \n        # Determine the threshold for the best split\n        threshold = X_train[sorted_indices[best_split - 1], best_feature]\n        # Calculate alpha_t\n        epsilon_t = min_error\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        \n        # Add the classifier to the list\n        classifiers.append((alpha_t, threshold))\n        \n        # Update the weights based on the current stump's performance\n        for i in range(m):\n            if (X_train[i, best_feature] >= threshold) != y_train[i]:\n                weights[i] *= np.exp(2 * alpha_t)\n    \n    # Predict using all the trained classifiers\n    predictions = []\n    for x in X_test:\n        total = 0.0\n        for alpha, threshold in classifiers:\n            if x >= threshold:\n                total += alpha\n            else:\n                total -= alpha\n        if total >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for each training example are initialized to 1/m, where m is the number of samples.\n2. **Training Each Stump**: For each iteration, the algorithm evaluates each feature to find the split that minimizes the weighted error. The error is calculated by considering all possible splits for each feature and selecting the one with the smallest error.\n3. **Weight Update**: After selecting the best split, the weights are updated to reflect the importance of each sample based on the error. Samples that are misclassified receive a higher weight to emphasize their importance in future iterations.\n4. **Prediction**: After training all stumps, each test sample is classified by summing the weighted outputs of all stumps and taking the sign of the sum to determine the predicted class.\n\nThis approach ensures that the AdaBoost algorithm effectively combines multiple weak classifiers to achieve better classification performance."}
{"task_id": 128, "completion_id": 0, "completion": "To solve this problem, we need to implement the classical Fisher\u2019s Linear Discriminant Analysis (LDA) for a multi-class scenario. The goal is to find a linear projection that maximizes the between-class scatter while minimizing the within-class scatter. This involves computing several key matrices and performing eigenvalue decomposition to derive the projection directions.\n\n### Approach\n1. **Compute Within-class Scatter Matrix (S_W)**: This matrix is calculated by summing the covariance matrices of each class.\n2. **Compute Between-class Scatter Matrix (S_B)**: This matrix is derived by summing the covariance matrices of the class means relative to the global mean, scaled by the number of samples in each class.\n3. **Form Matrix A**: This matrix is computed as the product of the pseudo-inverse of S_W and S_B to solve the generalized eigenvalue problem.\n4. **Eigenvalue Decomposition**: Decompose matrix A to get eigenvalues and eigenvectors. Sort these eigenvalues in descending order and adjust the sign of eigenvectors to ensure determinism.\n5. **Project Data**: Use the top n_components eigenvectors to project the data and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components == 0:\n        return []\n    \n    m, d = X.shape\n    if d == 0:\n        return []\n    \n    # Compute the global mean\n    mu = np.mean(X, axis=0)\n    \n    # Compute within-class scatter matrix S_W\n    S_W = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = (y == c)\n        mu_c = np.mean(X[mask], axis=0)\n        sum_ww = np.cov(X[mask], rowvar=False)\n        S_W += sum_ww\n    \n    # Compute between-class scatter matrix S_B\n    S_B = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = (y == c)\n        mu_c = np.mean(X[mask], axis=0)\n        n_c = np.sum(mask)\n        diff = mu_c - mu\n        S_B += n_c * (diff[:, np.newaxis] @ diff[:, np.newaxis].T)\n    \n    # Compute A = S_W^{-1} S_B using pseudo-inverse to handle singularity\n    try:\n        A = np.linalg.pinv(S_W) @ S_B\n    except np.linalg.LinAlgError:\n        A = np.linalg.pinv(S_W) @ S_B\n    \n    # Eigen-decomposition of A\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    # Sort eigenvalues and eigenvectors in descending order\n    eigenvalues = eigenvalues[::-1]\n    eigenvectors = eigenvectors[::-1]\n    \n    # Determine the sign of each eigenvector\n    for i in range(len(eigenvectors)):\n        vec = eigenvectors[i]\n        first_non_zero = np.argmax(np.abs(vec) > 1e-9)\n        if first_non_zero != 0:\n            if vec[first_non_zero] < 0:\n                eigenvectors[i] = -vec\n    \n    # Take the top n_components eigenvectors\n    W = eigenvectors[:n_components]\n    \n    # Project X onto the eigenvectors\n    projected = X @ W\n    # Round to four decimal places\n    projected_rounded = np.round(projected, 4)\n    \n    # Convert to list of lists\n    return projected_rounded.tolist()\n```\n\n### Explanation\n1. **Within-class Scatter Matrix (S_W)**: This matrix captures the variance within each class by summing the covariance matrices of each class.\n2. **Between-class Scatter Matrix (S_B)**: This matrix captures the variance between different classes by summing the covariance matrices of the differences between class means and the global mean, scaled by the number of samples in each class.\n3. **Matrix A**: This matrix is formed to solve the generalized eigenvalue problem, which helps in finding the optimal projection directions.\n4. **Eigenvalue Decomposition**: This step provides the projection directions (eigenvectors) that maximize the between-class variance.\n5. **Projection and Rounding**: The data is projected onto the top n_components eigenvectors, and the results are rounded to four decimal places for the final output.\n\nThis approach ensures that the projection maximizes the separation between classes while minimizing within-class variation, providing an effective dimensionality reduction technique."}
{"task_id": 140, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to navigate an \\(n \\times n\\) maze where the agent starts at the upper-left corner and aims to reach the lower-right corner while avoiding fire. The fire spreads every minute, and the agent moves after the fire has spread. The goal is to determine the minimum number of minutes required for the agent to reach the goal safely, or determine if it is impossible.\n\n### Approach\n1. **Initial Checks**: \n   - If the start or goal cell is on fire, return -1 immediately as the agent cannot start or finish.\n   - If the maze is a single cell and it is free ('.'), return 0.\n\n2. **Compute Fire Spread Times**:\n   - Use a multi-source BFS to compute the earliest time each cell catches fire. This helps in determining the state of the fire at any given time.\n\n3. **Agent BFS**:\n   - Use BFS to explore the agent's movement, tracking the current position and time. The agent can only move to cells that are not on fire at the current time.\n\n### Solution Code\n```python\nfrom collections import deque\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    if grid[0][0] == 'F' or grid[-1][-1] == 'F':\n        return -1\n    if n == 1:\n        return 0\n\n    # Compute fire_time for each cell\n    fire_time = [[float('inf')] * n for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    \n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        i, j = q.popleft()\n        for dx, dy in directions:\n            ni, nj = i + dx, j + dy\n            if 0 <= ni < n and 0 <= nj < n:\n                if fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n\n    # BFS for the agent\n    visited = [[float('inf')] * n for _ in range(n)]\n    visited[0][0] = 0\n    q = deque()\n    q.append((0, 0, 0))\n\n    while q:\n        x, y, t = q.popleft()\n        if x == n - 1 and y == n - 1:\n            return t\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n:\n                if grid[nx][ny] != '#' and fire_time[nx][ny] > t:\n                    new_t = t + 1\n                    if new_t < visited[nx][ny]:\n                        visited[nx][ny] = new_t\n                        q.append((nx, ny, new_t))\n    \n    return -1\n```\n\n### Explanation\n1. **Initial Checks**: The function first checks if the start or goal is on fire, returning -1 if true. For a single-cell maze, it returns 0 if the cell is free.\n\n2. **Fire Spread Calculation**: Using BFS, the function calculates the earliest time each cell catches fire, starting from all initial fire cells ('F'). This helps in determining the state of the fire at any given time.\n\n3. **Agent Movement**: The agent's BFS explores possible moves, ensuring each move is to a cell that is not on fire at the current time. The BFS tracks the minimum time to reach each cell, ensuring the shortest path is found efficiently.\n\nThis approach ensures that the agent navigates the maze safely while considering the dynamic spread of fire, providing an optimal solution to the problem."}
{"task_id": 141, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier as a single function. The function will take a labeled training set and an unlabeled test set, and it will predict labels for the test set based on the nearest neighbors in the training set using one of three distance metrics: Euclidean, Manhattan, or Cosine.\n\n### Approach\n1. **Determine the Metric**: The function first checks if the provided metric is valid. If not, it defaults to the Euclidean metric.\n2. **Compute Distances**: For each test sample, compute the distance to every training sample using the specified metric.\n3. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances.\n4. **Predict Labels**: Determine the most frequent label among the k nearest neighbors. In case of a tie, the smallest label is chosen.\n5. **Return Predictions**: Collect the predicted labels for all test samples and return them as a numpy array.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement your code here.\"\"\"\n    # Determine the metric to use\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    predicted_labels = []\n    \n    for x in X_test:\n        dists = []\n        for x_train in X_train:\n            if metric == 'manhattan':\n                dist = np.linalg.norm(x - x_train, ord=1)\n            elif metric == 'cosine':\n                dot = np.dot(x, x_train)\n                norm_x = np.linalg.norm(x, ord=2)\n                norm_train = np.linalg.norm(x_train, ord=2)\n                if norm_x == 0:\n                    cosine_sim = 0.0\n                else:\n                    cosine_sim = dot / (norm_x * norm_train)\n                dist = 1 - cosine_sim\n            else:\n                dist = np.linalg.norm(x - x_train, ord=2)\n            dists.append(dist)\n        \n        # Find the k nearest neighbors\n        paired = list(zip(dists, range(len(X_train))))\n        paired.sort()\n        top_k = paired[:k]\n        indices = [idx for (d, idx) in top_k]\n        labels = y_train[indices]\n        \n        # Determine the most common label, tiebreaker is smallest label\n        counts = Counter(labels)\n        max_count = max(counts.values())\n        candidates = [label for label, count in counts.items() if count == max_count]\n        candidates.sort()\n        predicted_label = candidates[0]\n        predicted_labels.append(predicted_label)\n    \n    return np.array(predicted_labels)\n```\n\n### Explanation\n1. **Metric Handling**: The function starts by validating the provided metric and defaulting to Euclidean if an unknown metric is provided.\n2. **Distance Calculation**: For each test sample, the function calculates the distance to each training sample using the specified metric. Euclidean distance uses the L2 norm, Manhattan distance uses the L1 norm, and Cosine distance is computed as 1 minus the cosine similarity.\n3. **Nearest Neighbor Selection**: The function pairs each test sample with its distances to training samples, sorts these pairs by distance, and selects the top k neighbors.\n4. **Label Prediction**: The function then extracts the labels of these k neighbors, counts their occurrences, and determines the most frequent label. In case of a tie, the smallest label is chosen.\n5. **Result Compilation**: The predicted labels for all test samples are compiled into a numpy array and returned as the final result."}
{"task_id": 146, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier from scratch. The k-NN algorithm is a non-parametric method used for classification tasks. It works by finding the k closest samples from the training set for each test sample and then predicting the class of the test sample based on the majority vote of these k neighbors.\n\n### Approach\n1. **Compute Distances**: For each test sample, compute the distance to every training sample using the specified distance metric (Euclidean, Manhattan, or Cosine).\n2. **Find Closest Neighbors**: Determine the k closest neighbors based on the computed distances. If k is larger than the number of training samples, use all samples.\n3. **Majority Vote**: Collect the labels of these k neighbors and perform a majority vote to predict the class of the test sample. In case of a tie, return the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    n_samples = X.shape[0]\n    m_samples = X_test.shape[0]\n    predicted = []\n    \n    for x_test in X_test:\n        if metric == 'euclidean':\n            distances = np.linalg.norm(x_test - X, axis=0)\n        elif metric == 'manhattan':\n            x_test_reshaped = x_test.reshape(1, -1)\n            manhattan_distances = np.sum(np.abs(x_test_reshaped - X), axis=1)\n            distances = manhattan_distances\n        elif metric == 'cosine':\n            # Compute dot product\n            dot = np.dot(x_test, X)\n            # Compute norms\n            norm_x = np.linalg.norm(x_test, axis=0)\n            norm_X = np.linalg.norm(X, axis=0)\n            denominator = norm_x * norm_X + 1e-12\n            distances = 1 - (dot / denominator)\n        else:\n            raise ValueError(\"Invalid metric\")\n        \n        # Get indices of training samples\n        indices = np.arange(n_samples)\n        \n        # Combine distances and indices\n        dist_indices = sorted(zip(distances, indices), key=lambda x: x[0])\n        \n        # Determine number of neighbors to take\n        n = min(k, n_samples)\n        top_k = dist_indices[:n]\n        \n        # Collect labels\n        labels = [y[i] for i, d in top_k]\n        \n        # Count frequencies\n        counts = defaultdict(int)\n        for label in labels:\n            counts[label] += 1\n        \n        # Find the most frequent label, smallest in case of tie\n        if not counts:\n            predicted_label = None\n        else:\n            max_count = max(counts.values())\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            if len(candidates) == 1:\n                predicted_label = candidates[0]\n            else:\n                predicted_label = min(candidates)\n        \n        predicted.append(predicted_label)\n    \n    return predicted\n```\n\n### Explanation\n1. **Distance Calculation**: The function computes the distance from each test sample to every training sample using the specified metric. For Euclidean distance, it uses `np.linalg.norm`. For Manhattan distance, it reshapes the test sample to match the training set's shape and computes the sum of absolute differences. For Cosine distance, it computes the dot product and norms, then applies the formula to avoid division by zero.\n2. **Finding Neighbors**: The function sorts the training samples based on their distances to the test sample and selects the top k samples. If k is larger than the number of training samples, it uses all samples.\n3. **Majority Vote**: The function collects the labels of the top k neighbors, counts their frequencies, and determines the most frequent label. In case of a tie, it returns the smallest label.\n\nThis approach ensures that the k-NN algorithm is implemented efficiently and correctly, handling different distance metrics and ties appropriately."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a two-hidden-layer actor network using NumPy. The network architecture is designed to convert an environment state into an action through a series of linear transformations and nonlinear activations.\n\n### Approach\nThe network topology is as follows:\n1. **State**: A one-dimensional vector representing the current state of the environment.\n2. **First Layer**: A linear transformation followed by a ReLU (Rectified Linear Unit) activation.\n3. **Second Layer**: Another linear transformation followed by a ReLU activation.\n4. **Output Layer**: A linear transformation followed by a tanh (hyperbolic tangent) activation, which ensures the output is within the range [-1, 1].\n\nThe steps to compute the forward pass are:\n1. Convert the input state to a NumPy array.\n2. Apply the first linear transformation using the first weight matrix and bias vector, followed by ReLU.\n3. Apply the second linear transformation using the second weight matrix and bias vector, followed by ReLU.\n4. Apply the third linear transformation using the third weight matrix and bias vector, followed by tanh.\n5. Round the resulting action vector to four decimal places and return it as a list of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is: Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    \n    # First affine transformation and ReLU\n    output1 = np.dot(state, weights['W1']) + weights['b1']\n    output1 = np.relu(output1)\n    \n    # Second affine transformation and ReLU\n    output2 = np.dot(output1, weights['W2']) + weights['b2']\n    output2 = np.relu(output2)\n    \n    # Third affine transformation and tanh\n    action_before_tanh = np.dot(output2, weights['W3']) + weights['b3']\n    action = np.tanh(action_before_tanh)\n    \n    # Round to four decimal places\n    action = [round(x, 4) for x in action]\n    \n    return action\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted to a NumPy array to facilitate matrix operations.\n2. **First Layer**: The state is multiplied by the first weight matrix (`W1`) and added to the first bias vector (`b1`). The result is then passed through a ReLU function.\n3. **Second Layer**: The output from the first layer is multiplied by the second weight matrix (`W2`) and added to the second bias vector (`b2`). This result is then passed through another ReLU function.\n4. **Output Layer**: The output from the second layer is multiplied by the third weight matrix (`W3`) and added to the third bias vector (`b3`). This pre-activation is then passed through a tanh function to produce the final action.\n5. **Rounding**: The resulting action vector is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the network correctly transforms the state into an action through the specified layers and activations, providing a continuous output suitable for many reinforcement learning algorithms."}
{"task_id": 165, "completion_id": 0, "completion": "To solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a metric used to measure the similarity between two strings of equal length, where the distance is the proportion of positions at which the corresponding elements are different.\n\n### Approach\n1. **Input Handling**: The function accepts both Python lists and NumPy arrays. If the inputs are lists, they are converted to NumPy arrays to facilitate element-wise operations.\n2. **Length Check**: The function first checks if the lengths of the two vectors are equal. If they are not, it returns -1.\n3. **Empty Vectors**: If both vectors are empty, the Hamming distance is considered 0.0.\n4. **Compute Differences**: Using NumPy, the function computes the element-wise differences between the two vectors and counts the number of differing positions.\n5. **Normalize and Round**: The count of differing positions is divided by the length of the vectors to get the normalized Hamming distance, which is then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert to NumPy arrays if they are lists\n    x = np.array(x) if isinstance(x, list) else x\n    y = np.array(y) if isinstance(y, list) else y\n\n    # Check if lengths are equal\n    if x.size != y.size:\n        return -1\n\n    # If both are empty, return 0.0\n    if x.size == 0:\n        return 0.0\n\n    # Compute the Hamming distance\n    diffs = (x != y)\n    sum_diffs = np.sum(diffs)\n    distance = sum_diffs / x.size\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The function converts input lists to NumPy arrays to handle both list and array inputs uniformly.\n2. **Length Check**: The function checks if the lengths of the two vectors are the same. If not, it returns -1.\n3. **Empty Vectors Handling**: If both vectors are empty, the function returns 0.0 as there are no elements to compare.\n4. **Element-wise Comparison**: Using NumPy, the function creates a boolean array indicating where the elements differ. The sum of this array gives the count of differing positions.\n5. **Normalization and Rounding**: The count of differing positions is divided by the total number of elements to get the normalized distance, which is then rounded to four decimal places for the final result.\n\nThis approach ensures that the function efficiently computes the Hamming distance while handling various input types and edge cases such as empty vectors."}
{"task_id": 171, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary Logistic Regression classifier using mini-batch Gradient Descent from scratch. The function should handle various binary labels, add an intercept term, and use a decision threshold of 0.5 to make predictions.\n\n### Approach\n1. **Check for Special Case**: If all training labels are identical, return that label for all test samples without further training.\n2. **Convert Labels**: Map the training labels to 0 and 1 for easier computation.\n3. **Add Intercept Term**: Convert the feature matrix to include an intercept term (bias) for each sample.\n4. **Initialize Weights**: Randomly initialize the weights with a small variance to avoid symmetry issues.\n5. **Training Loop**: Use mini-batch Gradient Descent to minimize the negative log-likelihood loss. Process the training data in batches, shuffle them for better convergence, and update the weights iteratively.\n6. **Predictions**: After training, compute the probabilities for each sample in the test set using the logistic function and map these probabilities back to the original labels using a threshold of 0.5.\n\n### Solution Code\n```python\nimport numpy as np\nfrom sklearn.utils import shuffle\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Check if all labels are the same\n    if len(set(y_train)) == 1:\n        unique_label = min(y_train)\n        return [unique_label] * len(X_test)\n    \n    # Convert labels to 0 and 1\n    original_labels = sorted(list(set(y_train)))\n    label_map = {v: i for i, v in enumerate(original_labels)}\n    n_classes = len(original_labels)\n    \n    # Add intercept to training and test data\n    X_train = np.array([np.append(x, 1) for x in X_train])\n    X_test = np.array([np.append(x, 1) for x in X_test])\n    \n    # Initialize weights\n    n_features = X_train.shape[1] - 1  # exclude intercept\n    weights = np.random.randn(n_features + 1, 1) * 0.01\n    \n    # Training loop\n    for epoch in range(epochs):\n        # Shuffle training data\n        shuffled_indices = shuffle(range(len(X_train)))\n        X_batch = X_train[shuffled_indices]\n        y_batch = y_train[shuffled_indices]\n        \n        # Split into batches\n        for batch_idx in range(0, len(X_train), batch_size):\n            X_batch = X_batch[batch_idx:batch_idx+batch_size]\n            y_batch = y_batch[batch_idx:batch_idx+batch_size]\n            \n            # Compute a\n            a = np.sigmoid(weights.dot(X_batch))\n            \n            # Compute gradient\n            gradient = (a - y_batch[:, np.newaxis]) * X_batch\n            \n            # Update weights\n            weights -= learning_rate * gradient\n            \n            # Compute loss\n            loss = -np.mean(np.log(a + 1e-15) - np.log(1 - a + 1e-15) * (1 - y_batch))\n            \n            # Print loss if needed\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss}\")\n    \n    # Make predictions\n    a_pred = np.sigmoid(weights.dot(X_test))\n    y_pred = np.zeros_like(a_pred)\n    y_pred[a_pred >= 0.5] = label_map[1]\n    y_pred[a_pred < 0.5] = label_map[0]\n    \n    return y_pred.tolist()\n```\n\n### Explanation\n1. **Special Case Handling**: If all training labels are the same, the function immediately returns that label for all test samples.\n2. **Label Conversion**: The training labels are converted to 0 and 1 for easier computation. The original labels are stored to map predictions back later.\n3. **Intercept Term Addition**: An intercept column of ones is added to the feature matrix to include the bias term in the weight vector.\n4. **Weight Initialization**: Weights are initialized with a small random variance to avoid symmetry issues during training.\n5. **Training Loop**: The training data is processed in batches. For each batch, the model computes the predicted probabilities, the gradient of the loss, and updates the weights. The loss is printed periodically to monitor training progress.\n6. **Predictions**: After training, the model computes probabilities for each test sample and maps these probabilities back to the original labels using a threshold of 0.5 to determine the predicted labels."}
{"task_id": 178, "completion_id": 0, "completion": "To solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as the sum of the squares of the coordinates in a multi-dimensional space. The goal is to find the minimum value of this function using a swarm of particles that adjust their positions based on their own and the global best positions.\n\n### Approach\nThe PSO algorithm works by maintaining a swarm of particles, each with a position, velocity, and best known position. The particles move towards the best positions found by the swarm, guided by their velocities and a set of weights. The steps involved in the algorithm are as follows:\n\n1. **Initialization**: Each particle is initialized to a random position within the specified bounds. The initial velocity is set to zero, and each particle's personal best is its initial position.\n2. **Iteration**: For each iteration, the velocity of each particle is updated using a combination of the particle's inertia, the cognitive component, and the social component. The position of each particle is then updated by adding its velocity, clipped to stay within the bounds.\n3. **Best Update**: Each particle's personal best is updated if its new position results in a lower function value. The global best is the minimum of all personal bests.\n4. **Termination**: After the specified number of iterations, the best function value found is returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Input validation\n    if not (isinstance(n_dims, int) and isinstance(num_particles, int) and isinstance(num_iterations, int)):\n        return -1\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize particles\n    x = rng.uniform(-1, 1, size=(num_particles, n_dims))\n    v = np.zeros_like(x)\n    p_best = x.copy()\n    f_val = np.sum(x**2)\n    g_best = f_val\n    current_min = f_val\n\n    w = 0.5  # Inertia weight\n    c1 = 1.5  # Cognitive weight\n    c2 = 1.5  # Social weight\n\n    for _ in range(num_iterations):\n        # Generate random numbers for velocity updates\n        r1 = rng.uniform(0, 1, size=num_particles)\n        r2 = rng.uniform(0, 1, size=num_particles)\n\n        # Compute new velocities\n        v_new = w * v + c1 * r1 * (p_best - x) + c2 * r2 * (g_best - x)\n\n        # Update positions\n        x_new = x + v_new\n        x_new = np.clip(x_new, -1, 1)\n\n        # Compute function values for new positions\n        f_new = np.sum(x_new**2)\n\n        # Update personal bests\n        for i in range(num_particles):\n            if f_new[i] < p_best[i]:\n                p_best[i] = x_new[i]\n\n        # Find global best\n        g_best = np.min(p_best)\n        current_min = g_best\n\n        # Update for next iteration\n        x = x_new\n        v = v_new\n\n    # Return the best value rounded to four decimals\n    return round(current_min, 4)\n```\n\n### Explanation\n1. **Initialization**: The function starts by validating the input parameters. If any parameter is invalid, it returns -1. The random number generator is initialized to ensure reproducibility.\n2. **Particle Initialization**: Each particle's initial position is set to a random value within the range [-1, 1]. The initial velocity is zero, and each particle's personal best is its initial position.\n3. **Velocity and Position Update**: For each iteration, new velocities are computed using the inertia weight, cognitive component, and social component. The particle's position is updated by adding the new velocity, ensuring it stays within the bounds.\n4. **Best Update**: Each particle's personal best is updated if the new position results in a lower function value. The global best is the minimum of all personal bests.\n5. **Termination**: After the specified number of iterations, the best function value found is returned, rounded to four decimal places.\n\nThis approach ensures that the PSO algorithm efficiently explores the search space and converges to the minimum of the Sphere function."}
{"task_id": 180, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a pure-function version of the k-Nearest Neighbors (k-NN) algorithm that can work both as a classifier and as a regressor. The function should handle both classification (using majority vote) and regression (using average) tasks.\n\n### Approach\n1. **Handle k Parameter**: If k is 0 or larger than the number of training samples, use all training samples.\n2. **Convert Training Data**: Convert the training feature vectors into a numpy array for efficient distance computation.\n3. **Compute Distances**: For each test feature vector, compute the Euclidean distance to all training feature vectors using vectorized operations.\n4. **Find Nearest Neighbors**: Sort the training samples based on their distance to the test sample and select the top k nearest neighbors.\n5. **Predict Based on Task**:\n   - **Classification**: Determine the most frequent label among the k nearest neighbors. In case of a tie, return the smallest label.\n   - **Regression**: Compute the arithmetic mean of the target values of the k nearest neighbors and round it to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Determine effective k\n    if k == 0:\n        effective_k = len(y_train)\n    else:\n        effective_k = min(k, len(y_train))\n    \n    # Convert training data to numpy arrays for efficient computation\n    X_train_np = np.array(X_train)\n    predictions = []\n    \n    for test_sample in X_test:\n        test_sample_np = np.array(test_sample)\n        # Compute distances from test sample to all training samples\n        distances = np.linalg.norm(X_train_np - test_sample_np, axis=1)\n        # Create list of tuples (distance, index)\n        dist_indices = list(zip(distances, np.arange(len(X_train))))\n        # Sort by distance\n        sorted_pairs = sorted(dist_indices, key=lambda x: x[0])\n        # Take the first effective_k neighbors\n        nearest = sorted_pairs[:effective_k]\n        # Extract the indices of the nearest neighbors\n        nearest_indices = [pair[1] for pair in nearest]\n        # Get the corresponding target values\n        targets = [y_train[i] for i in nearest_indices]\n        \n        if task == \"classification\":\n            # Count the occurrences of each label\n            counts = Counter(targets)\n            # Find the most common label\n            max_count = max(counts.values())\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            # Sort the candidates and pick the smallest one in case of tie\n            candidates.sort()\n            prediction = candidates[0]\n        else:\n            # Compute the average of the target values\n            avg = np.mean(targets)\n            # Round to four decimal places\n            prediction = round(avg, 4)\n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation\n1. **Effective k Calculation**: The function first determines the effective number of neighbors to consider, which is either k or the total number of training samples if k is 0 or larger than the number of samples.\n2. **Numpy Conversion**: The training feature vectors are converted into a numpy array to leverage efficient vectorized operations for distance computation.\n3. **Distance Calculation**: For each test feature vector, the Euclidean distances to all training feature vectors are computed using numpy's `norm` function.\n4. **Nearest Neighbor Selection**: The training samples are sorted based on their distance to the test sample, and the top k nearest neighbors are selected.\n5. **Prediction**: Depending on the task, the function either uses the majority vote for classification or the average for regression, ensuring the results are formatted correctly.\n\nThis approach ensures that the function efficiently handles both classification and regression tasks while preserving the order of the test samples in the output."}
{"task_id": 184, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary decision tree classifier using the CART algorithm. The decision tree will be built from the training data and used to predict class labels for unseen test samples. The tree is constructed using Gini impurity and recursive binary splitting.\n\n### Approach\n1. **Problem Analysis**: The task is to build a decision tree that can predict class labels for unseen samples based on the training data. The decision tree should handle various stopping criteria and splitting rules as specified.\n\n2. **Intuition**: The decision tree is built recursively, where each node represents a split based on a feature and a threshold. The split is chosen to maximize the Gini-gain, which measures the impurity reduction from splitting the node.\n\n3. **Algorithm Selection**: The CART algorithm is chosen for its ability to handle various splitting criteria and impurity measures. The Gini impurity is used to evaluate the effectiveness of each split.\n\n4. **Complexity Analysis**: The time complexity of building the decision tree is O(n^2 * m), where n is the number of samples and m is the number of features. This is due to the recursive nature of the tree construction and the need to evaluate all possible splits at each node.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    def build_tree(samples, labels, max_depth, current_depth):\n        # Check if all labels are the same\n        if np.unique(labels).size == 1:\n            return {'majority_label': labels[0]}\n        # Check if max_depth is reached\n        if max_depth is not None and current_depth >= max_depth:\n            return {'majority_label': mode(labels)}\n        # Check if any split would produce an empty child\n        best_feature, best_threshold = find_best_split(samples, labels)\n        left_samples, left_labels, right_samples, right_labels = split(samples, labels, best_feature, best_threshold)\n        if len(left_samples) == 0 or len(right_samples) == 0:\n            return {'majority_label': mode(labels)}\n        # Build left and right children\n        left_child = build_tree(left_samples, left_labels, max_depth, current_depth + 1)\n        right_child = build_tree(right_samples, right_labels, max_depth, current_depth + 1)\n        # Compute majority label for current node\n        majority_label = mode(labels)\n        return {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_child,\n            'right': right_child,\n            'majority_label': majority_label\n        }\n\n    def find_best_split(samples, labels):\n        n_samples = len(samples)\n        if n_samples == 0:\n            return None, None\n        n_features = len(samples[0])\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        for feature in range(n_features):\n            feature_values = samples[:, feature]\n            sorted_values = np.sort(feature_values)\n            unique_values = np.unique(sorted_values)\n            midpoints = []\n            for i in range(len(unique_values) - 1):\n                mid = (unique_values[i] + unique_values[i+1]) / 2\n                midpoints.append(mid)\n            for midpoint in midpoints:\n                left_samples, left_labels, right_samples, right_labels = split(samples, labels, feature, midpoint)\n                left_impurity = 0 if np.unique(left_labels).size == 1 else 1\n                right_impurity = 0 if np.unique(right_labels).size == 1 else 1\n                gain = 1 - (left_impurity * len(left_samples) + right_impurity * len(right_samples)) / n_samples\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = midpoint\n        return (best_feature, best_threshold)\n\n    def split(samples, labels, feature, threshold):\n        feature_values = samples[:, feature]\n        split_index = np.where(feature_values <= threshold)[0]\n        left_samples = samples[:, :split_index + 1]\n        left_labels = labels[:split_index + 1]\n        right_samples = samples[:, split_index + 1:]\n        right_labels = labels[split_index + 1:]\n        return left_samples, left_labels, right_samples, right_labels\n\n    def mode(labels):\n        return np mode labeling=np.bincount(np.asarray(labels)).argmax()\n\n    # Build the decision tree\n    root = build_tree(X_train, y_train, max_depth, 0)\n\n    # Function to predict a single sample\n    def predict_sample(sample, node):\n        if 'majority_label' in node:\n            return node['majority_label']\n        else:\n            feature = node['feature']\n            threshold = node['threshold']\n            if sample[feature] <= threshold:\n                return predict_sample(sample, node['left'])\n            else:\n                return predict_sample(sample, node['right'])\n\n    # Predict for each sample in X_test\n    predictions = []\n    for sample in X_test:\n        label = predict_sample(sample, root)\n        predictions.append(label)\n    return predictions\n```\n\n### Explanation\n1. **Building the Decision Tree**: The `build_tree` function recursively constructs the decision tree. It checks if the current node is a leaf based on the stopping criteria. If not, it finds the best split using Gini-gain and recursively builds the left and right subtrees.\n\n2. **Finding the Best Split**: The `find_best_split` function evaluates all possible splits for each feature, computes the Gini-gain, and selects the split with the maximum gain.\n\n3. **Splitting the Samples**: The `split` function divides the samples into left and right subsets based on the current feature and threshold.\n\n4. **Predicting Samples**: The `predict_sample` function traverses the decision tree to predict the class label for each test sample by following the appropriate branches based on the feature values.\n\nThis approach ensures that the decision tree is built efficiently and accurately, providing deterministic predictions for unseen samples."}
{"task_id": 190, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the best binary split of a given dataset that minimizes the weighted Gini impurity. The Gini impurity is a measure of impurity in a decision tree node, and the weighted Gini impurity for a split is a weighted average of the impurities of the left and right child nodes.\n\n### Approach\n1. **Compute the Gini Impurity of the Whole Dataset**: This serves as the initial impurity value if no valid splits are found.\n2. **Iterate Over Each Feature**: For each feature, consider every unique value as a potential threshold for splitting the data.\n3. **Split the Data**: For each threshold, split the data into left and right child nodes.\n4. **Check Validity of Split**: Ensure both child nodes have at least one sample. If not, skip the split.\n5. **Compute Gini Impurity for Each Child Node**: Calculate the Gini impurity for both the left and right child nodes.\n6. **Compute Weighted Gini Impurity**: Combine the impurities of the child nodes into a weighted average.\n7. **Track the Best Split**: Keep track of the split with the minimum Gini impurity, using tie-breakers to handle ties.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n    n_total = len(y)\n    \n    # Compute Gini impurity of the whole dataset\n    class_counts = np.unique(y, return_counts=True)[1]\n    sum_p_sq = sum((count / n_total) ** 2 for count in class_counts)\n    G_whole = 1 - sum_p_sq\n    \n    best_g = G_whole\n    best_feature = -1\n    best_threshold = None\n    \n    for j in range(n_features):\n        unique_v = np.unique(X[:, j])\n        for v in unique_v:\n            mask_left = (X[:, j] <= v)\n            left = X[mask_left]\n            right = X[~mask_left]\n            \n            if len(left) == 0 or len(right) == 0:\n                continue  # Skip invalid split\n            \n            # Calculate Gini impurity for left and right\n            # For left\n            class_counts_left = np.unique(left[:, 0], return_counts=True)[1]\n            sum_p_sq_left = sum((count / len(left)) ** 2 for count in class_counts_left)\n            G_left = 1 - sum_p_sq_left\n            \n            # For right\n            class_counts_right = np.unique(right[:, 0], return_counts=True)[1]\n            sum_p_sq_right = sum((count / len(right)) ** 2 for count in class_counts_right)\n            G_right = 1 - sum_p_sq_right\n            \n            # Compute weighted Gini impurity for the split\n            n_left = len(left)\n            n_right = len(right)\n            g_split = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n            \n            # Update the best split if this is better\n            if g_split < best_g:\n                best_g = g_split\n                best_feature = j\n                best_threshold = v\n            elif g_split == best_g:\n                # Apply tie-breaker rules\n                if j < best_feature:\n                    best_feature = j\n                    best_threshold = v\n                elif j == best_feature:\n                    if v < best_threshold:\n                        best_threshold = v\n    \n    if best_feature == -1:\n        return (-1, None, round(G_whole, 4))\n    else:\n        return (best_feature, best_threshold, round(best_g, 4))\n```\n\n### Explanation\n1. **Initialization**: Convert the input data to numpy arrays for easier manipulation. Compute the Gini impurity of the whole dataset.\n2. **Feature Iteration**: For each feature, extract unique values to consider as potential thresholds.\n3. **Splitting**: For each threshold, split the data into left and right child nodes.\n4. **Validity Check**: Ensure both child nodes have samples. If not, skip the split.\n5. **Gini Calculation**: Compute the Gini impurity for each child node and the weighted average for the split.\n6. **Tracking Best Split**: Keep track of the split with the lowest Gini impurity, using tie-breakers to handle ties in impurity values.\n\nThis approach ensures that we find the optimal split that minimizes the weighted Gini impurity, providing a robust solution for decision tree construction."}
{"task_id": 191, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a factory function that generates a callable function for computing one of four classical distance metrics (Euclidean, Manhattan, Chebyshev, and Cosine) between two numeric vectors. The generated function must validate the input vectors before computing the distance. If the validation fails or the metric name is unsupported, the function returns -1.\n\n### Approach\n1. **Validation of Input Vectors**: The function must ensure that both input vectors are one-dimensional, non-empty, and of the same length. Additionally, all elements in the vectors must be numeric (integers or floats).\n2. **Metric Support Check**: The function must check if the provided metric name is one of the supported names: 'euclidean', 'manhattan', 'chebyshev', or 'cosine'.\n3. **Cosine Metric Norm Check**: For the Cosine metric, the function must ensure that the norms (magnitudes) of both vectors are non-zero.\n4. **Distance Computation**: Based on the metric name, the function computes the corresponding distance using the appropriate formula:\n   - **Euclidean**: The square root of the sum of squared differences.\n   - **Manhattan**: The sum of absolute differences.\n   - **Chebyshev**: The maximum absolute difference.\n   - **Cosine**: 1 minus the ratio of the dot product to the product of the norms of the vectors.\n5. **Result Rounding**: The computed distance is rounded to four decimal places before being returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\"\"\"\n    if name not in ['euclidean', 'manhattan', 'chebyshev', 'cosine']:\n        def d(x, y):\n            return -1\n        return d\n\n    def d(x, y):\n        try:\n            x_arr = np.asarray(x)\n            y_arr = np.asarray(y)\n        except:\n            return -1\n\n        if x_arr.ndim != 1 or y_arr.ndim != 1:\n            return -1\n\n        if len(x_arr) != len(y_arr):\n            return -1\n\n        if len(x_arr) == 0 or len(y_arr) == 0:\n            return -1\n\n        for element in x_arr:\n            if not isinstance(element, (int, float)):\n                return -1\n        for element in y_arr:\n            if not isinstance(element, (int, float)):\n                return -1\n\n        if name == 'cosine':\n            x_norm = np.linalg.norm(x_arr)\n            y_norm = np.linalg.norm(y_arr)\n            if x_norm == 0 or y_norm == 0:\n                return -1\n\n        if name == 'euclidean':\n            diff = x_arr - y_arr\n            distance = np.sqrt(np.sum(diff**2))\n        elif name == 'manhattan':\n            distance = np.sum(np.abs(x_arr - y_arr))\n        elif name == 'chebyshev':\n            distance = np.max(np.abs(x_arr - y_arr))\n        elif name == 'cosine':\n            dot_product = np.dot(x_arr, y_arr)\n            x_norm = np.linalg.norm(x_arr)\n            y_norm = np.linalg.norm(y_arr)\n            distance = 1 - (dot_product / (x_norm * y_norm))\n        else:\n            return -1\n\n        distance = round(distance, 4)\n        return distance\n\n    return d\n```\n\n### Explanation\n- **Validation**: The function first converts the input vectors to NumPy arrays and checks if they are one-dimensional, non-empty, and of the same length. It also ensures all elements are numeric.\n- **Cosine Norm Check**: For the Cosine metric, it checks if the norms of the vectors are non-zero.\n- **Distance Calculation**: Depending on the metric, the function computes the distance using the respective formula.\n- **Rounding**: The result is rounded to four decimal places before being returned.\n\nThis approach ensures that the function handles various edge cases and validation checks efficiently, providing accurate distance computations for supported metrics."}
{"task_id": 197, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that summarizes the action space of a Reinforcement Learning (RL) environment. The function should handle both continuous and discrete action spaces, as well as single-dimensional and multi-dimensional cases. The goal is to return basic statistics about the action space, including the number of actions per dimension, a list of all valid discrete actions, and the total number of action dimensions.\n\n### Approach\n1. **Determine if the Action Space is Continuous**: Check if any sub-space of the action space is continuous. If any sub-space is continuous, the entire action space is considered continuous.\n2. **Handle Continuous Action Space**: If the action space is continuous, set the corresponding entries in the number of actions per dimension to infinity (`numpy.inf`). Return `None` for the list of valid discrete actions.\n3. **Handle Discrete Action Space**: If the action space is discrete, compute the list of valid actions as the Cartesian product of all possible actions in each dimension. For multi-dimensional discrete spaces, this involves generating all possible combinations of actions across each dimension.\n4. **Return the Results**: Return the number of actions per dimension, the list of valid discrete actions, and the total number of action dimensions.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import product\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    is_continuous = False\n    action_dim = 0\n\n    if hasattr(env.action_space, 'spaces'):\n        # Multi-dimensional action space\n        action_dim = len(env.action_space.spaces)\n        for sub_space in env.action_space.spaces:\n            if hasattr(sub_space, 'shape'):\n                is_continuous = True\n                break\n    else:\n        # Single-dimensional action space\n        if hasattr(env.action_space, 'shape'):\n            is_continuous = True\n        else:\n            # Single-dimensional discrete\n            action_dim = 1\n\n    if is_continuous:\n        n_actions_per_dim = [np.inf] * action_dim\n        action_ids = None\n    else:\n        if hasattr(env.action_space, 'spaces'):\n            # Multi-dimensional discrete\n            possible_actions = []\n            for sub_space in env.action_space.spaces:\n                possible_actions.append(list(range(sub_space.n)))\n            action_ids = list(product(*possible_actions))\n            n_actions_per_dim = [sub_space.n for sub_space in env.action_space.spaces]\n        else:\n            # Single-dimensional discrete\n            n_actions = env.action_space.n\n            n_actions_per_dim = [n_actions]\n            action_ids = list(range(n_actions))\n\n    return (n_actions_per_dim, action_ids, action_dim)\n```\n\n### Explanation\n1. **Determine Continuity**: The function first checks if the action space is continuous by examining each sub-space in the case of a multi-dimensional space or the shape attribute in the case of a single-dimensional space.\n2. **Handle Continuous Space**: If any sub-space is continuous, the function sets the corresponding entries in `n_actions_per_dim` to infinity and returns `None` for `action_ids`.\n3. **Handle Discrete Space**: If the action space is discrete, the function computes the Cartesian product of all possible actions across each dimension to generate `action_ids`. For multi-dimensional spaces, this involves generating all combinations of actions from each sub-space.\n4. **Return Results**: The function returns a tuple containing the number of actions per dimension, the list of valid discrete actions, and the total number of action dimensions.\n\nThis approach ensures that the function correctly handles both continuous and discrete action spaces, as well as single-dimensional and multi-dimensional cases, providing a comprehensive summary of the environment's action space."}
{"task_id": 202, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Partitioning Around Medoids (PAM) clustering algorithm. The goal is to group samples into k clusters by iteratively improving a set of representative points called medoids. The algorithm follows a deterministic approach to ensure consistent results.\n\n### Approach\n1. **Initialization**: Start with the first k samples as the initial medoids.\n2. **Cost Calculation**: Compute the initial clustering cost, which is the sum of the Euclidean distances from each sample to its nearest medoid.\n3. **Iterative Improvement**: Repeatedly attempt to reduce the total clustering cost by swapping any current medoid with a non-medoid sample. The swap that provides the largest cost reduction is chosen. If no swap reduces the cost, the algorithm stops.\n4. **Label Assignment**: After convergence, assign each sample to the closest medoid. If two medoids are equally close, the one that appears first in the current medoid list is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most. Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    n_samples = X.shape[0]\n    if k == 0:\n        return []\n    \n    # Initialize medoids with the first k samples\n    medoid_indices = set(range(k))\n    current_medoids = [X[i] for i in medoid_indices]\n    \n    # Calculate initial clustering cost\n    current_cost = 0\n    for sample in X:\n        min_dist = float('inf')\n        min_medoid_idx = -1\n        for medoid_idx, medoid in enumerate(current_medoids):\n            dist = np.linalg.norm(medoid - sample)\n            if dist < min_dist:\n                min_dist = dist\n                min_medoid_idx = medoid_idx\n        current_cost += min_dist\n    \n    # Iterate to find the best swap\n    while True:\n        best_delta = 0\n        best_swap = None\n        \n        for i in range(k):\n            for j in range(n_samples):\n                if j not in medoid_indices:\n                    # Create new_medoids by swapping i and j\n                    new_medoids = current_medoids.copy()\n                    new_medoids[i] = X[j]\n                    \n                    # Calculate new clustering cost\n                    new_cost = 0\n                    for sample in X:\n                        min_dist = float('inf')\n                        min_medoid_idx = -1\n                        for medoid_idx, medoid in enumerate(new_medoids):\n                            dist = np.linalg.norm(medoid - sample)\n                            if dist < min_dist:\n                                min_dist = dist\n                                min_medoid_idx = medoid_idx\n                        new_cost += min_dist\n                    \n                    delta = new_cost - current_cost\n                    if delta < 0:\n                        if delta > best_delta:\n                            best_delta = delta\n                            best_swap = (i, j)\n        \n        if best_swap is not None:\n            i, j = best_swap\n            # Update current_medoids and medoid_indices\n            new_medoids = current_medoids.copy()\n            new_medoids[i] = X[j]\n            current_medoids = new_medoids\n            medoid_indices.discard(i)\n            medoid_indices.add(j)\n            current_cost = new_cost\n        else:\n            break\n    \n    # Assign labels based on the closest medoid\n    labels = []\n    for sample in X:\n        min_dist = float('inf')\n        min_label = 0\n        for label, medoid in enumerate(current_medoids):\n            dist = np.linalg.norm(medoid - sample)\n            if dist < min_dist:\n                min_dist = dist\n                min_label = label\n        labels.append(min_label)\n    \n    return labels\n```\n\n### Explanation\n1. **Initialization**: The first k samples are chosen as the initial medoids.\n2. **Cost Calculation**: The initial cost is computed by summing the distances from each sample to its nearest medoid.\n3. **Iterative Improvement**: For each possible swap between a medoid and a non-medoid, the new cost is calculated. The swap that reduces the cost the most is chosen. This process continues until no further cost reduction is possible.\n4. **Label Assignment**: Each sample is assigned to the closest medoid. If two medoids are equally close, the one that appears first in the list is chosen.\n\nThis approach ensures that the algorithm converges to an optimal clustering with the given number of clusters, following a deterministic and systematic process."}
{"task_id": 216, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic Net regression from scratch using batch gradient descent. The model must be able to generate polynomial features, standardize non-bias features, learn weights with both L1 and L2 regularization, and make predictions for unseen samples.\n\n### Approach\n1. **Input Validation**: Check if the input matrices and vectors are valid and compatible in shape.\n2. **Feature Expansion**: Expand each feature in the training data matrix into a polynomial of the specified degree.\n3. **Bias Term Addition**: Add a bias term (intercept) to the feature matrix.\n4. **Standardization**: Standardize the non-bias features to zero-mean and unit-variance.\n5. **Weight Initialization**: Initialize the weights vector, which includes coefficients for both the expanded features and the bias term.\n6. **Batch Gradient Descent Training**: Train the model for the specified number of iterations, updating the weights using the gradient descent method with both L1 and L2 regularization.\n7. **Prediction**: Use the trained model to make predictions for the test data matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Check input validity\n    if not (isinstance(x_train, (np.ndarray, list)) and isinstance(y_train, (np.ndarray, list)) and isinstance(x_test, (np.ndarray, list))):\n        return -1\n\n    m_train = x_train.shape[0] if isinstance(x_train, np.ndarray) else len(x_train)\n    m_test = x_test.shape[0] if isinstance(x_test, np.ndarray) else len(x_test)\n\n    if m_train != m_test:\n        return -1\n\n    if isinstance(x_train, np.ndarray) and x_train.ndim != 2:\n        return -1\n    if isinstance(x_test, np.ndarray) and x_test.ndim != 2:\n        return -1\n\n    k_train = x_train.shape[1] if isinstance(x_train, np.ndarray) else len(x_train[0])\n    k_test = x_test.shape[1] if isinstance(x_test, np.ndarray) else len(x_test[0])\n\n    if k_train == 0 or k_test == 0:\n        return -1\n\n    if n_iterations < 1:\n        return -1\n\n    # Build expanded feature matrix for x_train\n    expanded_features_train = []\n    for feature in x_train:\n        poly = [np.ones(degree + 1)]\n        for d in range(degree):\n            poly.append(feature[d])\n        expanded_features_train.append(poly)\n    expanded_features_train = np.array(expanded_features_train)\n\n    # Add bias term\n    feature_matrix = np.hstack((expanded_features_train, np.ones((m_train, 1))))\n\n    # Check if x_test has the same number of features as feature_matrix\n    if isinstance(x_test, np.ndarray) and x_test.shape[1] != feature_matrix.shape[1]:\n        return -1\n    if not isinstance(x_test, np.ndarray) and len(x_test[0]) != feature_matrix.shape[1]:\n        return -1\n\n    # Standardize non-bias features\n    X = feature_matrix[:, :-1]\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    standardized_X = (X - X_mean) / X_std\n\n    feature_matrix = np.hstack((standardized_X, np.ones((m_train, 1))))\n\n    # Initialize weights\n    n_features = feature_matrix.shape[1]\n    weights = np.zeros(n_features, 1)\n\n    # Training loop\n    for _ in range(n_iterations):\n        y_pred = feature_matrix @ weights\n        errors = y_train - y_pred\n        y_pred_mean = np.mean(y_pred)\n        y_mean = np.mean(y_train)\n        grad_bias = y_pred_mean - y_mean\n\n        X = feature_matrix[:, :-1]\n        grad_non_bias = (errors @ X) / m_train\n\n        # Compute penalty\n        sum_w2 = np.sum(weights**2)\n        sum_w1 = np.sum(np.abs(weights[:-1]))  # exclude bias term\n        penalty = (1 - l1_ratio) * sum_w2 + l1_ratio * sum_w1\n\n        # Compute gradients with penalty\n        grad_non_bias_with_penalty = grad_non_bias + (1 - l1_ratio) * weights + l1_ratio * np.sign(weights)\n\n        # Combine gradients\n        grad = np.concatenate([grad_non_bias_with_penalty, np.array([grad_bias])])\n\n        # Update weights\n        weights -= learning_rate * grad\n\n    # Make predictions for x_test\n    y_pred_test = feature_matrix @ weights\n    predictions = np.round(y_pred_test, 4).tolist()\n\n    return predictions\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices and vectors are valid and if their shapes are compatible.\n2. **Feature Expansion**: Each feature in the training data is expanded into a polynomial of the specified degree.\n3. **Bias Term Addition**: A bias term (intercept) is added to the feature matrix to account for the intercept in linear regression.\n4. **Standardization**: The non-bias features are standardized to zero-mean and unit-variance to improve convergence.\n5. **Weight Initialization**: Weights are initialized to zero for all features, including the bias term.\n6. **Batch Gradient Descent Training**: The model is trained using batch gradient descent for the specified number of iterations. The gradient is computed considering both L1 and L2 regularization.\n7. **Prediction**: After training, the model makes predictions for the test data, which are then rounded to four decimal places and returned."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic (sigmoid) loss function used in binary classification algorithms. These components are the gradient, the Hessian (second derivative), and the probability (sigmoid transformation) for each observation. The function should handle various input types and return the results rounded to six decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function first converts the input arrays into NumPy arrays to facilitate vectorized computations.\n2. **Compute Gradient**: The gradient of the logistic loss for each observation is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`.\n3. **Compute Hessian**: The Hessian (second derivative) of the logistic loss for each observation is calculated using the formula `sigmoid(predicted_i) * (1 - sigmoid(predicted_i))`.\n4. **Compute Probability**: The probability for each observation is obtained by applying the sigmoid transformation to the predicted values.\n5. **Round Results**: Each computed component is rounded to six decimal places.\n6. **Convert Results to Lists**: The results are converted back from NumPy arrays to plain Python lists before being returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute gradient\n    z = -actual * predicted\n    sigmoid_z = np.sigmoid(z)\n    grad = actual * sigmoid_z\n    \n    # Compute Hessian\n    sigmoid_p = np.sigmoid(predicted)\n    hessian = sigmoid_p * (1 - sigmoid_p)\n    \n    # Compute probability\n    prob = sigmoid_p\n    \n    # Round each to 6 decimal places\n    grad_rounded = np.round(grad, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert NumPy arrays to lists\n    gradient_list = grad_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    prob_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, prob_list)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`, which measures the rate of change of the loss function with respect to the model's predictions.\n- **Hessian Calculation**: The Hessian is derived from the second derivative of the logistic loss function and is given by `sigmoid(predicted_i) * (1 - sigmoid(predicted_i))`.\n- **Probability Calculation**: The probability for each observation is obtained by applying the sigmoid function to the predicted values, giving a probability between 0 and 1.\n- **Rounding and Conversion**: Each result is rounded to six decimal places and converted from NumPy arrays to Python lists for the final output.\n\nThis approach ensures that the function efficiently computes the required components using vectorized operations, making it suitable for large datasets and various input types."}
{"task_id": 221, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Mean Squared Logarithmic Error (MSLE) metric for evaluating regression models. The MSLE is a measure of the error between actual and predicted values, where the error is computed using the squared difference of the logarithms of these values.\n\n### Approach\n1. **Helper Function**: We create a helper function `squared_log_error` that computes the element-wise squared differences between the logarithms of the actual and predicted values. This function converts the inputs into NumPy arrays to handle various input types (lists, tuples, or NumPy arrays).\n\n2. **Main Function**: The main function `mean_squared_log_error` performs the following steps:\n   - **Input Validation**: Checks if the actual and predicted values have the same shape and if all values are non-negative.\n   - **Error Calculation**: Uses the helper function to compute the squared logarithmic errors.\n   - **Mean Calculation**: Computes the mean of these squared errors and rounds the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"TODO: implement helper that returns element-wise squared log errors.\"\"\"\n    a = np.array(actual)\n    p = np.array(predicted)\n    log_diff = math.log(1 + a) - math.log(1 + p)\n    squared = np.square(log_diff)\n    return squared\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns \u20111.\n    \"\"\"\n    a = np.array(actual)\n    p = np.array(predicted)\n    \n    # Check if the shapes of actual and predicted are the same\n    if a.shape != p.shape:\n        return -1\n    \n    # Check if all values in actual and predicted are non-negative\n    if np.any(a < 0) or np.any(p < 0):\n        return -1\n    \n    # Calculate squared logarithmic errors\n    squared_errors = squared_log_error(a, p)\n    \n    # Compute the mean of the squared errors\n    mean = np.mean(squared_errors)\n    \n    # Round to 4 decimal places\n    return round(mean, 4)\n```\n\n### Explanation\n1. **Helper Function (`squared_log_error`)**:\n   - Converts the input values into NumPy arrays.\n   - Computes the logarithm of each element in the actual values and the predicted values.\n   - Calculates the difference between these logarithms.\n   - Squares the differences and returns the resulting array.\n\n2. **Main Function (`mean_squared_log_error`)**:\n   - Converts the input values into NumPy arrays for uniform handling.\n   - Checks if the actual and predicted values have the same shape and if all values are non-negative.\n   - If any validation check fails, it returns -1.\n   - Computes the squared logarithmic errors using the helper function.\n   - Calculates the mean of these squared errors and rounds the result to four decimal places.\n\nThis approach ensures that the MSLE is computed accurately while handling various input types and validating the inputs to ensure correctness."}
{"task_id": 222, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the sigmoid (logistic) activation function, which can handle both scalar and array-like inputs. The function must be numerically stable for very large positive or negative values and return the results rounded to four decimal places.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n\nTo ensure numerical stability, especially for very large values of \\( z \\), we use a vectorized approach. The key insight is to compute the function differently based on whether \\( z \\) is non-negative or negative:\n- For \\( z \\geq 0 \\), compute \\( e^z \\) and use the formula \\( 1 - \\frac{1}{1 + e^z} \\).\n- For \\( z < 0 \\), compute \\( e^{-z} \\) and use the formula \\( \\frac{1}{1 + e^{-z}} \\).\n\nThis approach avoids potential overflow or underflow issues that might arise from directly computing \\( e^{-z} \\) for very large \\( z \\).\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if np.isscalar(z):\n        # Convert scalar to a 0-dimensional array\n        z = np.array([z])\n    else:\n        # Convert list-like to a NumPy array\n        z = np.asarray(z)\n    \n    # Compute the exponential of z\n    e_z = math.exp(z)\n    \n    # Compute sigmoid values\n    sigmoid_z = 1 - 1 / (1 + e_z)\n    \n    # Check if the input was a scalar\n    if z.ndim == 0:\n        # Return a float rounded to 4 decimal places\n        return round(sigmoid_z[0], 4)\n    else:\n        # Return a list with the same nested structure, rounded to 4 decimals\n        return np.round(sigmoid_z, 4).tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a scalar. If it is, it converts it into a 0-dimensional NumPy array. If the input is an array-like structure (list or NumPy array), it converts it into a NumPy array.\n2. **Exponential Calculation**: The exponential of \\( z \\) is computed. For large positive \\( z \\), this results in a very large number, and for large negative \\( z \\), it results in a very small number.\n3. **Sigmoid Calculation**: Using the computed exponential values, the sigmoid function is calculated using the formula \\( 1 - \\frac{1}{1 + e^z} \\). This formula is chosen to avoid numerical issues for large values of \\( z \\).\n4. **Output Handling**: If the input was a scalar, the result is returned as a float rounded to four decimal places. If the input was an array, the result is returned as a list with the same nested structure, with each element rounded to four decimal places.\n\nThis approach ensures that the function is both element-wise and numerically stable, handling all specified input types correctly."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies the Leaky ReLU activation function to every element of an input tensor. The input can be a scalar, a nested list, or a numpy array of arbitrary dimensions. The function should return the activated values as a plain Python list with the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as:\n\\[\n\\operatorname{LeakyReLU}(x) = \\begin{cases} \nx & \\text{if } x \\ge 0 \\\\\na \\cdot x & \\text{if } x < 0 \n\\end{cases}\n\\]\nwhere \\(a\\) is a slope parameter in the interval \\([0, 1)\\).\n\nThe approach involves:\n1. Checking if the input is a numpy array. If it is, we use vectorized operations to apply the Leaky ReLU function efficiently and then convert the result back to a list.\n2. If the input is a list, we recursively process each element. Each element is checked to see if it is a list or a scalar. If it is a scalar, we apply the Leaky ReLU function directly. If it is a list, we recursively apply the function to each element.\n3. For scalar inputs, we directly apply the Leaky ReLU function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        # Process as numpy array\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    else:\n        # Process as a list or scalar\n        if isinstance(z, list):\n            return [leaky_relu(elem, a) for elem in z]\n        else:\n            # It's a scalar\n            if z >= 0:\n                return z\n            else:\n                return a * z\n```\n\n### Explanation\n- **Numpy Array Handling**: If the input `z` is a numpy array, we use `np.where` to create a new array where each element is either the original value (if non-negative) or the value multiplied by the slope `a` (if negative). The result is then converted to a list using `tolist()`.\n- **List Handling**: If the input `z` is a list, we recursively process each element. Each element is checked to determine if it is a list or a scalar. If it is a scalar, we apply the Leaky ReLU function directly. If it is a list, we continue the recursion.\n- **Scalar Handling**: For scalar inputs, we directly apply the Leaky ReLU function, returning the value as is if it is non-negative or scaled by `a` if it is negative.\n\nThis approach ensures that the function handles various input types and structures correctly, returning the activated values in the same structure as the input."}
{"task_id": 241, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a NumPy array with an orthogonal weight matrix based on the Saxe et al. (2014) algorithm. This initialization method helps prevent the activations from shrinking or exploding at the beginning of training by ensuring the weights are orthogonal.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Check Shape Validity**: If the input shape has fewer than two dimensions, return -1 as it's not possible to initialize an orthogonal matrix in such cases.\n2. **Generate Random Matrix**: Create a 2-D matrix filled with i.i.d. samples from the standard normal distribution with the specified shape.\n3. **Singular Value Decomposition (SVD)**: Compute the SVD of the generated matrix to obtain the left and right singular vectors.\n4. **Determine Orthogonal Matrix**: Choose the appropriate matrix (either the left singular vectors or the transpose of the right singular vectors) to form the orthogonal matrix.\n5. **Reshape and Scale**: Reshape the chosen orthogonal matrix to the specified shape and scale it by the given factor.\n6. **Return Result**: Convert the resulting matrix to a list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    A = np.random.randn(*shape)\n    U, sigma, V = np.svd(A)\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = V.T\n    Q_reshaped = Q.T.reshape(shape)\n    result = Q_reshaped * scale\n    return result.tolist()\n```\n\n### Explanation\n1. **Check Shape Validity**: The function first checks if the shape has fewer than two dimensions. If so, it returns -1.\n2. **Generate Random Matrix**: Using `np.random.randn`, a matrix `A` is created with the specified shape, filled with random normal samples.\n3. **SVD Calculation**: The SVD of matrix `A` is computed using `np.svd`, which returns the left singular vectors (U), singular values (sigma), and right singular vectors (V).\n4. **Orthogonal Matrix Selection**: The function checks if the shape of U matches the shape of A. If they match, U is used as the orthogonal matrix (Q). Otherwise, the transpose of V (V.T) is used.\n5. **Reshape and Scale**: The orthogonal matrix Q is transposed and reshaped to match the specified shape. It is then scaled by the given factor.\n6. **Return Result**: The resulting matrix is converted to a list of lists and returned.\n\nThis approach ensures that the initialized weights are orthogonal, which helps in preventing the activations from exploding or shrinking during training."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using NumPy. The network consists of four dense layers and two separate heads for the actor and critic. The goal is to compute the action probabilities and the state value for a given input state vector.\n\n### Approach\nThe network architecture is as follows:\n1. **Dense-1**: Input (3 neurons) \u2192 4 neurons, ReLU activation.\n2. **Dense-2**: 4 neurons \u2192 4 neurons, ReLU activation.\n3. **Dense-3**: 4 neurons \u2192 4 neurons, ReLU activation.\n4. **Actor head**: 4 neurons \u2192 3 neurons, Softmax activation.\n5. **Critic head**: 4 neurons \u2192 1 neuron, no activation.\n\nGiven the fixed weights and biases, the network's behavior is deterministic. The key steps in the forward pass are:\n1. Compute the output of the first dense layer (z\u2081) as the sum of the input state elements.\n2. Compute the output of the second dense layer (z\u2082) by scaling z\u2081 by 4.\n3. Compute the output of the third dense layer (z\u2083) by scaling z\u2082 by 4.\n4. Compute the logits for the actor head by scaling z\u2083 by 4.\n5. Compute the action probabilities using the Softmax function, which results in equal probabilities for each action.\n6. Compute the state value by taking the first element of the scaled z\u2083 output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_state = sum(state)\n    z1 = sum_state\n    z2 = 4 * z1\n    z3 = 4 * z2\n    logits = 4 * z3\n    action_probs = [1/3, 1/3, 1/3]\n    state_value = 4 * z3\n    # Round to four decimal places\n    return (round(action_probs, 4), round(state_value, 4))\n```\n\n### Explanation\n1. **Summing the State**: The first layer computes the sum of the input state elements, which serves as the output of the first dense layer (z\u2081).\n2. **Scaling z\u2081**: The output of the second dense layer (z\u2082) is obtained by scaling z\u2081 by 4.\n3. **Scaling z\u2082**: The output of the third dense layer (z\u2083) is obtained by scaling z\u2082 by 4.\n4. **Actor Logits**: The logits for the actor head are computed by scaling z\u2083 by 4.\n5. **Action Probabilities**: Using the Softmax function on the logits, we get equal probabilities for each action, which simplifies to [1/3, 1/3, 1/3].\n6. **State Value**: The state value is computed by taking the first element of the scaled z\u2083 output, which is 4 times z\u2083.\n\nThis approach efficiently computes the required outputs using basic NumPy operations, ensuring the network's deterministic behavior."}
{"task_id": 249, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an Actor-Critic network using NumPy. The network consists of three shared dense layers with ReLU activations, followed by two independent output heads: one for the actor to produce action probabilities and another for the critic to produce a state value.\n\n### Approach\n1. **Extract Parameters**: The network parameters (weights and biases) are provided in a dictionary. We extract these parameters for the three shared layers, the actor head, and the critic head.\n2. **Affine Transformations and ReLU**: The input state undergoes three affine transformations followed by ReLU activations in each layer.\n3. **Actor Head**: The final hidden vector from the third layer is passed through the actor head to produce action scores, which are then converted into a probability distribution using the soft-max function.\n4. **Critic Head**: The same hidden vector is passed through the critic head to estimate the state value.\n5. **Rounding**: The action probabilities and state value are rounded to four decimal places. If the result is exactly 0 or 1, it is represented with one decimal place.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Extract parameters\n    W1 = params['W1']\n    b1 = params['b1']\n    W2 = params['W2']\n    b2 = params['b2']\n    W3 = params['W3']\n    b3 = params['b3']\n    Wa = params['Wa']\n    ba = params['ba']\n    Wc = params['Wc']\n    bc = params['bc']\n    \n    # Convert state to numpy array\n    state = np.array(state)\n    \n    # First layer\n    z1 = np.dot(state, W1) + b1\n    a1 = np.relu(z1)\n    \n    # Second layer\n    z2 = np.dot(a1, W2) + b2\n    a2 = np.relu(z2)\n    \n    # Third layer\n    z3 = np.dot(a2, W3) + b3\n    a3 = np.relu(z3)\n    \n    # Actor head\n    action_scores = np.dot(a3, Wa) + ba\n    probabilities = np.softmax(action_scores)\n    \n    # Critic head\n    value = np.dot(a3, Wc) + bc\n    \n    # Rounding\n    probabilities = np.round(probabilities, 4)\n    value = np.round(value, 4)\n    \n    # Convert to lists\n    probabilities_list = probabilities.tolist()\n    value_list = value\n    \n    return (probabilities_list, value_list)\n```\n\n### Explanation\n1. **Parameter Extraction**: The weights and biases for each layer and head are extracted from the provided dictionary.\n2. **Layer Processing**: The input state is processed through three dense layers with ReLU activations to produce a hidden vector.\n3. **Actor Head**: The hidden vector is transformed into action scores using the actor head, followed by soft-max to get probabilities.\n4. **Critic Head**: The same hidden vector is used to estimate the state value using the critic head.\n5. **Rounding**: The results are rounded to four decimal places, ensuring that exact 0 or 1 values are represented with one decimal place.\n\nThis approach ensures that the network processes the state through the necessary layers and heads, providing both action probabilities and state values as required."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute either the Elastic-Net penalty value or its gradient for a given weight vector. Elastic-Net is a combination of L1 and L2 regularizations, which helps in reducing model complexity and preventing over-fitting in machine learning models.\n\n### Approach\nThe Elastic-Net penalty function is defined as a convex combination of L1 and L2 regularizations. The penalty function is given by:\n\n\\[ R(w) = \\alpha \\left( \\lambda \\cdot \\|w\\|_2 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w \\right) \\]\n\nwhere:\n- \\( w \\) is the weight vector,\n- \\( \\alpha \\) is the overall regularisation strength,\n- \\( \\lambda \\) is the ratio of the L1 component.\n\nThe gradient of this penalty function with respect to \\( w \\) is given by:\n\n\\[ \\nabla R(w) = \\alpha \\left( \\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w \\right) \\]\n\nThe approach involves:\n1. Converting the weight vector \\( w \\) into a numpy array for easier computation.\n2. Calculating the L2 norm of \\( w \\) and the sum of the squares of its elements (which is the squared L1 norm).\n3. Using these values to compute the penalty function.\n4. If the gradient is required, compute the sign of each element in \\( w \\) and use it to calculate the gradient vector.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert w to numpy array if it's a list\n    w = np.array(w) if isinstance(w, list) else w\n    # Compute L2 norm of w\n    norm_w = np.linalg.norm(w, ord=2)\n    # Compute sum of squares (L1 norm squared)\n    sum_squares = np.sum(w ** 2)\n    # Compute penalty\n    penalty = alpha * (l1_ratio * norm_w + (1 - l1_ratio) * 0.5 * sum_squares)\n    if not gradient:\n        return round(penalty, 4)\n    else:\n        # Compute gradient\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        # Round gradient to 4 decimals\n        gradient = np.round(gradient, 4)\n        # Convert to list of floats\n        return gradient.tolist()\n```\n\n### Explanation\n1. **Conversion to Numpy Array**: The weight vector \\( w \\) is converted to a numpy array to facilitate vectorized operations.\n2. **L2 Norm Calculation**: The L2 norm of \\( w \\) is computed using `np.linalg.norm(w, ord=2)`.\n3. **Sum of Squares Calculation**: The sum of the squares of the elements of \\( w \\) is computed using `np.sum(w ** 2)`.\n4. **Penalty Calculation**: The penalty is calculated using the formula for the Elastic-Net penalty function.\n5. **Gradient Calculation**: If the gradient is required, the sign of each element in \\( w \\) is computed using `np.sign(w)`, and the gradient vector is derived using the provided formula.\n6. **Rounding**: The result is rounded to four decimal places before being returned.\n\nThis approach ensures that both the penalty value and the gradient are computed efficiently and accurately, adhering to the requirements of the Elastic-Net regularization method."}
{"task_id": 257, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost (Adaptive Boosting) algorithm using decision stumps as weak learners. The goal is to iteratively re-weight training examples and build a set of decision stumps to minimize the weighted classification error. Each decision stump is built by choosing the feature that best splits the training data, minimizing the weighted error. The weights are updated iteratively based on the error of each stump, and the final prediction for test samples is made by aggregating the votes from all stumps.\n\n### Approach\n1. **Initialization**: Start with uniform weights for all training examples.\n2. **Iterative Boosting**: For each iteration, evaluate all features to find the one that minimizes the weighted classification error. This involves:\n   - Sorting the training examples based on the current feature.\n   - Finding the optimal split point that minimizes the error.\n   - Calculating the error for each feature and selecting the feature with the smallest error.\n3. **Update Weights**: After selecting the best feature, compute the weight (\u03b1_t) for the current stump using the formula \u03b1_t = 0.5 * ln((1 - \u03b5_t)/\u03b5_t), where \u03b5_t is the weighted error. Update the weights of the training examples based on the error.\n4. **Predict Test Samples**: For each test sample, compute the aggregated vote from all stumps. The prediction is based on the sign of the aggregated vote.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    weights = np.ones(m) / m\n    stumps = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_j = -1\n        \n        for j in range(n):\n            # Compute error for feature j\n            sorted_samples = sorted(zip(X_train[:,j], y_train, weights), key=lambda x: x[0])\n            sorted_weights = [s[2] for s in sorted_samples]\n            cumulative = np.cumsum(sorted_weights)\n            total_weight = sum(weights)\n            half = total_weight / 2\n            split_index = np.argmax(cumulative <= half)\n            \n            # Calculate error\n            error = 0.0\n            for i in range(split_index):\n                if sorted_samples[i][1] == 1:\n                    error += sorted_samples[i][2]\n            for i in range(split_index, len(sorted_samples)):\n                if sorted_samples[i][1] == -1:\n                    error += sorted_samples[i][2]\n            \n            # Update min_error and best_j if this feature is better\n            if error < min_error:\n                min_error = error\n                best_j = j\n        \n        # Build the stump for the best feature\n        if best_j == -1:\n            continue  # should not happen as n_clf >=1\n        \n        # Get the best feature's data\n        sorted_samples = sorted(zip(X_train[:,best_j], y_train, weights), key=lambda x: x[0])\n        sorted_weights = [s[2] for s in sorted_samples]\n        cumulative = np.cumsum(sorted_weights)\n        total_weight = sum(weights)\n        half = total_weight / 2\n        split_index = np.argmax(cumulative <= half)\n        threshold = sorted_samples[split_index][0]\n        \n        # Compute alpha_t\n        alpha_t = 0.5 * np.log((1 - min_error) / min_error)\n        \n        # Add to stumps\n        stumps.append( (alpha_t, threshold) )\n        \n        # Update weights\n        new_weights = []\n        for i in range(m):\n            if sorted_samples[i][1] == 1:\n                if i < split_index:\n                    new_weights.append( sorted_samples[i][2] * min_error )\n                else:\n                    new_weights.append( sorted_samples[i][2] * (1 - min_error) )\n            else:\n                if i < split_index:\n                    new_weights.append( sorted_samples[i][2] * min_error )\n                else:\n                    new_weights.append( sorted_samples[i][2] * (1 - min_error) )\n        weights = np.array(new_weights)\n    \n    # Now, predict for X_test\n    predicted_labels = []\n    for x in X_test:\n        total = 0.0\n        for alpha, threshold in stumps:\n            if x >= threshold:\n                total += alpha\n            else:\n                total -= alpha\n        if total > 0:\n            predicted_labels.append(1)\n        else:\n            predicted_labels.append(-1)\n    \n    return predicted_labels\n```\n\n### Explanation\n1. **Initialization**: The weights for each training example are initialized to be uniform.\n2. **Iterative Boosting**: For each iteration, each feature is evaluated to find the one that minimizes the weighted error. The error is computed by sorting the training examples and finding the optimal split point.\n3. **Update Weights**: After selecting the best feature, the weight (\u03b1_t) is computed and the weights of the training examples are updated based on the error.\n4. **Predict Test Samples**: For each test sample, the aggregated vote from all stumps is computed. The prediction is based on the sign of this aggregated vote, resulting in either -1 or 1.\n\nThis approach ensures that the AdaBoost algorithm is implemented efficiently, using decision stumps as weak learners to build an ensemble that minimizes the classification error."}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should maintain the order of vertices as they appear in `V` and ensure that each vertex's neighbors are listed without duplicates.\n\n### Approach\n1. **Initialize Adjacency List**: Create a dictionary where each key is a vertex from `V`, and the corresponding value is an empty list to store its adjacent vertices.\n2. **Process Edges**: Use a set to track processed edges to avoid duplicates. For each edge, add both directions (u to v and v to u) to the adjacency lists.\n3. **Sort Neighbors**: For each vertex, sort its adjacency list based on the order of vertices in `V` to ensure the neighbors are listed in the correct order.\n4. **Return Result**: Convert the dictionary into a list of lists, preserving the order of vertices as in `V`.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of unique vertex identifiers. The order in this list must be preserved.\n    E : List[Tuple[Any, Any]]\n        A list of edges, where each edge is a tuple (u, v) representing an undirected edge.\n\n    Returns\n    -------\n    List[List[Any]]\n        The adjacency list where each G[i] contains all vertices adjacent to V[i], sorted by their appearance in V.\n    \"\"\"\n    # Create adjacency dictionary\n    adj = {v: [] for v in V}\n    processed_edges = set()\n    \n    for edge in E:\n        u, v = edge\n        # Use frozenset to handle undirected edges correctly\n        edge_frozen = frozenset((u, v))\n        if edge_frozen not in processed_edges:\n            processed_edges.add(edge_frozen)\n            adj[u].append(v)\n            adj[v].append(u)\n    \n    # Create a mapping from vertex to its index in V for sorting\n    vertex_to_index = {v: i for i, v in enumerate(V)}\n    \n    # Sort each adjacency list based on the order of vertices in V\n    for u in V:\n        adj[u].sort(key=lambda x: vertex_to_index[x])\n    \n    # Convert the dictionary to a list of lists, preserving the order of V\n    result = []\n    for u in V:\n        result.append(adj[u])\n    \n    return result\n```\n\n### Explanation\n1. **Initialization**: The adjacency dictionary `adj` is initialized with each vertex having an empty list for its neighbors.\n2. **Edge Processing**: Each edge is processed to add both directions to the adjacency lists. Using a frozenset ensures that each edge is processed only once, regardless of its representation in `E`.\n3. **Sorting**: Each vertex's adjacency list is sorted based on the order of vertices in `V` to ensure the neighbors are listed in the correct order.\n4. **Result Construction**: The adjacency dictionary is converted into a list of lists, maintaining the order of vertices as in `V`.\n\nThis approach ensures that the adjacency list is built efficiently, without duplicates, and maintains the required order for both the outer list and the inner lists."}
{"task_id": 273, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the optimal numerical threshold for a decision tree split that maximizes the information gain. The information gain is calculated using the Shannon entropy of the class labels in the subsets created by the split. If multiple thresholds yield the same maximum information gain, the smallest threshold is chosen. If no useful split exists, we return (None, 0.0).\n\n### Approach\n1. **Preprocessing**: Convert the feature and target arrays to lists if they are numpy arrays.\n2. **Combining and Sorting**: Combine the feature and target values into a list of tuples, sort them based on the feature values, and extract the sorted features and targets.\n3. **Unique Features Check**: If all feature values are the same, return (None, 0.0) as no split is possible.\n4. **Midpoints Calculation**: Compute midpoints between consecutive distinct feature values. These midpoints are the candidate thresholds.\n5. **Entropy Calculation**: For each candidate threshold, compute the entropy of the parent set (entire dataset), left subset, and right subset.\n6. **Information Gain Calculation**: For each threshold, compute the information gain using the formula provided.\n7. **Maximize Information Gain**: Track the maximum information gain and the corresponding threshold. If multiple thresholds yield the same maximum gain, choose the smallest threshold.\n8. **Return Result**: Return the threshold and information gain rounded to four decimal places. If no positive information gain is found, return (None, 0.0).\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport bisect\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    if not feature or not target:\n        return (None, 0.0)\n    \n    # Convert numpy arrays to lists for easier handling\n    if isinstance(feature, np.ndarray):\n        feature = feature.tolist()\n    if isinstance(target, np.ndarray):\n        target = target.tolist()\n    \n    # Combine and sort the feature and target\n    combined = list(zip(feature, target))\n    combined.sort(key=lambda x: x[0])\n    sorted_features = [x[0] for x in combined]\n    sorted_target = [x[1] for x in combined]\n    \n    # Check if all features are the same\n    if len(set(sorted_features)) == 1:\n        return (None, 0.0)\n    \n    # Find unique sorted features\n    unique_features = []\n    prev = None\n    for f in sorted_features:\n        if f != prev:\n            unique_features.append(f)\n            prev = f\n    if len(unique_features) < 2:\n        return (None, 0.0)\n    \n    # Compute midpoints between consecutive unique features\n    midpoints = []\n    for i in range(len(unique_features) - 1):\n        mid = (unique_features[i] + unique_features[i+1]) / 2\n        midpoints.append(mid)\n    \n    # Function to compute entropy\n    def compute_entropy(subset):\n        if not subset:\n            return 0.0\n        counts = Counter(subset)\n        n = len(subset)\n        entropy = 0.0\n        for count in counts.values():\n            prob = count / n\n            entropy -= prob * math.log2(prob)\n        return entropy\n    \n    n = len(sorted_target)\n    max_ig = -1\n    best_t = None\n    \n    for t in midpoints:\n        split_index = bisect.bisect_left(sorted_features, t)\n        L = sorted_target[:split_index]\n        R = sorted_target[split_index:]\n        \n        H_parent = compute_entropy(sorted_target)\n        H_L = compute_entropy(L)\n        H_R = compute_entropy(R)\n        \n        ig = H_parent - (len(L)/n)*H_L - (len(R)/n)*H_R\n        \n        if ig > max_ig:\n            max_ig = ig\n            best_t = t\n        elif ig == max_ig:\n            if t < best_t:\n                best_t = t\n    \n    if max_ig <= 0:\n        return (None, 0.0)\n    else:\n        rounded_t = round(best_t, 4)\n        rounded_ig = round(max_ig, 4)\n        return (rounded_t, rounded_ig)\n```\n\n### Explanation\n1. **Preprocessing**: The feature and target arrays are converted to lists if they are numpy arrays to ensure compatibility with list operations.\n2. **Combining and Sorting**: The feature and target values are combined into tuples, sorted by the feature values, and then separated into sorted lists.\n3. **Unique Features Check**: If all feature values are the same, no split is possible, so we return (None, 0.0).\n4. **Midpoints Calculation**: Midpoints between consecutive distinct feature values are computed to determine candidate thresholds.\n5. **Entropy Calculation**: For each subset (parent, left, right), the entropy is computed using the Shannon entropy formula.\n6. **Information Gain Calculation**: For each threshold, the information gain is calculated and compared to find the maximum gain.\n7. **Result**: The threshold and information gain are rounded to four decimal places and returned. If no positive information gain is found, (None, 0.0) is returned."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that dynamically retrieves a weight initialization function based on a given name. The function should return the corresponding initializer if it exists, or raise an error if it does not.\n\n### Approach\nThe approach involves the following steps:\n1. **Look up the initializer function by name**: Use the `globals()` function to access the module's global namespace and search for the initializer function by its name.\n2. **Check if the found object is callable**: Ensure that the object retrieved from the global namespace is a function that can be called.\n3. **Return the initializer function**: If a valid initializer function is found, return it.\n4. **Raise an error if no initializer is found**: If the initializer function with the given name does not exist, raise a `ValueError`.\n\nThis approach ensures that we dynamically retrieve the correct initializer function based on the provided name, making the solution flexible and scalable.\n\n### Solution Code\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    matrix = [[0.0 for _ in range(cols)] for rows in range(shape[0])]\n    return matrix\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    matrix = [[1.0 for _ in range(cols)] for rows in range(shape[0])]\n    return matrix\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    import random\n    random.seed(seed)\n    matrix = []\n    for row in range(shape[0]):\n        row_data = []\n        for col in range(shape[1]):\n            row_data.append(random.uniform(low, high))\n        matrix.append(row_data)\n    return matrix\n\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    for key, value in globals().items():\n        if key == name and callable(value):\n            return value\n    raise ValueError(\"Invalid initialization function.\")\n```\n\n### Explanation\n- **globals()**: This function returns a dictionary of the module's global variables, allowing us to search for the initializer function by name.\n- **callable()**: This function checks if an object is callable, ensuring that we only return functions that can be used to initialize weights.\n- **Error Handling**: If no initializer function is found with the given name, a `ValueError` is raised, providing clear feedback to the user.\n\nThis solution efficiently retrieves the appropriate initializer function, ensuring flexibility and correctness in dynamic weight initialization."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same features and thresholds, and produce the same predictions for the given data.\n\n### Approach\nThe approach to solve this problem involves recursively comparing the structure and content of the two decision trees. Here are the key steps:\n\n1. **Check Node Types**: Ensure both trees are either both Nodes or both Leaves. If they are of different types, the trees are not equivalent.\n2. **Compare Leaf Values**: If both trees are Leaves, check if their values are numerically equal within a small tolerance using `numpy.allclose`.\n3. **Compare Node Features and Thresholds**: If both trees are Nodes, check if they use the same feature index and have thresholds that are numerically equal up to a tolerance of \\(1 \\times 10^{-8}\\).\n4. **Recursive Comparison**: Recursively compare the left and right subtrees of each Node to ensure they are structurally and content-wise equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    def compare_leaf(a, b):\n        if type(a) != type(b):\n            return False\n        if isinstance(a, np.ndarray):\n            return np.allclose(a, b, atol=1e-8)\n        else:\n            return a == b\n\n    def compare_node(a, b):\n        if a.feature != b.feature:\n            return False\n        if abs(a.threshold - b.threshold) > 1e-8:\n            return False\n        return compare_trees(a.left, b.left) and compare_trees(a.right, b.right)\n\n    if type(tree_a) != type(tree_b):\n        return False\n    if isinstance(tree_a, Leaf):\n        return compare_leaf(tree_a.value, tree_b.value)\n    else:\n        # Both are Nodes\n        if tree_a.feature != tree_b.feature:\n            return False\n        if abs(tree_a.threshold - tree_b.threshold) > 1e-8:\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n```\n\n### Explanation\n- **Leaf Comparison**: The `compare_leaf` function checks if the values of two leaves are numerically equal. It handles different types (int, float, list, numpy array) and uses a tolerance for numerical comparisons.\n- **Node Comparison**: The `compare_node` function ensures that two nodes have the same feature index and threshold, then recursively compares their left and right subtrees.\n- **Recursive Structure Check**: The main `compare_trees` function uses these helper functions to ensure both trees have the same structure and content, returning `True` if they are equivalent and `False` otherwise.\n\nThis approach ensures that the decision trees are compared both structurally and content-wise, providing a robust solution to determine their equivalence."}
{"task_id": 292, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a single-point column crossover on two weight matrices, which is a common operation in genetic algorithms for neuro-evolution. This operation involves randomly selecting a column index and swapping all columns from that index onwards between the two parent matrices to create two new offspring matrices.\n\n### Approach\n1. **Validation**: First, we check if the two input weight matrices (parent1 and parent2) have the same shape. If they do not, we return -1 as specified.\n2. **Conversion to NumPy Arrays**: Convert the input lists of lists into NumPy arrays for efficient manipulation.\n3. **Column Crossover**: Create two new child matrices. For each child, the columns before the cutoff index are taken from the respective parent, and the columns from the cutoff index onwards are taken from the other parent.\n4. **Conversion Back to Lists**: Convert the resulting NumPy arrays back into lists of lists to match the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff: Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert parent matrices to NumPy arrays for efficient manipulation\n    parent1_array = np.array(parent1)\n    parent2_array = np.array(parent2)\n    \n    # Check if the parent matrices have the same shape\n    if parent1_array.shape != parent2_array.shape:\n        return -1\n    \n    # Get the number of columns in the matrices\n    num_columns = parent1_array.shape[1]\n    \n    # Create the two children matrices\n    # child1 takes from parent1 before cutoff and parent2 from cutoff onwards\n    child1 = np.column_stack((parent1_array[:, :cutoff], parent2_array[:, cutoff:])).tolist()\n    # child2 takes from parent2 before cutoff and parent1 from cutoff onwards\n    child2 = np.column_stack((parent2_array[:, :cutoff], parent1_array[:, cutoff:])).tolist()\n    \n    return (child1, child2)\n```\n\n### Explanation\n1. **Validation**: The function first converts the input lists into NumPy arrays and checks if their shapes match. If they do not, it returns -1.\n2. **Conversion to NumPy Arrays**: This allows efficient column-wise operations using NumPy's advanced array handling capabilities.\n3. **Column Crossover**: Using NumPy's `column_stack` function, we combine the appropriate columns from each parent to form the two new children matrices. The first child matrix takes columns before the cutoff from the first parent and from the cutoff onwards from the second parent. The second child matrix does the opposite.\n4. **Conversion Back to Lists**: Finally, the resulting NumPy arrays are converted back into lists of lists to meet the function's output requirements.\n\nThis approach ensures that the function efficiently handles the column crossover operation while maintaining the integrity of the input matrices and returning the results in the required format."}
{"task_id": 298, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the maximum likelihood N-gram log-probability for a given query using a small corpus. The function should return the sum of the natural logarithms of the probabilities for each consecutive N-gram in the query, rounded to 4 decimal places. If any N-gram is unseen in the corpus or if the query is too short, the function should return negative infinity.\n\n### Approach\n1. **Preprocessing the Corpus**: \n   - Split each sentence in the corpus into tokens.\n   - For N > 1, pad each token sequence with N-1 special tokens \"<bol>\" at the beginning and one \"<eol>\" at the end.\n   - Count all possible (N-1)-grams and N-grams from these padded token sequences.\n\n2. **Processing the Query**:\n   - Split the query into tokens.\n   - For N > 1, pad the query token sequence similarly.\n   - Check if the query is long enough to form N-grams. If not, return negative infinity.\n   - For each consecutive N-gram in the padded query, compute the probability using the counts from the preprocessed corpus. If any N-gram or its (N-1)-gram prefix is unseen, return negative infinity.\n\n3. **Log Probability Calculation**:\n   - For each N-gram, compute the probability and sum their natural logarithms.\n   - Return the sum rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict, Counter\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    if N < 1:\n        return float('-inf')\n\n    ngrams_counter = defaultdict(int)\n    n_minus_1_grams_counter = defaultdict(int)\n    total_tokens = 0\n\n    if N == 1:\n        # For unigrams, count all tokens in the entire corpus\n        total_tokens = sum(len(sentence.split()) for sentence in corpus)\n    else:\n        for sentence in corpus:\n            tokens = sentence.split()\n            # Pad with N-1 <bol> and <eol>\n            padded_tokens = ['<bol>'] * (N - 1) + tokens + ['<eol>']\n            # Count (N-1)-grams\n            for i in range(len(padded_tokens) - (N - 1) + 1):\n                gram = ' '.join(padded_tokens[i:i + (N - 1)])\n                n_minus_1_grams_counter[gram] += 1\n            # Count N-grams\n            for i in range(len(padded_tokens) - N + 1):\n                gram = ' '.join(padded_tokens[i:i + N])\n                ngrams_counter[gram] += 1\n\n    # Process the query\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N - 1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n\n    # Check if query is too short\n    if len(padded_query) < N:\n        return float('-inf')\n\n    total_log = 0.0\n\n    for i in range(len(padded_query) - N + 1):\n        current_gram = ' '.join(padded_query[i:i + N])\n        # Check if current_gram exists in ngrams_counter\n        if current_gram not in ngrams_counter:\n            return float('-inf')\n        # Check prefix if N > 1\n        if N > 1:\n            prefix = ' '.join(padded_query[i:i + (N - 1)])\n            if prefix not in n_minus_1_grams_counter:\n                return float('-inf')\n        # Compute probability\n        if N == 1:\n            prob = ngrams_counter[current_gram] / total_tokens\n        else:\n            prob = ngrams_counter[current_gram] / n_minus_1_grams_counter[prefix]\n        # Add to log\n        total_log += math.log(prob)\n\n    # Round to 4 decimal places\n    return round(total_log, 4)\n```\n\n### Explanation\n- **Preprocessing**: The corpus is split into tokens, and for N > 1, each token sequence is padded to create context for N-grams. This helps in accurately counting the occurrences of all possible (N-1)-grams and N-grams.\n- **Query Processing**: The query is similarly split into tokens and padded if necessary. Each consecutive N-gram in the padded query is checked against the preprocessed counts to ensure it exists.\n- **Probability Calculation**: For each valid N-gram, the probability is computed using the counts from the preprocessed data. The natural logarithm of these probabilities is summed up and returned, rounded to 4 decimal places. If any N-gram is missing, the function returns negative infinity."}
{"task_id": 302, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Spectral Clustering algorithm from scratch. The goal is to cluster data points into a specified number of clusters using an unsupervised learning technique. The algorithm involves several steps, including constructing a weighted adjacency matrix, computing the graph Laplacian, finding eigenvectors, and running k-means clustering on the resulting projection matrix.\n\n### Approach\n1. **Construct the Weighted Adjacency Matrix (W):** The weight between two points is defined as \\( w_{ij} = \\frac{1}{1 + \\|x_i - x_j\\|_2} \\). The diagonal elements are set to 0.\n\n2. **Compute the Graph Laplacian (L):** The Laplacian matrix is computed as \\( L = D - W \\), where \\( D \\) is the diagonal degree matrix, with each diagonal element being the sum of the corresponding row in \\( W \\).\n\n3. **Find Eigenvectors of the Laplacian:** The first \\( k \\) smallest eigenvalues of \\( L \\) are found, along with their corresponding eigenvectors. These eigenvectors are stacked to form the projection matrix \\( E \\).\n\n4. **Run k-means Clustering:** Using the first \\( k \\) rows of \\( E \\) as initial centroids, the algorithm iteratively assigns each point to the nearest centroid, updates the centroids, and continues until the assignments stabilize or a maximum of 100 iterations is reached. If a centroid becomes empty, it is reinitialized with a random point from \\( E \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return []\n    \n    # Step 1: Compute adjacency matrix W\n    W = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                W[i][j] = 0\n            else:\n                dx = X[i] - X[j]\n                distance = np.linalg.norm(dx)\n                W[i][j] = 1 / (1 + distance)\n    \n    # Step 2: Compute Laplacian matrix L = D - W\n    D = np.zeros_like(W)\n    for i in range(n):\n        D[i][i] = np.sum(W[i])\n    L = D - W\n    \n    # Step 3: Compute first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    eigenvalues.sort()\n    selected_eigenvalues = eigenvalues[:k]\n    selected_eigenvectors = eigenvectors[:, :k]\n    E = selected_eigenvectors.T  # Stack column-wise\n    \n    # Step 4: Run k-means on E\n    centroids = E[:k].copy()\n    assignments = None\n    for iter in range(100):\n        # Compute distances from each point to centroids\n        distances = np.zeros((n, k))\n        for i in range(n):\n            for j in range(k):\n                distances[i][j] = np.linalg.norm(E[i] - centroids[j])\n        # Find closest centroids\n        current_assignments = np.argmin(distances, axis=1)\n        # Check for empty centroids\n        empty_indices = np.where(current_assignments == -1)[0]\n        if len(empty_indices) > 0:\n            # Re-initialize empty centroids with random points\n            for idx in empty_indices:\n                centroids[idx] = E[np.random.randint(n)]\n            # Recompute distances and current_assignments\n            distances = np.zeros((n, k))\n            for i in range(n):\n                for j in range(k):\n                    distances[i][j] = np.linalg.norm(E[i] - centroids[j])\n            current_assignments = np.argmin(distances, axis=1)\n        # Check for convergence\n        if assignments is not None:\n            if np.array_equal(current_assignments, assignments):\n                break\n        else:\n            assignments = current_assignments.copy()\n        # Update centroids\n        for j in range(k):\n            mask = current_assignments == j\n            centroids[j] = np.mean(E[mask], axis=0)\n    else:\n        # Max iterations reached\n        pass\n    \n    # Convert assignments to list of integers\n    return current_assignments.tolist()\n```\n\n### Explanation\n1. **Weighted Adjacency Matrix (W):** This matrix is constructed by computing the weights between each pair of points based on their Euclidean distance.\n\n2. **Graph Laplacian (L):** The Laplacian matrix is derived from the adjacency matrix by subtracting it from the diagonal degree matrix.\n\n3. **Eigenvectors of Laplacian:** The smallest \\( k \\) eigenvalues and their corresponding eigenvectors are computed. These eigenvectors form the projection matrix \\( E \\).\n\n4. **k-means Clustering:** The algorithm initializes centroids with the first \\( k \\) rows of \\( E \\) and iteratively updates the assignments and centroids until convergence or a maximum number of iterations is reached. Empty centroids are reinitialized with random points from \\( E \\) to avoid empty clusters.\n\nThis approach ensures that the data points are clustered into the desired number of clusters using an unsupervised method, leveraging the properties of the Laplacian matrix and eigenvectors."}
{"task_id": 303, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a detailed debugging string comparing the current output of a program with the expected gold-standard output. The function should format the output in a specific multi-line string layout, including warnings if provided.\n\n### Approach\n1. **Extract Previous and Current Elements**: Determine the previous and current elements in the list based on the given index. The previous element is the one at the index just before the current element, or the first element if the current index is 0.\n2. **Retrieve Gold Values**: Fetch the expected gold values for both the previous and current labels from the provided dictionary.\n3. **Construct Lines**: Format each line of the debugging string according to the specified structure, including the top and bottom borders.\n4. **Combine Lines**: Join all the formatted lines into a single string with appropriate line breaks.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_index = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_index]\n    curr_mine, curr_label = params[ix]\n    \n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    top_border = \"------------------------- DEBUG -------------------------\"\n    bottom_border = \"----------------------- END DEBUG -----------------------\"\n    \n    line2 = f\"Mine (prev) [{prev_label}]: {prev_mine}\"\n    line3 = f\"Theirs (prev) [{prev_label}]: {prev_gold}\"\n    line4 = f\"Mine [{curr_label}]: {curr_mine}\"\n    line5 = f\"Theirs [{curr_label}]: {curr_gold}{warn_str}\"\n    \n    lines = [top_border, line2, line3, line4, line5, bottom_border]\n    return '\\n'.join(lines)\n```\n\n### Explanation\n1. **Extracting Elements**: The function calculates the previous index as the maximum of `ix - 1` and 0 to ensure it doesn't go out of bounds. It then extracts the mine value and label for both the previous and current elements.\n2. **Gold Values**: The expected gold values for the previous and current labels are retrieved from the dictionary.\n3. **Formatting Lines**: Each line of the debugging string is formatted using string formatting to ensure the correct structure, including the top and bottom borders.\n4. **Combining Lines**: The formatted lines are combined into a single string with newline characters to produce the final output.\n\nThis approach ensures that the debugging string is correctly formatted and includes all necessary information for comparison, including any provided warnings."}
{"task_id": 304, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the posterior mean (Maximum\u2013A-Posteriori estimate) of the regression coefficients in Bayesian linear regression with an unknown noise variance. The model assumes a normal\u2013inverse-gamma prior on the coefficients and the noise variance, and a Gaussian likelihood with identity noise covariance.\n\n### Approach\n1. **Preprocessing the Design Matrix**: If the `fit_intercept` flag is set to `True`, prepend a column of ones to the design matrix `X` to include an intercept term.\n2. **Determine the Size of the Coefficient Vector**: The size of the coefficient vector `b` is determined by the number of features in `X` after potentially adding the intercept term.\n3. **Compute the Prior Inverse Variance Matrix (V\u207b\u00b9)**: Convert the prior scale parameter `V` into an inverse matrix. Handle different cases for `V` being `None`, a scalar, a list/tuple, or a 2D array.\n4. **Compute the Design Matrix Cross-Product (X\u1d40X)**: This matrix is used to form the sum matrix for the posterior covariance.\n5. **Form the Sum Matrix (V\u207b\u00b9 + X\u1d40X)**: This matrix is inverted to get the posterior covariance matrix of the coefficients.\n6. **Compute the Prior Mean and Likelihood Terms**: Calculate the prior mean of the coefficients and the product of the design matrix and the target vector.\n7. **Compute the Posterior Mean (MAP Estimate)**: Use the posterior covariance matrix and the combined terms to get the posterior mean of the coefficients.\n8. **Round the Results**: Round each coefficient to 4 decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Prepend intercept if needed\n    if fit_intercept:\n        intercept = np.ones(X.shape[0], dtype=np.float64)\n        X = np.hstack((intercept.reshape(-1, 1), X))\n    \n    # Determine size of b\n    size_b = X.shape[1]\n    \n    # Compute V_inv\n    if V is None:\n        V_inv = np.identity(size_b)\n    elif isinstance(V, (int, float)):\n        V_inv = np.identity(size_b) * V\n    elif isinstance(V, (list, tuple)):\n        V_inv = np.identity(size_b)\n        for i in range(len(V)):\n            V_inv[i, i] = V[i]\n    else:\n        if V.shape != (size_b, size_b):\n            raise ValueError(\"V must be a square matrix of size (M+1) if fit_intercept is True, else (M, M).\")\n        V_inv = V\n    \n    # Compute X.T @ X\n    XtX = X.T @ X\n    \n    # Compute sum_matrix = V_inv + XtX\n    sum_matrix = V_inv + XtX\n    \n    # Compute \u03a3_b\n    try:\n        \u03a3_b = np.linalg.inv(sum_matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"sum_matrix is not invertible. This could be due to a degenerate design matrix.\")\n    \n    # Compute prior_mean = V_inv @ mu\n    prior_mean = V_inv @ mu\n    \n    # Compute X.T @ y\n    xty = X.T @ y\n    \n    # Compute sum_terms = prior_mean + xty\n    sum_terms = prior_mean + xty\n    \n    # Compute mu_b = \u03a3_b @ sum_terms\n    mu_b = \u03a3_b @ sum_terms\n    \n    # Round to 4 decimals\n    mu_b_rounded = [round(coeff, 4) for coeff in mu_b]\n    \n    return mu_b_rounded\n```\n\n### Explanation\nThe function `bayesian_posterior_mean` computes the posterior mean of the regression coefficients using Bayesian linear regression. It handles different prior specifications for the coefficients and noise variance, and optionally includes an intercept term. The function processes the input data, computes the necessary matrices, and applies the Bayesian framework to derive the posterior mean, which is then rounded to 4 decimal places for the final result."}
{"task_id": 308, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Mel filterbank matrix that converts a power spectrum into Mel band energies using matrix multiplication. The Mel filterbank is a set of triangular filters laid out on the Mel scale, which provides high resolution to low frequencies and low resolution to high frequencies.\n\n### Approach\n1. **Convert Frequency Limits to Mel Scale**: Convert the given minimum and maximum frequencies from Hertz to the Mel scale using the provided helper functions.\n2. **Generate Equally Spaced Mel Points**: Create `n_filters + 2` equally spaced points on the Mel scale between the converted minimum and maximum frequencies.\n3. **Convert Mel Points Back to Hertz**: Convert these Mel points back to Hertz to determine the corner frequencies of the triangular filters.\n4. **Compute Filter Weights**: For each DFT bin and each Mel filter, compute the left and right slopes of the triangular filter. The weight for each bin is the positive minimum of these slopes.\n5. **Normalize Weights**: If normalization is required, scale each weight so that the area under each filter equals 1.\n6. **Construct the Filterbank Matrix**: Return the filterbank matrix as a list of lists, with each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\"\"\"\n    if max_freq is None:\n        max_freq = fs // 2\n\n    # Convert min and max frequencies to Mel\n    mel_min = 2595 * np.log10(1 + min_freq / 700)\n    mel_max = 2595 * np.log10(1 + max_freq / 700)\n\n    # Create n_filters + 2 equally spaced Mel points\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n\n    # Convert Mel points back to Hz\n    f_points = 700 * (10**(mel_points / 2595) - 1)\n\n    # Prepare the filterbank matrix\n    filterbank = []\n    for i in range(n_filters):\n        if i == 0:\n            f_i_center = f_points[0]\n            f_i_low = 0\n            f_i_high = f_points[1]\n        elif i == n_filters - 1:\n            f_i_center = f_points[-1]\n            f_i_low = f_points[-2]\n            f_i_high = fs / 2\n        else:\n            f_i_center = f_points[i]\n            f_i_low = f_points[i - 1]\n            f_i_high = f_points[i + 1]\n\n        row = []\n        for k in range(N // 2 + 1):\n            k_center = k * fs / N\n            if k_center < f_i_low or k_center > f_i_high:\n                weight = 0.0\n            else:\n                left_dist = f_i_center - f_i_low\n                right_dist = f_i_high - f_i_center\n                left_slope = left_dist / (f_i_center - k_center)\n                right_slope = right_dist / (k_center - f_i_center)\n                # Take positive part of each slope\n                left_slope = max(left_slope, 0)\n                right_slope = max(right_slope, 0)\n                weight = min(left_slope, right_slope)\n            row.append(weight)\n        # Normalize if needed\n        if normalize:\n            total_width = f_i_high - f_i_low\n            if total_width == 0:\n                normalized_row = [0.0 for _ in row]\n            else:\n                scaling = 2.0 / total_width\n                normalized_row = [w * scaling for w in row]\n        else:\n            normalized_row = row\n        filterbank.append(normalized_row)\n\n    # Round each element to 4 decimal places\n    rounded_filterbank = []\n    for row in filterbank:\n        rounded_row = [round(w, 4) for w in row]\n        rounded_filterbank.append(rounded_row)\n    return rounded_filterbank\n```\n\n### Explanation\n1. **Mel Scale Conversion**: The function starts by converting the given minimum and maximum frequencies from Hertz to the Mel scale using the provided helper functions.\n2. **Mel Points Generation**: It then generates `n_filters + 2` equally spaced points on the Mel scale, which are then converted back to Hertz to determine the corner frequencies of the triangular filters.\n3. **Filter Weight Calculation**: For each DFT bin and each Mel filter, the function calculates the left and right slopes of the triangular filter. The weight for each bin is determined by the positive minimum of these slopes.\n4. **Normalization**: If required, each weight is scaled to ensure the area under each filter equals 1.\n5. **Matrix Construction**: The resulting filterbank matrix is constructed as a list of lists, with each element rounded to 4 decimal places for precision.\n\nThis approach ensures that the filterbank matrix accurately converts a power spectrum into Mel band energies, allowing for effective speech and audio processing tasks."}
{"task_id": 312, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Blackman-Harris window of arbitrary length. This window is a popular member of the cosine-sum family and is used to reduce spectral leakage in signal processing applications. The window can be generated in two forms: symmetric and periodic. The symmetric form is typically used for filter design, while the periodic form is suitable for FFT-based spectral analysis.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length is less than or equal to 0, return an empty list.\n   - If the window length is 1, return a list containing a single element, 1.0, regardless of the variant.\n\n2. **Window Coefficients**:\n   - The Blackman-Harris window is defined by the formula:\n     \\[\n     \\operatorname{BH}(n) = a_0 - a_1 \\cos\\left(\\frac{2\\pi n}{N}\\right) + a_2 \\cos\\left(\\frac{4\\pi n}{N}\\right) - a_3 \\cos\\left(\\frac{6\\pi n}{N}\\right)\n     \\]\n     where \\(a_0 = 0.35875\\), \\(a_1 = 0.48829\\), \\(a_2 = 0.14128\\), and \\(a_3 = 0.01168\\).\n\n3. **Determine Window Length**:\n   - For the symmetric form, the window length \\(N\\) is set to `window_len - 1`.\n   - For the periodic form, the window length \\(N\\) is set to `window_len`.\n\n4. **Generate Window**:\n   - For each sample index \\(n\\) from 0 to `window_len - 1`, compute the value using the Blackman-Harris formula.\n   - Use the cosine function to evaluate the angles and compute the window coefficients.\n   - Round each coefficient to four decimal places and store the result.\n\n### Solution Code\n```python\nimport math\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    if symmetric:\n        N = window_len - 1\n    else:\n        N = window_len\n    \n    window = []\n    for n in range(window_len):\n        angle1 = 2 * math.pi * n / N\n        angle2 = 4 * math.pi * n / N\n        angle3 = 6 * math.pi * n / N\n        \n        cos1 = math.cos(angle1)\n        cos2 = math.cos(angle2)\n        cos3 = math.cos(angle3)\n        \n        value = a0 - a1 * cos1 + a2 * cos2 - a3 * cos3\n        window.append(round(value, 4))\n    \n    return window\n```\n\n### Explanation\n- **Special Cases**: The function first checks for special cases where the window length is 0 or 1, returning appropriate results immediately.\n- **Coefficient Calculation**: The coefficients \\(a_0, a_1, a_2, a_3\\) are predefined constants.\n- **Window Length Determination**: Depending on whether the window is symmetric or periodic, the appropriate value of \\(N\\) is chosen.\n- **Window Generation**: For each index \\(n\\), the function computes the cosine terms, evaluates the Blackman-Harris formula, and appends the result to the window list after rounding to four decimal places.\n\nThis approach ensures that the function efficiently generates the required window coefficients, handling both symmetric and periodic forms correctly."}
{"task_id": 313, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the expected value of the logarithm of a specific component of a Dirichlet-distributed random variable. The Dirichlet distribution is commonly used in Bayesian models, such as Latent Dirichlet Allocation, where the parameters are represented as a 2-D matrix. The task requires us to compute the expectation using the digamma function, which is the first derivative of the logarithm of the Gamma function.\n\n### Approach\n1. **Problem Analysis**: The expectation of the logarithm of the t-th component of a Dirichlet random variable can be computed using the formula:\n   \\[\n   E[\\log X_t] = \\psi(\\gamma[d,t]) - \\psi\\left(\\sum_{k} \\gamma[d,k]\\right)\n   \\]\n   where \\(\\psi\\) is the digamma function. The digamma function can be computed using a combination of a recursive approach for small values and an asymptotic expansion for larger values.\n\n2. **Digamma Function Implementation**: The digamma function is implemented in two parts:\n   - For small values (x \u2264 6), we use the recursive relation \\(\\psi(x) = \\psi(x+1) - 1/x\\) starting from the known value \\(\\psi(1) = 0\\).\n   - For larger values (x > 6), we use the asymptotic expansion:\n     \\[\n     \\psi(x) \\approx \\ln x - \\frac{1}{2x} - \\frac{1}{12x^2} + \\frac{1}{120x^4} - \\frac{1}{252x^6}\n     \\]\n\n3. **Sum Calculation**: Compute the sum of the parameters for the given row (d) and the specific component (t).\n\n4. **Result Calculation**: Subtract the digamma values at the component parameter and the sum of the row parameters, then round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def digamma(x):\n        if x <= 1:\n            return 0.0\n        elif x <= 6:\n            return sum(1.0 / k for k in range(1, x))\n        else:\n            ln_x = math.log(x)\n            term1 = ln_x\n            term2 = -1.0 / (2 * x)\n            term3 = -1.0 / (12 * x**2)\n            term4 = 1.0 / (120 * x**4)\n            term5 = -1.0 / (252 * x**6)\n            return term1 + term2 + term3 + term4 + term5\n\n    sum_gamma = sum(gamma[d])\n    gamma_val = gamma[d][t]\n    psi_g = digamma(gamma_val)\n    psi_sum = digamma(sum_gamma)\n    result = psi_g - psi_sum\n    return round(result, 4)\n```\n\n### Explanation\n- **Digamma Function**: The function `digamma` computes the digamma value for a given x. For x \u2264 6, it uses the sum of reciprocals of integers up to x-1. For x > 6, it uses the asymptotic expansion to approximate the digamma value.\n- **Sum Calculation**: The sum of the parameters for the given row is computed to determine the normalization parameter for the Dirichlet distribution.\n- **Result Calculation**: The difference between the digamma values at the component parameter and the sum of the row parameters is computed and rounded to four decimal places to get the final result.\n\nThis approach efficiently computes the expected value using the properties of the digamma function and handles both small and large values appropriately."}
{"task_id": 331, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the out-of-bag mean squared error (OOB-MSE) for a Random Forest regressor. The OOB-MSE is an unbiased performance estimate that uses predictions from trees that did not see the sample during training.\n\n### Approach\n1. **Understand the Inputs**: \n   - `y_true` is the true target values for each sample.\n   - `predictions` is a 2D array where each row represents a sample and each column represents a tree's prediction for that sample.\n   - `oob_mask` is a boolean matrix indicating whether a sample was out-of-bag (OOB) for each tree.\n\n2. **Collect OOB Predictions**: For each sample, collect predictions from trees where the sample was OOB. This is determined by checking the corresponding value in `oob_mask`.\n\n3. **Compute Average Prediction**: For each sample that has at least one OOB prediction, compute the average of these OOB predictions.\n\n4. **Calculate Squared Error**: Compute the squared error between the average prediction and the true value for each sample.\n\n5. **Compute OOB-MSE**: Average the squared errors across all samples that have at least one OOB prediction. If no sample has an OOB prediction, return -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\"\"\"\n    n_samples = y_true.shape[0]\n    n_estimators = predictions.shape[1]\n    \n    total_error = 0.0\n    count = 0\n    \n    for i in range(n_samples):\n        oob_preds = []\n        for j in range(n_estimators):\n            if oob_mask[i][j]:\n                oob_preds.append(predictions[i][j])\n        if oob_preds:\n            avg_pred = np.mean(oob_preds)\n            error = (avg_pred - y_true[i]) ** 2\n            total_error += error\n            count += 1\n    \n    if count == 0:\n        return -1\n    else:\n        mean_error = total_error / count\n        return round(mean_error, 4)\n```\n\n### Explanation\n- **Initialization**: The function initializes variables to accumulate the total error and count the number of samples with OOB predictions.\n- **Loop Through Samples**: For each sample, it collects predictions from trees where the sample was OOB.\n- **Compute Average and Error**: For each sample with OOB predictions, it computes the average prediction, the squared error, and accumulates these values.\n- **Return Result**: If no sample has OOB predictions, it returns -1. Otherwise, it computes the mean of the squared errors and returns it rounded to four decimal places.\n\nThis approach ensures that we accurately compute the unbiased performance estimate for the Random Forest regressor using only the OOB predictions."}
{"task_id": 332, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Frequent-Pattern (FP) Tree from a given set of transactions and a minimum support threshold. The FP-Tree is represented as a nested dictionary where each node stores the number of transactions that share the path ending at that node and its direct descendants.\n\n### Approach\n1. **Compute Frequencies**: Calculate the frequency of each item across all transactions. The frequency is the number of transactions in which an item appears.\n2. **Filter Items**: Discard items whose frequency is less than the given minimum support threshold.\n3. **Sort Items**: Create a global ordering of the remaining items. The items are sorted first by decreasing frequency and then alphabetically to break ties.\n4. **Build the Tree**: For each transaction, remove duplicates, filter out non-frequent items, and sort the remaining items according to the global order. Traverse the tree from the root, updating the support count for each node along the path.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\"\"\"\n    if not transactions:\n        return {'support': 0, 'children': {}}\n    \n    # Compute the frequency of each item across all transactions\n    frequency = Counter()\n    for trans in transactions:\n        for item in trans:\n            frequency[item] += 1\n    \n    # Filter items that meet or exceed the minimum support\n    items = [item for item in frequency if frequency[item] >= min_support]\n    if not items:\n        return {'support': 0, 'children': {}}\n    \n    # Sort items by decreasing support, then alphabetically\n    sorted_items = sorted(items, key=lambda x: (-frequency[x], x))\n    \n    # Initialize the root node\n    root = {'support': 0, 'children': {}}\n    \n    for trans in transactions:\n        # Deduplicate the transaction and filter frequent items\n        unique_items = list(set(trans))\n        filtered = [item for item in unique_items if frequency[item] >= min_support]\n        # Sort the transaction according to the global order\n        sorted_trans = sorted(filtered, key=lambda x: (-frequency[x], x))\n        \n        current = root\n        for item in sorted_trans:\n            if item in current['children']:\n                current = current['children'][item]\n                current['support'] += 1\n            else:\n                new_node = {'support': 1, 'children': {}}\n                current['children'][item] = new_node\n                current = new_node\n                current['support'] += 1  # Increment support for the new node\n    \n    return root\n```\n\n### Explanation\n1. **Frequency Calculation**: We use a `Counter` to count how many times each item appears across all transactions.\n2. **Filtering**: Items with a frequency below the minimum support threshold are discarded.\n3. **Sorting**: Items are sorted by their frequency in descending order and alphabetically for ties.\n4. **Tree Construction**: For each transaction, we deduplicate and filter items, then traverse the tree to update the support count for each node along the path. This ensures that each node's support count accurately reflects the number of transactions that include the path ending at that node.\n\nThis approach efficiently constructs the FP-Tree by leveraging sorting and traversal, ensuring that the tree accurately represents the frequent patterns in the transactions."}
{"task_id": 336, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the kernel matrix using the linear kernel function. The kernel function is defined as \\( k(x, y) = x \\cdot y + c_0 \\), where \\( x \\cdot y \\) is the dot product of two input vectors, and \\( c_0 \\) is an optional constant. The function should handle both NumPy arrays and Python lists, and return the kernel matrix rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: \n   - Check if the input collection of vectors \\( X \\) is empty. If it is, return an empty list.\n   - If \\( Y \\) is not provided (i.e., it is `None`), set \\( Y \\) to be the same as \\( X \\).\n   - Ensure that both \\( X \\) and \\( Y \\) have the same number of columns. If they do not, return -1.\n\n2. **Conversion to NumPy Arrays**:\n   - Convert both \\( X \\) and \\( Y \\) into NumPy arrays for consistent manipulation.\n\n3. **Kernel Matrix Computation**:\n   - Compute the dot product of each row in \\( X \\) with each row in \\( Y \\).\n   - Add the constant \\( c_0 \\) to each element of the resulting matrix.\n\n4. **Rounding and Conversion**:\n   - Round each element of the kernel matrix to four decimal places.\n   - Convert the resulting matrix into a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    if not X:\n        return []\n    \n    if Y is None:\n        Y = X\n    \n    # Check if X and Y have the same number of columns\n    if isinstance(X, np.ndarray):\n        cols_X = X.shape[1]\n    else:\n        cols_X = len(X[0]) if X else 0\n    \n    if isinstance(Y, np.ndarray):\n        cols_Y = Y.shape[1]\n    else:\n        cols_Y = len(Y[0]) if Y else 0\n    \n    if cols_X != cols_Y:\n        return -1\n    \n    # Convert X and Y to NumPy arrays\n    X_arr = np.array(X) if not isinstance(X, np.ndarray) else X\n    Y_arr = np.array(Y) if not isinstance(Y, np.ndarray) else Y\n    \n    # Compute the kernel matrix\n    K = X_arr.dot(Y_arr) + c0\n    K_rounded = np.round(K, 4)\n    \n    return K_rounded.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function first checks if \\( X \\) is empty and returns an empty list if true. If \\( Y \\) is not provided, it is set to \\( X \\).\n- **Validation**: The function ensures that both \\( X \\) and \\( Y \\) have the same number of columns. If they do not, it returns -1.\n- **Conversion and Computation**: The input vectors are converted to NumPy arrays to leverage efficient matrix operations. The dot product of these arrays is computed, and the constant \\( c_0 \\) is added to each element.\n- **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted into a list of lists for the final output.\n\nThis approach ensures that the function handles various input types and edge cases efficiently, providing the correct kernel matrix as required."}
{"task_id": 340, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single-step update of the Adam optimizer. The Adam optimizer is a popular gradient descent optimization algorithm that uses adaptive moment estimation to update model parameters. The goal is to compute the updated parameter value, along with the new moment estimates, using the provided gradient, time-step, and current moment estimates.\n\n### Approach\nThe Adam optimizer updates model parameters using the following steps:\n1. **Compute the new moment estimates (m_t and v_t):**\n   - The first moment estimate (m_t) is computed as a weighted average of the current moment estimate (m) and the gradient (g).\n   - The second moment estimate (v_t) is computed similarly using the current variance estimate (v) and the squared gradient (g\u00b2).\n\n2. **Compute the corrected moment estimates (m_hat and v_hat):**\n   - These are the normalized versions of m_t and v_t, adjusted by the decay rates beta1 and beta2 respectively.\n\n3. **Update the parameter (\u03b8'):**\n   - If the gradient is zero, the parameter remains unchanged.\n   - Otherwise, the parameter is updated using the learning rate (lr), the corrected first moment estimate (m_hat), and the corrected second moment estimate (v_hat).\n\n4. **Return the updated parameter and the new moment estimates:**\n   - The results are rounded to 8 decimal places and converted to lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to **8** decimal places.\n    \"\"\"\n    # Compute the new moment estimates\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * grad**2\n\n    # Compute the corrected moment estimates\n    m_hat = m_t / (1 - beta1**t)\n    v_hat = v_t / (1 - beta2**t)\n\n    # Check if the gradient is zero\n    if np.all(grad == 0):\n        new_theta = param\n    else:\n        # Update the parameter using the Adam rule\n        new_theta = param - lr * m_hat / np.sqrt(v_hat + eps)\n\n    # The new moment estimates are m_t and v_t\n    new_m = m_t\n    new_v = v_t\n\n    # Round to 8 decimal places and convert to lists\n    new_theta_rounded = np.round(new_theta, 8).tolist()\n    new_m_rounded = np.round(new_m, 8).tolist()\n    new_v_rounded = np.round(new_v, 8).tolist()\n\n    return (new_theta_rounded, new_m_rounded, new_v_rounded)\n```\n\n### Explanation\nThe function `adam_update` performs a single optimization step using the Adam algorithm. It takes the current parameter value, gradient, time-step, and moment estimates as inputs, and returns the updated parameter along with the new moment estimates. The key steps include computing the new moment estimates, correcting them using decay rates, updating the parameter based on the learning rate and corrected moments, and returning the results rounded to 8 decimal places. This approach ensures that the parameter is updated in a way that adapts to the gradient changes, making it efficient for training deep learning models."}
{"task_id": 343, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) used in Reinforcement Learning to search for good policy parameters. The goal is to update the sampling distribution by retaining the top fraction of samples based on their rewards and then compute the mean and variance of these retained samples.\n\n### Approach\n1. **Calculate the Number of Samples to Retain**: Determine the number of top samples to retain using the given fraction of the total samples. This is done by computing the integer part of the product of the fraction and the number of samples.\n2. **Sort Samples by Rewards**: Sort the samples based on their rewards in ascending order. This allows us to easily pick the top samples with the highest rewards.\n3. **Select Top Samples**: Extract the top samples from the sorted list. These are the samples with the highest rewards.\n4. **Compute Mean and Variance**: Calculate the mean and variance of the retained samples. The mean is computed across each dimension, and the variance is computed as the average of the squared differences from the mean for each dimension.\n5. **Round Results**: Round the computed mean and variance to four decimal places and return them as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    N = len(theta_samples)\n    K = int(retain_prcnt * N)\n    \n    # Sort the indices based on rewards in ascending order\n    sorted_indices = np.argsort(rewards)\n    \n    # Get the top K indices\n    top_indices = sorted_indices[-K:]\n    \n    # Extract the top samples\n    top_samples = theta_samples[top_indices]\n    \n    # Compute mean and variance\n    mean = np.mean(top_samples, axis=0)\n    var = np.var(top_samples, axis=0)\n    \n    # Round to four decimal places\n    rounded_mean = np.round(mean, 4)\n    rounded_var = np.round(var, 4)\n    \n    # Convert to lists\n    return (rounded_mean.tolist(), rounded_var.tolist())\n```\n\n### Explanation\n1. **Calculate K**: The number of samples to retain is calculated using `int(retain_prcnt * N)`, where `N` is the total number of samples.\n2. **Sort Indices**: The indices of the samples are sorted based on their rewards. This sorting ensures that the highest rewards come first.\n3. **Top Samples**: The top `K` samples are selected from the sorted indices, ensuring we retain the most rewarding samples.\n4. **Mean and Variance**: The mean and variance are computed across each dimension of the retained samples. The mean is the average of each column, and the variance is the average of the squared differences from the mean for each column.\n5. **Rounding**: The results are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently narrows down the most promising parameter vectors and provides a statistically significant update for the sampling distribution in the Cross-Entropy Method."}
{"task_id": 353, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the base-2 Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of impurity in decision trees and is defined as the negative sum of the probabilities of each class label multiplied by the logarithm of those probabilities.\n\n### Approach\n1. **Handle Special Cases**: \n   - If the input is an empty list, return 0.0.\n   - If all labels are identical, return 0.0.\n\n2. **Compute Probabilities**:\n   - Use a counter to count the occurrences of each label.\n   - Calculate the probability of each label by dividing its count by the total number of labels.\n\n3. **Calculate Entropy**:\n   - For each probability, compute the term \\( p_i \\log_2 p_i \\) and sum these terms.\n   - The entropy is the negative of this sum.\n\n4. **Round the Result**:\n   - Round the computed entropy to 5 decimal places before returning.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    if len(counts) == 1:\n        return 0.0\n    total = len(labels)\n    entropy_sum = 0.0\n    for p in counts.values():\n        entropy_sum += p * math.log2(p)\n    entropy = -entropy_sum\n    return round(entropy, 5)\n```\n\n### Explanation\n- **Special Cases Handling**: The function first checks if the input is empty or if all labels are the same, returning 0.0 in both cases as there is no entropy.\n- **Probability Calculation**: Using the `Counter` from the `collections` module, we count the occurrences of each label. The probability of each label is then computed by dividing its count by the total number of labels.\n- **Entropy Calculation**: For each probability, the term \\( p_i \\log_2 p_i \\) is computed and summed. The entropy is the negative of this sum, which gives the measure of impurity.\n- **Rounding**: The result is rounded to 5 decimal places to meet the problem's requirements.\n\nThis approach efficiently handles various input types, including lists and NumPy arrays, ensuring accurate computation of Shannon entropy."}
{"task_id": 355, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Principal Component Analysis (PCA) with a deterministic sign fix. PCA is a dimensionality reduction technique that transforms data into a set of principal components, which capture the most variance in the data. The solution must handle two solvers: singular value decomposition (SVD) and eigen-decomposition of the covariance matrix.\n\n### Approach\n1. **Standardize the Data**: Subtract the feature-wise mean from the data to center it.\n2. **Determine Principal Directions**: Use either SVD or eigen-decomposition to find the principal directions (eigenvectors or singular vectors).\n3. **Sort Directions**: Sort the principal directions in descending order based on the variance they explain.\n4. **Fix Sign Deterministically**: Ensure the sign of each direction is consistent by flipping the direction and the corresponding projection if the first non-zero element is negative.\n5. **Project Data**: Project the centered data onto the retained principal directions to obtain the scores.\n6. **Compute Variance Ratios**: Calculate the fraction of total variance explained by each principal component.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Standardize the data by subtracting the feature-wise mean\n    mean_data = data - np.mean(data, axis=1)\n    \n    # Cap the number of components at the number of features\n    n_components = min(n_components, data.shape[1])\n    \n    if solver == \"svd\":\n        # Use singular value decomposition\n        svd = np.linalg.svd(mean_data)\n        directions = svd[1][:, :n_components]\n        singular_values = svd[0][:n_components]\n        projection_matrix = directions / singular_values\n        \n        # Sort directions by variance descending\n        components = sorted(zip(singular_values, directions), key=lambda x: -x[0])\n        sorted_directions = [d for _, d in components]\n        \n        # Fix the sign of each direction\n        for i, direction in enumerate(sorted_directions):\n            if np.any(direction < 0):\n                first_non_zero = np.argmax(direction)\n                if direction[first_non_zero] < 0:\n                    direction *= -1\n                    projection_matrix[:, i] *= -1\n        \n        # Project the data\n        scores = mean_data @ projection_matrix\n    else:\n        # Use eigen-decomposition of the covariance matrix\n        cov = np.linalg.covariance(mean_data, rowvar=False, ddof=1)\n        eig = np.linalg.eigh(cov)\n        directions = eig[1][:, :n_components]\n        eigenvalues = eig[0][:n_components]\n        projection_matrix = directions / eigenvalues\n        \n        # Sort directions by variance descending\n        components = sorted(zip(eigenvalues, directions), key=lambda x: -x[0])\n        sorted_directions = [d for _, d in components]\n        \n        # Fix the sign of each direction\n        for i, direction in enumerate(sorted_directions):\n            if np.any(direction < 0):\n                first_non_zero = np.argmax(direction)\n                if direction[first_non_zero] < 0:\n                    direction *= -1\n                    projection_matrix[:, i] *= -1\n        \n        # Project the data\n        scores = mean_data @ projection_matrix\n    \n    # Calculate explained variance ratios\n    if solver == \"svd\":\n        total_variance = np.sum(singular_values)\n        ratios = []\n        cumulative = 0.0\n        for i in range(n_components):\n            cumulative += singular_values[i]\n            ratio = cumulative / total_variance\n            ratios.append(round(ratio, 4))\n    else:\n        total_variance = np.sum(eigenvalues)\n        ratios = []\n        cumulative = 0.0\n        for i in range(n_components):\n            cumulative += eigenvalues[i]\n            ratio = cumulative / total_variance\n            ratios.append(round(ratio, 4))\n    \n    # Convert scores to a list of lists with rounded values\n    rounded_scores = np.round(scores, 4)\n    scores_list = [list(row) for row in rounded_scores]\n    \n    return (scores_list, ratios)\n```\n\n### Explanation\n1. **Standardization**: The data is centered by subtracting the mean of each feature from the data matrix.\n2. **Principal Directions**: Depending on the solver, either SVD or eigen-decomposition is used to find the principal directions. SVD provides singular vectors and values, while eigen-decomposition provides eigenvectors and eigenvalues.\n3. **Sorting and Sign Fixing**: Directions are sorted by the variance they explain. The sign of each direction is fixed to ensure consistency by flipping the direction and the corresponding projection if the first non-zero element is negative.\n4. **Projection**: The centered data is projected onto the principal directions to obtain the scores.\n5. **Variance Ratios**: The fraction of total variance explained by each principal component is computed and returned as a list of ratios.\n\nThis approach ensures that the PCA results are consistent and deterministic, making it suitable for applications where reproducibility is crucial."}
{"task_id": 356, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that extracts the correct prediction from a `Leaf` instance in a decision tree. The function should handle both classification and regression cases, returning the appropriate result based on the type of tree.\n\n### Approach\n1. **Problem Analysis**: The `Leaf` class stores either a list of class probabilities (for classification) or a scalar value (for regression). The task is to predict the class index for classification trees by finding the index of the highest probability, or return the scalar value for regression trees.\n2. **Intuition**: For classification, the highest probability value is found, and its index is returned. For regression, the scalar value is returned directly. The function must handle both cases efficiently.\n3. **Algorithm Selection**: Use Python's built-in functions to find the maximum value in a list and its index. This approach is efficient and straightforward for the given problem constraints.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        prob_list = leaf.value\n        max_prob = max(prob_list)\n        return prob_list.index(max_prob)\n    else:\n        return leaf.value\n```\n\n### Explanation\n- **Classification Case**: When `classifier` is `True`, the function retrieves the list of probabilities from the `Leaf` object. It finds the maximum probability value and returns the index of this value. This index represents the predicted class.\n- **Regression Case**: When `classifier` is `False`, the function directly returns the scalar value stored in the `Leaf` object, which represents the predicted mean for the regression task.\n\nThis approach ensures that the function efficiently and correctly handles both classification and regression predictions based on the structure of the `Leaf` object."}
{"task_id": 357, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to partition the given data into K clusters such that each sample belongs to the cluster with the nearest centroid. The centroids are initialized with the first K samples and are updated iteratively until convergence or a specified maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: If K is 1, the centroid is the mean of all data points. Otherwise, the centroids are initialized with the first K samples.\n2. **Iteration**: For each iteration, assign each sample to the nearest centroid, compute new centroids as the mean of the samples in each cluster, and check for convergence.\n3. **Convergence Check**: If the centroids do not change significantly (within a tolerance of 1e-6), the algorithm stops early.\n4. **Sorting and Rounding**: After convergence, the centroids are sorted lexicographically and rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    if K == 0:\n        return []\n    \n    if K == 1:\n        centroids = np.mean(data, axis=0)\n    else:\n        centroids = data[:K]\n    \n    for _ in range(max_iters):\n        # Compute distances from each sample to each centroid\n        distances = np.array([np.linalg.norm(sample - c) for sample, c in zip(data, centroids)])\n        closest_indices = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = []\n        for i in range(K):\n            mask = closest_indices == i\n            samples = data[mask]\n            if len(samples) == 0:\n                new_c = centroids[i]\n            else:\n                new_c = np.mean(samples, axis=0)\n            new_centroids.append(new_c)\n        \n        # Check for convergence\n        if np.max(np.abs(centroids - new_centroids)) < 1e-6:\n            break\n        centroids = new_centroids\n    \n    # Sort the centroids lexicographically\n    sorted_centroids = np.sort(centroids, axis=0)\n    # Round to four decimals\n    rounded = [[round(x, 4) for x in c] for c in sorted_centroids]\n    \n    return rounded\n```\n\n### Explanation\n1. **Initialization**: The centroids are set based on the first K samples if K > 1. If K is 1, the centroid is the mean of all data points.\n2. **Distance Calculation**: For each sample, the Euclidean distance to each centroid is computed to determine the closest centroid.\n3. **Cluster Assignment**: Each sample is assigned to the nearest centroid, and new centroids are computed as the mean of the samples in each cluster.\n4. **Convergence Check**: The algorithm stops if the centroids do not change significantly between iterations.\n5. **Sorting and Rounding**: The final centroids are sorted lexicographically and rounded to four decimal places before being returned.\n\nThis approach ensures that the data is partitioned into K clusters efficiently and correctly, providing a deterministic result for testing and analysis."}
{"task_id": 362, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an arbitrary 2-D list or NumPy array of non-negative numbers into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. If any row sums to 0, making normalization impossible, the function should return -1.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input data is first converted into a NumPy array of floats to handle both lists and NumPy arrays uniformly.\n2. **Compute Row Sums**: Calculate the sum of each row in the NumPy array.\n3. **Check for Zero Row Sums**: If any row sum is zero, return -1 as normalization is impossible.\n4. **Normalize Each Row**: Divide each element in a row by the row's sum to make the row stochastic.\n5. **Round Values**: Round each element of the resulting matrix to 4 decimal places.\n6. **Convert to List of Lists**: Convert the normalized matrix back into a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    data = np.array(data, dtype=np.float64)\n    \n    # Calculate the sum of each row\n    row_sums = np.sum(data, axis=1)\n    \n    # Check if any row sum is zero\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row\n    normalized_data = data / row_sums\n    \n    # Round to 4 decimal places\n    rounded_data = np.round(normalized_data, 4)\n    \n    # Convert the result to a list of lists\n    result = rounded_data.tolist()\n    \n    return result\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage its efficient array operations.\n2. **Row Sum Calculation**: Using `np.sum` with `axis=1` computes the sum of each row.\n3. **Zero Row Check**: The function checks if any row sum is zero using `np.any(row_sums == 0)`. If true, it returns -1.\n4. **Normalization**: Each row is normalized by dividing each element by the row sum.\n5. **Rounding**: The normalized values are rounded to 4 decimal places using `np.round`.\n6. **Conversion to List**: The resulting matrix is converted back to a list of lists using `tolist()` to meet the output requirements.\n\nThis approach ensures that the function efficiently handles both lists and NumPy arrays, performs necessary checks, and returns the correctly normalized matrix or -1 if normalization is impossible."}
{"task_id": 363, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if the action and observation spaces of an environment in a reinforcement learning (RL) setup are continuous based on specific conditions. The problem involves checking whether these spaces are continuous, considering both composite spaces (like Tuple and Dict) and simple spaces (like Box).\n\n### Approach\n1. **Problem Analysis**:\n   - The action and observation spaces can be either continuous or non-continuous.\n   - A space is considered continuous if it is an instance of `Box`.\n   - For composite spaces (Tuple or Dict), the space is continuous only if every sub-space is a `Box`.\n\n2. **Key Insights**:\n   - If the action space is a composite (Tuple or Dict), each sub-space must be a `Box` for the action space to be continuous.\n   - Similarly, if the observation space is a composite, each sub-space must be a `Box` for the observation space to be continuous.\n   - If the action or observation space is not a composite, it is continuous if it is a `Box`.\n\n3. **Algorithm**:\n   - Create a helper function to check if a space is continuous based on whether it is a composite.\n   - Use this helper function to check both the action and observation spaces separately.\n   - Return a tuple indicating whether each space is continuous.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# Minimal imitation of Gym's space classes\nclass Space:\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def is_continuous_space(space, is_tuple_or_dict):\n        if is_tuple_or_dict:\n            if not isinstance(space, (Tuple, Dict)):\n                return False\n            if isinstance(space, Tuple):\n                subs = space.spaces\n            else:  # Dict\n                subs = space.spaces.values()\n            for s in subs:\n                if not isinstance(s, Box):\n                    return False\n            return True\n        else:\n            return isinstance(space, Box)\n    \n    cont_action = is_continuous_space(env.action_space, tuple_action)\n    cont_obs = is_continuous_space(env.observation_space, tuple_obs)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Helper Function**: The helper function `is_continuous_space` checks if a given space is continuous. It takes two parameters: the space to check and a flag indicating if the space is a composite (Tuple or Dict).\n- **Composite Check**: If the space is a composite, it checks each sub-space to ensure they are all `Box` instances.\n- **Non-Composite Check**: If the space is not a composite, it directly checks if it is a `Box`.\n- **Main Function**: The main function `is_continuous` uses this helper function to check both the action and observation spaces, returning a tuple of boolean values indicating their continuity."}
{"task_id": 369, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that aggregates the outputs of multiple Gradient Boosting trees to produce predictions for either regression or multi-class classification tasks. The function should handle both cases efficiently using vectorized operations in NumPy.\n\n### Approach\n1. **Initialization**: Start with a running prediction initialized to zero, which will be updated by each tree's output.\n2. **Summing Updates**: Sum all the individual tree outputs and scale them by the learning rate. This sum is then subtracted from the running prediction.\n3. **Regression Handling**: For regression, the running prediction is returned after rounding to 4 decimal places.\n4. **Classification Handling**: For multi-class classification, the running prediction (which is in the form of logits) is converted to probabilities using the soft-max function. The predicted class is then obtained by taking the argmax of these probabilities.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        if regression:\n            return np.array([], dtype=np.float64)\n        else:\n            return np.array([], dtype=np.int64)\n    \n    # Get the shape of the updates\n    update_shape = updates[0].shape\n    n_samples = update_shape[0]\n    n_classes = update_shape[1] if not regression else 0  # Not used for regression\n    \n    # Initialize running prediction\n    running_pred = np.zeros_like(updates[0])\n    \n    # Sum all updates and scale by learning rate\n    summed_updates = np.sum(updates, axis=0)\n    running_pred = - learning_rate * summed_updates\n    \n    if regression:\n        # Round to 4 decimals\n        return np.round(running_pred, 4)\n    else:\n        # Compute probabilities\n        probabilities = np.exp(running_pred) / np.sum(np.exp(running_pred), axis=1)\n        # Get predicted classes\n        predicted_classes = np.argmax(probabilities, axis=1)\n        return predicted_classes\n```\n\n### Explanation\n1. **Initialization**: The running prediction is initialized to a zero array with the same shape as the first update.\n2. **Summing Updates**: The function sums all the updates along the first axis (which corresponds to the samples) and scales the result by the learning rate. This scaled sum is then subtracted from the running prediction.\n3. **Regression**: If the task is regression, the running prediction is returned after rounding to 4 decimal places.\n4. **Classification**: If the task is classification, the running prediction (logits) is converted to probabilities using the exponential function and the soft-max transformation. The predicted class for each sample is obtained by taking the index of the maximum probability value in each row.\n\nThis approach ensures that the function is fully vectorized and efficient, leveraging NumPy operations to handle large datasets without the need for explicit loops."}
{"task_id": 373, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Gini impurity of a discrete label sequence. The Gini impurity is a measure used in decision-tree learning to quantify how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n\n### Approach\nThe approach to calculate the Gini impurity involves the following steps:\n\n1. **Check for Empty Input**: If the input sequence is empty, return 0.0 by convention.\n2. **Convert Input to NumPy Array**: If the input is a list, convert it to a NumPy array for efficient processing.\n3. **Identify Unique Classes**: Use NumPy's `unique` function to get the unique class labels from the input sequence.\n4. **Count Frequencies**: For each unique class, count how many times it appears in the input sequence.\n5. **Calculate Relative Frequencies**: Compute the relative frequency of each class by dividing the count of each class by the total number of samples.\n6. **Compute Sum of Squares**: Sum the squares of these relative frequencies.\n7. **Calculate Gini Impurity**: Subtract the sum of squares from 1 to get the Gini impurity.\n8. **Round the Result**: Round the Gini impurity to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if not y:\n        return 0.0\n    \n    if isinstance(y, list):\n        y = np.array(y)\n    \n    unique_y = np.unique(y)\n    counts = {}\n    for c in unique_y:\n        counts[c] = np.count_nonzero(y == c)\n    \n    N = len(y)\n    sum_p_sq = 0.0\n    for c in counts:\n        p = counts[c] / N\n        sum_p_sq += p ** 2\n    \n    gini_impurity = 1 - sum_p_sq\n    return round(gini_impurity, 4)\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input is empty and returns 0.0 immediately if true.\n- **Conversion to NumPy Array**: If the input is a list, it is converted to a NumPy array to leverage efficient array operations.\n- **Unique Classes Identification**: Using `np.unique`, the function identifies all unique class labels in the input sequence.\n- **Frequency Counting**: For each unique class, the function counts how many times it appears in the input sequence using `np.count_nonzero`.\n- **Relative Frequencies Calculation**: The relative frequency of each class is calculated by dividing the count by the total number of samples.\n- **Sum of Squares Calculation**: The function computes the sum of the squares of these relative frequencies.\n- **Gini Impurity Calculation**: The Gini impurity is computed by subtracting the sum of squares from 1.\n- **Rounding**: The result is rounded to four decimal places before being returned.\n\nThis approach ensures that the Gini impurity is calculated efficiently and accurately, handling both list and NumPy array inputs appropriately."}
{"task_id": 374, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to train a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) between the predicted values and the actual target values. The function should handle both the initialization of parameters and the iterative optimization process, returning the learned parameters rounded to four decimal places.\n\n### Approach\n1. **Check Input Consistency**: Ensure that the number of samples in the feature matrix (X) matches the number of samples in the target vector (y). If they don't match, return -1.\n2. **Initialize Parameters**: Start with zero weights and a zero bias.\n3. **Iterative Optimization**: For each iteration, compute the predicted values using the current weights and bias. Calculate the residuals (the difference between predicted and actual values). Use these residuals to compute the gradients for the weights and bias using batch gradient descent.\n4. **Update Parameters**: Adjust the weights and bias by subtracting the learning rate multiplied by the computed gradients.\n5. **Return Results**: After completing the specified number of iterations, round the weights and bias to four decimal places and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    m = X.shape[0]\n    if m != y.shape[0] or m == 0:\n        return -1\n    n = X.shape[1]\n    \u03b8 = np.zeros(n)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = np.dot(X, \u03b8) + b\n        residuals = y_hat - y\n        sum_res = np.sum(residuals)\n        db = (2.0 / m) * sum_res\n        dw = (2.0 / m) * np.dot(X.T, residuals)\n        \u03b8 -= learning_rate * dw\n        b -= learning_rate * db\n    \u03b8 = np.round(\u03b8, 4)\n    b = np.round(b, 4)\n    weights = \u03b8.tolist()\n    return (weights, b)\n```\n\n### Explanation\n1. **Input Check**: The function first checks if the number of samples in X and y are the same and if there are any samples. If not, it returns -1.\n2. **Parameter Initialization**: The weights (\u03b8) are initialized as a zero vector, and the bias (b) is initialized to zero.\n3. **Gradient Descent Loop**: For each iteration, the function computes the predicted values (y_hat) using the current weights and bias. It then calculates the residuals, which are the differences between the predicted and actual values.\n4. **Gradients Calculation**: The gradients for the weights (dw) and bias (db) are computed using the residuals. These gradients are scaled by the number of samples to avoid division by zero and to maintain stability in the optimization process.\n5. **Parameter Update**: The weights and bias are updated by subtracting the learning rate multiplied by the gradients.\n6. **Result Rounding**: After completing the iterations, the weights and bias are rounded to four decimal places and returned as a tuple.\n\nThis approach ensures that the model is trained effectively using batch gradient descent, minimizing the MSE over the entire dataset in each update step."}
{"task_id": 376, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is considered symmetric if it is square and the entry at row i and column j is equal to the entry at row j and column i for every valid index pair (i, j). Due to potential floating-point rounding errors, we treat two numbers as equal if their absolute difference does not exceed 1 \u00d7 10\u207b\u2078.\n\n### Approach\n1. **Check for Empty Matrix**: If the input matrix is empty, it is trivially symmetric, so we return True.\n2. **Check if Matrix is Square**: Ensure that the matrix has the same number of rows and columns. If not, return False.\n3. **Convert to NumPy Array**: Convert the input matrix into a NumPy array for efficient manipulation.\n4. **Compute Transpose**: Calculate the transpose of the matrix. A symmetric matrix is equal to its transpose.\n5. **Compare with Tolerance**: Use NumPy's `allclose` function to check if the original matrix and its transpose are equal within the specified tolerance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Handle empty matrix\n    if not X:\n        return True\n\n    # Check if the matrix is square\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n\n    # Convert to NumPy array\n    X_np = np.array(X)\n\n    # Compute transpose\n    X_trans = X_np.T\n\n    # Compare with tolerance\n    return np.allclose(X_np, X_trans, atol=1e-8)\n```\n\n### Explanation\n1. **Empty Matrix Check**: The function first checks if the input matrix is empty. If it is, the function returns True because an empty matrix is trivially symmetric.\n2. **Square Matrix Check**: The function iterates through each row to ensure all rows have the same length as the number of rows, confirming the matrix is square.\n3. **NumPy Conversion**: The matrix is converted into a NumPy array to leverage efficient numerical operations.\n4. **Transpose Calculation**: The transpose of the matrix is computed. For a symmetric matrix, the original matrix should be equal to its transpose.\n5. **Tolerance Comparison**: Using NumPy's `allclose` function with a tolerance of 1 \u00d7 10\u207b\u2078, the function checks if the original matrix and its transpose are effectively equal, accounting for potential floating-point precision issues.\n\nThis approach ensures that the function efficiently and accurately determines the symmetry of the matrix while handling edge cases such as empty matrices and non-square matrices."}
{"task_id": 380, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The SELU function is defined as a scaled version of the Exponential Linear Unit (ELU) function, which is piecewise linear and exponential. The task involves computing the SELU function for a given input and its derivatives for different orders.\n\n### Approach\n1. **Understanding SELU Function**: The SELU function is defined as `SELU(x) = scale * ELU(x, \u03b1)`, where `ELU(x, \u03b1)` is a piecewise function. For `x > 0`, `ELU(x, \u03b1)` is `x`, and for `x <= 0`, it is `\u03b1 * (e^x - 1)`. The constants `\u03b1` and `scale` are given as 1.6732632423543772848170429916717 and 1.0507009873554804934193349852946, respectively.\n\n2. **First Derivative**: The first derivative of SELU is computed based on whether `x` is positive or not. For `x > 0`, the derivative is `scale`, and for `x <= 0`, it is `scale * \u03b1 * e^x`.\n\n3. **Second Derivative**: The second derivative of SELU is computed similarly. For `x > 0`, the second derivative is 0, and for `x <= 0`, it is `scale * \u03b1 * e^x`.\n\n4. **Implementation Steps**:\n   - Convert the input to a NumPy array for efficient computation.\n   - Compute the necessary parts of the function or derivative based on the value of `x` using conditional checks.\n   - Round the results to 4 decimal places and convert them to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\"\"\"\n    \u03b1 = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    x = np.asarray(x)\n    if order == 0:\n        mask = x > 0\n        elu = np.where(mask, x, \u03b1 * (np.exp(x) - 1))\n        result = elu * scale\n    elif order == 1:\n        mask = x > 0\n        derivative = np.where(mask, scale, scale * \u03b1 * np.exp(x))\n        result = derivative\n    elif order == 2:\n        mask = x > 0\n        second_derivative = np.where(mask, 0, scale * \u03b1 * np.exp(x))\n        result = second_derivative\n    else:\n        raise ValueError(\"order must be 0, 1, or 2\")\n\n    # Round to 4 decimal places\n    rounded_result = np.round(result, 4)\n    # Convert to list\n    return rounded_result.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to handle both scalar and array inputs uniformly.\n- **Conditional Checks**: Using `np.where`, the function checks if `x` is greater than 0 to compute the appropriate parts of the SELU function or its derivatives.\n- **Rounding and Conversion**: The results are rounded to 4 decimal places and converted to a list to meet the problem requirements.\n\nThis approach ensures that the function works efficiently for both scalar and multi-dimensional arrays, providing the correct activation values or derivatives as specified."}
{"task_id": 387, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor using one-dimensional decision stumps. The goal is to build a regressor that works on a single numerical feature and iteratively improves its predictions by minimizing the sum of squared residuals.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Initialization**: Start with the initial prediction as the mean of the target values.\n2. **Boosting Rounds**: For each boosting round, compute the residuals, sort the data, and find the optimal split point (stump) that minimizes the sum of squared errors.\n3. **Residual Calculation**: For each point, compute the residual as the difference between the target value and the current prediction.\n4. **Sorting and Splitting**: Sort the data by the feature values and evaluate possible split points between consecutive feature values.\n5. **Optimal Split Selection**: For each possible split point, compute the mean residuals on both sides of the split and choose the split that minimizes the total squared error.\n6. **Update Prediction**: Update the current prediction using the optimal split's residuals and the learning rate.\n7. **Final Prediction**: After all boosting rounds, return the final predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    current_y_hat = np.mean(y)\n    \n    for _ in range(n_estimators):\n        r = y - current_y_hat\n        \n        # Sort the data by x values\n        sorted_points = sorted(zip(x, r), key=lambda p: p[0])\n        sorted_x, sorted_r = zip(*sorted_points)\n        sorted_x = list(sorted_x)\n        sorted_r = list(sorted_r)\n        \n        # Check if all x are the same\n        if len(set(sorted_x)) == 1:\n            mean_r = np.mean(sorted_r)\n            current_y_hat += learning_rate * mean_r\n            continue\n        \n        # Generate possible split points\n        possible_ts = []\n        for i in range(len(sorted_x) - 1):\n            t = (sorted_x[i] + sorted_x[i + 1]) / 2\n            possible_ts.append(t)\n        \n        best_t = None\n        min_total_error = float('inf')\n        \n        for t in possible_ts:\n            split_idx = np.argwhere(sorted_x <= t).size\n            left_r = sorted_r[:split_idx]\n            right_r = sorted_r[split_idx:]\n            left_mean = np.mean(left_r)\n            right_mean = np.mean(right_r)\n            total_error = np.sum((left_r - left_mean)**2) + np.sum((right_r - right_mean)**2)\n            if total_error < min_total_error:\n                min_total_error = total_error\n                best_t = t\n        \n        # Compute the best split for the best_t\n        split_idx = np.argwhere(sorted_x <= best_t).size\n        left_mean = np.mean(sorted_r[:split_idx])\n        right_mean = np.mean(sorted_r[split_idx:])\n        \n        # Update current_y_hat\n        new_residual = []\n        for xi, ri in zip(x, r):\n            if xi <= best_t:\n                new_residual.append(left_mean)\n            else:\n                new_residual.append(right_mean)\n        \n        current_y_hat += learning_rate * np.array(new_residual)\n    \n    # Return the predictions rounded to 4 decimals\n    return np.round(current_y_hat, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial prediction is set to the mean of the target values.\n2. **Residual Calculation**: For each sample, compute the residual as the difference between the target value and the current prediction.\n3. **Sorting and Splitting**: The data is sorted by the feature values to facilitate finding the optimal split point.\n4. **Optimal Split Selection**: Evaluate each possible split point and compute the total squared error for each. The split point with the smallest error is chosen.\n5. **Update Prediction**: The prediction is updated using the optimal split's residuals and the learning rate, which controls the shrinkage of the model.\n6. **Final Prediction**: After all boosting rounds, the final predictions are returned, rounded to four decimal places.\n\nThis approach ensures that each boosting round improves the model's performance by iteratively reducing the error, leading to a more accurate regression model."}
{"task_id": 394, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimizer update step. RMSprop is an adaptive learning rate optimization algorithm used in training neural networks. It maintains a running average of the squared gradients and scales the learning rate by the inverse square root of this average, which helps in stabilizing the learning process.\n\n### Approach\nThe RMSprop algorithm follows these steps for each parameter update:\n1. **Initialise the Running Average**: If the running average of squared gradients (`E_g`) is not provided, initialise it to zeros with the same shape as the gradient.\n2. **Compute Squared Gradients**: Square each element of the gradient to get the squared gradients.\n3. **Update the Running Average**: Compute the new running average using the decay rate (`rho`) and the current squared gradients.\n4. **Compute the Denominator**: Calculate the square root of the new running average plus a small constant to avoid numerical instability.\n5. **Update Parameters**: Adjust each parameter using the learning rate and the computed denominator.\n6. **Round and Convert**: Round the updated parameters and the new running average to four decimal places and convert them to regular Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    g_sq = grad ** 2\n    new_Eg = rho * Eg + (1 - rho) * g_sq\n    denominator = np.sqrt(new_Eg + 1e-8)\n    \n    w_next = w - (learning_rate * grad) / denominator\n    \n    w_next_rounded = np.round(w_next, 4)\n    new_Eg_rounded = np.round(new_Eg, 4)\n    \n    w_next_list = [float(x) for x in w_next_rounded.tolist()]\n    new_Eg_list = [float(x) for x in new_Eg_rounded.tolist()]\n    \n    return (w_next_list, new_Eg_list)\n```\n\n### Explanation\n1. **Initialisation**: The function checks if `Eg` is `None` and initializes it to a zero array of the same shape as `grad`.\n2. **Squared Gradients**: The gradient is squared element-wise to compute the squared gradients.\n3. **Running Average Update**: The new running average is computed using the decay rate and the current squared gradients.\n4. **Denominator Calculation**: The square root of the new running average plus a small constant ensures numerical stability.\n5. **Parameter Update**: Each parameter is updated by subtracting the scaled gradient using the learning rate and the computed denominator.\n6. **Rounding and Conversion**: The updated parameters and the new running average are rounded to four decimal places and converted to lists of floats for the final output.\n\nThis approach ensures that the RMSprop optimizer correctly updates the parameters and maintains the running average of squared gradients, providing a stable learning process for training neural networks."}
{"task_id": 398, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is used in deep learning models to help with convergence and reduce the vanishing-gradient problem. The function and its gradient are computed based on the input value and a hyper-parameter alpha.\n\n### Approach\nThe ELU activation function is defined as:\n- ELU(x, \u03b1) = x, if x \u2265 0\n- ELU(x, \u03b1) = \u03b1 * (e^x - 1), if x < 0\n\nThe gradient of ELU is given by:\n- ELU'(x, \u03b1) = 1, if x \u2265 0\n- ELU'(x, \u03b1) = ELU(x, \u03b1) + \u03b1, if x < 0\n\nThe approach involves:\n1. Iterating over each element in the input list or NumPy array.\n2. For each element, compute the ELU activation if the derivative flag is False.\n3. If the derivative flag is True, compute the gradient for each element.\n4. Round each result to 4 decimal places and return the results as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    result = []\n    for xi in x:\n        if derivative:\n            if xi >= 0:\n                deriv = 1.0\n            else:\n                act = alpha * (np.exp(xi) - 1)\n                deriv = act + alpha\n        else:\n            if xi >= 0:\n                act = xi\n            else:\n                act = alpha * (np.exp(xi) - 1)\n        # Round to 4 decimal places\n        rounded = round(deriv, 4) if derivative else round(act, 4)\n        result.append(rounded)\n    return result\n```\n\n### Explanation\n- The function `elu` takes three parameters: `x` (a list or NumPy array of numbers), `alpha` (a positive float, default 0.1), and `derivative` (a boolean, default False).\n- If `derivative` is False, the function computes the ELU activation for each element in `x`.\n- If `derivative` is True, the function computes the gradient of ELU for each element.\n- Each computed value is rounded to 4 decimal places and collected into a list, which is then returned.\n\nThis approach ensures that both the activation function and its gradient are computed efficiently and accurately, handling both positive and negative input values appropriately."}
{"task_id": 411, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute statistics about the actions and observations collected from an environment. These statistics include whether the data are multi-dimensional, whether the values are discrete or continuous, and the number of unique values in each dimension. The goal is to return a dictionary with these statistics.\n\n### Approach\n1. **Check for Tuples and Multi-Dimensionality**: Determine if any action or observation is a tuple or a numpy array, which indicates that the data might be multi-dimensional.\n2. **Determine Dimensionality**: Calculate the dimensionality of the action and observation spaces by examining the structure of the first element in each list.\n3. **Check for Continuous Values**: Identify if any value in the actions or observations is a non-integer float, which indicates continuous data.\n4. **Collect Unique Values**: For each dimension in the action and observation spaces, collect the unique values and sort them.\n5. **Construct Result Dictionary**: Compile all the computed statistics into a dictionary with the specified keys.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    # Check for tuple actions\n    tuple_actions = any(isinstance(action, (list, tuple, np.ndarray)) for action in actions)\n    \n    # Check for multi-dimensional actions\n    multidim_actions = False\n    if actions:\n        first_action = actions[0]\n        if isinstance(first_action, np.ndarray):\n            shape = np.array(first_action).shape\n            multidim_actions = len(shape) > 1\n        elif isinstance(first_action, (list, tuple)):\n            multidim_actions = len(first_action) > 1\n        else:\n            multidim_actions = False\n    \n    # Determine action dimensionality\n    if not actions:\n        action_dim = 0\n    else:\n        first_action = actions[0]\n        if isinstance(first_action, np.ndarray):\n            shape = np.array(first_action).shape\n            action_dim = len(shape)\n        elif isinstance(first_action, (list, tuple)):\n            action_dim = len(first_action)\n        else:\n            action_dim = 0\n    \n    # Process each action into its dimensions\n    dims_actions = []\n    for action in actions:\n        if isinstance(action, np.ndarray):\n            shape = action.shape\n            if len(shape) == 0:\n                dim_values = [action]\n            else:\n                dim_values = [action[i] for i in range(len(shape))]\n        elif isinstance(action, (list, tuple)):\n            dim_values = list(action)\n        else:\n            dim_values = [action]\n        while len(dim_values) < action_dim:\n            dim_values.append(None)\n        dims_actions.append(tuple(dim_values))\n    \n    # Check for continuous actions\n    continuous_actions = False\n    for action in actions:\n        for dim in dims_actions:\n            for value in dim:\n                if isinstance(value, float) and not isinstance(value, int):\n                    continuous_actions = True\n                    break\n            if continuous_actions:\n                break\n        if continuous_actions:\n            break\n    \n    # Check for tuple observations\n    tuple_observations = any(isinstance(obs, (list, tuple, np.ndarray)) for obs in observations)\n    \n    # Check for multi-dimensional observations\n    multidim_observations = False\n    if observations:\n        first_obs = observations[0]\n        if isinstance(first_obs, np.ndarray):\n            shape = np.array(first_obs).shape\n            multidim_observations = len(shape) > 1\n        elif isinstance(first_obs, (list, tuple)):\n            multidim_observations = len(first_obs) > 1\n        else:\n            multidim_observations = False\n    \n    # Determine observation dimensionality\n    if not observations:\n        obs_dim = 0\n    else:\n        first_obs = observations[0]\n        if isinstance(first_obs, np.ndarray):\n            shape = np.array(first_obs).shape\n            obs_dim = len(shape)\n        elif isinstance(first_obs, (list, tuple)):\n            obs_dim = len(first_obs)\n        else:\n            obs_dim = 0\n    \n    # Process each observation into its dimensions\n    dims_observations = []\n    for obs in observations:\n        if isinstance(obs, np.ndarray):\n            shape = obs.shape\n            if len(shape) == 0:\n                dim_values = [obs]\n            else:\n                dim_values = [obs[i] for i in range(len(shape))]\n        elif isinstance(obs, (list, tuple)):\n            dim_values = list(obs)\n        else:\n            dim_values = [obs]\n        while len(dim_values) < obs_dim:\n            dim_values.append(None)\n        dims_observations.append(tuple(dim_values))\n    \n    # Check for continuous observations\n    continuous_observations = False\n    for obs in observations:\n        for dim in dims_observations:\n            for value in dim:\n                if isinstance(value, float) and not isinstance(value, int):\n                    continuous_observations = True\n                    break\n            if continuous_observations:\n                break\n        if continuous_observations:\n            break\n    \n    # Collect unique values per dimension for actions and observations\n    action_ids = []\n    for dim_idx in range(action_dim):\n        values = []\n        for action_dims in dims_actions:\n            value = action_dims[dim_idx]\n            if value is not None:\n                values.append(value)\n        unique = sorted(list(set(values)))\n        action_ids.append(unique)\n    \n    obs_ids = []\n    for dim_idx in range(obs_dim):\n        values = []\n        for obs_dims in dims_observations:\n            value = obs_dims[dim_idx]\n            if value is not None:\n                values.append(value)\n        unique = sorted(list(set(values)))\n        obs_ids.append(unique)\n    \n    return {\n        'tuple_actions': tuple_actions,\n        'tuple_observations': tuple_observations,\n        'multidim_actions': multidim_actions,\n        'multidim_observations': multidim_observations,\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': [len(dim) for dim in action_ids],\n        'action_dim': action_dim,\n        'n_obs_per_dim': [len(dim) for dim in obs_ids],\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids\n    }\n```\n\n### Explanation\n1. **Tuple and Multi-Dimensional Check**: The function checks if any action or observation is a tuple or numpy array to determine if the data is multi-dimensional.\n2. **Dimensionality Calculation**: The dimensionality of the action and observation spaces is determined by examining the structure of the first element in each list.\n3. **Continuous Value Check**: The function checks if any value is a non-integer float to determine if the data is continuous.\n4. **Unique Values Collection**: For each dimension, the function collects all unique values, sorts them, and counts them.\n5. **Result Construction**: The function constructs a dictionary with all the computed statistics, including the dimensionality, whether the data is continuous, and the unique values in each dimension.\n\nThis approach ensures that we accurately capture the structure and nature of the data collected from the environment, providing a comprehensive summary for quick sanity checks."}
{"task_id": 413, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch. The goal is to predict values for test samples using a boosting approach with decision trees. The solution involves iteratively fitting regression trees to the residuals of the training targets and updating the predictions based on these trees.\n\n### Approach\n1. **Initialization**: Start with an initial prediction equal to the mean of the training targets.\n2. **Boosting Iterations**: For each iteration, compute the residuals (negative gradients of the squared-error loss), fit a CART regression tree to these residuals, and update the running prediction by adding the tree\u2019s output multiplied by the learning rate.\n3. **Tree Construction**: Each tree is built recursively, splitting the data based on the feature that minimizes the mean squared error (MSE). The tree stops growing when the maximum depth is reached or no further reduction in error is possible.\n4. **Prediction**: After all iterations, return the final predictions for test samples, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\"\"\"\n    \n    def build_tree(X, residual, max_depth):\n        if max_depth == 0:\n            return np.mean(residual)\n        \n        n_samples = X.shape[0]\n        best_mse = float('inf')\n        best_feature = None\n        best_split_point = None\n        \n        for j in range(X.shape[1]):\n            # Sort the data by the j-th feature\n            sorted_data = sorted(zip(X[:, j], residual), key=lambda x: x[0])\n            X_sorted = sorted_data[:, 0]\n            residual_sorted = sorted_data[:, 1]\n            \n            # Try all possible split points\n            for k in range(1, n_samples):\n                # Split into left and right\n                left_X = X_sorted[:k]\n                left_residual = residual_sorted[:k]\n                right_X = X_sorted[k:]\n                right_residual = residual_sorted[k:]\n                \n                # Compute MSE for left and right\n                mse_left = np.mean((left_residual - np.mean(left_residual))**2)\n                mse_right = np.mean((right_residual - np.mean(right_residual))**2)\n                total_mse = mse_left + mse_right\n                \n                if total_mse < best_mse:\n                    best_mse = total_mse\n                    best_feature = j\n                    best_split_point = k\n        \n        if best_feature is None:\n            return np.mean(residual)\n        \n        feature = best_feature\n        split_point = best_split_point\n        \n        # Split the data\n        X_left = X[X_sorted[:split_point]]\n        residual_left = residual_sorted[:split_point]\n        X_right = X[X_sorted[split_point:]]\n        residual_right = residual_sorted[split_point:]\n        \n        # Sort left and right subsets based on feature j\n        X_left_sorted = sorted(X_left, key=lambda x: x[feature])\n        residual_left_sorted = sorted(residual_left, key=lambda x: x[feature])\n        left_pred = build_tree(X_left_sorted, residual_left_sorted, max_depth - 1)\n        \n        X_right_sorted = sorted(X_right, key=lambda x: x[feature])\n        residual_right_sorted = sorted(residual_right, key=lambda x: x[feature])\n        right_pred = build_tree(X_right_sorted, residual_right_sorted, max_depth - 1)\n        \n        # Now, for each sample, determine if it's in left or right\n        split_value = X_sorted[split_point-1]\n        predicted_residual = []\n        for i in range(n_samples):\n            if X[i, feature] <= split_value:\n                predicted_residual.append(left_pred[i])\n            else:\n                predicted_residual.append(right_pred[i])\n        \n        return np.array(predicted_residual)\n    \n    # Initialize y_pred as the mean of y_train\n    y_pred = np.mean(y_train)\n    \n    for _ in range(n_estimators):\n        # Compute residual: 2*(y_train - y_pred)\n        residual = 2 * (y_train - y_pred)\n        \n        # Build the regression tree for residual\n        tree_output = build_tree(X_train, residual, max_depth)\n        \n        # Update y_pred\n        y_pred += learning_rate * tree_output\n    \n    # Convert y_pred to a list of floats, rounded to 4 decimals\n    return [round(x, 4) for x in y_pred]\n```\n\n### Explanation\n1. **Initialization**: The initial prediction is set to the mean of the training targets.\n2. **Residual Calculation**: For each iteration, the residual is computed as twice the difference between the target and the current prediction.\n3. **Tree Construction**: A regression tree is built recursively. At each node, the feature that minimizes the MSE is chosen, and the data is split accordingly. The tree stops growing when the maximum depth is reached or no further improvement is possible.\n4. **Prediction Update**: The output of each tree is added to the current prediction, scaled by the learning rate.\n5. **Final Prediction**: After all iterations, the final predictions for test samples are returned, rounded to four decimal places.\n\nThis approach efficiently combines the strengths of gradient boosting and decision trees to provide accurate predictions for regression problems."}
{"task_id": 419, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance. The goal is to compute the MAP (Maximum A Posteriori) predictive mean for a new set of observations given a training set and targets.\n\n### Approach\n1. **Add Intercept Column**: If the `fit_intercept` flag is set to `True`, add a column of ones to the design matrix `X` and the new design matrix `X_new`. This column represents an intercept term.\n2. **Determine Parameter Vector Size**: The size of the parameter vector `\u03b2` is determined based on whether an intercept is fitted. If an intercept is fitted, the size is `M + 1`, otherwise, it is `M`.\n3. **Handle Prior Variance Matrix**: The prior variance matrix `V` can be a scalar, a 1D array, or a square matrix. If `V` is `None`, it is assumed to be the identity matrix. The matrix is scaled appropriately if it is a scalar.\n4. **Compute Posterior Distribution**: Using the given formula, compute the posterior distribution parameters. This involves calculating the posterior covariance matrix and the posterior mean.\n5. **Predict for New Observations**: Using the posterior mean, compute the MAP predictive mean for each new observation in `X_new`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # Add intercept column of ones if fit_intercept is True\n    if fit_intercept:\n        intercept = np.ones((1, X.shape[1]))\n        X = np.hstack((intercept, X))\n        intercept_new = np.ones((1, X_new.shape[1]))\n        X_new = np.hstack((intercept_new, X_new))\n    \n    N = X.shape[0]\n    M = X.shape[1]\n    \n    # Determine the size of the parameter vector \u03b2\n    size_beta = M + 1 if fit_intercept else M\n    \n    # Determine the V matrix based on the input\n    if V is None:\n        V_matrix = np.identity(size_beta)\n    elif isinstance(V, (float, int)):\n        V_matrix = V * np.identity(size_beta)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            if len(V) == size_beta:\n                V_matrix = np.diag(V)\n            else:\n                raise ValueError(\"V must be a 1D array of length size_beta\")\n        else:\n            if V.shape == (size_beta, size_beta):\n                V_matrix = V\n            else:\n                raise ValueError(\"V must be a square matrix of size_beta x size_beta\")\n    else:\n        raise ValueError(\"V must be a scalar, 1D array, or 2D array\")\n    \n    # Compute X.T @ X\n    XtX = X.T.dot(X)\n    \n    # Compute the inverse of V\n    try:\n        V_inv = np.linalg.inv(V_matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"V must be invertible\")\n    \n    # Compute the posterior covariance matrix\n    Sigma_posterior = np.linalg.inv(V_inv + XtX)\n    \n    # Compute the posterior mean\n    if isinstance(mu, (float, int)):\n        term1 = V_inv * mu\n    else:\n        term1 = V_inv.dot(mu)\n    \n    term2 = X.T.dot(y)\n    sum_terms = term1 + term2\n    mu_posterior = sum_terms.dot(Sigma_posterior)\n    \n    # Compute predictions for X_new\n    preds = X_new.dot(mu_posterior)\n    \n    # Round predictions to 4 decimal places\n    preds_rounded = [round(p, 4) for p in preds]\n    \n    return preds_rounded\n```\n\n### Explanation\n1. **Intercept Handling**: The code first checks if an intercept is needed and adds a column of ones to both the training and new design matrices.\n2. **Prior Variance Matrix**: The prior variance matrix `V` is constructed based on the input. If `V` is `None`, it defaults to the identity matrix. If `V` is a scalar, it is scaled to form a scaled identity matrix.\n3. **Posterior Calculation**: The posterior distribution parameters are computed using the given formulas. This involves matrix inversion and solving linear equations.\n4. **Predictions**: The MAP predictive mean for each new observation is computed by multiplying the new design matrix with the posterior mean vector.\n5. **Rounding**: The predicted values are rounded to four decimal places and returned as a list.\n\nThis approach ensures that the Bayesian linear regression model is correctly specified and efficiently computes the MAP predictions for new observations."}
{"task_id": 423, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Root Mean Squared Logarithmic Error (RMSLE) between two lists of non-negative numbers. RMSLE is a metric used to evaluate the performance of regression models, focusing on the ratio between predicted and actual values rather than their absolute differences.\n\n### Approach\n1. **Validation**: Ensure both input lists have the same length and contain only non-negative numbers. If any of these conditions are not met, return -1.\n2. **Computation**: Calculate the RMSLE using the formula:\n   \\[\n   \\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(1 + \\text{predicted}_i) - \\log(1 + \\text{actual}_i) \\right)^2}\n   \\]\n   where \\( n \\) is the number of elements in the lists.\n3. **Efficiency**: Use numpy for efficient computation of logarithms, squaring, summing, and taking the square root.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculates the Root Mean Squared Logarithmic Error between two lists.\"\"\"\n    # Check if the lengths of actual and predicted are equal\n    if len(actual) != len(predicted):\n        return -1\n    # Check if any element in actual or predicted is negative\n    if any(x < 0 for x in actual) or any(y < 0 for y in predicted):\n        return -1\n    # Check if either list is empty\n    if len(actual) == 0:\n        return -1\n    # Convert lists to numpy arrays for efficient computation\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    # Calculate the logarithmic differences\n    diffs = np.log(1 + predicted_np) - np.log(1 + actual_np)\n    # Square the differences and compute the mean\n    squared_diffs = diffs ** 2\n    mean = np.mean(squared_diffs)\n    # Compute the root mean squared error\n    rmsle = np.sqrt(mean)\n    # Round the result to four decimal places\n    return round(rmsle, 4)\n```\n\n### Explanation\n1. **Validation**: The function first checks if the lengths of the input lists are equal and if all elements are non-negative. If any check fails, it returns -1.\n2. **Conversion to Numpy Arrays**: The lists are converted to numpy arrays for efficient computation of logarithms and other operations.\n3. **Logarithmic Differences**: The function computes the difference between the logarithms of (1 + predicted) and (1 + actual) for each corresponding pair of values.\n4. **Squaring and Summing**: The squared differences are summed up, and the mean of these squared differences is computed.\n5. **Square Root and Rounding**: The square root of the mean is taken to get the RMSLE, which is then rounded to four decimal places and returned.\n\nThis approach ensures that the function efficiently and accurately computes the RMSLE while handling edge cases such as empty lists and negative values appropriately."}
{"task_id": 428, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Hann window, which is a type of window used in digital signal processing to reduce spectral leakage. The Hann window is symmetric and smoothly reaches zero at the endpoints, making it suitable for applications like short-time Fourier transforms and spectral analysis.\n\n### Approach\nThe approach to generate a Hann window involves the following steps:\n\n1. **Special Cases Handling**:\n   - If the window length (`window_len`) is less than or equal to 0, return an empty list.\n   - If the window length is exactly 1, return a list containing a single element, 1.0, as the window is a single sample.\n\n2. **Determine Window Length**:\n   - For a symmetric window, the length is `window_len`.\n   - For a periodic window, the length is `window_len + 1` because we need an extra sample to ensure smoothness between adjacent windows when using FFT.\n\n3. **Compute Window Coefficients**:\n   - For each sample `n` in the range from 0 to the determined window length minus 1, compute the Hann window coefficient using the formula:\n     \\[\n     \\text{hann}(n) = 0.5 - 0.5 \\times \\cos\\left(\\frac{2\\pi n}{N-1}\\right)\n     \\]\n     where \\( N \\) is the window length.\n\n4. **Handle Periodic Window**:\n   - If the window is periodic, compute the coefficients for `N = window_len + 1` and then discard the last element to get the required window length.\n\n5. **Rounding**:\n   - Round each coefficient to four decimal places before returning the result.\n\n### Solution Code\n```python\nimport math\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    if symmetric:\n        N = window_len\n    else:\n        N = window_len + 1\n    \n    window = []\n    for n in range(N):\n        angle = 2 * math.pi * n / (N - 1)\n        value = 0.5 - 0.5 * math.cos(angle)\n        window.append(value)\n    \n    if not symmetric:\n        window = window[:window_len]\n    \n    # Round to 4 decimal places\n    rounded_window = [round(val, 4) for val in window]\n    return rounded_window\n```\n\n### Explanation\n- **Special Cases**: The function immediately handles cases where the window length is 0 or 1, returning appropriate results without further computation.\n- **Window Length Determination**: Depending on whether the window is symmetric or periodic, the function sets the appropriate length for computation.\n- **Coefficient Computation**: Using the given formula, the function computes each coefficient iteratively, ensuring the smooth transition to zero at the endpoints.\n- **Periodic Window Handling**: For periodic windows, the function computes an extra sample and then discards it to match the required window length.\n- **Rounding**: Each coefficient is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the Hann window is generated efficiently and correctly, adhering to the specified constraints and properties."}
{"task_id": 433, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the fan-in and fan-out values for a given weight-tensor shape in a neural network. The fan-in and fan-out are important concepts in neural network initialization schemes like Xavier/Glorot, where they help in determining the appropriate weights for layers.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Identify the Tensor Type**: Determine if the given shape is a 4-dimensional tensor, which would indicate a 2-D convolutional kernel. If it is, we apply specific rules for calculating fan-in and fan-out. For all other cases, we treat the tensor as a dense layer or another type of tensor and use the first two dimensions directly.\n\n2. **Convolutional Kernel Handling**: If the shape is a 4-dimensional tensor, it is assumed to be a 2-D convolutional kernel with the layout `(out_channels, in_channels, kernel_height, kernel_width)`. Here, the receptive field size is calculated as the product of the kernel's height and width. The fan-in is then the product of the in_channels and the receptive field size, while the fan-out is the product of the out_channels and the receptive field size.\n\n3. **General Case Handling**: For tensors that are not 4-dimensional, the fan-in is taken as the first dimension of the shape, and the fan-out is taken as the second dimension.\n\n### Solution Code\n```python\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        in_channels = shape[1]\n        out_channels = shape[0]\n        kernel_height = shape[2]\n        kernel_width = shape[3]\n        receptive_field = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **Convolutional Kernel Case**: When the shape has exactly 4 dimensions, the function calculates the fan-in and fan-out based on the convolutional kernel's dimensions. The in_channels and out_channels are taken from the first and second elements of the shape, respectively. The kernel's height and width are used to compute the receptive field size, which is then used to determine the fan-in and fan-out.\n- **General Case**: For shapes with fewer than 4 dimensions, the function directly uses the first two dimensions to compute the fan-in and fan-out. This handles dense layers, embedding matrices, and other tensors.\n\nThis approach ensures that the function correctly computes the fan-in and fan-out values for both convolutional kernels and other types of tensors, providing the necessary values for neural network initialization."}
{"task_id": 435, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a tiny Gradient Boosting Regressor using ordinary least squares (OLS) linear regression as the weak learner. The goal is to predict the target values based on a given training matrix and target vector, using a boosting approach with a specified number of estimators and a learning rate.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Initialization**: Convert the input features and target values into NumPy arrays. Initialize the current prediction with the mean of the target values.\n2. **Boosting Process**: For each estimator:\n   - Compute the residuals between the target values and the current prediction.\n   - Fit an OLS linear regression model to predict these residuals from the features.\n   - Update the current prediction by adding the learning rate times the predictions from the fitted model.\n3. **Special Cases Handling**: If the number of estimators is non-positive or the learning rate is zero, return a vector filled with the target mean.\n4. **Final Prediction**: After all estimators are applied, round the final prediction to four decimal places and return it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to the nearest 4th decimal and packed into a plain\n    Python list.\n    \"\"\"\n    m = len(y)\n    if n_estimators <= 0 or learning_rate == 0:\n        mean_y = np.mean(y)\n        return [round(mean_y, 4)] * m\n    \n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    mean_y = np.mean(y_np)\n    hat_y = np.array([mean_y] * m, dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        residuals = y_np - hat_y\n        w, b = np.linalg.lstsq(X_np, residuals, rcond=None)\n        h_t = b + np.dot(X_np, w)\n        hat_y += learning_rate * h_t\n    \n    rounded_hat_y = np.round(hat_y, 4)\n    return rounded_hat_y.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by converting the input features (X) and target values (y) into NumPy arrays. It initializes the current prediction (`hat_y`) with the mean of the target values.\n2. **Boosting Loop**: For each estimator, the function computes the residuals (errors between the target and current prediction). It then fits an OLS linear regression model to predict these residuals. The model's coefficients (weights and bias) are obtained using `np.linalg.lstsq`, and the predictions from this model are used to update the current prediction.\n3. **Update and Special Cases**: The current prediction is updated by adding the learning rate times the model's predictions. Special cases where no boosting is performed are handled by returning the target mean directly.\n4. **Final Output**: After all estimators are applied, the final prediction is rounded to four decimal places and converted into a list for the output.\n\nThis approach ensures that the model is built using a simple yet effective boosting method, leveraging OLS linear regression for each step, and provides a clear and efficient implementation using NumPy."}
{"task_id": 437, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that evaluates the logistic sigmoid activation function and its first or second derivatives. The function should handle scalar, list, or NumPy array inputs and return the result rounded to four decimal places. The function should also handle different orders of derivatives as specified.\n\n### Approach\n1. **Check Order Validity**: The function first checks if the provided order is valid (0, 1, or 2). If not, it returns -1.\n2. **Convert Input to NumPy Array**: The input is converted to a NumPy array to handle vectorized operations efficiently.\n3. **Compute Sigmoid Function**: The logistic sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Compute Derivatives**: Depending on the order, the function computes the first or second derivative using the results from the sigmoid function.\n5. **Round Result**: The result is rounded to four decimal places.\n6. **Return Appropriate Type**: If the input was a scalar, the result is returned as a float. Otherwise, it is returned as a list preserving the original shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\"\"\"\n    if order not in [0, 1, 2]:\n        return -1\n    x = np.asarray(x)\n    e_x = np.exp(-x)\n    sig = 1 / (1 + e_x)\n    if order == 0:\n        result = sig\n    elif order == 1:\n        result = sig * (1 - sig)\n    elif order == 2:\n        result = sig * (1 - sig) * (1 - 2 * sig)\n    rounded_result = np.round(result, 4)\n    if rounded_result.ndim == 0:\n        return rounded_result.item()\n    else:\n        return rounded_result.tolist()\n```\n\n### Explanation\n- **Order Check**: The function starts by validating the order parameter. If the order is not 0, 1, or 2, it returns -1.\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to facilitate vectorized operations, which are more efficient for handling large datasets.\n- **Sigmoid Calculation**: The sigmoid function is calculated using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n- **Derivative Calculation**: Depending on the order, the first or second derivative is computed using the results from the sigmoid function.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's precision requirements.\n- **Return Type Handling**: The function checks if the result is a scalar (0-dimensional array) and returns it as a float. For higher-dimensional results, it converts the NumPy array to a list and returns it.\n\nThis approach ensures that the function efficiently handles different input types and correctly computes the required derivative while maintaining precision and appropriate data types for the output."}
{"task_id": 438, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a vector of class labels into a one-hot encoded matrix. This matrix will have each class label represented as a unique row with a single 1 at the column corresponding to that label and 0s elsewhere.\n\n### Approach\n1. **Input Handling**: The function accepts a 1-D list or NumPy array `y` containing non-negative integer class indices and an optional `num_classes` argument.\n2. **Determine Number of Classes**: If `num_classes` is not provided, it is inferred as `max(y) + 1`. If `num_classes` is provided but is smaller than `max(y) + 1`, the function returns `-1` to indicate that one-hot encoding is impossible.\n3. **Check for Empty Input**: If the input `y` is empty, the function returns `-1` as it's invalid for one-hot encoding.\n4. **Create One-Hot Matrix**: Using NumPy, a matrix is initialized with zeros. Each row is then set to 1 at the column corresponding to the respective class label in `y`.\n5. **Convert to List of Lists**: The resulting NumPy matrix is converted to a list of lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Check if y is a list or array\n    if isinstance(y, list):\n        y_array = np.array(y)\n    else:\n        y_array = y\n\n    # Check if y is empty\n    if len(y_array) == 0:\n        return [-1]\n\n    # Determine the number of classes\n    if num_classes is None:\n        num_classes = max(y_array) + 1\n    else:\n        # Check if num_classes is sufficient\n        if num_classes < (max(y_array) + 1):\n            return [-1]\n\n    # Create the one-hot matrix\n    n = len(y_array)\n    matrix = np.zeros((n, num_classes), dtype=int)\n    for i in range(n):\n        label = y_array[i]\n        matrix[i, label] = 1\n\n    # Convert to list of lists\n    result = matrix.tolist()\n\n    return result\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input `y` is a list and converts it to a NumPy array if necessary.\n2. **Empty Input Check**: If the input `y` is empty, the function returns `-1` immediately.\n3. **Class Determination**: The number of classes is determined either from the `num_classes` parameter or inferred from the input `y`.\n4. **Validation**: If `num_classes` is provided and is insufficient, the function returns `-1`.\n5. **Matrix Construction**: A matrix of zeros is created, and each row is updated to have a 1 at the position corresponding to the respective class label in `y`.\n6. **Conversion**: The resulting NumPy matrix is converted to a list of lists to meet the function's return requirements.\n\nThis approach ensures that the function efficiently handles various edge cases and correctly converts the input into a one-hot encoded matrix."}
{"task_id": 440, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate probability predictions from multiple estimators (such as trees in a Random Forest) to form a single prediction. The goal is to average the probability vectors across all estimators for each sample, round these averaged probabilities to four decimal places, and determine the predicted class label for each sample.\n\n### Approach\n1. **Problem Analysis**: Each estimator returns a probability distribution for each sample. The task is to average these distributions across all estimators, round the results, and determine the class with the highest probability.\n2. **Intuition**: By averaging the probability vectors across all estimators, we can form a more robust prediction. Rounding the averaged probabilities ensures precision, and finding the class with the highest probability gives the predicted label.\n3. **Algorithm Selection**: We use numpy for efficient averaging and rounding operations. The `np.average` function is used to compute the mean of probability vectors across estimators, and `np.argmax` is used to find the class with the highest probability after rounding.\n4. **Complexity Analysis**: The algorithm runs in O(n_estimators * n_samples * n_classes) time, where n_estimators is the number of estimators, n_samples is the number of samples, and n_classes is the number of classes. This is efficient for typical use cases in machine learning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    n_estimators = len(predictions)\n    if n_estimators == 0:\n        return ([], [])\n    n_samples = len(predictions[0])\n    n_classes = len(predictions[0][0])\n    \n    averaged_probs = []\n    predicted_labels = []\n    \n    for sample_idx in range(n_samples):\n        prob_vectors = []\n        for est_idx in range(n_estimators):\n            prob_vector = predictions[est_idx][sample_idx]\n            prob_vectors.append(prob_vector)\n        avg_vector = np.average(prob_vectors, axis=0)\n        rounded = [round(v, 4) for v in avg_vector]\n        max_idx = np.argmax(rounded)\n        averaged_probs.append(rounded)\n        predicted_labels.append(max_idx)\n    \n    return (averaged_probs, predicted_labels)\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of estimators, samples, and classes from the input data.\n2. **Loop Through Samples**: For each sample, the function collects the probability vectors from all estimators.\n3. **Averaging**: The probability vectors for each sample across all estimators are averaged using numpy's `average` function.\n4. **Rounding**: Each element of the averaged probability vector is rounded to four decimal places.\n5. **Predicted Labels**: The class with the highest probability (smallest index in case of ties) is determined using `np.argmax`.\n6. **Return Results**: The function returns the rounded probability matrix and the predicted class labels for each sample.\n\nThis approach efficiently aggregates predictions from multiple estimators, ensuring robust and accurate results."}
{"task_id": 444, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the Radial Basis Function (RBF) kernel matrix. The RBF kernel is a key component in kernel methods such as Gaussian Processes and Support Vector Machines. The kernel matrix is computed based on the input vectors and a scale parameter, which can be isotropic or anisotropic.\n\n### Approach\n1. **Input Validation**: The function first validates the input vectors and the scale parameter. It checks if the input vectors have the same number of features and if the scale parameter is valid. If any validation fails, the function returns -1.\n2. **Default Scale Parameter**: If the scale parameter is `None`, it defaults to the square root of half the number of features.\n3. **Kernel Matrix Computation**: The kernel matrix is computed by iterating over each pair of vectors, calculating the squared Euclidean distance between them, and applying the RBF transformation. The result is then rounded to four decimal places.\n4. **Return the Result**: The computed kernel matrix is returned as a nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Validate inputs\n    if len(X) == 0 or (Y is not None and len(Y) == 0):\n        return -1\n\n    # Check all rows in X have the same length\n    if any(len(row) != len(X[0]) for row in X):\n        return -1\n\n    # Check Y's rows if Y is not None\n    if Y is not None:\n        if any(len(row) != len(Y[0]) for row in Y):\n            return -1\n        if len(Y[0]) != len(X[0]):\n            return -1\n\n    C = len(X[0])\n\n    # Validate sigma\n    if sigma is None:\n        sigma_list = [np.sqrt(C / 2)] * C\n    else:\n        if isinstance(sigma, list):\n            if len(sigma) != C:\n                return -1\n            for s in sigma:\n                if not isinstance(s, (int, float)):\n                    return -1\n                if s <= 0:\n                    return -1\n            sigma_list = sigma\n        elif isinstance(sigma, (int, float)):\n            if sigma <= 0:\n                return -1\n            if C != 1:\n                return -1\n            sigma_list = [sigma]\n        else:\n            return -1  # invalid type\n\n    # Compute the kernel matrix\n    n = len(X)\n    m = len(Y) if Y is not None else n\n\n    kernel = []\n    for i in range(n):\n        x_i = X[i]\n        row = []\n        for j in range(m):\n            if Y is not None:\n                y_j = Y[j]\n            else:\n                y_j = x_i  # when Y is None, use X's j-th row\n            sum_sq = 0.0\n            for k in range(C):\n                dx = x_i[k] - y_j[k]\n                sum_sq += dx * dx / sigma_list[k]\n            kernel_val = np.exp(-0.5 * sum_sq)\n            # Round to four decimal places\n            kernel_val = round(kernel_val, 4)\n            row.append(kernel_val)\n        kernel.append(row)\n\n    return kernel\n```\n\n### Explanation\n1. **Input Validation**: The function checks if the input vectors are empty or have mismatched feature dimensions. It also validates the scale parameter to ensure it is positive and appropriately dimensioned.\n2. **Default Scale Parameter**: If the scale parameter is `None`, it uses the square root of half the number of features as the default.\n3. **Kernel Matrix Computation**: For each pair of vectors, the function computes the squared Euclidean distance, applies the RBF transformation, and rounds the result to four decimal places.\n4. **Return the Result**: The computed kernel matrix is returned as a nested list, ensuring the result is in the required format."}
{"task_id": 452, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a given dataset into two subsets based on a specified feature and a threshold. The function should handle both numeric and categorical thresholds while preserving the original order of the samples.\n\n### Approach\n1. **Determine Threshold Type**: Check if the threshold is numeric (int or float) or categorical (string).\n2. **Iterate Through Samples**: Loop through each sample in the dataset.\n3. **Check Condition**: For each sample, check if the value at the specified feature column meets the threshold condition.\n4. **Split Samples**: Depending on the threshold type, split the samples into two subsets. For numeric thresholds, include samples where the feature value is greater than or equal to the threshold. For categorical thresholds, include samples where the feature value exactly matches the threshold.\n5. **Maintain Order**: Ensure both subsets maintain the original order of the samples.\n6. **Convert to Lists**: Convert the resulting subsets into lists of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\"\"\"\n    first_subset = []\n    second_subset = []\n    \n    for row in X:\n        value = row[feature_i]\n        if isinstance(threshold, (int, float)):\n            if value >= threshold:\n                first_subset.append(row.tolist())\n            else:\n                second_subset.append(row.tolist())\n        else:\n            if value == threshold:\n                first_subset.append(row.tolist())\n            else:\n                second_subset.append(row.tolist())\n    \n    return [first_subset, second_subset]\n```\n\n### Explanation\n- **Threshold Type Check**: The function first checks if the threshold is numeric or categorical. This determines how the samples are compared.\n- **Loop Through Samples**: Each sample is processed in the order they appear in the dataset.\n- **Condition Check**: For each sample, the value at the specified feature column is compared against the threshold. Depending on whether the threshold is numeric or categorical, the sample is added to the appropriate subset.\n- **Conversion to Lists**: After splitting, each subset is converted from a NumPy array (if applicable) to a list of lists to meet the output requirements.\n\nThis approach ensures that the function efficiently splits the dataset while preserving the original order and handling both numeric and categorical thresholds correctly."}
{"task_id": 453, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to update the Q-values for a Deep Q-Network (DQN) model based on the given inputs. The Q-values are updated using the current Q-values, next-state Q-values, actions taken, rewards received, and whether the next state is terminal. The goal is to compute the training targets for each sample in the batch.\n\n### Approach\n1. **Input Validation**: First, we check if the input arrays have consistent lengths and if the discount factor (gamma) is within the valid range [0, 1]. If any of these conditions are not met, we return -1.\n2. **Target Calculation**: For each sample in the batch:\n   - If the next state is terminal (dones[i] is True), the target is simply the reward received.\n   - If the next state is not terminal, the target is computed as the reward plus the discount factor (gamma) multiplied by the maximum Q-value in the next state.\n3. **Update Q-Values**: We create a copy of the current Q-values and update the entry corresponding to the action taken in each state with the computed target value.\n4. **Rounding**: Each element of the updated Q-values is rounded to four decimal places to ensure precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check input validity\n    if len(Q) != len(Q_next) or len(Q) != len(actions) or len(Q) != len(rewards) or len(Q) != len(dones):\n        return -1\n    if not (0 <= gamma <= 1):\n        return -1\n\n    # Create a copy of Q to update\n    y = Q.copy()\n\n    for i in range(len(rewards)):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            max_next_q = np.max(Q_next[i])\n            target = rewards[i] + gamma * max_next_q\n        # Update the corresponding action in y\n        y[i][actions[i]] = target\n\n    # Round to four decimal places\n    y_rounded = []\n    for row in y:\n        rounded_row = [round(x, 4) for x in row]\n        y_rounded.append(rounded_row)\n    \n    return y_rounded\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the lengths of the input arrays are consistent and if the discount factor is within the valid range. This ensures that the inputs are valid before proceeding.\n- **Target Calculation**: For each sample, the target is determined based on whether the next state is terminal. If it is terminal, the target is the reward. Otherwise, it is computed using the reward and the maximum next-state Q-value.\n- **Update and Round**: The current Q-values are copied, and the target values are updated at the corresponding action indices. Each value is then rounded to four decimal places to ensure precision in the final output.\n\nThis approach ensures that the Q-values are updated correctly according to the DQN algorithm, providing the necessary training targets for each sample in the batch."}
{"task_id": 461, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor for 1-D data using decision stumps. The goal is to create a function that fits an ensemble of these stumps and returns predictions for the given training inputs.\n\n### Approach\nThe approach involves iteratively fitting decision stumps to the training data and updating the model's predictions based on the residuals from previous iterations. Here's a detailed breakdown of the steps:\n\n1. **Initialization**: Start with the mean of the target values as the initial prediction.\n2. **Iterative Boosting**: For each iteration (up to `n_estimators`):\n   - Compute the residuals between the current prediction and the target values.\n   - Identify potential split points in the feature values. Each split point divides the data into left and right subsets.\n   - For each split point, calculate the sum of squared errors (SSE) for the left and right subsets. The subset with the smallest SSE is chosen.\n   - Update the residuals based on the best split and adjust the current prediction by adding the learning rate times the new residuals.\n3. **Special Cases**: Handle cases where no valid split exists by setting the residuals to the mean of the entire dataset.\n4. **Final Predictions**: After all iterations, return the final predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    if n_estimators <= 0:\n        current_pred = np.mean(y)\n        return np.round(current_pred, 4).tolist()\n    \n    x = np.array(x)\n    y = np.array(y)\n    current_pred = np.mean(y)\n    \n    for _ in range(n_estimators):\n        residuals = y - current_pred\n        \n        # Find split points\n        sorted_x = np.sort(x)\n        unique_x = np.unique(sorted_x)\n        split_points = unique_x[:-1]  # exclude last\n        \n        if not split_points:\n            # No valid split\n            residuals = np.mean(residuals)\n        else:\n            best_t = None\n            min_sse = float('inf')\n            \n            for t in split_points:\n                mask_left = (x <= t)\n                mask_right = (x > t)\n                \n                if not (np.any(mask_left) and np.any(mask_right)):\n                    continue  # skip invalid splits\n                \n                left_residuals = residuals[mask_left]\n                v_L = np.mean(left_residuals)\n                right_residuals = residuals[mask_right]\n                v_R = np.mean(right_residuals)\n                \n                sse = (v_L ** 2) * len(left_residuals) + (v_R ** 2) * len(right_residuals)\n                \n                if sse < min_sse:\n                    min_sse = sse\n                    best_t = t\n            \n            if best_t is None:\n                # No valid split\n                residuals = np.mean(residuals)\n            else:\n                mask_left = (x <= best_t)\n                mask_right = (x > best_t)\n                \n                left_residuals = residuals[mask_left]\n                v_L = np.mean(left_residuals)\n                right_residuals = residuals[mask_right]\n                v_R = np.mean(right_residuals)\n                \n                new_residuals = np.where(mask_left, v_L, v_R)\n                \n                current_pred += learning_rate * new_residuals\n        \n    # Round to four decimals\n    return np.round(current_pred, 4).tolist()\n```\n\n### Explanation\n- **Initialization**: The initial prediction is set to the mean of the target values.\n- **Iterative Boosting**: For each iteration, residuals are computed. The data is split at each unique feature value (except the largest), and the split with the smallest SSE is chosen. The residuals are updated based on this split, and the current prediction is adjusted.\n- **Special Cases**: If no valid split exists, the residuals are set to the mean of the entire dataset to avoid division by zero or other issues.\n- **Final Predictions**: After all iterations, the final predictions are rounded to four decimal places and returned.\n\nThis approach ensures that the model is built efficiently and accurately, leveraging the strengths of gradient boosting with decision stumps for 1-D regression."}
{"task_id": 471, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement one step of the Expected SARSA TD(0) algorithm for a tabular Q-table. This algorithm is used in reinforcement learning to update the Q-values based on the expected future rewards, considering the exploration rate and discount factor.\n\n### Approach\nThe task involves updating the Q-table based on the given transition, which includes the current state, action, reward, next state, exploration rate, learning rate, and discount factor. The update is performed using the Expected SARSA TD(0) method, which involves computing the target Q-value and then updating the current Q-value.\n\n1. **Check for Terminal State**: If the next state is `None`, it means the transition terminates the episode, so the target Q-value is simply the immediate reward.\n2. **Compute Target Q-Value**: For a non-terminal next state, compute the expected value of the next state's Q-values using the \u03b5-soft policy. This involves finding the greedy action (the action with the highest Q-value) and calculating the probabilities for each action.\n3. **Update Q-Table**: Using the computed target Q-value, update the current Q-value by adding the learning rate multiplied by the difference between the target and current Q-value.\n4. **Round Values**: After updating the Q-table, round all values to four decimal places for the final output.\n\n### Solution Code\n```python\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # Create a deep copy of the q_table to avoid modifying the original\n    new_q_table = [row.copy() for row in q_table]\n    \n    if next_state is None:\n        target = reward\n    else:\n        s_prime = next_state\n        current_q_values = new_q_table[s_prime]\n        num_actions = len(current_q_values)\n        max_q = max(current_q_values)\n        # Find all actions with the maximum Q-value; pick the first as the greedy action\n        greedy_actions = [a for a in range(num_actions) if current_q_values[a] == max_q]\n        greedy_action = greedy_actions[0] if greedy_actions else None\n        \n        # Calculate the probabilities for each action\n        prob = []\n        for a in range(num_actions):\n            if a == greedy_action:\n                p = 1 - epsilon + (epsilon / num_actions)\n            else:\n                p = epsilon / num_actions\n            prob.append(p)\n        \n        # Compute the expectation of Q[s', a'] under the policy\n        expectation = sum(prob[a] * current_q_values[a] for a in range(num_actions))\n        target = reward + gamma * expectation\n    \n    # Update the Q value for the current state and action\n    current_q = new_q_table[state][action]\n    delta = lr * (target - current_q)\n    new_q_table[state][action] = current_q + delta\n    \n    # Round all values to four decimal places\n    rounded_q_table = []\n    for row in new_q_table:\n        rounded_row = [round(val, 4) for val in row]\n        rounded_q_table.append(rounded_row)\n    \n    return rounded_q_table\n```\n\n### Explanation\n1. **Deep Copy of Q-Table**: The function starts by creating a deep copy of the original Q-table to ensure that the updates do not affect the original table.\n2. **Handling Terminal State**: If the next state is `None`, the target Q-value is set to the immediate reward.\n3. **Greedy Action and Probabilities**: For non-terminal states, the function identifies the greedy action (the action with the highest Q-value) and calculates the probabilities for each action based on the exploration rate.\n4. **Expectation Calculation**: The expected value of the next state's Q-values is computed using the probabilities derived from the \u03b5-soft policy.\n5. **Q-Table Update**: The current Q-value is updated using the target Q-value and the learning rate.\n6. **Rounding**: Finally, all values in the Q-table are rounded to four decimal places to ensure precision in the output.\n\nThis approach ensures that the Q-table is updated correctly according to the Expected SARSA TD(0) algorithm, providing a foundation for reinforcement learning agents to learn optimal policies."}
