{"task_id": 0, "completion_id": 0, "completion": "To solve this problem, we need to implement the closed-form solution for Ridge (L2-regularised) Linear Regression. The goal is to compute the weight vector that minimizes the sum of squared errors while incorporating a regularization term to prevent overfitting. The solution involves matrix operations and inversion, which requires careful handling to ensure numerical stability.\n\n### Approach\n1. **Extract Features and Target**: Separate the input data into features (X) and target (y). The target is the last column of the input data, and the features are all preceding columns.\n2. **Add Bias Term**: Append a column of ones to the feature matrix to include the bias term in the model.\n3. **Compute X Transpose and XTX**: Calculate the transpose of the feature matrix and then compute the product of this transpose with the feature matrix (X^T X).\n4. **Construct Regularized Matrix**: Add a scaled identity matrix to XTX, where the scaling factor is the regularization strength (alpha). This forms the matrix A.\n5. **Check Invertibility**: Compute the inverse of matrix A. If the matrix is singular (non-invertible), return -1.\n6. **Compute Weights**: Multiply the inverse matrix by the product of X transpose and the target vector to get the weight vector.\n7. **Round Weights**: Round each element of the weight vector to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Ridge regression using closed-form solution.\"\"\"\n    # Extract features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add a bias term\n    n_samples = X.shape[0]\n    X = np.hstack((X, np.ones((n_samples, 1)))\n    \n    # Compute X transpose and XTX\n    X_transpose = X.T\n    XTX = X_transpose.dot(X)\n    \n    # Create identity matrix of appropriate size\n    n_features = X.shape[1]\n    I = np.identity(n_features)\n    \n    # Construct the matrix A\n    A = XTX + alpha * I\n    \n    # Check if matrix A is invertible\n    try:\n        A_inv = np.linalg.inv(A)\n    except np.linalg.LinAlgError:\n        return [-1]\n    \n    # Compute the weights\n    w = A_inv.dot(X_transpose.dot(y))\n    \n    # Round to 4 decimal places\n    w_rounded = [round(weight, 4) for weight in w.ravel()]\n    \n    return w_rounded\n```\n\n### Explanation\n- **Extracting Features and Target**: The input data is split into features (X) and target (y). The target is the last column, and features are all other columns.\n- **Bias Term Addition**: A column of ones is added to the feature matrix to include the intercept term in the model.\n- **Matrix Operations**: The product of the transpose of the feature matrix with itself (XTX) is computed, which is a key step in forming the regularized matrix.\n- **Regularization**: The identity matrix scaled by alpha is added to XTX to form matrix A, which is then inverted if possible.\n- **Inverse Check**: The inverse of matrix A is computed, and if it is not invertible (determinant zero), the function returns -1.\n- **Weight Calculation**: The weights are computed using the inverse matrix and the product of X transpose and y.\n- **Rounding**: The resulting weights are rounded to four decimal places for the final output.\n\nThis approach ensures that the solution is both efficient and numerically stable, leveraging matrix operations to find the optimal weights for the linear regression model with L2 regularization."}
{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to implement the standard link functions for Generalized Linear Models (GLMs) which include the identity, log, and logit links. Each link function must provide the link, its inverse, and its derivative, all of which should work with both numpy scalars and arrays.\n\n### Approach\n1. **Identity Link**: The identity function is straightforward, where the link function is the same as the response variable, its inverse is also the same, and the derivative is 1.\n2. **Log Link**: The log function is used for Poisson-distributed data. The inverse of the log function is the exponential function, and the derivative is the reciprocal of the response variable.\n3. **Logit Link**: The logit function is used for Bernoulli or Binomial-distributed data. The inverse of the logit function is the logit function itself, and the derivative is the reciprocal of the product of the response variable and its complement.\n\nEach function is implemented as a lambda to ensure it works with both scalars and numpy arrays, leveraging numpy's element-wise operations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {}\n    \n    # Identity link\n    links['identity'] = {\n        'link': lambda x: x,\n        'inv_link': lambda x: x,\n        'derivative': lambda x: 1\n    }\n    \n    # Log link\n    links['log'] = {\n        'link': lambda x: np.log(x),\n        'inv_link': lambda x: np.exp(x),\n        'derivative': lambda x: 1.0 / x\n    }\n    \n    # Logit link\n    links['logit'] = {\n        'link': lambda x: np.log(x / (1 - x)),\n        'inv_link': lambda x: np.log(x / (1 - x)),\n        'derivative': lambda x: 1.0 / (x * (1 - x))\n    }\n    \n    return links\n```\n\n### Explanation\n- **Identity Link**: The identity function simply returns the input, its inverse is the same, and the derivative is 1.\n- **Log Link**: The log function transforms the response variable, its inverse is the exponential function, and the derivative is the reciprocal of the response variable.\n- **Logit Link**: The logit function transforms the response variable, its inverse is the same function, and the derivative is the reciprocal of the product of the response variable and its complement.\n\nThis implementation ensures that each function works seamlessly with both scalars and numpy arrays, making it versatile for various applications in GLM analysis."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to compute the depth of the leaf reached by each sample in a random isolation tree. An isolation tree is a binary tree where each node splits the samples into left and right children based on a randomly chosen feature and a split value. The goal is to determine the depth of the leaf each sample ends up in after traversing the tree.\n\n### Approach\n1. **Set Random Seed**: To ensure reproducibility, we set the random seed to 0.\n2. **Recursive Tree Construction**: We use a helper function to recursively build the tree. Each node in the tree processes a subset of samples.\n3. **Leaf Node Check**: If a node has fewer than three samples, it is a leaf node, and we record the current depth for all samples in this node.\n4. **Splitting**: For non-leaf nodes, we choose a feature uniformly at random, determine the split value, and split the samples into left and right children.\n5. **Depth Assignment**: As we build the tree, we recursively process each child node and assign the appropriate depth to each sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    depths = [0] * n_samples\n\n    def helper(samples, depth):\n        if len(samples) < 3:\n            for s in samples:\n                depths[s] = depth\n        else:\n            n_features = data.shape[1]\n            f = np.random.randint(n_features)\n            f_values = data[samples, f]\n            down = min(f_values)\n            up = max(f_values)\n            v = np.random.uniform(down, up)\n            left = [s for s in samples if data[s, f] <= v]\n            right = [s for s in samples if data[s, f] > v]\n            helper(left, depth + 1)\n            helper(right, depth + 1)\n\n    helper(list(range(n_samples)), 0\n    return depths\n```\n\n### Explanation\n1. **Initialization**: The function starts by setting the random seed to ensure reproducibility.\n2. **Helper Function**: The helper function is defined to process each node recursively. It takes the current samples and the current depth as arguments.\n3. **Leaf Node Handling**: If a node has fewer than three samples, it is a leaf, and the depth of each sample in this node is set to the current depth.\n4. **Splitting Process**: For non-leaf nodes, a feature is chosen uniformly at random. The samples are split into left and right based on the chosen feature value.\n5. **Recursive Processing**: The helper function is called recursively on the left and right children, incrementing the depth for each subsequent level.\n\nThis approach ensures that each sample's depth is accurately recorded as it traverses the tree from the root to its respective leaf. The solution efficiently builds the tree and assigns depths using a recursive strategy, ensuring correctness and reproducibility."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to identify all frequent itemsets in a list of transactions based on a given minimum support threshold. An itemset is considered frequent if it appears in enough transactions, specifically at least the ceiling of the minimum support multiplied by the total number of transactions.\n\n### Approach\n1. **Preprocessing Transactions**: Convert each transaction into a set to facilitate quick lookups.\n2. **Calculate Required Support**: Determine the minimum number of transactions an itemset must appear in to be considered frequent. This is calculated as the ceiling of the minimum support threshold multiplied by the total number of transactions.\n3. **Generate Combinations**: For each possible size of item combinations (from 1 to the number of unique items), generate all possible combinations of items.\n4. **Count Support for Each Combination**: For each combination, count how many transactions include all items in the combination.\n5. **Check Frequency**: If a combination meets or exceeds the required support, add it to the result with its support count.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    if not transactions:\n        return {}\n    \n    N = len(transactions)\n    required_support = math.ceil(minsup * N)\n    \n    # Preprocess each transaction into a set for quick lookups\n    transactions_sets = [set(t) for t in transactions]\n    \n    # Collect all unique items across all transactions\n    all_items = set()\n    for t in transactions:\n        all_items.update(t)\n    all_items = list(all_items)\n    \n    result = {}\n    \n    # Iterate over all possible combination sizes\n    for k in range(1, len(all_items) + 1):\n        for combo in combinations(all_items, k):\n            count = 0\n            for t_set in transactions_sets:\n                if all(item in t_set for item in combo):\n                    count += 1\n            if count >= required_support:\n                sorted_combo = tuple(sorted(combo))\n                result[sorted_combo] = count\n    \n    return result\n```\n\n### Explanation\n1. **Preprocessing**: Each transaction is converted into a set to allow efficient membership checking.\n2. **Required Support Calculation**: The minimum number of transactions an itemset must appear in is determined using the formula `ceil(minsup * N)`, where `N` is the total number of transactions.\n3. **Combination Generation**: Using itertools, we generate all possible combinations of items of each possible size.\n4. **Support Counting**: For each combination, we count how many transactions include all items in the combination. If this count meets or exceeds the required support, the combination is added to the result.\n5. **Result Construction**: The result is a dictionary where each key is a tuple of sorted items, and the value is the support count.\n\nThis approach ensures that we efficiently find all frequent itemsets while adhering to the constraints and requirements of the problem."}
{"task_id": 11, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to cluster the given data into `k` groups, where each group is represented by a center point. The algorithm should repeatedly assign samples to the nearest center, recompute the centers as the mean of the samples in each cluster, and continue until the centers converge or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples as the initial cluster centers.\n2. **Assignment**: For each sample, determine the nearest cluster center based on the squared Euclidean distance.\n3. **Reinitialization**: If any cluster becomes empty during the assignment step, reinitialize its center with a random sample from the dataset.\n4. **Recompute Centers**: Calculate new cluster centers as the mean of the samples assigned to each cluster.\n5. **Convergence Check**: Stop the algorithm if the maximum change in any center between iterations is less than a specified threshold (`epsilon`) or if the maximum number of iterations (`max_iter`) is reached.\n6. **Rounding**: After convergence, round each coordinate of the centers to 4 decimal places and return the final centers along with the cluster labels for each sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n\n    if k < 1 or k > n_samples:\n        raise ValueError(\"k must be between 1 and the number of samples\")\n\n    old_centers = data[:k]\n\n    for _ in range(max_iter):\n        clusters = np.zeros(len(data), dtype=int)\n        for i, sample in enumerate(data):\n            min_dist = float('inf')\n            min_center = 0\n            for j, center in enumerate(old_centers):\n                dist = np.sum((sample - center) ** 2)\n                if dist < min_dist:\n                    min_dist = dist\n                    min_center = j\n            clusters[i] = min_center\n\n        # Check for empty clusters and reinitialize if necessary\n        empty_clusters = []\n        for j in range(k):\n            if clusters.count(j) == 0:\n                empty_clusters.append(j)\n\n        for j in empty_clusters:\n            idx = np.random.randint(len(data))\n            old_centers[j] = data[idx]\n\n        # Compute new centers as the mean of the samples in each cluster\n        new_centers = []\n        for j in range(k):\n            cluster_samples = data[clusters == j]\n            new_centers.append(np.mean(cluster_samples, axis=0))\n\n        # Check for convergence\n        max_diff = 0\n        for i in range(k):\n            diff = np.abs(old_centers[i] - new_centers[i])\n            if diff > max_diff:\n                max_diff = diff\n\n        if max_diff < epsilon:\n            break\n\n        old_centers = new_centers\n\n    # Round the centers to 4 decimal places\n    rounded_centers = [[round(c, 4) for c in center] for center in old_centers]\n\n    # Get the cluster labels for each sample\n    cluster_labels = clusters\n\n    return (rounded_centers, cluster_labels)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first `k` samples from the dataset.\n2. **Assignment**: Each sample is assigned to the nearest cluster center based on the squared Euclidean distance.\n3. **Reinitialization**: If any cluster becomes empty, its center is reinitialized with a random sample from the dataset to ensure the algorithm can continue.\n4. **Recompute Centers**: The new cluster centers are calculated as the mean of the samples in each cluster.\n5. **Convergence Check**: The algorithm stops if the maximum change in any center is less than `epsilon` or the maximum number of iterations is reached.\n6. **Rounding**: The final centers are rounded to 4 decimal places, and the cluster labels for each sample are returned along with the centers."}
{"task_id": 13, "completion_id": 0, "completion": "To solve this problem, we need to implement an item-based k-nearest neighbor (k-NN) collaborative filtering recommender. The goal is to predict the most attractive items for a given active user based on their ratings and the similarity between items. The similarity can be measured using either cosine similarity or Pearson correlation.\n\n### Approach\n1. **Build an Item-Item Similarity Matrix**: For each pair of items, compute their similarity based on the ratings provided by the users. The similarity is computed using either cosine similarity or Pearson correlation depending on the specified criterion.\n2. **Collect Rated Items**: Identify the items that the active user has already rated.\n3. **Compute Predicted Attractiveness**: For each unrated item, compute a score that represents its predicted attractiveness based on the similarity matrix and the ratings of the active user.\n4. **Sort and Return Recommendations**: Sort the unrated items based on their predicted attractiveness in descending order. In case of ties, the item with the smaller index is chosen first. Return the top k items.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_user, n_item = data.shape\n    if n_user == 0:\n        return []\n\n    # Build item-user sets\n    item_users = [set() for _ in range(n_item)]\n    for user in range(n_user):\n        for i in range(n_item):\n            if data[user, i] > 0:\n                item_users[i].add(user)\n\n    # Initialize similarity matrix\n    S = np.zeros((n_item, n_item))\n\n    # Compute similarity matrix\n    for i in range(n_item):\n        for j in range(i + 1, n_item):\n            common_users = item_users[i].intersection(item_users[j])\n            if not common_users:\n                S[i, j] = 0.0\n                S[j, i] = 0.0\n            else:\n                v1 = [data[u, i] for u in common_users]\n                v2 = [data[u, j] for u in common_users]\n                if criterion == 'cosine':\n                    mean_v1 = np.mean(v1)\n                    mean_v2 = np.mean(v2)\n                    if np.std(v1) > 1e-3:\n                        v1_centered = v1 - mean_v1\n                    else:\n                        v1_centered = v1\n                    if np.std(v2) > 1e-3:\n                        v2_centered = v2 - mean_v2\n                    else:\n                        v2_centered = v2\n                    dot = np.dot(v1_centered, v2_centered)\n                    len_v1 = np.linalg.norm(v1_centered)\n                    len_v2 = np.linalg.norm(v2_centered)\n                    if len_v1 == 0 or len_v2 == 0:\n                        sim = 0.0\n                    else:\n                        sim = dot / (len_v1 * len_v2)\n                    S[i, j] = sim\n                    S[j, i] = sim\n                elif criterion == 'pearson':\n                    correlation = np.corrcoef(v1, v2)[0, 0]\n                    S[i, j] = correlation\n                    S[j, i] = correlation\n\n    # Get rated items for the active user\n    rated_items = np.where(data[user_ind, :] > 0)[0]\n    if len(rated_items) == n_item:\n        return []\n\n    r = data[user_ind, rated_items]\n    unrated_items = [t for t in range(n_item) if t not in rated_items]\n\n    # Compute scores for unrated items\n    scores = []\n    for t in unrated_items:\n        sum_product = 0.0\n        sum_abs = 0.0\n        for i in rated_items:\n            sum_product += r[i] * S[t, i]\n            sum_abs += abs(S[t, i])\n        if sum_abs == 0:\n            score = 0.0\n        else:\n            score = sum_product / sum_abs\n        scores.append((score, t))\n\n    # Sort by descending score, then ascending item index\n    scores.sort(key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in scores[:k]]\n\n    return result\n```\n\n### Explanation\n1. **Building the Similarity Matrix**: The code first constructs a similarity matrix `S` where each entry `S[i, j]` represents the similarity between item `i` and item `j`. This is done by finding common users who rated both items and computing the similarity based on the specified criterion.\n2. **Collecting Rated Items**: The code identifies which items the active user has rated and extracts their ratings.\n3. **Computing Predicted Attractiveness**: For each unrated item, the code computes a score by summing the product of the item's similarity with each rated item's rating, normalized by the sum of the absolute similarities.\n4. **Sorting and Returning Recommendations**: The unrated items are sorted by their predicted scores in descending order, and the top `k` items are returned. In case of ties, the item with the smaller index is chosen first.\n\nThis approach ensures that the recommendations are based on the collaborative filtering technique, leveraging the similarity between items to predict the most attractive items for the active user."}
{"task_id": 20, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes both the sigmoid activation and its gradient for a given input. The function should handle various input types, including scalars, lists, and NumPy arrays, and return the results rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a scalar, a list, or a NumPy array. We first convert it into a NumPy array to handle it uniformly.\n2. **Compute Sigmoid Function**: The sigmoid function is defined as \\( s(x) = \\frac{1}{1 + e^{-x}} \\). We compute this for the input array.\n3. **Compute Gradient**: The gradient of the sigmoid function is given by \\( s'(x) = s(x) \\times (1 - s(x)) \\). Using the result from the sigmoid function, we compute the gradient.\n4. **Round Results**: Both the sigmoid values and gradients are rounded to four decimal places.\n5. **Return Appropriate Data Types**: If the input was a scalar, return the results as floats. If the input was an array, return the results as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    x = np.asarray(x)\n    sig_x = 1 / (1 + np.exp(-x))\n    grad_x = sig_x * (1 - sig_x)\n    rounded_sig = np.round(sig_x, 4)\n    rounded_grad = np.round(grad_x, 4)\n    \n    if x.ndim == 0:\n        return (rounded_sig.item(), rounded_grad.item())\n    else:\n        return (rounded_sig.tolist(), rounded_grad.tolist())\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to handle both scalar and array inputs uniformly.\n- **Sigmoid Calculation**: Using the formula \\( s(x) = \\frac{1}{1 + e^{-x}} \\), we compute the sigmoid values for each element in the input array.\n- **Gradient Calculation**: The gradient is computed using the derivative of the sigmoid function, which is \\( s'(x) = s(x) \\times (1 - s(x)) \\).\n- **Rounding**: Both the sigmoid values and gradients are rounded to four decimal places to meet the problem requirements.\n- **Return Type Handling**: The function checks if the input was a scalar (0-dimensional array) and returns the results as floats. For arrays, it returns the results as lists.\n\nThis approach ensures that the function works efficiently for various input types and returns the results in the required format."}
{"task_id": 25, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that predicts labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function \u2013 RBF) kernel. The function should compute the decision function for each test sample and return the predicted labels as a list of integers.\n\n### Approach\nThe decision function for an SVM with a Gaussian kernel is given by:\n\\[ g(z) = \\sum_{i=1}^{n} \\alpha_i y_i \\exp(-\\gamma ||x_i - z||^2) + b \\]\nwhere \\( z \\) is a test sample, \\( \\alpha \\) are the Lagrange multipliers, \\( y \\) are the class labels, \\( \\gamma \\) is the Gaussian kernel hyper-parameter, and \\( b \\) is the bias term. The predicted class is determined by the sign of \\( g(z) \\).\n\nThe steps to implement this function are as follows:\n1. **Compute the Kernel Matrix**: The kernel matrix \\( K \\) is computed where each element \\( K_{ij} \\) is the Gaussian kernel between the \\( i \\)-th training sample and the \\( j \\)-th test sample. This is done using the formula \\( K_{ij} = \\exp(-\\gamma ||x_i - z_j||^2) \\).\n2. **Compute Scores**: For each test sample, compute the score by taking the dot product of the vector of Lagrange multipliers multiplied by their corresponding class labels and the kernel vector, then add the bias term.\n3. **Predict Labels**: Convert the scores to predicted labels by taking the sign of each score, resulting in 1 or -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters:\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns:\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly 1 or -1.\n    \"\"\"\n    # Compute the kernel matrix K\n    kernel = np.exp(-gamma * (X_train - X_test) @ (X_train - X_test).T)\n    # Compute the scores for each test sample\n    scores = (alpha * y_train) @ kernel + b\n    # Determine the predicted labels\n    predicted_labels = np.sign(scores).astype(int)\n    return predicted_labels.tolist()\n```\n\n### Explanation\n1. **Kernel Matrix Calculation**: The kernel matrix is computed using the formula \\( K = \\exp(-\\gamma (X_{\\text{train}} - X_{\\text{test}})^T (X_{\\text{train}} - X_{\\text{test}})) \\). This matrix efficiently captures the Gaussian similarity between each training and test sample.\n2. **Score Calculation**: For each test sample, the score is computed by multiplying the vector of Lagrange multipliers (scaled by their class labels) with the corresponding kernel vector and adding the bias term.\n3. **Prediction**: The predicted label for each test sample is determined by the sign of the computed score, resulting in either 1 or -1.\n\nThis approach ensures that the function is efficient and vectorized, leveraging numpy operations to handle large datasets effectively."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to compute the optimal rank-k reconstruction of a given matrix using the truncated Singular Value Decomposition (SVD). This reconstruction is equivalent to the best linear autoencoder with k latent units. The task involves checking the validity of the rank k, computing the SVD, reconstructing the matrix, and calculating the mean squared error (MSE).\n\n### Approach\n1. **Check Validity of k**: Ensure that the given rank k is within the valid range, i.e., 1 \u2264 k \u2264 min(m, n), where m and n are the dimensions of the input matrix X. If k is invalid, return (-1, -1).\n2. **Compute SVD**: Use numpy's SVD function to decompose the matrix X into three matrices: U, S, and V.\n3. **Reconstruct the Matrix**: Using the top k singular values and corresponding singular vectors from U and V, reconstruct the matrix. The reconstruction is done by multiplying the first k columns of U with the first k singular values and then with the transpose of the first k rows of V.\n4. **Calculate MSE**: Compute the mean squared error between the original matrix X and the reconstructed matrix X_hat.\n5. **Round Results**: Round both the reconstructed matrix and the MSE to four decimal places and convert the matrix back to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns (-1, -1).\n    \"\"\"\n    # Check if k is valid\n    m = len(X)\n    if m == 0:\n        return (-1, -1)\n    n = len(X[0])\n    if k < 1 or k > min(m, n):\n        return (-1, -1)\n    \n    # Convert X to a numpy array for SVD computation\n    X_array = np.array(X)\n    \n    # Compute SVD\n    U, S, V = np.svd(X_array)\n    \n    # Extract the first k components\n    U_k = U[:, :k]\n    S_k = S[:k, :]  # S is k x k, so slicing as is\n    V_k = V  # V is k x n\n    \n    # Compute the rank-k reconstruction\n    X_hat = np.dot(U_k, np.dot(S_k, V.T))\n    \n    # Calculate the mean squared error\n    mse = np.mean(np.square(X_array - X_hat))\n    \n    # Round to four decimal places\n    X_hat_rounded = np.round(X_hat, 4)\n    mse_rounded = np.round(mse, 4)\n    \n    # Convert X_hat to a list of lists\n    X_hat_list = X_hat_rounded.tolist()\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if k is within the valid range. If not, it returns (-1, -1).\n2. **SVD Computation**: The SVD of the input matrix X is computed using numpy's `svd` function, which returns the matrices U, S, and V.\n3. **Matrix Reconstruction**: The top k singular values and vectors are extracted from U, S, and V. The reconstruction is performed by multiplying these components in the order U_k @ S_k @ V.T.\n4. **MSE Calculation**: The mean squared error between the original and reconstructed matrices is computed and rounded to four decimal places.\n5. **Result Conversion**: The reconstructed matrix is converted back to a list of lists and both the matrix and MSE are returned as a tuple, rounded to four decimal places."}
{"task_id": 34, "completion_id": 0, "completion": "To solve this problem, we need to generate a NumPy tensor filled with 0.0s and 1.0s based on the specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and handle edge cases such as invalid sparsity values.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return a float array filled with -1.0 of the specified shape.\n2. **Seed Handling**: If a seed is provided, set it using `np.random.seed(seed)` to ensure reproducibility.\n3. **Random Array Generation**: Generate a random array using `np.random.rand(*shape)` to create an array of random floats in the specified shape.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity` to determine which values will be set to 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where values greater than or equal to the threshold are set to 1.0 and others to 0.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generate a binary tensor with 0.0 and 1.0 values based on the given shape and sparsity.\n\n    Parameters:\n        shape (tuple[int, ...]): The shape of the output tensor.\n        sparsity (float, optional): Probability of generating 1.0. Must be in [0, 1]. Default is 0.5.\n        seed (int, optional): Seed for reproducibility. If provided, np.random.seed is called. Default is None.\n\n    Returns:\n        numpy.ndarray: A tensor of float dtype with 0.0 and 1.0 values. If sparsity is invalid, returns a float array with -1.0 in the same shape.\n    \"\"\"\n    # Check if sparsity is within the valid range\n    if not (0.0 <= sparsity <= 1.0):\n        return np.full(shape, -1.0, dtype=np.float64)\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate the random array\n    rand_arr = np.random.rand(*shape)\n    \n    # Calculate the threshold for determining 1.0 values\n    threshold = 1.0 - sparsity\n    \n    # Create the binary array\n    binary_arr = np.where(rand_arr >= threshold, 1.0, 0.0)\n    \n    return binary_arr\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns a float array filled with -1.0 of the specified shape.\n- **Seed Handling**: If a seed is provided, it initializes the random number generator to ensure consistent results.\n- **Random Array Generation**: Using `np.random.rand(*shape)`, the function generates an array of random values between 0 and 1.\n- **Threshold Calculation**: The threshold is computed as `1 - sparsity`, determining which values are set to 1.0.\n- **Binary Conversion**: The function converts the random array into a binary array using `np.where`, setting values above the threshold to 1.0 and others to 0.0.\n\nThis approach ensures that the function efficiently generates the required binary tensor while handling edge cases and ensuring reproducibility when a seed is provided."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs a median split on a dataset, which is a fundamental operation used in constructing a KD-Tree. The function will split the dataset into three parts: the pivot (the row with the median value), the left part (rows with values strictly less than the median), and the right part (rows with values strictly greater than the median).\n\n### Approach\n1. **Extract the Column**: Extract the specified column from the dataset.\n2. **Sort the Column**: Use `numpy.argpartition` to sort the column and get the indices of the sorted values.\n3. **Find the Median**: Determine the median value from the sorted column. For an even number of samples, the median is the element at the middle index.\n4. **Identify the Pivot**: The pivot is the row index of the median value in the original dataset.\n5. **Split the Data**: Create two lists, `left` and `right`, containing the indices of rows with values strictly less than and greater than the median, respectively.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger values.\n    \"\"\"\n    n = data.shape[0]\n    sorted_indices = np.argpartition(data[:, d])\n    median_value = data[sorted_indices[n//2]]\n    pivot = sorted_indices[n//2]\n    \n    left = []\n    right = []\n    for i in range(n):\n        if data[i][d] < median_value:\n            left.append(i)\n        elif data[i][d] > median_value:\n            right.append(i)\n    \n    return (pivot, left, right)\n```\n\n### Explanation\n1. **Extract the Column**: The specified column `d` is extracted from the dataset using slicing.\n2. **Sort the Column**: `numpy.argpartition` is used to sort the column and return the indices of the sorted values. This helps in efficiently finding the median without sorting the entire array.\n3. **Find the Median**: The median is found at the middle index of the sorted column. For an even number of samples, this is the element at `n//2`.\n4. **Identify the Pivot**: The pivot is the row index of the median value in the original dataset.\n5. **Split the Data**: By iterating through the dataset, we collect indices of rows with values less than and greater than the median into `left` and `right` lists, respectively. These lists are naturally sorted as we iterate from the start to the end of the dataset.\n\nThis approach ensures that the function runs in O(n) time, making it efficient for large datasets. The resulting lists are sorted increasingly, ensuring deterministic output."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that converts a one-dimensional signal into overlapping frames using NumPy. Each frame is a window of a specified width, and consecutive frames are separated by a given stride. The function should return a view on the original array, ensuring efficient memory usage.\n\n### Approach\n1. **Check Pre-Conditions**: Ensure the input array is one-dimensional, the stride is positive, and the array is long enough to form the frames.\n2. **Calculate Number of Frames**: Determine the number of frames by using the formula `(len(x) - frame_width) // stride + 1`, which accounts for the number of complete frames that can be formed.\n3. **Create Overlapping Frames**: Use NumPy's `as_strided` function to create a view of the original array. This function allows us to efficiently create the frames without copying the data, thus maintaining memory efficiency.\n4. **Set Writeable Flag**: Adjust the writeable flag of the resulting view based on the provided `writeable` parameter.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    if x.ndim != 1:\n        raise AssertionError(\"x must be 1-D\")\n    if stride < 1:\n        raise AssertionError(\"stride must be >= 1\")\n    if x.size < frame_width:\n        raise AssertionError(\"x must be at least frame_width in length\")\n\n    # Calculate the number of frames\n    n_frames = (x.size - frame_width) // stride + 1\n\n    # Create the view using as_strided\n    view = as_strided(x, shape=(n_frames, frame_width), strides=(stride, 1))\n\n    # Set the writeable flag\n    view.flags.writeable = writeable\n\n    return view\n```\n\n### Explanation\n- **Pre-Conditions Check**: The function first ensures that the input array is one-dimensional, the stride is positive, and the array is long enough to form the frames. If any of these conditions are not met, an `AssertionError` is raised.\n- **Frame Calculation**: The number of frames is calculated to ensure we only consider complete frames, dropping any trailing samples that do not form a complete frame.\n- **View Creation**: Using `as_strided`, the function efficiently creates a view of the original array, which is memory efficient as it does not require copying the data.\n- **Writeable Flag**: The resulting view's writeable flag is set based on the provided `writeable` parameter, ensuring the returned array is either writeable or read-only as required.\n\nThis approach ensures that the function is both efficient and meets the problem requirements, providing a clear and concise solution."}
{"task_id": 48, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given matrix is row-stochastic. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1, within a small numerical tolerance.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check if the input is a numpy array or a list of lists**: The function should handle both cases uniformly.\n2. **Check if the matrix is two-dimensional**: For numpy arrays, this is done by checking the number of dimensions. For lists, ensure each element is a list and all rows have the same length.\n3. **Check if all elements are valid probabilities**: Each element must be between 0 and 1 inclusive.\n4. **Check if each row sums to 1 within a tolerance**: The sum of each row should be approximately 1, allowing for a small numerical error (1e-8).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        # Check if the matrix is at least 2D\n        if X.ndim < 2:\n            return False\n        # Check if all elements are between 0 and 1\n        if not np.all((X >= 0) & (X <= 1)):\n            return False\n        # Check if each row sums to 1 within a tolerance\n        row_sums = np.sum(X, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-8):\n            return False\n        return True\n    else:\n        # X is a list of lists\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Check if all rows have the same length\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        # Check if the matrix is not empty\n        if len(X) == 0:\n            return False\n        # Convert to numpy array\n        try:\n            X_np = np.array(X)\n        except:\n            return False\n        # Check if all elements are numeric and of float type\n        if not np.issubdtype(X_np.dtype, np.floating):\n            return False\n        # Check if all elements are between 0 and 1\n        if not np.all((X_np >= 0) & (X_np <= 1)):\n            return False\n        # Check if each row sums to 1 within a tolerance\n        row_sums = np.sum(X_np, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-8):\n            return False\n        return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a numpy array or a list of lists. This ensures that the function can handle both types uniformly.\n2. **Dimension Check**: For numpy arrays, it checks if the array is at least 2D. For lists, it ensures each element is a list and all rows have the same length.\n3. **Element Check**: The function verifies that all elements are within the range [0, 1]. This is done using numpy's array operations for efficiency.\n4. **Row Sum Check**: The function calculates the sum of each row and checks if each sum is approximately 1, allowing for a small numerical tolerance (1e-8) to account for floating-point precision issues.\n\nThis approach ensures that the function efficiently and accurately determines if the given matrix is row-stochastic, covering all edge cases and ensuring numerical stability."}
{"task_id": 55, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least squares (OLS) linear regression. The goal is to find the weight vector that minimizes the mean squared error between the predicted values and the actual target values. The RMSprop algorithm is an optimization technique that efficiently updates the weights by adjusting them in the direction opposite to the gradient of the loss function, scaled by a learning rate and a decay factor for the squared gradients.\n\n### Approach\n1. **Problem Analysis**: The task is to minimize the mean squared error (MSE) between the predicted values and the actual target values. The loss function is given by \\( L(w) = \\frac{1}{2n} \\| Xw - y \\|_2^2 \\), where \\( X \\) is the feature matrix, \\( y \\) is the target vector, and \\( w \\) is the weight vector.\n\n2. **Gradient Calculation**: The gradient of the loss function with respect to \\( w \\) is computed as \\( g = \\frac{1}{n} X^T (Xw - y) \\), where \\( X^T \\) is the transpose of the feature matrix.\n\n3. **RMSprop Update Rule**: The RMSprop algorithm updates the weights using the following steps:\n   - Compute the squared gradients and maintain a running average using a decay factor \\( \\rho \\).\n   - Update the weights by moving in the direction opposite to the gradient, scaled by a learning rate \\( \\eta \\) and divided by the square root of the running average of squared gradients, plus a small constant to avoid division by zero.\n\n4. **Batch Processing**: The data is processed in mini-batches. If the number of samples \\( n \\) is less than the batch size, the entire dataset is used as one batch. Otherwise, successive blocks of rows are processed, wrapping around to the beginning when the end of the matrix is reached.\n\n5. **Stopping Conditions**: The optimization stops when the norm of the gradient becomes smaller than a given threshold \\( \\epsilon \\) or the maximum number of iterations \\( \\text{max\\_iter} \\) is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n = X.shape[0]\n    d = X.shape[1]\n    w = np.zeros(d, dtype=np.float64)\n    s = np.zeros(d, dtype=np.float64)\n    current_pos = 0\n\n    for iteration in range(max_iter):\n        if n < batch_size:\n            batch_x = X\n            batch_y = y\n        else:\n            start = current_pos\n            end = start + batch_size\n            if end > n:\n                take_end = n - start\n                take_start = batch_size - take_end\n                if take_start < 0:\n                    take_start = 0\n                batch_x = X[start: start + take_end] + X[:take_start]\n                batch_y = y[start: start + take_end] + y[:take_start]\n            else:\n                batch_x = X[start:end]\n                batch_y = y[start:end]\n            current_pos = end % n\n\n        # Compute Xw\n        Xw = batch_x @ w\n        residuals = Xw - batch_y\n        # Compute gradient\n        g = (1.0 / n) * np.dot(X.T, residuals)\n        # Compute squared gradient\n        g_squared = g ** 2\n        # Update s\n        s = rho * s + (1 - rho) * g_squared\n        # Compute denominator\n        sqrt_s = np.sqrt(s)\n        denominator = sqrt_s + eps_station\n        # Update weights\n        w = w - eta * g / denominator\n        # Check stopping conditions\n        norm_g = np.linalg.norm(g)\n        if norm_g < epsilon:\n            break\n        if iteration == max_iter - 1:\n            break\n\n    # Round to four decimal places\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The weights \\( w \\) and the squared gradient average \\( s \\) are initialized to zero. The current position for batch processing is set to the start of the data.\n- **Batch Processing**: For each iteration, the function processes a batch of data. If the batch size is larger than the number of samples, the entire dataset is used. Otherwise, successive blocks of rows are processed, wrapping around to the beginning when the end is reached.\n- **Gradient Calculation**: The gradient \\( g \\) is computed using the current batch, and the squared gradient is used to update the running average \\( s \\).\n- **Weight Update**: The weights are updated using the RMSprop rule, which adjusts the learning rate by the gradient and the decay factor.\n- **Stopping Conditions**: The optimization stops when the gradient's norm is below a threshold or the maximum number of iterations is reached.\n- **Result**: The final weights are rounded to four decimal places and returned as a list."}
{"task_id": 56, "completion_id": 0, "completion": "To solve this problem, we need to implement the SoftPlus activation function, which is widely used in deep learning models. The function should be able to compute both the SoftPlus value and its derivative (which is the sigmoid function) based on the input parameters. The solution must handle both scalar and array inputs, ensuring numerical stability for very large positive or negative values.\n\n### Approach\nThe SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\). The derivative of this function is the sigmoid function, \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). \n\nTo handle numerical stability, we use a piece-wise approach:\n1. For positive values of \\( x \\), compute \\( \\text{SoftPlus}(x) \\) as \\( x + \\log(1 + e^{-x}) \\) to avoid overflow.\n2. For non-positive values of \\( x \\), compute \\( \\text{SoftPlus}(x) \\) as \\( \\log(1 + e^x) \\).\n\nFor the derivative (sigmoid function), we use the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\), which is computed efficiently using a piece-wise approach to avoid numerical issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function and its derivative (sigmoid).\"\"\"\n    if isinstance(x, (int, float)):\n        # Handle scalar input\n        if deriv:\n            return round(1.0 / (1 + np.exp(-x)), 4)\n        else:\n            if x > 0:\n                return round(x + np.log(1 + np.exp(-x)), 4)\n            else:\n                return round(np.log(1 + np.exp(x)), 4)\n    else:\n        # Convert list or array to numpy array\n        x = np.array(x)\n        if deriv:\n            # Compute sigmoid\n            sig = 1 / (1 + np.exp(-x))\n            sig_rounded = np.round(sig, 4)\n            return sig_rounded.tolist()\n        else:\n            # Compute SoftPlus\n            exp_x = np.exp(x)\n            log_val = np.where(x > 0, x + np.log(1 + np.exp(-x)), np.log(1 + exp_x))\n            log_val_rounded = np.round(log_val, 4)\n            return log_val_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Scalar Input**: If the input `x` is a scalar (int or float), the function directly computes the result using the appropriate formula based on whether the derivative is required.\n2. **Handling Array Input**: If the input `x` is an array or list, it is converted to a numpy array for efficient computation. The function then computes the SoftPlus or sigmoid values element-wise.\n3. **Numerical Stability**: The function uses `np.exp` and `np.log` functions to handle large values, ensuring numerical stability by avoiding overflow through a piece-wise approach.\n4. **Rounding**: The results are rounded to four decimal places and returned as a list for array inputs or as a single float for scalar inputs.\n\nThis approach ensures that the function efficiently and accurately computes both the SoftPlus activation and its derivative, handling various input types and ensuring numerical stability."}
{"task_id": 58, "completion_id": 0, "completion": "To solve this problem, we need to compute the unsmoothed Maximum-Likelihood N-gram log-probability of a given sequence using a training corpus. The solution involves building an N-gram model from the corpus and then using this model to compute the log probability of the target sequence.\n\n### Approach\n1. **Building the N-gram Model**: \n   - For N=1 (unigram), we count the occurrences of each word in the corpus.\n   - For N>1, we build a nested dictionary where each key is an (N-1)-gram, and the value is another dictionary of N-grams that start with that (N-1)-gram, with their counts.\n\n2. **Computing the Log Probability**:\n   - For each window of size N in the sequence, compute the log probability using the precomputed N-gram model.\n   - For N=1, the probability of each word is its count divided by the total number of words in the corpus.\n   - For N>1, the probability of each N-gram is the count of the N-gram divided by the count of its (N-1)-gram prefix.\n   - Sum the log probabilities of all valid windows in the sequence. If any required count is zero, return negative infinity.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    The function builds an N-gram count table from the *corpus* and returns the total log-probability\n    of the *sequence* under the resulting unsmoothed language model. If any required count is zero,\n    it returns ``float('-inf')``.\n\n    Args:\n        corpus: List of word tokens.\n        sequence: List of word tokens to compute the log-probability for.\n        N: Order of the N-gram model.\n\n    Returns:\n        The total log-probability rounded to 4 decimal places, or ``float('-inf')`` if any required count is zero.\n    \"\"\"\n    if N == 0:\n        return float('-inf')  # N must be at least 1\n\n    # Build the ngram counts\n    if N == 1:\n        ngram_counts = defaultdict(int)\n        for word in corpus:\n            ngram_counts[word] += 1\n    else:\n        ngram_counts = defaultdict(lambda: defaultdict(int))\n        for i in range(len(corpus) - N + 1):\n            current_gram = tuple(corpus[i:i+N])\n            prefix = tuple(corpus[i:i+N-1])\n            ngram_counts[prefix][current_gram] += 1\n\n    total = 0.0\n\n    if N == 1:\n        total_words = len(corpus)\n        for word in sequence:\n            if word not in ngram_counts or ngram_counts[word] == 0:\n                return float('-inf')\n            prob = math.log(ngram_counts[word] / total_words)\n            total += prob\n    else:\n        for i in range(len(sequence) - N + 1):\n            window = tuple(sequence[i:i+N])\n            prefix = tuple(sequence[i:i+N-1])\n            if prefix not in ngram_counts or window not in ngram_counts[prefix]:\n                return float('-inf')\n            count_window = ngram_counts[prefix][window]\n            count_prefix = sum(ngram_counts[prefix].values())\n            if count_window == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = math.log(count_window / count_prefix)\n            total += prob\n\n    return round(total, 4)\n```\n\n### Explanation\n1. **Building the N-gram Model**:\n   - For N=1, we use a flat dictionary to count word occurrences.\n   - For N>1, we use a nested dictionary to count occurrences of each N-gram and their respective (N-1)-gram prefixes.\n\n2. **Computing Log Probabilities**:\n   - For each window of size N in the sequence, we check if the window and its prefix exist in the model.\n   - If any window or prefix is missing, we return negative infinity.\n   - For each valid window, we compute the log probability and sum these values.\n   - The result is rounded to four decimal places and returned.\n\nThis approach ensures that we efficiently compute the log probability using the Maximum-Likelihood estimation for the given sequence under the unsmoothed N-gram model."}
{"task_id": 65, "completion_id": 0, "completion": "To solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The HMM is defined by its transition matrix `A`, emission matrix `B`, and initial distribution `pi`. The backward algorithm efficiently computes this probability by recursively calculating intermediate values known as beta values.\n\n### Approach\n1. **Check for Empty Inputs**: If any of the input lists (A, B, pi, or obs) are empty, return 0.0 immediately as no valid probability can be computed.\n2. **Initialize Beta Values**: The beta values represent the probability of the remaining part of the observation sequence given the current state. For the last observation, the beta value for each state is initialized to 1.0.\n3. **Compute Beta Values Recursively**: Starting from the second-to-last observation and moving backwards, compute the beta values for each state using the transition matrix `A` and emission matrix `B`.\n4. **Compute Final Probability**: After processing all observations, compute the final probability by summing the contributions from each state, considering the initial distribution `pi` and the computed beta values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to the nearest 6th decimal.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n    N = len(A)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n\n    # Initialize beta_prev for t = T-1\n    beta_prev = [1.0 for _ in range(N)]\n\n    for t in range(T-2, -1, -1):\n        current_obs = obs[t+1]\n        beta_current = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                beta_current[i] += A[i][j] * B[j][current_obs] * beta_prev[j]\n        beta_prev = beta_current\n\n    # Calculate the final probability\n    result = 0.0\n    for i in range(N):\n        result += pi[i] * B[i][obs[0]] * beta_prev[i]\n    return round(result, 6)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if any input list is empty and returns 0.0 if so. It then initializes the beta values for the last observation to 1.0.\n2. **Backward Calculation**: For each observation from the second last to the first, the function updates the beta values using the transition and emission probabilities. This is done iteratively, updating the beta values for each state based on the next observation.\n3. **Final Probability**: After processing all observations, the function computes the final probability by summing the contributions from each state, considering the initial distribution and the computed beta values. The result is then rounded to six decimal places.\n\nThis approach efficiently computes the probability using dynamic programming, ensuring that the solution is both time and space efficient."}
{"task_id": 69, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward algorithm for Hidden Markov Models (HMMs) to compute the likelihood of a given sequence of observations. The HMM is defined by its initial state probabilities, transition matrix, and emission matrix. The goal is to determine the probability that the model generates the exact sequence of observations provided.\n\n### Approach\n1. **Input Validation**: \n   - Ensure the initial state vector `S` is a valid probability vector.\n   - Ensure the transition matrix `A` is a valid square matrix.\n   - Ensure the emission matrix `B` is a valid matrix with the correct dimensions.\n   - Ensure the observations list is non-empty and each observation is within the valid range.\n\n2. **Forward Algorithm**:\n   - Initialize the current state vector with the initial state probabilities.\n   - For each observation in the sequence, update the state vector by first transitioning to the next state using the transition matrix `A`, then emitting the observation using the emission matrix `B`.\n   - After processing all observations, the sum of the current state vector gives the total probability of the sequence.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)\n    if n == 0:\n        return -1\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    m = len(B[0]) if B else 0\n    for row in B:\n        if len(row) != m:\n            return -1\n\n    # Validate observations\n    for obs in observations:\n        if not isinstance(obs, int):\n            return -1\n        if obs < 0 or obs >= m:\n            return -1\n\n    if len(observations) == 0:\n        return -1  # Empty sequence is invalid\n\n    # Initialize the current state vector\n    current_state = np.array(S, dtype=np.float64)\n\n    # Process each observation\n    for obs in observations:\n        current_state = current_state.dot(A)\n        current_state = current_state.dot(B)\n\n    # Return the total probability rounded to 4 decimal places\n    total_probability = np.sum(current_state)\n    return round(total_probability, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices and vectors have the correct dimensions and if the observations are valid. If any check fails, it returns -1.\n2. **State Initialization**: The initial state vector `current_state` is initialized with the given probabilities.\n3. **Processing Observations**: For each observation, the state vector is updated by first transitioning to the next state using the transition matrix `A` and then emitting the observation using the emission matrix `B`.\n4. **Result Calculation**: After processing all observations, the sum of the current state vector is computed, which represents the total probability of the sequence. This value is then rounded to four decimal places and returned.\n\nThis approach ensures that the function correctly computes the likelihood of the given sequence using the forward algorithm for HMMs, handling both valid and invalid inputs appropriately."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The goal is to predict class labels for each sample in an unlabeled test set based on the majority vote of the k closest labeled training samples. The solution must handle edge cases such as invalid values of k and ensure that ties in class labels are resolved by selecting the smallest label.\n\n### Approach\n1. **Input Validation**: Check if the value of k is a positive integer and does not exceed the number of training samples. If not, return -1.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. Using squared distance avoids the computational cost of taking square roots while preserving the order of distances.\n3. **Nearest Neighbors**: For each test sample, identify the k nearest neighbors based on the computed distances.\n4. **Majority Vote**: Collect the class labels of these k nearest neighbors and determine the most frequent label. In case of a tie, select the smallest label.\n5. **Result Compilation**: Store the predicted class labels for each test sample and return the results.\n\n### Solution Code\n```python\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is a positive integer and within valid range\n    if not isinstance(k, int) or k < 1 or k > len(X_train):\n        return -1\n    if len(X_train) == 0:\n        return -1  # No training samples to use\n    \n    result = []\n    for x_test in X_test:\n        distances = []\n        for x_train, y in zip(X_train, y_train):\n            # Compute squared Euclidean distance\n            dist = sum((x_test[i] - x_train[i])**2 for i in range(len(x_test)))\n            distances.append((dist, y))\n        # Sort the samples by distance\n        distances.sort()\n        # Take the first k samples\n        k_neighbors = distances[:k]\n        # Extract the class labels\n        y_values = [y for (d, y) in k_neighbors]\n        # Count frequency of each class\n        freq = {}\n        for y in y_values:\n            freq[y] = freq.get(y, 0) + 1\n        # Find the maximum frequency\n        max_freq = max(freq.values())\n        # Collect all classes with max frequency\n        candidates = [y for y in freq if freq[y] == max_freq]\n        # Choose the smallest class in case of tie\n        prediction = min(candidates)\n        result.append(prediction)\n    return result\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if k is a positive integer and within the valid range. If not, it returns -1.\n2. **Distance Calculation**: For each test sample, the function computes the squared Euclidean distance to each training sample, which is efficient and avoids the computational overhead of square root operations.\n3. **Nearest Neighbors**: The distances are sorted, and the k nearest neighbors are selected.\n4. **Majority Vote**: The class labels of these neighbors are collected, and the most frequent label is determined. In case of a tie, the smallest label is chosen.\n5. **Result Compilation**: The predicted labels for all test samples are compiled into a list and returned.\n\nThis approach ensures that the solution is both efficient and correct, handling all edge cases as specified."}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to compute the unnormalised categorical cross-entropy loss for a batch of one-hot encoded targets. The cross-entropy loss is a common loss function used in machine learning, especially in classification tasks. It measures the difference between the predicted probabilities and the actual labels, providing a way to train models to minimize this loss.\n\n### Approach\nThe categorical cross-entropy loss is calculated using the formula:\n\\[ L = -\\sum_{i=1}^{n\\_samples} \\sum_{j=1}^{n\\_classes} y_{ij} \\cdot \\log(y_{ij} + \\epsilon) \\]\nwhere \\( y \\) is the true class labels (one-hot encoded) and \\( y_{pred} \\) are the predicted probabilities. Here, \\( \\epsilon \\) is a small value added to avoid taking the logarithm of zero.\n\nThe steps to compute the loss are as follows:\n1. Convert the input arrays \\( y \\) and \\( y_{pred} \\) to numpy arrays if they are not already.\n2. Compute the sum of the product of each element in \\( y \\) with the logarithm of the corresponding element in \\( y_{pred} \\), adding a small epsilon to each element of \\( y_{pred} \\) to avoid taking the logarithm of zero.\n3. Sum all these values to get the total loss.\n4. Multiply the total loss by -1 to get the negative of the sum, which is the cross-entropy loss.\n5. Return the result rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    epsilon = 1e-10\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    sum_loss = np.sum(y * np.log(y_pred + epsilon))\n    L = -sum_loss\n    return round(L, 4)\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: The function first converts the input lists or numpy arrays to numpy arrays to handle them uniformly.\n- **Logarithm and Summation**: It computes the sum of the product of each true label (from the one-hot encoded matrix) with the logarithm of the corresponding predicted probabilities. This step ensures that each term in the sum contributes only when the true label is 1, effectively summing the log probabilities of the correct predictions.\n- **Negative Sum**: The sum is then negated to match the standard cross-entropy loss formula, which is a positive value.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the cross-entropy loss using vectorized operations, making it suitable for large batches of data."}
{"task_id": 77, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward propagation for an L-layer neural network. The network uses ReLU activation for all hidden layers and the sigmoid function for the output layer. The goal is to compute the activation values for each layer and return the final activation of the output layer.\n\n### Approach\n1. **Initialization**: Start with the input matrix `X` and initialize the activation values `A_prev` to `X`.\n2. **Iterate through each layer**: For each layer from 1 to L:\n   - Compute the linear transformation using the current layer's weights (`W`) and bias (`b`).\n   - Apply the appropriate activation function: ReLU for hidden layers and sigmoid for the output layer.\n   - Store the computed activation values and the activation function used for each layer in a cache.\n3. **Update the activation values**: After processing each layer, update `A_prev` to the activation values of the current layer.\n4. **Return the final activation**: After processing all layers, return the activation values of the output layer, rounded to 4 decimal places and converted to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    L = len(parameters) // 2  # Number of layers\n    A_prev = X\n    caches = []\n    \n    for i in range(1, L + 1):\n        W = parameters[f'W{i}']\n        b = parameters[f'b{i}']\n        Z = np.dot(W, A_prev) + b\n        \n        if i < L:\n            A = np.maximum(0, Z)\n            caches.append((Z, 'ReLU'))\n        else:\n            A = 1 / (1 + np.exp(-Z))\n            caches.append((Z, 'sigmoid'))\n        \n        A_prev = A\n    \n    AL = np.round(A, 4)\n    return AL.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by determining the number of layers `L` from the parameters dictionary. The input `X` is the initial activation values for the first layer.\n- **Layer Processing**: For each layer, the linear transformation is computed using the current weights and biases. The activation function is applied based on whether the layer is a hidden layer (ReLU) or the output layer (sigmoid).\n- **Caching**: Each layer's pre-activation values (`Z`) and the activation function used are stored in a cache for later use in backpropagation.\n- **Update and Return**: After processing all layers, the final activation values are rounded to 4 decimal places and converted to a list of lists to be returned as the result.\n\nThis approach ensures that the forward propagation is correctly computed for each layer, preparing the necessary information for the subsequent backpropagation step."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy (log-loss) cost between two NumPy arrays, A2 (predicted probabilities) and Y (ground-truth labels). The binary cross-entropy cost is a common loss function used in classification tasks, and it measures the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clipping Values**: To avoid taking the logarithm of zero or one, we first clip the values of A2 to the interval [\u03b5, 1-\u03b5], where \u03b5 is a very small value (1e-15). This ensures that no element in A2 is exactly 0 or 1.\n2. **Compute Logarithms**: For each element in A2, compute the logarithm of the clipped values. Multiply these logarithms by the corresponding elements in Y.\n3. **Compute the Remaining Terms**: For elements where Y is 0, compute the logarithm of (1 - A2) and multiply by 1 (since 1 - Y will be 1 in these cases).\n4. **Sum the Costs**: Sum all the computed terms to get the total cost.\n5. **Round the Result**: Finally, round the total cost to six decimal places for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    y_part = Y * np.log(A2_clipped)\n    one_minus_y_part = (1 - Y) * np.log(1 - A2_clipped)\n    cost = np.sum(y_part + one_minus_y_part)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The `np.clip` function ensures that all values in A2 are within the range [\u03b5, 1-\u03b5], preventing any log(0) or log(1) operations.\n- **Logarithms and Multiplication**: Using NumPy's `log` function, we compute the logarithms of the clipped values and multiply them by the corresponding elements in Y. For elements where Y is 0, we compute the logarithm of (1 - A2) and multiply by 1.\n- **Summing Costs**: The sum of these terms gives the total cost, which is then rounded to six decimal places for the final result.\n\nThis approach efficiently computes the binary cross-entropy cost while handling potential numerical issues by clipping the probabilities, ensuring the function works robustly for all valid inputs."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to implement an \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy (best) action. The policy is defined by specific probabilities based on the Q-values of the actions and an exploration parameter \u03b5.\n\n### Approach\nThe \u03b5-soft exploration policy is designed to balance exploration and exploitation. The key steps to implement this policy are as follows:\n\n1. **Identify the Greedy Action**: Determine the action with the highest Q-value. If there are multiple actions with the same highest Q-value, select the first occurrence (smallest index).\n2. **Calculate Base Probability**: Compute the base probability for non-greedy actions using the formula `base = \u03b5 / n`, where `n` is the number of actions.\n3. **Compute Probabilities**: For the greedy action, the probability is calculated as `1 - \u03b5 + base`. For all other actions, the probability is `base`.\n4. **Round Probabilities**: Each probability is rounded to four decimal places to ensure precision and meet the problem requirements.\n\n### Solution Code\n```python\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:"}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to aggregate predictions from multiple decision trees using a majority vote mechanism. Each decision tree provides a prediction for every sample in the dataset, and the final prediction for each sample is determined by the majority vote across all trees. In case of a tie, the smallest label is chosen based on its type (numeric or string).\n\n### Approach\n1. **Input Structure**: The input is a 2D list where each inner list represents the predictions of one decision tree for all samples.\n2. **Iterate Over Samples**: For each sample, collect predictions from all trees.\n3. **Count Votes**: For each sample, count the occurrences of each label.\n4. **Determine Majority**: Identify the label(s) with the highest count. If there's a tie, break it by choosing the smallest label:\n   - For numeric labels, choose the smallest numeric value.\n   - For string labels, use lexicographic order.\n5. **Construct Result**: Append the determined label to the result list for each sample.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    \n    num_samples = len(predictions[0])\n    result = []\n    \n    for i in range(num_samples):\n        sample = [tree[i] for tree in predictions]\n        label_type = type(sample[0])\n        counts = {}\n        for label in sample:\n            counts[label] = counts.get(label, 0) + 1\n        \n        max_count = max(counts.values())\n        tied_labels = [label for label, cnt in counts.items() if cnt == max_count]\n        \n        if len(tied_labels) == 1:\n            chosen_label = tied_labels[0]\n        else:\n            if isinstance(label_type, (int, float)):\n                # Sort numerically\n                sorted_labels = sorted(tied_labels, key=lambda x: float(x))\n            else:\n                # Sort lexicographically\n                sorted_labels = sorted(tied_labels)\n            chosen_label = sorted_labels[0]\n        \n        result.append(chosen_label)\n    \n    return result\n```\n\n### Explanation\n1. **Input Handling**: The function starts by checking if the input is empty and returns an empty list if true.\n2. **Sample Processing**: For each sample, it collects predictions from all trees and counts the occurrences of each label.\n3. **Vote Aggregation**: It determines the label(s) with the highest count. If there's a tie, it sorts the tied labels appropriately (numerically or lexicographically) and selects the smallest one.\n4. **Result Construction**: The chosen label for each sample is appended to the result list, which is returned at the end.\n\nThis approach ensures that the final prediction is the majority vote, with ties broken by the smallest label as specified."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement an N-gram language model that computes the additive-smoothed log-probability of a given sentence using a provided corpus. The solution involves building frequency tables for all possible N-grams, applying additive smoothing, and then computing the log probability of the target sentence.\n\n### Approach\n1. **Collect Unique Words**: Extract all unique words from the provided corpus and include an additional \"<UNK>\" token to handle unseen words.\n2. **Build N-gram Frequency Tables**: For each sentence in the corpus, generate all possible 1-gram, 2-gram, ..., N-gram sequences and count their occurrences.\n3. **Process the Target Sentence**: For each contiguous N-gram in the target sentence, compute its probability using additive smoothing. Sum the log probabilities of all these N-grams to get the final result.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Collect all unique words in the corpus\n    all_words = set()\n    for sentence in corpus:\n        words = sentence.split()\n        all_words.update(words)\n    \n    # Add the unknown word token\n    vocabulary = all_words.union({'<UNK>')\n    vocab_size = len(vocabulary)\n    \n    # Build n-gram frequency tables\n    counters = [defaultdict(int) for _ in range(N+1)]\n    for sentence in corpus:\n        words = sentence.split()\n        for n in range(1, N+1):\n            for i in range(len(words) - n + 1):\n                ngram = ' '.join(words[i:i+n])\n                counters[n][ngram] += 1\n    \n    # Process the target sequence\n    target_words = sequence.split()\n    if len(target_words) < N:\n        return 0.0\n    \n    total_log = 0.0\n    for i in range(len(target_words) - N + 1):\n        ngram = ' '.join(target_words[i:i+N])\n        count = counters[N].get(ngram, 0)\n        prob = (count + K) / (count + K * vocab_size)\n        total_log += math.log(prob)\n    \n    return round(total_log, 4)\n```\n\n### Explanation\n1. **Collect Unique Words**: The code first gathers all unique words from the provided corpus and adds the \"<UNK>\" token to handle unseen words.\n2. **Build N-gram Frequency Tables**: For each sentence, the code generates all possible N-grams and updates their counts in respective frequency tables.\n3. **Process Target Sentence**: The target sentence is split into words. For each N-gram in the sentence, the code retrieves its count from the frequency table, applies additive smoothing, computes the log probability, and accumulates the total log probability.\n4. **Return Result**: The accumulated log probability is rounded to four decimal places and returned.\n\nThis approach ensures that we efficiently compute the additive-smoothed log probability of the target sentence using the provided N-gram model."}
{"task_id": 111, "completion_id": 0, "completion": "To solve this problem, we need to create three functions for character-level sequence encoding and decoding. These functions will convert raw text into a fixed-length list of integer token IDs, process these IDs through a neural model, and then convert the predicted IDs back into a human-readable string.\n\n### Approach\n1. **string_to_int**: This function converts each character of the input string into an integer using a provided vocabulary. The output is a fixed-length list of integers, padded or truncated to the specified length (TIME_STEPS). Characters not found in the vocabulary are replaced with a padding token (0).\n\n2. **int_to_string**: This function converts a list of integer IDs back into a string, ignoring any padding tokens (0s). It uses an inverse vocabulary to map each integer to its corresponding character.\n\n3. **run_example**: This function combines the encoding and decoding processes. It encodes the input text, runs it through a neural model, decodes the predicted IDs, and returns the resulting string.\n\n### Solution Code\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a raw text string into a fixed-length list of integer token ids.\"\"\"\n    encoded = []\n    for char in text:\n        if char in vocabulary:\n            encoded.append(vocabulary[char])\n        else:\n            encoded.append(0)\n    # Pad or truncate to TIME_STEPS\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    filtered_indices = [i for i in indices if i != 0]\n    return ''.join([inverse_vocab[i] for i in filtered_indices])\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on the input text and returns the decoded prediction.\"\"\"\n    # Encode the input text into integer tokens\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Reshape the encoded list to (1, TIME_STEPS) for model input\n    encoded_array = np.array(encoded, dtype=np.int64).reshape(1, -1)\n    # Get model predictions\n    predicted = model.predict(encoded_array)\n    # Extract the predicted indices for each time step\n    predicted_ids = np.argmax(predicted, axis=1)\n    # Decode the predicted indices into a string, ignoring padding tokens\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n1. **string_to_int**:\n   - Iterates through each character of the input string.\n   - Converts each character to its corresponding integer using the vocabulary. If a character is not found, it uses 0.\n   - Ensures the output list is exactly TIME_STEPS long by padding with zeros if necessary or truncating if too long.\n\n2. **int_to_string**:\n   - Takes a list of integers and converts them back into characters using the inverse vocabulary.\n   - Ignores any padding tokens (0s) to form the final string.\n\n3. **run_example**:\n   - Encodes the input text into integer tokens.\n   - Converts the encoded list into a numpy array with the correct shape for the model.\n   - Predicts the model's output for the encoded input.\n   - Converts the predicted integer IDs back into a string, ignoring any padding tokens, to get the final decoded text."}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to create a function that automates the process of running a trained model on multiple input strings and collecting the predictions. This is particularly useful in sequence-to-sequence or language generation projects where we want to quickly see the model's predictions on various inputs.\n\n### Approach\nThe function `run_examples` is designed to handle the following tasks:\n1. **Input Handling**: The function takes a model, input vocabulary, inverse output vocabulary, and an iterable of input strings. If the input strings are not provided, it falls back to a global constant `EXAMPLES`.\n2. **Model Execution**: For each input string, the function calls another helper function `run_example` to get the model's predictions.\n3. **Prediction Processing**: The predictions from `run_example` are processed into a single string.\n4. **Output Collection**: Each prediction is printed in the format \"input: [input string] and output: [predicted string]\", and the predicted string is collected into a list.\n5. **Return Result**: The list of predicted strings is returned.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    outputs = []\n    for example in examples:\n        prediction_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        prediction = ''.join(prediction_chars)\n        print(f\"input: {example}\")\n        print(f\"output: {prediction}\")\n        outputs.append(prediction)\n    return outputs\n```\n\n### Explanation\n1. **Function Definition**: The function `run_examples` is defined with four parameters: `model`, `input_vocabulary`, `inv_output_vocabulary`, and `examples`. The `examples` parameter defaults to `None` if not provided.\n2. **Fallback to Global Constant**: If `examples` is `None`, the function uses the global constant `EXAMPLES`.\n3. **Loop Through Examples**: For each input string in the examples, the function calls `run_example` to get the model's predictions.\n4. **Process Predictions**: The predictions are converted from a list of characters into a single string.\n5. **Print and Collect**: Each prediction is printed in the specified format, and the predicted string is added to the output list.\n6. **Return Result**: The list of predicted strings is returned after processing all examples.\n\nThis approach ensures that the function efficiently processes each example, provides clear output, and returns the results in the required format."}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost is an ensemble method that iteratively builds a set of weak classifiers (decision stumps) to improve classification performance. Each decision stump is a one-level decision tree, which is a simple threshold-based classifier.\n\n### Approach\n1. **Initialization**: Start with equal weights for all training examples.\n2. **Iterative Training**: For each iteration, find the decision stump that minimizes the weighted classification error. This involves:\n   - Sorting the training data based on each feature.\n   - Trying all possible splits for each feature to find the one that results in the minimum error.\n   - Selecting the feature with the smallest error and computing the corresponding weight (alpha_t) using the formula: \n     \\[\n     \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n     \\]\n   - Updating the weights based on the error of the selected stump.\n3. **Prediction**: After training all stumps, predict the class for each test sample by summing the weighted outputs of all stumps and taking the sign of the sum.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf: Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    m = X_train.shape[0]\n    if m == 0:\n        return []\n    n = X_train.shape[1]\n    \n    # Initialize weights\n    weights = np.ones(m) / m\n    classifiers = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature = -1\n        best_split = 0\n        \n        for j in range(n):\n            # Sort the data based on the current feature\n            sorted_indices = np.argsort(X_train[:, j])\n            sorted_weights = [weights[i] for i in sorted_indices]\n            \n            # Try all possible splits for the current feature\n            min_error_j = float('inf')\n            best_split_j = 0\n            for i in range(len(sorted_weights) - 1):\n                # Split after the i-th element\n                left_y = y_train[sorted_indices[:i+1]]\n                left_weights = sorted_weights[:i+1]\n                right_y = y_train[sorted_indices[i+1:]]\n                right_weights = sorted_weights[i+1:]\n                \n                # Calculate the error for this split\n                error = 0.0\n                for k in range(i+1):\n                    if left_y[k] == 1:\n                        error += left_weights[k]\n                for k in range(len(right_weights)):\n                    if right_y[k] == -1:\n                        error += right_weights[k]\n                \n                if error < min_error_j:\n                    min_error_j = error\n                    best_split_j = i + 1\n            \n            # Update the best feature and split if current feature is better\n            if min_error_j < min_error:\n                min_error = min_error_j\n                best_feature = j\n                best_split = best_split_j\n        \n        # Determine the threshold for the best split\n        threshold = X_train[sorted_indices[best_split - 1], best_feature]\n        # Calculate alpha_t\n        epsilon_t = min_error\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        \n        # Add the classifier to the list\n        classifiers.append((alpha_t, threshold))\n        \n        # Update the weights based on the current stump's performance\n        for i in range(m):\n            if (X_train[i, best_feature] >= threshold) != y_train[i]:\n                weights[i] *= np.exp(2 * alpha_t)\n    \n    # Predict using all the trained classifiers\n    predictions = []\n    for x in X_test:\n        total = 0.0\n        for alpha, threshold in classifiers:\n            if x >= threshold:\n                total += alpha\n            else:\n                total -= alpha\n        if total >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for each training example are initialized to 1/m, where m is the number of samples.\n2. **Training Each Stump**: For each iteration, the algorithm evaluates each feature to find the split that minimizes the weighted error. The error is calculated by considering all possible splits for each feature and selecting the one with the smallest error.\n3. **Weight Update**: After selecting the best split, the weights are updated to reflect the importance of each sample based on the error. Samples that are misclassified receive a higher weight to emphasize their importance in future iterations.\n4. **Prediction**: After training all stumps, each test sample is classified by summing the weighted outputs of all stumps and taking the sign of the sum to determine the predicted class.\n\nThis approach ensures that the AdaBoost algorithm effectively combines multiple weak classifiers to achieve better classification performance."}
{"task_id": 128, "completion_id": 0, "completion": "To solve this problem, we need to implement the classical Fisher\u2019s Linear Discriminant Analysis (LDA) for a multi-class scenario. The goal is to find a linear projection that maximizes the between-class scatter while minimizing the within-class scatter. This involves computing several key matrices and performing eigenvalue decomposition to derive the projection directions.\n\n### Approach\n1. **Compute Within-class Scatter Matrix (S_W)**: This matrix is calculated by summing the covariance matrices of each class.\n2. **Compute Between-class Scatter Matrix (S_B)**: This matrix is derived by summing the covariance matrices of the class means relative to the global mean, scaled by the number of samples in each class.\n3. **Form Matrix A**: This matrix is computed as the product of the pseudo-inverse of S_W and S_B to solve the generalized eigenvalue problem.\n4. **Eigenvalue Decomposition**: Decompose matrix A to get eigenvalues and eigenvectors. Sort these eigenvalues in descending order and adjust the sign of eigenvectors to ensure determinism.\n5. **Project Data**: Use the top n_components eigenvectors to project the data and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components == 0:\n        return []\n    \n    m, d = X.shape\n    if d == 0:\n        return []\n    \n    # Compute the global mean\n    mu = np.mean(X, axis=0)\n    \n    # Compute within-class scatter matrix S_W\n    S_W = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = (y == c)\n        mu_c = np.mean(X[mask], axis=0)\n        sum_ww = np.cov(X[mask], rowvar=False)\n        S_W += sum_ww\n    \n    # Compute between-class scatter matrix S_B\n    S_B = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = (y == c)\n        mu_c = np.mean(X[mask], axis=0)\n        n_c = np.sum(mask)\n        diff = mu_c - mu\n        S_B += n_c * (diff[:, np.newaxis] @ diff[:, np.newaxis].T)\n    \n    # Compute A = S_W^{-1} S_B using pseudo-inverse to handle singularity\n    try:\n        A = np.linalg.pinv(S_W) @ S_B\n    except np.linalg.LinAlgError:\n        A = np.linalg.pinv(S_W) @ S_B\n    \n    # Eigen-decomposition of A\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    # Sort eigenvalues and eigenvectors in descending order\n    eigenvalues = eigenvalues[::-1]\n    eigenvectors = eigenvectors[::-1]\n    \n    # Determine the sign of each eigenvector\n    for i in range(len(eigenvectors)):\n        vec = eigenvectors[i]\n        first_non_zero = np.argmax(np.abs(vec) > 1e-9)\n        if first_non_zero != 0:\n            if vec[first_non_zero] < 0:\n                eigenvectors[i] = -vec\n    \n    # Take the top n_components eigenvectors\n    W = eigenvectors[:n_components]\n    \n    # Project X onto the eigenvectors\n    projected = X @ W\n    # Round to four decimal places\n    projected_rounded = np.round(projected, 4)\n    \n    # Convert to list of lists\n    return projected_rounded.tolist()\n```\n\n### Explanation\n1. **Within-class Scatter Matrix (S_W)**: This matrix captures the variance within each class by summing the covariance matrices of each class.\n2. **Between-class Scatter Matrix (S_B)**: This matrix captures the variance between different classes by summing the covariance matrices of the differences between class means and the global mean, scaled by the number of samples in each class.\n3. **Matrix A**: This matrix is formed to solve the generalized eigenvalue problem, which helps in finding the optimal projection directions.\n4. **Eigenvalue Decomposition**: This step provides the projection directions (eigenvectors) that maximize the between-class variance.\n5. **Projection and Rounding**: The data is projected onto the top n_components eigenvectors, and the results are rounded to four decimal places for the final output.\n\nThis approach ensures that the projection maximizes the separation between classes while minimizing within-class variation, providing an effective dimensionality reduction technique."}
{"task_id": 165, "completion_id": 0, "completion": "To solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a metric used to measure the similarity between two strings of equal length, where the distance is the proportion of positions at which the corresponding elements are different.\n\n### Approach\n1. **Input Handling**: The function accepts both Python lists and NumPy arrays. If the inputs are lists, they are converted to NumPy arrays to facilitate element-wise operations.\n2. **Length Check**: The function first checks if the lengths of the two vectors are equal. If they are not, it returns -1.\n3. **Empty Vectors**: If both vectors are empty, the Hamming distance is considered 0.0.\n4. **Compute Differences**: Using NumPy, the function computes the element-wise differences between the two vectors and counts the number of differing positions.\n5. **Normalize and Round**: The count of differing positions is divided by the length of the vectors to get the normalized Hamming distance, which is then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert to NumPy arrays if they are lists\n    x = np.array(x) if isinstance(x, list) else x\n    y = np.array(y) if isinstance(y, list) else y\n\n    # Check if lengths are equal\n    if x.size != y.size:\n        return -1\n\n    # If both are empty, return 0.0\n    if x.size == 0:\n        return 0.0\n\n    # Compute the Hamming distance\n    diffs = (x != y)\n    sum_diffs = np.sum(diffs)\n    distance = sum_diffs / x.size\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The function converts input lists to NumPy arrays to handle both list and array inputs uniformly.\n2. **Length Check**: The function checks if the lengths of the two vectors are the same. If not, it returns -1.\n3. **Empty Vectors Handling**: If both vectors are empty, the function returns 0.0 as there are no elements to compare.\n4. **Element-wise Comparison**: Using NumPy, the function creates a boolean array indicating where the elements differ. The sum of this array gives the count of differing positions.\n5. **Normalization and Rounding**: The count of differing positions is divided by the total number of elements to get the normalized distance, which is then rounded to four decimal places for the final result.\n\nThis approach ensures that the function efficiently computes the Hamming distance while handling various input types and edge cases such as empty vectors."}
{"task_id": 178, "completion_id": 0, "completion": "To solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as the sum of the squares of the coordinates in a multi-dimensional space. The goal is to find the minimum value of this function using a swarm of particles that adjust their positions based on their own and the global best positions.\n\n### Approach\nThe PSO algorithm works by maintaining a swarm of particles, each with a position, velocity, and best known position. The particles move towards the best positions found by the swarm, guided by their velocities and a set of weights. The steps involved in the algorithm are as follows:\n\n1. **Initialization**: Each particle is initialized to a random position within the specified bounds. The initial velocity is set to zero, and each particle's personal best is its initial position.\n2. **Iteration**: For each iteration, the velocity of each particle is updated using a combination of the particle's inertia, the cognitive component, and the social component. The position of each particle is then updated by adding its velocity, clipped to stay within the bounds.\n3. **Best Update**: Each particle's personal best is updated if its new position results in a lower function value. The global best is the minimum of all personal bests.\n4. **Termination**: After the specified number of iterations, the best function value found is returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Input validation\n    if not (isinstance(n_dims, int) and isinstance(num_particles, int) and isinstance(num_iterations, int)):\n        return -1\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize particles\n    x = rng.uniform(-1, 1, size=(num_particles, n_dims))\n    v = np.zeros_like(x)\n    p_best = x.copy()\n    f_val = np.sum(x**2)\n    g_best = f_val\n    current_min = f_val\n\n    w = 0.5  # Inertia weight\n    c1 = 1.5  # Cognitive weight\n    c2 = 1.5  # Social weight\n\n    for _ in range(num_iterations):\n        # Generate random numbers for velocity updates\n        r1 = rng.uniform(0, 1, size=num_particles)\n        r2 = rng.uniform(0, 1, size=num_particles)\n\n        # Compute new velocities\n        v_new = w * v + c1 * r1 * (p_best - x) + c2 * r2 * (g_best - x)\n\n        # Update positions\n        x_new = x + v_new\n        x_new = np.clip(x_new, -1, 1)\n\n        # Compute function values for new positions\n        f_new = np.sum(x_new**2)\n\n        # Update personal bests\n        for i in range(num_particles):\n            if f_new[i] < p_best[i]:\n                p_best[i] = x_new[i]\n\n        # Find global best\n        g_best = np.min(p_best)\n        current_min = g_best\n\n        # Update for next iteration\n        x = x_new\n        v = v_new\n\n    # Return the best value rounded to four decimals\n    return round(current_min, 4)\n```\n\n### Explanation\n1. **Initialization**: The function starts by validating the input parameters. If any parameter is invalid, it returns -1. The random number generator is initialized to ensure reproducibility.\n2. **Particle Initialization**: Each particle's initial position is set to a random value within the range [-1, 1]. The initial velocity is zero, and each particle's personal best is its initial position.\n3. **Velocity and Position Update**: For each iteration, new velocities are computed using the inertia weight, cognitive component, and social component. The particle's position is updated by adding the new velocity, ensuring it stays within the bounds.\n4. **Best Update**: Each particle's personal best is updated if the new position results in a lower function value. The global best is the minimum of all personal bests.\n5. **Termination**: After the specified number of iterations, the best function value found is returned, rounded to four decimal places.\n\nThis approach ensures that the PSO algorithm efficiently explores the search space and converges to the minimum of the Sphere function."}
